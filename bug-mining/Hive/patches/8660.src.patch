diff --git a/llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/daemon/rpc/LlapDaemonProtocolProtos.java b/llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/daemon/rpc/LlapDaemonProtocolProtos.java
index b50448053b..dafa1e7a78 100644
--- a/llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/daemon/rpc/LlapDaemonProtocolProtos.java
+++ b/llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/daemon/rpc/LlapDaemonProtocolProtos.java
@@ -5,8 +5,14 @@
 
 public final class LlapDaemonProtocolProtos {
   private LlapDaemonProtocolProtos() {}
+  public static void registerAllExtensions(
+      com.google.protobuf.ExtensionRegistryLite registry) {
+  }
+
   public static void registerAllExtensions(
       com.google.protobuf.ExtensionRegistry registry) {
+    registerAllExtensions(
+        (com.google.protobuf.ExtensionRegistryLite) registry);
   }
   /**
    * Protobuf enum {@code SourceStateProto}
@@ -16,11 +22,11 @@ public enum SourceStateProto
     /**
      * <code>S_SUCCEEDED = 1;</code>
      */
-    S_SUCCEEDED(0, 1),
+    S_SUCCEEDED(1),
     /**
      * <code>S_RUNNING = 2;</code>
      */
-    S_RUNNING(1, 2),
+    S_RUNNING(2),
     ;
 
     /**
@@ -33,9 +39,25 @@ public enum SourceStateProto
     public static final int S_RUNNING_VALUE = 2;
 
 
-    public final int getNumber() { return value; }
+    public final int getNumber() {
+      return value;
+    }
 
+    /**
+     * @param value The numeric wire value of the corresponding enum entry.
+     * @return The enum associated with the given numeric wire value.
+     * @deprecated Use {@link #forNumber(int)} instead.
+     */
+    @java.lang.Deprecated
     public static SourceStateProto valueOf(int value) {
+      return forNumber(value);
+    }
+
+    /**
+     * @param value The numeric wire value of the corresponding enum entry.
+     * @return The enum associated with the given numeric wire value.
+     */
+    public static SourceStateProto forNumber(int value) {
       switch (value) {
         case 1: return S_SUCCEEDED;
         case 2: return S_RUNNING;
@@ -47,17 +69,17 @@ public static SourceStateProto valueOf(int value) {
         internalGetValueMap() {
       return internalValueMap;
     }
-    private static com.google.protobuf.Internal.EnumLiteMap<SourceStateProto>
-        internalValueMap =
+    private static final com.google.protobuf.Internal.EnumLiteMap<
+        SourceStateProto> internalValueMap =
           new com.google.protobuf.Internal.EnumLiteMap<SourceStateProto>() {
             public SourceStateProto findValueByNumber(int number) {
-              return SourceStateProto.valueOf(number);
+              return SourceStateProto.forNumber(number);
             }
           };
 
     public final com.google.protobuf.Descriptors.EnumValueDescriptor
         getValueDescriptor() {
-      return getDescriptor().getValues().get(index);
+      return getDescriptor().getValues().get(ordinal());
     }
     public final com.google.protobuf.Descriptors.EnumDescriptor
         getDescriptorForType() {
@@ -79,11 +101,9 @@ public static SourceStateProto valueOf(
       return VALUES[desc.getIndex()];
     }
 
-    private final int index;
     private final int value;
 
-    private SourceStateProto(int index, int value) {
-      this.index = index;
+    private SourceStateProto(int value) {
       this.value = value;
     }
 
@@ -98,15 +118,15 @@ public enum SubmissionStateProto
     /**
      * <code>ACCEPTED = 1;</code>
      */
-    ACCEPTED(0, 1),
+    ACCEPTED(1),
     /**
      * <code>REJECTED = 2;</code>
      */
-    REJECTED(1, 2),
+    REJECTED(2),
     /**
      * <code>EVICTED_OTHER = 3;</code>
      */
-    EVICTED_OTHER(2, 3),
+    EVICTED_OTHER(3),
     ;
 
     /**
@@ -123,9 +143,25 @@ public enum SubmissionStateProto
     public static final int EVICTED_OTHER_VALUE = 3;
 
 
-    public final int getNumber() { return value; }
+    public final int getNumber() {
+      return value;
+    }
 
+    /**
+     * @param value The numeric wire value of the corresponding enum entry.
+     * @return The enum associated with the given numeric wire value.
+     * @deprecated Use {@link #forNumber(int)} instead.
+     */
+    @java.lang.Deprecated
     public static SubmissionStateProto valueOf(int value) {
+      return forNumber(value);
+    }
+
+    /**
+     * @param value The numeric wire value of the corresponding enum entry.
+     * @return The enum associated with the given numeric wire value.
+     */
+    public static SubmissionStateProto forNumber(int value) {
       switch (value) {
         case 1: return ACCEPTED;
         case 2: return REJECTED;
@@ -138,17 +174,17 @@ public static SubmissionStateProto valueOf(int value) {
         internalGetValueMap() {
       return internalValueMap;
     }
-    private static com.google.protobuf.Internal.EnumLiteMap<SubmissionStateProto>
-        internalValueMap =
+    private static final com.google.protobuf.Internal.EnumLiteMap<
+        SubmissionStateProto> internalValueMap =
           new com.google.protobuf.Internal.EnumLiteMap<SubmissionStateProto>() {
             public SubmissionStateProto findValueByNumber(int number) {
-              return SubmissionStateProto.valueOf(number);
+              return SubmissionStateProto.forNumber(number);
             }
           };
 
     public final com.google.protobuf.Descriptors.EnumValueDescriptor
         getValueDescriptor() {
-      return getDescriptor().getValues().get(index);
+      return getDescriptor().getValues().get(ordinal());
     }
     public final com.google.protobuf.Descriptors.EnumDescriptor
         getDescriptorForType() {
@@ -170,37 +206,38 @@ public static SubmissionStateProto valueOf(
       return VALUES[desc.getIndex()];
     }
 
-    private final int index;
     private final int value;
 
-    private SubmissionStateProto(int index, int value) {
-      this.index = index;
+    private SubmissionStateProto(int value) {
       this.value = value;
     }
 
     // @@protoc_insertion_point(enum_scope:SubmissionStateProto)
   }
 
-  public interface UserPayloadProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface UserPayloadProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:UserPayloadProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional bytes user_payload = 1;
     /**
      * <code>optional bytes user_payload = 1;</code>
+     * @return Whether the userPayload field is set.
      */
     boolean hasUserPayload();
     /**
      * <code>optional bytes user_payload = 1;</code>
+     * @return The userPayload.
      */
     com.google.protobuf.ByteString getUserPayload();
 
-    // optional int32 version = 2;
     /**
      * <code>optional int32 version = 2;</code>
+     * @return Whether the version field is set.
      */
     boolean hasVersion();
     /**
      * <code>optional int32 version = 2;</code>
+     * @return The version.
      */
     int getVersion();
   }
@@ -208,186 +245,119 @@ public interface UserPayloadProtoOrBuilder
    * Protobuf type {@code UserPayloadProto}
    */
   public static final class UserPayloadProto extends
-      com.google.protobuf.GeneratedMessage
-      implements UserPayloadProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:UserPayloadProto)
+      UserPayloadProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use UserPayloadProto.newBuilder() to construct.
-    private UserPayloadProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private UserPayloadProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private UserPayloadProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final UserPayloadProto defaultInstance;
-    public static UserPayloadProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public UserPayloadProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private UserPayloadProto() {
+      userPayload_ = com.google.protobuf.ByteString.EMPTY;
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private UserPayloadProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              userPayload_ = input.readBytes();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              version_ = input.readInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new UserPayloadProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<UserPayloadProto> PARSER =
-        new com.google.protobuf.AbstractParser<UserPayloadProto>() {
-      public UserPayloadProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new UserPayloadProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<UserPayloadProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional bytes user_payload = 1;
     public static final int USER_PAYLOAD_FIELD_NUMBER = 1;
-    private com.google.protobuf.ByteString userPayload_;
+    private com.google.protobuf.ByteString userPayload_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>optional bytes user_payload = 1;</code>
+     * @return Whether the userPayload field is set.
      */
+    @java.lang.Override
     public boolean hasUserPayload() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional bytes user_payload = 1;</code>
+     * @return The userPayload.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getUserPayload() {
       return userPayload_;
     }
 
-    // optional int32 version = 2;
     public static final int VERSION_FIELD_NUMBER = 2;
-    private int version_;
+    private int version_ = 0;
     /**
      * <code>optional int32 version = 2;</code>
+     * @return Whether the version field is set.
      */
+    @java.lang.Override
     public boolean hasVersion() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional int32 version = 2;</code>
+     * @return The version.
      */
+    @java.lang.Override
     public int getVersion() {
       return version_;
     }
 
-    private void initFields() {
-      userPayload_ = com.google.protobuf.ByteString.EMPTY;
-      version_ = 0;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeBytes(1, userPayload_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeInt32(2, version_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(1, userPayload_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(2, version_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -398,30 +368,27 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto) obj;
 
-      boolean result = true;
-      result = result && (hasUserPayload() == other.hasUserPayload());
+      if (hasUserPayload() != other.hasUserPayload()) return false;
       if (hasUserPayload()) {
-        result = result && getUserPayload()
-            .equals(other.getUserPayload());
+        if (!getUserPayload()
+            .equals(other.getUserPayload())) return false;
       }
-      result = result && (hasVersion() == other.hasVersion());
+      if (hasVersion() != other.hasVersion()) return false;
       if (hasVersion()) {
-        result = result && (getVersion()
-            == other.getVersion());
+        if (getVersion()
+            != other.getVersion()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasUserPayload()) {
         hash = (37 * hash) + USER_PAYLOAD_FIELD_NUMBER;
         hash = (53 * hash) + getUserPayload().hashCode();
@@ -435,6 +402,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -458,46 +436,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Us
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -505,14 +498,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code UserPayloadProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:UserPayloadProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -521,44 +516,35 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         userPayload_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000001);
         version_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -567,23 +553,61 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPaylo
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.userPayload_ = userPayload_;
           to_bitField0_ |= 0x00000001;
         }
-        result.userPayload_ = userPayload_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.version_ = version_;
           to_bitField0_ |= 0x00000002;
         }
-        result.version_ = version_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto)other);
@@ -602,60 +626,90 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setVersion(other.getVersion());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                userPayload_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 16: {
+                version_ = input.readInt32();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 16
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional bytes user_payload = 1;
       private com.google.protobuf.ByteString userPayload_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>optional bytes user_payload = 1;</code>
+       * @return Whether the userPayload field is set.
        */
+      @java.lang.Override
       public boolean hasUserPayload() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional bytes user_payload = 1;</code>
+       * @return The userPayload.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getUserPayload() {
         return userPayload_;
       }
       /**
        * <code>optional bytes user_payload = 1;</code>
+       * @param value The userPayload to set.
+       * @return This builder for chaining.
        */
       public Builder setUserPayload(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         userPayload_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional bytes user_payload = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearUserPayload() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -664,31 +718,38 @@ public Builder clearUserPayload() {
         return this;
       }
 
-      // optional int32 version = 2;
       private int version_ ;
       /**
        * <code>optional int32 version = 2;</code>
+       * @return Whether the version field is set.
        */
+      @java.lang.Override
       public boolean hasVersion() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional int32 version = 2;</code>
+       * @return The version.
        */
+      @java.lang.Override
       public int getVersion() {
         return version_;
       }
       /**
        * <code>optional int32 version = 2;</code>
+       * @param value The version to set.
+       * @return This builder for chaining.
        */
       public Builder setVersion(int value) {
-        bitField0_ |= 0x00000002;
+
         version_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 version = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearVersion() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -696,43 +757,99 @@ public Builder clearVersion() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:UserPayloadProto)
     }
 
+    // @@protoc_insertion_point(class_scope:UserPayloadProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new UserPayloadProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<UserPayloadProto>
+        PARSER = new com.google.protobuf.AbstractParser<UserPayloadProto>() {
+      @java.lang.Override
+      public UserPayloadProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<UserPayloadProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<UserPayloadProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:UserPayloadProto)
   }
 
-  public interface EntityDescriptorProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface EntityDescriptorProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:EntityDescriptorProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string class_name = 1;
     /**
      * <code>optional string class_name = 1;</code>
+     * @return Whether the className field is set.
      */
     boolean hasClassName();
     /**
      * <code>optional string class_name = 1;</code>
+     * @return The className.
      */
     java.lang.String getClassName();
     /**
      * <code>optional string class_name = 1;</code>
+     * @return The bytes for className.
      */
     com.google.protobuf.ByteString
         getClassNameBytes();
 
-    // optional .UserPayloadProto user_payload = 2;
     /**
      * <code>optional .UserPayloadProto user_payload = 2;</code>
+     * @return Whether the userPayload field is set.
      */
     boolean hasUserPayload();
     /**
      * <code>optional .UserPayloadProto user_payload = 2;</code>
+     * @return The userPayload.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getUserPayload();
     /**
@@ -740,13 +857,14 @@ public interface EntityDescriptorProtoOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder getUserPayloadOrBuilder();
 
-    // optional bytes history_text = 3;
     /**
      * <code>optional bytes history_text = 3;</code>
+     * @return Whether the historyText field is set.
      */
     boolean hasHistoryText();
     /**
      * <code>optional bytes history_text = 3;</code>
+     * @return The historyText.
      */
     com.google.protobuf.ByteString getHistoryText();
   }
@@ -754,128 +872,56 @@ public interface EntityDescriptorProtoOrBuilder
    * Protobuf type {@code EntityDescriptorProto}
    */
   public static final class EntityDescriptorProto extends
-      com.google.protobuf.GeneratedMessage
-      implements EntityDescriptorProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:EntityDescriptorProto)
+      EntityDescriptorProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use EntityDescriptorProto.newBuilder() to construct.
-    private EntityDescriptorProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private EntityDescriptorProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private EntityDescriptorProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final EntityDescriptorProto defaultInstance;
-    public static EntityDescriptorProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public EntityDescriptorProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private EntityDescriptorProto() {
+      className_ = "";
+      historyText_ = com.google.protobuf.ByteString.EMPTY;
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private EntityDescriptorProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              className_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000002) == 0x00000002)) {
-                subBuilder = userPayload_.toBuilder();
-              }
-              userPayload_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(userPayload_);
-                userPayload_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000002;
-              break;
-            }
-            case 26: {
-              bitField0_ |= 0x00000004;
-              historyText_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new EntityDescriptorProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<EntityDescriptorProto> PARSER =
-        new com.google.protobuf.AbstractParser<EntityDescriptorProto>() {
-      public EntityDescriptorProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new EntityDescriptorProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<EntityDescriptorProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string class_name = 1;
     public static final int CLASS_NAME_FIELD_NUMBER = 1;
-    private java.lang.Object className_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object className_ = "";
     /**
      * <code>optional string class_name = 1;</code>
+     * @return Whether the className field is set.
      */
+    @java.lang.Override
     public boolean hasClassName() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string class_name = 1;</code>
+     * @return The className.
      */
+    @java.lang.Override
     public java.lang.String getClassName() {
       java.lang.Object ref = className_;
       if (ref instanceof java.lang.String) {
@@ -892,7 +938,9 @@ public java.lang.String getClassName() {
     }
     /**
      * <code>optional string class_name = 1;</code>
+     * @return The bytes for className.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getClassNameBytes() {
       java.lang.Object ref = className_;
@@ -907,103 +955,99 @@ public java.lang.String getClassName() {
       }
     }
 
-    // optional .UserPayloadProto user_payload = 2;
     public static final int USER_PAYLOAD_FIELD_NUMBER = 2;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto userPayload_;
     /**
      * <code>optional .UserPayloadProto user_payload = 2;</code>
+     * @return Whether the userPayload field is set.
      */
+    @java.lang.Override
     public boolean hasUserPayload() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional .UserPayloadProto user_payload = 2;</code>
+     * @return The userPayload.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getUserPayload() {
-      return userPayload_;
+      return userPayload_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance() : userPayload_;
     }
     /**
      * <code>optional .UserPayloadProto user_payload = 2;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder getUserPayloadOrBuilder() {
-      return userPayload_;
+      return userPayload_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance() : userPayload_;
     }
 
-    // optional bytes history_text = 3;
     public static final int HISTORY_TEXT_FIELD_NUMBER = 3;
-    private com.google.protobuf.ByteString historyText_;
+    private com.google.protobuf.ByteString historyText_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>optional bytes history_text = 3;</code>
+     * @return Whether the historyText field is set.
      */
+    @java.lang.Override
     public boolean hasHistoryText() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional bytes history_text = 3;</code>
+     * @return The historyText.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getHistoryText() {
       return historyText_;
     }
 
-    private void initFields() {
-      className_ = "";
-      userPayload_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance();
-      historyText_ = com.google.protobuf.ByteString.EMPTY;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getClassNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, className_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeMessage(2, userPayload_);
+      if (((bitField0_ & 0x00000002) != 0)) {
+        output.writeMessage(2, getUserPayload());
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         output.writeBytes(3, historyText_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getClassNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, className_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(2, userPayload_);
+          .computeMessageSize(2, getUserPayload());
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(3, historyText_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -1014,35 +1058,32 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto) obj;
 
-      boolean result = true;
-      result = result && (hasClassName() == other.hasClassName());
+      if (hasClassName() != other.hasClassName()) return false;
       if (hasClassName()) {
-        result = result && getClassName()
-            .equals(other.getClassName());
+        if (!getClassName()
+            .equals(other.getClassName())) return false;
       }
-      result = result && (hasUserPayload() == other.hasUserPayload());
+      if (hasUserPayload() != other.hasUserPayload()) return false;
       if (hasUserPayload()) {
-        result = result && getUserPayload()
-            .equals(other.getUserPayload());
+        if (!getUserPayload()
+            .equals(other.getUserPayload())) return false;
       }
-      result = result && (hasHistoryText() == other.hasHistoryText());
+      if (hasHistoryText() != other.hasHistoryText()) return false;
       if (hasHistoryText()) {
-        result = result && getHistoryText()
-            .equals(other.getHistoryText());
+        if (!getHistoryText()
+            .equals(other.getHistoryText())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasClassName()) {
         hash = (37 * hash) + CLASS_NAME_FIELD_NUMBER;
         hash = (53 * hash) + getClassName().hashCode();
@@ -1060,6 +1101,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -1083,46 +1135,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.En
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -1130,14 +1197,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code EntityDescriptorProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:EntityDescriptorProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -1150,47 +1219,42 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getUserPayloadFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         className_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
-        if (userPayloadBuilder_ == null) {
-          userPayload_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance();
-        } else {
-          userPayloadBuilder_.clear();
+        userPayload_ = null;
+        if (userPayloadBuilder_ != null) {
+          userPayloadBuilder_.dispose();
+          userPayloadBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000002);
         historyText_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -1199,31 +1263,67 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDes
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.className_ = className_;
           to_bitField0_ |= 0x00000001;
         }
-        result.className_ = className_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.userPayload_ = userPayloadBuilder_ == null
+              ? userPayload_
+              : userPayloadBuilder_.build();
           to_bitField0_ |= 0x00000002;
         }
-        if (userPayloadBuilder_ == null) {
-          result.userPayload_ = userPayload_;
-        } else {
-          result.userPayload_ = userPayloadBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.historyText_ = historyText_;
           to_bitField0_ |= 0x00000004;
         }
-        result.historyText_ = historyText_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto)other);
@@ -1236,8 +1336,8 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) return this;
         if (other.hasClassName()) {
-          bitField0_ |= 0x00000001;
           className_ = other.className_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasUserPayload()) {
@@ -1247,49 +1347,86 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setHistoryText(other.getHistoryText());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                className_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                input.readMessage(
+                    getUserPayloadFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 26: {
+                historyText_ = input.readBytes();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 26
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string class_name = 1;
       private java.lang.Object className_ = "";
       /**
        * <code>optional string class_name = 1;</code>
+       * @return Whether the className field is set.
        */
       public boolean hasClassName() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string class_name = 1;</code>
+       * @return The className.
        */
       public java.lang.String getClassName() {
         java.lang.Object ref = className_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          className_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            className_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -1297,6 +1434,7 @@ public java.lang.String getClassName() {
       }
       /**
        * <code>optional string class_name = 1;</code>
+       * @return The bytes for className.
        */
       public com.google.protobuf.ByteString
           getClassNameBytes() {
@@ -1313,56 +1451,58 @@ public java.lang.String getClassName() {
       }
       /**
        * <code>optional string class_name = 1;</code>
+       * @param value The className to set.
+       * @return This builder for chaining.
        */
       public Builder setClassName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         className_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string class_name = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearClassName() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         className_ = getDefaultInstance().getClassName();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string class_name = 1;</code>
+       * @param value The bytes for className to set.
+       * @return This builder for chaining.
        */
       public Builder setClassNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         className_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // optional .UserPayloadProto user_payload = 2;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto userPayload_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto userPayload_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder> userPayloadBuilder_;
       /**
        * <code>optional .UserPayloadProto user_payload = 2;</code>
+       * @return Whether the userPayload field is set.
        */
       public boolean hasUserPayload() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional .UserPayloadProto user_payload = 2;</code>
+       * @return The userPayload.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getUserPayload() {
         if (userPayloadBuilder_ == null) {
-          return userPayload_;
+          return userPayload_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance() : userPayload_;
         } else {
           return userPayloadBuilder_.getMessage();
         }
@@ -1376,11 +1516,11 @@ public Builder setUserPayload(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonP
             throw new NullPointerException();
           }
           userPayload_ = value;
-          onChanged();
         } else {
           userPayloadBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000002;
+        onChanged();
         return this;
       }
       /**
@@ -1390,11 +1530,11 @@ public Builder setUserPayload(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder builderForValue) {
         if (userPayloadBuilder_ == null) {
           userPayload_ = builderForValue.build();
-          onChanged();
         } else {
           userPayloadBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000002;
+        onChanged();
         return this;
       }
       /**
@@ -1402,31 +1542,33 @@ public Builder setUserPayload(
        */
       public Builder mergeUserPayload(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto value) {
         if (userPayloadBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002) &&
-              userPayload_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance()) {
-            userPayload_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.newBuilder(userPayload_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000002) != 0) &&
+            userPayload_ != null &&
+            userPayload_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance()) {
+            getUserPayloadBuilder().mergeFrom(value);
           } else {
             userPayload_ = value;
           }
-          onChanged();
         } else {
           userPayloadBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000002;
-        return this;
+        if (userPayload_ != null) {
+          bitField0_ |= 0x00000002;
+          onChanged();
+        }
+        return this;
       }
       /**
        * <code>optional .UserPayloadProto user_payload = 2;</code>
        */
       public Builder clearUserPayload() {
-        if (userPayloadBuilder_ == null) {
-          userPayload_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance();
-          onChanged();
-        } else {
-          userPayloadBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000002);
+        userPayload_ = null;
+        if (userPayloadBuilder_ != null) {
+          userPayloadBuilder_.dispose();
+          userPayloadBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -1444,19 +1586,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPaylo
         if (userPayloadBuilder_ != null) {
           return userPayloadBuilder_.getMessageOrBuilder();
         } else {
-          return userPayload_;
+          return userPayload_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance() : userPayload_;
         }
       }
       /**
        * <code>optional .UserPayloadProto user_payload = 2;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder> 
           getUserPayloadFieldBuilder() {
         if (userPayloadBuilder_ == null) {
-          userPayloadBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          userPayloadBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder>(
-                  userPayload_,
+                  getUserPayload(),
                   getParentForChildren(),
                   isClean());
           userPayload_ = null;
@@ -1464,34 +1607,38 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPaylo
         return userPayloadBuilder_;
       }
 
-      // optional bytes history_text = 3;
       private com.google.protobuf.ByteString historyText_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>optional bytes history_text = 3;</code>
+       * @return Whether the historyText field is set.
        */
+      @java.lang.Override
       public boolean hasHistoryText() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional bytes history_text = 3;</code>
+       * @return The historyText.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getHistoryText() {
         return historyText_;
       }
       /**
        * <code>optional bytes history_text = 3;</code>
+       * @param value The historyText to set.
+       * @return This builder for chaining.
        */
       public Builder setHistoryText(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000004;
+        if (value == null) { throw new NullPointerException(); }
         historyText_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>optional bytes history_text = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearHistoryText() {
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -1499,43 +1646,99 @@ public Builder clearHistoryText() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:EntityDescriptorProto)
     }
 
+    // @@protoc_insertion_point(class_scope:EntityDescriptorProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new EntityDescriptorProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<EntityDescriptorProto>
+        PARSER = new com.google.protobuf.AbstractParser<EntityDescriptorProto>() {
+      @java.lang.Override
+      public EntityDescriptorProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<EntityDescriptorProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<EntityDescriptorProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:EntityDescriptorProto)
   }
 
-  public interface IOSpecProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface IOSpecProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:IOSpecProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string connected_vertex_name = 1;
     /**
      * <code>optional string connected_vertex_name = 1;</code>
+     * @return Whether the connectedVertexName field is set.
      */
     boolean hasConnectedVertexName();
     /**
      * <code>optional string connected_vertex_name = 1;</code>
+     * @return The connectedVertexName.
      */
     java.lang.String getConnectedVertexName();
     /**
      * <code>optional string connected_vertex_name = 1;</code>
+     * @return The bytes for connectedVertexName.
      */
     com.google.protobuf.ByteString
         getConnectedVertexNameBytes();
 
-    // optional .EntityDescriptorProto io_descriptor = 2;
     /**
      * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
+     * @return Whether the ioDescriptor field is set.
      */
     boolean hasIoDescriptor();
     /**
      * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
+     * @return The ioDescriptor.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getIoDescriptor();
     /**
@@ -1543,13 +1746,14 @@ public interface IOSpecProtoOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getIoDescriptorOrBuilder();
 
-    // optional int32 physical_edge_count = 3;
     /**
      * <code>optional int32 physical_edge_count = 3;</code>
+     * @return Whether the physicalEdgeCount field is set.
      */
     boolean hasPhysicalEdgeCount();
     /**
      * <code>optional int32 physical_edge_count = 3;</code>
+     * @return The physicalEdgeCount.
      */
     int getPhysicalEdgeCount();
   }
@@ -1557,128 +1761,55 @@ public interface IOSpecProtoOrBuilder
    * Protobuf type {@code IOSpecProto}
    */
   public static final class IOSpecProto extends
-      com.google.protobuf.GeneratedMessage
-      implements IOSpecProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:IOSpecProto)
+      IOSpecProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use IOSpecProto.newBuilder() to construct.
-    private IOSpecProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private IOSpecProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private IOSpecProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final IOSpecProto defaultInstance;
-    public static IOSpecProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public IOSpecProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private IOSpecProto() {
+      connectedVertexName_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private IOSpecProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              connectedVertexName_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000002) == 0x00000002)) {
-                subBuilder = ioDescriptor_.toBuilder();
-              }
-              ioDescriptor_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(ioDescriptor_);
-                ioDescriptor_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000002;
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              physicalEdgeCount_ = input.readInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new IOSpecProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<IOSpecProto> PARSER =
-        new com.google.protobuf.AbstractParser<IOSpecProto>() {
-      public IOSpecProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new IOSpecProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<IOSpecProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string connected_vertex_name = 1;
     public static final int CONNECTED_VERTEX_NAME_FIELD_NUMBER = 1;
-    private java.lang.Object connectedVertexName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object connectedVertexName_ = "";
     /**
      * <code>optional string connected_vertex_name = 1;</code>
+     * @return Whether the connectedVertexName field is set.
      */
+    @java.lang.Override
     public boolean hasConnectedVertexName() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string connected_vertex_name = 1;</code>
+     * @return The connectedVertexName.
      */
+    @java.lang.Override
     public java.lang.String getConnectedVertexName() {
       java.lang.Object ref = connectedVertexName_;
       if (ref instanceof java.lang.String) {
@@ -1695,7 +1826,9 @@ public java.lang.String getConnectedVertexName() {
     }
     /**
      * <code>optional string connected_vertex_name = 1;</code>
+     * @return The bytes for connectedVertexName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getConnectedVertexNameBytes() {
       java.lang.Object ref = connectedVertexName_;
@@ -1710,103 +1843,99 @@ public java.lang.String getConnectedVertexName() {
       }
     }
 
-    // optional .EntityDescriptorProto io_descriptor = 2;
     public static final int IO_DESCRIPTOR_FIELD_NUMBER = 2;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto ioDescriptor_;
     /**
      * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
+     * @return Whether the ioDescriptor field is set.
      */
+    @java.lang.Override
     public boolean hasIoDescriptor() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
+     * @return The ioDescriptor.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getIoDescriptor() {
-      return ioDescriptor_;
+      return ioDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : ioDescriptor_;
     }
     /**
      * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getIoDescriptorOrBuilder() {
-      return ioDescriptor_;
+      return ioDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : ioDescriptor_;
     }
 
-    // optional int32 physical_edge_count = 3;
     public static final int PHYSICAL_EDGE_COUNT_FIELD_NUMBER = 3;
-    private int physicalEdgeCount_;
+    private int physicalEdgeCount_ = 0;
     /**
      * <code>optional int32 physical_edge_count = 3;</code>
+     * @return Whether the physicalEdgeCount field is set.
      */
+    @java.lang.Override
     public boolean hasPhysicalEdgeCount() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional int32 physical_edge_count = 3;</code>
+     * @return The physicalEdgeCount.
      */
+    @java.lang.Override
     public int getPhysicalEdgeCount() {
       return physicalEdgeCount_;
     }
 
-    private void initFields() {
-      connectedVertexName_ = "";
-      ioDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-      physicalEdgeCount_ = 0;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getConnectedVertexNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, connectedVertexName_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeMessage(2, ioDescriptor_);
+      if (((bitField0_ & 0x00000002) != 0)) {
+        output.writeMessage(2, getIoDescriptor());
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         output.writeInt32(3, physicalEdgeCount_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getConnectedVertexNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, connectedVertexName_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(2, ioDescriptor_);
+          .computeMessageSize(2, getIoDescriptor());
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(3, physicalEdgeCount_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -1817,35 +1946,32 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto) obj;
 
-      boolean result = true;
-      result = result && (hasConnectedVertexName() == other.hasConnectedVertexName());
+      if (hasConnectedVertexName() != other.hasConnectedVertexName()) return false;
       if (hasConnectedVertexName()) {
-        result = result && getConnectedVertexName()
-            .equals(other.getConnectedVertexName());
+        if (!getConnectedVertexName()
+            .equals(other.getConnectedVertexName())) return false;
       }
-      result = result && (hasIoDescriptor() == other.hasIoDescriptor());
+      if (hasIoDescriptor() != other.hasIoDescriptor()) return false;
       if (hasIoDescriptor()) {
-        result = result && getIoDescriptor()
-            .equals(other.getIoDescriptor());
+        if (!getIoDescriptor()
+            .equals(other.getIoDescriptor())) return false;
       }
-      result = result && (hasPhysicalEdgeCount() == other.hasPhysicalEdgeCount());
+      if (hasPhysicalEdgeCount() != other.hasPhysicalEdgeCount()) return false;
       if (hasPhysicalEdgeCount()) {
-        result = result && (getPhysicalEdgeCount()
-            == other.getPhysicalEdgeCount());
+        if (getPhysicalEdgeCount()
+            != other.getPhysicalEdgeCount()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasConnectedVertexName()) {
         hash = (37 * hash) + CONNECTED_VERTEX_NAME_FIELD_NUMBER;
         hash = (53 * hash) + getConnectedVertexName().hashCode();
@@ -1863,6 +1989,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -1886,46 +2023,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IO
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -1933,14 +2085,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code IOSpecProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:IOSpecProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -1953,47 +2107,42 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getIoDescriptorFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         connectedVertexName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
-        if (ioDescriptorBuilder_ == null) {
-          ioDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-        } else {
-          ioDescriptorBuilder_.clear();
+        ioDescriptor_ = null;
+        if (ioDescriptorBuilder_ != null) {
+          ioDescriptorBuilder_.dispose();
+          ioDescriptorBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000002);
         physicalEdgeCount_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -2002,31 +2151,67 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecPro
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.connectedVertexName_ = connectedVertexName_;
           to_bitField0_ |= 0x00000001;
         }
-        result.connectedVertexName_ = connectedVertexName_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.ioDescriptor_ = ioDescriptorBuilder_ == null
+              ? ioDescriptor_
+              : ioDescriptorBuilder_.build();
           to_bitField0_ |= 0x00000002;
         }
-        if (ioDescriptorBuilder_ == null) {
-          result.ioDescriptor_ = ioDescriptor_;
-        } else {
-          result.ioDescriptor_ = ioDescriptorBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.physicalEdgeCount_ = physicalEdgeCount_;
           to_bitField0_ |= 0x00000004;
         }
-        result.physicalEdgeCount_ = physicalEdgeCount_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto)other);
@@ -2039,8 +2224,8 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.getDefaultInstance()) return this;
         if (other.hasConnectedVertexName()) {
-          bitField0_ |= 0x00000001;
           connectedVertexName_ = other.connectedVertexName_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasIoDescriptor()) {
@@ -2050,49 +2235,86 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setPhysicalEdgeCount(other.getPhysicalEdgeCount());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                connectedVertexName_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                input.readMessage(
+                    getIoDescriptorFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 24: {
+                physicalEdgeCount_ = input.readInt32();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 24
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string connected_vertex_name = 1;
       private java.lang.Object connectedVertexName_ = "";
       /**
        * <code>optional string connected_vertex_name = 1;</code>
+       * @return Whether the connectedVertexName field is set.
        */
       public boolean hasConnectedVertexName() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string connected_vertex_name = 1;</code>
+       * @return The connectedVertexName.
        */
       public java.lang.String getConnectedVertexName() {
         java.lang.Object ref = connectedVertexName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          connectedVertexName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            connectedVertexName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -2100,6 +2322,7 @@ public java.lang.String getConnectedVertexName() {
       }
       /**
        * <code>optional string connected_vertex_name = 1;</code>
+       * @return The bytes for connectedVertexName.
        */
       public com.google.protobuf.ByteString
           getConnectedVertexNameBytes() {
@@ -2116,56 +2339,58 @@ public java.lang.String getConnectedVertexName() {
       }
       /**
        * <code>optional string connected_vertex_name = 1;</code>
+       * @param value The connectedVertexName to set.
+       * @return This builder for chaining.
        */
       public Builder setConnectedVertexName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         connectedVertexName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string connected_vertex_name = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearConnectedVertexName() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         connectedVertexName_ = getDefaultInstance().getConnectedVertexName();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string connected_vertex_name = 1;</code>
+       * @param value The bytes for connectedVertexName to set.
+       * @return This builder for chaining.
        */
       public Builder setConnectedVertexNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         connectedVertexName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // optional .EntityDescriptorProto io_descriptor = 2;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto ioDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto ioDescriptor_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> ioDescriptorBuilder_;
       /**
        * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
+       * @return Whether the ioDescriptor field is set.
        */
       public boolean hasIoDescriptor() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
+       * @return The ioDescriptor.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getIoDescriptor() {
         if (ioDescriptorBuilder_ == null) {
-          return ioDescriptor_;
+          return ioDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : ioDescriptor_;
         } else {
           return ioDescriptorBuilder_.getMessage();
         }
@@ -2179,11 +2404,11 @@ public Builder setIoDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemon
             throw new NullPointerException();
           }
           ioDescriptor_ = value;
-          onChanged();
         } else {
           ioDescriptorBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000002;
+        onChanged();
         return this;
       }
       /**
@@ -2193,11 +2418,11 @@ public Builder setIoDescriptor(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder builderForValue) {
         if (ioDescriptorBuilder_ == null) {
           ioDescriptor_ = builderForValue.build();
-          onChanged();
         } else {
           ioDescriptorBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000002;
+        onChanged();
         return this;
       }
       /**
@@ -2205,31 +2430,33 @@ public Builder setIoDescriptor(
        */
       public Builder mergeIoDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto value) {
         if (ioDescriptorBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002) &&
-              ioDescriptor_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) {
-            ioDescriptor_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.newBuilder(ioDescriptor_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000002) != 0) &&
+            ioDescriptor_ != null &&
+            ioDescriptor_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) {
+            getIoDescriptorBuilder().mergeFrom(value);
           } else {
             ioDescriptor_ = value;
           }
-          onChanged();
         } else {
           ioDescriptorBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000002;
+        if (ioDescriptor_ != null) {
+          bitField0_ |= 0x00000002;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
        */
       public Builder clearIoDescriptor() {
-        if (ioDescriptorBuilder_ == null) {
-          ioDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-          onChanged();
-        } else {
-          ioDescriptorBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000002);
+        ioDescriptor_ = null;
+        if (ioDescriptorBuilder_ != null) {
+          ioDescriptorBuilder_.dispose();
+          ioDescriptorBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -2247,19 +2474,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDes
         if (ioDescriptorBuilder_ != null) {
           return ioDescriptorBuilder_.getMessageOrBuilder();
         } else {
-          return ioDescriptor_;
+          return ioDescriptor_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : ioDescriptor_;
         }
       }
       /**
        * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> 
           getIoDescriptorFieldBuilder() {
         if (ioDescriptorBuilder_ == null) {
-          ioDescriptorBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          ioDescriptorBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder>(
-                  ioDescriptor_,
+                  getIoDescriptor(),
                   getParentForChildren(),
                   isClean());
           ioDescriptor_ = null;
@@ -2267,31 +2495,38 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDes
         return ioDescriptorBuilder_;
       }
 
-      // optional int32 physical_edge_count = 3;
       private int physicalEdgeCount_ ;
       /**
        * <code>optional int32 physical_edge_count = 3;</code>
+       * @return Whether the physicalEdgeCount field is set.
        */
+      @java.lang.Override
       public boolean hasPhysicalEdgeCount() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional int32 physical_edge_count = 3;</code>
+       * @return The physicalEdgeCount.
        */
+      @java.lang.Override
       public int getPhysicalEdgeCount() {
         return physicalEdgeCount_;
       }
       /**
        * <code>optional int32 physical_edge_count = 3;</code>
+       * @param value The physicalEdgeCount to set.
+       * @return This builder for chaining.
        */
       public Builder setPhysicalEdgeCount(int value) {
-        bitField0_ |= 0x00000004;
+
         physicalEdgeCount_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 physical_edge_count = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearPhysicalEdgeCount() {
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -2299,63 +2534,124 @@ public Builder clearPhysicalEdgeCount() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:IOSpecProto)
     }
 
+    // @@protoc_insertion_point(class_scope:IOSpecProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new IOSpecProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<IOSpecProto>
+        PARSER = new com.google.protobuf.AbstractParser<IOSpecProto>() {
+      @java.lang.Override
+      public IOSpecProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<IOSpecProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<IOSpecProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:IOSpecProto)
   }
 
-  public interface GroupInputSpecProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface GroupInputSpecProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:GroupInputSpecProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string group_name = 1;
     /**
      * <code>optional string group_name = 1;</code>
+     * @return Whether the groupName field is set.
      */
     boolean hasGroupName();
     /**
      * <code>optional string group_name = 1;</code>
+     * @return The groupName.
      */
     java.lang.String getGroupName();
     /**
      * <code>optional string group_name = 1;</code>
+     * @return The bytes for groupName.
      */
     com.google.protobuf.ByteString
         getGroupNameBytes();
 
-    // repeated string group_vertices = 2;
     /**
      * <code>repeated string group_vertices = 2;</code>
+     * @return A list containing the groupVertices.
      */
     java.util.List<java.lang.String>
-    getGroupVerticesList();
+        getGroupVerticesList();
     /**
      * <code>repeated string group_vertices = 2;</code>
+     * @return The count of groupVertices.
      */
     int getGroupVerticesCount();
     /**
      * <code>repeated string group_vertices = 2;</code>
+     * @param index The index of the element to return.
+     * @return The groupVertices at the given index.
      */
     java.lang.String getGroupVertices(int index);
     /**
      * <code>repeated string group_vertices = 2;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the groupVertices at the given index.
      */
     com.google.protobuf.ByteString
         getGroupVerticesBytes(int index);
 
-    // optional .EntityDescriptorProto merged_input_descriptor = 3;
     /**
      * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
+     * @return Whether the mergedInputDescriptor field is set.
      */
     boolean hasMergedInputDescriptor();
     /**
      * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
+     * @return The mergedInputDescriptor.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getMergedInputDescriptor();
     /**
@@ -2367,134 +2663,57 @@ public interface GroupInputSpecProtoOrBuilder
    * Protobuf type {@code GroupInputSpecProto}
    */
   public static final class GroupInputSpecProto extends
-      com.google.protobuf.GeneratedMessage
-      implements GroupInputSpecProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:GroupInputSpecProto)
+      GroupInputSpecProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use GroupInputSpecProto.newBuilder() to construct.
-    private GroupInputSpecProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private GroupInputSpecProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private GroupInputSpecProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final GroupInputSpecProto defaultInstance;
-    public static GroupInputSpecProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public GroupInputSpecProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private GroupInputSpecProto() {
+      groupName_ = "";
+      groupVertices_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private GroupInputSpecProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              groupName_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-                groupVertices_ = new com.google.protobuf.LazyStringArrayList();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              groupVertices_.add(input.readBytes());
-              break;
-            }
-            case 26: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000002) == 0x00000002)) {
-                subBuilder = mergedInputDescriptor_.toBuilder();
-              }
-              mergedInputDescriptor_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(mergedInputDescriptor_);
-                mergedInputDescriptor_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000002;
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-          groupVertices_ = new com.google.protobuf.UnmodifiableLazyStringList(groupVertices_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new GroupInputSpecProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<GroupInputSpecProto> PARSER =
-        new com.google.protobuf.AbstractParser<GroupInputSpecProto>() {
-      public GroupInputSpecProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new GroupInputSpecProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<GroupInputSpecProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string group_name = 1;
     public static final int GROUP_NAME_FIELD_NUMBER = 1;
-    private java.lang.Object groupName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object groupName_ = "";
     /**
      * <code>optional string group_name = 1;</code>
+     * @return Whether the groupName field is set.
      */
+    @java.lang.Override
     public boolean hasGroupName() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string group_name = 1;</code>
+     * @return The groupName.
      */
+    @java.lang.Override
     public java.lang.String getGroupName() {
       java.lang.Object ref = groupName_;
       if (ref instanceof java.lang.String) {
@@ -2511,7 +2730,9 @@ public java.lang.String getGroupName() {
     }
     /**
      * <code>optional string group_name = 1;</code>
+     * @return The bytes for groupName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getGroupNameBytes() {
       java.lang.Object ref = groupName_;
@@ -2526,122 +2747,121 @@ public java.lang.String getGroupName() {
       }
     }
 
-    // repeated string group_vertices = 2;
     public static final int GROUP_VERTICES_FIELD_NUMBER = 2;
-    private com.google.protobuf.LazyStringList groupVertices_;
+    @SuppressWarnings("serial")
+    private com.google.protobuf.LazyStringArrayList groupVertices_ =
+        com.google.protobuf.LazyStringArrayList.emptyList();
     /**
      * <code>repeated string group_vertices = 2;</code>
+     * @return A list containing the groupVertices.
      */
-    public java.util.List<java.lang.String>
+    public com.google.protobuf.ProtocolStringList
         getGroupVerticesList() {
       return groupVertices_;
     }
     /**
      * <code>repeated string group_vertices = 2;</code>
+     * @return The count of groupVertices.
      */
     public int getGroupVerticesCount() {
       return groupVertices_.size();
     }
     /**
      * <code>repeated string group_vertices = 2;</code>
+     * @param index The index of the element to return.
+     * @return The groupVertices at the given index.
      */
     public java.lang.String getGroupVertices(int index) {
       return groupVertices_.get(index);
     }
     /**
      * <code>repeated string group_vertices = 2;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the groupVertices at the given index.
      */
     public com.google.protobuf.ByteString
         getGroupVerticesBytes(int index) {
       return groupVertices_.getByteString(index);
     }
 
-    // optional .EntityDescriptorProto merged_input_descriptor = 3;
     public static final int MERGED_INPUT_DESCRIPTOR_FIELD_NUMBER = 3;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto mergedInputDescriptor_;
     /**
      * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
+     * @return Whether the mergedInputDescriptor field is set.
      */
+    @java.lang.Override
     public boolean hasMergedInputDescriptor() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
+     * @return The mergedInputDescriptor.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getMergedInputDescriptor() {
-      return mergedInputDescriptor_;
+      return mergedInputDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : mergedInputDescriptor_;
     }
     /**
      * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getMergedInputDescriptorOrBuilder() {
-      return mergedInputDescriptor_;
+      return mergedInputDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : mergedInputDescriptor_;
     }
 
-    private void initFields() {
-      groupName_ = "";
-      groupVertices_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-      mergedInputDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getGroupNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, groupName_);
       }
       for (int i = 0; i < groupVertices_.size(); i++) {
-        output.writeBytes(2, groupVertices_.getByteString(i));
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, groupVertices_.getRaw(i));
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeMessage(3, mergedInputDescriptor_);
+      if (((bitField0_ & 0x00000002) != 0)) {
+        output.writeMessage(3, getMergedInputDescriptor());
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getGroupNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, groupName_);
       }
       {
         int dataSize = 0;
         for (int i = 0; i < groupVertices_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(groupVertices_.getByteString(i));
+          dataSize += computeStringSizeNoTag(groupVertices_.getRaw(i));
         }
         size += dataSize;
         size += 1 * getGroupVerticesList().size();
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(3, mergedInputDescriptor_);
+          .computeMessageSize(3, getMergedInputDescriptor());
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -2652,32 +2872,29 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto) obj;
 
-      boolean result = true;
-      result = result && (hasGroupName() == other.hasGroupName());
+      if (hasGroupName() != other.hasGroupName()) return false;
       if (hasGroupName()) {
-        result = result && getGroupName()
-            .equals(other.getGroupName());
+        if (!getGroupName()
+            .equals(other.getGroupName())) return false;
       }
-      result = result && getGroupVerticesList()
-          .equals(other.getGroupVerticesList());
-      result = result && (hasMergedInputDescriptor() == other.hasMergedInputDescriptor());
+      if (!getGroupVerticesList()
+          .equals(other.getGroupVerticesList())) return false;
+      if (hasMergedInputDescriptor() != other.hasMergedInputDescriptor()) return false;
       if (hasMergedInputDescriptor()) {
-        result = result && getMergedInputDescriptor()
-            .equals(other.getMergedInputDescriptor());
+        if (!getMergedInputDescriptor()
+            .equals(other.getMergedInputDescriptor())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasGroupName()) {
         hash = (37 * hash) + GROUP_NAME_FIELD_NUMBER;
         hash = (53 * hash) + getGroupName().hashCode();
@@ -2695,6 +2912,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -2718,46 +2946,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Gr
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -2765,14 +3008,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code GroupInputSpecProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:GroupInputSpecProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -2785,47 +3030,43 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getMergedInputDescriptorFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         groupName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
-        groupVertices_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        if (mergedInputDescriptorBuilder_ == null) {
-          mergedInputDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-        } else {
-          mergedInputDescriptorBuilder_.clear();
+        groupVertices_ =
+            com.google.protobuf.LazyStringArrayList.emptyList();
+        mergedInputDescriptor_ = null;
+        if (mergedInputDescriptorBuilder_ != null) {
+          mergedInputDescriptorBuilder_.dispose();
+          mergedInputDescriptorBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -2834,33 +3075,67 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInpu
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.groupName_ = groupName_;
           to_bitField0_ |= 0x00000001;
         }
-        result.groupName_ = groupName_;
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          groupVertices_ = new com.google.protobuf.UnmodifiableLazyStringList(
-              groupVertices_);
-          bitField0_ = (bitField0_ & ~0x00000002);
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          groupVertices_.makeImmutable();
+          result.groupVertices_ = groupVertices_;
         }
-        result.groupVertices_ = groupVertices_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.mergedInputDescriptor_ = mergedInputDescriptorBuilder_ == null
+              ? mergedInputDescriptor_
+              : mergedInputDescriptorBuilder_.build();
           to_bitField0_ |= 0x00000002;
         }
-        if (mergedInputDescriptorBuilder_ == null) {
-          result.mergedInputDescriptor_ = mergedInputDescriptor_;
-        } else {
-          result.mergedInputDescriptor_ = mergedInputDescriptorBuilder_.build();
-        }
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto)other);
@@ -2873,14 +3148,14 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.getDefaultInstance()) return this;
         if (other.hasGroupName()) {
-          bitField0_ |= 0x00000001;
           groupName_ = other.groupName_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (!other.groupVertices_.isEmpty()) {
           if (groupVertices_.isEmpty()) {
             groupVertices_ = other.groupVertices_;
-            bitField0_ = (bitField0_ & ~0x00000002);
+            bitField0_ |= 0x00000002;
           } else {
             ensureGroupVerticesIsMutable();
             groupVertices_.addAll(other.groupVertices_);
@@ -2891,49 +3166,87 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           mergeMergedInputDescriptor(other.getMergedInputDescriptor());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                groupName_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                com.google.protobuf.ByteString bs = input.readBytes();
+                ensureGroupVerticesIsMutable();
+                groupVertices_.add(bs);
+                break;
+              } // case 18
+              case 26: {
+                input.readMessage(
+                    getMergedInputDescriptorFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 26
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string group_name = 1;
       private java.lang.Object groupName_ = "";
       /**
        * <code>optional string group_name = 1;</code>
+       * @return Whether the groupName field is set.
        */
       public boolean hasGroupName() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string group_name = 1;</code>
+       * @return The groupName.
        */
       public java.lang.String getGroupName() {
         java.lang.Object ref = groupName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          groupName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            groupName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -2941,6 +3254,7 @@ public java.lang.String getGroupName() {
       }
       /**
        * <code>optional string group_name = 1;</code>
+       * @return The bytes for groupName.
        */
       public com.google.protobuf.ByteString
           getGroupNameBytes() {
@@ -2957,69 +3271,77 @@ public java.lang.String getGroupName() {
       }
       /**
        * <code>optional string group_name = 1;</code>
+       * @param value The groupName to set.
+       * @return This builder for chaining.
        */
       public Builder setGroupName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         groupName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string group_name = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearGroupName() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         groupName_ = getDefaultInstance().getGroupName();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string group_name = 1;</code>
+       * @param value The bytes for groupName to set.
+       * @return This builder for chaining.
        */
       public Builder setGroupNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         groupName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // repeated string group_vertices = 2;
-      private com.google.protobuf.LazyStringList groupVertices_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private com.google.protobuf.LazyStringArrayList groupVertices_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
       private void ensureGroupVerticesIsMutable() {
-        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
+        if (!groupVertices_.isModifiable()) {
           groupVertices_ = new com.google.protobuf.LazyStringArrayList(groupVertices_);
-          bitField0_ |= 0x00000002;
-         }
+        }
+        bitField0_ |= 0x00000002;
       }
       /**
        * <code>repeated string group_vertices = 2;</code>
+       * @return A list containing the groupVertices.
        */
-      public java.util.List<java.lang.String>
+      public com.google.protobuf.ProtocolStringList
           getGroupVerticesList() {
-        return java.util.Collections.unmodifiableList(groupVertices_);
+        groupVertices_.makeImmutable();
+        return groupVertices_;
       }
       /**
        * <code>repeated string group_vertices = 2;</code>
+       * @return The count of groupVertices.
        */
       public int getGroupVerticesCount() {
         return groupVertices_.size();
       }
       /**
        * <code>repeated string group_vertices = 2;</code>
+       * @param index The index of the element to return.
+       * @return The groupVertices at the given index.
        */
       public java.lang.String getGroupVertices(int index) {
         return groupVertices_.get(index);
       }
       /**
        * <code>repeated string group_vertices = 2;</code>
+       * @param index The index of the value to return.
+       * @return The bytes of the groupVertices at the given index.
        */
       public com.google.protobuf.ByteString
           getGroupVerticesBytes(int index) {
@@ -3027,79 +3349,90 @@ public java.lang.String getGroupVertices(int index) {
       }
       /**
        * <code>repeated string group_vertices = 2;</code>
+       * @param index The index to set the value at.
+       * @param value The groupVertices to set.
+       * @return This builder for chaining.
        */
       public Builder setGroupVertices(
           int index, java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureGroupVerticesIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureGroupVerticesIsMutable();
         groupVertices_.set(index, value);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string group_vertices = 2;</code>
+       * @param value The groupVertices to add.
+       * @return This builder for chaining.
        */
       public Builder addGroupVertices(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureGroupVerticesIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureGroupVerticesIsMutable();
         groupVertices_.add(value);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string group_vertices = 2;</code>
+       * @param values The groupVertices to add.
+       * @return This builder for chaining.
        */
       public Builder addAllGroupVertices(
           java.lang.Iterable<java.lang.String> values) {
         ensureGroupVerticesIsMutable();
-        super.addAll(values, groupVertices_);
+        com.google.protobuf.AbstractMessageLite.Builder.addAll(
+            values, groupVertices_);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string group_vertices = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearGroupVertices() {
-        groupVertices_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
+        groupVertices_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+        bitField0_ = (bitField0_ & ~0x00000002);;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string group_vertices = 2;</code>
+       * @param value The bytes of the groupVertices to add.
+       * @return This builder for chaining.
        */
       public Builder addGroupVerticesBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureGroupVerticesIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureGroupVerticesIsMutable();
         groupVertices_.add(value);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
 
-      // optional .EntityDescriptorProto merged_input_descriptor = 3;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto mergedInputDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto mergedInputDescriptor_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> mergedInputDescriptorBuilder_;
       /**
        * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
+       * @return Whether the mergedInputDescriptor field is set.
        */
       public boolean hasMergedInputDescriptor() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
+       * @return The mergedInputDescriptor.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getMergedInputDescriptor() {
         if (mergedInputDescriptorBuilder_ == null) {
-          return mergedInputDescriptor_;
+          return mergedInputDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : mergedInputDescriptor_;
         } else {
           return mergedInputDescriptorBuilder_.getMessage();
         }
@@ -3113,11 +3446,11 @@ public Builder setMergedInputDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.L
             throw new NullPointerException();
           }
           mergedInputDescriptor_ = value;
-          onChanged();
         } else {
           mergedInputDescriptorBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000004;
+        onChanged();
         return this;
       }
       /**
@@ -3127,11 +3460,11 @@ public Builder setMergedInputDescriptor(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder builderForValue) {
         if (mergedInputDescriptorBuilder_ == null) {
           mergedInputDescriptor_ = builderForValue.build();
-          onChanged();
         } else {
           mergedInputDescriptorBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000004;
+        onChanged();
         return this;
       }
       /**
@@ -3139,31 +3472,33 @@ public Builder setMergedInputDescriptor(
        */
       public Builder mergeMergedInputDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto value) {
         if (mergedInputDescriptorBuilder_ == null) {
-          if (((bitField0_ & 0x00000004) == 0x00000004) &&
-              mergedInputDescriptor_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) {
-            mergedInputDescriptor_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.newBuilder(mergedInputDescriptor_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000004) != 0) &&
+            mergedInputDescriptor_ != null &&
+            mergedInputDescriptor_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) {
+            getMergedInputDescriptorBuilder().mergeFrom(value);
           } else {
             mergedInputDescriptor_ = value;
           }
-          onChanged();
         } else {
           mergedInputDescriptorBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000004;
+        if (mergedInputDescriptor_ != null) {
+          bitField0_ |= 0x00000004;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
        */
       public Builder clearMergedInputDescriptor() {
-        if (mergedInputDescriptorBuilder_ == null) {
-          mergedInputDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-          onChanged();
-        } else {
-          mergedInputDescriptorBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000004);
+        mergedInputDescriptor_ = null;
+        if (mergedInputDescriptorBuilder_ != null) {
+          mergedInputDescriptorBuilder_.dispose();
+          mergedInputDescriptorBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -3181,72 +3516,130 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDes
         if (mergedInputDescriptorBuilder_ != null) {
           return mergedInputDescriptorBuilder_.getMessageOrBuilder();
         } else {
-          return mergedInputDescriptor_;
+          return mergedInputDescriptor_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : mergedInputDescriptor_;
         }
       }
       /**
        * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> 
           getMergedInputDescriptorFieldBuilder() {
         if (mergedInputDescriptorBuilder_ == null) {
-          mergedInputDescriptorBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          mergedInputDescriptorBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder>(
-                  mergedInputDescriptor_,
+                  getMergedInputDescriptor(),
                   getParentForChildren(),
                   isClean());
           mergedInputDescriptor_ = null;
         }
         return mergedInputDescriptorBuilder_;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:GroupInputSpecProto)
     }
 
+    // @@protoc_insertion_point(class_scope:GroupInputSpecProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new GroupInputSpecProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<GroupInputSpecProto>
+        PARSER = new com.google.protobuf.AbstractParser<GroupInputSpecProto>() {
+      @java.lang.Override
+      public GroupInputSpecProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<GroupInputSpecProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<GroupInputSpecProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:GroupInputSpecProto)
   }
 
-  public interface SignableVertexSpecOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface SignableVertexSpecOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:SignableVertexSpec)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string user = 1;
     /**
      * <code>optional string user = 1;</code>
+     * @return Whether the user field is set.
      */
     boolean hasUser();
     /**
      * <code>optional string user = 1;</code>
+     * @return The user.
      */
     java.lang.String getUser();
     /**
      * <code>optional string user = 1;</code>
+     * @return The bytes for user.
      */
     com.google.protobuf.ByteString
         getUserBytes();
 
-    // optional int64 signatureKeyId = 2;
     /**
      * <code>optional int64 signatureKeyId = 2;</code>
+     * @return Whether the signatureKeyId field is set.
      */
     boolean hasSignatureKeyId();
     /**
      * <code>optional int64 signatureKeyId = 2;</code>
+     * @return The signatureKeyId.
      */
     long getSignatureKeyId();
 
-    // optional .QueryIdentifierProto query_identifier = 3;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
+     * @return Whether the queryIdentifier field is set.
      */
     boolean hasQueryIdentifier();
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
+     * @return The queryIdentifier.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
     /**
@@ -3254,107 +3647,117 @@ public interface SignableVertexSpecOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();
 
-    // optional string hive_query_id = 4;
     /**
      * <code>optional string hive_query_id = 4;</code>
+     * @return Whether the hiveQueryId field is set.
      */
     boolean hasHiveQueryId();
     /**
      * <code>optional string hive_query_id = 4;</code>
+     * @return The hiveQueryId.
      */
     java.lang.String getHiveQueryId();
     /**
      * <code>optional string hive_query_id = 4;</code>
+     * @return The bytes for hiveQueryId.
      */
     com.google.protobuf.ByteString
         getHiveQueryIdBytes();
 
-    // optional string dag_name = 5;
     /**
-     * <code>optional string dag_name = 5;</code>
-     *
      * <pre>
      * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
      * </pre>
+     *
+     * <code>optional string dag_name = 5;</code>
+     * @return Whether the dagName field is set.
      */
     boolean hasDagName();
     /**
-     * <code>optional string dag_name = 5;</code>
-     *
      * <pre>
      * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
      * </pre>
+     *
+     * <code>optional string dag_name = 5;</code>
+     * @return The dagName.
      */
     java.lang.String getDagName();
     /**
-     * <code>optional string dag_name = 5;</code>
-     *
      * <pre>
      * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
      * </pre>
+     *
+     * <code>optional string dag_name = 5;</code>
+     * @return The bytes for dagName.
      */
     com.google.protobuf.ByteString
         getDagNameBytes();
 
-    // optional string vertex_name = 6;
     /**
      * <code>optional string vertex_name = 6;</code>
+     * @return Whether the vertexName field is set.
      */
     boolean hasVertexName();
     /**
      * <code>optional string vertex_name = 6;</code>
+     * @return The vertexName.
      */
     java.lang.String getVertexName();
     /**
      * <code>optional string vertex_name = 6;</code>
+     * @return The bytes for vertexName.
      */
     com.google.protobuf.ByteString
         getVertexNameBytes();
 
-    // optional int32 vertex_index = 7;
     /**
      * <code>optional int32 vertex_index = 7;</code>
+     * @return Whether the vertexIndex field is set.
      */
     boolean hasVertexIndex();
     /**
      * <code>optional int32 vertex_index = 7;</code>
+     * @return The vertexIndex.
      */
     int getVertexIndex();
 
-    // optional string token_identifier = 8;
     /**
-     * <code>optional string token_identifier = 8;</code>
-     *
      * <pre>
      * The core vertex stuff 
      * </pre>
+     *
+     * <code>optional string token_identifier = 8;</code>
+     * @return Whether the tokenIdentifier field is set.
      */
     boolean hasTokenIdentifier();
     /**
-     * <code>optional string token_identifier = 8;</code>
-     *
      * <pre>
      * The core vertex stuff 
      * </pre>
+     *
+     * <code>optional string token_identifier = 8;</code>
+     * @return The tokenIdentifier.
      */
     java.lang.String getTokenIdentifier();
     /**
-     * <code>optional string token_identifier = 8;</code>
-     *
      * <pre>
      * The core vertex stuff 
      * </pre>
+     *
+     * <code>optional string token_identifier = 8;</code>
+     * @return The bytes for tokenIdentifier.
      */
     com.google.protobuf.ByteString
         getTokenIdentifierBytes();
 
-    // optional .EntityDescriptorProto processor_descriptor = 9;
     /**
      * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
+     * @return Whether the processorDescriptor field is set.
      */
     boolean hasProcessorDescriptor();
     /**
      * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
+     * @return The processorDescriptor.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getProcessorDescriptor();
     /**
@@ -3362,7 +3765,6 @@ public interface SignableVertexSpecOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getProcessorDescriptorOrBuilder();
 
-    // repeated .IOSpecProto input_specs = 10;
     /**
      * <code>repeated .IOSpecProto input_specs = 10;</code>
      */
@@ -3387,7 +3789,6 @@ public interface SignableVertexSpecOrBuilder
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getInputSpecsOrBuilder(
         int index);
 
-    // repeated .IOSpecProto output_specs = 11;
     /**
      * <code>repeated .IOSpecProto output_specs = 11;</code>
      */
@@ -3412,7 +3813,6 @@ org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBui
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getOutputSpecsOrBuilder(
         int index);
 
-    // repeated .GroupInputSpecProto grouped_input_specs = 12;
     /**
      * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
      */
@@ -3437,245 +3837,100 @@ org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBui
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder getGroupedInputSpecsOrBuilder(
         int index);
 
-    // optional int32 vertex_parallelism = 13;
     /**
-     * <code>optional int32 vertex_parallelism = 13;</code>
-     *
      * <pre>
      * An internal field required for Tez.
      * </pre>
+     *
+     * <code>optional int32 vertex_parallelism = 13;</code>
+     * @return Whether the vertexParallelism field is set.
      */
     boolean hasVertexParallelism();
     /**
-     * <code>optional int32 vertex_parallelism = 13;</code>
-     *
      * <pre>
      * An internal field required for Tez.
      * </pre>
+     *
+     * <code>optional int32 vertex_parallelism = 13;</code>
+     * @return The vertexParallelism.
      */
     int getVertexParallelism();
 
-    // optional bool is_external_submission = 14 [default = false];
     /**
      * <code>optional bool is_external_submission = 14 [default = false];</code>
+     * @return Whether the isExternalSubmission field is set.
      */
     boolean hasIsExternalSubmission();
     /**
      * <code>optional bool is_external_submission = 14 [default = false];</code>
+     * @return The isExternalSubmission.
      */
     boolean getIsExternalSubmission();
   }
   /**
-   * Protobuf type {@code SignableVertexSpec}
-   *
    * <pre>
    * The part of SubmitWork that can be signed 
    * </pre>
+   *
+   * Protobuf type {@code SignableVertexSpec}
    */
   public static final class SignableVertexSpec extends
-      com.google.protobuf.GeneratedMessage
-      implements SignableVertexSpecOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:SignableVertexSpec)
+      SignableVertexSpecOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use SignableVertexSpec.newBuilder() to construct.
-    private SignableVertexSpec(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private SignableVertexSpec(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private SignableVertexSpec(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final SignableVertexSpec defaultInstance;
-    public static SignableVertexSpec getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public SignableVertexSpec getDefaultInstanceForType() {
-      return defaultInstance;
+    private SignableVertexSpec() {
+      user_ = "";
+      hiveQueryId_ = "";
+      dagName_ = "";
+      vertexName_ = "";
+      tokenIdentifier_ = "";
+      inputSpecs_ = java.util.Collections.emptyList();
+      outputSpecs_ = java.util.Collections.emptyList();
+      groupedInputSpecs_ = java.util.Collections.emptyList();
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private SignableVertexSpec(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              user_ = input.readBytes();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              signatureKeyId_ = input.readInt64();
-              break;
-            }
-            case 26: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000004) == 0x00000004)) {
-                subBuilder = queryIdentifier_.toBuilder();
-              }
-              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(queryIdentifier_);
-                queryIdentifier_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000004;
-              break;
-            }
-            case 34: {
-              bitField0_ |= 0x00000008;
-              hiveQueryId_ = input.readBytes();
-              break;
-            }
-            case 42: {
-              bitField0_ |= 0x00000010;
-              dagName_ = input.readBytes();
-              break;
-            }
-            case 50: {
-              bitField0_ |= 0x00000020;
-              vertexName_ = input.readBytes();
-              break;
-            }
-            case 56: {
-              bitField0_ |= 0x00000040;
-              vertexIndex_ = input.readInt32();
-              break;
-            }
-            case 66: {
-              bitField0_ |= 0x00000080;
-              tokenIdentifier_ = input.readBytes();
-              break;
-            }
-            case 74: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000100) == 0x00000100)) {
-                subBuilder = processorDescriptor_.toBuilder();
-              }
-              processorDescriptor_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(processorDescriptor_);
-                processorDescriptor_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000100;
-              break;
-            }
-            case 82: {
-              if (!((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
-                inputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto>();
-                mutable_bitField0_ |= 0x00000200;
-              }
-              inputSpecs_.add(input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.PARSER, extensionRegistry));
-              break;
-            }
-            case 90: {
-              if (!((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
-                outputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto>();
-                mutable_bitField0_ |= 0x00000400;
-              }
-              outputSpecs_.add(input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.PARSER, extensionRegistry));
-              break;
-            }
-            case 98: {
-              if (!((mutable_bitField0_ & 0x00000800) == 0x00000800)) {
-                groupedInputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto>();
-                mutable_bitField0_ |= 0x00000800;
-              }
-              groupedInputSpecs_.add(input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.PARSER, extensionRegistry));
-              break;
-            }
-            case 104: {
-              bitField0_ |= 0x00000200;
-              vertexParallelism_ = input.readInt32();
-              break;
-            }
-            case 112: {
-              bitField0_ |= 0x00000400;
-              isExternalSubmission_ = input.readBool();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
-          inputSpecs_ = java.util.Collections.unmodifiableList(inputSpecs_);
-        }
-        if (((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
-          outputSpecs_ = java.util.Collections.unmodifiableList(outputSpecs_);
-        }
-        if (((mutable_bitField0_ & 0x00000800) == 0x00000800)) {
-          groupedInputSpecs_ = java.util.Collections.unmodifiableList(groupedInputSpecs_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new SignableVertexSpec();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<SignableVertexSpec> PARSER =
-        new com.google.protobuf.AbstractParser<SignableVertexSpec>() {
-      public SignableVertexSpec parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new SignableVertexSpec(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<SignableVertexSpec> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string user = 1;
     public static final int USER_FIELD_NUMBER = 1;
-    private java.lang.Object user_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object user_ = "";
     /**
      * <code>optional string user = 1;</code>
+     * @return Whether the user field is set.
      */
+    @java.lang.Override
     public boolean hasUser() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string user = 1;</code>
+     * @return The user.
      */
+    @java.lang.Override
     public java.lang.String getUser() {
       java.lang.Object ref = user_;
       if (ref instanceof java.lang.String) {
@@ -3692,7 +3947,9 @@ public java.lang.String getUser() {
     }
     /**
      * <code>optional string user = 1;</code>
+     * @return The bytes for user.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getUserBytes() {
       java.lang.Object ref = user_;
@@ -3707,56 +3964,67 @@ public java.lang.String getUser() {
       }
     }
 
-    // optional int64 signatureKeyId = 2;
     public static final int SIGNATUREKEYID_FIELD_NUMBER = 2;
-    private long signatureKeyId_;
+    private long signatureKeyId_ = 0L;
     /**
      * <code>optional int64 signatureKeyId = 2;</code>
+     * @return Whether the signatureKeyId field is set.
      */
+    @java.lang.Override
     public boolean hasSignatureKeyId() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional int64 signatureKeyId = 2;</code>
+     * @return The signatureKeyId.
      */
+    @java.lang.Override
     public long getSignatureKeyId() {
       return signatureKeyId_;
     }
 
-    // optional .QueryIdentifierProto query_identifier = 3;
     public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 3;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
+     * @return Whether the queryIdentifier field is set.
      */
+    @java.lang.Override
     public boolean hasQueryIdentifier() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
+     * @return The queryIdentifier.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
 
-    // optional string hive_query_id = 4;
     public static final int HIVE_QUERY_ID_FIELD_NUMBER = 4;
-    private java.lang.Object hiveQueryId_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object hiveQueryId_ = "";
     /**
      * <code>optional string hive_query_id = 4;</code>
+     * @return Whether the hiveQueryId field is set.
      */
+    @java.lang.Override
     public boolean hasHiveQueryId() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
+      return ((bitField0_ & 0x00000008) != 0);
     }
     /**
      * <code>optional string hive_query_id = 4;</code>
+     * @return The hiveQueryId.
      */
+    @java.lang.Override
     public java.lang.String getHiveQueryId() {
       java.lang.Object ref = hiveQueryId_;
       if (ref instanceof java.lang.String) {
@@ -3773,7 +4041,9 @@ public java.lang.String getHiveQueryId() {
     }
     /**
      * <code>optional string hive_query_id = 4;</code>
+     * @return The bytes for hiveQueryId.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getHiveQueryIdBytes() {
       java.lang.Object ref = hiveQueryId_;
@@ -3788,26 +4058,30 @@ public java.lang.String getHiveQueryId() {
       }
     }
 
-    // optional string dag_name = 5;
     public static final int DAG_NAME_FIELD_NUMBER = 5;
-    private java.lang.Object dagName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object dagName_ = "";
     /**
-     * <code>optional string dag_name = 5;</code>
-     *
      * <pre>
      * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
      * </pre>
+     *
+     * <code>optional string dag_name = 5;</code>
+     * @return Whether the dagName field is set.
      */
+    @java.lang.Override
     public boolean hasDagName() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+      return ((bitField0_ & 0x00000010) != 0);
     }
     /**
-     * <code>optional string dag_name = 5;</code>
-     *
      * <pre>
      * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
      * </pre>
+     *
+     * <code>optional string dag_name = 5;</code>
+     * @return The dagName.
      */
+    @java.lang.Override
     public java.lang.String getDagName() {
       java.lang.Object ref = dagName_;
       if (ref instanceof java.lang.String) {
@@ -3823,12 +4097,14 @@ public java.lang.String getDagName() {
       }
     }
     /**
-     * <code>optional string dag_name = 5;</code>
-     *
      * <pre>
      * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
      * </pre>
+     *
+     * <code>optional string dag_name = 5;</code>
+     * @return The bytes for dagName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getDagNameBytes() {
       java.lang.Object ref = dagName_;
@@ -3843,18 +4119,22 @@ public java.lang.String getDagName() {
       }
     }
 
-    // optional string vertex_name = 6;
     public static final int VERTEX_NAME_FIELD_NUMBER = 6;
-    private java.lang.Object vertexName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object vertexName_ = "";
     /**
      * <code>optional string vertex_name = 6;</code>
+     * @return Whether the vertexName field is set.
      */
+    @java.lang.Override
     public boolean hasVertexName() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+      return ((bitField0_ & 0x00000020) != 0);
     }
     /**
      * <code>optional string vertex_name = 6;</code>
+     * @return The vertexName.
      */
+    @java.lang.Override
     public java.lang.String getVertexName() {
       java.lang.Object ref = vertexName_;
       if (ref instanceof java.lang.String) {
@@ -3871,7 +4151,9 @@ public java.lang.String getVertexName() {
     }
     /**
      * <code>optional string vertex_name = 6;</code>
+     * @return The bytes for vertexName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getVertexNameBytes() {
       java.lang.Object ref = vertexName_;
@@ -3886,42 +4168,49 @@ public java.lang.String getVertexName() {
       }
     }
 
-    // optional int32 vertex_index = 7;
     public static final int VERTEX_INDEX_FIELD_NUMBER = 7;
-    private int vertexIndex_;
+    private int vertexIndex_ = 0;
     /**
      * <code>optional int32 vertex_index = 7;</code>
+     * @return Whether the vertexIndex field is set.
      */
+    @java.lang.Override
     public boolean hasVertexIndex() {
-      return ((bitField0_ & 0x00000040) == 0x00000040);
+      return ((bitField0_ & 0x00000040) != 0);
     }
     /**
      * <code>optional int32 vertex_index = 7;</code>
+     * @return The vertexIndex.
      */
+    @java.lang.Override
     public int getVertexIndex() {
       return vertexIndex_;
     }
 
-    // optional string token_identifier = 8;
     public static final int TOKEN_IDENTIFIER_FIELD_NUMBER = 8;
-    private java.lang.Object tokenIdentifier_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object tokenIdentifier_ = "";
     /**
-     * <code>optional string token_identifier = 8;</code>
-     *
      * <pre>
      * The core vertex stuff 
      * </pre>
+     *
+     * <code>optional string token_identifier = 8;</code>
+     * @return Whether the tokenIdentifier field is set.
      */
+    @java.lang.Override
     public boolean hasTokenIdentifier() {
-      return ((bitField0_ & 0x00000080) == 0x00000080);
+      return ((bitField0_ & 0x00000080) != 0);
     }
     /**
-     * <code>optional string token_identifier = 8;</code>
-     *
      * <pre>
      * The core vertex stuff 
      * </pre>
+     *
+     * <code>optional string token_identifier = 8;</code>
+     * @return The tokenIdentifier.
      */
+    @java.lang.Override
     public java.lang.String getTokenIdentifier() {
       java.lang.Object ref = tokenIdentifier_;
       if (ref instanceof java.lang.String) {
@@ -3937,12 +4226,14 @@ public java.lang.String getTokenIdentifier() {
       }
     }
     /**
-     * <code>optional string token_identifier = 8;</code>
-     *
      * <pre>
      * The core vertex stuff 
      * </pre>
+     *
+     * <code>optional string token_identifier = 8;</code>
+     * @return The bytes for tokenIdentifier.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getTokenIdentifierBytes() {
       java.lang.Object ref = tokenIdentifier_;
@@ -3957,40 +4248,46 @@ public java.lang.String getTokenIdentifier() {
       }
     }
 
-    // optional .EntityDescriptorProto processor_descriptor = 9;
     public static final int PROCESSOR_DESCRIPTOR_FIELD_NUMBER = 9;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto processorDescriptor_;
     /**
      * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
+     * @return Whether the processorDescriptor field is set.
      */
+    @java.lang.Override
     public boolean hasProcessorDescriptor() {
-      return ((bitField0_ & 0x00000100) == 0x00000100);
+      return ((bitField0_ & 0x00000100) != 0);
     }
     /**
      * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
+     * @return The processorDescriptor.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getProcessorDescriptor() {
-      return processorDescriptor_;
+      return processorDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : processorDescriptor_;
     }
     /**
      * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getProcessorDescriptorOrBuilder() {
-      return processorDescriptor_;
+      return processorDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : processorDescriptor_;
     }
 
-    // repeated .IOSpecProto input_specs = 10;
     public static final int INPUT_SPECS_FIELD_NUMBER = 10;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> inputSpecs_;
     /**
      * <code>repeated .IOSpecProto input_specs = 10;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> getInputSpecsList() {
       return inputSpecs_;
     }
     /**
      * <code>repeated .IOSpecProto input_specs = 10;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
         getInputSpecsOrBuilderList() {
       return inputSpecs_;
@@ -3998,35 +4295,40 @@ public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolP
     /**
      * <code>repeated .IOSpecProto input_specs = 10;</code>
      */
+    @java.lang.Override
     public int getInputSpecsCount() {
       return inputSpecs_.size();
     }
     /**
      * <code>repeated .IOSpecProto input_specs = 10;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getInputSpecs(int index) {
       return inputSpecs_.get(index);
     }
     /**
      * <code>repeated .IOSpecProto input_specs = 10;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getInputSpecsOrBuilder(
         int index) {
       return inputSpecs_.get(index);
     }
 
-    // repeated .IOSpecProto output_specs = 11;
     public static final int OUTPUT_SPECS_FIELD_NUMBER = 11;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> outputSpecs_;
     /**
      * <code>repeated .IOSpecProto output_specs = 11;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> getOutputSpecsList() {
       return outputSpecs_;
     }
     /**
      * <code>repeated .IOSpecProto output_specs = 11;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
         getOutputSpecsOrBuilderList() {
       return outputSpecs_;
@@ -4034,35 +4336,40 @@ public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolP
     /**
      * <code>repeated .IOSpecProto output_specs = 11;</code>
      */
+    @java.lang.Override
     public int getOutputSpecsCount() {
       return outputSpecs_.size();
     }
     /**
      * <code>repeated .IOSpecProto output_specs = 11;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getOutputSpecs(int index) {
       return outputSpecs_.get(index);
     }
     /**
      * <code>repeated .IOSpecProto output_specs = 11;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getOutputSpecsOrBuilder(
         int index) {
       return outputSpecs_.get(index);
     }
 
-    // repeated .GroupInputSpecProto grouped_input_specs = 12;
     public static final int GROUPED_INPUT_SPECS_FIELD_NUMBER = 12;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> groupedInputSpecs_;
     /**
      * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> getGroupedInputSpecsList() {
       return groupedInputSpecs_;
     }
     /**
      * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder> 
         getGroupedInputSpecsOrBuilderList() {
       return groupedInputSpecs_;
@@ -4070,117 +4377,112 @@ public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolP
     /**
      * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
      */
+    @java.lang.Override
     public int getGroupedInputSpecsCount() {
       return groupedInputSpecs_.size();
     }
     /**
      * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getGroupedInputSpecs(int index) {
       return groupedInputSpecs_.get(index);
     }
     /**
      * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder getGroupedInputSpecsOrBuilder(
         int index) {
       return groupedInputSpecs_.get(index);
     }
 
-    // optional int32 vertex_parallelism = 13;
     public static final int VERTEX_PARALLELISM_FIELD_NUMBER = 13;
-    private int vertexParallelism_;
+    private int vertexParallelism_ = 0;
     /**
-     * <code>optional int32 vertex_parallelism = 13;</code>
-     *
      * <pre>
      * An internal field required for Tez.
      * </pre>
+     *
+     * <code>optional int32 vertex_parallelism = 13;</code>
+     * @return Whether the vertexParallelism field is set.
      */
+    @java.lang.Override
     public boolean hasVertexParallelism() {
-      return ((bitField0_ & 0x00000200) == 0x00000200);
+      return ((bitField0_ & 0x00000200) != 0);
     }
     /**
-     * <code>optional int32 vertex_parallelism = 13;</code>
-     *
      * <pre>
      * An internal field required for Tez.
      * </pre>
+     *
+     * <code>optional int32 vertex_parallelism = 13;</code>
+     * @return The vertexParallelism.
      */
+    @java.lang.Override
     public int getVertexParallelism() {
       return vertexParallelism_;
     }
 
-    // optional bool is_external_submission = 14 [default = false];
     public static final int IS_EXTERNAL_SUBMISSION_FIELD_NUMBER = 14;
-    private boolean isExternalSubmission_;
+    private boolean isExternalSubmission_ = false;
     /**
      * <code>optional bool is_external_submission = 14 [default = false];</code>
+     * @return Whether the isExternalSubmission field is set.
      */
+    @java.lang.Override
     public boolean hasIsExternalSubmission() {
-      return ((bitField0_ & 0x00000400) == 0x00000400);
+      return ((bitField0_ & 0x00000400) != 0);
     }
     /**
      * <code>optional bool is_external_submission = 14 [default = false];</code>
+     * @return The isExternalSubmission.
      */
+    @java.lang.Override
     public boolean getIsExternalSubmission() {
       return isExternalSubmission_;
     }
 
-    private void initFields() {
-      user_ = "";
-      signatureKeyId_ = 0L;
-      queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      hiveQueryId_ = "";
-      dagName_ = "";
-      vertexName_ = "";
-      vertexIndex_ = 0;
-      tokenIdentifier_ = "";
-      processorDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-      inputSpecs_ = java.util.Collections.emptyList();
-      outputSpecs_ = java.util.Collections.emptyList();
-      groupedInputSpecs_ = java.util.Collections.emptyList();
-      vertexParallelism_ = 0;
-      isExternalSubmission_ = false;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getUserBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, user_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeInt64(2, signatureKeyId_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeMessage(3, queryIdentifier_);
+      if (((bitField0_ & 0x00000004) != 0)) {
+        output.writeMessage(3, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeBytes(4, getHiveQueryIdBytes());
+      if (((bitField0_ & 0x00000008) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, hiveQueryId_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeBytes(5, getDagNameBytes());
+      if (((bitField0_ & 0x00000010) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, dagName_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        output.writeBytes(6, getVertexNameBytes());
+      if (((bitField0_ & 0x00000020) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 6, vertexName_);
       }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
+      if (((bitField0_ & 0x00000040) != 0)) {
         output.writeInt32(7, vertexIndex_);
       }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        output.writeBytes(8, getTokenIdentifierBytes());
+      if (((bitField0_ & 0x00000080) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, tokenIdentifier_);
       }
-      if (((bitField0_ & 0x00000100) == 0x00000100)) {
-        output.writeMessage(9, processorDescriptor_);
+      if (((bitField0_ & 0x00000100) != 0)) {
+        output.writeMessage(9, getProcessorDescriptor());
       }
       for (int i = 0; i < inputSpecs_.size(); i++) {
         output.writeMessage(10, inputSpecs_.get(i));
@@ -4191,56 +4493,51 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       for (int i = 0; i < groupedInputSpecs_.size(); i++) {
         output.writeMessage(12, groupedInputSpecs_.get(i));
       }
-      if (((bitField0_ & 0x00000200) == 0x00000200)) {
+      if (((bitField0_ & 0x00000200) != 0)) {
         output.writeInt32(13, vertexParallelism_);
       }
-      if (((bitField0_ & 0x00000400) == 0x00000400)) {
+      if (((bitField0_ & 0x00000400) != 0)) {
         output.writeBool(14, isExternalSubmission_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getUserBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, user_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(2, signatureKeyId_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(3, queryIdentifier_);
+          .computeMessageSize(3, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(4, getHiveQueryIdBytes());
+      if (((bitField0_ & 0x00000008) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, hiveQueryId_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(5, getDagNameBytes());
+      if (((bitField0_ & 0x00000010) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, dagName_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(6, getVertexNameBytes());
+      if (((bitField0_ & 0x00000020) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(6, vertexName_);
       }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
+      if (((bitField0_ & 0x00000040) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(7, vertexIndex_);
       }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(8, getTokenIdentifierBytes());
+      if (((bitField0_ & 0x00000080) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, tokenIdentifier_);
       }
-      if (((bitField0_ & 0x00000100) == 0x00000100)) {
+      if (((bitField0_ & 0x00000100) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(9, processorDescriptor_);
+          .computeMessageSize(9, getProcessorDescriptor());
       }
       for (int i = 0; i < inputSpecs_.size(); i++) {
         size += com.google.protobuf.CodedOutputStream
@@ -4254,26 +4551,19 @@ public int getSerializedSize() {
         size += com.google.protobuf.CodedOutputStream
           .computeMessageSize(12, groupedInputSpecs_.get(i));
       }
-      if (((bitField0_ & 0x00000200) == 0x00000200)) {
+      if (((bitField0_ & 0x00000200) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(13, vertexParallelism_);
       }
-      if (((bitField0_ & 0x00000400) == 0x00000400)) {
+      if (((bitField0_ & 0x00000400) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBoolSize(14, isExternalSubmission_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -4284,88 +4574,86 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec) obj;
 
-      boolean result = true;
-      result = result && (hasUser() == other.hasUser());
+      if (hasUser() != other.hasUser()) return false;
       if (hasUser()) {
-        result = result && getUser()
-            .equals(other.getUser());
+        if (!getUser()
+            .equals(other.getUser())) return false;
       }
-      result = result && (hasSignatureKeyId() == other.hasSignatureKeyId());
+      if (hasSignatureKeyId() != other.hasSignatureKeyId()) return false;
       if (hasSignatureKeyId()) {
-        result = result && (getSignatureKeyId()
-            == other.getSignatureKeyId());
+        if (getSignatureKeyId()
+            != other.getSignatureKeyId()) return false;
       }
-      result = result && (hasQueryIdentifier() == other.hasQueryIdentifier());
+      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
       if (hasQueryIdentifier()) {
-        result = result && getQueryIdentifier()
-            .equals(other.getQueryIdentifier());
+        if (!getQueryIdentifier()
+            .equals(other.getQueryIdentifier())) return false;
       }
-      result = result && (hasHiveQueryId() == other.hasHiveQueryId());
+      if (hasHiveQueryId() != other.hasHiveQueryId()) return false;
       if (hasHiveQueryId()) {
-        result = result && getHiveQueryId()
-            .equals(other.getHiveQueryId());
+        if (!getHiveQueryId()
+            .equals(other.getHiveQueryId())) return false;
       }
-      result = result && (hasDagName() == other.hasDagName());
+      if (hasDagName() != other.hasDagName()) return false;
       if (hasDagName()) {
-        result = result && getDagName()
-            .equals(other.getDagName());
+        if (!getDagName()
+            .equals(other.getDagName())) return false;
       }
-      result = result && (hasVertexName() == other.hasVertexName());
+      if (hasVertexName() != other.hasVertexName()) return false;
       if (hasVertexName()) {
-        result = result && getVertexName()
-            .equals(other.getVertexName());
+        if (!getVertexName()
+            .equals(other.getVertexName())) return false;
       }
-      result = result && (hasVertexIndex() == other.hasVertexIndex());
+      if (hasVertexIndex() != other.hasVertexIndex()) return false;
       if (hasVertexIndex()) {
-        result = result && (getVertexIndex()
-            == other.getVertexIndex());
+        if (getVertexIndex()
+            != other.getVertexIndex()) return false;
       }
-      result = result && (hasTokenIdentifier() == other.hasTokenIdentifier());
+      if (hasTokenIdentifier() != other.hasTokenIdentifier()) return false;
       if (hasTokenIdentifier()) {
-        result = result && getTokenIdentifier()
-            .equals(other.getTokenIdentifier());
+        if (!getTokenIdentifier()
+            .equals(other.getTokenIdentifier())) return false;
       }
-      result = result && (hasProcessorDescriptor() == other.hasProcessorDescriptor());
+      if (hasProcessorDescriptor() != other.hasProcessorDescriptor()) return false;
       if (hasProcessorDescriptor()) {
-        result = result && getProcessorDescriptor()
-            .equals(other.getProcessorDescriptor());
-      }
-      result = result && getInputSpecsList()
-          .equals(other.getInputSpecsList());
-      result = result && getOutputSpecsList()
-          .equals(other.getOutputSpecsList());
-      result = result && getGroupedInputSpecsList()
-          .equals(other.getGroupedInputSpecsList());
-      result = result && (hasVertexParallelism() == other.hasVertexParallelism());
+        if (!getProcessorDescriptor()
+            .equals(other.getProcessorDescriptor())) return false;
+      }
+      if (!getInputSpecsList()
+          .equals(other.getInputSpecsList())) return false;
+      if (!getOutputSpecsList()
+          .equals(other.getOutputSpecsList())) return false;
+      if (!getGroupedInputSpecsList()
+          .equals(other.getGroupedInputSpecsList())) return false;
+      if (hasVertexParallelism() != other.hasVertexParallelism()) return false;
       if (hasVertexParallelism()) {
-        result = result && (getVertexParallelism()
-            == other.getVertexParallelism());
+        if (getVertexParallelism()
+            != other.getVertexParallelism()) return false;
       }
-      result = result && (hasIsExternalSubmission() == other.hasIsExternalSubmission());
+      if (hasIsExternalSubmission() != other.hasIsExternalSubmission()) return false;
       if (hasIsExternalSubmission()) {
-        result = result && (getIsExternalSubmission()
-            == other.getIsExternalSubmission());
+        if (getIsExternalSubmission()
+            != other.getIsExternalSubmission()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasUser()) {
         hash = (37 * hash) + USER_FIELD_NUMBER;
         hash = (53 * hash) + getUser().hashCode();
       }
       if (hasSignatureKeyId()) {
         hash = (37 * hash) + SIGNATUREKEYID_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getSignatureKeyId());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getSignatureKeyId());
       }
       if (hasQueryIdentifier()) {
         hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
@@ -4413,13 +4701,25 @@ public int hashCode() {
       }
       if (hasIsExternalSubmission()) {
         hash = (37 * hash) + IS_EXTERNAL_SUBMISSION_FIELD_NUMBER;
-        hash = (53 * hash) + hashBoolean(getIsExternalSubmission());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
+            getIsExternalSubmission());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -4443,65 +4743,82 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Si
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
     /**
-     * Protobuf type {@code SignableVertexSpec}
-     *
      * <pre>
      * The part of SubmitWork that can be signed 
      * </pre>
+     *
+     * Protobuf type {@code SignableVertexSpec}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:SignableVertexSpec)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -4514,12 +4831,13 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getQueryIdentifierFieldBuilder();
           getProcessorDescriptorFieldBuilder();
           getInputSpecsFieldBuilder();
@@ -4527,76 +4845,65 @@ private void maybeForceBuilderInitialization() {
           getGroupedInputSpecsFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         user_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
         signatureKeyId_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-        } else {
-          queryIdentifierBuilder_.clear();
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000004);
         hiveQueryId_ = "";
-        bitField0_ = (bitField0_ & ~0x00000008);
         dagName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000010);
         vertexName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000020);
         vertexIndex_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000040);
         tokenIdentifier_ = "";
-        bitField0_ = (bitField0_ & ~0x00000080);
-        if (processorDescriptorBuilder_ == null) {
-          processorDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-        } else {
-          processorDescriptorBuilder_.clear();
+        processorDescriptor_ = null;
+        if (processorDescriptorBuilder_ != null) {
+          processorDescriptorBuilder_.dispose();
+          processorDescriptorBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000100);
         if (inputSpecsBuilder_ == null) {
           inputSpecs_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000200);
         } else {
+          inputSpecs_ = null;
           inputSpecsBuilder_.clear();
         }
+        bitField0_ = (bitField0_ & ~0x00000200);
         if (outputSpecsBuilder_ == null) {
           outputSpecs_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000400);
         } else {
+          outputSpecs_ = null;
           outputSpecsBuilder_.clear();
         }
+        bitField0_ = (bitField0_ & ~0x00000400);
         if (groupedInputSpecsBuilder_ == null) {
           groupedInputSpecs_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000800);
         } else {
+          groupedInputSpecs_ = null;
           groupedInputSpecsBuilder_.clear();
         }
+        bitField0_ = (bitField0_ & ~0x00000800);
         vertexParallelism_ = 0;
-        bitField0_ = (bitField0_ & ~0x00001000);
         isExternalSubmission_ = false;
-        bitField0_ = (bitField0_ & ~0x00002000);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec result = buildPartial();
         if (!result.isInitialized()) {
@@ -4605,56 +4912,18 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableV
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.user_ = user_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.signatureKeyId_ = signatureKeyId_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        if (queryIdentifierBuilder_ == null) {
-          result.queryIdentifier_ = queryIdentifier_;
-        } else {
-          result.queryIdentifier_ = queryIdentifierBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
-          to_bitField0_ |= 0x00000008;
-        }
-        result.hiveQueryId_ = hiveQueryId_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
-          to_bitField0_ |= 0x00000010;
-        }
-        result.dagName_ = dagName_;
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
-          to_bitField0_ |= 0x00000020;
-        }
-        result.vertexName_ = vertexName_;
-        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
-          to_bitField0_ |= 0x00000040;
-        }
-        result.vertexIndex_ = vertexIndex_;
-        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
-          to_bitField0_ |= 0x00000080;
-        }
-        result.tokenIdentifier_ = tokenIdentifier_;
-        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
-          to_bitField0_ |= 0x00000100;
-        }
-        if (processorDescriptorBuilder_ == null) {
-          result.processorDescriptor_ = processorDescriptor_;
-        } else {
-          result.processorDescriptor_ = processorDescriptorBuilder_.build();
-        }
+        buildPartialRepeatedFields(result);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartialRepeatedFields(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec result) {
         if (inputSpecsBuilder_ == null) {
-          if (((bitField0_ & 0x00000200) == 0x00000200)) {
+          if (((bitField0_ & 0x00000200) != 0)) {
             inputSpecs_ = java.util.Collections.unmodifiableList(inputSpecs_);
             bitField0_ = (bitField0_ & ~0x00000200);
           }
@@ -4663,7 +4932,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableV
           result.inputSpecs_ = inputSpecsBuilder_.build();
         }
         if (outputSpecsBuilder_ == null) {
-          if (((bitField0_ & 0x00000400) == 0x00000400)) {
+          if (((bitField0_ & 0x00000400) != 0)) {
             outputSpecs_ = java.util.Collections.unmodifiableList(outputSpecs_);
             bitField0_ = (bitField0_ & ~0x00000400);
           }
@@ -4672,7 +4941,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableV
           result.outputSpecs_ = outputSpecsBuilder_.build();
         }
         if (groupedInputSpecsBuilder_ == null) {
-          if (((bitField0_ & 0x00000800) == 0x00000800)) {
+          if (((bitField0_ & 0x00000800) != 0)) {
             groupedInputSpecs_ = java.util.Collections.unmodifiableList(groupedInputSpecs_);
             bitField0_ = (bitField0_ & ~0x00000800);
           }
@@ -4680,19 +4949,95 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableV
         } else {
           result.groupedInputSpecs_ = groupedInputSpecsBuilder_.build();
         }
-        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec result) {
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.user_ = user_;
+          to_bitField0_ |= 0x00000001;
+        }
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.signatureKeyId_ = signatureKeyId_;
+          to_bitField0_ |= 0x00000002;
+        }
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.queryIdentifier_ = queryIdentifierBuilder_ == null
+              ? queryIdentifier_
+              : queryIdentifierBuilder_.build();
+          to_bitField0_ |= 0x00000004;
+        }
+        if (((from_bitField0_ & 0x00000008) != 0)) {
+          result.hiveQueryId_ = hiveQueryId_;
+          to_bitField0_ |= 0x00000008;
+        }
+        if (((from_bitField0_ & 0x00000010) != 0)) {
+          result.dagName_ = dagName_;
+          to_bitField0_ |= 0x00000010;
+        }
+        if (((from_bitField0_ & 0x00000020) != 0)) {
+          result.vertexName_ = vertexName_;
+          to_bitField0_ |= 0x00000020;
+        }
+        if (((from_bitField0_ & 0x00000040) != 0)) {
+          result.vertexIndex_ = vertexIndex_;
+          to_bitField0_ |= 0x00000040;
+        }
+        if (((from_bitField0_ & 0x00000080) != 0)) {
+          result.tokenIdentifier_ = tokenIdentifier_;
+          to_bitField0_ |= 0x00000080;
+        }
+        if (((from_bitField0_ & 0x00000100) != 0)) {
+          result.processorDescriptor_ = processorDescriptorBuilder_ == null
+              ? processorDescriptor_
+              : processorDescriptorBuilder_.build();
+          to_bitField0_ |= 0x00000100;
+        }
+        if (((from_bitField0_ & 0x00001000) != 0)) {
+          result.vertexParallelism_ = vertexParallelism_;
           to_bitField0_ |= 0x00000200;
         }
-        result.vertexParallelism_ = vertexParallelism_;
-        if (((from_bitField0_ & 0x00002000) == 0x00002000)) {
+        if (((from_bitField0_ & 0x00002000) != 0)) {
+          result.isExternalSubmission_ = isExternalSubmission_;
           to_bitField0_ |= 0x00000400;
         }
-        result.isExternalSubmission_ = isExternalSubmission_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec)other);
@@ -4705,8 +5050,8 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance()) return this;
         if (other.hasUser()) {
-          bitField0_ |= 0x00000001;
           user_ = other.user_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasSignatureKeyId()) {
@@ -4716,26 +5061,26 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           mergeQueryIdentifier(other.getQueryIdentifier());
         }
         if (other.hasHiveQueryId()) {
-          bitField0_ |= 0x00000008;
           hiveQueryId_ = other.hiveQueryId_;
+          bitField0_ |= 0x00000008;
           onChanged();
         }
         if (other.hasDagName()) {
-          bitField0_ |= 0x00000010;
           dagName_ = other.dagName_;
+          bitField0_ |= 0x00000010;
           onChanged();
         }
         if (other.hasVertexName()) {
-          bitField0_ |= 0x00000020;
           vertexName_ = other.vertexName_;
+          bitField0_ |= 0x00000020;
           onChanged();
         }
         if (other.hasVertexIndex()) {
           setVertexIndex(other.getVertexIndex());
         }
         if (other.hasTokenIdentifier()) {
-          bitField0_ |= 0x00000080;
           tokenIdentifier_ = other.tokenIdentifier_;
+          bitField0_ |= 0x00000080;
           onChanged();
         }
         if (other.hasProcessorDescriptor()) {
@@ -4760,7 +5105,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
               inputSpecs_ = other.inputSpecs_;
               bitField0_ = (bitField0_ & ~0x00000200);
               inputSpecsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getInputSpecsFieldBuilder() : null;
             } else {
               inputSpecsBuilder_.addAllMessages(other.inputSpecs_);
@@ -4786,7 +5131,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
               outputSpecs_ = other.outputSpecs_;
               bitField0_ = (bitField0_ & ~0x00000400);
               outputSpecsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getOutputSpecsFieldBuilder() : null;
             } else {
               outputSpecsBuilder_.addAllMessages(other.outputSpecs_);
@@ -4812,7 +5157,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
               groupedInputSpecs_ = other.groupedInputSpecs_;
               bitField0_ = (bitField0_ & ~0x00000800);
               groupedInputSpecsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getGroupedInputSpecsFieldBuilder() : null;
             } else {
               groupedInputSpecsBuilder_.addAllMessages(other.groupedInputSpecs_);
@@ -4826,49 +5171,167 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setIsExternalSubmission(other.getIsExternalSubmission());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                user_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 16: {
+                signatureKeyId_ = input.readInt64();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 16
+              case 26: {
+                input.readMessage(
+                    getQueryIdentifierFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 26
+              case 34: {
+                hiveQueryId_ = input.readBytes();
+                bitField0_ |= 0x00000008;
+                break;
+              } // case 34
+              case 42: {
+                dagName_ = input.readBytes();
+                bitField0_ |= 0x00000010;
+                break;
+              } // case 42
+              case 50: {
+                vertexName_ = input.readBytes();
+                bitField0_ |= 0x00000020;
+                break;
+              } // case 50
+              case 56: {
+                vertexIndex_ = input.readInt32();
+                bitField0_ |= 0x00000040;
+                break;
+              } // case 56
+              case 66: {
+                tokenIdentifier_ = input.readBytes();
+                bitField0_ |= 0x00000080;
+                break;
+              } // case 66
+              case 74: {
+                input.readMessage(
+                    getProcessorDescriptorFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000100;
+                break;
+              } // case 74
+              case 82: {
+                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.PARSER,
+                        extensionRegistry);
+                if (inputSpecsBuilder_ == null) {
+                  ensureInputSpecsIsMutable();
+                  inputSpecs_.add(m);
+                } else {
+                  inputSpecsBuilder_.addMessage(m);
+                }
+                break;
+              } // case 82
+              case 90: {
+                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.PARSER,
+                        extensionRegistry);
+                if (outputSpecsBuilder_ == null) {
+                  ensureOutputSpecsIsMutable();
+                  outputSpecs_.add(m);
+                } else {
+                  outputSpecsBuilder_.addMessage(m);
+                }
+                break;
+              } // case 90
+              case 98: {
+                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.PARSER,
+                        extensionRegistry);
+                if (groupedInputSpecsBuilder_ == null) {
+                  ensureGroupedInputSpecsIsMutable();
+                  groupedInputSpecs_.add(m);
+                } else {
+                  groupedInputSpecsBuilder_.addMessage(m);
+                }
+                break;
+              } // case 98
+              case 104: {
+                vertexParallelism_ = input.readInt32();
+                bitField0_ |= 0x00001000;
+                break;
+              } // case 104
+              case 112: {
+                isExternalSubmission_ = input.readBool();
+                bitField0_ |= 0x00002000;
+                break;
+              } // case 112
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string user = 1;
       private java.lang.Object user_ = "";
       /**
        * <code>optional string user = 1;</code>
+       * @return Whether the user field is set.
        */
       public boolean hasUser() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string user = 1;</code>
+       * @return The user.
        */
       public java.lang.String getUser() {
         java.lang.Object ref = user_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          user_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            user_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -4876,6 +5339,7 @@ public java.lang.String getUser() {
       }
       /**
        * <code>optional string user = 1;</code>
+       * @return The bytes for user.
        */
       public com.google.protobuf.ByteString
           getUserBytes() {
@@ -4892,65 +5356,73 @@ public java.lang.String getUser() {
       }
       /**
        * <code>optional string user = 1;</code>
+       * @param value The user to set.
+       * @return This builder for chaining.
        */
       public Builder setUser(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         user_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string user = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearUser() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         user_ = getDefaultInstance().getUser();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string user = 1;</code>
+       * @param value The bytes for user to set.
+       * @return This builder for chaining.
        */
       public Builder setUserBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         user_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // optional int64 signatureKeyId = 2;
       private long signatureKeyId_ ;
       /**
        * <code>optional int64 signatureKeyId = 2;</code>
+       * @return Whether the signatureKeyId field is set.
        */
+      @java.lang.Override
       public boolean hasSignatureKeyId() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional int64 signatureKeyId = 2;</code>
+       * @return The signatureKeyId.
        */
+      @java.lang.Override
       public long getSignatureKeyId() {
         return signatureKeyId_;
       }
       /**
        * <code>optional int64 signatureKeyId = 2;</code>
+       * @param value The signatureKeyId to set.
+       * @return This builder for chaining.
        */
       public Builder setSignatureKeyId(long value) {
-        bitField0_ |= 0x00000002;
+
         signatureKeyId_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 signatureKeyId = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearSignatureKeyId() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -4959,22 +5431,23 @@ public Builder clearSignatureKeyId() {
         return this;
       }
 
-      // optional .QueryIdentifierProto query_identifier = 3;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
+       * @return Whether the queryIdentifier field is set.
        */
       public boolean hasQueryIdentifier() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
+       * @return The queryIdentifier.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
         if (queryIdentifierBuilder_ == null) {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         } else {
           return queryIdentifierBuilder_.getMessage();
         }
@@ -4988,11 +5461,11 @@ public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDae
             throw new NullPointerException();
           }
           queryIdentifier_ = value;
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000004;
+        onChanged();
         return this;
       }
       /**
@@ -5002,11 +5475,11 @@ public Builder setQueryIdentifier(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
         if (queryIdentifierBuilder_ == null) {
           queryIdentifier_ = builderForValue.build();
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000004;
+        onChanged();
         return this;
       }
       /**
@@ -5014,31 +5487,33 @@ public Builder setQueryIdentifier(
        */
       public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
         if (queryIdentifierBuilder_ == null) {
-          if (((bitField0_ & 0x00000004) == 0x00000004) &&
-              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
-            queryIdentifier_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000004) != 0) &&
+            queryIdentifier_ != null &&
+            queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
+            getQueryIdentifierBuilder().mergeFrom(value);
           } else {
             queryIdentifier_ = value;
           }
-          onChanged();
         } else {
           queryIdentifierBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000004;
+        if (queryIdentifier_ != null) {
+          bitField0_ |= 0x00000004;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
        */
       public Builder clearQueryIdentifier() {
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-          onChanged();
-        } else {
-          queryIdentifierBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000004);
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -5056,19 +5531,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         if (queryIdentifierBuilder_ != null) {
           return queryIdentifierBuilder_.getMessageOrBuilder();
         } else {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         }
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
           getQueryIdentifierFieldBuilder() {
         if (queryIdentifierBuilder_ == null) {
-          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
-                  queryIdentifier_,
+                  getQueryIdentifier(),
                   getParentForChildren(),
                   isClean());
           queryIdentifier_ = null;
@@ -5076,23 +5552,27 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         return queryIdentifierBuilder_;
       }
 
-      // optional string hive_query_id = 4;
       private java.lang.Object hiveQueryId_ = "";
       /**
        * <code>optional string hive_query_id = 4;</code>
+       * @return Whether the hiveQueryId field is set.
        */
       public boolean hasHiveQueryId() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
+        return ((bitField0_ & 0x00000008) != 0);
       }
       /**
        * <code>optional string hive_query_id = 4;</code>
+       * @return The hiveQueryId.
        */
       public java.lang.String getHiveQueryId() {
         java.lang.Object ref = hiveQueryId_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          hiveQueryId_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            hiveQueryId_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -5100,6 +5580,7 @@ public java.lang.String getHiveQueryId() {
       }
       /**
        * <code>optional string hive_query_id = 4;</code>
+       * @return The bytes for hiveQueryId.
        */
       public com.google.protobuf.ByteString
           getHiveQueryIdBytes() {
@@ -5116,76 +5597,82 @@ public java.lang.String getHiveQueryId() {
       }
       /**
        * <code>optional string hive_query_id = 4;</code>
+       * @param value The hiveQueryId to set.
+       * @return This builder for chaining.
        */
       public Builder setHiveQueryId(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000008;
+        if (value == null) { throw new NullPointerException(); }
         hiveQueryId_ = value;
+        bitField0_ |= 0x00000008;
         onChanged();
         return this;
       }
       /**
        * <code>optional string hive_query_id = 4;</code>
+       * @return This builder for chaining.
        */
       public Builder clearHiveQueryId() {
-        bitField0_ = (bitField0_ & ~0x00000008);
         hiveQueryId_ = getDefaultInstance().getHiveQueryId();
+        bitField0_ = (bitField0_ & ~0x00000008);
         onChanged();
         return this;
       }
       /**
        * <code>optional string hive_query_id = 4;</code>
+       * @param value The bytes for hiveQueryId to set.
+       * @return This builder for chaining.
        */
       public Builder setHiveQueryIdBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000008;
+        if (value == null) { throw new NullPointerException(); }
         hiveQueryId_ = value;
+        bitField0_ |= 0x00000008;
         onChanged();
         return this;
       }
 
-      // optional string dag_name = 5;
       private java.lang.Object dagName_ = "";
       /**
-       * <code>optional string dag_name = 5;</code>
-       *
        * <pre>
        * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
        * </pre>
+       *
+       * <code>optional string dag_name = 5;</code>
+       * @return Whether the dagName field is set.
        */
       public boolean hasDagName() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
+        return ((bitField0_ & 0x00000010) != 0);
       }
       /**
-       * <code>optional string dag_name = 5;</code>
-       *
        * <pre>
        * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
        * </pre>
+       *
+       * <code>optional string dag_name = 5;</code>
+       * @return The dagName.
        */
       public java.lang.String getDagName() {
         java.lang.Object ref = dagName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          dagName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            dagName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
         }
       }
       /**
-       * <code>optional string dag_name = 5;</code>
-       *
        * <pre>
        * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
        * </pre>
+       *
+       * <code>optional string dag_name = 5;</code>
+       * @return The bytes for dagName.
        */
       public com.google.protobuf.ByteString
           getDagNameBytes() {
@@ -5201,70 +5688,75 @@ public java.lang.String getDagName() {
         }
       }
       /**
-       * <code>optional string dag_name = 5;</code>
-       *
        * <pre>
        * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
        * </pre>
+       *
+       * <code>optional string dag_name = 5;</code>
+       * @param value The dagName to set.
+       * @return This builder for chaining.
        */
       public Builder setDagName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000010;
+        if (value == null) { throw new NullPointerException(); }
         dagName_ = value;
+        bitField0_ |= 0x00000010;
         onChanged();
         return this;
       }
       /**
-       * <code>optional string dag_name = 5;</code>
-       *
        * <pre>
        * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
        * </pre>
+       *
+       * <code>optional string dag_name = 5;</code>
+       * @return This builder for chaining.
        */
       public Builder clearDagName() {
-        bitField0_ = (bitField0_ & ~0x00000010);
         dagName_ = getDefaultInstance().getDagName();
+        bitField0_ = (bitField0_ & ~0x00000010);
         onChanged();
         return this;
       }
       /**
-       * <code>optional string dag_name = 5;</code>
-       *
        * <pre>
        * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
        * </pre>
+       *
+       * <code>optional string dag_name = 5;</code>
+       * @param value The bytes for dagName to set.
+       * @return This builder for chaining.
        */
       public Builder setDagNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000010;
+        if (value == null) { throw new NullPointerException(); }
         dagName_ = value;
+        bitField0_ |= 0x00000010;
         onChanged();
         return this;
       }
 
-      // optional string vertex_name = 6;
       private java.lang.Object vertexName_ = "";
       /**
        * <code>optional string vertex_name = 6;</code>
+       * @return Whether the vertexName field is set.
        */
       public boolean hasVertexName() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
+        return ((bitField0_ & 0x00000020) != 0);
       }
       /**
        * <code>optional string vertex_name = 6;</code>
+       * @return The vertexName.
        */
       public java.lang.String getVertexName() {
         java.lang.Object ref = vertexName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          vertexName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            vertexName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -5272,6 +5764,7 @@ public java.lang.String getVertexName() {
       }
       /**
        * <code>optional string vertex_name = 6;</code>
+       * @return The bytes for vertexName.
        */
       public com.google.protobuf.ByteString
           getVertexNameBytes() {
@@ -5288,65 +5781,73 @@ public java.lang.String getVertexName() {
       }
       /**
        * <code>optional string vertex_name = 6;</code>
+       * @param value The vertexName to set.
+       * @return This builder for chaining.
        */
       public Builder setVertexName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000020;
+        if (value == null) { throw new NullPointerException(); }
         vertexName_ = value;
+        bitField0_ |= 0x00000020;
         onChanged();
         return this;
       }
       /**
        * <code>optional string vertex_name = 6;</code>
+       * @return This builder for chaining.
        */
       public Builder clearVertexName() {
-        bitField0_ = (bitField0_ & ~0x00000020);
         vertexName_ = getDefaultInstance().getVertexName();
+        bitField0_ = (bitField0_ & ~0x00000020);
         onChanged();
         return this;
       }
       /**
        * <code>optional string vertex_name = 6;</code>
+       * @param value The bytes for vertexName to set.
+       * @return This builder for chaining.
        */
       public Builder setVertexNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000020;
+        if (value == null) { throw new NullPointerException(); }
         vertexName_ = value;
+        bitField0_ |= 0x00000020;
         onChanged();
         return this;
       }
 
-      // optional int32 vertex_index = 7;
       private int vertexIndex_ ;
       /**
        * <code>optional int32 vertex_index = 7;</code>
+       * @return Whether the vertexIndex field is set.
        */
+      @java.lang.Override
       public boolean hasVertexIndex() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
+        return ((bitField0_ & 0x00000040) != 0);
       }
       /**
        * <code>optional int32 vertex_index = 7;</code>
+       * @return The vertexIndex.
        */
+      @java.lang.Override
       public int getVertexIndex() {
         return vertexIndex_;
       }
       /**
        * <code>optional int32 vertex_index = 7;</code>
+       * @param value The vertexIndex to set.
+       * @return This builder for chaining.
        */
       public Builder setVertexIndex(int value) {
-        bitField0_ |= 0x00000040;
+
         vertexIndex_ = value;
+        bitField0_ |= 0x00000040;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 vertex_index = 7;</code>
+       * @return This builder for chaining.
        */
       public Builder clearVertexIndex() {
         bitField0_ = (bitField0_ & ~0x00000040);
@@ -5355,42 +5856,47 @@ public Builder clearVertexIndex() {
         return this;
       }
 
-      // optional string token_identifier = 8;
       private java.lang.Object tokenIdentifier_ = "";
       /**
-       * <code>optional string token_identifier = 8;</code>
-       *
        * <pre>
        * The core vertex stuff 
        * </pre>
+       *
+       * <code>optional string token_identifier = 8;</code>
+       * @return Whether the tokenIdentifier field is set.
        */
       public boolean hasTokenIdentifier() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
+        return ((bitField0_ & 0x00000080) != 0);
       }
       /**
-       * <code>optional string token_identifier = 8;</code>
-       *
        * <pre>
        * The core vertex stuff 
        * </pre>
+       *
+       * <code>optional string token_identifier = 8;</code>
+       * @return The tokenIdentifier.
        */
       public java.lang.String getTokenIdentifier() {
         java.lang.Object ref = tokenIdentifier_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          tokenIdentifier_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            tokenIdentifier_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
         }
       }
       /**
-       * <code>optional string token_identifier = 8;</code>
-       *
        * <pre>
        * The core vertex stuff 
        * </pre>
+       *
+       * <code>optional string token_identifier = 8;</code>
+       * @return The bytes for tokenIdentifier.
        */
       public com.google.protobuf.ByteString
           getTokenIdentifierBytes() {
@@ -5406,69 +5912,71 @@ public java.lang.String getTokenIdentifier() {
         }
       }
       /**
-       * <code>optional string token_identifier = 8;</code>
-       *
        * <pre>
        * The core vertex stuff 
        * </pre>
+       *
+       * <code>optional string token_identifier = 8;</code>
+       * @param value The tokenIdentifier to set.
+       * @return This builder for chaining.
        */
       public Builder setTokenIdentifier(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000080;
+        if (value == null) { throw new NullPointerException(); }
         tokenIdentifier_ = value;
+        bitField0_ |= 0x00000080;
         onChanged();
         return this;
       }
       /**
-       * <code>optional string token_identifier = 8;</code>
-       *
        * <pre>
        * The core vertex stuff 
        * </pre>
+       *
+       * <code>optional string token_identifier = 8;</code>
+       * @return This builder for chaining.
        */
       public Builder clearTokenIdentifier() {
-        bitField0_ = (bitField0_ & ~0x00000080);
         tokenIdentifier_ = getDefaultInstance().getTokenIdentifier();
+        bitField0_ = (bitField0_ & ~0x00000080);
         onChanged();
         return this;
       }
       /**
-       * <code>optional string token_identifier = 8;</code>
-       *
        * <pre>
        * The core vertex stuff 
        * </pre>
+       *
+       * <code>optional string token_identifier = 8;</code>
+       * @param value The bytes for tokenIdentifier to set.
+       * @return This builder for chaining.
        */
       public Builder setTokenIdentifierBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000080;
+        if (value == null) { throw new NullPointerException(); }
         tokenIdentifier_ = value;
+        bitField0_ |= 0x00000080;
         onChanged();
         return this;
       }
 
-      // optional .EntityDescriptorProto processor_descriptor = 9;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto processorDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto processorDescriptor_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> processorDescriptorBuilder_;
       /**
        * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
+       * @return Whether the processorDescriptor field is set.
        */
       public boolean hasProcessorDescriptor() {
-        return ((bitField0_ & 0x00000100) == 0x00000100);
+        return ((bitField0_ & 0x00000100) != 0);
       }
       /**
        * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
+       * @return The processorDescriptor.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getProcessorDescriptor() {
         if (processorDescriptorBuilder_ == null) {
-          return processorDescriptor_;
+          return processorDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : processorDescriptor_;
         } else {
           return processorDescriptorBuilder_.getMessage();
         }
@@ -5482,11 +5990,11 @@ public Builder setProcessorDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.Lla
             throw new NullPointerException();
           }
           processorDescriptor_ = value;
-          onChanged();
         } else {
           processorDescriptorBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000100;
+        onChanged();
         return this;
       }
       /**
@@ -5496,11 +6004,11 @@ public Builder setProcessorDescriptor(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder builderForValue) {
         if (processorDescriptorBuilder_ == null) {
           processorDescriptor_ = builderForValue.build();
-          onChanged();
         } else {
           processorDescriptorBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000100;
+        onChanged();
         return this;
       }
       /**
@@ -5508,31 +6016,33 @@ public Builder setProcessorDescriptor(
        */
       public Builder mergeProcessorDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto value) {
         if (processorDescriptorBuilder_ == null) {
-          if (((bitField0_ & 0x00000100) == 0x00000100) &&
-              processorDescriptor_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) {
-            processorDescriptor_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.newBuilder(processorDescriptor_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000100) != 0) &&
+            processorDescriptor_ != null &&
+            processorDescriptor_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) {
+            getProcessorDescriptorBuilder().mergeFrom(value);
           } else {
             processorDescriptor_ = value;
           }
-          onChanged();
         } else {
           processorDescriptorBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000100;
+        if (processorDescriptor_ != null) {
+          bitField0_ |= 0x00000100;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
        */
       public Builder clearProcessorDescriptor() {
-        if (processorDescriptorBuilder_ == null) {
-          processorDescriptor_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
-          onChanged();
-        } else {
-          processorDescriptorBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000100);
+        processorDescriptor_ = null;
+        if (processorDescriptorBuilder_ != null) {
+          processorDescriptorBuilder_.dispose();
+          processorDescriptorBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -5550,19 +6060,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDes
         if (processorDescriptorBuilder_ != null) {
           return processorDescriptorBuilder_.getMessageOrBuilder();
         } else {
-          return processorDescriptor_;
+          return processorDescriptor_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : processorDescriptor_;
         }
       }
       /**
        * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> 
           getProcessorDescriptorFieldBuilder() {
         if (processorDescriptorBuilder_ == null) {
-          processorDescriptorBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          processorDescriptorBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder>(
-                  processorDescriptor_,
+                  getProcessorDescriptor(),
                   getParentForChildren(),
                   isClean());
           processorDescriptor_ = null;
@@ -5570,17 +6081,16 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDes
         return processorDescriptorBuilder_;
       }
 
-      // repeated .IOSpecProto input_specs = 10;
       private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> inputSpecs_ =
         java.util.Collections.emptyList();
       private void ensureInputSpecsIsMutable() {
-        if (!((bitField0_ & 0x00000200) == 0x00000200)) {
+        if (!((bitField0_ & 0x00000200) != 0)) {
           inputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto>(inputSpecs_);
           bitField0_ |= 0x00000200;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> inputSpecsBuilder_;
 
       /**
@@ -5712,7 +6222,8 @@ public Builder addAllInputSpecs(
           java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> values) {
         if (inputSpecsBuilder_ == null) {
           ensureInputSpecsIsMutable();
-          super.addAll(values, inputSpecs_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, inputSpecs_);
           onChanged();
         } else {
           inputSpecsBuilder_.addAllMessages(values);
@@ -5795,14 +6306,14 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecPro
            getInputSpecsBuilderList() {
         return getInputSpecsFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
           getInputSpecsFieldBuilder() {
         if (inputSpecsBuilder_ == null) {
-          inputSpecsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          inputSpecsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder>(
                   inputSpecs_,
-                  ((bitField0_ & 0x00000200) == 0x00000200),
+                  ((bitField0_ & 0x00000200) != 0),
                   getParentForChildren(),
                   isClean());
           inputSpecs_ = null;
@@ -5810,17 +6321,16 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecPro
         return inputSpecsBuilder_;
       }
 
-      // repeated .IOSpecProto output_specs = 11;
       private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> outputSpecs_ =
         java.util.Collections.emptyList();
       private void ensureOutputSpecsIsMutable() {
-        if (!((bitField0_ & 0x00000400) == 0x00000400)) {
+        if (!((bitField0_ & 0x00000400) != 0)) {
           outputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto>(outputSpecs_);
           bitField0_ |= 0x00000400;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> outputSpecsBuilder_;
 
       /**
@@ -5952,7 +6462,8 @@ public Builder addAllOutputSpecs(
           java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> values) {
         if (outputSpecsBuilder_ == null) {
           ensureOutputSpecsIsMutable();
-          super.addAll(values, outputSpecs_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, outputSpecs_);
           onChanged();
         } else {
           outputSpecsBuilder_.addAllMessages(values);
@@ -6035,14 +6546,14 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecPro
            getOutputSpecsBuilderList() {
         return getOutputSpecsFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
           getOutputSpecsFieldBuilder() {
         if (outputSpecsBuilder_ == null) {
-          outputSpecsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          outputSpecsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder>(
                   outputSpecs_,
-                  ((bitField0_ & 0x00000400) == 0x00000400),
+                  ((bitField0_ & 0x00000400) != 0),
                   getParentForChildren(),
                   isClean());
           outputSpecs_ = null;
@@ -6050,17 +6561,16 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecPro
         return outputSpecsBuilder_;
       }
 
-      // repeated .GroupInputSpecProto grouped_input_specs = 12;
       private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> groupedInputSpecs_ =
         java.util.Collections.emptyList();
       private void ensureGroupedInputSpecsIsMutable() {
-        if (!((bitField0_ & 0x00000800) == 0x00000800)) {
+        if (!((bitField0_ & 0x00000800) != 0)) {
           groupedInputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto>(groupedInputSpecs_);
           bitField0_ |= 0x00000800;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder> groupedInputSpecsBuilder_;
 
       /**
@@ -6192,7 +6702,8 @@ public Builder addAllGroupedInputSpecs(
           java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> values) {
         if (groupedInputSpecsBuilder_ == null) {
           ensureGroupedInputSpecsIsMutable();
-          super.addAll(values, groupedInputSpecs_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, groupedInputSpecs_);
           onChanged();
         } else {
           groupedInputSpecsBuilder_.addAllMessages(values);
@@ -6275,14 +6786,14 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInpu
            getGroupedInputSpecsBuilderList() {
         return getGroupedInputSpecsFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder> 
           getGroupedInputSpecsFieldBuilder() {
         if (groupedInputSpecsBuilder_ == null) {
-          groupedInputSpecsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          groupedInputSpecsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder>(
                   groupedInputSpecs_,
-                  ((bitField0_ & 0x00000800) == 0x00000800),
+                  ((bitField0_ & 0x00000800) != 0),
                   getParentForChildren(),
                   isClean());
           groupedInputSpecs_ = null;
@@ -6290,47 +6801,54 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInpu
         return groupedInputSpecsBuilder_;
       }
 
-      // optional int32 vertex_parallelism = 13;
       private int vertexParallelism_ ;
       /**
-       * <code>optional int32 vertex_parallelism = 13;</code>
-       *
        * <pre>
        * An internal field required for Tez.
        * </pre>
+       *
+       * <code>optional int32 vertex_parallelism = 13;</code>
+       * @return Whether the vertexParallelism field is set.
        */
+      @java.lang.Override
       public boolean hasVertexParallelism() {
-        return ((bitField0_ & 0x00001000) == 0x00001000);
+        return ((bitField0_ & 0x00001000) != 0);
       }
       /**
-       * <code>optional int32 vertex_parallelism = 13;</code>
-       *
        * <pre>
        * An internal field required for Tez.
        * </pre>
+       *
+       * <code>optional int32 vertex_parallelism = 13;</code>
+       * @return The vertexParallelism.
        */
+      @java.lang.Override
       public int getVertexParallelism() {
         return vertexParallelism_;
       }
       /**
-       * <code>optional int32 vertex_parallelism = 13;</code>
-       *
        * <pre>
        * An internal field required for Tez.
        * </pre>
+       *
+       * <code>optional int32 vertex_parallelism = 13;</code>
+       * @param value The vertexParallelism to set.
+       * @return This builder for chaining.
        */
       public Builder setVertexParallelism(int value) {
-        bitField0_ |= 0x00001000;
+
         vertexParallelism_ = value;
+        bitField0_ |= 0x00001000;
         onChanged();
         return this;
       }
       /**
-       * <code>optional int32 vertex_parallelism = 13;</code>
-       *
        * <pre>
        * An internal field required for Tez.
        * </pre>
+       *
+       * <code>optional int32 vertex_parallelism = 13;</code>
+       * @return This builder for chaining.
        */
       public Builder clearVertexParallelism() {
         bitField0_ = (bitField0_ & ~0x00001000);
@@ -6339,31 +6857,38 @@ public Builder clearVertexParallelism() {
         return this;
       }
 
-      // optional bool is_external_submission = 14 [default = false];
       private boolean isExternalSubmission_ ;
       /**
        * <code>optional bool is_external_submission = 14 [default = false];</code>
+       * @return Whether the isExternalSubmission field is set.
        */
+      @java.lang.Override
       public boolean hasIsExternalSubmission() {
-        return ((bitField0_ & 0x00002000) == 0x00002000);
+        return ((bitField0_ & 0x00002000) != 0);
       }
       /**
        * <code>optional bool is_external_submission = 14 [default = false];</code>
+       * @return The isExternalSubmission.
        */
+      @java.lang.Override
       public boolean getIsExternalSubmission() {
         return isExternalSubmission_;
       }
       /**
        * <code>optional bool is_external_submission = 14 [default = false];</code>
+       * @param value The isExternalSubmission to set.
+       * @return This builder for chaining.
        */
       public Builder setIsExternalSubmission(boolean value) {
-        bitField0_ |= 0x00002000;
+
         isExternalSubmission_ = value;
+        bitField0_ |= 0x00002000;
         onChanged();
         return this;
       }
       /**
        * <code>optional bool is_external_submission = 14 [default = false];</code>
+       * @return This builder for chaining.
        */
       public Builder clearIsExternalSubmission() {
         bitField0_ = (bitField0_ & ~0x00002000);
@@ -6371,28 +6896,82 @@ public Builder clearIsExternalSubmission() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:SignableVertexSpec)
     }
 
+    // @@protoc_insertion_point(class_scope:SignableVertexSpec)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec DEFAULT_INSTANCE;
     static {
-      defaultInstance = new SignableVertexSpec(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<SignableVertexSpec>
+        PARSER = new com.google.protobuf.AbstractParser<SignableVertexSpec>() {
+      @java.lang.Override
+      public SignableVertexSpec parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<SignableVertexSpec> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<SignableVertexSpec> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:SignableVertexSpec)
   }
 
-  public interface VertexOrBinaryOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface VertexOrBinaryOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:VertexOrBinary)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional .SignableVertexSpec vertex = 1;
     /**
      * <code>optional .SignableVertexSpec vertex = 1;</code>
+     * @return Whether the vertex field is set.
      */
     boolean hasVertex();
     /**
      * <code>optional .SignableVertexSpec vertex = 1;</code>
+     * @return The vertex.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getVertex();
     /**
@@ -6400,234 +6979,161 @@ public interface VertexOrBinaryOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder getVertexOrBuilder();
 
-    // optional bytes vertexBinary = 2;
     /**
-     * <code>optional bytes vertexBinary = 2;</code>
-     *
      * <pre>
      * SignableVertexSpec
      * </pre>
+     *
+     * <code>optional bytes vertexBinary = 2;</code>
+     * @return Whether the vertexBinary field is set.
      */
     boolean hasVertexBinary();
     /**
-     * <code>optional bytes vertexBinary = 2;</code>
-     *
      * <pre>
      * SignableVertexSpec
      * </pre>
+     *
+     * <code>optional bytes vertexBinary = 2;</code>
+     * @return The vertexBinary.
      */
     com.google.protobuf.ByteString getVertexBinary();
   }
   /**
-   * Protobuf type {@code VertexOrBinary}
-   *
    * <pre>
    * Union
    * </pre>
+   *
+   * Protobuf type {@code VertexOrBinary}
    */
   public static final class VertexOrBinary extends
-      com.google.protobuf.GeneratedMessage
-      implements VertexOrBinaryOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:VertexOrBinary)
+      VertexOrBinaryOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use VertexOrBinary.newBuilder() to construct.
-    private VertexOrBinary(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private VertexOrBinary(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private VertexOrBinary(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final VertexOrBinary defaultInstance;
-    public static VertexOrBinary getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public VertexOrBinary getDefaultInstanceForType() {
-      return defaultInstance;
+    private VertexOrBinary() {
+      vertexBinary_ = com.google.protobuf.ByteString.EMPTY;
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private VertexOrBinary(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = vertex_.toBuilder();
-              }
-              vertex_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(vertex_);
-                vertex_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              vertexBinary_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new VertexOrBinary();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<VertexOrBinary> PARSER =
-        new com.google.protobuf.AbstractParser<VertexOrBinary>() {
-      public VertexOrBinary parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new VertexOrBinary(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<VertexOrBinary> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional .SignableVertexSpec vertex = 1;
     public static final int VERTEX_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec vertex_;
     /**
      * <code>optional .SignableVertexSpec vertex = 1;</code>
+     * @return Whether the vertex field is set.
      */
+    @java.lang.Override
     public boolean hasVertex() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional .SignableVertexSpec vertex = 1;</code>
+     * @return The vertex.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getVertex() {
-      return vertex_;
+      return vertex_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance() : vertex_;
     }
     /**
      * <code>optional .SignableVertexSpec vertex = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder getVertexOrBuilder() {
-      return vertex_;
+      return vertex_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance() : vertex_;
     }
 
-    // optional bytes vertexBinary = 2;
     public static final int VERTEXBINARY_FIELD_NUMBER = 2;
-    private com.google.protobuf.ByteString vertexBinary_;
+    private com.google.protobuf.ByteString vertexBinary_ = com.google.protobuf.ByteString.EMPTY;
     /**
-     * <code>optional bytes vertexBinary = 2;</code>
-     *
      * <pre>
      * SignableVertexSpec
      * </pre>
+     *
+     * <code>optional bytes vertexBinary = 2;</code>
+     * @return Whether the vertexBinary field is set.
      */
+    @java.lang.Override
     public boolean hasVertexBinary() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
-     * <code>optional bytes vertexBinary = 2;</code>
-     *
      * <pre>
      * SignableVertexSpec
      * </pre>
+     *
+     * <code>optional bytes vertexBinary = 2;</code>
+     * @return The vertexBinary.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getVertexBinary() {
       return vertexBinary_;
     }
 
-    private void initFields() {
-      vertex_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance();
-      vertexBinary_ = com.google.protobuf.ByteString.EMPTY;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, vertex_);
+      if (((bitField0_ & 0x00000001) != 0)) {
+        output.writeMessage(1, getVertex());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeBytes(2, vertexBinary_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, vertex_);
+          .computeMessageSize(1, getVertex());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(2, vertexBinary_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -6638,30 +7144,27 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary) obj;
 
-      boolean result = true;
-      result = result && (hasVertex() == other.hasVertex());
+      if (hasVertex() != other.hasVertex()) return false;
       if (hasVertex()) {
-        result = result && getVertex()
-            .equals(other.getVertex());
+        if (!getVertex()
+            .equals(other.getVertex())) return false;
       }
-      result = result && (hasVertexBinary() == other.hasVertexBinary());
+      if (hasVertexBinary() != other.hasVertexBinary()) return false;
       if (hasVertexBinary()) {
-        result = result && getVertexBinary()
-            .equals(other.getVertexBinary());
+        if (!getVertexBinary()
+            .equals(other.getVertexBinary())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasVertex()) {
         hash = (37 * hash) + VERTEX_FIELD_NUMBER;
         hash = (53 * hash) + getVertex().hashCode();
@@ -6675,6 +7178,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -6698,65 +7212,82 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ve
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
     /**
-     * Protobuf type {@code VertexOrBinary}
-     *
      * <pre>
      * Union
      * </pre>
+     *
+     * Protobuf type {@code VertexOrBinary}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:VertexOrBinary)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -6769,45 +7300,41 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getVertexFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
-        if (vertexBuilder_ == null) {
-          vertex_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance();
-        } else {
-          vertexBuilder_.clear();
+        bitField0_ = 0;
+        vertex_ = null;
+        if (vertexBuilder_ != null) {
+          vertexBuilder_.dispose();
+          vertexBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000001);
         vertexBinary_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary result = buildPartial();
         if (!result.isInitialized()) {
@@ -6816,27 +7343,63 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrB
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.vertex_ = vertexBuilder_ == null
+              ? vertex_
+              : vertexBuilder_.build();
           to_bitField0_ |= 0x00000001;
         }
-        if (vertexBuilder_ == null) {
-          result.vertex_ = vertex_;
-        } else {
-          result.vertex_ = vertexBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.vertexBinary_ = vertexBinary_;
           to_bitField0_ |= 0x00000002;
         }
-        result.vertexBinary_ = vertexBinary_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary)other);
@@ -6855,48 +7418,77 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setVertexBinary(other.getVertexBinary());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                input.readMessage(
+                    getVertexFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                vertexBinary_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional .SignableVertexSpec vertex = 1;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec vertex_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec vertex_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder> vertexBuilder_;
       /**
        * <code>optional .SignableVertexSpec vertex = 1;</code>
+       * @return Whether the vertex field is set.
        */
       public boolean hasVertex() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional .SignableVertexSpec vertex = 1;</code>
+       * @return The vertex.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getVertex() {
         if (vertexBuilder_ == null) {
-          return vertex_;
+          return vertex_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance() : vertex_;
         } else {
           return vertexBuilder_.getMessage();
         }
@@ -6910,11 +7502,11 @@ public Builder setVertex(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
             throw new NullPointerException();
           }
           vertex_ = value;
-          onChanged();
         } else {
           vertexBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -6924,11 +7516,11 @@ public Builder setVertex(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder builderForValue) {
         if (vertexBuilder_ == null) {
           vertex_ = builderForValue.build();
-          onChanged();
         } else {
           vertexBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -6936,31 +7528,33 @@ public Builder setVertex(
        */
       public Builder mergeVertex(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec value) {
         if (vertexBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              vertex_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance()) {
-            vertex_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.newBuilder(vertex_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000001) != 0) &&
+            vertex_ != null &&
+            vertex_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance()) {
+            getVertexBuilder().mergeFrom(value);
           } else {
             vertex_ = value;
           }
-          onChanged();
         } else {
           vertexBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000001;
+        if (vertex_ != null) {
+          bitField0_ |= 0x00000001;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .SignableVertexSpec vertex = 1;</code>
        */
       public Builder clearVertex() {
-        if (vertexBuilder_ == null) {
-          vertex_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance();
-          onChanged();
-        } else {
-          vertexBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000001);
+        vertex_ = null;
+        if (vertexBuilder_ != null) {
+          vertexBuilder_.dispose();
+          vertexBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -6978,19 +7572,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableV
         if (vertexBuilder_ != null) {
           return vertexBuilder_.getMessageOrBuilder();
         } else {
-          return vertex_;
+          return vertex_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance() : vertex_;
         }
       }
       /**
        * <code>optional .SignableVertexSpec vertex = 1;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder> 
           getVertexFieldBuilder() {
         if (vertexBuilder_ == null) {
-          vertexBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          vertexBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder>(
-                  vertex_,
+                  getVertex(),
                   getParentForChildren(),
                   isClean());
           vertex_ = null;
@@ -6998,50 +7593,54 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableV
         return vertexBuilder_;
       }
 
-      // optional bytes vertexBinary = 2;
       private com.google.protobuf.ByteString vertexBinary_ = com.google.protobuf.ByteString.EMPTY;
       /**
-       * <code>optional bytes vertexBinary = 2;</code>
-       *
        * <pre>
        * SignableVertexSpec
        * </pre>
+       *
+       * <code>optional bytes vertexBinary = 2;</code>
+       * @return Whether the vertexBinary field is set.
        */
+      @java.lang.Override
       public boolean hasVertexBinary() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
-       * <code>optional bytes vertexBinary = 2;</code>
-       *
        * <pre>
        * SignableVertexSpec
        * </pre>
+       *
+       * <code>optional bytes vertexBinary = 2;</code>
+       * @return The vertexBinary.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getVertexBinary() {
         return vertexBinary_;
       }
       /**
-       * <code>optional bytes vertexBinary = 2;</code>
-       *
        * <pre>
        * SignableVertexSpec
        * </pre>
+       *
+       * <code>optional bytes vertexBinary = 2;</code>
+       * @param value The vertexBinary to set.
+       * @return This builder for chaining.
        */
       public Builder setVertexBinary(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         vertexBinary_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
-       * <code>optional bytes vertexBinary = 2;</code>
-       *
        * <pre>
        * SignableVertexSpec
        * </pre>
+       *
+       * <code>optional bytes vertexBinary = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearVertexBinary() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -7049,78 +7648,137 @@ public Builder clearVertexBinary() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:VertexOrBinary)
     }
 
+    // @@protoc_insertion_point(class_scope:VertexOrBinary)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary DEFAULT_INSTANCE;
     static {
-      defaultInstance = new VertexOrBinary(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary();
     }
 
-    // @@protoc_insertion_point(class_scope:VertexOrBinary)
-  }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
 
-  public interface FragmentRuntimeInfoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<VertexOrBinary>
+        PARSER = new com.google.protobuf.AbstractParser<VertexOrBinary>() {
+      @java.lang.Override
+      public VertexOrBinary parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
 
-    // optional int32 num_self_and_upstream_tasks = 1;
-    /**
-     * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
-     */
-    boolean hasNumSelfAndUpstreamTasks();
-    /**
+    public static com.google.protobuf.Parser<VertexOrBinary> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<VertexOrBinary> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
+    }
+
+  }
+
+  public interface FragmentRuntimeInfoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:FragmentRuntimeInfo)
+      com.google.protobuf.MessageOrBuilder {
+
+    /**
+     * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
+     * @return Whether the numSelfAndUpstreamTasks field is set.
+     */
+    boolean hasNumSelfAndUpstreamTasks();
+    /**
      * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
+     * @return The numSelfAndUpstreamTasks.
      */
     int getNumSelfAndUpstreamTasks();
 
-    // optional int32 num_self_and_upstream_completed_tasks = 2;
     /**
      * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
+     * @return Whether the numSelfAndUpstreamCompletedTasks field is set.
      */
     boolean hasNumSelfAndUpstreamCompletedTasks();
     /**
      * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
+     * @return The numSelfAndUpstreamCompletedTasks.
      */
     int getNumSelfAndUpstreamCompletedTasks();
 
-    // optional int32 within_dag_priority = 3;
     /**
      * <code>optional int32 within_dag_priority = 3;</code>
+     * @return Whether the withinDagPriority field is set.
      */
     boolean hasWithinDagPriority();
     /**
      * <code>optional int32 within_dag_priority = 3;</code>
+     * @return The withinDagPriority.
      */
     int getWithinDagPriority();
 
-    // optional int64 dag_start_time = 4;
     /**
      * <code>optional int64 dag_start_time = 4;</code>
+     * @return Whether the dagStartTime field is set.
      */
     boolean hasDagStartTime();
     /**
      * <code>optional int64 dag_start_time = 4;</code>
+     * @return The dagStartTime.
      */
     long getDagStartTime();
 
-    // optional int64 first_attempt_start_time = 5;
     /**
      * <code>optional int64 first_attempt_start_time = 5;</code>
+     * @return Whether the firstAttemptStartTime field is set.
      */
     boolean hasFirstAttemptStartTime();
     /**
      * <code>optional int64 first_attempt_start_time = 5;</code>
+     * @return The firstAttemptStartTime.
      */
     long getFirstAttemptStartTime();
 
-    // optional int64 current_attempt_start_time = 6;
     /**
      * <code>optional int64 current_attempt_start_time = 6;</code>
+     * @return Whether the currentAttemptStartTime field is set.
      */
     boolean hasCurrentAttemptStartTime();
     /**
      * <code>optional int64 current_attempt_start_time = 6;</code>
+     * @return The currentAttemptStartTime.
      */
     long getCurrentAttemptStartTime();
   }
@@ -7128,302 +7786,222 @@ public interface FragmentRuntimeInfoOrBuilder
    * Protobuf type {@code FragmentRuntimeInfo}
    */
   public static final class FragmentRuntimeInfo extends
-      com.google.protobuf.GeneratedMessage
-      implements FragmentRuntimeInfoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:FragmentRuntimeInfo)
+      FragmentRuntimeInfoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use FragmentRuntimeInfo.newBuilder() to construct.
-    private FragmentRuntimeInfo(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private FragmentRuntimeInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private FragmentRuntimeInfo(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final FragmentRuntimeInfo defaultInstance;
-    public static FragmentRuntimeInfo getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public FragmentRuntimeInfo getDefaultInstanceForType() {
-      return defaultInstance;
+    private FragmentRuntimeInfo() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private FragmentRuntimeInfo(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              numSelfAndUpstreamTasks_ = input.readInt32();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              numSelfAndUpstreamCompletedTasks_ = input.readInt32();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              withinDagPriority_ = input.readInt32();
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              dagStartTime_ = input.readInt64();
-              break;
-            }
-            case 40: {
-              bitField0_ |= 0x00000010;
-              firstAttemptStartTime_ = input.readInt64();
-              break;
-            }
-            case 48: {
-              bitField0_ |= 0x00000020;
-              currentAttemptStartTime_ = input.readInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new FragmentRuntimeInfo();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<FragmentRuntimeInfo> PARSER =
-        new com.google.protobuf.AbstractParser<FragmentRuntimeInfo>() {
-      public FragmentRuntimeInfo parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new FragmentRuntimeInfo(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<FragmentRuntimeInfo> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional int32 num_self_and_upstream_tasks = 1;
     public static final int NUM_SELF_AND_UPSTREAM_TASKS_FIELD_NUMBER = 1;
-    private int numSelfAndUpstreamTasks_;
+    private int numSelfAndUpstreamTasks_ = 0;
     /**
      * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
+     * @return Whether the numSelfAndUpstreamTasks field is set.
      */
+    @java.lang.Override
     public boolean hasNumSelfAndUpstreamTasks() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
+     * @return The numSelfAndUpstreamTasks.
      */
+    @java.lang.Override
     public int getNumSelfAndUpstreamTasks() {
       return numSelfAndUpstreamTasks_;
     }
 
-    // optional int32 num_self_and_upstream_completed_tasks = 2;
     public static final int NUM_SELF_AND_UPSTREAM_COMPLETED_TASKS_FIELD_NUMBER = 2;
-    private int numSelfAndUpstreamCompletedTasks_;
+    private int numSelfAndUpstreamCompletedTasks_ = 0;
     /**
      * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
+     * @return Whether the numSelfAndUpstreamCompletedTasks field is set.
      */
+    @java.lang.Override
     public boolean hasNumSelfAndUpstreamCompletedTasks() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
+     * @return The numSelfAndUpstreamCompletedTasks.
      */
+    @java.lang.Override
     public int getNumSelfAndUpstreamCompletedTasks() {
       return numSelfAndUpstreamCompletedTasks_;
     }
 
-    // optional int32 within_dag_priority = 3;
     public static final int WITHIN_DAG_PRIORITY_FIELD_NUMBER = 3;
-    private int withinDagPriority_;
+    private int withinDagPriority_ = 0;
     /**
      * <code>optional int32 within_dag_priority = 3;</code>
+     * @return Whether the withinDagPriority field is set.
      */
+    @java.lang.Override
     public boolean hasWithinDagPriority() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional int32 within_dag_priority = 3;</code>
+     * @return The withinDagPriority.
      */
+    @java.lang.Override
     public int getWithinDagPriority() {
       return withinDagPriority_;
     }
 
-    // optional int64 dag_start_time = 4;
     public static final int DAG_START_TIME_FIELD_NUMBER = 4;
-    private long dagStartTime_;
+    private long dagStartTime_ = 0L;
     /**
      * <code>optional int64 dag_start_time = 4;</code>
+     * @return Whether the dagStartTime field is set.
      */
+    @java.lang.Override
     public boolean hasDagStartTime() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
+      return ((bitField0_ & 0x00000008) != 0);
     }
     /**
      * <code>optional int64 dag_start_time = 4;</code>
+     * @return The dagStartTime.
      */
+    @java.lang.Override
     public long getDagStartTime() {
       return dagStartTime_;
     }
 
-    // optional int64 first_attempt_start_time = 5;
     public static final int FIRST_ATTEMPT_START_TIME_FIELD_NUMBER = 5;
-    private long firstAttemptStartTime_;
+    private long firstAttemptStartTime_ = 0L;
     /**
      * <code>optional int64 first_attempt_start_time = 5;</code>
+     * @return Whether the firstAttemptStartTime field is set.
      */
+    @java.lang.Override
     public boolean hasFirstAttemptStartTime() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+      return ((bitField0_ & 0x00000010) != 0);
     }
     /**
      * <code>optional int64 first_attempt_start_time = 5;</code>
+     * @return The firstAttemptStartTime.
      */
+    @java.lang.Override
     public long getFirstAttemptStartTime() {
       return firstAttemptStartTime_;
     }
 
-    // optional int64 current_attempt_start_time = 6;
     public static final int CURRENT_ATTEMPT_START_TIME_FIELD_NUMBER = 6;
-    private long currentAttemptStartTime_;
+    private long currentAttemptStartTime_ = 0L;
     /**
      * <code>optional int64 current_attempt_start_time = 6;</code>
+     * @return Whether the currentAttemptStartTime field is set.
      */
+    @java.lang.Override
     public boolean hasCurrentAttemptStartTime() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+      return ((bitField0_ & 0x00000020) != 0);
     }
     /**
      * <code>optional int64 current_attempt_start_time = 6;</code>
+     * @return The currentAttemptStartTime.
      */
+    @java.lang.Override
     public long getCurrentAttemptStartTime() {
       return currentAttemptStartTime_;
     }
 
-    private void initFields() {
-      numSelfAndUpstreamTasks_ = 0;
-      numSelfAndUpstreamCompletedTasks_ = 0;
-      withinDagPriority_ = 0;
-      dagStartTime_ = 0L;
-      firstAttemptStartTime_ = 0L;
-      currentAttemptStartTime_ = 0L;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeInt32(1, numSelfAndUpstreamTasks_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeInt32(2, numSelfAndUpstreamCompletedTasks_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         output.writeInt32(3, withinDagPriority_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+      if (((bitField0_ & 0x00000008) != 0)) {
         output.writeInt64(4, dagStartTime_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+      if (((bitField0_ & 0x00000010) != 0)) {
         output.writeInt64(5, firstAttemptStartTime_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
+      if (((bitField0_ & 0x00000020) != 0)) {
         output.writeInt64(6, currentAttemptStartTime_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(1, numSelfAndUpstreamTasks_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(2, numSelfAndUpstreamCompletedTasks_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(3, withinDagPriority_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+      if (((bitField0_ & 0x00000008) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(4, dagStartTime_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+      if (((bitField0_ & 0x00000010) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(5, firstAttemptStartTime_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
+      if (((bitField0_ & 0x00000020) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(6, currentAttemptStartTime_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -7434,50 +8012,47 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo) obj;
 
-      boolean result = true;
-      result = result && (hasNumSelfAndUpstreamTasks() == other.hasNumSelfAndUpstreamTasks());
+      if (hasNumSelfAndUpstreamTasks() != other.hasNumSelfAndUpstreamTasks()) return false;
       if (hasNumSelfAndUpstreamTasks()) {
-        result = result && (getNumSelfAndUpstreamTasks()
-            == other.getNumSelfAndUpstreamTasks());
+        if (getNumSelfAndUpstreamTasks()
+            != other.getNumSelfAndUpstreamTasks()) return false;
       }
-      result = result && (hasNumSelfAndUpstreamCompletedTasks() == other.hasNumSelfAndUpstreamCompletedTasks());
+      if (hasNumSelfAndUpstreamCompletedTasks() != other.hasNumSelfAndUpstreamCompletedTasks()) return false;
       if (hasNumSelfAndUpstreamCompletedTasks()) {
-        result = result && (getNumSelfAndUpstreamCompletedTasks()
-            == other.getNumSelfAndUpstreamCompletedTasks());
+        if (getNumSelfAndUpstreamCompletedTasks()
+            != other.getNumSelfAndUpstreamCompletedTasks()) return false;
       }
-      result = result && (hasWithinDagPriority() == other.hasWithinDagPriority());
+      if (hasWithinDagPriority() != other.hasWithinDagPriority()) return false;
       if (hasWithinDagPriority()) {
-        result = result && (getWithinDagPriority()
-            == other.getWithinDagPriority());
+        if (getWithinDagPriority()
+            != other.getWithinDagPriority()) return false;
       }
-      result = result && (hasDagStartTime() == other.hasDagStartTime());
+      if (hasDagStartTime() != other.hasDagStartTime()) return false;
       if (hasDagStartTime()) {
-        result = result && (getDagStartTime()
-            == other.getDagStartTime());
+        if (getDagStartTime()
+            != other.getDagStartTime()) return false;
       }
-      result = result && (hasFirstAttemptStartTime() == other.hasFirstAttemptStartTime());
+      if (hasFirstAttemptStartTime() != other.hasFirstAttemptStartTime()) return false;
       if (hasFirstAttemptStartTime()) {
-        result = result && (getFirstAttemptStartTime()
-            == other.getFirstAttemptStartTime());
+        if (getFirstAttemptStartTime()
+            != other.getFirstAttemptStartTime()) return false;
       }
-      result = result && (hasCurrentAttemptStartTime() == other.hasCurrentAttemptStartTime());
+      if (hasCurrentAttemptStartTime() != other.hasCurrentAttemptStartTime()) return false;
       if (hasCurrentAttemptStartTime()) {
-        result = result && (getCurrentAttemptStartTime()
-            == other.getCurrentAttemptStartTime());
+        if (getCurrentAttemptStartTime()
+            != other.getCurrentAttemptStartTime()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasNumSelfAndUpstreamTasks()) {
         hash = (37 * hash) + NUM_SELF_AND_UPSTREAM_TASKS_FIELD_NUMBER;
         hash = (53 * hash) + getNumSelfAndUpstreamTasks();
@@ -7492,21 +8067,35 @@ public int hashCode() {
       }
       if (hasDagStartTime()) {
         hash = (37 * hash) + DAG_START_TIME_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getDagStartTime());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getDagStartTime());
       }
       if (hasFirstAttemptStartTime()) {
         hash = (37 * hash) + FIRST_ATTEMPT_START_TIME_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getFirstAttemptStartTime());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getFirstAttemptStartTime());
       }
       if (hasCurrentAttemptStartTime()) {
         hash = (37 * hash) + CURRENT_ATTEMPT_START_TIME_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getCurrentAttemptStartTime());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getCurrentAttemptStartTime());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -7530,46 +8119,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Fr
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -7577,14 +8181,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code FragmentRuntimeInfo}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:FragmentRuntimeInfo)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -7593,52 +8199,39 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         numSelfAndUpstreamTasks_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000001);
         numSelfAndUpstreamCompletedTasks_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
         withinDagPriority_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000004);
         dagStartTime_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000008);
         firstAttemptStartTime_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000010);
         currentAttemptStartTime_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000020);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo result = buildPartial();
         if (!result.isInitialized()) {
@@ -7647,39 +8240,77 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentR
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.numSelfAndUpstreamTasks_ = numSelfAndUpstreamTasks_;
           to_bitField0_ |= 0x00000001;
         }
-        result.numSelfAndUpstreamTasks_ = numSelfAndUpstreamTasks_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.numSelfAndUpstreamCompletedTasks_ = numSelfAndUpstreamCompletedTasks_;
           to_bitField0_ |= 0x00000002;
         }
-        result.numSelfAndUpstreamCompletedTasks_ = numSelfAndUpstreamCompletedTasks_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.withinDagPriority_ = withinDagPriority_;
           to_bitField0_ |= 0x00000004;
         }
-        result.withinDagPriority_ = withinDagPriority_;
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
+        if (((from_bitField0_ & 0x00000008) != 0)) {
+          result.dagStartTime_ = dagStartTime_;
           to_bitField0_ |= 0x00000008;
         }
-        result.dagStartTime_ = dagStartTime_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
+        if (((from_bitField0_ & 0x00000010) != 0)) {
+          result.firstAttemptStartTime_ = firstAttemptStartTime_;
           to_bitField0_ |= 0x00000010;
         }
-        result.firstAttemptStartTime_ = firstAttemptStartTime_;
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
+        if (((from_bitField0_ & 0x00000020) != 0)) {
+          result.currentAttemptStartTime_ = currentAttemptStartTime_;
           to_bitField0_ |= 0x00000020;
         }
-        result.currentAttemptStartTime_ = currentAttemptStartTime_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo)other);
@@ -7710,57 +8341,110 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setCurrentAttemptStartTime(other.getCurrentAttemptStartTime());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 8: {
+                numSelfAndUpstreamTasks_ = input.readInt32();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 8
+              case 16: {
+                numSelfAndUpstreamCompletedTasks_ = input.readInt32();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 16
+              case 24: {
+                withinDagPriority_ = input.readInt32();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 24
+              case 32: {
+                dagStartTime_ = input.readInt64();
+                bitField0_ |= 0x00000008;
+                break;
+              } // case 32
+              case 40: {
+                firstAttemptStartTime_ = input.readInt64();
+                bitField0_ |= 0x00000010;
+                break;
+              } // case 40
+              case 48: {
+                currentAttemptStartTime_ = input.readInt64();
+                bitField0_ |= 0x00000020;
+                break;
+              } // case 48
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional int32 num_self_and_upstream_tasks = 1;
       private int numSelfAndUpstreamTasks_ ;
       /**
        * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
+       * @return Whether the numSelfAndUpstreamTasks field is set.
        */
+      @java.lang.Override
       public boolean hasNumSelfAndUpstreamTasks() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
+       * @return The numSelfAndUpstreamTasks.
        */
+      @java.lang.Override
       public int getNumSelfAndUpstreamTasks() {
         return numSelfAndUpstreamTasks_;
       }
       /**
        * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
+       * @param value The numSelfAndUpstreamTasks to set.
+       * @return This builder for chaining.
        */
       public Builder setNumSelfAndUpstreamTasks(int value) {
-        bitField0_ |= 0x00000001;
+
         numSelfAndUpstreamTasks_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearNumSelfAndUpstreamTasks() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -7769,31 +8453,38 @@ public Builder clearNumSelfAndUpstreamTasks() {
         return this;
       }
 
-      // optional int32 num_self_and_upstream_completed_tasks = 2;
       private int numSelfAndUpstreamCompletedTasks_ ;
       /**
        * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
+       * @return Whether the numSelfAndUpstreamCompletedTasks field is set.
        */
+      @java.lang.Override
       public boolean hasNumSelfAndUpstreamCompletedTasks() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
+       * @return The numSelfAndUpstreamCompletedTasks.
        */
+      @java.lang.Override
       public int getNumSelfAndUpstreamCompletedTasks() {
         return numSelfAndUpstreamCompletedTasks_;
       }
       /**
        * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
+       * @param value The numSelfAndUpstreamCompletedTasks to set.
+       * @return This builder for chaining.
        */
       public Builder setNumSelfAndUpstreamCompletedTasks(int value) {
-        bitField0_ |= 0x00000002;
+
         numSelfAndUpstreamCompletedTasks_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearNumSelfAndUpstreamCompletedTasks() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -7802,31 +8493,38 @@ public Builder clearNumSelfAndUpstreamCompletedTasks() {
         return this;
       }
 
-      // optional int32 within_dag_priority = 3;
       private int withinDagPriority_ ;
       /**
        * <code>optional int32 within_dag_priority = 3;</code>
+       * @return Whether the withinDagPriority field is set.
        */
+      @java.lang.Override
       public boolean hasWithinDagPriority() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional int32 within_dag_priority = 3;</code>
+       * @return The withinDagPriority.
        */
+      @java.lang.Override
       public int getWithinDagPriority() {
         return withinDagPriority_;
       }
       /**
        * <code>optional int32 within_dag_priority = 3;</code>
+       * @param value The withinDagPriority to set.
+       * @return This builder for chaining.
        */
       public Builder setWithinDagPriority(int value) {
-        bitField0_ |= 0x00000004;
+
         withinDagPriority_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 within_dag_priority = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearWithinDagPriority() {
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -7835,31 +8533,38 @@ public Builder clearWithinDagPriority() {
         return this;
       }
 
-      // optional int64 dag_start_time = 4;
       private long dagStartTime_ ;
       /**
        * <code>optional int64 dag_start_time = 4;</code>
+       * @return Whether the dagStartTime field is set.
        */
+      @java.lang.Override
       public boolean hasDagStartTime() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
+        return ((bitField0_ & 0x00000008) != 0);
       }
       /**
        * <code>optional int64 dag_start_time = 4;</code>
+       * @return The dagStartTime.
        */
+      @java.lang.Override
       public long getDagStartTime() {
         return dagStartTime_;
       }
       /**
        * <code>optional int64 dag_start_time = 4;</code>
+       * @param value The dagStartTime to set.
+       * @return This builder for chaining.
        */
       public Builder setDagStartTime(long value) {
-        bitField0_ |= 0x00000008;
+
         dagStartTime_ = value;
+        bitField0_ |= 0x00000008;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 dag_start_time = 4;</code>
+       * @return This builder for chaining.
        */
       public Builder clearDagStartTime() {
         bitField0_ = (bitField0_ & ~0x00000008);
@@ -7868,31 +8573,38 @@ public Builder clearDagStartTime() {
         return this;
       }
 
-      // optional int64 first_attempt_start_time = 5;
       private long firstAttemptStartTime_ ;
       /**
        * <code>optional int64 first_attempt_start_time = 5;</code>
+       * @return Whether the firstAttemptStartTime field is set.
        */
+      @java.lang.Override
       public boolean hasFirstAttemptStartTime() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
+        return ((bitField0_ & 0x00000010) != 0);
       }
       /**
        * <code>optional int64 first_attempt_start_time = 5;</code>
+       * @return The firstAttemptStartTime.
        */
+      @java.lang.Override
       public long getFirstAttemptStartTime() {
         return firstAttemptStartTime_;
       }
       /**
        * <code>optional int64 first_attempt_start_time = 5;</code>
+       * @param value The firstAttemptStartTime to set.
+       * @return This builder for chaining.
        */
       public Builder setFirstAttemptStartTime(long value) {
-        bitField0_ |= 0x00000010;
+
         firstAttemptStartTime_ = value;
+        bitField0_ |= 0x00000010;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 first_attempt_start_time = 5;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFirstAttemptStartTime() {
         bitField0_ = (bitField0_ & ~0x00000010);
@@ -7901,31 +8613,38 @@ public Builder clearFirstAttemptStartTime() {
         return this;
       }
 
-      // optional int64 current_attempt_start_time = 6;
       private long currentAttemptStartTime_ ;
       /**
        * <code>optional int64 current_attempt_start_time = 6;</code>
+       * @return Whether the currentAttemptStartTime field is set.
        */
+      @java.lang.Override
       public boolean hasCurrentAttemptStartTime() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
+        return ((bitField0_ & 0x00000020) != 0);
       }
       /**
        * <code>optional int64 current_attempt_start_time = 6;</code>
+       * @return The currentAttemptStartTime.
        */
+      @java.lang.Override
       public long getCurrentAttemptStartTime() {
         return currentAttemptStartTime_;
       }
       /**
        * <code>optional int64 current_attempt_start_time = 6;</code>
+       * @param value The currentAttemptStartTime to set.
+       * @return This builder for chaining.
        */
       public Builder setCurrentAttemptStartTime(long value) {
-        bitField0_ |= 0x00000020;
+
         currentAttemptStartTime_ = value;
+        bitField0_ |= 0x00000020;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 current_attempt_start_time = 6;</code>
+       * @return This builder for chaining.
        */
       public Builder clearCurrentAttemptStartTime() {
         bitField0_ = (bitField0_ & ~0x00000020);
@@ -7933,53 +8652,110 @@ public Builder clearCurrentAttemptStartTime() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:FragmentRuntimeInfo)
     }
 
+    // @@protoc_insertion_point(class_scope:FragmentRuntimeInfo)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo DEFAULT_INSTANCE;
     static {
-      defaultInstance = new FragmentRuntimeInfo(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<FragmentRuntimeInfo>
+        PARSER = new com.google.protobuf.AbstractParser<FragmentRuntimeInfo>() {
+      @java.lang.Override
+      public FragmentRuntimeInfo parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<FragmentRuntimeInfo> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<FragmentRuntimeInfo> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:FragmentRuntimeInfo)
   }
 
-  public interface QueryIdentifierProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface QueryIdentifierProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:QueryIdentifierProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string application_id_string = 1;
     /**
      * <code>optional string application_id_string = 1;</code>
+     * @return Whether the applicationIdString field is set.
      */
     boolean hasApplicationIdString();
     /**
      * <code>optional string application_id_string = 1;</code>
+     * @return The applicationIdString.
      */
     java.lang.String getApplicationIdString();
     /**
      * <code>optional string application_id_string = 1;</code>
+     * @return The bytes for applicationIdString.
      */
     com.google.protobuf.ByteString
         getApplicationIdStringBytes();
 
-    // optional int32 dag_index = 2;
     /**
      * <code>optional int32 dag_index = 2;</code>
+     * @return Whether the dagIndex field is set.
      */
     boolean hasDagIndex();
     /**
      * <code>optional int32 dag_index = 2;</code>
+     * @return The dagIndex.
      */
     int getDagIndex();
 
-    // optional int32 app_attempt_number = 3;
     /**
      * <code>optional int32 app_attempt_number = 3;</code>
+     * @return Whether the appAttemptNumber field is set.
      */
     boolean hasAppAttemptNumber();
     /**
      * <code>optional int32 app_attempt_number = 3;</code>
+     * @return The appAttemptNumber.
      */
     int getAppAttemptNumber();
   }
@@ -7987,120 +8763,55 @@ public interface QueryIdentifierProtoOrBuilder
    * Protobuf type {@code QueryIdentifierProto}
    */
   public static final class QueryIdentifierProto extends
-      com.google.protobuf.GeneratedMessage
-      implements QueryIdentifierProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:QueryIdentifierProto)
+      QueryIdentifierProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use QueryIdentifierProto.newBuilder() to construct.
-    private QueryIdentifierProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private QueryIdentifierProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private QueryIdentifierProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final QueryIdentifierProto defaultInstance;
-    public static QueryIdentifierProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public QueryIdentifierProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private QueryIdentifierProto() {
+      applicationIdString_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private QueryIdentifierProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              applicationIdString_ = input.readBytes();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              dagIndex_ = input.readInt32();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              appAttemptNumber_ = input.readInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new QueryIdentifierProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<QueryIdentifierProto> PARSER =
-        new com.google.protobuf.AbstractParser<QueryIdentifierProto>() {
-      public QueryIdentifierProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new QueryIdentifierProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<QueryIdentifierProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string application_id_string = 1;
     public static final int APPLICATION_ID_STRING_FIELD_NUMBER = 1;
-    private java.lang.Object applicationIdString_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object applicationIdString_ = "";
     /**
      * <code>optional string application_id_string = 1;</code>
+     * @return Whether the applicationIdString field is set.
      */
+    @java.lang.Override
     public boolean hasApplicationIdString() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string application_id_string = 1;</code>
+     * @return The applicationIdString.
      */
+    @java.lang.Override
     public java.lang.String getApplicationIdString() {
       java.lang.Object ref = applicationIdString_;
       if (ref instanceof java.lang.String) {
@@ -8117,7 +8828,9 @@ public java.lang.String getApplicationIdString() {
     }
     /**
      * <code>optional string application_id_string = 1;</code>
+     * @return The bytes for applicationIdString.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getApplicationIdStringBytes() {
       java.lang.Object ref = applicationIdString_;
@@ -8132,97 +8845,92 @@ public java.lang.String getApplicationIdString() {
       }
     }
 
-    // optional int32 dag_index = 2;
     public static final int DAG_INDEX_FIELD_NUMBER = 2;
-    private int dagIndex_;
+    private int dagIndex_ = 0;
     /**
      * <code>optional int32 dag_index = 2;</code>
+     * @return Whether the dagIndex field is set.
      */
+    @java.lang.Override
     public boolean hasDagIndex() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional int32 dag_index = 2;</code>
+     * @return The dagIndex.
      */
+    @java.lang.Override
     public int getDagIndex() {
       return dagIndex_;
     }
 
-    // optional int32 app_attempt_number = 3;
     public static final int APP_ATTEMPT_NUMBER_FIELD_NUMBER = 3;
-    private int appAttemptNumber_;
+    private int appAttemptNumber_ = 0;
     /**
      * <code>optional int32 app_attempt_number = 3;</code>
+     * @return Whether the appAttemptNumber field is set.
      */
+    @java.lang.Override
     public boolean hasAppAttemptNumber() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional int32 app_attempt_number = 3;</code>
+     * @return The appAttemptNumber.
      */
+    @java.lang.Override
     public int getAppAttemptNumber() {
       return appAttemptNumber_;
     }
 
-    private void initFields() {
-      applicationIdString_ = "";
-      dagIndex_ = 0;
-      appAttemptNumber_ = 0;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getApplicationIdStringBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, applicationIdString_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeInt32(2, dagIndex_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         output.writeInt32(3, appAttemptNumber_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getApplicationIdStringBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, applicationIdString_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(2, dagIndex_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(3, appAttemptNumber_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -8233,35 +8941,32 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto) obj;
 
-      boolean result = true;
-      result = result && (hasApplicationIdString() == other.hasApplicationIdString());
+      if (hasApplicationIdString() != other.hasApplicationIdString()) return false;
       if (hasApplicationIdString()) {
-        result = result && getApplicationIdString()
-            .equals(other.getApplicationIdString());
+        if (!getApplicationIdString()
+            .equals(other.getApplicationIdString())) return false;
       }
-      result = result && (hasDagIndex() == other.hasDagIndex());
+      if (hasDagIndex() != other.hasDagIndex()) return false;
       if (hasDagIndex()) {
-        result = result && (getDagIndex()
-            == other.getDagIndex());
+        if (getDagIndex()
+            != other.getDagIndex()) return false;
       }
-      result = result && (hasAppAttemptNumber() == other.hasAppAttemptNumber());
+      if (hasAppAttemptNumber() != other.hasAppAttemptNumber()) return false;
       if (hasAppAttemptNumber()) {
-        result = result && (getAppAttemptNumber()
-            == other.getAppAttemptNumber());
+        if (getAppAttemptNumber()
+            != other.getAppAttemptNumber()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasApplicationIdString()) {
         hash = (37 * hash) + APPLICATION_ID_STRING_FIELD_NUMBER;
         hash = (53 * hash) + getApplicationIdString().hashCode();
@@ -8279,6 +8984,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -8302,46 +9018,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Qu
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -8349,14 +9080,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code QueryIdentifierProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:QueryIdentifierProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -8365,46 +9098,36 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         applicationIdString_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
         dagIndex_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
         appAttemptNumber_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -8413,27 +9136,65 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.applicationIdString_ = applicationIdString_;
           to_bitField0_ |= 0x00000001;
         }
-        result.applicationIdString_ = applicationIdString_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.dagIndex_ = dagIndex_;
           to_bitField0_ |= 0x00000002;
         }
-        result.dagIndex_ = dagIndex_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.appAttemptNumber_ = appAttemptNumber_;
           to_bitField0_ |= 0x00000004;
         }
-        result.appAttemptNumber_ = appAttemptNumber_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto)other);
@@ -8446,8 +9207,8 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) return this;
         if (other.hasApplicationIdString()) {
-          bitField0_ |= 0x00000001;
           applicationIdString_ = other.applicationIdString_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasDagIndex()) {
@@ -8457,49 +9218,84 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setAppAttemptNumber(other.getAppAttemptNumber());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                applicationIdString_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 16: {
+                dagIndex_ = input.readInt32();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 16
+              case 24: {
+                appAttemptNumber_ = input.readInt32();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 24
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string application_id_string = 1;
       private java.lang.Object applicationIdString_ = "";
       /**
        * <code>optional string application_id_string = 1;</code>
+       * @return Whether the applicationIdString field is set.
        */
       public boolean hasApplicationIdString() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string application_id_string = 1;</code>
+       * @return The applicationIdString.
        */
       public java.lang.String getApplicationIdString() {
         java.lang.Object ref = applicationIdString_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          applicationIdString_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            applicationIdString_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -8507,6 +9303,7 @@ public java.lang.String getApplicationIdString() {
       }
       /**
        * <code>optional string application_id_string = 1;</code>
+       * @return The bytes for applicationIdString.
        */
       public com.google.protobuf.ByteString
           getApplicationIdStringBytes() {
@@ -8523,65 +9320,73 @@ public java.lang.String getApplicationIdString() {
       }
       /**
        * <code>optional string application_id_string = 1;</code>
+       * @param value The applicationIdString to set.
+       * @return This builder for chaining.
        */
       public Builder setApplicationIdString(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         applicationIdString_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string application_id_string = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearApplicationIdString() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         applicationIdString_ = getDefaultInstance().getApplicationIdString();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string application_id_string = 1;</code>
+       * @param value The bytes for applicationIdString to set.
+       * @return This builder for chaining.
        */
       public Builder setApplicationIdStringBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         applicationIdString_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // optional int32 dag_index = 2;
       private int dagIndex_ ;
       /**
        * <code>optional int32 dag_index = 2;</code>
+       * @return Whether the dagIndex field is set.
        */
+      @java.lang.Override
       public boolean hasDagIndex() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional int32 dag_index = 2;</code>
+       * @return The dagIndex.
        */
+      @java.lang.Override
       public int getDagIndex() {
         return dagIndex_;
       }
       /**
        * <code>optional int32 dag_index = 2;</code>
+       * @param value The dagIndex to set.
+       * @return This builder for chaining.
        */
       public Builder setDagIndex(int value) {
-        bitField0_ |= 0x00000002;
+
         dagIndex_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 dag_index = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearDagIndex() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -8590,31 +9395,38 @@ public Builder clearDagIndex() {
         return this;
       }
 
-      // optional int32 app_attempt_number = 3;
       private int appAttemptNumber_ ;
       /**
        * <code>optional int32 app_attempt_number = 3;</code>
+       * @return Whether the appAttemptNumber field is set.
        */
+      @java.lang.Override
       public boolean hasAppAttemptNumber() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional int32 app_attempt_number = 3;</code>
+       * @return The appAttemptNumber.
        */
+      @java.lang.Override
       public int getAppAttemptNumber() {
         return appAttemptNumber_;
       }
       /**
        * <code>optional int32 app_attempt_number = 3;</code>
+       * @param value The appAttemptNumber to set.
+       * @return This builder for chaining.
        */
       public Builder setAppAttemptNumber(int value) {
-        bitField0_ |= 0x00000004;
+
         appAttemptNumber_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 app_attempt_number = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearAppAttemptNumber() {
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -8622,74 +9434,131 @@ public Builder clearAppAttemptNumber() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:QueryIdentifierProto)
     }
 
+    // @@protoc_insertion_point(class_scope:QueryIdentifierProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new QueryIdentifierProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<QueryIdentifierProto>
+        PARSER = new com.google.protobuf.AbstractParser<QueryIdentifierProto>() {
+      @java.lang.Override
+      public QueryIdentifierProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<QueryIdentifierProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<QueryIdentifierProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:QueryIdentifierProto)
   }
 
-  public interface NotTezEventOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface NotTezEventOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:NotTezEvent)
+      com.google.protobuf.MessageOrBuilder {
 
-    // required bytes input_event_proto_bytes = 1;
     /**
      * <code>required bytes input_event_proto_bytes = 1;</code>
+     * @return Whether the inputEventProtoBytes field is set.
      */
     boolean hasInputEventProtoBytes();
     /**
      * <code>required bytes input_event_proto_bytes = 1;</code>
+     * @return The inputEventProtoBytes.
      */
     com.google.protobuf.ByteString getInputEventProtoBytes();
 
-    // required string vertex_name = 2;
     /**
      * <code>required string vertex_name = 2;</code>
+     * @return Whether the vertexName field is set.
      */
     boolean hasVertexName();
     /**
      * <code>required string vertex_name = 2;</code>
+     * @return The vertexName.
      */
     java.lang.String getVertexName();
     /**
      * <code>required string vertex_name = 2;</code>
+     * @return The bytes for vertexName.
      */
     com.google.protobuf.ByteString
         getVertexNameBytes();
 
-    // required string dest_input_name = 3;
     /**
      * <code>required string dest_input_name = 3;</code>
+     * @return Whether the destInputName field is set.
      */
     boolean hasDestInputName();
     /**
      * <code>required string dest_input_name = 3;</code>
+     * @return The destInputName.
      */
     java.lang.String getDestInputName();
     /**
      * <code>required string dest_input_name = 3;</code>
+     * @return The bytes for destInputName.
      */
     com.google.protobuf.ByteString
         getDestInputNameBytes();
 
-    // optional int32 key_id = 4;
     /**
      * <code>optional int32 key_id = 4;</code>
+     * @return Whether the keyId field is set.
      */
     boolean hasKeyId();
     /**
      * <code>optional int32 key_id = 4;</code>
+     * @return The keyId.
      */
     int getKeyId();
   }
   /**
-   * Protobuf type {@code NotTezEvent}
-   *
    * <pre>
    **
    * Tez API implementation derives an enum value from instanceof on the event, then uses that enum
@@ -8698,143 +9567,80 @@ public interface NotTezEventOrBuilder
    * three times over to add anything there. So, we'd do our own "inspired" serialization.
    * Eventually we'll move away from events for API.
    * </pre>
+   *
+   * Protobuf type {@code NotTezEvent}
    */
   public static final class NotTezEvent extends
-      com.google.protobuf.GeneratedMessage
-      implements NotTezEventOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:NotTezEvent)
+      NotTezEventOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use NotTezEvent.newBuilder() to construct.
-    private NotTezEvent(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private NotTezEvent(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private NotTezEvent(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final NotTezEvent defaultInstance;
-    public static NotTezEvent getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public NotTezEvent getDefaultInstanceForType() {
-      return defaultInstance;
+    private NotTezEvent() {
+      inputEventProtoBytes_ = com.google.protobuf.ByteString.EMPTY;
+      vertexName_ = "";
+      destInputName_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private NotTezEvent(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              inputEventProtoBytes_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              vertexName_ = input.readBytes();
-              break;
-            }
-            case 26: {
-              bitField0_ |= 0x00000004;
-              destInputName_ = input.readBytes();
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              keyId_ = input.readInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new NotTezEvent();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<NotTezEvent> PARSER =
-        new com.google.protobuf.AbstractParser<NotTezEvent>() {
-      public NotTezEvent parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new NotTezEvent(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<NotTezEvent> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // required bytes input_event_proto_bytes = 1;
     public static final int INPUT_EVENT_PROTO_BYTES_FIELD_NUMBER = 1;
-    private com.google.protobuf.ByteString inputEventProtoBytes_;
+    private com.google.protobuf.ByteString inputEventProtoBytes_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>required bytes input_event_proto_bytes = 1;</code>
+     * @return Whether the inputEventProtoBytes field is set.
      */
+    @java.lang.Override
     public boolean hasInputEventProtoBytes() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>required bytes input_event_proto_bytes = 1;</code>
+     * @return The inputEventProtoBytes.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getInputEventProtoBytes() {
       return inputEventProtoBytes_;
     }
 
-    // required string vertex_name = 2;
     public static final int VERTEX_NAME_FIELD_NUMBER = 2;
-    private java.lang.Object vertexName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object vertexName_ = "";
     /**
      * <code>required string vertex_name = 2;</code>
+     * @return Whether the vertexName field is set.
      */
+    @java.lang.Override
     public boolean hasVertexName() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>required string vertex_name = 2;</code>
+     * @return The vertexName.
      */
+    @java.lang.Override
     public java.lang.String getVertexName() {
       java.lang.Object ref = vertexName_;
       if (ref instanceof java.lang.String) {
@@ -8851,7 +9657,9 @@ public java.lang.String getVertexName() {
     }
     /**
      * <code>required string vertex_name = 2;</code>
+     * @return The bytes for vertexName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getVertexNameBytes() {
       java.lang.Object ref = vertexName_;
@@ -8866,18 +9674,22 @@ public java.lang.String getVertexName() {
       }
     }
 
-    // required string dest_input_name = 3;
     public static final int DEST_INPUT_NAME_FIELD_NUMBER = 3;
-    private java.lang.Object destInputName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object destInputName_ = "";
     /**
      * <code>required string dest_input_name = 3;</code>
+     * @return Whether the destInputName field is set.
      */
+    @java.lang.Override
     public boolean hasDestInputName() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>required string dest_input_name = 3;</code>
+     * @return The destInputName.
      */
+    @java.lang.Override
     public java.lang.String getDestInputName() {
       java.lang.Object ref = destInputName_;
       if (ref instanceof java.lang.String) {
@@ -8894,7 +9706,9 @@ public java.lang.String getDestInputName() {
     }
     /**
      * <code>required string dest_input_name = 3;</code>
+     * @return The bytes for destInputName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getDestInputNameBytes() {
       java.lang.Object ref = destInputName_;
@@ -8909,32 +9723,31 @@ public java.lang.String getDestInputName() {
       }
     }
 
-    // optional int32 key_id = 4;
     public static final int KEY_ID_FIELD_NUMBER = 4;
-    private int keyId_;
+    private int keyId_ = 0;
     /**
      * <code>optional int32 key_id = 4;</code>
+     * @return Whether the keyId field is set.
      */
+    @java.lang.Override
     public boolean hasKeyId() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
+      return ((bitField0_ & 0x00000008) != 0);
     }
     /**
      * <code>optional int32 key_id = 4;</code>
+     * @return The keyId.
      */
+    @java.lang.Override
     public int getKeyId() {
       return keyId_;
     }
 
-    private void initFields() {
-      inputEventProtoBytes_ = com.google.protobuf.ByteString.EMPTY;
-      vertexName_ = "";
-      destInputName_ = "";
-      keyId_ = 0;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       if (!hasInputEventProtoBytes()) {
         memoizedIsInitialized = 0;
@@ -8952,58 +9765,49 @@ public final boolean isInitialized() {
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeBytes(1, inputEventProtoBytes_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getVertexNameBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, vertexName_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeBytes(3, getDestInputNameBytes());
+      if (((bitField0_ & 0x00000004) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, destInputName_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+      if (((bitField0_ & 0x00000008) != 0)) {
         output.writeInt32(4, keyId_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(1, inputEventProtoBytes_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getVertexNameBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, vertexName_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(3, getDestInputNameBytes());
+      if (((bitField0_ & 0x00000004) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, destInputName_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+      if (((bitField0_ & 0x00000008) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(4, keyId_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -9014,40 +9818,37 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent) obj;
 
-      boolean result = true;
-      result = result && (hasInputEventProtoBytes() == other.hasInputEventProtoBytes());
+      if (hasInputEventProtoBytes() != other.hasInputEventProtoBytes()) return false;
       if (hasInputEventProtoBytes()) {
-        result = result && getInputEventProtoBytes()
-            .equals(other.getInputEventProtoBytes());
+        if (!getInputEventProtoBytes()
+            .equals(other.getInputEventProtoBytes())) return false;
       }
-      result = result && (hasVertexName() == other.hasVertexName());
+      if (hasVertexName() != other.hasVertexName()) return false;
       if (hasVertexName()) {
-        result = result && getVertexName()
-            .equals(other.getVertexName());
+        if (!getVertexName()
+            .equals(other.getVertexName())) return false;
       }
-      result = result && (hasDestInputName() == other.hasDestInputName());
+      if (hasDestInputName() != other.hasDestInputName()) return false;
       if (hasDestInputName()) {
-        result = result && getDestInputName()
-            .equals(other.getDestInputName());
+        if (!getDestInputName()
+            .equals(other.getDestInputName())) return false;
       }
-      result = result && (hasKeyId() == other.hasKeyId());
+      if (hasKeyId() != other.hasKeyId()) return false;
       if (hasKeyId()) {
-        result = result && (getKeyId()
-            == other.getKeyId());
+        if (getKeyId()
+            != other.getKeyId()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasInputEventProtoBytes()) {
         hash = (37 * hash) + INPUT_EVENT_PROTO_BYTES_FIELD_NUMBER;
         hash = (53 * hash) + getInputEventProtoBytes().hashCode();
@@ -9069,6 +9870,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -9092,52 +9904,65 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.No
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
     /**
-     * Protobuf type {@code NotTezEvent}
-     *
      * <pre>
      **
      * Tez API implementation derives an enum value from instanceof on the event, then uses that enum
@@ -9146,16 +9971,20 @@ protected Builder newBuilderForType(
      * three times over to add anything there. So, we'd do our own "inspired" serialization.
      * Eventually we'll move away from events for API.
      * </pre>
+     *
+     * Protobuf type {@code NotTezEvent}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEventOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:NotTezEvent)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEventOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -9164,48 +9993,37 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         inputEventProtoBytes_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000001);
         vertexName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
         destInputName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000004);
         keyId_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000008);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent result = buildPartial();
         if (!result.isInitialized()) {
@@ -9214,31 +10032,69 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEve
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.inputEventProtoBytes_ = inputEventProtoBytes_;
           to_bitField0_ |= 0x00000001;
         }
-        result.inputEventProtoBytes_ = inputEventProtoBytes_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.vertexName_ = vertexName_;
           to_bitField0_ |= 0x00000002;
         }
-        result.vertexName_ = vertexName_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.destInputName_ = destInputName_;
           to_bitField0_ |= 0x00000004;
         }
-        result.destInputName_ = destInputName_;
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
+        if (((from_bitField0_ & 0x00000008) != 0)) {
+          result.keyId_ = keyId_;
           to_bitField0_ |= 0x00000008;
         }
-        result.keyId_ = keyId_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent)other);
@@ -9254,85 +10110,122 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setInputEventProtoBytes(other.getInputEventProtoBytes());
         }
         if (other.hasVertexName()) {
-          bitField0_ |= 0x00000002;
           vertexName_ = other.vertexName_;
+          bitField0_ |= 0x00000002;
           onChanged();
         }
         if (other.hasDestInputName()) {
-          bitField0_ |= 0x00000004;
           destInputName_ = other.destInputName_;
+          bitField0_ |= 0x00000004;
           onChanged();
         }
         if (other.hasKeyId()) {
           setKeyId(other.getKeyId());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         if (!hasInputEventProtoBytes()) {
-          
           return false;
         }
         if (!hasVertexName()) {
-          
           return false;
         }
         if (!hasDestInputName()) {
-          
           return false;
         }
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                inputEventProtoBytes_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                vertexName_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 26: {
+                destInputName_ = input.readBytes();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 26
+              case 32: {
+                keyId_ = input.readInt32();
+                bitField0_ |= 0x00000008;
+                break;
+              } // case 32
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // required bytes input_event_proto_bytes = 1;
       private com.google.protobuf.ByteString inputEventProtoBytes_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>required bytes input_event_proto_bytes = 1;</code>
+       * @return Whether the inputEventProtoBytes field is set.
        */
+      @java.lang.Override
       public boolean hasInputEventProtoBytes() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>required bytes input_event_proto_bytes = 1;</code>
+       * @return The inputEventProtoBytes.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getInputEventProtoBytes() {
         return inputEventProtoBytes_;
       }
       /**
        * <code>required bytes input_event_proto_bytes = 1;</code>
+       * @param value The inputEventProtoBytes to set.
+       * @return This builder for chaining.
        */
       public Builder setInputEventProtoBytes(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         inputEventProtoBytes_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>required bytes input_event_proto_bytes = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearInputEventProtoBytes() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -9341,23 +10234,27 @@ public Builder clearInputEventProtoBytes() {
         return this;
       }
 
-      // required string vertex_name = 2;
       private java.lang.Object vertexName_ = "";
       /**
        * <code>required string vertex_name = 2;</code>
+       * @return Whether the vertexName field is set.
        */
       public boolean hasVertexName() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>required string vertex_name = 2;</code>
+       * @return The vertexName.
        */
       public java.lang.String getVertexName() {
         java.lang.Object ref = vertexName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          vertexName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            vertexName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -9365,6 +10262,7 @@ public java.lang.String getVertexName() {
       }
       /**
        * <code>required string vertex_name = 2;</code>
+       * @return The bytes for vertexName.
        */
       public com.google.protobuf.ByteString
           getVertexNameBytes() {
@@ -9381,57 +10279,62 @@ public java.lang.String getVertexName() {
       }
       /**
        * <code>required string vertex_name = 2;</code>
+       * @param value The vertexName to set.
+       * @return This builder for chaining.
        */
       public Builder setVertexName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         vertexName_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>required string vertex_name = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearVertexName() {
-        bitField0_ = (bitField0_ & ~0x00000002);
         vertexName_ = getDefaultInstance().getVertexName();
+        bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
       /**
        * <code>required string vertex_name = 2;</code>
+       * @param value The bytes for vertexName to set.
+       * @return This builder for chaining.
        */
       public Builder setVertexNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         vertexName_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
 
-      // required string dest_input_name = 3;
       private java.lang.Object destInputName_ = "";
       /**
        * <code>required string dest_input_name = 3;</code>
+       * @return Whether the destInputName field is set.
        */
       public boolean hasDestInputName() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>required string dest_input_name = 3;</code>
+       * @return The destInputName.
        */
       public java.lang.String getDestInputName() {
         java.lang.Object ref = destInputName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          destInputName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            destInputName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -9439,6 +10342,7 @@ public java.lang.String getDestInputName() {
       }
       /**
        * <code>required string dest_input_name = 3;</code>
+       * @return The bytes for destInputName.
        */
       public com.google.protobuf.ByteString
           getDestInputNameBytes() {
@@ -9455,65 +10359,73 @@ public java.lang.String getDestInputName() {
       }
       /**
        * <code>required string dest_input_name = 3;</code>
+       * @param value The destInputName to set.
+       * @return This builder for chaining.
        */
       public Builder setDestInputName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000004;
+        if (value == null) { throw new NullPointerException(); }
         destInputName_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>required string dest_input_name = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearDestInputName() {
-        bitField0_ = (bitField0_ & ~0x00000004);
         destInputName_ = getDefaultInstance().getDestInputName();
+        bitField0_ = (bitField0_ & ~0x00000004);
         onChanged();
         return this;
       }
       /**
        * <code>required string dest_input_name = 3;</code>
+       * @param value The bytes for destInputName to set.
+       * @return This builder for chaining.
        */
       public Builder setDestInputNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000004;
+        if (value == null) { throw new NullPointerException(); }
         destInputName_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
 
-      // optional int32 key_id = 4;
       private int keyId_ ;
       /**
        * <code>optional int32 key_id = 4;</code>
+       * @return Whether the keyId field is set.
        */
+      @java.lang.Override
       public boolean hasKeyId() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
+        return ((bitField0_ & 0x00000008) != 0);
       }
       /**
        * <code>optional int32 key_id = 4;</code>
+       * @return The keyId.
        */
+      @java.lang.Override
       public int getKeyId() {
         return keyId_;
       }
       /**
        * <code>optional int32 key_id = 4;</code>
+       * @param value The keyId to set.
+       * @return This builder for chaining.
        */
       public Builder setKeyId(int value) {
-        bitField0_ |= 0x00000008;
+
         keyId_ = value;
+        bitField0_ |= 0x00000008;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 key_id = 4;</code>
+       * @return This builder for chaining.
        */
       public Builder clearKeyId() {
         bitField0_ = (bitField0_ & ~0x00000008);
@@ -9521,28 +10433,82 @@ public Builder clearKeyId() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:NotTezEvent)
     }
 
+    // @@protoc_insertion_point(class_scope:NotTezEvent)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent DEFAULT_INSTANCE;
     static {
-      defaultInstance = new NotTezEvent(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<NotTezEvent>
+        PARSER = new com.google.protobuf.AbstractParser<NotTezEvent>() {
+      @java.lang.Override
+      public NotTezEvent parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<NotTezEvent> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<NotTezEvent> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:NotTezEvent)
   }
 
-  public interface SubmitWorkRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface SubmitWorkRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:SubmitWorkRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional .VertexOrBinary work_spec = 1;
     /**
      * <code>optional .VertexOrBinary work_spec = 1;</code>
+     * @return Whether the workSpec field is set.
      */
     boolean hasWorkSpec();
     /**
      * <code>optional .VertexOrBinary work_spec = 1;</code>
+     * @return The workSpec.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getWorkSpec();
     /**
@@ -9550,180 +10516,196 @@ public interface SubmitWorkRequestProtoOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder getWorkSpecOrBuilder();
 
-    // optional bytes work_spec_signature = 2;
     /**
      * <code>optional bytes work_spec_signature = 2;</code>
+     * @return Whether the workSpecSignature field is set.
      */
     boolean hasWorkSpecSignature();
     /**
      * <code>optional bytes work_spec_signature = 2;</code>
+     * @return The workSpecSignature.
      */
     com.google.protobuf.ByteString getWorkSpecSignature();
 
-    // optional int32 fragment_number = 3;
     /**
      * <code>optional int32 fragment_number = 3;</code>
+     * @return Whether the fragmentNumber field is set.
      */
     boolean hasFragmentNumber();
     /**
      * <code>optional int32 fragment_number = 3;</code>
+     * @return The fragmentNumber.
      */
     int getFragmentNumber();
 
-    // optional int32 attempt_number = 4;
     /**
      * <code>optional int32 attempt_number = 4;</code>
+     * @return Whether the attemptNumber field is set.
      */
     boolean hasAttemptNumber();
     /**
      * <code>optional int32 attempt_number = 4;</code>
+     * @return The attemptNumber.
      */
     int getAttemptNumber();
 
-    // optional string container_id_string = 5;
     /**
      * <code>optional string container_id_string = 5;</code>
+     * @return Whether the containerIdString field is set.
      */
     boolean hasContainerIdString();
     /**
      * <code>optional string container_id_string = 5;</code>
+     * @return The containerIdString.
      */
     java.lang.String getContainerIdString();
     /**
      * <code>optional string container_id_string = 5;</code>
+     * @return The bytes for containerIdString.
      */
     com.google.protobuf.ByteString
         getContainerIdStringBytes();
 
-    // optional string am_host = 6;
     /**
      * <code>optional string am_host = 6;</code>
+     * @return Whether the amHost field is set.
      */
     boolean hasAmHost();
     /**
      * <code>optional string am_host = 6;</code>
+     * @return The amHost.
      */
     java.lang.String getAmHost();
     /**
      * <code>optional string am_host = 6;</code>
+     * @return The bytes for amHost.
      */
     com.google.protobuf.ByteString
         getAmHostBytes();
 
-    // optional int32 am_port = 7;
     /**
      * <code>optional int32 am_port = 7;</code>
+     * @return Whether the amPort field is set.
      */
     boolean hasAmPort();
     /**
      * <code>optional int32 am_port = 7;</code>
+     * @return The amPort.
      */
     int getAmPort();
 
-    // optional bytes credentials_binary = 8;
     /**
-     * <code>optional bytes credentials_binary = 8;</code>
-     *
      * <pre>
      * Credentials are not signed - the client can add e.g. his own HDFS tokens.
      * </pre>
+     *
+     * <code>optional bytes credentials_binary = 8;</code>
+     * @return Whether the credentialsBinary field is set.
      */
     boolean hasCredentialsBinary();
     /**
-     * <code>optional bytes credentials_binary = 8;</code>
-     *
      * <pre>
      * Credentials are not signed - the client can add e.g. his own HDFS tokens.
      * </pre>
+     *
+     * <code>optional bytes credentials_binary = 8;</code>
+     * @return The credentialsBinary.
      */
     com.google.protobuf.ByteString getCredentialsBinary();
 
-    // optional .FragmentRuntimeInfo fragment_runtime_info = 9;
     /**
-     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-     *
      * <pre>
      * Not supported/honored for external clients right now.
      * </pre>
+     *
+     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
+     * @return Whether the fragmentRuntimeInfo field is set.
      */
     boolean hasFragmentRuntimeInfo();
     /**
-     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-     *
      * <pre>
      * Not supported/honored for external clients right now.
      * </pre>
+     *
+     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
+     * @return The fragmentRuntimeInfo.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getFragmentRuntimeInfo();
     /**
-     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-     *
      * <pre>
      * Not supported/honored for external clients right now.
      * </pre>
+     *
+     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder getFragmentRuntimeInfoOrBuilder();
 
-    // optional bytes initial_event_bytes = 10;
     /**
-     * <code>optional bytes initial_event_bytes = 10;</code>
-     *
      * <pre>
      * Serialized (and signed) NotTezEvent; used only for external clients for now.
      * </pre>
+     *
+     * <code>optional bytes initial_event_bytes = 10;</code>
+     * @return Whether the initialEventBytes field is set.
      */
     boolean hasInitialEventBytes();
     /**
-     * <code>optional bytes initial_event_bytes = 10;</code>
-     *
      * <pre>
      * Serialized (and signed) NotTezEvent; used only for external clients for now.
      * </pre>
+     *
+     * <code>optional bytes initial_event_bytes = 10;</code>
+     * @return The initialEventBytes.
      */
     com.google.protobuf.ByteString getInitialEventBytes();
 
-    // optional bytes initial_event_signature = 11;
     /**
      * <code>optional bytes initial_event_signature = 11;</code>
+     * @return Whether the initialEventSignature field is set.
      */
     boolean hasInitialEventSignature();
     /**
      * <code>optional bytes initial_event_signature = 11;</code>
+     * @return The initialEventSignature.
      */
     com.google.protobuf.ByteString getInitialEventSignature();
 
-    // optional bool is_guaranteed = 12 [default = false];
     /**
      * <code>optional bool is_guaranteed = 12 [default = false];</code>
+     * @return Whether the isGuaranteed field is set.
      */
     boolean hasIsGuaranteed();
     /**
      * <code>optional bool is_guaranteed = 12 [default = false];</code>
+     * @return The isGuaranteed.
      */
     boolean getIsGuaranteed();
 
-    // optional string jwt = 13;
     /**
      * <code>optional string jwt = 13;</code>
+     * @return Whether the jwt field is set.
      */
     boolean hasJwt();
     /**
      * <code>optional string jwt = 13;</code>
+     * @return The jwt.
      */
     java.lang.String getJwt();
     /**
      * <code>optional string jwt = 13;</code>
+     * @return The bytes for jwt.
      */
     com.google.protobuf.ByteString
         getJwtBytes();
 
-    // optional bool is_external_client_request = 14 [default = false];
     /**
      * <code>optional bool is_external_client_request = 14 [default = false];</code>
+     * @return Whether the isExternalClientRequest field is set.
      */
     boolean hasIsExternalClientRequest();
     /**
      * <code>optional bool is_external_client_request = 14 [default = false];</code>
+     * @return The isExternalClientRequest.
      */
     boolean getIsExternalClientRequest();
   }
@@ -9731,261 +10713,144 @@ public interface SubmitWorkRequestProtoOrBuilder
    * Protobuf type {@code SubmitWorkRequestProto}
    */
   public static final class SubmitWorkRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements SubmitWorkRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:SubmitWorkRequestProto)
+      SubmitWorkRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use SubmitWorkRequestProto.newBuilder() to construct.
-    private SubmitWorkRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private SubmitWorkRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private SubmitWorkRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final SubmitWorkRequestProto defaultInstance;
-    public static SubmitWorkRequestProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public SubmitWorkRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
     }
-    private SubmitWorkRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = workSpec_.toBuilder();
-              }
-              workSpec_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(workSpec_);
-                workSpec_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              workSpecSignature_ = input.readBytes();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              fragmentNumber_ = input.readInt32();
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              attemptNumber_ = input.readInt32();
-              break;
-            }
-            case 42: {
-              bitField0_ |= 0x00000010;
-              containerIdString_ = input.readBytes();
-              break;
-            }
-            case 50: {
-              bitField0_ |= 0x00000020;
-              amHost_ = input.readBytes();
-              break;
-            }
-            case 56: {
-              bitField0_ |= 0x00000040;
-              amPort_ = input.readInt32();
-              break;
-            }
-            case 66: {
-              bitField0_ |= 0x00000080;
-              credentialsBinary_ = input.readBytes();
-              break;
-            }
-            case 74: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000100) == 0x00000100)) {
-                subBuilder = fragmentRuntimeInfo_.toBuilder();
-              }
-              fragmentRuntimeInfo_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(fragmentRuntimeInfo_);
-                fragmentRuntimeInfo_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000100;
-              break;
-            }
-            case 82: {
-              bitField0_ |= 0x00000200;
-              initialEventBytes_ = input.readBytes();
-              break;
-            }
-            case 90: {
-              bitField0_ |= 0x00000400;
-              initialEventSignature_ = input.readBytes();
-              break;
-            }
-            case 96: {
-              bitField0_ |= 0x00000800;
-              isGuaranteed_ = input.readBool();
-              break;
-            }
-            case 106: {
-              bitField0_ |= 0x00001000;
-              jwt_ = input.readBytes();
-              break;
-            }
-            case 112: {
-              bitField0_ |= 0x00002000;
-              isExternalClientRequest_ = input.readBool();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    private SubmitWorkRequestProto() {
+      workSpecSignature_ = com.google.protobuf.ByteString.EMPTY;
+      containerIdString_ = "";
+      amHost_ = "";
+      credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
+      initialEventBytes_ = com.google.protobuf.ByteString.EMPTY;
+      initialEventSignature_ = com.google.protobuf.ByteString.EMPTY;
+      jwt_ = "";
+    }
+
+    @java.lang.Override
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new SubmitWorkRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<SubmitWorkRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<SubmitWorkRequestProto>() {
-      public SubmitWorkRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new SubmitWorkRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<SubmitWorkRequestProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional .VertexOrBinary work_spec = 1;
     public static final int WORK_SPEC_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary workSpec_;
     /**
      * <code>optional .VertexOrBinary work_spec = 1;</code>
+     * @return Whether the workSpec field is set.
      */
+    @java.lang.Override
     public boolean hasWorkSpec() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional .VertexOrBinary work_spec = 1;</code>
+     * @return The workSpec.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getWorkSpec() {
-      return workSpec_;
+      return workSpec_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance() : workSpec_;
     }
     /**
      * <code>optional .VertexOrBinary work_spec = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder getWorkSpecOrBuilder() {
-      return workSpec_;
+      return workSpec_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance() : workSpec_;
     }
 
-    // optional bytes work_spec_signature = 2;
     public static final int WORK_SPEC_SIGNATURE_FIELD_NUMBER = 2;
-    private com.google.protobuf.ByteString workSpecSignature_;
+    private com.google.protobuf.ByteString workSpecSignature_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>optional bytes work_spec_signature = 2;</code>
+     * @return Whether the workSpecSignature field is set.
      */
+    @java.lang.Override
     public boolean hasWorkSpecSignature() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional bytes work_spec_signature = 2;</code>
+     * @return The workSpecSignature.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getWorkSpecSignature() {
       return workSpecSignature_;
     }
 
-    // optional int32 fragment_number = 3;
     public static final int FRAGMENT_NUMBER_FIELD_NUMBER = 3;
-    private int fragmentNumber_;
+    private int fragmentNumber_ = 0;
     /**
      * <code>optional int32 fragment_number = 3;</code>
+     * @return Whether the fragmentNumber field is set.
      */
+    @java.lang.Override
     public boolean hasFragmentNumber() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional int32 fragment_number = 3;</code>
+     * @return The fragmentNumber.
      */
+    @java.lang.Override
     public int getFragmentNumber() {
       return fragmentNumber_;
     }
 
-    // optional int32 attempt_number = 4;
     public static final int ATTEMPT_NUMBER_FIELD_NUMBER = 4;
-    private int attemptNumber_;
+    private int attemptNumber_ = 0;
     /**
      * <code>optional int32 attempt_number = 4;</code>
+     * @return Whether the attemptNumber field is set.
      */
+    @java.lang.Override
     public boolean hasAttemptNumber() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
+      return ((bitField0_ & 0x00000008) != 0);
     }
     /**
      * <code>optional int32 attempt_number = 4;</code>
+     * @return The attemptNumber.
      */
+    @java.lang.Override
     public int getAttemptNumber() {
       return attemptNumber_;
     }
 
-    // optional string container_id_string = 5;
     public static final int CONTAINER_ID_STRING_FIELD_NUMBER = 5;
-    private java.lang.Object containerIdString_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object containerIdString_ = "";
     /**
      * <code>optional string container_id_string = 5;</code>
+     * @return Whether the containerIdString field is set.
      */
+    @java.lang.Override
     public boolean hasContainerIdString() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+      return ((bitField0_ & 0x00000010) != 0);
     }
     /**
      * <code>optional string container_id_string = 5;</code>
+     * @return The containerIdString.
      */
+    @java.lang.Override
     public java.lang.String getContainerIdString() {
       java.lang.Object ref = containerIdString_;
       if (ref instanceof java.lang.String) {
@@ -10002,7 +10867,9 @@ public java.lang.String getContainerIdString() {
     }
     /**
      * <code>optional string container_id_string = 5;</code>
+     * @return The bytes for containerIdString.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getContainerIdStringBytes() {
       java.lang.Object ref = containerIdString_;
@@ -10017,18 +10884,22 @@ public java.lang.String getContainerIdString() {
       }
     }
 
-    // optional string am_host = 6;
     public static final int AM_HOST_FIELD_NUMBER = 6;
-    private java.lang.Object amHost_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object amHost_ = "";
     /**
      * <code>optional string am_host = 6;</code>
+     * @return Whether the amHost field is set.
      */
+    @java.lang.Override
     public boolean hasAmHost() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+      return ((bitField0_ & 0x00000020) != 0);
     }
     /**
      * <code>optional string am_host = 6;</code>
+     * @return The amHost.
      */
+    @java.lang.Override
     public java.lang.String getAmHost() {
       java.lang.Object ref = amHost_;
       if (ref instanceof java.lang.String) {
@@ -10045,7 +10916,9 @@ public java.lang.String getAmHost() {
     }
     /**
      * <code>optional string am_host = 6;</code>
+     * @return The bytes for amHost.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getAmHostBytes() {
       java.lang.Object ref = amHost_;
@@ -10060,148 +10933,171 @@ public java.lang.String getAmHost() {
       }
     }
 
-    // optional int32 am_port = 7;
     public static final int AM_PORT_FIELD_NUMBER = 7;
-    private int amPort_;
+    private int amPort_ = 0;
     /**
      * <code>optional int32 am_port = 7;</code>
+     * @return Whether the amPort field is set.
      */
+    @java.lang.Override
     public boolean hasAmPort() {
-      return ((bitField0_ & 0x00000040) == 0x00000040);
+      return ((bitField0_ & 0x00000040) != 0);
     }
     /**
      * <code>optional int32 am_port = 7;</code>
+     * @return The amPort.
      */
+    @java.lang.Override
     public int getAmPort() {
       return amPort_;
     }
 
-    // optional bytes credentials_binary = 8;
     public static final int CREDENTIALS_BINARY_FIELD_NUMBER = 8;
-    private com.google.protobuf.ByteString credentialsBinary_;
+    private com.google.protobuf.ByteString credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
     /**
-     * <code>optional bytes credentials_binary = 8;</code>
-     *
      * <pre>
      * Credentials are not signed - the client can add e.g. his own HDFS tokens.
      * </pre>
+     *
+     * <code>optional bytes credentials_binary = 8;</code>
+     * @return Whether the credentialsBinary field is set.
      */
+    @java.lang.Override
     public boolean hasCredentialsBinary() {
-      return ((bitField0_ & 0x00000080) == 0x00000080);
+      return ((bitField0_ & 0x00000080) != 0);
     }
     /**
-     * <code>optional bytes credentials_binary = 8;</code>
-     *
      * <pre>
      * Credentials are not signed - the client can add e.g. his own HDFS tokens.
      * </pre>
+     *
+     * <code>optional bytes credentials_binary = 8;</code>
+     * @return The credentialsBinary.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getCredentialsBinary() {
       return credentialsBinary_;
     }
 
-    // optional .FragmentRuntimeInfo fragment_runtime_info = 9;
     public static final int FRAGMENT_RUNTIME_INFO_FIELD_NUMBER = 9;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo fragmentRuntimeInfo_;
     /**
-     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-     *
      * <pre>
      * Not supported/honored for external clients right now.
      * </pre>
+     *
+     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
+     * @return Whether the fragmentRuntimeInfo field is set.
      */
+    @java.lang.Override
     public boolean hasFragmentRuntimeInfo() {
-      return ((bitField0_ & 0x00000100) == 0x00000100);
+      return ((bitField0_ & 0x00000100) != 0);
     }
     /**
-     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-     *
      * <pre>
      * Not supported/honored for external clients right now.
      * </pre>
+     *
+     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
+     * @return The fragmentRuntimeInfo.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getFragmentRuntimeInfo() {
-      return fragmentRuntimeInfo_;
+      return fragmentRuntimeInfo_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance() : fragmentRuntimeInfo_;
     }
     /**
-     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-     *
      * <pre>
      * Not supported/honored for external clients right now.
      * </pre>
+     *
+     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder getFragmentRuntimeInfoOrBuilder() {
-      return fragmentRuntimeInfo_;
+      return fragmentRuntimeInfo_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance() : fragmentRuntimeInfo_;
     }
 
-    // optional bytes initial_event_bytes = 10;
     public static final int INITIAL_EVENT_BYTES_FIELD_NUMBER = 10;
-    private com.google.protobuf.ByteString initialEventBytes_;
+    private com.google.protobuf.ByteString initialEventBytes_ = com.google.protobuf.ByteString.EMPTY;
     /**
-     * <code>optional bytes initial_event_bytes = 10;</code>
-     *
      * <pre>
      * Serialized (and signed) NotTezEvent; used only for external clients for now.
      * </pre>
+     *
+     * <code>optional bytes initial_event_bytes = 10;</code>
+     * @return Whether the initialEventBytes field is set.
      */
+    @java.lang.Override
     public boolean hasInitialEventBytes() {
-      return ((bitField0_ & 0x00000200) == 0x00000200);
+      return ((bitField0_ & 0x00000200) != 0);
     }
     /**
-     * <code>optional bytes initial_event_bytes = 10;</code>
-     *
      * <pre>
      * Serialized (and signed) NotTezEvent; used only for external clients for now.
      * </pre>
+     *
+     * <code>optional bytes initial_event_bytes = 10;</code>
+     * @return The initialEventBytes.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getInitialEventBytes() {
       return initialEventBytes_;
     }
 
-    // optional bytes initial_event_signature = 11;
     public static final int INITIAL_EVENT_SIGNATURE_FIELD_NUMBER = 11;
-    private com.google.protobuf.ByteString initialEventSignature_;
+    private com.google.protobuf.ByteString initialEventSignature_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>optional bytes initial_event_signature = 11;</code>
+     * @return Whether the initialEventSignature field is set.
      */
+    @java.lang.Override
     public boolean hasInitialEventSignature() {
-      return ((bitField0_ & 0x00000400) == 0x00000400);
+      return ((bitField0_ & 0x00000400) != 0);
     }
     /**
      * <code>optional bytes initial_event_signature = 11;</code>
+     * @return The initialEventSignature.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getInitialEventSignature() {
       return initialEventSignature_;
     }
 
-    // optional bool is_guaranteed = 12 [default = false];
     public static final int IS_GUARANTEED_FIELD_NUMBER = 12;
-    private boolean isGuaranteed_;
+    private boolean isGuaranteed_ = false;
     /**
      * <code>optional bool is_guaranteed = 12 [default = false];</code>
+     * @return Whether the isGuaranteed field is set.
      */
+    @java.lang.Override
     public boolean hasIsGuaranteed() {
-      return ((bitField0_ & 0x00000800) == 0x00000800);
+      return ((bitField0_ & 0x00000800) != 0);
     }
     /**
      * <code>optional bool is_guaranteed = 12 [default = false];</code>
+     * @return The isGuaranteed.
      */
+    @java.lang.Override
     public boolean getIsGuaranteed() {
       return isGuaranteed_;
     }
 
-    // optional string jwt = 13;
     public static final int JWT_FIELD_NUMBER = 13;
-    private java.lang.Object jwt_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object jwt_ = "";
     /**
      * <code>optional string jwt = 13;</code>
+     * @return Whether the jwt field is set.
      */
+    @java.lang.Override
     public boolean hasJwt() {
-      return ((bitField0_ & 0x00001000) == 0x00001000);
+      return ((bitField0_ & 0x00001000) != 0);
     }
     /**
      * <code>optional string jwt = 13;</code>
+     * @return The jwt.
      */
+    @java.lang.Override
     public java.lang.String getJwt() {
       java.lang.Object ref = jwt_;
       if (ref instanceof java.lang.String) {
@@ -10218,7 +11114,9 @@ public java.lang.String getJwt() {
     }
     /**
      * <code>optional string jwt = 13;</code>
+     * @return The bytes for jwt.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getJwtBytes() {
       java.lang.Object ref = jwt_;
@@ -10233,169 +11131,148 @@ public java.lang.String getJwt() {
       }
     }
 
-    // optional bool is_external_client_request = 14 [default = false];
     public static final int IS_EXTERNAL_CLIENT_REQUEST_FIELD_NUMBER = 14;
-    private boolean isExternalClientRequest_;
+    private boolean isExternalClientRequest_ = false;
     /**
      * <code>optional bool is_external_client_request = 14 [default = false];</code>
+     * @return Whether the isExternalClientRequest field is set.
      */
+    @java.lang.Override
     public boolean hasIsExternalClientRequest() {
-      return ((bitField0_ & 0x00002000) == 0x00002000);
+      return ((bitField0_ & 0x00002000) != 0);
     }
     /**
      * <code>optional bool is_external_client_request = 14 [default = false];</code>
+     * @return The isExternalClientRequest.
      */
+    @java.lang.Override
     public boolean getIsExternalClientRequest() {
       return isExternalClientRequest_;
     }
 
-    private void initFields() {
-      workSpec_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance();
-      workSpecSignature_ = com.google.protobuf.ByteString.EMPTY;
-      fragmentNumber_ = 0;
-      attemptNumber_ = 0;
-      containerIdString_ = "";
-      amHost_ = "";
-      amPort_ = 0;
-      credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
-      fragmentRuntimeInfo_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance();
-      initialEventBytes_ = com.google.protobuf.ByteString.EMPTY;
-      initialEventSignature_ = com.google.protobuf.ByteString.EMPTY;
-      isGuaranteed_ = false;
-      jwt_ = "";
-      isExternalClientRequest_ = false;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, workSpec_);
+      if (((bitField0_ & 0x00000001) != 0)) {
+        output.writeMessage(1, getWorkSpec());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeBytes(2, workSpecSignature_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         output.writeInt32(3, fragmentNumber_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+      if (((bitField0_ & 0x00000008) != 0)) {
         output.writeInt32(4, attemptNumber_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeBytes(5, getContainerIdStringBytes());
+      if (((bitField0_ & 0x00000010) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, containerIdString_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        output.writeBytes(6, getAmHostBytes());
+      if (((bitField0_ & 0x00000020) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 6, amHost_);
       }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
+      if (((bitField0_ & 0x00000040) != 0)) {
         output.writeInt32(7, amPort_);
       }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
+      if (((bitField0_ & 0x00000080) != 0)) {
         output.writeBytes(8, credentialsBinary_);
       }
-      if (((bitField0_ & 0x00000100) == 0x00000100)) {
-        output.writeMessage(9, fragmentRuntimeInfo_);
+      if (((bitField0_ & 0x00000100) != 0)) {
+        output.writeMessage(9, getFragmentRuntimeInfo());
       }
-      if (((bitField0_ & 0x00000200) == 0x00000200)) {
+      if (((bitField0_ & 0x00000200) != 0)) {
         output.writeBytes(10, initialEventBytes_);
       }
-      if (((bitField0_ & 0x00000400) == 0x00000400)) {
+      if (((bitField0_ & 0x00000400) != 0)) {
         output.writeBytes(11, initialEventSignature_);
       }
-      if (((bitField0_ & 0x00000800) == 0x00000800)) {
+      if (((bitField0_ & 0x00000800) != 0)) {
         output.writeBool(12, isGuaranteed_);
       }
-      if (((bitField0_ & 0x00001000) == 0x00001000)) {
-        output.writeBytes(13, getJwtBytes());
+      if (((bitField0_ & 0x00001000) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 13, jwt_);
       }
-      if (((bitField0_ & 0x00002000) == 0x00002000)) {
+      if (((bitField0_ & 0x00002000) != 0)) {
         output.writeBool(14, isExternalClientRequest_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, workSpec_);
+          .computeMessageSize(1, getWorkSpec());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(2, workSpecSignature_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(3, fragmentNumber_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+      if (((bitField0_ & 0x00000008) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(4, attemptNumber_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(5, getContainerIdStringBytes());
+      if (((bitField0_ & 0x00000010) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, containerIdString_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(6, getAmHostBytes());
+      if (((bitField0_ & 0x00000020) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(6, amHost_);
       }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
+      if (((bitField0_ & 0x00000040) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(7, amPort_);
       }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
+      if (((bitField0_ & 0x00000080) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(8, credentialsBinary_);
       }
-      if (((bitField0_ & 0x00000100) == 0x00000100)) {
+      if (((bitField0_ & 0x00000100) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(9, fragmentRuntimeInfo_);
+          .computeMessageSize(9, getFragmentRuntimeInfo());
       }
-      if (((bitField0_ & 0x00000200) == 0x00000200)) {
+      if (((bitField0_ & 0x00000200) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(10, initialEventBytes_);
       }
-      if (((bitField0_ & 0x00000400) == 0x00000400)) {
+      if (((bitField0_ & 0x00000400) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(11, initialEventSignature_);
       }
-      if (((bitField0_ & 0x00000800) == 0x00000800)) {
+      if (((bitField0_ & 0x00000800) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBoolSize(12, isGuaranteed_);
       }
-      if (((bitField0_ & 0x00001000) == 0x00001000)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(13, getJwtBytes());
+      if (((bitField0_ & 0x00001000) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(13, jwt_);
       }
-      if (((bitField0_ & 0x00002000) == 0x00002000)) {
+      if (((bitField0_ & 0x00002000) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBoolSize(14, isExternalClientRequest_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -10406,90 +11283,87 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasWorkSpec() == other.hasWorkSpec());
+      if (hasWorkSpec() != other.hasWorkSpec()) return false;
       if (hasWorkSpec()) {
-        result = result && getWorkSpec()
-            .equals(other.getWorkSpec());
+        if (!getWorkSpec()
+            .equals(other.getWorkSpec())) return false;
       }
-      result = result && (hasWorkSpecSignature() == other.hasWorkSpecSignature());
+      if (hasWorkSpecSignature() != other.hasWorkSpecSignature()) return false;
       if (hasWorkSpecSignature()) {
-        result = result && getWorkSpecSignature()
-            .equals(other.getWorkSpecSignature());
+        if (!getWorkSpecSignature()
+            .equals(other.getWorkSpecSignature())) return false;
       }
-      result = result && (hasFragmentNumber() == other.hasFragmentNumber());
+      if (hasFragmentNumber() != other.hasFragmentNumber()) return false;
       if (hasFragmentNumber()) {
-        result = result && (getFragmentNumber()
-            == other.getFragmentNumber());
+        if (getFragmentNumber()
+            != other.getFragmentNumber()) return false;
       }
-      result = result && (hasAttemptNumber() == other.hasAttemptNumber());
+      if (hasAttemptNumber() != other.hasAttemptNumber()) return false;
       if (hasAttemptNumber()) {
-        result = result && (getAttemptNumber()
-            == other.getAttemptNumber());
+        if (getAttemptNumber()
+            != other.getAttemptNumber()) return false;
       }
-      result = result && (hasContainerIdString() == other.hasContainerIdString());
+      if (hasContainerIdString() != other.hasContainerIdString()) return false;
       if (hasContainerIdString()) {
-        result = result && getContainerIdString()
-            .equals(other.getContainerIdString());
+        if (!getContainerIdString()
+            .equals(other.getContainerIdString())) return false;
       }
-      result = result && (hasAmHost() == other.hasAmHost());
+      if (hasAmHost() != other.hasAmHost()) return false;
       if (hasAmHost()) {
-        result = result && getAmHost()
-            .equals(other.getAmHost());
+        if (!getAmHost()
+            .equals(other.getAmHost())) return false;
       }
-      result = result && (hasAmPort() == other.hasAmPort());
+      if (hasAmPort() != other.hasAmPort()) return false;
       if (hasAmPort()) {
-        result = result && (getAmPort()
-            == other.getAmPort());
+        if (getAmPort()
+            != other.getAmPort()) return false;
       }
-      result = result && (hasCredentialsBinary() == other.hasCredentialsBinary());
+      if (hasCredentialsBinary() != other.hasCredentialsBinary()) return false;
       if (hasCredentialsBinary()) {
-        result = result && getCredentialsBinary()
-            .equals(other.getCredentialsBinary());
+        if (!getCredentialsBinary()
+            .equals(other.getCredentialsBinary())) return false;
       }
-      result = result && (hasFragmentRuntimeInfo() == other.hasFragmentRuntimeInfo());
+      if (hasFragmentRuntimeInfo() != other.hasFragmentRuntimeInfo()) return false;
       if (hasFragmentRuntimeInfo()) {
-        result = result && getFragmentRuntimeInfo()
-            .equals(other.getFragmentRuntimeInfo());
+        if (!getFragmentRuntimeInfo()
+            .equals(other.getFragmentRuntimeInfo())) return false;
       }
-      result = result && (hasInitialEventBytes() == other.hasInitialEventBytes());
+      if (hasInitialEventBytes() != other.hasInitialEventBytes()) return false;
       if (hasInitialEventBytes()) {
-        result = result && getInitialEventBytes()
-            .equals(other.getInitialEventBytes());
+        if (!getInitialEventBytes()
+            .equals(other.getInitialEventBytes())) return false;
       }
-      result = result && (hasInitialEventSignature() == other.hasInitialEventSignature());
+      if (hasInitialEventSignature() != other.hasInitialEventSignature()) return false;
       if (hasInitialEventSignature()) {
-        result = result && getInitialEventSignature()
-            .equals(other.getInitialEventSignature());
+        if (!getInitialEventSignature()
+            .equals(other.getInitialEventSignature())) return false;
       }
-      result = result && (hasIsGuaranteed() == other.hasIsGuaranteed());
+      if (hasIsGuaranteed() != other.hasIsGuaranteed()) return false;
       if (hasIsGuaranteed()) {
-        result = result && (getIsGuaranteed()
-            == other.getIsGuaranteed());
+        if (getIsGuaranteed()
+            != other.getIsGuaranteed()) return false;
       }
-      result = result && (hasJwt() == other.hasJwt());
+      if (hasJwt() != other.hasJwt()) return false;
       if (hasJwt()) {
-        result = result && getJwt()
-            .equals(other.getJwt());
+        if (!getJwt()
+            .equals(other.getJwt())) return false;
       }
-      result = result && (hasIsExternalClientRequest() == other.hasIsExternalClientRequest());
+      if (hasIsExternalClientRequest() != other.hasIsExternalClientRequest()) return false;
       if (hasIsExternalClientRequest()) {
-        result = result && (getIsExternalClientRequest()
-            == other.getIsExternalClientRequest());
+        if (getIsExternalClientRequest()
+            != other.getIsExternalClientRequest()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasWorkSpec()) {
         hash = (37 * hash) + WORK_SPEC_FIELD_NUMBER;
         hash = (53 * hash) + getWorkSpec().hashCode();
@@ -10536,7 +11410,8 @@ public int hashCode() {
       }
       if (hasIsGuaranteed()) {
         hash = (37 * hash) + IS_GUARANTEED_FIELD_NUMBER;
-        hash = (53 * hash) + hashBoolean(getIsGuaranteed());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
+            getIsGuaranteed());
       }
       if (hasJwt()) {
         hash = (37 * hash) + JWT_FIELD_NUMBER;
@@ -10544,13 +11419,25 @@ public int hashCode() {
       }
       if (hasIsExternalClientRequest()) {
         hash = (37 * hash) + IS_EXTERNAL_CLIENT_REQUEST_FIELD_NUMBER;
-        hash = (53 * hash) + hashBoolean(getIsExternalClientRequest());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
+            getIsExternalClientRequest());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -10574,46 +11461,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Su
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -10621,14 +11523,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code SubmitWorkRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:SubmitWorkRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -10641,74 +11545,58 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getWorkSpecFieldBuilder();
           getFragmentRuntimeInfoFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
-        if (workSpecBuilder_ == null) {
-          workSpec_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance();
-        } else {
-          workSpecBuilder_.clear();
+        bitField0_ = 0;
+        workSpec_ = null;
+        if (workSpecBuilder_ != null) {
+          workSpecBuilder_.dispose();
+          workSpecBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000001);
         workSpecSignature_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
         fragmentNumber_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000004);
         attemptNumber_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000008);
         containerIdString_ = "";
-        bitField0_ = (bitField0_ & ~0x00000010);
         amHost_ = "";
-        bitField0_ = (bitField0_ & ~0x00000020);
         amPort_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000040);
         credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000080);
-        if (fragmentRuntimeInfoBuilder_ == null) {
-          fragmentRuntimeInfo_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance();
-        } else {
-          fragmentRuntimeInfoBuilder_.clear();
+        fragmentRuntimeInfo_ = null;
+        if (fragmentRuntimeInfoBuilder_ != null) {
+          fragmentRuntimeInfoBuilder_.dispose();
+          fragmentRuntimeInfoBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000100);
         initialEventBytes_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000200);
         initialEventSignature_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000400);
         isGuaranteed_ = false;
-        bitField0_ = (bitField0_ & ~0x00000800);
         jwt_ = "";
-        bitField0_ = (bitField0_ & ~0x00001000);
         isExternalClientRequest_ = false;
-        bitField0_ = (bitField0_ & ~0x00002000);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -10717,79 +11605,113 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWor
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.workSpec_ = workSpecBuilder_ == null
+              ? workSpec_
+              : workSpecBuilder_.build();
           to_bitField0_ |= 0x00000001;
         }
-        if (workSpecBuilder_ == null) {
-          result.workSpec_ = workSpec_;
-        } else {
-          result.workSpec_ = workSpecBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.workSpecSignature_ = workSpecSignature_;
           to_bitField0_ |= 0x00000002;
         }
-        result.workSpecSignature_ = workSpecSignature_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.fragmentNumber_ = fragmentNumber_;
           to_bitField0_ |= 0x00000004;
         }
-        result.fragmentNumber_ = fragmentNumber_;
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
+        if (((from_bitField0_ & 0x00000008) != 0)) {
+          result.attemptNumber_ = attemptNumber_;
           to_bitField0_ |= 0x00000008;
         }
-        result.attemptNumber_ = attemptNumber_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
+        if (((from_bitField0_ & 0x00000010) != 0)) {
+          result.containerIdString_ = containerIdString_;
           to_bitField0_ |= 0x00000010;
         }
-        result.containerIdString_ = containerIdString_;
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
+        if (((from_bitField0_ & 0x00000020) != 0)) {
+          result.amHost_ = amHost_;
           to_bitField0_ |= 0x00000020;
         }
-        result.amHost_ = amHost_;
-        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
+        if (((from_bitField0_ & 0x00000040) != 0)) {
+          result.amPort_ = amPort_;
           to_bitField0_ |= 0x00000040;
         }
-        result.amPort_ = amPort_;
-        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
+        if (((from_bitField0_ & 0x00000080) != 0)) {
+          result.credentialsBinary_ = credentialsBinary_;
           to_bitField0_ |= 0x00000080;
         }
-        result.credentialsBinary_ = credentialsBinary_;
-        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
+        if (((from_bitField0_ & 0x00000100) != 0)) {
+          result.fragmentRuntimeInfo_ = fragmentRuntimeInfoBuilder_ == null
+              ? fragmentRuntimeInfo_
+              : fragmentRuntimeInfoBuilder_.build();
           to_bitField0_ |= 0x00000100;
         }
-        if (fragmentRuntimeInfoBuilder_ == null) {
-          result.fragmentRuntimeInfo_ = fragmentRuntimeInfo_;
-        } else {
-          result.fragmentRuntimeInfo_ = fragmentRuntimeInfoBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
+        if (((from_bitField0_ & 0x00000200) != 0)) {
+          result.initialEventBytes_ = initialEventBytes_;
           to_bitField0_ |= 0x00000200;
         }
-        result.initialEventBytes_ = initialEventBytes_;
-        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
+        if (((from_bitField0_ & 0x00000400) != 0)) {
+          result.initialEventSignature_ = initialEventSignature_;
           to_bitField0_ |= 0x00000400;
         }
-        result.initialEventSignature_ = initialEventSignature_;
-        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
+        if (((from_bitField0_ & 0x00000800) != 0)) {
+          result.isGuaranteed_ = isGuaranteed_;
           to_bitField0_ |= 0x00000800;
         }
-        result.isGuaranteed_ = isGuaranteed_;
-        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
+        if (((from_bitField0_ & 0x00001000) != 0)) {
+          result.jwt_ = jwt_;
           to_bitField0_ |= 0x00001000;
         }
-        result.jwt_ = jwt_;
-        if (((from_bitField0_ & 0x00002000) == 0x00002000)) {
+        if (((from_bitField0_ & 0x00002000) != 0)) {
+          result.isExternalClientRequest_ = isExternalClientRequest_;
           to_bitField0_ |= 0x00002000;
         }
-        result.isExternalClientRequest_ = isExternalClientRequest_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto)other);
@@ -10814,13 +11736,13 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setAttemptNumber(other.getAttemptNumber());
         }
         if (other.hasContainerIdString()) {
-          bitField0_ |= 0x00000010;
           containerIdString_ = other.containerIdString_;
+          bitField0_ |= 0x00000010;
           onChanged();
         }
         if (other.hasAmHost()) {
-          bitField0_ |= 0x00000020;
           amHost_ = other.amHost_;
+          bitField0_ |= 0x00000020;
           onChanged();
         }
         if (other.hasAmPort()) {
@@ -10842,56 +11764,147 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setIsGuaranteed(other.getIsGuaranteed());
         }
         if (other.hasJwt()) {
-          bitField0_ |= 0x00001000;
           jwt_ = other.jwt_;
+          bitField0_ |= 0x00001000;
           onChanged();
         }
         if (other.hasIsExternalClientRequest()) {
           setIsExternalClientRequest(other.getIsExternalClientRequest());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                input.readMessage(
+                    getWorkSpecFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                workSpecSignature_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 24: {
+                fragmentNumber_ = input.readInt32();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 24
+              case 32: {
+                attemptNumber_ = input.readInt32();
+                bitField0_ |= 0x00000008;
+                break;
+              } // case 32
+              case 42: {
+                containerIdString_ = input.readBytes();
+                bitField0_ |= 0x00000010;
+                break;
+              } // case 42
+              case 50: {
+                amHost_ = input.readBytes();
+                bitField0_ |= 0x00000020;
+                break;
+              } // case 50
+              case 56: {
+                amPort_ = input.readInt32();
+                bitField0_ |= 0x00000040;
+                break;
+              } // case 56
+              case 66: {
+                credentialsBinary_ = input.readBytes();
+                bitField0_ |= 0x00000080;
+                break;
+              } // case 66
+              case 74: {
+                input.readMessage(
+                    getFragmentRuntimeInfoFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000100;
+                break;
+              } // case 74
+              case 82: {
+                initialEventBytes_ = input.readBytes();
+                bitField0_ |= 0x00000200;
+                break;
+              } // case 82
+              case 90: {
+                initialEventSignature_ = input.readBytes();
+                bitField0_ |= 0x00000400;
+                break;
+              } // case 90
+              case 96: {
+                isGuaranteed_ = input.readBool();
+                bitField0_ |= 0x00000800;
+                break;
+              } // case 96
+              case 106: {
+                jwt_ = input.readBytes();
+                bitField0_ |= 0x00001000;
+                break;
+              } // case 106
+              case 112: {
+                isExternalClientRequest_ = input.readBool();
+                bitField0_ |= 0x00002000;
+                break;
+              } // case 112
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional .VertexOrBinary work_spec = 1;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary workSpec_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary workSpec_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder> workSpecBuilder_;
       /**
        * <code>optional .VertexOrBinary work_spec = 1;</code>
+       * @return Whether the workSpec field is set.
        */
       public boolean hasWorkSpec() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional .VertexOrBinary work_spec = 1;</code>
+       * @return The workSpec.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getWorkSpec() {
         if (workSpecBuilder_ == null) {
-          return workSpec_;
+          return workSpec_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance() : workSpec_;
         } else {
           return workSpecBuilder_.getMessage();
         }
@@ -10905,11 +11918,11 @@ public Builder setWorkSpec(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProt
             throw new NullPointerException();
           }
           workSpec_ = value;
-          onChanged();
         } else {
           workSpecBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -10919,11 +11932,11 @@ public Builder setWorkSpec(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder builderForValue) {
         if (workSpecBuilder_ == null) {
           workSpec_ = builderForValue.build();
-          onChanged();
         } else {
           workSpecBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -10931,31 +11944,33 @@ public Builder setWorkSpec(
        */
       public Builder mergeWorkSpec(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary value) {
         if (workSpecBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              workSpec_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance()) {
-            workSpec_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.newBuilder(workSpec_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000001) != 0) &&
+            workSpec_ != null &&
+            workSpec_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance()) {
+            getWorkSpecBuilder().mergeFrom(value);
           } else {
             workSpec_ = value;
           }
-          onChanged();
         } else {
           workSpecBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000001;
+        if (workSpec_ != null) {
+          bitField0_ |= 0x00000001;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .VertexOrBinary work_spec = 1;</code>
        */
       public Builder clearWorkSpec() {
-        if (workSpecBuilder_ == null) {
-          workSpec_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance();
-          onChanged();
-        } else {
-          workSpecBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000001);
+        workSpec_ = null;
+        if (workSpecBuilder_ != null) {
+          workSpecBuilder_.dispose();
+          workSpecBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -10973,19 +11988,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrB
         if (workSpecBuilder_ != null) {
           return workSpecBuilder_.getMessageOrBuilder();
         } else {
-          return workSpec_;
+          return workSpec_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance() : workSpec_;
         }
       }
       /**
        * <code>optional .VertexOrBinary work_spec = 1;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder> 
           getWorkSpecFieldBuilder() {
         if (workSpecBuilder_ == null) {
-          workSpecBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          workSpecBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder>(
-                  workSpec_,
+                  getWorkSpec(),
                   getParentForChildren(),
                   isClean());
           workSpec_ = null;
@@ -10993,34 +12009,38 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrB
         return workSpecBuilder_;
       }
 
-      // optional bytes work_spec_signature = 2;
       private com.google.protobuf.ByteString workSpecSignature_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>optional bytes work_spec_signature = 2;</code>
+       * @return Whether the workSpecSignature field is set.
        */
+      @java.lang.Override
       public boolean hasWorkSpecSignature() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional bytes work_spec_signature = 2;</code>
+       * @return The workSpecSignature.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getWorkSpecSignature() {
         return workSpecSignature_;
       }
       /**
        * <code>optional bytes work_spec_signature = 2;</code>
+       * @param value The workSpecSignature to set.
+       * @return This builder for chaining.
        */
       public Builder setWorkSpecSignature(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         workSpecSignature_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional bytes work_spec_signature = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearWorkSpecSignature() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -11029,31 +12049,38 @@ public Builder clearWorkSpecSignature() {
         return this;
       }
 
-      // optional int32 fragment_number = 3;
       private int fragmentNumber_ ;
       /**
        * <code>optional int32 fragment_number = 3;</code>
+       * @return Whether the fragmentNumber field is set.
        */
+      @java.lang.Override
       public boolean hasFragmentNumber() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional int32 fragment_number = 3;</code>
+       * @return The fragmentNumber.
        */
+      @java.lang.Override
       public int getFragmentNumber() {
         return fragmentNumber_;
       }
       /**
        * <code>optional int32 fragment_number = 3;</code>
+       * @param value The fragmentNumber to set.
+       * @return This builder for chaining.
        */
       public Builder setFragmentNumber(int value) {
-        bitField0_ |= 0x00000004;
+
         fragmentNumber_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 fragment_number = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFragmentNumber() {
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -11062,31 +12089,38 @@ public Builder clearFragmentNumber() {
         return this;
       }
 
-      // optional int32 attempt_number = 4;
       private int attemptNumber_ ;
       /**
        * <code>optional int32 attempt_number = 4;</code>
+       * @return Whether the attemptNumber field is set.
        */
+      @java.lang.Override
       public boolean hasAttemptNumber() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
+        return ((bitField0_ & 0x00000008) != 0);
       }
       /**
        * <code>optional int32 attempt_number = 4;</code>
+       * @return The attemptNumber.
        */
+      @java.lang.Override
       public int getAttemptNumber() {
         return attemptNumber_;
       }
       /**
        * <code>optional int32 attempt_number = 4;</code>
+       * @param value The attemptNumber to set.
+       * @return This builder for chaining.
        */
       public Builder setAttemptNumber(int value) {
-        bitField0_ |= 0x00000008;
+
         attemptNumber_ = value;
+        bitField0_ |= 0x00000008;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 attempt_number = 4;</code>
+       * @return This builder for chaining.
        */
       public Builder clearAttemptNumber() {
         bitField0_ = (bitField0_ & ~0x00000008);
@@ -11095,23 +12129,27 @@ public Builder clearAttemptNumber() {
         return this;
       }
 
-      // optional string container_id_string = 5;
       private java.lang.Object containerIdString_ = "";
       /**
        * <code>optional string container_id_string = 5;</code>
+       * @return Whether the containerIdString field is set.
        */
       public boolean hasContainerIdString() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
+        return ((bitField0_ & 0x00000010) != 0);
       }
       /**
        * <code>optional string container_id_string = 5;</code>
+       * @return The containerIdString.
        */
       public java.lang.String getContainerIdString() {
         java.lang.Object ref = containerIdString_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          containerIdString_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            containerIdString_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -11119,6 +12157,7 @@ public java.lang.String getContainerIdString() {
       }
       /**
        * <code>optional string container_id_string = 5;</code>
+       * @return The bytes for containerIdString.
        */
       public com.google.protobuf.ByteString
           getContainerIdStringBytes() {
@@ -11135,57 +12174,62 @@ public java.lang.String getContainerIdString() {
       }
       /**
        * <code>optional string container_id_string = 5;</code>
+       * @param value The containerIdString to set.
+       * @return This builder for chaining.
        */
       public Builder setContainerIdString(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000010;
+        if (value == null) { throw new NullPointerException(); }
         containerIdString_ = value;
+        bitField0_ |= 0x00000010;
         onChanged();
         return this;
       }
       /**
        * <code>optional string container_id_string = 5;</code>
+       * @return This builder for chaining.
        */
       public Builder clearContainerIdString() {
-        bitField0_ = (bitField0_ & ~0x00000010);
         containerIdString_ = getDefaultInstance().getContainerIdString();
+        bitField0_ = (bitField0_ & ~0x00000010);
         onChanged();
         return this;
       }
       /**
        * <code>optional string container_id_string = 5;</code>
+       * @param value The bytes for containerIdString to set.
+       * @return This builder for chaining.
        */
       public Builder setContainerIdStringBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000010;
+        if (value == null) { throw new NullPointerException(); }
         containerIdString_ = value;
+        bitField0_ |= 0x00000010;
         onChanged();
         return this;
       }
 
-      // optional string am_host = 6;
       private java.lang.Object amHost_ = "";
       /**
        * <code>optional string am_host = 6;</code>
+       * @return Whether the amHost field is set.
        */
       public boolean hasAmHost() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
+        return ((bitField0_ & 0x00000020) != 0);
       }
       /**
        * <code>optional string am_host = 6;</code>
+       * @return The amHost.
        */
       public java.lang.String getAmHost() {
         java.lang.Object ref = amHost_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          amHost_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            amHost_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -11193,6 +12237,7 @@ public java.lang.String getAmHost() {
       }
       /**
        * <code>optional string am_host = 6;</code>
+       * @return The bytes for amHost.
        */
       public com.google.protobuf.ByteString
           getAmHostBytes() {
@@ -11209,65 +12254,73 @@ public java.lang.String getAmHost() {
       }
       /**
        * <code>optional string am_host = 6;</code>
+       * @param value The amHost to set.
+       * @return This builder for chaining.
        */
       public Builder setAmHost(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000020;
+        if (value == null) { throw new NullPointerException(); }
         amHost_ = value;
+        bitField0_ |= 0x00000020;
         onChanged();
         return this;
       }
       /**
        * <code>optional string am_host = 6;</code>
+       * @return This builder for chaining.
        */
       public Builder clearAmHost() {
-        bitField0_ = (bitField0_ & ~0x00000020);
         amHost_ = getDefaultInstance().getAmHost();
+        bitField0_ = (bitField0_ & ~0x00000020);
         onChanged();
         return this;
       }
       /**
        * <code>optional string am_host = 6;</code>
+       * @param value The bytes for amHost to set.
+       * @return This builder for chaining.
        */
       public Builder setAmHostBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000020;
+        if (value == null) { throw new NullPointerException(); }
         amHost_ = value;
+        bitField0_ |= 0x00000020;
         onChanged();
         return this;
       }
 
-      // optional int32 am_port = 7;
       private int amPort_ ;
       /**
        * <code>optional int32 am_port = 7;</code>
+       * @return Whether the amPort field is set.
        */
+      @java.lang.Override
       public boolean hasAmPort() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
+        return ((bitField0_ & 0x00000040) != 0);
       }
       /**
        * <code>optional int32 am_port = 7;</code>
+       * @return The amPort.
        */
+      @java.lang.Override
       public int getAmPort() {
         return amPort_;
       }
       /**
        * <code>optional int32 am_port = 7;</code>
+       * @param value The amPort to set.
+       * @return This builder for chaining.
        */
       public Builder setAmPort(int value) {
-        bitField0_ |= 0x00000040;
+
         amPort_ = value;
+        bitField0_ |= 0x00000040;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 am_port = 7;</code>
+       * @return This builder for chaining.
        */
       public Builder clearAmPort() {
         bitField0_ = (bitField0_ & ~0x00000040);
@@ -11276,50 +12329,54 @@ public Builder clearAmPort() {
         return this;
       }
 
-      // optional bytes credentials_binary = 8;
       private com.google.protobuf.ByteString credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
       /**
-       * <code>optional bytes credentials_binary = 8;</code>
-       *
        * <pre>
        * Credentials are not signed - the client can add e.g. his own HDFS tokens.
        * </pre>
+       *
+       * <code>optional bytes credentials_binary = 8;</code>
+       * @return Whether the credentialsBinary field is set.
        */
+      @java.lang.Override
       public boolean hasCredentialsBinary() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
+        return ((bitField0_ & 0x00000080) != 0);
       }
       /**
-       * <code>optional bytes credentials_binary = 8;</code>
-       *
        * <pre>
        * Credentials are not signed - the client can add e.g. his own HDFS tokens.
        * </pre>
+       *
+       * <code>optional bytes credentials_binary = 8;</code>
+       * @return The credentialsBinary.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getCredentialsBinary() {
         return credentialsBinary_;
       }
       /**
-       * <code>optional bytes credentials_binary = 8;</code>
-       *
        * <pre>
        * Credentials are not signed - the client can add e.g. his own HDFS tokens.
        * </pre>
+       *
+       * <code>optional bytes credentials_binary = 8;</code>
+       * @param value The credentialsBinary to set.
+       * @return This builder for chaining.
        */
       public Builder setCredentialsBinary(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000080;
+        if (value == null) { throw new NullPointerException(); }
         credentialsBinary_ = value;
+        bitField0_ |= 0x00000080;
         onChanged();
         return this;
       }
       /**
-       * <code>optional bytes credentials_binary = 8;</code>
-       *
        * <pre>
        * Credentials are not signed - the client can add e.g. his own HDFS tokens.
        * </pre>
+       *
+       * <code>optional bytes credentials_binary = 8;</code>
+       * @return This builder for chaining.
        */
       public Builder clearCredentialsBinary() {
         bitField0_ = (bitField0_ & ~0x00000080);
@@ -11328,40 +12385,41 @@ public Builder clearCredentialsBinary() {
         return this;
       }
 
-      // optional .FragmentRuntimeInfo fragment_runtime_info = 9;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo fragmentRuntimeInfo_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo fragmentRuntimeInfo_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder> fragmentRuntimeInfoBuilder_;
       /**
-       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-       *
        * <pre>
        * Not supported/honored for external clients right now.
        * </pre>
+       *
+       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
+       * @return Whether the fragmentRuntimeInfo field is set.
        */
       public boolean hasFragmentRuntimeInfo() {
-        return ((bitField0_ & 0x00000100) == 0x00000100);
+        return ((bitField0_ & 0x00000100) != 0);
       }
       /**
-       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-       *
        * <pre>
        * Not supported/honored for external clients right now.
        * </pre>
+       *
+       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
+       * @return The fragmentRuntimeInfo.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getFragmentRuntimeInfo() {
         if (fragmentRuntimeInfoBuilder_ == null) {
-          return fragmentRuntimeInfo_;
+          return fragmentRuntimeInfo_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance() : fragmentRuntimeInfo_;
         } else {
           return fragmentRuntimeInfoBuilder_.getMessage();
         }
       }
       /**
-       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-       *
        * <pre>
        * Not supported/honored for external clients right now.
        * </pre>
+       *
+       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
        */
       public Builder setFragmentRuntimeInfo(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo value) {
         if (fragmentRuntimeInfoBuilder_ == null) {
@@ -11369,77 +12427,79 @@ public Builder setFragmentRuntimeInfo(org.apache.hadoop.hive.llap.daemon.rpc.Lla
             throw new NullPointerException();
           }
           fragmentRuntimeInfo_ = value;
-          onChanged();
         } else {
           fragmentRuntimeInfoBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000100;
+        onChanged();
         return this;
       }
       /**
-       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-       *
        * <pre>
        * Not supported/honored for external clients right now.
        * </pre>
+       *
+       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
        */
       public Builder setFragmentRuntimeInfo(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder builderForValue) {
         if (fragmentRuntimeInfoBuilder_ == null) {
           fragmentRuntimeInfo_ = builderForValue.build();
-          onChanged();
         } else {
           fragmentRuntimeInfoBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000100;
+        onChanged();
         return this;
       }
       /**
-       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-       *
        * <pre>
        * Not supported/honored for external clients right now.
        * </pre>
+       *
+       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
        */
       public Builder mergeFragmentRuntimeInfo(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo value) {
         if (fragmentRuntimeInfoBuilder_ == null) {
-          if (((bitField0_ & 0x00000100) == 0x00000100) &&
-              fragmentRuntimeInfo_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance()) {
-            fragmentRuntimeInfo_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.newBuilder(fragmentRuntimeInfo_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000100) != 0) &&
+            fragmentRuntimeInfo_ != null &&
+            fragmentRuntimeInfo_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance()) {
+            getFragmentRuntimeInfoBuilder().mergeFrom(value);
           } else {
             fragmentRuntimeInfo_ = value;
           }
-          onChanged();
         } else {
           fragmentRuntimeInfoBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000100;
+        if (fragmentRuntimeInfo_ != null) {
+          bitField0_ |= 0x00000100;
+          onChanged();
+        }
         return this;
       }
       /**
-       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-       *
        * <pre>
        * Not supported/honored for external clients right now.
        * </pre>
+       *
+       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
        */
       public Builder clearFragmentRuntimeInfo() {
-        if (fragmentRuntimeInfoBuilder_ == null) {
-          fragmentRuntimeInfo_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance();
-          onChanged();
-        } else {
-          fragmentRuntimeInfoBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000100);
+        fragmentRuntimeInfo_ = null;
+        if (fragmentRuntimeInfoBuilder_ != null) {
+          fragmentRuntimeInfoBuilder_.dispose();
+          fragmentRuntimeInfoBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
-       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-       *
        * <pre>
        * Not supported/honored for external clients right now.
        * </pre>
+       *
+       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder getFragmentRuntimeInfoBuilder() {
         bitField0_ |= 0x00000100;
@@ -11447,33 +12507,34 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentR
         return getFragmentRuntimeInfoFieldBuilder().getBuilder();
       }
       /**
-       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-       *
        * <pre>
        * Not supported/honored for external clients right now.
        * </pre>
+       *
+       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder getFragmentRuntimeInfoOrBuilder() {
         if (fragmentRuntimeInfoBuilder_ != null) {
           return fragmentRuntimeInfoBuilder_.getMessageOrBuilder();
         } else {
-          return fragmentRuntimeInfo_;
+          return fragmentRuntimeInfo_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance() : fragmentRuntimeInfo_;
         }
       }
       /**
-       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
-       *
        * <pre>
        * Not supported/honored for external clients right now.
        * </pre>
+       *
+       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder> 
           getFragmentRuntimeInfoFieldBuilder() {
         if (fragmentRuntimeInfoBuilder_ == null) {
-          fragmentRuntimeInfoBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          fragmentRuntimeInfoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder>(
-                  fragmentRuntimeInfo_,
+                  getFragmentRuntimeInfo(),
                   getParentForChildren(),
                   isClean());
           fragmentRuntimeInfo_ = null;
@@ -11481,50 +12542,54 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentR
         return fragmentRuntimeInfoBuilder_;
       }
 
-      // optional bytes initial_event_bytes = 10;
       private com.google.protobuf.ByteString initialEventBytes_ = com.google.protobuf.ByteString.EMPTY;
       /**
-       * <code>optional bytes initial_event_bytes = 10;</code>
-       *
        * <pre>
        * Serialized (and signed) NotTezEvent; used only for external clients for now.
        * </pre>
+       *
+       * <code>optional bytes initial_event_bytes = 10;</code>
+       * @return Whether the initialEventBytes field is set.
        */
+      @java.lang.Override
       public boolean hasInitialEventBytes() {
-        return ((bitField0_ & 0x00000200) == 0x00000200);
+        return ((bitField0_ & 0x00000200) != 0);
       }
       /**
-       * <code>optional bytes initial_event_bytes = 10;</code>
-       *
        * <pre>
        * Serialized (and signed) NotTezEvent; used only for external clients for now.
        * </pre>
+       *
+       * <code>optional bytes initial_event_bytes = 10;</code>
+       * @return The initialEventBytes.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getInitialEventBytes() {
         return initialEventBytes_;
       }
       /**
-       * <code>optional bytes initial_event_bytes = 10;</code>
-       *
        * <pre>
        * Serialized (and signed) NotTezEvent; used only for external clients for now.
        * </pre>
+       *
+       * <code>optional bytes initial_event_bytes = 10;</code>
+       * @param value The initialEventBytes to set.
+       * @return This builder for chaining.
        */
       public Builder setInitialEventBytes(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000200;
+        if (value == null) { throw new NullPointerException(); }
         initialEventBytes_ = value;
+        bitField0_ |= 0x00000200;
         onChanged();
         return this;
       }
       /**
-       * <code>optional bytes initial_event_bytes = 10;</code>
-       *
        * <pre>
        * Serialized (and signed) NotTezEvent; used only for external clients for now.
        * </pre>
+       *
+       * <code>optional bytes initial_event_bytes = 10;</code>
+       * @return This builder for chaining.
        */
       public Builder clearInitialEventBytes() {
         bitField0_ = (bitField0_ & ~0x00000200);
@@ -11533,34 +12598,38 @@ public Builder clearInitialEventBytes() {
         return this;
       }
 
-      // optional bytes initial_event_signature = 11;
       private com.google.protobuf.ByteString initialEventSignature_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>optional bytes initial_event_signature = 11;</code>
+       * @return Whether the initialEventSignature field is set.
        */
+      @java.lang.Override
       public boolean hasInitialEventSignature() {
-        return ((bitField0_ & 0x00000400) == 0x00000400);
+        return ((bitField0_ & 0x00000400) != 0);
       }
       /**
        * <code>optional bytes initial_event_signature = 11;</code>
+       * @return The initialEventSignature.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getInitialEventSignature() {
         return initialEventSignature_;
       }
       /**
        * <code>optional bytes initial_event_signature = 11;</code>
+       * @param value The initialEventSignature to set.
+       * @return This builder for chaining.
        */
       public Builder setInitialEventSignature(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000400;
+        if (value == null) { throw new NullPointerException(); }
         initialEventSignature_ = value;
+        bitField0_ |= 0x00000400;
         onChanged();
         return this;
       }
       /**
        * <code>optional bytes initial_event_signature = 11;</code>
+       * @return This builder for chaining.
        */
       public Builder clearInitialEventSignature() {
         bitField0_ = (bitField0_ & ~0x00000400);
@@ -11569,31 +12638,38 @@ public Builder clearInitialEventSignature() {
         return this;
       }
 
-      // optional bool is_guaranteed = 12 [default = false];
       private boolean isGuaranteed_ ;
       /**
        * <code>optional bool is_guaranteed = 12 [default = false];</code>
+       * @return Whether the isGuaranteed field is set.
        */
+      @java.lang.Override
       public boolean hasIsGuaranteed() {
-        return ((bitField0_ & 0x00000800) == 0x00000800);
+        return ((bitField0_ & 0x00000800) != 0);
       }
       /**
        * <code>optional bool is_guaranteed = 12 [default = false];</code>
+       * @return The isGuaranteed.
        */
+      @java.lang.Override
       public boolean getIsGuaranteed() {
         return isGuaranteed_;
       }
       /**
        * <code>optional bool is_guaranteed = 12 [default = false];</code>
+       * @param value The isGuaranteed to set.
+       * @return This builder for chaining.
        */
       public Builder setIsGuaranteed(boolean value) {
-        bitField0_ |= 0x00000800;
+
         isGuaranteed_ = value;
+        bitField0_ |= 0x00000800;
         onChanged();
         return this;
       }
       /**
        * <code>optional bool is_guaranteed = 12 [default = false];</code>
+       * @return This builder for chaining.
        */
       public Builder clearIsGuaranteed() {
         bitField0_ = (bitField0_ & ~0x00000800);
@@ -11602,23 +12678,27 @@ public Builder clearIsGuaranteed() {
         return this;
       }
 
-      // optional string jwt = 13;
       private java.lang.Object jwt_ = "";
       /**
        * <code>optional string jwt = 13;</code>
+       * @return Whether the jwt field is set.
        */
       public boolean hasJwt() {
-        return ((bitField0_ & 0x00001000) == 0x00001000);
+        return ((bitField0_ & 0x00001000) != 0);
       }
       /**
        * <code>optional string jwt = 13;</code>
+       * @return The jwt.
        */
       public java.lang.String getJwt() {
         java.lang.Object ref = jwt_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          jwt_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            jwt_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -11626,6 +12706,7 @@ public java.lang.String getJwt() {
       }
       /**
        * <code>optional string jwt = 13;</code>
+       * @return The bytes for jwt.
        */
       public com.google.protobuf.ByteString
           getJwtBytes() {
@@ -11642,65 +12723,73 @@ public java.lang.String getJwt() {
       }
       /**
        * <code>optional string jwt = 13;</code>
+       * @param value The jwt to set.
+       * @return This builder for chaining.
        */
       public Builder setJwt(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00001000;
+        if (value == null) { throw new NullPointerException(); }
         jwt_ = value;
+        bitField0_ |= 0x00001000;
         onChanged();
         return this;
       }
       /**
        * <code>optional string jwt = 13;</code>
+       * @return This builder for chaining.
        */
       public Builder clearJwt() {
-        bitField0_ = (bitField0_ & ~0x00001000);
         jwt_ = getDefaultInstance().getJwt();
+        bitField0_ = (bitField0_ & ~0x00001000);
         onChanged();
         return this;
       }
       /**
        * <code>optional string jwt = 13;</code>
+       * @param value The bytes for jwt to set.
+       * @return This builder for chaining.
        */
       public Builder setJwtBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00001000;
+        if (value == null) { throw new NullPointerException(); }
         jwt_ = value;
+        bitField0_ |= 0x00001000;
         onChanged();
         return this;
       }
 
-      // optional bool is_external_client_request = 14 [default = false];
       private boolean isExternalClientRequest_ ;
       /**
        * <code>optional bool is_external_client_request = 14 [default = false];</code>
+       * @return Whether the isExternalClientRequest field is set.
        */
+      @java.lang.Override
       public boolean hasIsExternalClientRequest() {
-        return ((bitField0_ & 0x00002000) == 0x00002000);
+        return ((bitField0_ & 0x00002000) != 0);
       }
       /**
        * <code>optional bool is_external_client_request = 14 [default = false];</code>
+       * @return The isExternalClientRequest.
        */
+      @java.lang.Override
       public boolean getIsExternalClientRequest() {
         return isExternalClientRequest_;
       }
       /**
        * <code>optional bool is_external_client_request = 14 [default = false];</code>
+       * @param value The isExternalClientRequest to set.
+       * @return This builder for chaining.
        */
       public Builder setIsExternalClientRequest(boolean value) {
-        bitField0_ |= 0x00002000;
+
         isExternalClientRequest_ = value;
+        bitField0_ |= 0x00002000;
         onChanged();
         return this;
       }
       /**
        * <code>optional bool is_external_client_request = 14 [default = false];</code>
+       * @return This builder for chaining.
        */
       public Builder clearIsExternalClientRequest() {
         bitField0_ = (bitField0_ & ~0x00002000);
@@ -11708,43 +12797,99 @@ public Builder clearIsExternalClientRequest() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:SubmitWorkRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:SubmitWorkRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new SubmitWorkRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<SubmitWorkRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<SubmitWorkRequestProto>() {
+      @java.lang.Override
+      public SubmitWorkRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<SubmitWorkRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<SubmitWorkRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:SubmitWorkRequestProto)
   }
 
-  public interface RegisterDagRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface RegisterDagRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:RegisterDagRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string user = 1;
     /**
      * <code>optional string user = 1;</code>
+     * @return Whether the user field is set.
      */
     boolean hasUser();
     /**
      * <code>optional string user = 1;</code>
+     * @return The user.
      */
     java.lang.String getUser();
     /**
      * <code>optional string user = 1;</code>
+     * @return The bytes for user.
      */
     com.google.protobuf.ByteString
         getUserBytes();
 
-    // required .QueryIdentifierProto query_identifier = 2;
     /**
      * <code>required .QueryIdentifierProto query_identifier = 2;</code>
+     * @return Whether the queryIdentifier field is set.
      */
     boolean hasQueryIdentifier();
     /**
      * <code>required .QueryIdentifierProto query_identifier = 2;</code>
+     * @return The queryIdentifier.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
     /**
@@ -11752,13 +12897,14 @@ public interface RegisterDagRequestProtoOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();
 
-    // optional bytes credentials_binary = 3;
     /**
      * <code>optional bytes credentials_binary = 3;</code>
+     * @return Whether the credentialsBinary field is set.
      */
     boolean hasCredentialsBinary();
     /**
      * <code>optional bytes credentials_binary = 3;</code>
+     * @return The credentialsBinary.
      */
     com.google.protobuf.ByteString getCredentialsBinary();
   }
@@ -11766,128 +12912,56 @@ public interface RegisterDagRequestProtoOrBuilder
    * Protobuf type {@code RegisterDagRequestProto}
    */
   public static final class RegisterDagRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements RegisterDagRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:RegisterDagRequestProto)
+      RegisterDagRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use RegisterDagRequestProto.newBuilder() to construct.
-    private RegisterDagRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private RegisterDagRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private RegisterDagRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final RegisterDagRequestProto defaultInstance;
-    public static RegisterDagRequestProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public RegisterDagRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private RegisterDagRequestProto() {
+      user_ = "";
+      credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private RegisterDagRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              user_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000002) == 0x00000002)) {
-                subBuilder = queryIdentifier_.toBuilder();
-              }
-              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(queryIdentifier_);
-                queryIdentifier_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000002;
-              break;
-            }
-            case 26: {
-              bitField0_ |= 0x00000004;
-              credentialsBinary_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new RegisterDagRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<RegisterDagRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<RegisterDagRequestProto>() {
-      public RegisterDagRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new RegisterDagRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<RegisterDagRequestProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string user = 1;
     public static final int USER_FIELD_NUMBER = 1;
-    private java.lang.Object user_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object user_ = "";
     /**
      * <code>optional string user = 1;</code>
+     * @return Whether the user field is set.
      */
+    @java.lang.Override
     public boolean hasUser() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string user = 1;</code>
+     * @return The user.
      */
+    @java.lang.Override
     public java.lang.String getUser() {
       java.lang.Object ref = user_;
       if (ref instanceof java.lang.String) {
@@ -11904,7 +12978,9 @@ public java.lang.String getUser() {
     }
     /**
      * <code>optional string user = 1;</code>
+     * @return The bytes for user.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getUserBytes() {
       java.lang.Object ref = user_;
@@ -11919,53 +12995,57 @@ public java.lang.String getUser() {
       }
     }
 
-    // required .QueryIdentifierProto query_identifier = 2;
     public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 2;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
     /**
      * <code>required .QueryIdentifierProto query_identifier = 2;</code>
+     * @return Whether the queryIdentifier field is set.
      */
+    @java.lang.Override
     public boolean hasQueryIdentifier() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>required .QueryIdentifierProto query_identifier = 2;</code>
+     * @return The queryIdentifier.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
     /**
      * <code>required .QueryIdentifierProto query_identifier = 2;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
 
-    // optional bytes credentials_binary = 3;
     public static final int CREDENTIALS_BINARY_FIELD_NUMBER = 3;
-    private com.google.protobuf.ByteString credentialsBinary_;
+    private com.google.protobuf.ByteString credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>optional bytes credentials_binary = 3;</code>
+     * @return Whether the credentialsBinary field is set.
      */
+    @java.lang.Override
     public boolean hasCredentialsBinary() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional bytes credentials_binary = 3;</code>
+     * @return The credentialsBinary.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getCredentialsBinary() {
       return credentialsBinary_;
     }
 
-    private void initFields() {
-      user_ = "";
-      queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       if (!hasQueryIdentifier()) {
         memoizedIsInitialized = 0;
@@ -11975,51 +13055,43 @@ public final boolean isInitialized() {
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getUserBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, user_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeMessage(2, queryIdentifier_);
+      if (((bitField0_ & 0x00000002) != 0)) {
+        output.writeMessage(2, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         output.writeBytes(3, credentialsBinary_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getUserBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, user_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(2, queryIdentifier_);
+          .computeMessageSize(2, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(3, credentialsBinary_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -12030,35 +13102,32 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasUser() == other.hasUser());
+      if (hasUser() != other.hasUser()) return false;
       if (hasUser()) {
-        result = result && getUser()
-            .equals(other.getUser());
+        if (!getUser()
+            .equals(other.getUser())) return false;
       }
-      result = result && (hasQueryIdentifier() == other.hasQueryIdentifier());
+      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
       if (hasQueryIdentifier()) {
-        result = result && getQueryIdentifier()
-            .equals(other.getQueryIdentifier());
+        if (!getQueryIdentifier()
+            .equals(other.getQueryIdentifier())) return false;
       }
-      result = result && (hasCredentialsBinary() == other.hasCredentialsBinary());
+      if (hasCredentialsBinary() != other.hasCredentialsBinary()) return false;
       if (hasCredentialsBinary()) {
-        result = result && getCredentialsBinary()
-            .equals(other.getCredentialsBinary());
+        if (!getCredentialsBinary()
+            .equals(other.getCredentialsBinary())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasUser()) {
         hash = (37 * hash) + USER_FIELD_NUMBER;
         hash = (53 * hash) + getUser().hashCode();
@@ -12076,6 +13145,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -12099,46 +13179,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Re
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -12146,14 +13241,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code RegisterDagRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:RegisterDagRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -12166,47 +13263,42 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getQueryIdentifierFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         user_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-        } else {
-          queryIdentifierBuilder_.clear();
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000002);
         credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -12215,31 +13307,67 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterD
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.user_ = user_;
           to_bitField0_ |= 0x00000001;
         }
-        result.user_ = user_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.queryIdentifier_ = queryIdentifierBuilder_ == null
+              ? queryIdentifier_
+              : queryIdentifierBuilder_.build();
           to_bitField0_ |= 0x00000002;
         }
-        if (queryIdentifierBuilder_ == null) {
-          result.queryIdentifier_ = queryIdentifier_;
-        } else {
-          result.queryIdentifier_ = queryIdentifierBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.credentialsBinary_ = credentialsBinary_;
           to_bitField0_ |= 0x00000004;
         }
-        result.credentialsBinary_ = credentialsBinary_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto)other);
@@ -12252,8 +13380,8 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.getDefaultInstance()) return this;
         if (other.hasUser()) {
-          bitField0_ |= 0x00000001;
           user_ = other.user_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasQueryIdentifier()) {
@@ -12263,53 +13391,89 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setCredentialsBinary(other.getCredentialsBinary());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         if (!hasQueryIdentifier()) {
-          
           return false;
         }
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                user_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                input.readMessage(
+                    getQueryIdentifierFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 26: {
+                credentialsBinary_ = input.readBytes();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 26
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string user = 1;
       private java.lang.Object user_ = "";
       /**
        * <code>optional string user = 1;</code>
+       * @return Whether the user field is set.
        */
       public boolean hasUser() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string user = 1;</code>
+       * @return The user.
        */
       public java.lang.String getUser() {
         java.lang.Object ref = user_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          user_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            user_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -12317,6 +13481,7 @@ public java.lang.String getUser() {
       }
       /**
        * <code>optional string user = 1;</code>
+       * @return The bytes for user.
        */
       public com.google.protobuf.ByteString
           getUserBytes() {
@@ -12333,56 +13498,58 @@ public java.lang.String getUser() {
       }
       /**
        * <code>optional string user = 1;</code>
+       * @param value The user to set.
+       * @return This builder for chaining.
        */
       public Builder setUser(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         user_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string user = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearUser() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         user_ = getDefaultInstance().getUser();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string user = 1;</code>
+       * @param value The bytes for user to set.
+       * @return This builder for chaining.
        */
       public Builder setUserBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         user_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // required .QueryIdentifierProto query_identifier = 2;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
       /**
        * <code>required .QueryIdentifierProto query_identifier = 2;</code>
+       * @return Whether the queryIdentifier field is set.
        */
       public boolean hasQueryIdentifier() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>required .QueryIdentifierProto query_identifier = 2;</code>
+       * @return The queryIdentifier.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
         if (queryIdentifierBuilder_ == null) {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         } else {
           return queryIdentifierBuilder_.getMessage();
         }
@@ -12396,11 +13563,11 @@ public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDae
             throw new NullPointerException();
           }
           queryIdentifier_ = value;
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000002;
+        onChanged();
         return this;
       }
       /**
@@ -12410,11 +13577,11 @@ public Builder setQueryIdentifier(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
         if (queryIdentifierBuilder_ == null) {
           queryIdentifier_ = builderForValue.build();
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000002;
+        onChanged();
         return this;
       }
       /**
@@ -12422,31 +13589,33 @@ public Builder setQueryIdentifier(
        */
       public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
         if (queryIdentifierBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002) &&
-              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
-            queryIdentifier_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000002) != 0) &&
+            queryIdentifier_ != null &&
+            queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
+            getQueryIdentifierBuilder().mergeFrom(value);
           } else {
             queryIdentifier_ = value;
           }
-          onChanged();
         } else {
           queryIdentifierBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000002;
+        if (queryIdentifier_ != null) {
+          bitField0_ |= 0x00000002;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>required .QueryIdentifierProto query_identifier = 2;</code>
        */
       public Builder clearQueryIdentifier() {
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-          onChanged();
-        } else {
-          queryIdentifierBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000002);
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -12464,19 +13633,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         if (queryIdentifierBuilder_ != null) {
           return queryIdentifierBuilder_.getMessageOrBuilder();
         } else {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         }
       }
       /**
        * <code>required .QueryIdentifierProto query_identifier = 2;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
           getQueryIdentifierFieldBuilder() {
         if (queryIdentifierBuilder_ == null) {
-          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
-                  queryIdentifier_,
+                  getQueryIdentifier(),
                   getParentForChildren(),
                   isClean());
           queryIdentifier_ = null;
@@ -12484,34 +13654,38 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         return queryIdentifierBuilder_;
       }
 
-      // optional bytes credentials_binary = 3;
       private com.google.protobuf.ByteString credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>optional bytes credentials_binary = 3;</code>
+       * @return Whether the credentialsBinary field is set.
        */
+      @java.lang.Override
       public boolean hasCredentialsBinary() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional bytes credentials_binary = 3;</code>
+       * @return The credentialsBinary.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getCredentialsBinary() {
         return credentialsBinary_;
       }
       /**
        * <code>optional bytes credentials_binary = 3;</code>
+       * @param value The credentialsBinary to set.
+       * @return This builder for chaining.
        */
       public Builder setCredentialsBinary(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000004;
+        if (value == null) { throw new NullPointerException(); }
         credentialsBinary_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>optional bytes credentials_binary = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearCredentialsBinary() {
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -12519,145 +13693,137 @@ public Builder clearCredentialsBinary() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:RegisterDagRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:RegisterDagRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new RegisterDagRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto();
     }
 
-    // @@protoc_insertion_point(class_scope:RegisterDagRequestProto)
-  }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
 
-  public interface RegisterDagResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-  }
-  /**
-   * Protobuf type {@code RegisterDagResponseProto}
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<RegisterDagRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<RegisterDagRequestProto>() {
+      @java.lang.Override
+      public RegisterDagRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<RegisterDagRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<RegisterDagRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
+    }
+
+  }
+
+  public interface RegisterDagResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:RegisterDagResponseProto)
+      com.google.protobuf.MessageOrBuilder {
+  }
+  /**
+   * Protobuf type {@code RegisterDagResponseProto}
    */
   public static final class RegisterDagResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements RegisterDagResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:RegisterDagResponseProto)
+      RegisterDagResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use RegisterDagResponseProto.newBuilder() to construct.
-    private RegisterDagResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private RegisterDagResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private RegisterDagResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final RegisterDagResponseProto defaultInstance;
-    public static RegisterDagResponseProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public RegisterDagResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private RegisterDagResponseProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private RegisterDagResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new RegisterDagResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<RegisterDagResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<RegisterDagResponseProto>() {
-      public RegisterDagResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new RegisterDagResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<RegisterDagResponseProto> getParserForType() {
-      return PARSER;
-    }
-
-    private void initFields() {
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -12668,25 +13834,33 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto) obj;
 
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -12710,46 +13884,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Re
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -12757,14 +13946,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code RegisterDagResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:RegisterDagResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -12773,40 +13964,32 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -12815,12 +13998,46 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterD
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto(this);
         onBuilt();
         return result;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto)other);
@@ -12833,66 +14050,138 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.getDefaultInstance()) return this;
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:RegisterDagResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:RegisterDagResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new RegisterDagResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<RegisterDagResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<RegisterDagResponseProto>() {
+      @java.lang.Override
+      public RegisterDagResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<RegisterDagResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<RegisterDagResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:RegisterDagResponseProto)
   }
 
-  public interface SubmitWorkResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface SubmitWorkResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:SubmitWorkResponseProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional .SubmissionStateProto submission_state = 1;
     /**
      * <code>optional .SubmissionStateProto submission_state = 1;</code>
+     * @return Whether the submissionState field is set.
      */
     boolean hasSubmissionState();
     /**
      * <code>optional .SubmissionStateProto submission_state = 1;</code>
+     * @return The submissionState.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto getSubmissionState();
 
-    // optional string unique_node_id = 2;
     /**
      * <code>optional string unique_node_id = 2;</code>
+     * @return Whether the uniqueNodeId field is set.
      */
     boolean hasUniqueNodeId();
     /**
      * <code>optional string unique_node_id = 2;</code>
+     * @return The uniqueNodeId.
      */
     java.lang.String getUniqueNodeId();
     /**
      * <code>optional string unique_node_id = 2;</code>
+     * @return The bytes for uniqueNodeId.
      */
     com.google.protobuf.ByteString
         getUniqueNodeIdBytes();
@@ -12901,137 +14190,74 @@ public interface SubmitWorkResponseProtoOrBuilder
    * Protobuf type {@code SubmitWorkResponseProto}
    */
   public static final class SubmitWorkResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements SubmitWorkResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:SubmitWorkResponseProto)
+      SubmitWorkResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use SubmitWorkResponseProto.newBuilder() to construct.
-    private SubmitWorkResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private SubmitWorkResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private SubmitWorkResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final SubmitWorkResponseProto defaultInstance;
-    public static SubmitWorkResponseProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public SubmitWorkResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private SubmitWorkResponseProto() {
+      submissionState_ = 1;
+      uniqueNodeId_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private SubmitWorkResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              int rawValue = input.readEnum();
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto value = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(1, rawValue);
-              } else {
-                bitField0_ |= 0x00000001;
-                submissionState_ = value;
-              }
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              uniqueNodeId_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new SubmitWorkResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<SubmitWorkResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<SubmitWorkResponseProto>() {
-      public SubmitWorkResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new SubmitWorkResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<SubmitWorkResponseProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional .SubmissionStateProto submission_state = 1;
     public static final int SUBMISSION_STATE_FIELD_NUMBER = 1;
-    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto submissionState_;
+    private int submissionState_ = 1;
     /**
      * <code>optional .SubmissionStateProto submission_state = 1;</code>
+     * @return Whether the submissionState field is set.
      */
-    public boolean hasSubmissionState() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+    @java.lang.Override public boolean hasSubmissionState() {
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional .SubmissionStateProto submission_state = 1;</code>
+     * @return The submissionState.
      */
-    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto getSubmissionState() {
-      return submissionState_;
+    @java.lang.Override public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto getSubmissionState() {
+      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto result = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.forNumber(submissionState_);
+      return result == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.ACCEPTED : result;
     }
 
-    // optional string unique_node_id = 2;
     public static final int UNIQUE_NODE_ID_FIELD_NUMBER = 2;
-    private java.lang.Object uniqueNodeId_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object uniqueNodeId_ = "";
     /**
      * <code>optional string unique_node_id = 2;</code>
+     * @return Whether the uniqueNodeId field is set.
      */
+    @java.lang.Override
     public boolean hasUniqueNodeId() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional string unique_node_id = 2;</code>
+     * @return The uniqueNodeId.
      */
+    @java.lang.Override
     public java.lang.String getUniqueNodeId() {
       java.lang.Object ref = uniqueNodeId_;
       if (ref instanceof java.lang.String) {
@@ -13048,7 +14274,9 @@ public java.lang.String getUniqueNodeId() {
     }
     /**
      * <code>optional string unique_node_id = 2;</code>
+     * @return The bytes for uniqueNodeId.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getUniqueNodeIdBytes() {
       java.lang.Object ref = uniqueNodeId_;
@@ -13063,57 +14291,47 @@ public java.lang.String getUniqueNodeId() {
       }
     }
 
-    private void initFields() {
-      submissionState_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.ACCEPTED;
-      uniqueNodeId_ = "";
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeEnum(1, submissionState_.getNumber());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        output.writeEnum(1, submissionState_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getUniqueNodeIdBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, uniqueNodeId_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeEnumSize(1, submissionState_.getNumber());
+          .computeEnumSize(1, submissionState_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getUniqueNodeIdBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, uniqueNodeId_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -13124,33 +14342,29 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto) obj;
 
-      boolean result = true;
-      result = result && (hasSubmissionState() == other.hasSubmissionState());
+      if (hasSubmissionState() != other.hasSubmissionState()) return false;
       if (hasSubmissionState()) {
-        result = result &&
-            (getSubmissionState() == other.getSubmissionState());
+        if (submissionState_ != other.submissionState_) return false;
       }
-      result = result && (hasUniqueNodeId() == other.hasUniqueNodeId());
+      if (hasUniqueNodeId() != other.hasUniqueNodeId()) return false;
       if (hasUniqueNodeId()) {
-        result = result && getUniqueNodeId()
-            .equals(other.getUniqueNodeId());
+        if (!getUniqueNodeId()
+            .equals(other.getUniqueNodeId())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasSubmissionState()) {
         hash = (37 * hash) + SUBMISSION_STATE_FIELD_NUMBER;
-        hash = (53 * hash) + hashEnum(getSubmissionState());
+        hash = (53 * hash) + submissionState_;
       }
       if (hasUniqueNodeId()) {
         hash = (37 * hash) + UNIQUE_NODE_ID_FIELD_NUMBER;
@@ -13161,6 +14375,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -13184,46 +14409,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Su
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -13231,14 +14471,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code SubmitWorkResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:SubmitWorkResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -13247,44 +14489,35 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
-        submissionState_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.ACCEPTED;
-        bitField0_ = (bitField0_ & ~0x00000001);
+        bitField0_ = 0;
+        submissionState_ = 1;
         uniqueNodeId_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -13293,23 +14526,61 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWor
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.submissionState_ = submissionState_;
           to_bitField0_ |= 0x00000001;
         }
-        result.submissionState_ = submissionState_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.uniqueNodeId_ = uniqueNodeId_;
           to_bitField0_ |= 0x00000002;
         }
-        result.uniqueNodeId_ = uniqueNodeId_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto)other);
@@ -13325,90 +14596,133 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setSubmissionState(other.getSubmissionState());
         }
         if (other.hasUniqueNodeId()) {
-          bitField0_ |= 0x00000002;
           uniqueNodeId_ = other.uniqueNodeId_;
+          bitField0_ |= 0x00000002;
           onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 8: {
+                int tmpRaw = input.readEnum();
+                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto tmpValue =
+                    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.forNumber(tmpRaw);
+                if (tmpValue == null) {
+                  mergeUnknownVarintField(1, tmpRaw);
+                } else {
+                  submissionState_ = tmpRaw;
+                  bitField0_ |= 0x00000001;
+                }
+                break;
+              } // case 8
+              case 18: {
+                uniqueNodeId_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional .SubmissionStateProto submission_state = 1;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto submissionState_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.ACCEPTED;
+      private int submissionState_ = 1;
       /**
        * <code>optional .SubmissionStateProto submission_state = 1;</code>
+       * @return Whether the submissionState field is set.
        */
-      public boolean hasSubmissionState() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+      @java.lang.Override public boolean hasSubmissionState() {
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional .SubmissionStateProto submission_state = 1;</code>
+       * @return The submissionState.
        */
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto getSubmissionState() {
-        return submissionState_;
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto result = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.forNumber(submissionState_);
+        return result == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.ACCEPTED : result;
       }
       /**
        * <code>optional .SubmissionStateProto submission_state = 1;</code>
+       * @param value The submissionState to set.
+       * @return This builder for chaining.
        */
       public Builder setSubmissionState(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto value) {
         if (value == null) {
           throw new NullPointerException();
         }
         bitField0_ |= 0x00000001;
-        submissionState_ = value;
+        submissionState_ = value.getNumber();
         onChanged();
         return this;
       }
       /**
        * <code>optional .SubmissionStateProto submission_state = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearSubmissionState() {
         bitField0_ = (bitField0_ & ~0x00000001);
-        submissionState_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.ACCEPTED;
+        submissionState_ = 1;
         onChanged();
         return this;
       }
 
-      // optional string unique_node_id = 2;
       private java.lang.Object uniqueNodeId_ = "";
       /**
        * <code>optional string unique_node_id = 2;</code>
+       * @return Whether the uniqueNodeId field is set.
        */
       public boolean hasUniqueNodeId() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional string unique_node_id = 2;</code>
+       * @return The uniqueNodeId.
        */
       public java.lang.String getUniqueNodeId() {
         java.lang.Object ref = uniqueNodeId_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          uniqueNodeId_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            uniqueNodeId_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -13416,6 +14730,7 @@ public java.lang.String getUniqueNodeId() {
       }
       /**
        * <code>optional string unique_node_id = 2;</code>
+       * @return The bytes for uniqueNodeId.
        */
       public com.google.protobuf.ByteString
           getUniqueNodeIdBytes() {
@@ -13432,61 +14747,116 @@ public java.lang.String getUniqueNodeId() {
       }
       /**
        * <code>optional string unique_node_id = 2;</code>
+       * @param value The uniqueNodeId to set.
+       * @return This builder for chaining.
        */
       public Builder setUniqueNodeId(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         uniqueNodeId_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional string unique_node_id = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearUniqueNodeId() {
-        bitField0_ = (bitField0_ & ~0x00000002);
         uniqueNodeId_ = getDefaultInstance().getUniqueNodeId();
+        bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
       /**
        * <code>optional string unique_node_id = 2;</code>
+       * @param value The bytes for uniqueNodeId to set.
+       * @return This builder for chaining.
        */
       public Builder setUniqueNodeIdBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         uniqueNodeId_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:SubmitWorkResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:SubmitWorkResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new SubmitWorkResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<SubmitWorkResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<SubmitWorkResponseProto>() {
+      @java.lang.Override
+      public SubmitWorkResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<SubmitWorkResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<SubmitWorkResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:SubmitWorkResponseProto)
   }
 
-  public interface SourceStateUpdatedRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface SourceStateUpdatedRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:SourceStateUpdatedRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional .QueryIdentifierProto query_identifier = 1;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return Whether the queryIdentifier field is set.
      */
     boolean hasQueryIdentifier();
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return The queryIdentifier.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
     /**
@@ -13494,28 +14864,31 @@ public interface SourceStateUpdatedRequestProtoOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();
 
-    // optional string src_name = 2;
     /**
      * <code>optional string src_name = 2;</code>
+     * @return Whether the srcName field is set.
      */
     boolean hasSrcName();
     /**
      * <code>optional string src_name = 2;</code>
+     * @return The srcName.
      */
     java.lang.String getSrcName();
     /**
      * <code>optional string src_name = 2;</code>
+     * @return The bytes for srcName.
      */
     com.google.protobuf.ByteString
         getSrcNameBytes();
 
-    // optional .SourceStateProto state = 3;
     /**
      * <code>optional .SourceStateProto state = 3;</code>
+     * @return Whether the state field is set.
      */
     boolean hasState();
     /**
      * <code>optional .SourceStateProto state = 3;</code>
+     * @return The state.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto getState();
   }
@@ -13523,156 +14896,82 @@ public interface SourceStateUpdatedRequestProtoOrBuilder
    * Protobuf type {@code SourceStateUpdatedRequestProto}
    */
   public static final class SourceStateUpdatedRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements SourceStateUpdatedRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:SourceStateUpdatedRequestProto)
+      SourceStateUpdatedRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use SourceStateUpdatedRequestProto.newBuilder() to construct.
-    private SourceStateUpdatedRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private SourceStateUpdatedRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private SourceStateUpdatedRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final SourceStateUpdatedRequestProto defaultInstance;
-    public static SourceStateUpdatedRequestProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public SourceStateUpdatedRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private SourceStateUpdatedRequestProto() {
+      srcName_ = "";
+      state_ = 1;
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private SourceStateUpdatedRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = queryIdentifier_.toBuilder();
-              }
-              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(queryIdentifier_);
-                queryIdentifier_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              srcName_ = input.readBytes();
-              break;
-            }
-            case 24: {
-              int rawValue = input.readEnum();
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto value = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(3, rawValue);
-              } else {
-                bitField0_ |= 0x00000004;
-                state_ = value;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new SourceStateUpdatedRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<SourceStateUpdatedRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<SourceStateUpdatedRequestProto>() {
-      public SourceStateUpdatedRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new SourceStateUpdatedRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<SourceStateUpdatedRequestProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional .QueryIdentifierProto query_identifier = 1;
     public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return Whether the queryIdentifier field is set.
      */
+    @java.lang.Override
     public boolean hasQueryIdentifier() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return The queryIdentifier.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
 
-    // optional string src_name = 2;
     public static final int SRC_NAME_FIELD_NUMBER = 2;
-    private java.lang.Object srcName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object srcName_ = "";
     /**
      * <code>optional string src_name = 2;</code>
+     * @return Whether the srcName field is set.
      */
+    @java.lang.Override
     public boolean hasSrcName() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional string src_name = 2;</code>
+     * @return The srcName.
      */
+    @java.lang.Override
     public java.lang.String getSrcName() {
       java.lang.Object ref = srcName_;
       if (ref instanceof java.lang.String) {
@@ -13689,7 +14988,9 @@ public java.lang.String getSrcName() {
     }
     /**
      * <code>optional string src_name = 2;</code>
+     * @return The bytes for srcName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getSrcNameBytes() {
       java.lang.Object ref = srcName_;
@@ -13704,81 +15005,72 @@ public java.lang.String getSrcName() {
       }
     }
 
-    // optional .SourceStateProto state = 3;
     public static final int STATE_FIELD_NUMBER = 3;
-    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto state_;
+    private int state_ = 1;
     /**
      * <code>optional .SourceStateProto state = 3;</code>
+     * @return Whether the state field is set.
      */
-    public boolean hasState() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+    @java.lang.Override public boolean hasState() {
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional .SourceStateProto state = 3;</code>
+     * @return The state.
      */
-    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto getState() {
-      return state_;
+    @java.lang.Override public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto getState() {
+      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto result = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.forNumber(state_);
+      return result == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.S_SUCCEEDED : result;
     }
 
-    private void initFields() {
-      queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      srcName_ = "";
-      state_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.S_SUCCEEDED;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, queryIdentifier_);
+      if (((bitField0_ & 0x00000001) != 0)) {
+        output.writeMessage(1, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getSrcNameBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, srcName_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeEnum(3, state_.getNumber());
+      if (((bitField0_ & 0x00000004) != 0)) {
+        output.writeEnum(3, state_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, queryIdentifier_);
+          .computeMessageSize(1, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getSrcNameBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, srcName_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeEnumSize(3, state_.getNumber());
+          .computeEnumSize(3, state_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -13789,35 +15081,31 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasQueryIdentifier() == other.hasQueryIdentifier());
+      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
       if (hasQueryIdentifier()) {
-        result = result && getQueryIdentifier()
-            .equals(other.getQueryIdentifier());
+        if (!getQueryIdentifier()
+            .equals(other.getQueryIdentifier())) return false;
       }
-      result = result && (hasSrcName() == other.hasSrcName());
+      if (hasSrcName() != other.hasSrcName()) return false;
       if (hasSrcName()) {
-        result = result && getSrcName()
-            .equals(other.getSrcName());
+        if (!getSrcName()
+            .equals(other.getSrcName())) return false;
       }
-      result = result && (hasState() == other.hasState());
+      if (hasState() != other.hasState()) return false;
       if (hasState()) {
-        result = result &&
-            (getState() == other.getState());
+        if (state_ != other.state_) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasQueryIdentifier()) {
         hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
         hash = (53 * hash) + getQueryIdentifier().hashCode();
@@ -13828,13 +15116,24 @@ public int hashCode() {
       }
       if (hasState()) {
         hash = (37 * hash) + STATE_FIELD_NUMBER;
-        hash = (53 * hash) + hashEnum(getState());
+        hash = (53 * hash) + state_;
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -13858,46 +15157,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.So
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -13905,14 +15219,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code SourceStateUpdatedRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:SourceStateUpdatedRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -13925,47 +15241,42 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getQueryIdentifierFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-        } else {
-          queryIdentifierBuilder_.clear();
+        bitField0_ = 0;
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000001);
         srcName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
-        state_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.S_SUCCEEDED;
-        bitField0_ = (bitField0_ & ~0x00000004);
+        state_ = 1;
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -13974,31 +15285,67 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceSta
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.queryIdentifier_ = queryIdentifierBuilder_ == null
+              ? queryIdentifier_
+              : queryIdentifierBuilder_.build();
           to_bitField0_ |= 0x00000001;
         }
-        if (queryIdentifierBuilder_ == null) {
-          result.queryIdentifier_ = queryIdentifier_;
-        } else {
-          result.queryIdentifier_ = queryIdentifierBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.srcName_ = srcName_;
           to_bitField0_ |= 0x00000002;
         }
-        result.srcName_ = srcName_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.state_ = state_;
           to_bitField0_ |= 0x00000004;
         }
-        result.state_ = state_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto)other);
@@ -14014,56 +15361,97 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           mergeQueryIdentifier(other.getQueryIdentifier());
         }
         if (other.hasSrcName()) {
-          bitField0_ |= 0x00000002;
           srcName_ = other.srcName_;
+          bitField0_ |= 0x00000002;
           onChanged();
         }
         if (other.hasState()) {
           setState(other.getState());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                input.readMessage(
+                    getQueryIdentifierFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                srcName_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 24: {
+                int tmpRaw = input.readEnum();
+                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto tmpValue =
+                    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.forNumber(tmpRaw);
+                if (tmpValue == null) {
+                  mergeUnknownVarintField(3, tmpRaw);
+                } else {
+                  state_ = tmpRaw;
+                  bitField0_ |= 0x00000004;
+                }
+                break;
+              } // case 24
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional .QueryIdentifierProto query_identifier = 1;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+       * @return Whether the queryIdentifier field is set.
        */
       public boolean hasQueryIdentifier() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+       * @return The queryIdentifier.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
         if (queryIdentifierBuilder_ == null) {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         } else {
           return queryIdentifierBuilder_.getMessage();
         }
@@ -14077,11 +15465,11 @@ public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDae
             throw new NullPointerException();
           }
           queryIdentifier_ = value;
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -14091,11 +15479,11 @@ public Builder setQueryIdentifier(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
         if (queryIdentifierBuilder_ == null) {
           queryIdentifier_ = builderForValue.build();
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -14103,31 +15491,33 @@ public Builder setQueryIdentifier(
        */
       public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
         if (queryIdentifierBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
-            queryIdentifier_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000001) != 0) &&
+            queryIdentifier_ != null &&
+            queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
+            getQueryIdentifierBuilder().mergeFrom(value);
           } else {
             queryIdentifier_ = value;
           }
-          onChanged();
         } else {
           queryIdentifierBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000001;
+        if (queryIdentifier_ != null) {
+          bitField0_ |= 0x00000001;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
        */
       public Builder clearQueryIdentifier() {
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-          onChanged();
-        } else {
-          queryIdentifierBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000001);
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -14145,19 +15535,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         if (queryIdentifierBuilder_ != null) {
           return queryIdentifierBuilder_.getMessageOrBuilder();
         } else {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         }
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
           getQueryIdentifierFieldBuilder() {
         if (queryIdentifierBuilder_ == null) {
-          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
-                  queryIdentifier_,
+                  getQueryIdentifier(),
                   getParentForChildren(),
                   isClean());
           queryIdentifier_ = null;
@@ -14165,23 +15556,27 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         return queryIdentifierBuilder_;
       }
 
-      // optional string src_name = 2;
       private java.lang.Object srcName_ = "";
       /**
        * <code>optional string src_name = 2;</code>
+       * @return Whether the srcName field is set.
        */
       public boolean hasSrcName() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional string src_name = 2;</code>
+       * @return The srcName.
        */
       public java.lang.String getSrcName() {
         java.lang.Object ref = srcName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          srcName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            srcName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -14189,6 +15584,7 @@ public java.lang.String getSrcName() {
       }
       /**
        * <code>optional string src_name = 2;</code>
+       * @return The bytes for srcName.
        */
       public com.google.protobuf.ByteString
           getSrcNameBytes() {
@@ -14205,214 +15601,213 @@ public java.lang.String getSrcName() {
       }
       /**
        * <code>optional string src_name = 2;</code>
+       * @param value The srcName to set.
+       * @return This builder for chaining.
        */
       public Builder setSrcName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         srcName_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional string src_name = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearSrcName() {
-        bitField0_ = (bitField0_ & ~0x00000002);
         srcName_ = getDefaultInstance().getSrcName();
+        bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
       /**
        * <code>optional string src_name = 2;</code>
+       * @param value The bytes for srcName to set.
+       * @return This builder for chaining.
        */
       public Builder setSrcNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         srcName_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
 
-      // optional .SourceStateProto state = 3;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto state_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.S_SUCCEEDED;
+      private int state_ = 1;
       /**
        * <code>optional .SourceStateProto state = 3;</code>
+       * @return Whether the state field is set.
        */
-      public boolean hasState() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+      @java.lang.Override public boolean hasState() {
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional .SourceStateProto state = 3;</code>
+       * @return The state.
        */
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto getState() {
-        return state_;
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto result = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.forNumber(state_);
+        return result == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.S_SUCCEEDED : result;
       }
       /**
        * <code>optional .SourceStateProto state = 3;</code>
+       * @param value The state to set.
+       * @return This builder for chaining.
        */
       public Builder setState(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto value) {
         if (value == null) {
           throw new NullPointerException();
         }
         bitField0_ |= 0x00000004;
-        state_ = value;
+        state_ = value.getNumber();
         onChanged();
         return this;
       }
       /**
        * <code>optional .SourceStateProto state = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearState() {
         bitField0_ = (bitField0_ & ~0x00000004);
-        state_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.S_SUCCEEDED;
+        state_ = 1;
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:SourceStateUpdatedRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:SourceStateUpdatedRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new SourceStateUpdatedRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<SourceStateUpdatedRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<SourceStateUpdatedRequestProto>() {
+      @java.lang.Override
+      public SourceStateUpdatedRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<SourceStateUpdatedRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<SourceStateUpdatedRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:SourceStateUpdatedRequestProto)
   }
 
-  public interface SourceStateUpdatedResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface SourceStateUpdatedResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:SourceStateUpdatedResponseProto)
+      com.google.protobuf.MessageOrBuilder {
   }
   /**
    * Protobuf type {@code SourceStateUpdatedResponseProto}
    */
   public static final class SourceStateUpdatedResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements SourceStateUpdatedResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:SourceStateUpdatedResponseProto)
+      SourceStateUpdatedResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use SourceStateUpdatedResponseProto.newBuilder() to construct.
-    private SourceStateUpdatedResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private SourceStateUpdatedResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private SourceStateUpdatedResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final SourceStateUpdatedResponseProto defaultInstance;
-    public static SourceStateUpdatedResponseProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public SourceStateUpdatedResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private SourceStateUpdatedResponseProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private SourceStateUpdatedResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new SourceStateUpdatedResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<SourceStateUpdatedResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<SourceStateUpdatedResponseProto>() {
-      public SourceStateUpdatedResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new SourceStateUpdatedResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<SourceStateUpdatedResponseProto> getParserForType() {
-      return PARSER;
-    }
-
-    private void initFields() {
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -14423,25 +15818,33 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto) obj;
 
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -14465,46 +15868,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.So
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -14512,14 +15930,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code SourceStateUpdatedResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:SourceStateUpdatedResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -14528,40 +15948,32 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -14570,12 +15982,46 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceSta
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto(this);
         onBuilt();
         return result;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto)other);
@@ -14588,52 +16034,122 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.getDefaultInstance()) return this;
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:SourceStateUpdatedResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:SourceStateUpdatedResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new SourceStateUpdatedResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<SourceStateUpdatedResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<SourceStateUpdatedResponseProto>() {
+      @java.lang.Override
+      public SourceStateUpdatedResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<SourceStateUpdatedResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<SourceStateUpdatedResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:SourceStateUpdatedResponseProto)
   }
 
-  public interface QueryCompleteRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface QueryCompleteRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:QueryCompleteRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional .QueryIdentifierProto query_identifier = 1;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return Whether the queryIdentifier field is set.
      */
     boolean hasQueryIdentifier();
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return The queryIdentifier.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
     /**
@@ -14641,13 +16157,14 @@ public interface QueryCompleteRequestProtoOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();
 
-    // optional int64 delete_delay = 2 [default = 0];
     /**
      * <code>optional int64 delete_delay = 2 [default = 0];</code>
+     * @return Whether the deleteDelay field is set.
      */
     boolean hasDeleteDelay();
     /**
      * <code>optional int64 delete_delay = 2 [default = 0];</code>
+     * @return The deleteDelay.
      */
     long getDeleteDelay();
   }
@@ -14655,200 +16172,125 @@ public interface QueryCompleteRequestProtoOrBuilder
    * Protobuf type {@code QueryCompleteRequestProto}
    */
   public static final class QueryCompleteRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements QueryCompleteRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:QueryCompleteRequestProto)
+      QueryCompleteRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use QueryCompleteRequestProto.newBuilder() to construct.
-    private QueryCompleteRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private QueryCompleteRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private QueryCompleteRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+    private QueryCompleteRequestProto() {
+    }
 
-    private static final QueryCompleteRequestProto defaultInstance;
-    public static QueryCompleteRequestProto getDefaultInstance() {
-      return defaultInstance;
+    @java.lang.Override
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new QueryCompleteRequestProto();
     }
 
-    public QueryCompleteRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_descriptor;
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private QueryCompleteRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = queryIdentifier_.toBuilder();
-              }
-              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(queryIdentifier_);
-                queryIdentifier_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              deleteDelay_ = input.readInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<QueryCompleteRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<QueryCompleteRequestProto>() {
-      public QueryCompleteRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new QueryCompleteRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<QueryCompleteRequestProto> getParserForType() {
-      return PARSER;
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.Builder.class);
     }
 
     private int bitField0_;
-    // optional .QueryIdentifierProto query_identifier = 1;
     public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return Whether the queryIdentifier field is set.
      */
+    @java.lang.Override
     public boolean hasQueryIdentifier() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return The queryIdentifier.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
 
-    // optional int64 delete_delay = 2 [default = 0];
     public static final int DELETE_DELAY_FIELD_NUMBER = 2;
-    private long deleteDelay_;
+    private long deleteDelay_ = 0L;
     /**
      * <code>optional int64 delete_delay = 2 [default = 0];</code>
+     * @return Whether the deleteDelay field is set.
      */
+    @java.lang.Override
     public boolean hasDeleteDelay() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional int64 delete_delay = 2 [default = 0];</code>
+     * @return The deleteDelay.
      */
+    @java.lang.Override
     public long getDeleteDelay() {
       return deleteDelay_;
     }
 
-    private void initFields() {
-      queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      deleteDelay_ = 0L;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, queryIdentifier_);
+      if (((bitField0_ & 0x00000001) != 0)) {
+        output.writeMessage(1, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeInt64(2, deleteDelay_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, queryIdentifier_);
+          .computeMessageSize(1, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(2, deleteDelay_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -14859,43 +16301,52 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasQueryIdentifier() == other.hasQueryIdentifier());
+      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
       if (hasQueryIdentifier()) {
-        result = result && getQueryIdentifier()
-            .equals(other.getQueryIdentifier());
+        if (!getQueryIdentifier()
+            .equals(other.getQueryIdentifier())) return false;
       }
-      result = result && (hasDeleteDelay() == other.hasDeleteDelay());
+      if (hasDeleteDelay() != other.hasDeleteDelay()) return false;
       if (hasDeleteDelay()) {
-        result = result && (getDeleteDelay()
-            == other.getDeleteDelay());
+        if (getDeleteDelay()
+            != other.getDeleteDelay()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasQueryIdentifier()) {
         hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
         hash = (53 * hash) + getQueryIdentifier().hashCode();
       }
       if (hasDeleteDelay()) {
         hash = (37 * hash) + DELETE_DELAY_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getDeleteDelay());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getDeleteDelay());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -14919,46 +16370,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Qu
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -14966,14 +16432,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code QueryCompleteRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:QueryCompleteRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -14986,45 +16454,41 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getQueryIdentifierFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-        } else {
-          queryIdentifierBuilder_.clear();
+        bitField0_ = 0;
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000001);
         deleteDelay_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -15033,27 +16497,63 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryComp
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.queryIdentifier_ = queryIdentifierBuilder_ == null
+              ? queryIdentifier_
+              : queryIdentifierBuilder_.build();
           to_bitField0_ |= 0x00000001;
         }
-        if (queryIdentifierBuilder_ == null) {
-          result.queryIdentifier_ = queryIdentifier_;
-        } else {
-          result.queryIdentifier_ = queryIdentifierBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.deleteDelay_ = deleteDelay_;
           to_bitField0_ |= 0x00000002;
         }
-        result.deleteDelay_ = deleteDelay_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto)other);
@@ -15072,48 +16572,77 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setDeleteDelay(other.getDeleteDelay());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                input.readMessage(
+                    getQueryIdentifierFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 16: {
+                deleteDelay_ = input.readInt64();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 16
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional .QueryIdentifierProto query_identifier = 1;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+       * @return Whether the queryIdentifier field is set.
        */
       public boolean hasQueryIdentifier() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+       * @return The queryIdentifier.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
         if (queryIdentifierBuilder_ == null) {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         } else {
           return queryIdentifierBuilder_.getMessage();
         }
@@ -15127,11 +16656,11 @@ public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDae
             throw new NullPointerException();
           }
           queryIdentifier_ = value;
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -15141,11 +16670,11 @@ public Builder setQueryIdentifier(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
         if (queryIdentifierBuilder_ == null) {
           queryIdentifier_ = builderForValue.build();
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -15153,31 +16682,33 @@ public Builder setQueryIdentifier(
        */
       public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
         if (queryIdentifierBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
-            queryIdentifier_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000001) != 0) &&
+            queryIdentifier_ != null &&
+            queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
+            getQueryIdentifierBuilder().mergeFrom(value);
           } else {
             queryIdentifier_ = value;
           }
-          onChanged();
         } else {
           queryIdentifierBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000001;
+        if (queryIdentifier_ != null) {
+          bitField0_ |= 0x00000001;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
        */
       public Builder clearQueryIdentifier() {
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-          onChanged();
-        } else {
-          queryIdentifierBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000001);
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -15195,19 +16726,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         if (queryIdentifierBuilder_ != null) {
           return queryIdentifierBuilder_.getMessageOrBuilder();
         } else {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         }
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
           getQueryIdentifierFieldBuilder() {
         if (queryIdentifierBuilder_ == null) {
-          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
-                  queryIdentifier_,
+                  getQueryIdentifier(),
                   getParentForChildren(),
                   isClean());
           queryIdentifier_ = null;
@@ -15215,31 +16747,38 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         return queryIdentifierBuilder_;
       }
 
-      // optional int64 delete_delay = 2 [default = 0];
       private long deleteDelay_ ;
       /**
        * <code>optional int64 delete_delay = 2 [default = 0];</code>
+       * @return Whether the deleteDelay field is set.
        */
+      @java.lang.Override
       public boolean hasDeleteDelay() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional int64 delete_delay = 2 [default = 0];</code>
+       * @return The deleteDelay.
        */
+      @java.lang.Override
       public long getDeleteDelay() {
         return deleteDelay_;
       }
       /**
        * <code>optional int64 delete_delay = 2 [default = 0];</code>
+       * @param value The deleteDelay to set.
+       * @return This builder for chaining.
        */
       public Builder setDeleteDelay(long value) {
-        bitField0_ |= 0x00000002;
+
         deleteDelay_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 delete_delay = 2 [default = 0];</code>
+       * @return This builder for chaining.
        */
       public Builder clearDeleteDelay() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -15247,145 +16786,137 @@ public Builder clearDeleteDelay() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:QueryCompleteRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:QueryCompleteRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new QueryCompleteRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<QueryCompleteRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<QueryCompleteRequestProto>() {
+      @java.lang.Override
+      public QueryCompleteRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<QueryCompleteRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<QueryCompleteRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:QueryCompleteRequestProto)
   }
 
-  public interface QueryCompleteResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface QueryCompleteResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:QueryCompleteResponseProto)
+      com.google.protobuf.MessageOrBuilder {
   }
   /**
    * Protobuf type {@code QueryCompleteResponseProto}
    */
   public static final class QueryCompleteResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements QueryCompleteResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:QueryCompleteResponseProto)
+      QueryCompleteResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use QueryCompleteResponseProto.newBuilder() to construct.
-    private QueryCompleteResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private QueryCompleteResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private QueryCompleteResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final QueryCompleteResponseProto defaultInstance;
-    public static QueryCompleteResponseProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public QueryCompleteResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private QueryCompleteResponseProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private QueryCompleteResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new QueryCompleteResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<QueryCompleteResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<QueryCompleteResponseProto>() {
-      public QueryCompleteResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new QueryCompleteResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<QueryCompleteResponseProto> getParserForType() {
-      return PARSER;
-    }
-
-    private void initFields() {
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -15396,25 +16927,33 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto) obj;
 
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -15438,46 +16977,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Qu
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -15485,14 +17039,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code QueryCompleteResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:QueryCompleteResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -15501,40 +17057,32 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -15543,12 +17091,46 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryComp
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto(this);
         onBuilt();
         return result;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto)other);
@@ -15561,52 +17143,122 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.getDefaultInstance()) return this;
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:QueryCompleteResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:QueryCompleteResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new QueryCompleteResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<QueryCompleteResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<QueryCompleteResponseProto>() {
+      @java.lang.Override
+      public QueryCompleteResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<QueryCompleteResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<QueryCompleteResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:QueryCompleteResponseProto)
   }
 
-  public interface TerminateFragmentRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface TerminateFragmentRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:TerminateFragmentRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional .QueryIdentifierProto query_identifier = 1;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return Whether the queryIdentifier field is set.
      */
     boolean hasQueryIdentifier();
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return The queryIdentifier.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
     /**
@@ -15614,17 +17266,19 @@ public interface TerminateFragmentRequestProtoOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();
 
-    // optional string fragment_identifier_string = 2;
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return Whether the fragmentIdentifierString field is set.
      */
     boolean hasFragmentIdentifierString();
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return The fragmentIdentifierString.
      */
     java.lang.String getFragmentIdentifierString();
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return The bytes for fragmentIdentifierString.
      */
     com.google.protobuf.ByteString
         getFragmentIdentifierStringBytes();
@@ -15633,145 +17287,81 @@ public interface TerminateFragmentRequestProtoOrBuilder
    * Protobuf type {@code TerminateFragmentRequestProto}
    */
   public static final class TerminateFragmentRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements TerminateFragmentRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:TerminateFragmentRequestProto)
+      TerminateFragmentRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use TerminateFragmentRequestProto.newBuilder() to construct.
-    private TerminateFragmentRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private TerminateFragmentRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private TerminateFragmentRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final TerminateFragmentRequestProto defaultInstance;
-    public static TerminateFragmentRequestProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public TerminateFragmentRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private TerminateFragmentRequestProto() {
+      fragmentIdentifierString_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private TerminateFragmentRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = queryIdentifier_.toBuilder();
-              }
-              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(queryIdentifier_);
-                queryIdentifier_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              fragmentIdentifierString_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new TerminateFragmentRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<TerminateFragmentRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<TerminateFragmentRequestProto>() {
-      public TerminateFragmentRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new TerminateFragmentRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<TerminateFragmentRequestProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional .QueryIdentifierProto query_identifier = 1;
     public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return Whether the queryIdentifier field is set.
      */
+    @java.lang.Override
     public boolean hasQueryIdentifier() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return The queryIdentifier.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
 
-    // optional string fragment_identifier_string = 2;
     public static final int FRAGMENT_IDENTIFIER_STRING_FIELD_NUMBER = 2;
-    private java.lang.Object fragmentIdentifierString_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object fragmentIdentifierString_ = "";
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return Whether the fragmentIdentifierString field is set.
      */
+    @java.lang.Override
     public boolean hasFragmentIdentifierString() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return The fragmentIdentifierString.
      */
+    @java.lang.Override
     public java.lang.String getFragmentIdentifierString() {
       java.lang.Object ref = fragmentIdentifierString_;
       if (ref instanceof java.lang.String) {
@@ -15788,7 +17378,9 @@ public java.lang.String getFragmentIdentifierString() {
     }
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return The bytes for fragmentIdentifierString.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getFragmentIdentifierStringBytes() {
       java.lang.Object ref = fragmentIdentifierString_;
@@ -15803,57 +17395,47 @@ public java.lang.String getFragmentIdentifierString() {
       }
     }
 
-    private void initFields() {
-      queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      fragmentIdentifierString_ = "";
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, queryIdentifier_);
+      if (((bitField0_ & 0x00000001) != 0)) {
+        output.writeMessage(1, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getFragmentIdentifierStringBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, fragmentIdentifierString_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, queryIdentifier_);
+          .computeMessageSize(1, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getFragmentIdentifierStringBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, fragmentIdentifierString_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -15864,30 +17446,27 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasQueryIdentifier() == other.hasQueryIdentifier());
+      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
       if (hasQueryIdentifier()) {
-        result = result && getQueryIdentifier()
-            .equals(other.getQueryIdentifier());
+        if (!getQueryIdentifier()
+            .equals(other.getQueryIdentifier())) return false;
       }
-      result = result && (hasFragmentIdentifierString() == other.hasFragmentIdentifierString());
+      if (hasFragmentIdentifierString() != other.hasFragmentIdentifierString()) return false;
       if (hasFragmentIdentifierString()) {
-        result = result && getFragmentIdentifierString()
-            .equals(other.getFragmentIdentifierString());
+        if (!getFragmentIdentifierString()
+            .equals(other.getFragmentIdentifierString())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasQueryIdentifier()) {
         hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
         hash = (53 * hash) + getQueryIdentifier().hashCode();
@@ -15901,6 +17480,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -15924,46 +17514,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Te
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -15971,14 +17576,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code TerminateFragmentRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:TerminateFragmentRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -15991,45 +17598,41 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getQueryIdentifierFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-        } else {
-          queryIdentifierBuilder_.clear();
+        bitField0_ = 0;
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000001);
         fragmentIdentifierString_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -16038,27 +17641,63 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Terminate
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.queryIdentifier_ = queryIdentifierBuilder_ == null
+              ? queryIdentifier_
+              : queryIdentifierBuilder_.build();
           to_bitField0_ |= 0x00000001;
         }
-        if (queryIdentifierBuilder_ == null) {
-          result.queryIdentifier_ = queryIdentifier_;
-        } else {
-          result.queryIdentifier_ = queryIdentifierBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.fragmentIdentifierString_ = fragmentIdentifierString_;
           to_bitField0_ |= 0x00000002;
         }
-        result.fragmentIdentifierString_ = fragmentIdentifierString_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto)other);
@@ -16074,53 +17713,82 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           mergeQueryIdentifier(other.getQueryIdentifier());
         }
         if (other.hasFragmentIdentifierString()) {
-          bitField0_ |= 0x00000002;
           fragmentIdentifierString_ = other.fragmentIdentifierString_;
+          bitField0_ |= 0x00000002;
           onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                input.readMessage(
+                    getQueryIdentifierFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                fragmentIdentifierString_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional .QueryIdentifierProto query_identifier = 1;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+       * @return Whether the queryIdentifier field is set.
        */
       public boolean hasQueryIdentifier() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+       * @return The queryIdentifier.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
         if (queryIdentifierBuilder_ == null) {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         } else {
           return queryIdentifierBuilder_.getMessage();
         }
@@ -16134,11 +17802,11 @@ public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDae
             throw new NullPointerException();
           }
           queryIdentifier_ = value;
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -16148,11 +17816,11 @@ public Builder setQueryIdentifier(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
         if (queryIdentifierBuilder_ == null) {
           queryIdentifier_ = builderForValue.build();
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -16160,31 +17828,33 @@ public Builder setQueryIdentifier(
        */
       public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
         if (queryIdentifierBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
-            queryIdentifier_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000001) != 0) &&
+            queryIdentifier_ != null &&
+            queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
+            getQueryIdentifierBuilder().mergeFrom(value);
           } else {
             queryIdentifier_ = value;
           }
-          onChanged();
         } else {
           queryIdentifierBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000001;
+        if (queryIdentifier_ != null) {
+          bitField0_ |= 0x00000001;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
        */
       public Builder clearQueryIdentifier() {
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-          onChanged();
-        } else {
-          queryIdentifierBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000001);
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -16202,19 +17872,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         if (queryIdentifierBuilder_ != null) {
           return queryIdentifierBuilder_.getMessageOrBuilder();
         } else {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         }
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
           getQueryIdentifierFieldBuilder() {
         if (queryIdentifierBuilder_ == null) {
-          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
-                  queryIdentifier_,
+                  getQueryIdentifier(),
                   getParentForChildren(),
                   isClean());
           queryIdentifier_ = null;
@@ -16222,23 +17893,27 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         return queryIdentifierBuilder_;
       }
 
-      // optional string fragment_identifier_string = 2;
       private java.lang.Object fragmentIdentifierString_ = "";
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @return Whether the fragmentIdentifierString field is set.
        */
       public boolean hasFragmentIdentifierString() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @return The fragmentIdentifierString.
        */
       public java.lang.String getFragmentIdentifierString() {
         java.lang.Object ref = fragmentIdentifierString_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          fragmentIdentifierString_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            fragmentIdentifierString_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -16246,6 +17921,7 @@ public java.lang.String getFragmentIdentifierString() {
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @return The bytes for fragmentIdentifierString.
        */
       public com.google.protobuf.ByteString
           getFragmentIdentifierStringBytes() {
@@ -16262,178 +17938,171 @@ public java.lang.String getFragmentIdentifierString() {
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @param value The fragmentIdentifierString to set.
+       * @return This builder for chaining.
        */
       public Builder setFragmentIdentifierString(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         fragmentIdentifierString_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFragmentIdentifierString() {
-        bitField0_ = (bitField0_ & ~0x00000002);
         fragmentIdentifierString_ = getDefaultInstance().getFragmentIdentifierString();
+        bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @param value The bytes for fragmentIdentifierString to set.
+       * @return This builder for chaining.
        */
       public Builder setFragmentIdentifierStringBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         fragmentIdentifierString_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:TerminateFragmentRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:TerminateFragmentRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new TerminateFragmentRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<TerminateFragmentRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<TerminateFragmentRequestProto>() {
+      @java.lang.Override
+      public TerminateFragmentRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<TerminateFragmentRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<TerminateFragmentRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:TerminateFragmentRequestProto)
   }
 
-  public interface TerminateFragmentResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface TerminateFragmentResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:TerminateFragmentResponseProto)
+      com.google.protobuf.MessageOrBuilder {
   }
   /**
    * Protobuf type {@code TerminateFragmentResponseProto}
    */
   public static final class TerminateFragmentResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements TerminateFragmentResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:TerminateFragmentResponseProto)
+      TerminateFragmentResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use TerminateFragmentResponseProto.newBuilder() to construct.
-    private TerminateFragmentResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private TerminateFragmentResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private TerminateFragmentResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final TerminateFragmentResponseProto defaultInstance;
-    public static TerminateFragmentResponseProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public TerminateFragmentResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private TerminateFragmentResponseProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private TerminateFragmentResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new TerminateFragmentResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<TerminateFragmentResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<TerminateFragmentResponseProto>() {
-      public TerminateFragmentResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new TerminateFragmentResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<TerminateFragmentResponseProto> getParserForType() {
-      return PARSER;
-    }
-
-    private void initFields() {
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -16444,25 +18113,33 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto) obj;
 
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -16486,46 +18163,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Te
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -16533,14 +18225,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code TerminateFragmentResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:TerminateFragmentResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -16549,40 +18243,32 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -16591,12 +18277,46 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Terminate
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto(this);
         onBuilt();
         return result;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto)other);
@@ -16609,52 +18329,122 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.getDefaultInstance()) return this;
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:TerminateFragmentResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:TerminateFragmentResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new TerminateFragmentResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<TerminateFragmentResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<TerminateFragmentResponseProto>() {
+      @java.lang.Override
+      public TerminateFragmentResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<TerminateFragmentResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<TerminateFragmentResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:TerminateFragmentResponseProto)
   }
 
-  public interface UpdateFragmentRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface UpdateFragmentRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:UpdateFragmentRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional .QueryIdentifierProto query_identifier = 1;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return Whether the queryIdentifier field is set.
      */
     boolean hasQueryIdentifier();
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return The queryIdentifier.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
     /**
@@ -16662,28 +18452,31 @@ public interface UpdateFragmentRequestProtoOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();
 
-    // optional string fragment_identifier_string = 2;
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return Whether the fragmentIdentifierString field is set.
      */
     boolean hasFragmentIdentifierString();
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return The fragmentIdentifierString.
      */
     java.lang.String getFragmentIdentifierString();
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return The bytes for fragmentIdentifierString.
      */
     com.google.protobuf.ByteString
         getFragmentIdentifierStringBytes();
 
-    // optional bool is_guaranteed = 3;
     /**
      * <code>optional bool is_guaranteed = 3;</code>
+     * @return Whether the isGuaranteed field is set.
      */
     boolean hasIsGuaranteed();
     /**
      * <code>optional bool is_guaranteed = 3;</code>
+     * @return The isGuaranteed.
      */
     boolean getIsGuaranteed();
   }
@@ -16691,150 +18484,81 @@ public interface UpdateFragmentRequestProtoOrBuilder
    * Protobuf type {@code UpdateFragmentRequestProto}
    */
   public static final class UpdateFragmentRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements UpdateFragmentRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:UpdateFragmentRequestProto)
+      UpdateFragmentRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use UpdateFragmentRequestProto.newBuilder() to construct.
-    private UpdateFragmentRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private UpdateFragmentRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private UpdateFragmentRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final UpdateFragmentRequestProto defaultInstance;
-    public static UpdateFragmentRequestProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public UpdateFragmentRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private UpdateFragmentRequestProto() {
+      fragmentIdentifierString_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private UpdateFragmentRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = queryIdentifier_.toBuilder();
-              }
-              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(queryIdentifier_);
-                queryIdentifier_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              fragmentIdentifierString_ = input.readBytes();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              isGuaranteed_ = input.readBool();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new UpdateFragmentRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<UpdateFragmentRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<UpdateFragmentRequestProto>() {
-      public UpdateFragmentRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new UpdateFragmentRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<UpdateFragmentRequestProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional .QueryIdentifierProto query_identifier = 1;
     public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return Whether the queryIdentifier field is set.
      */
+    @java.lang.Override
     public boolean hasQueryIdentifier() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+     * @return The queryIdentifier.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
     /**
      * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
-      return queryIdentifier_;
+      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
     }
 
-    // optional string fragment_identifier_string = 2;
     public static final int FRAGMENT_IDENTIFIER_STRING_FIELD_NUMBER = 2;
-    private java.lang.Object fragmentIdentifierString_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object fragmentIdentifierString_ = "";
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return Whether the fragmentIdentifierString field is set.
      */
+    @java.lang.Override
     public boolean hasFragmentIdentifierString() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return The fragmentIdentifierString.
      */
+    @java.lang.Override
     public java.lang.String getFragmentIdentifierString() {
       java.lang.Object ref = fragmentIdentifierString_;
       if (ref instanceof java.lang.String) {
@@ -16851,7 +18575,9 @@ public java.lang.String getFragmentIdentifierString() {
     }
     /**
      * <code>optional string fragment_identifier_string = 2;</code>
+     * @return The bytes for fragmentIdentifierString.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getFragmentIdentifierStringBytes() {
       java.lang.Object ref = fragmentIdentifierString_;
@@ -16866,81 +18592,73 @@ public java.lang.String getFragmentIdentifierString() {
       }
     }
 
-    // optional bool is_guaranteed = 3;
     public static final int IS_GUARANTEED_FIELD_NUMBER = 3;
-    private boolean isGuaranteed_;
+    private boolean isGuaranteed_ = false;
     /**
      * <code>optional bool is_guaranteed = 3;</code>
+     * @return Whether the isGuaranteed field is set.
      */
+    @java.lang.Override
     public boolean hasIsGuaranteed() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional bool is_guaranteed = 3;</code>
+     * @return The isGuaranteed.
      */
+    @java.lang.Override
     public boolean getIsGuaranteed() {
       return isGuaranteed_;
     }
 
-    private void initFields() {
-      queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      fragmentIdentifierString_ = "";
-      isGuaranteed_ = false;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, queryIdentifier_);
+      if (((bitField0_ & 0x00000001) != 0)) {
+        output.writeMessage(1, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getFragmentIdentifierStringBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, fragmentIdentifierString_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         output.writeBool(3, isGuaranteed_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, queryIdentifier_);
+          .computeMessageSize(1, getQueryIdentifier());
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getFragmentIdentifierStringBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, fragmentIdentifierString_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBoolSize(3, isGuaranteed_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -16951,35 +18669,32 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasQueryIdentifier() == other.hasQueryIdentifier());
+      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
       if (hasQueryIdentifier()) {
-        result = result && getQueryIdentifier()
-            .equals(other.getQueryIdentifier());
+        if (!getQueryIdentifier()
+            .equals(other.getQueryIdentifier())) return false;
       }
-      result = result && (hasFragmentIdentifierString() == other.hasFragmentIdentifierString());
+      if (hasFragmentIdentifierString() != other.hasFragmentIdentifierString()) return false;
       if (hasFragmentIdentifierString()) {
-        result = result && getFragmentIdentifierString()
-            .equals(other.getFragmentIdentifierString());
+        if (!getFragmentIdentifierString()
+            .equals(other.getFragmentIdentifierString())) return false;
       }
-      result = result && (hasIsGuaranteed() == other.hasIsGuaranteed());
+      if (hasIsGuaranteed() != other.hasIsGuaranteed()) return false;
       if (hasIsGuaranteed()) {
-        result = result && (getIsGuaranteed()
-            == other.getIsGuaranteed());
+        if (getIsGuaranteed()
+            != other.getIsGuaranteed()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasQueryIdentifier()) {
         hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
         hash = (53 * hash) + getQueryIdentifier().hashCode();
@@ -16990,13 +18705,25 @@ public int hashCode() {
       }
       if (hasIsGuaranteed()) {
         hash = (37 * hash) + IS_GUARANTEED_FIELD_NUMBER;
-        hash = (53 * hash) + hashBoolean(getIsGuaranteed());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
+            getIsGuaranteed());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -17020,46 +18747,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Up
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -17067,14 +18809,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code UpdateFragmentRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:UpdateFragmentRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -17087,47 +18831,42 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getQueryIdentifierFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-        } else {
-          queryIdentifierBuilder_.clear();
+        bitField0_ = 0;
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000001);
         fragmentIdentifierString_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
         isGuaranteed_ = false;
-        bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -17136,31 +18875,67 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFra
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.queryIdentifier_ = queryIdentifierBuilder_ == null
+              ? queryIdentifier_
+              : queryIdentifierBuilder_.build();
           to_bitField0_ |= 0x00000001;
         }
-        if (queryIdentifierBuilder_ == null) {
-          result.queryIdentifier_ = queryIdentifier_;
-        } else {
-          result.queryIdentifier_ = queryIdentifierBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.fragmentIdentifierString_ = fragmentIdentifierString_;
           to_bitField0_ |= 0x00000002;
         }
-        result.fragmentIdentifierString_ = fragmentIdentifierString_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.isGuaranteed_ = isGuaranteed_;
           to_bitField0_ |= 0x00000004;
         }
-        result.isGuaranteed_ = isGuaranteed_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto)other);
@@ -17176,56 +18951,90 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           mergeQueryIdentifier(other.getQueryIdentifier());
         }
         if (other.hasFragmentIdentifierString()) {
-          bitField0_ |= 0x00000002;
           fragmentIdentifierString_ = other.fragmentIdentifierString_;
+          bitField0_ |= 0x00000002;
           onChanged();
         }
         if (other.hasIsGuaranteed()) {
           setIsGuaranteed(other.getIsGuaranteed());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                input.readMessage(
+                    getQueryIdentifierFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                fragmentIdentifierString_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 24: {
+                isGuaranteed_ = input.readBool();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 24
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional .QueryIdentifierProto query_identifier = 1;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+       * @return Whether the queryIdentifier field is set.
        */
       public boolean hasQueryIdentifier() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
+       * @return The queryIdentifier.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
         if (queryIdentifierBuilder_ == null) {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         } else {
           return queryIdentifierBuilder_.getMessage();
         }
@@ -17239,11 +19048,11 @@ public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDae
             throw new NullPointerException();
           }
           queryIdentifier_ = value;
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -17253,11 +19062,11 @@ public Builder setQueryIdentifier(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
         if (queryIdentifierBuilder_ == null) {
           queryIdentifier_ = builderForValue.build();
-          onChanged();
         } else {
           queryIdentifierBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -17265,31 +19074,33 @@ public Builder setQueryIdentifier(
        */
       public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
         if (queryIdentifierBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
-            queryIdentifier_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000001) != 0) &&
+            queryIdentifier_ != null &&
+            queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
+            getQueryIdentifierBuilder().mergeFrom(value);
           } else {
             queryIdentifier_ = value;
           }
-          onChanged();
         } else {
           queryIdentifierBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000001;
+        if (queryIdentifier_ != null) {
+          bitField0_ |= 0x00000001;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
        */
       public Builder clearQueryIdentifier() {
-        if (queryIdentifierBuilder_ == null) {
-          queryIdentifier_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
-          onChanged();
-        } else {
-          queryIdentifierBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000001);
+        queryIdentifier_ = null;
+        if (queryIdentifierBuilder_ != null) {
+          queryIdentifierBuilder_.dispose();
+          queryIdentifierBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -17307,19 +19118,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         if (queryIdentifierBuilder_ != null) {
           return queryIdentifierBuilder_.getMessageOrBuilder();
         } else {
-          return queryIdentifier_;
+          return queryIdentifier_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
         }
       }
       /**
        * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
           getQueryIdentifierFieldBuilder() {
         if (queryIdentifierBuilder_ == null) {
-          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
-                  queryIdentifier_,
+                  getQueryIdentifier(),
                   getParentForChildren(),
                   isClean());
           queryIdentifier_ = null;
@@ -17327,23 +19139,27 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIden
         return queryIdentifierBuilder_;
       }
 
-      // optional string fragment_identifier_string = 2;
       private java.lang.Object fragmentIdentifierString_ = "";
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @return Whether the fragmentIdentifierString field is set.
        */
       public boolean hasFragmentIdentifierString() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @return The fragmentIdentifierString.
        */
       public java.lang.String getFragmentIdentifierString() {
         java.lang.Object ref = fragmentIdentifierString_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          fragmentIdentifierString_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            fragmentIdentifierString_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -17351,6 +19167,7 @@ public java.lang.String getFragmentIdentifierString() {
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @return The bytes for fragmentIdentifierString.
        */
       public com.google.protobuf.ByteString
           getFragmentIdentifierStringBytes() {
@@ -17367,65 +19184,73 @@ public java.lang.String getFragmentIdentifierString() {
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @param value The fragmentIdentifierString to set.
+       * @return This builder for chaining.
        */
       public Builder setFragmentIdentifierString(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         fragmentIdentifierString_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFragmentIdentifierString() {
-        bitField0_ = (bitField0_ & ~0x00000002);
         fragmentIdentifierString_ = getDefaultInstance().getFragmentIdentifierString();
+        bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
       /**
        * <code>optional string fragment_identifier_string = 2;</code>
+       * @param value The bytes for fragmentIdentifierString to set.
+       * @return This builder for chaining.
        */
       public Builder setFragmentIdentifierStringBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         fragmentIdentifierString_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
 
-      // optional bool is_guaranteed = 3;
       private boolean isGuaranteed_ ;
       /**
        * <code>optional bool is_guaranteed = 3;</code>
+       * @return Whether the isGuaranteed field is set.
        */
+      @java.lang.Override
       public boolean hasIsGuaranteed() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional bool is_guaranteed = 3;</code>
+       * @return The isGuaranteed.
        */
+      @java.lang.Override
       public boolean getIsGuaranteed() {
         return isGuaranteed_;
       }
       /**
        * <code>optional bool is_guaranteed = 3;</code>
+       * @param value The isGuaranteed to set.
+       * @return This builder for chaining.
        */
       public Builder setIsGuaranteed(boolean value) {
-        bitField0_ |= 0x00000004;
+
         isGuaranteed_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>optional bool is_guaranteed = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearIsGuaranteed() {
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -17433,38 +19258,93 @@ public Builder clearIsGuaranteed() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:UpdateFragmentRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:UpdateFragmentRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new UpdateFragmentRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<UpdateFragmentRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<UpdateFragmentRequestProto>() {
+      @java.lang.Override
+      public UpdateFragmentRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<UpdateFragmentRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<UpdateFragmentRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:UpdateFragmentRequestProto)
   }
 
-  public interface UpdateFragmentResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface UpdateFragmentResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:UpdateFragmentResponseProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional bool result = 1;
     /**
      * <code>optional bool result = 1;</code>
+     * @return Whether the result field is set.
      */
     boolean hasResult();
     /**
      * <code>optional bool result = 1;</code>
+     * @return The result.
      */
     boolean getResult();
 
-    // optional bool is_guaranteed = 2;
     /**
      * <code>optional bool is_guaranteed = 2;</code>
+     * @return Whether the isGuaranteed field is set.
      */
     boolean hasIsGuaranteed();
     /**
      * <code>optional bool is_guaranteed = 2;</code>
+     * @return The isGuaranteed.
      */
     boolean getIsGuaranteed();
   }
@@ -17472,186 +19352,118 @@ public interface UpdateFragmentResponseProtoOrBuilder
    * Protobuf type {@code UpdateFragmentResponseProto}
    */
   public static final class UpdateFragmentResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements UpdateFragmentResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:UpdateFragmentResponseProto)
+      UpdateFragmentResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use UpdateFragmentResponseProto.newBuilder() to construct.
-    private UpdateFragmentResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private UpdateFragmentResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private UpdateFragmentResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final UpdateFragmentResponseProto defaultInstance;
-    public static UpdateFragmentResponseProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public UpdateFragmentResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private UpdateFragmentResponseProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private UpdateFragmentResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              result_ = input.readBool();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              isGuaranteed_ = input.readBool();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new UpdateFragmentResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<UpdateFragmentResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<UpdateFragmentResponseProto>() {
-      public UpdateFragmentResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new UpdateFragmentResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<UpdateFragmentResponseProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional bool result = 1;
     public static final int RESULT_FIELD_NUMBER = 1;
-    private boolean result_;
+    private boolean result_ = false;
     /**
      * <code>optional bool result = 1;</code>
+     * @return Whether the result field is set.
      */
+    @java.lang.Override
     public boolean hasResult() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional bool result = 1;</code>
+     * @return The result.
      */
+    @java.lang.Override
     public boolean getResult() {
       return result_;
     }
 
-    // optional bool is_guaranteed = 2;
     public static final int IS_GUARANTEED_FIELD_NUMBER = 2;
-    private boolean isGuaranteed_;
+    private boolean isGuaranteed_ = false;
     /**
      * <code>optional bool is_guaranteed = 2;</code>
+     * @return Whether the isGuaranteed field is set.
      */
+    @java.lang.Override
     public boolean hasIsGuaranteed() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional bool is_guaranteed = 2;</code>
+     * @return The isGuaranteed.
      */
+    @java.lang.Override
     public boolean getIsGuaranteed() {
       return isGuaranteed_;
     }
 
-    private void initFields() {
-      result_ = false;
-      isGuaranteed_ = false;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeBool(1, result_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeBool(2, isGuaranteed_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBoolSize(1, result_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBoolSize(2, isGuaranteed_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -17662,43 +19474,53 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto) obj;
 
-      boolean result = true;
-      result = result && (hasResult() == other.hasResult());
+      if (hasResult() != other.hasResult()) return false;
       if (hasResult()) {
-        result = result && (getResult()
-            == other.getResult());
+        if (getResult()
+            != other.getResult()) return false;
       }
-      result = result && (hasIsGuaranteed() == other.hasIsGuaranteed());
+      if (hasIsGuaranteed() != other.hasIsGuaranteed()) return false;
       if (hasIsGuaranteed()) {
-        result = result && (getIsGuaranteed()
-            == other.getIsGuaranteed());
+        if (getIsGuaranteed()
+            != other.getIsGuaranteed()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasResult()) {
         hash = (37 * hash) + RESULT_FIELD_NUMBER;
-        hash = (53 * hash) + hashBoolean(getResult());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
+            getResult());
       }
       if (hasIsGuaranteed()) {
         hash = (37 * hash) + IS_GUARANTEED_FIELD_NUMBER;
-        hash = (53 * hash) + hashBoolean(getIsGuaranteed());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
+            getIsGuaranteed());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -17722,46 +19544,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Up
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -17769,14 +19606,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code UpdateFragmentResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:UpdateFragmentResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -17785,44 +19624,35 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         result_ = false;
-        bitField0_ = (bitField0_ & ~0x00000001);
         isGuaranteed_ = false;
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -17831,23 +19661,61 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFra
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.result_ = result_;
           to_bitField0_ |= 0x00000001;
         }
-        result.result_ = result_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.isGuaranteed_ = isGuaranteed_;
           to_bitField0_ |= 0x00000002;
         }
-        result.isGuaranteed_ = isGuaranteed_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto)other);
@@ -17866,57 +19734,90 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setIsGuaranteed(other.getIsGuaranteed());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 8: {
+                result_ = input.readBool();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 8
+              case 16: {
+                isGuaranteed_ = input.readBool();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 16
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional bool result = 1;
       private boolean result_ ;
       /**
        * <code>optional bool result = 1;</code>
+       * @return Whether the result field is set.
        */
+      @java.lang.Override
       public boolean hasResult() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional bool result = 1;</code>
+       * @return The result.
        */
+      @java.lang.Override
       public boolean getResult() {
         return result_;
       }
       /**
        * <code>optional bool result = 1;</code>
+       * @param value The result to set.
+       * @return This builder for chaining.
        */
       public Builder setResult(boolean value) {
-        bitField0_ |= 0x00000001;
+
         result_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional bool result = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearResult() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -17925,31 +19826,38 @@ public Builder clearResult() {
         return this;
       }
 
-      // optional bool is_guaranteed = 2;
       private boolean isGuaranteed_ ;
       /**
        * <code>optional bool is_guaranteed = 2;</code>
+       * @return Whether the isGuaranteed field is set.
        */
+      @java.lang.Override
       public boolean hasIsGuaranteed() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional bool is_guaranteed = 2;</code>
+       * @return The isGuaranteed.
        */
+      @java.lang.Override
       public boolean getIsGuaranteed() {
         return isGuaranteed_;
       }
       /**
        * <code>optional bool is_guaranteed = 2;</code>
+       * @param value The isGuaranteed to set.
+       * @return This builder for chaining.
        */
       public Builder setIsGuaranteed(boolean value) {
-        bitField0_ |= 0x00000002;
+
         isGuaranteed_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional bool is_guaranteed = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearIsGuaranteed() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -17957,32 +19865,87 @@ public Builder clearIsGuaranteed() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:UpdateFragmentResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:UpdateFragmentResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new UpdateFragmentResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<UpdateFragmentResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<UpdateFragmentResponseProto>() {
+      @java.lang.Override
+      public UpdateFragmentResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<UpdateFragmentResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<UpdateFragmentResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:UpdateFragmentResponseProto)
   }
 
-  public interface GetTokenRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface GetTokenRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:GetTokenRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string app_id = 1;
     /**
      * <code>optional string app_id = 1;</code>
+     * @return Whether the appId field is set.
      */
     boolean hasAppId();
     /**
      * <code>optional string app_id = 1;</code>
+     * @return The appId.
      */
     java.lang.String getAppId();
     /**
      * <code>optional string app_id = 1;</code>
+     * @return The bytes for appId.
      */
     com.google.protobuf.ByteString
         getAppIdBytes();
@@ -17991,110 +19954,55 @@ public interface GetTokenRequestProtoOrBuilder
    * Protobuf type {@code GetTokenRequestProto}
    */
   public static final class GetTokenRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements GetTokenRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:GetTokenRequestProto)
+      GetTokenRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use GetTokenRequestProto.newBuilder() to construct.
-    private GetTokenRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private GetTokenRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private GetTokenRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final GetTokenRequestProto defaultInstance;
-    public static GetTokenRequestProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public GetTokenRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private GetTokenRequestProto() {
+      appId_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private GetTokenRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              appId_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new GetTokenRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<GetTokenRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<GetTokenRequestProto>() {
-      public GetTokenRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new GetTokenRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<GetTokenRequestProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string app_id = 1;
     public static final int APP_ID_FIELD_NUMBER = 1;
-    private java.lang.Object appId_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object appId_ = "";
     /**
      * <code>optional string app_id = 1;</code>
+     * @return Whether the appId field is set.
      */
+    @java.lang.Override
     public boolean hasAppId() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string app_id = 1;</code>
+     * @return The appId.
      */
+    @java.lang.Override
     public java.lang.String getAppId() {
       java.lang.Object ref = appId_;
       if (ref instanceof java.lang.String) {
@@ -18111,7 +20019,9 @@ public java.lang.String getAppId() {
     }
     /**
      * <code>optional string app_id = 1;</code>
+     * @return The bytes for appId.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getAppIdBytes() {
       java.lang.Object ref = appId_;
@@ -18126,49 +20036,40 @@ public java.lang.String getAppId() {
       }
     }
 
-    private void initFields() {
-      appId_ = "";
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getAppIdBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, appId_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getAppIdBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, appId_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -18179,25 +20080,22 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasAppId() == other.hasAppId());
+      if (hasAppId() != other.hasAppId()) return false;
       if (hasAppId()) {
-        result = result && getAppId()
-            .equals(other.getAppId());
+        if (!getAppId()
+            .equals(other.getAppId())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasAppId()) {
         hash = (37 * hash) + APP_ID_FIELD_NUMBER;
         hash = (53 * hash) + getAppId().hashCode();
@@ -18207,6 +20105,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -18230,46 +20139,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ge
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -18277,14 +20201,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code GetTokenRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:GetTokenRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -18293,42 +20219,34 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         appId_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -18337,19 +20255,57 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenR
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.appId_ = appId_;
           to_bitField0_ |= 0x00000001;
         }
-        result.appId_ = appId_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto)other);
@@ -18362,54 +20318,79 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.getDefaultInstance()) return this;
         if (other.hasAppId()) {
-          bitField0_ |= 0x00000001;
           appId_ = other.appId_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                appId_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string app_id = 1;
       private java.lang.Object appId_ = "";
       /**
        * <code>optional string app_id = 1;</code>
+       * @return Whether the appId field is set.
        */
       public boolean hasAppId() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string app_id = 1;</code>
+       * @return The appId.
        */
       public java.lang.String getAppId() {
         java.lang.Object ref = appId_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          appId_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            appId_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -18417,6 +20398,7 @@ public java.lang.String getAppId() {
       }
       /**
        * <code>optional string app_id = 1;</code>
+       * @return The bytes for appId.
        */
       public com.google.protobuf.ByteString
           getAppIdBytes() {
@@ -18433,61 +20415,116 @@ public java.lang.String getAppId() {
       }
       /**
        * <code>optional string app_id = 1;</code>
+       * @param value The appId to set.
+       * @return This builder for chaining.
        */
       public Builder setAppId(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         appId_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string app_id = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearAppId() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         appId_ = getDefaultInstance().getAppId();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string app_id = 1;</code>
+       * @param value The bytes for appId to set.
+       * @return This builder for chaining.
        */
       public Builder setAppIdBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         appId_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:GetTokenRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:GetTokenRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new GetTokenRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetTokenRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<GetTokenRequestProto>() {
+      @java.lang.Override
+      public GetTokenRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<GetTokenRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<GetTokenRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:GetTokenRequestProto)
   }
 
-  public interface GetTokenResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface GetTokenResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:GetTokenResponseProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional bytes token = 1;
     /**
      * <code>optional bytes token = 1;</code>
+     * @return Whether the token field is set.
      */
     boolean hasToken();
     /**
      * <code>optional bytes token = 1;</code>
+     * @return The token.
      */
     com.google.protobuf.ByteString getToken();
   }
@@ -18495,157 +20532,93 @@ public interface GetTokenResponseProtoOrBuilder
    * Protobuf type {@code GetTokenResponseProto}
    */
   public static final class GetTokenResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements GetTokenResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:GetTokenResponseProto)
+      GetTokenResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use GetTokenResponseProto.newBuilder() to construct.
-    private GetTokenResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private GetTokenResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private GetTokenResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final GetTokenResponseProto defaultInstance;
-    public static GetTokenResponseProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public GetTokenResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private GetTokenResponseProto() {
+      token_ = com.google.protobuf.ByteString.EMPTY;
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private GetTokenResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              token_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new GetTokenResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<GetTokenResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<GetTokenResponseProto>() {
-      public GetTokenResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new GetTokenResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<GetTokenResponseProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional bytes token = 1;
     public static final int TOKEN_FIELD_NUMBER = 1;
-    private com.google.protobuf.ByteString token_;
+    private com.google.protobuf.ByteString token_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>optional bytes token = 1;</code>
+     * @return Whether the token field is set.
      */
+    @java.lang.Override
     public boolean hasToken() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional bytes token = 1;</code>
+     * @return The token.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getToken() {
       return token_;
     }
 
-    private void initFields() {
-      token_ = com.google.protobuf.ByteString.EMPTY;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeBytes(1, token_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(1, token_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -18656,25 +20629,22 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto) obj;
 
-      boolean result = true;
-      result = result && (hasToken() == other.hasToken());
+      if (hasToken() != other.hasToken()) return false;
       if (hasToken()) {
-        result = result && getToken()
-            .equals(other.getToken());
+        if (!getToken()
+            .equals(other.getToken())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasToken()) {
         hash = (37 * hash) + TOKEN_FIELD_NUMBER;
         hash = (53 * hash) + getToken().hashCode();
@@ -18684,6 +20654,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -18707,46 +20688,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ge
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -18754,14 +20750,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code GetTokenResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:GetTokenResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -18770,42 +20768,34 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         token_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -18814,19 +20804,57 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenR
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.token_ = token_;
           to_bitField0_ |= 0x00000001;
         }
-        result.token_ = token_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto)other);
@@ -18842,60 +20870,85 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setToken(other.getToken());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                token_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional bytes token = 1;
       private com.google.protobuf.ByteString token_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>optional bytes token = 1;</code>
+       * @return Whether the token field is set.
        */
+      @java.lang.Override
       public boolean hasToken() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional bytes token = 1;</code>
+       * @return The token.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getToken() {
         return token_;
       }
       /**
        * <code>optional bytes token = 1;</code>
+       * @param value The token to set.
+       * @return This builder for chaining.
        */
       public Builder setToken(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         token_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional bytes token = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearToken() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -18903,163 +20956,160 @@ public Builder clearToken() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:GetTokenResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:GetTokenResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new GetTokenResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetTokenResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<GetTokenResponseProto>() {
+      @java.lang.Override
+      public GetTokenResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<GetTokenResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<GetTokenResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:GetTokenResponseProto)
   }
 
-  public interface LlapOutputSocketInitMessageOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface LlapOutputSocketInitMessageOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:LlapOutputSocketInitMessage)
+      com.google.protobuf.MessageOrBuilder {
 
-    // required string fragment_id = 1;
     /**
      * <code>required string fragment_id = 1;</code>
+     * @return Whether the fragmentId field is set.
      */
     boolean hasFragmentId();
     /**
      * <code>required string fragment_id = 1;</code>
+     * @return The fragmentId.
      */
     java.lang.String getFragmentId();
     /**
      * <code>required string fragment_id = 1;</code>
+     * @return The bytes for fragmentId.
      */
     com.google.protobuf.ByteString
         getFragmentIdBytes();
 
-    // optional bytes token = 2;
     /**
      * <code>optional bytes token = 2;</code>
+     * @return Whether the token field is set.
      */
     boolean hasToken();
     /**
      * <code>optional bytes token = 2;</code>
+     * @return The token.
      */
     com.google.protobuf.ByteString getToken();
   }
   /**
-   * Protobuf type {@code LlapOutputSocketInitMessage}
-   *
    * <pre>
    * The message sent by external client to claim the output from the output socket.
    * </pre>
+   *
+   * Protobuf type {@code LlapOutputSocketInitMessage}
    */
   public static final class LlapOutputSocketInitMessage extends
-      com.google.protobuf.GeneratedMessage
-      implements LlapOutputSocketInitMessageOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:LlapOutputSocketInitMessage)
+      LlapOutputSocketInitMessageOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use LlapOutputSocketInitMessage.newBuilder() to construct.
-    private LlapOutputSocketInitMessage(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private LlapOutputSocketInitMessage(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private LlapOutputSocketInitMessage(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final LlapOutputSocketInitMessage defaultInstance;
-    public static LlapOutputSocketInitMessage getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public LlapOutputSocketInitMessage getDefaultInstanceForType() {
-      return defaultInstance;
+    private LlapOutputSocketInitMessage() {
+      fragmentId_ = "";
+      token_ = com.google.protobuf.ByteString.EMPTY;
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private LlapOutputSocketInitMessage(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              fragmentId_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              token_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new LlapOutputSocketInitMessage();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<LlapOutputSocketInitMessage> PARSER =
-        new com.google.protobuf.AbstractParser<LlapOutputSocketInitMessage>() {
-      public LlapOutputSocketInitMessage parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new LlapOutputSocketInitMessage(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<LlapOutputSocketInitMessage> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // required string fragment_id = 1;
     public static final int FRAGMENT_ID_FIELD_NUMBER = 1;
-    private java.lang.Object fragmentId_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object fragmentId_ = "";
     /**
      * <code>required string fragment_id = 1;</code>
+     * @return Whether the fragmentId field is set.
      */
+    @java.lang.Override
     public boolean hasFragmentId() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>required string fragment_id = 1;</code>
+     * @return The fragmentId.
      */
+    @java.lang.Override
     public java.lang.String getFragmentId() {
       java.lang.Object ref = fragmentId_;
       if (ref instanceof java.lang.String) {
@@ -19076,7 +21126,9 @@ public java.lang.String getFragmentId() {
     }
     /**
      * <code>required string fragment_id = 1;</code>
+     * @return The bytes for fragmentId.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getFragmentIdBytes() {
       java.lang.Object ref = fragmentId_;
@@ -19091,30 +21143,31 @@ public java.lang.String getFragmentId() {
       }
     }
 
-    // optional bytes token = 2;
     public static final int TOKEN_FIELD_NUMBER = 2;
-    private com.google.protobuf.ByteString token_;
+    private com.google.protobuf.ByteString token_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>optional bytes token = 2;</code>
+     * @return Whether the token field is set.
      */
+    @java.lang.Override
     public boolean hasToken() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional bytes token = 2;</code>
+     * @return The token.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getToken() {
       return token_;
     }
 
-    private void initFields() {
-      fragmentId_ = "";
-      token_ = com.google.protobuf.ByteString.EMPTY;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       if (!hasFragmentId()) {
         memoizedIsInitialized = 0;
@@ -19124,44 +21177,36 @@ public final boolean isInitialized() {
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getFragmentIdBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, fragmentId_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeBytes(2, token_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getFragmentIdBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, fragmentId_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(2, token_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -19172,30 +21217,27 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage) obj;
 
-      boolean result = true;
-      result = result && (hasFragmentId() == other.hasFragmentId());
+      if (hasFragmentId() != other.hasFragmentId()) return false;
       if (hasFragmentId()) {
-        result = result && getFragmentId()
-            .equals(other.getFragmentId());
+        if (!getFragmentId()
+            .equals(other.getFragmentId())) return false;
       }
-      result = result && (hasToken() == other.hasToken());
+      if (hasToken() != other.hasToken()) return false;
       if (hasToken()) {
-        result = result && getToken()
-            .equals(other.getToken());
+        if (!getToken()
+            .equals(other.getToken())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasFragmentId()) {
         hash = (37 * hash) + FRAGMENT_ID_FIELD_NUMBER;
         hash = (53 * hash) + getFragmentId().hashCode();
@@ -19209,6 +21251,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -19232,65 +21285,82 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ll
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
     /**
-     * Protobuf type {@code LlapOutputSocketInitMessage}
-     *
      * <pre>
      * The message sent by external client to claim the output from the output socket.
      * </pre>
+     *
+     * Protobuf type {@code LlapOutputSocketInitMessage}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessageOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:LlapOutputSocketInitMessage)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessageOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -19299,44 +21369,35 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         fragmentId_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
         token_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage result = buildPartial();
         if (!result.isInitialized()) {
@@ -19345,23 +21406,61 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutpu
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.fragmentId_ = fragmentId_;
           to_bitField0_ |= 0x00000001;
         }
-        result.fragmentId_ = fragmentId_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.token_ = token_;
           to_bitField0_ |= 0x00000002;
         }
-        result.token_ = token_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage)other);
@@ -19374,61 +21473,90 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.getDefaultInstance()) return this;
         if (other.hasFragmentId()) {
-          bitField0_ |= 0x00000001;
           fragmentId_ = other.fragmentId_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasToken()) {
           setToken(other.getToken());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         if (!hasFragmentId()) {
-          
           return false;
         }
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                fragmentId_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                token_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // required string fragment_id = 1;
       private java.lang.Object fragmentId_ = "";
       /**
        * <code>required string fragment_id = 1;</code>
+       * @return Whether the fragmentId field is set.
        */
       public boolean hasFragmentId() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>required string fragment_id = 1;</code>
+       * @return The fragmentId.
        */
       public java.lang.String getFragmentId() {
         java.lang.Object ref = fragmentId_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          fragmentId_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            fragmentId_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -19436,6 +21564,7 @@ public java.lang.String getFragmentId() {
       }
       /**
        * <code>required string fragment_id = 1;</code>
+       * @return The bytes for fragmentId.
        */
       public com.google.protobuf.ByteString
           getFragmentIdBytes() {
@@ -19452,68 +21581,73 @@ public java.lang.String getFragmentId() {
       }
       /**
        * <code>required string fragment_id = 1;</code>
+       * @param value The fragmentId to set.
+       * @return This builder for chaining.
        */
       public Builder setFragmentId(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         fragmentId_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>required string fragment_id = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFragmentId() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         fragmentId_ = getDefaultInstance().getFragmentId();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>required string fragment_id = 1;</code>
+       * @param value The bytes for fragmentId to set.
+       * @return This builder for chaining.
        */
       public Builder setFragmentIdBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         fragmentId_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // optional bytes token = 2;
       private com.google.protobuf.ByteString token_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>optional bytes token = 2;</code>
+       * @return Whether the token field is set.
        */
+      @java.lang.Override
       public boolean hasToken() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional bytes token = 2;</code>
+       * @return The token.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getToken() {
         return token_;
       }
       /**
        * <code>optional bytes token = 2;</code>
+       * @param value The token to set.
+       * @return This builder for chaining.
        */
       public Builder setToken(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         token_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional bytes token = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearToken() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -19521,145 +21655,137 @@ public Builder clearToken() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:LlapOutputSocketInitMessage)
     }
 
+    // @@protoc_insertion_point(class_scope:LlapOutputSocketInitMessage)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage DEFAULT_INSTANCE;
     static {
-      defaultInstance = new LlapOutputSocketInitMessage(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<LlapOutputSocketInitMessage>
+        PARSER = new com.google.protobuf.AbstractParser<LlapOutputSocketInitMessage>() {
+      @java.lang.Override
+      public LlapOutputSocketInitMessage parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<LlapOutputSocketInitMessage> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<LlapOutputSocketInitMessage> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:LlapOutputSocketInitMessage)
   }
 
-  public interface PurgeCacheRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface PurgeCacheRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:PurgeCacheRequestProto)
+      com.google.protobuf.MessageOrBuilder {
   }
   /**
    * Protobuf type {@code PurgeCacheRequestProto}
    */
   public static final class PurgeCacheRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements PurgeCacheRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:PurgeCacheRequestProto)
+      PurgeCacheRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use PurgeCacheRequestProto.newBuilder() to construct.
-    private PurgeCacheRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private PurgeCacheRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private PurgeCacheRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final PurgeCacheRequestProto defaultInstance;
-    public static PurgeCacheRequestProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public PurgeCacheRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private PurgeCacheRequestProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private PurgeCacheRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new PurgeCacheRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<PurgeCacheRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<PurgeCacheRequestProto>() {
-      public PurgeCacheRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new PurgeCacheRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<PurgeCacheRequestProto> getParserForType() {
-      return PARSER;
-    }
-
-    private void initFields() {
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -19670,25 +21796,33 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto) obj;
 
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -19712,46 +21846,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Pu
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -19759,14 +21908,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code PurgeCacheRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:PurgeCacheRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -19775,40 +21926,32 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -19817,12 +21960,46 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCach
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto(this);
         onBuilt();
         return result;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto)other);
@@ -19835,52 +22012,122 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.getDefaultInstance()) return this;
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:PurgeCacheRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:PurgeCacheRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new PurgeCacheRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<PurgeCacheRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<PurgeCacheRequestProto>() {
+      @java.lang.Override
+      public PurgeCacheRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<PurgeCacheRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<PurgeCacheRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:PurgeCacheRequestProto)
   }
 
-  public interface PurgeCacheResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface PurgeCacheResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:PurgeCacheResponseProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional int64 purged_memory_bytes = 1;
     /**
      * <code>optional int64 purged_memory_bytes = 1;</code>
+     * @return Whether the purgedMemoryBytes field is set.
      */
     boolean hasPurgedMemoryBytes();
     /**
      * <code>optional int64 purged_memory_bytes = 1;</code>
+     * @return The purgedMemoryBytes.
      */
     long getPurgedMemoryBytes();
   }
@@ -19888,157 +22135,92 @@ public interface PurgeCacheResponseProtoOrBuilder
    * Protobuf type {@code PurgeCacheResponseProto}
    */
   public static final class PurgeCacheResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements PurgeCacheResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:PurgeCacheResponseProto)
+      PurgeCacheResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use PurgeCacheResponseProto.newBuilder() to construct.
-    private PurgeCacheResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private PurgeCacheResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private PurgeCacheResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final PurgeCacheResponseProto defaultInstance;
-    public static PurgeCacheResponseProto getDefaultInstance() {
-      return defaultInstance;
+    private PurgeCacheResponseProto() {
     }
 
-    public PurgeCacheResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private PurgeCacheResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              purgedMemoryBytes_ = input.readInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new PurgeCacheResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<PurgeCacheResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<PurgeCacheResponseProto>() {
-      public PurgeCacheResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new PurgeCacheResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<PurgeCacheResponseProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional int64 purged_memory_bytes = 1;
     public static final int PURGED_MEMORY_BYTES_FIELD_NUMBER = 1;
-    private long purgedMemoryBytes_;
+    private long purgedMemoryBytes_ = 0L;
     /**
      * <code>optional int64 purged_memory_bytes = 1;</code>
+     * @return Whether the purgedMemoryBytes field is set.
      */
+    @java.lang.Override
     public boolean hasPurgedMemoryBytes() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional int64 purged_memory_bytes = 1;</code>
+     * @return The purgedMemoryBytes.
      */
+    @java.lang.Override
     public long getPurgedMemoryBytes() {
       return purgedMemoryBytes_;
     }
 
-    private void initFields() {
-      purgedMemoryBytes_ = 0L;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeInt64(1, purgedMemoryBytes_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(1, purgedMemoryBytes_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -20049,34 +22231,43 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto) obj;
 
-      boolean result = true;
-      result = result && (hasPurgedMemoryBytes() == other.hasPurgedMemoryBytes());
+      if (hasPurgedMemoryBytes() != other.hasPurgedMemoryBytes()) return false;
       if (hasPurgedMemoryBytes()) {
-        result = result && (getPurgedMemoryBytes()
-            == other.getPurgedMemoryBytes());
+        if (getPurgedMemoryBytes()
+            != other.getPurgedMemoryBytes()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasPurgedMemoryBytes()) {
         hash = (37 * hash) + PURGED_MEMORY_BYTES_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getPurgedMemoryBytes());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getPurgedMemoryBytes());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -20100,46 +22291,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Pu
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -20147,14 +22353,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code PurgeCacheResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:PurgeCacheResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -20163,42 +22371,34 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         purgedMemoryBytes_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -20207,19 +22407,57 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCach
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.purgedMemoryBytes_ = purgedMemoryBytes_;
           to_bitField0_ |= 0x00000001;
         }
-        result.purgedMemoryBytes_ = purgedMemoryBytes_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto)other);
@@ -20235,57 +22473,85 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setPurgedMemoryBytes(other.getPurgedMemoryBytes());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 8: {
+                purgedMemoryBytes_ = input.readInt64();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 8
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional int64 purged_memory_bytes = 1;
       private long purgedMemoryBytes_ ;
       /**
        * <code>optional int64 purged_memory_bytes = 1;</code>
+       * @return Whether the purgedMemoryBytes field is set.
        */
+      @java.lang.Override
       public boolean hasPurgedMemoryBytes() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional int64 purged_memory_bytes = 1;</code>
+       * @return The purgedMemoryBytes.
        */
+      @java.lang.Override
       public long getPurgedMemoryBytes() {
         return purgedMemoryBytes_;
       }
       /**
        * <code>optional int64 purged_memory_bytes = 1;</code>
+       * @param value The purgedMemoryBytes to set.
+       * @return This builder for chaining.
        */
       public Builder setPurgedMemoryBytes(long value) {
-        bitField0_ |= 0x00000001;
+
         purgedMemoryBytes_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 purged_memory_bytes = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearPurgedMemoryBytes() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -20293,43 +22559,99 @@ public Builder clearPurgedMemoryBytes() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:PurgeCacheResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:PurgeCacheResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new PurgeCacheResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<PurgeCacheResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<PurgeCacheResponseProto>() {
+      @java.lang.Override
+      public PurgeCacheResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<PurgeCacheResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<PurgeCacheResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:PurgeCacheResponseProto)
   }
 
-  public interface MapEntryOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface MapEntryOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:MapEntry)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string key = 1;
     /**
      * <code>optional string key = 1;</code>
+     * @return Whether the key field is set.
      */
     boolean hasKey();
     /**
      * <code>optional string key = 1;</code>
+     * @return The key.
      */
     java.lang.String getKey();
     /**
      * <code>optional string key = 1;</code>
+     * @return The bytes for key.
      */
     com.google.protobuf.ByteString
         getKeyBytes();
 
-    // optional int64 value = 2;
     /**
      * <code>optional int64 value = 2;</code>
+     * @return Whether the value field is set.
      */
     boolean hasValue();
     /**
      * <code>optional int64 value = 2;</code>
+     * @return The value.
      */
     long getValue();
   }
@@ -20337,115 +22659,55 @@ public interface MapEntryOrBuilder
    * Protobuf type {@code MapEntry}
    */
   public static final class MapEntry extends
-      com.google.protobuf.GeneratedMessage
-      implements MapEntryOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:MapEntry)
+      MapEntryOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use MapEntry.newBuilder() to construct.
-    private MapEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private MapEntry(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private MapEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final MapEntry defaultInstance;
-    public static MapEntry getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public MapEntry getDefaultInstanceForType() {
-      return defaultInstance;
+    private MapEntry() {
+      key_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private MapEntry(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              key_ = input.readBytes();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              value_ = input.readInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new MapEntry();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<MapEntry> PARSER =
-        new com.google.protobuf.AbstractParser<MapEntry>() {
-      public MapEntry parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new MapEntry(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<MapEntry> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string key = 1;
     public static final int KEY_FIELD_NUMBER = 1;
-    private java.lang.Object key_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object key_ = "";
     /**
      * <code>optional string key = 1;</code>
+     * @return Whether the key field is set.
      */
+    @java.lang.Override
     public boolean hasKey() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string key = 1;</code>
+     * @return The key.
      */
+    @java.lang.Override
     public java.lang.String getKey() {
       java.lang.Object ref = key_;
       if (ref instanceof java.lang.String) {
@@ -20462,7 +22724,9 @@ public java.lang.String getKey() {
     }
     /**
      * <code>optional string key = 1;</code>
+     * @return The bytes for key.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getKeyBytes() {
       java.lang.Object ref = key_;
@@ -20477,73 +22741,66 @@ public java.lang.String getKey() {
       }
     }
 
-    // optional int64 value = 2;
     public static final int VALUE_FIELD_NUMBER = 2;
-    private long value_;
+    private long value_ = 0L;
     /**
      * <code>optional int64 value = 2;</code>
+     * @return Whether the value field is set.
      */
+    @java.lang.Override
     public boolean hasValue() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional int64 value = 2;</code>
+     * @return The value.
      */
+    @java.lang.Override
     public long getValue() {
       return value_;
     }
 
-    private void initFields() {
-      key_ = "";
-      value_ = 0L;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getKeyBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeInt64(2, value_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getKeyBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(2, value_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -20554,43 +22811,52 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry) obj;
 
-      boolean result = true;
-      result = result && (hasKey() == other.hasKey());
+      if (hasKey() != other.hasKey()) return false;
       if (hasKey()) {
-        result = result && getKey()
-            .equals(other.getKey());
+        if (!getKey()
+            .equals(other.getKey())) return false;
       }
-      result = result && (hasValue() == other.hasValue());
+      if (hasValue() != other.hasValue()) return false;
       if (hasValue()) {
-        result = result && (getValue()
-            == other.getValue());
+        if (getValue()
+            != other.getValue()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasKey()) {
         hash = (37 * hash) + KEY_FIELD_NUMBER;
         hash = (53 * hash) + getKey().hashCode();
       }
       if (hasValue()) {
         hash = (37 * hash) + VALUE_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getValue());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getValue());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -20614,46 +22880,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ma
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -20661,14 +22942,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code MapEntry}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:MapEntry)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -20677,44 +22960,35 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         key_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
         value_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry result = buildPartial();
         if (!result.isInitialized()) {
@@ -20723,23 +22997,61 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.key_ = key_;
           to_bitField0_ |= 0x00000001;
         }
-        result.key_ = key_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.value_ = value_;
           to_bitField0_ |= 0x00000002;
         }
-        result.value_ = value_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry)other);
@@ -20752,57 +23064,87 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.getDefaultInstance()) return this;
         if (other.hasKey()) {
-          bitField0_ |= 0x00000001;
           key_ = other.key_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasValue()) {
           setValue(other.getValue());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                key_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 16: {
+                value_ = input.readInt64();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 16
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string key = 1;
       private java.lang.Object key_ = "";
       /**
        * <code>optional string key = 1;</code>
+       * @return Whether the key field is set.
        */
       public boolean hasKey() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string key = 1;</code>
+       * @return The key.
        */
       public java.lang.String getKey() {
         java.lang.Object ref = key_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          key_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            key_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -20810,6 +23152,7 @@ public java.lang.String getKey() {
       }
       /**
        * <code>optional string key = 1;</code>
+       * @return The bytes for key.
        */
       public com.google.protobuf.ByteString
           getKeyBytes() {
@@ -20826,65 +23169,73 @@ public java.lang.String getKey() {
       }
       /**
        * <code>optional string key = 1;</code>
+       * @param value The key to set.
+       * @return This builder for chaining.
        */
       public Builder setKey(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         key_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string key = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearKey() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         key_ = getDefaultInstance().getKey();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string key = 1;</code>
+       * @param value The bytes for key to set.
+       * @return This builder for chaining.
        */
       public Builder setKeyBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         key_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // optional int64 value = 2;
       private long value_ ;
       /**
        * <code>optional int64 value = 2;</code>
+       * @return Whether the value field is set.
        */
+      @java.lang.Override
       public boolean hasValue() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional int64 value = 2;</code>
+       * @return The value.
        */
+      @java.lang.Override
       public long getValue() {
         return value_;
       }
       /**
        * <code>optional int64 value = 2;</code>
+       * @param value The value to set.
+       * @return This builder for chaining.
        */
       public Builder setValue(long value) {
-        bitField0_ |= 0x00000002;
+
         value_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 value = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearValue() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -20892,145 +23243,137 @@ public Builder clearValue() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:MapEntry)
     }
 
+    // @@protoc_insertion_point(class_scope:MapEntry)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry DEFAULT_INSTANCE;
     static {
-      defaultInstance = new MapEntry(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<MapEntry>
+        PARSER = new com.google.protobuf.AbstractParser<MapEntry>() {
+      @java.lang.Override
+      public MapEntry parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<MapEntry> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<MapEntry> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:MapEntry)
   }
 
-  public interface GetDaemonMetricsRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface GetDaemonMetricsRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:GetDaemonMetricsRequestProto)
+      com.google.protobuf.MessageOrBuilder {
   }
   /**
    * Protobuf type {@code GetDaemonMetricsRequestProto}
    */
   public static final class GetDaemonMetricsRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements GetDaemonMetricsRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:GetDaemonMetricsRequestProto)
+      GetDaemonMetricsRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use GetDaemonMetricsRequestProto.newBuilder() to construct.
-    private GetDaemonMetricsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private GetDaemonMetricsRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private GetDaemonMetricsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final GetDaemonMetricsRequestProto defaultInstance;
-    public static GetDaemonMetricsRequestProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public GetDaemonMetricsRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private GetDaemonMetricsRequestProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private GetDaemonMetricsRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new GetDaemonMetricsRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<GetDaemonMetricsRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<GetDaemonMetricsRequestProto>() {
-      public GetDaemonMetricsRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new GetDaemonMetricsRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<GetDaemonMetricsRequestProto> getParserForType() {
-      return PARSER;
-    }
-
-    private void initFields() {
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -21041,25 +23384,33 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto) obj;
 
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -21083,46 +23434,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ge
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -21130,14 +23496,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code GetDaemonMetricsRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:GetDaemonMetricsRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -21146,40 +23514,32 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -21188,12 +23548,46 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemon
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto(this);
         onBuilt();
         return result;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto)other);
@@ -21206,46 +23600,114 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.getDefaultInstance()) return this;
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:GetDaemonMetricsRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:GetDaemonMetricsRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new GetDaemonMetricsRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetDaemonMetricsRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<GetDaemonMetricsRequestProto>() {
+      @java.lang.Override
+      public GetDaemonMetricsRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<GetDaemonMetricsRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<GetDaemonMetricsRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:GetDaemonMetricsRequestProto)
   }
 
-  public interface GetDaemonMetricsResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface GetDaemonMetricsResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:GetDaemonMetricsResponseProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // repeated .MapEntry metrics = 1;
     /**
      * <code>repeated .MapEntry metrics = 1;</code>
      */
@@ -21274,115 +23736,52 @@ org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilde
    * Protobuf type {@code GetDaemonMetricsResponseProto}
    */
   public static final class GetDaemonMetricsResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements GetDaemonMetricsResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:GetDaemonMetricsResponseProto)
+      GetDaemonMetricsResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use GetDaemonMetricsResponseProto.newBuilder() to construct.
-    private GetDaemonMetricsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private GetDaemonMetricsResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private GetDaemonMetricsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final GetDaemonMetricsResponseProto defaultInstance;
-    public static GetDaemonMetricsResponseProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public GetDaemonMetricsResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private GetDaemonMetricsResponseProto() {
+      metrics_ = java.util.Collections.emptyList();
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private GetDaemonMetricsResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                metrics_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              metrics_.add(input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.PARSER, extensionRegistry));
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          metrics_ = java.util.Collections.unmodifiableList(metrics_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new GetDaemonMetricsResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<GetDaemonMetricsResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<GetDaemonMetricsResponseProto>() {
-      public GetDaemonMetricsResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new GetDaemonMetricsResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<GetDaemonMetricsResponseProto> getParserForType() {
-      return PARSER;
-    }
-
-    // repeated .MapEntry metrics = 1;
     public static final int METRICS_FIELD_NUMBER = 1;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> metrics_;
     /**
      * <code>repeated .MapEntry metrics = 1;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> getMetricsList() {
       return metrics_;
     }
     /**
      * <code>repeated .MapEntry metrics = 1;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder> 
         getMetricsOrBuilderList() {
       return metrics_;
@@ -21390,47 +23789,49 @@ public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolP
     /**
      * <code>repeated .MapEntry metrics = 1;</code>
      */
+    @java.lang.Override
     public int getMetricsCount() {
       return metrics_.size();
     }
     /**
      * <code>repeated .MapEntry metrics = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getMetrics(int index) {
       return metrics_.get(index);
     }
     /**
      * <code>repeated .MapEntry metrics = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder getMetricsOrBuilder(
         int index) {
       return metrics_.get(index);
     }
 
-    private void initFields() {
-      metrics_ = java.util.Collections.emptyList();
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       for (int i = 0; i < metrics_.size(); i++) {
         output.writeMessage(1, metrics_.get(i));
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
@@ -21439,17 +23840,10 @@ public int getSerializedSize() {
           .computeMessageSize(1, metrics_.get(i));
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -21460,22 +23854,19 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto) obj;
 
-      boolean result = true;
-      result = result && getMetricsList()
-          .equals(other.getMetricsList());
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getMetricsList()
+          .equals(other.getMetricsList())) return false;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (getMetricsCount() > 0) {
         hash = (37 * hash) + METRICS_FIELD_NUMBER;
         hash = (53 * hash) + getMetricsList().hashCode();
@@ -21485,6 +23876,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -21508,46 +23910,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ge
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -21555,14 +23972,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code GetDaemonMetricsResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:GetDaemonMetricsResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -21571,47 +23990,40 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getMetricsFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         if (metricsBuilder_ == null) {
           metrics_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
         } else {
+          metrics_ = null;
           metricsBuilder_.clear();
         }
+        bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -21620,11 +24032,18 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemon
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto(this);
-        int from_bitField0_ = bitField0_;
+        buildPartialRepeatedFields(result);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartialRepeatedFields(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto result) {
         if (metricsBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
+          if (((bitField0_ & 0x00000001) != 0)) {
             metrics_ = java.util.Collections.unmodifiableList(metrics_);
             bitField0_ = (bitField0_ & ~0x00000001);
           }
@@ -21632,10 +24051,45 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemon
         } else {
           result.metrics_ = metricsBuilder_.build();
         }
-        onBuilt();
-        return result;
       }
 
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto result) {
+        int from_bitField0_ = bitField0_;
+      }
+
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto)other);
@@ -21666,7 +24120,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
               metrics_ = other.metrics_;
               bitField0_ = (bitField0_ & ~0x00000001);
               metricsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getMetricsFieldBuilder() : null;
             } else {
               metricsBuilder_.addAllMessages(other.metrics_);
@@ -21674,43 +24128,71 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           }
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.PARSER,
+                        extensionRegistry);
+                if (metricsBuilder_ == null) {
+                  ensureMetricsIsMutable();
+                  metrics_.add(m);
+                } else {
+                  metricsBuilder_.addMessage(m);
+                }
+                break;
+              } // case 10
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // repeated .MapEntry metrics = 1;
       private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> metrics_ =
         java.util.Collections.emptyList();
       private void ensureMetricsIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+        if (!((bitField0_ & 0x00000001) != 0)) {
           metrics_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry>(metrics_);
           bitField0_ |= 0x00000001;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder> metricsBuilder_;
 
       /**
@@ -21842,7 +24324,8 @@ public Builder addAllMetrics(
           java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> values) {
         if (metricsBuilder_ == null) {
           ensureMetricsIsMutable();
-          super.addAll(values, metrics_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, metrics_);
           onChanged();
         } else {
           metricsBuilder_.addAllMessages(values);
@@ -21925,52 +24408,107 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.
            getMetricsBuilderList() {
         return getMetricsFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder> 
           getMetricsFieldBuilder() {
         if (metricsBuilder_ == null) {
-          metricsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          metricsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder>(
                   metrics_,
-                  ((bitField0_ & 0x00000001) == 0x00000001),
+                  ((bitField0_ & 0x00000001) != 0),
                   getParentForChildren(),
                   isClean());
           metrics_ = null;
         }
         return metricsBuilder_;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:GetDaemonMetricsResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:GetDaemonMetricsResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new GetDaemonMetricsResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetDaemonMetricsResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<GetDaemonMetricsResponseProto>() {
+      @java.lang.Override
+      public GetDaemonMetricsResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<GetDaemonMetricsResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<GetDaemonMetricsResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:GetDaemonMetricsResponseProto)
   }
 
-  public interface SetCapacityRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface SetCapacityRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:SetCapacityRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional int32 executorNum = 1;
     /**
      * <code>optional int32 executorNum = 1;</code>
+     * @return Whether the executorNum field is set.
      */
     boolean hasExecutorNum();
     /**
      * <code>optional int32 executorNum = 1;</code>
+     * @return The executorNum.
      */
     int getExecutorNum();
 
-    // optional int32 queueSize = 2;
     /**
      * <code>optional int32 queueSize = 2;</code>
+     * @return Whether the queueSize field is set.
      */
     boolean hasQueueSize();
     /**
      * <code>optional int32 queueSize = 2;</code>
+     * @return The queueSize.
      */
     int getQueueSize();
   }
@@ -21978,186 +24516,118 @@ public interface SetCapacityRequestProtoOrBuilder
    * Protobuf type {@code SetCapacityRequestProto}
    */
   public static final class SetCapacityRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements SetCapacityRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:SetCapacityRequestProto)
+      SetCapacityRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use SetCapacityRequestProto.newBuilder() to construct.
-    private SetCapacityRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private SetCapacityRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private SetCapacityRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final SetCapacityRequestProto defaultInstance;
-    public static SetCapacityRequestProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public SetCapacityRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private SetCapacityRequestProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private SetCapacityRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              executorNum_ = input.readInt32();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              queueSize_ = input.readInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new SetCapacityRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<SetCapacityRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<SetCapacityRequestProto>() {
-      public SetCapacityRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new SetCapacityRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<SetCapacityRequestProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional int32 executorNum = 1;
     public static final int EXECUTORNUM_FIELD_NUMBER = 1;
-    private int executorNum_;
+    private int executorNum_ = 0;
     /**
      * <code>optional int32 executorNum = 1;</code>
+     * @return Whether the executorNum field is set.
      */
+    @java.lang.Override
     public boolean hasExecutorNum() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional int32 executorNum = 1;</code>
+     * @return The executorNum.
      */
+    @java.lang.Override
     public int getExecutorNum() {
       return executorNum_;
     }
 
-    // optional int32 queueSize = 2;
     public static final int QUEUESIZE_FIELD_NUMBER = 2;
-    private int queueSize_;
+    private int queueSize_ = 0;
     /**
      * <code>optional int32 queueSize = 2;</code>
+     * @return Whether the queueSize field is set.
      */
+    @java.lang.Override
     public boolean hasQueueSize() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional int32 queueSize = 2;</code>
+     * @return The queueSize.
      */
+    @java.lang.Override
     public int getQueueSize() {
       return queueSize_;
     }
 
-    private void initFields() {
-      executorNum_ = 0;
-      queueSize_ = 0;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeInt32(1, executorNum_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeInt32(2, queueSize_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(1, executorNum_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(2, queueSize_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -22168,30 +24638,27 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasExecutorNum() == other.hasExecutorNum());
+      if (hasExecutorNum() != other.hasExecutorNum()) return false;
       if (hasExecutorNum()) {
-        result = result && (getExecutorNum()
-            == other.getExecutorNum());
+        if (getExecutorNum()
+            != other.getExecutorNum()) return false;
       }
-      result = result && (hasQueueSize() == other.hasQueueSize());
+      if (hasQueueSize() != other.hasQueueSize()) return false;
       if (hasQueueSize()) {
-        result = result && (getQueueSize()
-            == other.getQueueSize());
+        if (getQueueSize()
+            != other.getQueueSize()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasExecutorNum()) {
         hash = (37 * hash) + EXECUTORNUM_FIELD_NUMBER;
         hash = (53 * hash) + getExecutorNum();
@@ -22205,6 +24672,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -22228,46 +24706,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Se
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -22275,14 +24768,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code SetCapacityRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:SetCapacityRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -22291,44 +24786,35 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         executorNum_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000001);
         queueSize_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -22337,23 +24823,61 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapaci
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.executorNum_ = executorNum_;
           to_bitField0_ |= 0x00000001;
         }
-        result.executorNum_ = executorNum_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.queueSize_ = queueSize_;
           to_bitField0_ |= 0x00000002;
         }
-        result.queueSize_ = queueSize_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto)other);
@@ -22372,57 +24896,90 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setQueueSize(other.getQueueSize());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 8: {
+                executorNum_ = input.readInt32();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 8
+              case 16: {
+                queueSize_ = input.readInt32();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 16
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional int32 executorNum = 1;
       private int executorNum_ ;
       /**
        * <code>optional int32 executorNum = 1;</code>
+       * @return Whether the executorNum field is set.
        */
+      @java.lang.Override
       public boolean hasExecutorNum() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional int32 executorNum = 1;</code>
+       * @return The executorNum.
        */
+      @java.lang.Override
       public int getExecutorNum() {
         return executorNum_;
       }
       /**
        * <code>optional int32 executorNum = 1;</code>
+       * @param value The executorNum to set.
+       * @return This builder for chaining.
        */
       public Builder setExecutorNum(int value) {
-        bitField0_ |= 0x00000001;
+
         executorNum_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 executorNum = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearExecutorNum() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -22431,31 +24988,38 @@ public Builder clearExecutorNum() {
         return this;
       }
 
-      // optional int32 queueSize = 2;
       private int queueSize_ ;
       /**
        * <code>optional int32 queueSize = 2;</code>
+       * @return Whether the queueSize field is set.
        */
+      @java.lang.Override
       public boolean hasQueueSize() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional int32 queueSize = 2;</code>
+       * @return The queueSize.
        */
+      @java.lang.Override
       public int getQueueSize() {
         return queueSize_;
       }
       /**
        * <code>optional int32 queueSize = 2;</code>
+       * @param value The queueSize to set.
+       * @return This builder for chaining.
        */
       public Builder setQueueSize(int value) {
-        bitField0_ |= 0x00000002;
+
         queueSize_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 queueSize = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearQueueSize() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -22463,145 +25027,137 @@ public Builder clearQueueSize() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:SetCapacityRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:SetCapacityRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new SetCapacityRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<SetCapacityRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<SetCapacityRequestProto>() {
+      @java.lang.Override
+      public SetCapacityRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<SetCapacityRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<SetCapacityRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:SetCapacityRequestProto)
   }
 
-  public interface SetCapacityResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface SetCapacityResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:SetCapacityResponseProto)
+      com.google.protobuf.MessageOrBuilder {
   }
   /**
    * Protobuf type {@code SetCapacityResponseProto}
    */
   public static final class SetCapacityResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements SetCapacityResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:SetCapacityResponseProto)
+      SetCapacityResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use SetCapacityResponseProto.newBuilder() to construct.
-    private SetCapacityResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private SetCapacityResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private SetCapacityResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final SetCapacityResponseProto defaultInstance;
-    public static SetCapacityResponseProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public SetCapacityResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private SetCapacityResponseProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private SetCapacityResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new SetCapacityResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<SetCapacityResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<SetCapacityResponseProto>() {
-      public SetCapacityResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new SetCapacityResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<SetCapacityResponseProto> getParserForType() {
-      return PARSER;
-    }
-
-    private void initFields() {
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -22612,25 +25168,33 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto) obj;
 
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -22654,46 +25218,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Se
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -22701,14 +25280,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code SetCapacityResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:SetCapacityResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -22717,40 +25298,32 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -22759,12 +25332,46 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapaci
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto(this);
         onBuilt();
         return result;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto)other);
@@ -22777,61 +25384,131 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.getDefaultInstance()) return this;
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
-      public Builder mergeFrom(
+      @java.lang.Override
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
+        try {
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.unwrapIOException();
+        } finally {
+          onChanged();
+        } // finally
+        return this;
+      }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
+
+      // @@protoc_insertion_point(builder_scope:SetCapacityResponseProto)
+    }
+
+    // @@protoc_insertion_point(class_scope:SetCapacityResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto DEFAULT_INSTANCE;
+    static {
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<SetCapacityResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<SetCapacityResponseProto>() {
+      @java.lang.Override
+      public SetCapacityResponseProto parsePartialFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parsedMessage = null;
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          builder.mergeFrom(input, extensionRegistry);
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
         }
-        return this;
+        return builder.buildPartial();
       }
+    };
 
-      // @@protoc_insertion_point(builder_scope:SetCapacityResponseProto)
+    public static com.google.protobuf.Parser<SetCapacityResponseProto> parser() {
+      return PARSER;
     }
 
-    static {
-      defaultInstance = new SetCapacityResponseProto(true);
-      defaultInstance.initFields();
+    @java.lang.Override
+    public com.google.protobuf.Parser<SetCapacityResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:SetCapacityResponseProto)
   }
 
-  public interface EvictEntityRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface EvictEntityRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:EvictEntityRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // required string db_name = 1;
     /**
      * <code>required string db_name = 1;</code>
+     * @return Whether the dbName field is set.
      */
     boolean hasDbName();
     /**
      * <code>required string db_name = 1;</code>
+     * @return The dbName.
      */
     java.lang.String getDbName();
     /**
      * <code>required string db_name = 1;</code>
+     * @return The bytes for dbName.
      */
     com.google.protobuf.ByteString
         getDbNameBytes();
 
-    // repeated .TableProto table = 2;
     /**
      * <code>repeated .TableProto table = 2;</code>
      */
@@ -22857,128 +25534,63 @@ org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuil
         int index);
   }
   /**
-   * Protobuf type {@code EvictEntityRequestProto}
-   *
    * <pre>
    * Used for proactive eviction request. Must contain one DB name, and optionally table information.
    * </pre>
+   *
+   * Protobuf type {@code EvictEntityRequestProto}
    */
   public static final class EvictEntityRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements EvictEntityRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:EvictEntityRequestProto)
+      EvictEntityRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use EvictEntityRequestProto.newBuilder() to construct.
-    private EvictEntityRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private EvictEntityRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private EvictEntityRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final EvictEntityRequestProto defaultInstance;
-    public static EvictEntityRequestProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public EvictEntityRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private EvictEntityRequestProto() {
+      dbName_ = "";
+      table_ = java.util.Collections.emptyList();
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private EvictEntityRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              dbName_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-                table_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto>();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              table_.add(input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.PARSER, extensionRegistry));
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-          table_ = java.util.Collections.unmodifiableList(table_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new EvictEntityRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<EvictEntityRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<EvictEntityRequestProto>() {
-      public EvictEntityRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new EvictEntityRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<EvictEntityRequestProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // required string db_name = 1;
     public static final int DB_NAME_FIELD_NUMBER = 1;
-    private java.lang.Object dbName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object dbName_ = "";
     /**
      * <code>required string db_name = 1;</code>
+     * @return Whether the dbName field is set.
      */
+    @java.lang.Override
     public boolean hasDbName() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>required string db_name = 1;</code>
+     * @return The dbName.
      */
+    @java.lang.Override
     public java.lang.String getDbName() {
       java.lang.Object ref = dbName_;
       if (ref instanceof java.lang.String) {
@@ -22995,7 +25607,9 @@ public java.lang.String getDbName() {
     }
     /**
      * <code>required string db_name = 1;</code>
+     * @return The bytes for dbName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getDbNameBytes() {
       java.lang.Object ref = dbName_;
@@ -23010,18 +25624,20 @@ public java.lang.String getDbName() {
       }
     }
 
-    // repeated .TableProto table = 2;
     public static final int TABLE_FIELD_NUMBER = 2;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> table_;
     /**
      * <code>repeated .TableProto table = 2;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> getTableList() {
       return table_;
     }
     /**
      * <code>repeated .TableProto table = 2;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder> 
         getTableOrBuilderList() {
       return table_;
@@ -23029,31 +25645,32 @@ public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolP
     /**
      * <code>repeated .TableProto table = 2;</code>
      */
+    @java.lang.Override
     public int getTableCount() {
       return table_.size();
     }
     /**
      * <code>repeated .TableProto table = 2;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getTable(int index) {
       return table_.get(index);
     }
     /**
      * <code>repeated .TableProto table = 2;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder getTableOrBuilder(
         int index) {
       return table_.get(index);
     }
 
-    private void initFields() {
-      dbName_ = "";
-      table_ = java.util.Collections.emptyList();
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       if (!hasDbName()) {
         memoizedIsInitialized = 0;
@@ -23069,11 +25686,11 @@ public final boolean isInitialized() {
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getDbNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, dbName_);
       }
       for (int i = 0; i < table_.size(); i++) {
         output.writeMessage(2, table_.get(i));
@@ -23081,32 +25698,24 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getDbNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, dbName_);
       }
       for (int i = 0; i < table_.size(); i++) {
         size += com.google.protobuf.CodedOutputStream
           .computeMessageSize(2, table_.get(i));
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -23117,27 +25726,24 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasDbName() == other.hasDbName());
+      if (hasDbName() != other.hasDbName()) return false;
       if (hasDbName()) {
-        result = result && getDbName()
-            .equals(other.getDbName());
+        if (!getDbName()
+            .equals(other.getDbName())) return false;
       }
-      result = result && getTableList()
-          .equals(other.getTableList());
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getTableList()
+          .equals(other.getTableList())) return false;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasDbName()) {
         hash = (37 * hash) + DB_NAME_FIELD_NUMBER;
         hash = (53 * hash) + getDbName().hashCode();
@@ -23151,6 +25757,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -23174,65 +25791,82 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ev
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
     /**
-     * Protobuf type {@code EvictEntityRequestProto}
-     *
      * <pre>
      * Used for proactive eviction request. Must contain one DB name, and optionally table information.
      * </pre>
+     *
+     * Protobuf type {@code EvictEntityRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:EvictEntityRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -23241,49 +25875,41 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getTableFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         dbName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
         if (tableBuilder_ == null) {
           table_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000002);
         } else {
+          table_ = null;
           tableBuilder_.clear();
         }
+        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -23292,16 +25918,18 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEnti
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.dbName_ = dbName_;
+        buildPartialRepeatedFields(result);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartialRepeatedFields(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto result) {
         if (tableBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002)) {
+          if (((bitField0_ & 0x00000002) != 0)) {
             table_ = java.util.Collections.unmodifiableList(table_);
             bitField0_ = (bitField0_ & ~0x00000002);
           }
@@ -23309,11 +25937,51 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEnti
         } else {
           result.table_ = tableBuilder_.build();
         }
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
       }
 
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto result) {
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.dbName_ = dbName_;
+          to_bitField0_ |= 0x00000001;
+        }
+        result.bitField0_ |= to_bitField0_;
+      }
+
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto)other);
@@ -23326,8 +25994,8 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.getDefaultInstance()) return this;
         if (other.hasDbName()) {
-          bitField0_ |= 0x00000001;
           dbName_ = other.dbName_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (tableBuilder_ == null) {
@@ -23349,7 +26017,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
               table_ = other.table_;
               bitField0_ = (bitField0_ & ~0x00000002);
               tableBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getTableFieldBuilder() : null;
             } else {
               tableBuilder_.addAllMessages(other.table_);
@@ -23357,59 +26025,95 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           }
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         if (!hasDbName()) {
-          
           return false;
         }
         for (int i = 0; i < getTableCount(); i++) {
           if (!getTable(i).isInitialized()) {
-            
             return false;
           }
         }
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                dbName_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.PARSER,
+                        extensionRegistry);
+                if (tableBuilder_ == null) {
+                  ensureTableIsMutable();
+                  table_.add(m);
+                } else {
+                  tableBuilder_.addMessage(m);
+                }
+                break;
+              } // case 18
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // required string db_name = 1;
       private java.lang.Object dbName_ = "";
       /**
        * <code>required string db_name = 1;</code>
+       * @return Whether the dbName field is set.
        */
       public boolean hasDbName() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>required string db_name = 1;</code>
+       * @return The dbName.
        */
       public java.lang.String getDbName() {
         java.lang.Object ref = dbName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          dbName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            dbName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -23417,6 +26121,7 @@ public java.lang.String getDbName() {
       }
       /**
        * <code>required string db_name = 1;</code>
+       * @return The bytes for dbName.
        */
       public com.google.protobuf.ByteString
           getDbNameBytes() {
@@ -23433,51 +26138,51 @@ public java.lang.String getDbName() {
       }
       /**
        * <code>required string db_name = 1;</code>
+       * @param value The dbName to set.
+       * @return This builder for chaining.
        */
       public Builder setDbName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         dbName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>required string db_name = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearDbName() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         dbName_ = getDefaultInstance().getDbName();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>required string db_name = 1;</code>
+       * @param value The bytes for dbName to set.
+       * @return This builder for chaining.
        */
       public Builder setDbNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         dbName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // repeated .TableProto table = 2;
       private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> table_ =
         java.util.Collections.emptyList();
       private void ensureTableIsMutable() {
-        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
+        if (!((bitField0_ & 0x00000002) != 0)) {
           table_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto>(table_);
           bitField0_ |= 0x00000002;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder> tableBuilder_;
 
       /**
@@ -23609,7 +26314,8 @@ public Builder addAllTable(
           java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> values) {
         if (tableBuilder_ == null) {
           ensureTableIsMutable();
-          super.addAll(values, table_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, table_);
           onChanged();
         } else {
           tableBuilder_.addAllMessages(values);
@@ -23692,227 +26398,219 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProt
            getTableBuilderList() {
         return getTableFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder> 
           getTableFieldBuilder() {
         if (tableBuilder_ == null) {
-          tableBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          tableBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder>(
                   table_,
-                  ((bitField0_ & 0x00000002) == 0x00000002),
+                  ((bitField0_ & 0x00000002) != 0),
                   getParentForChildren(),
                   isClean());
           table_ = null;
         }
         return tableBuilder_;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:EvictEntityRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:EvictEntityRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new EvictEntityRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<EvictEntityRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<EvictEntityRequestProto>() {
+      @java.lang.Override
+      public EvictEntityRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<EvictEntityRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<EvictEntityRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:EvictEntityRequestProto)
   }
 
-  public interface TableProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface TableProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:TableProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // required string table_name = 1;
     /**
      * <code>required string table_name = 1;</code>
+     * @return Whether the tableName field is set.
      */
     boolean hasTableName();
     /**
      * <code>required string table_name = 1;</code>
+     * @return The tableName.
      */
     java.lang.String getTableName();
     /**
      * <code>required string table_name = 1;</code>
+     * @return The bytes for tableName.
      */
     com.google.protobuf.ByteString
         getTableNameBytes();
 
-    // repeated string part_key = 2;
     /**
      * <code>repeated string part_key = 2;</code>
+     * @return A list containing the partKey.
      */
     java.util.List<java.lang.String>
-    getPartKeyList();
+        getPartKeyList();
     /**
      * <code>repeated string part_key = 2;</code>
+     * @return The count of partKey.
      */
     int getPartKeyCount();
     /**
      * <code>repeated string part_key = 2;</code>
+     * @param index The index of the element to return.
+     * @return The partKey at the given index.
      */
     java.lang.String getPartKey(int index);
     /**
      * <code>repeated string part_key = 2;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the partKey at the given index.
      */
     com.google.protobuf.ByteString
         getPartKeyBytes(int index);
 
-    // repeated string part_val = 3;
     /**
      * <code>repeated string part_val = 3;</code>
+     * @return A list containing the partVal.
      */
     java.util.List<java.lang.String>
-    getPartValList();
+        getPartValList();
     /**
      * <code>repeated string part_val = 3;</code>
+     * @return The count of partVal.
      */
     int getPartValCount();
     /**
      * <code>repeated string part_val = 3;</code>
+     * @param index The index of the element to return.
+     * @return The partVal at the given index.
      */
     java.lang.String getPartVal(int index);
     /**
      * <code>repeated string part_val = 3;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the partVal at the given index.
      */
     com.google.protobuf.ByteString
         getPartValBytes(int index);
   }
   /**
-   * Protobuf type {@code TableProto}
-   *
    * <pre>
    * Used in EvictEntityRequestProto, can be used for non-partitioned and partitioned tables too.
    * For the latter part_key contains only the keys, part_val has the values for all partitions on all keys:
    * e.g.: for partitions pk0=p00/pk1=p01/pk2=p02 and pk0=p10/pk1=p11/pk2=p12
    * part_key: [pk0, pk1, pk2], part_val: [p00, p01, p02, p10, p11, p12]
    * </pre>
+   *
+   * Protobuf type {@code TableProto}
    */
   public static final class TableProto extends
-      com.google.protobuf.GeneratedMessage
-      implements TableProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:TableProto)
+      TableProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use TableProto.newBuilder() to construct.
-    private TableProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private TableProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private TableProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final TableProto defaultInstance;
-    public static TableProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public TableProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private TableProto() {
+      tableName_ = "";
+      partKey_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+      partVal_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private TableProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              tableName_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-                partKey_ = new com.google.protobuf.LazyStringArrayList();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              partKey_.add(input.readBytes());
-              break;
-            }
-            case 26: {
-              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
-                partVal_ = new com.google.protobuf.LazyStringArrayList();
-                mutable_bitField0_ |= 0x00000004;
-              }
-              partVal_.add(input.readBytes());
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-          partKey_ = new com.google.protobuf.UnmodifiableLazyStringList(partKey_);
-        }
-        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
-          partVal_ = new com.google.protobuf.UnmodifiableLazyStringList(partVal_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new TableProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<TableProto> PARSER =
-        new com.google.protobuf.AbstractParser<TableProto>() {
-      public TableProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new TableProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<TableProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // required string table_name = 1;
     public static final int TABLE_NAME_FIELD_NUMBER = 1;
-    private java.lang.Object tableName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object tableName_ = "";
     /**
      * <code>required string table_name = 1;</code>
+     * @return Whether the tableName field is set.
      */
+    @java.lang.Override
     public boolean hasTableName() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>required string table_name = 1;</code>
+     * @return The tableName.
      */
+    @java.lang.Override
     public java.lang.String getTableName() {
       java.lang.Object ref = tableName_;
       if (ref instanceof java.lang.String) {
@@ -23929,7 +26627,9 @@ public java.lang.String getTableName() {
     }
     /**
      * <code>required string table_name = 1;</code>
+     * @return The bytes for tableName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getTableNameBytes() {
       java.lang.Object ref = tableName_;
@@ -23944,75 +26644,86 @@ public java.lang.String getTableName() {
       }
     }
 
-    // repeated string part_key = 2;
     public static final int PART_KEY_FIELD_NUMBER = 2;
-    private com.google.protobuf.LazyStringList partKey_;
+    @SuppressWarnings("serial")
+    private com.google.protobuf.LazyStringArrayList partKey_ =
+        com.google.protobuf.LazyStringArrayList.emptyList();
     /**
      * <code>repeated string part_key = 2;</code>
+     * @return A list containing the partKey.
      */
-    public java.util.List<java.lang.String>
+    public com.google.protobuf.ProtocolStringList
         getPartKeyList() {
       return partKey_;
     }
     /**
      * <code>repeated string part_key = 2;</code>
+     * @return The count of partKey.
      */
     public int getPartKeyCount() {
       return partKey_.size();
     }
     /**
      * <code>repeated string part_key = 2;</code>
+     * @param index The index of the element to return.
+     * @return The partKey at the given index.
      */
     public java.lang.String getPartKey(int index) {
       return partKey_.get(index);
     }
     /**
      * <code>repeated string part_key = 2;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the partKey at the given index.
      */
     public com.google.protobuf.ByteString
         getPartKeyBytes(int index) {
       return partKey_.getByteString(index);
     }
 
-    // repeated string part_val = 3;
     public static final int PART_VAL_FIELD_NUMBER = 3;
-    private com.google.protobuf.LazyStringList partVal_;
+    @SuppressWarnings("serial")
+    private com.google.protobuf.LazyStringArrayList partVal_ =
+        com.google.protobuf.LazyStringArrayList.emptyList();
     /**
      * <code>repeated string part_val = 3;</code>
+     * @return A list containing the partVal.
      */
-    public java.util.List<java.lang.String>
+    public com.google.protobuf.ProtocolStringList
         getPartValList() {
       return partVal_;
     }
     /**
      * <code>repeated string part_val = 3;</code>
+     * @return The count of partVal.
      */
     public int getPartValCount() {
       return partVal_.size();
     }
     /**
      * <code>repeated string part_val = 3;</code>
+     * @param index The index of the element to return.
+     * @return The partVal at the given index.
      */
     public java.lang.String getPartVal(int index) {
       return partVal_.get(index);
     }
     /**
      * <code>repeated string part_val = 3;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the partVal at the given index.
      */
     public com.google.protobuf.ByteString
         getPartValBytes(int index) {
       return partVal_.getByteString(index);
     }
 
-    private void initFields() {
-      tableName_ = "";
-      partKey_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-      partVal_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       if (!hasTableName()) {
         memoizedIsInitialized = 0;
@@ -24022,36 +26733,34 @@ public final boolean isInitialized() {
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getTableNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, tableName_);
       }
       for (int i = 0; i < partKey_.size(); i++) {
-        output.writeBytes(2, partKey_.getByteString(i));
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, partKey_.getRaw(i));
       }
       for (int i = 0; i < partVal_.size(); i++) {
-        output.writeBytes(3, partVal_.getByteString(i));
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, partVal_.getRaw(i));
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getTableNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, tableName_);
       }
       {
         int dataSize = 0;
         for (int i = 0; i < partKey_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(partKey_.getByteString(i));
+          dataSize += computeStringSizeNoTag(partKey_.getRaw(i));
         }
         size += dataSize;
         size += 1 * getPartKeyList().size();
@@ -24059,24 +26768,16 @@ public int getSerializedSize() {
       {
         int dataSize = 0;
         for (int i = 0; i < partVal_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(partVal_.getByteString(i));
+          dataSize += computeStringSizeNoTag(partVal_.getRaw(i));
         }
         size += dataSize;
         size += 1 * getPartValList().size();
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -24087,29 +26788,26 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto) obj;
 
-      boolean result = true;
-      result = result && (hasTableName() == other.hasTableName());
+      if (hasTableName() != other.hasTableName()) return false;
       if (hasTableName()) {
-        result = result && getTableName()
-            .equals(other.getTableName());
-      }
-      result = result && getPartKeyList()
-          .equals(other.getPartKeyList());
-      result = result && getPartValList()
-          .equals(other.getPartValList());
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+        if (!getTableName()
+            .equals(other.getTableName())) return false;
+      }
+      if (!getPartKeyList()
+          .equals(other.getPartKeyList())) return false;
+      if (!getPartValList()
+          .equals(other.getPartValList())) return false;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasTableName()) {
         hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
         hash = (53 * hash) + getTableName().hashCode();
@@ -24127,6 +26825,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -24150,68 +26859,85 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ta
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
     /**
-     * Protobuf type {@code TableProto}
-     *
      * <pre>
      * Used in EvictEntityRequestProto, can be used for non-partitioned and partitioned tables too.
      * For the latter part_key contains only the keys, part_val has the values for all partitions on all keys:
      * e.g.: for partitions pk0=p00/pk1=p01/pk2=p02 and pk0=p10/pk1=p11/pk2=p12
      * part_key: [pk0, pk1, pk2], part_val: [p00, p01, p02, p10, p11, p12]
      * </pre>
+     *
+     * Protobuf type {@code TableProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:TableProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -24220,46 +26946,38 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
+
       }
 
+      private Builder(
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
+        super(parent);
+
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         tableName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
-        partKey_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        partVal_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000004);
+        partKey_ =
+            com.google.protobuf.LazyStringArrayList.emptyList();
+        partVal_ =
+            com.google.protobuf.LazyStringArrayList.emptyList();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -24268,31 +26986,65 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProt
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.tableName_ = tableName_;
           to_bitField0_ |= 0x00000001;
         }
-        result.tableName_ = tableName_;
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          partKey_ = new com.google.protobuf.UnmodifiableLazyStringList(
-              partKey_);
-          bitField0_ = (bitField0_ & ~0x00000002);
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          partKey_.makeImmutable();
+          result.partKey_ = partKey_;
         }
-        result.partKey_ = partKey_;
-        if (((bitField0_ & 0x00000004) == 0x00000004)) {
-          partVal_ = new com.google.protobuf.UnmodifiableLazyStringList(
-              partVal_);
-          bitField0_ = (bitField0_ & ~0x00000004);
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          partVal_.makeImmutable();
+          result.partVal_ = partVal_;
         }
-        result.partVal_ = partVal_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto)other);
@@ -24305,14 +27057,14 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.getDefaultInstance()) return this;
         if (other.hasTableName()) {
-          bitField0_ |= 0x00000001;
           tableName_ = other.tableName_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (!other.partKey_.isEmpty()) {
           if (partKey_.isEmpty()) {
             partKey_ = other.partKey_;
-            bitField0_ = (bitField0_ & ~0x00000002);
+            bitField0_ |= 0x00000002;
           } else {
             ensurePartKeyIsMutable();
             partKey_.addAll(other.partKey_);
@@ -24322,7 +27074,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
         if (!other.partVal_.isEmpty()) {
           if (partVal_.isEmpty()) {
             partVal_ = other.partVal_;
-            bitField0_ = (bitField0_ & ~0x00000004);
+            bitField0_ |= 0x00000004;
           } else {
             ensurePartValIsMutable();
             partVal_.addAll(other.partVal_);
@@ -24330,53 +27082,89 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         if (!hasTableName()) {
-          
           return false;
         }
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                tableName_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                com.google.protobuf.ByteString bs = input.readBytes();
+                ensurePartKeyIsMutable();
+                partKey_.add(bs);
+                break;
+              } // case 18
+              case 26: {
+                com.google.protobuf.ByteString bs = input.readBytes();
+                ensurePartValIsMutable();
+                partVal_.add(bs);
+                break;
+              } // case 26
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // required string table_name = 1;
       private java.lang.Object tableName_ = "";
       /**
        * <code>required string table_name = 1;</code>
+       * @return Whether the tableName field is set.
        */
       public boolean hasTableName() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>required string table_name = 1;</code>
+       * @return The tableName.
        */
       public java.lang.String getTableName() {
         java.lang.Object ref = tableName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          tableName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            tableName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -24384,6 +27172,7 @@ public java.lang.String getTableName() {
       }
       /**
        * <code>required string table_name = 1;</code>
+       * @return The bytes for tableName.
        */
       public com.google.protobuf.ByteString
           getTableNameBytes() {
@@ -24400,69 +27189,77 @@ public java.lang.String getTableName() {
       }
       /**
        * <code>required string table_name = 1;</code>
+       * @param value The tableName to set.
+       * @return This builder for chaining.
        */
       public Builder setTableName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         tableName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>required string table_name = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearTableName() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         tableName_ = getDefaultInstance().getTableName();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>required string table_name = 1;</code>
+       * @param value The bytes for tableName to set.
+       * @return This builder for chaining.
        */
       public Builder setTableNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         tableName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // repeated string part_key = 2;
-      private com.google.protobuf.LazyStringList partKey_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private com.google.protobuf.LazyStringArrayList partKey_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
       private void ensurePartKeyIsMutable() {
-        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
+        if (!partKey_.isModifiable()) {
           partKey_ = new com.google.protobuf.LazyStringArrayList(partKey_);
-          bitField0_ |= 0x00000002;
-         }
+        }
+        bitField0_ |= 0x00000002;
       }
       /**
        * <code>repeated string part_key = 2;</code>
+       * @return A list containing the partKey.
        */
-      public java.util.List<java.lang.String>
+      public com.google.protobuf.ProtocolStringList
           getPartKeyList() {
-        return java.util.Collections.unmodifiableList(partKey_);
+        partKey_.makeImmutable();
+        return partKey_;
       }
       /**
        * <code>repeated string part_key = 2;</code>
+       * @return The count of partKey.
        */
       public int getPartKeyCount() {
         return partKey_.size();
       }
       /**
        * <code>repeated string part_key = 2;</code>
+       * @param index The index of the element to return.
+       * @return The partKey at the given index.
        */
       public java.lang.String getPartKey(int index) {
         return partKey_.get(index);
       }
       /**
        * <code>repeated string part_key = 2;</code>
+       * @param index The index of the value to return.
+       * @return The bytes of the partKey at the given index.
        */
       public com.google.protobuf.ByteString
           getPartKeyBytes(int index) {
@@ -24470,92 +27267,109 @@ public java.lang.String getPartKey(int index) {
       }
       /**
        * <code>repeated string part_key = 2;</code>
+       * @param index The index to set the value at.
+       * @param value The partKey to set.
+       * @return This builder for chaining.
        */
       public Builder setPartKey(
           int index, java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensurePartKeyIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensurePartKeyIsMutable();
         partKey_.set(index, value);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string part_key = 2;</code>
+       * @param value The partKey to add.
+       * @return This builder for chaining.
        */
       public Builder addPartKey(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensurePartKeyIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensurePartKeyIsMutable();
         partKey_.add(value);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string part_key = 2;</code>
+       * @param values The partKey to add.
+       * @return This builder for chaining.
        */
       public Builder addAllPartKey(
           java.lang.Iterable<java.lang.String> values) {
         ensurePartKeyIsMutable();
-        super.addAll(values, partKey_);
+        com.google.protobuf.AbstractMessageLite.Builder.addAll(
+            values, partKey_);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string part_key = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearPartKey() {
-        partKey_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
+        partKey_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+        bitField0_ = (bitField0_ & ~0x00000002);;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string part_key = 2;</code>
+       * @param value The bytes of the partKey to add.
+       * @return This builder for chaining.
        */
       public Builder addPartKeyBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensurePartKeyIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensurePartKeyIsMutable();
         partKey_.add(value);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
 
-      // repeated string part_val = 3;
-      private com.google.protobuf.LazyStringList partVal_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private com.google.protobuf.LazyStringArrayList partVal_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
       private void ensurePartValIsMutable() {
-        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
+        if (!partVal_.isModifiable()) {
           partVal_ = new com.google.protobuf.LazyStringArrayList(partVal_);
-          bitField0_ |= 0x00000004;
-         }
+        }
+        bitField0_ |= 0x00000004;
       }
       /**
        * <code>repeated string part_val = 3;</code>
+       * @return A list containing the partVal.
        */
-      public java.util.List<java.lang.String>
+      public com.google.protobuf.ProtocolStringList
           getPartValList() {
-        return java.util.Collections.unmodifiableList(partVal_);
+        partVal_.makeImmutable();
+        return partVal_;
       }
       /**
        * <code>repeated string part_val = 3;</code>
+       * @return The count of partVal.
        */
       public int getPartValCount() {
         return partVal_.size();
       }
       /**
        * <code>repeated string part_val = 3;</code>
+       * @param index The index of the element to return.
+       * @return The partVal at the given index.
        */
       public java.lang.String getPartVal(int index) {
         return partVal_.get(index);
       }
       /**
        * <code>repeated string part_val = 3;</code>
+       * @param index The index of the value to return.
+       * @return The bytes of the partVal at the given index.
        */
       public com.google.protobuf.ByteString
           getPartValBytes(int index) {
@@ -24563,84 +27377,148 @@ public java.lang.String getPartVal(int index) {
       }
       /**
        * <code>repeated string part_val = 3;</code>
+       * @param index The index to set the value at.
+       * @param value The partVal to set.
+       * @return This builder for chaining.
        */
       public Builder setPartVal(
           int index, java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensurePartValIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensurePartValIsMutable();
         partVal_.set(index, value);
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string part_val = 3;</code>
+       * @param value The partVal to add.
+       * @return This builder for chaining.
        */
       public Builder addPartVal(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensurePartValIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensurePartValIsMutable();
         partVal_.add(value);
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string part_val = 3;</code>
+       * @param values The partVal to add.
+       * @return This builder for chaining.
        */
       public Builder addAllPartVal(
           java.lang.Iterable<java.lang.String> values) {
         ensurePartValIsMutable();
-        super.addAll(values, partVal_);
+        com.google.protobuf.AbstractMessageLite.Builder.addAll(
+            values, partVal_);
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string part_val = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearPartVal() {
-        partVal_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000004);
+        partVal_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+        bitField0_ = (bitField0_ & ~0x00000004);;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string part_val = 3;</code>
+       * @param value The bytes of the partVal to add.
+       * @return This builder for chaining.
        */
       public Builder addPartValBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensurePartValIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensurePartValIsMutable();
         partVal_.add(value);
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:TableProto)
     }
 
+    // @@protoc_insertion_point(class_scope:TableProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new TableProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<TableProto>
+        PARSER = new com.google.protobuf.AbstractParser<TableProto>() {
+      @java.lang.Override
+      public TableProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<TableProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<TableProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:TableProto)
   }
 
-  public interface EvictEntityResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface EvictEntityResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:EvictEntityResponseProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // required int64 evicted_bytes = 1;
     /**
      * <code>required int64 evicted_bytes = 1;</code>
+     * @return Whether the evictedBytes field is set.
      */
     boolean hasEvictedBytes();
     /**
      * <code>required int64 evicted_bytes = 1;</code>
+     * @return The evictedBytes.
      */
     long getEvictedBytes();
   }
@@ -24648,121 +27526,63 @@ public interface EvictEntityResponseProtoOrBuilder
    * Protobuf type {@code EvictEntityResponseProto}
    */
   public static final class EvictEntityResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements EvictEntityResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:EvictEntityResponseProto)
+      EvictEntityResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use EvictEntityResponseProto.newBuilder() to construct.
-    private EvictEntityResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private EvictEntityResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private EvictEntityResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final EvictEntityResponseProto defaultInstance;
-    public static EvictEntityResponseProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public EvictEntityResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private EvictEntityResponseProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private EvictEntityResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              evictedBytes_ = input.readInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new EvictEntityResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<EvictEntityResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<EvictEntityResponseProto>() {
-      public EvictEntityResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new EvictEntityResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<EvictEntityResponseProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // required int64 evicted_bytes = 1;
     public static final int EVICTED_BYTES_FIELD_NUMBER = 1;
-    private long evictedBytes_;
+    private long evictedBytes_ = 0L;
     /**
      * <code>required int64 evicted_bytes = 1;</code>
+     * @return Whether the evictedBytes field is set.
      */
+    @java.lang.Override
     public boolean hasEvictedBytes() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>required int64 evicted_bytes = 1;</code>
+     * @return The evictedBytes.
      */
+    @java.lang.Override
     public long getEvictedBytes() {
       return evictedBytes_;
     }
 
-    private void initFields() {
-      evictedBytes_ = 0L;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       if (!hasEvictedBytes()) {
         memoizedIsInitialized = 0;
@@ -24772,37 +27592,30 @@ public final boolean isInitialized() {
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeInt64(1, evictedBytes_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(1, evictedBytes_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -24813,34 +27626,43 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto) obj;
 
-      boolean result = true;
-      result = result && (hasEvictedBytes() == other.hasEvictedBytes());
+      if (hasEvictedBytes() != other.hasEvictedBytes()) return false;
       if (hasEvictedBytes()) {
-        result = result && (getEvictedBytes()
-            == other.getEvictedBytes());
+        if (getEvictedBytes()
+            != other.getEvictedBytes()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasEvictedBytes()) {
         hash = (37 * hash) + EVICTED_BYTES_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getEvictedBytes());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getEvictedBytes());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -24864,46 +27686,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ev
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -24911,14 +27748,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code EvictEntityResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:EvictEntityResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -24927,42 +27766,34 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         evictedBytes_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -24971,19 +27802,57 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEnti
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.evictedBytes_ = evictedBytes_;
           to_bitField0_ |= 0x00000001;
         }
-        result.evictedBytes_ = evictedBytes_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto)other);
@@ -24999,61 +27868,88 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setEvictedBytes(other.getEvictedBytes());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         if (!hasEvictedBytes()) {
-          
           return false;
         }
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 8: {
+                evictedBytes_ = input.readInt64();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 8
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // required int64 evicted_bytes = 1;
       private long evictedBytes_ ;
       /**
        * <code>required int64 evicted_bytes = 1;</code>
+       * @return Whether the evictedBytes field is set.
        */
+      @java.lang.Override
       public boolean hasEvictedBytes() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>required int64 evicted_bytes = 1;</code>
+       * @return The evictedBytes.
        */
+      @java.lang.Override
       public long getEvictedBytes() {
         return evictedBytes_;
       }
       /**
        * <code>required int64 evicted_bytes = 1;</code>
+       * @param value The evictedBytes to set.
+       * @return This builder for chaining.
        */
       public Builder setEvictedBytes(long value) {
-        bitField0_ |= 0x00000001;
+
         evictedBytes_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>required int64 evicted_bytes = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearEvictedBytes() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -25061,145 +27957,137 @@ public Builder clearEvictedBytes() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:EvictEntityResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:EvictEntityResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new EvictEntityResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<EvictEntityResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<EvictEntityResponseProto>() {
+      @java.lang.Override
+      public EvictEntityResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<EvictEntityResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<EvictEntityResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:EvictEntityResponseProto)
   }
 
-  public interface GetCacheContentRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface GetCacheContentRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:GetCacheContentRequestProto)
+      com.google.protobuf.MessageOrBuilder {
   }
   /**
    * Protobuf type {@code GetCacheContentRequestProto}
    */
   public static final class GetCacheContentRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements GetCacheContentRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:GetCacheContentRequestProto)
+      GetCacheContentRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use GetCacheContentRequestProto.newBuilder() to construct.
-    private GetCacheContentRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private GetCacheContentRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private GetCacheContentRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final GetCacheContentRequestProto defaultInstance;
-    public static GetCacheContentRequestProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public GetCacheContentRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private GetCacheContentRequestProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private GetCacheContentRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new GetCacheContentRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<GetCacheContentRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<GetCacheContentRequestProto>() {
-      public GetCacheContentRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new GetCacheContentRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<GetCacheContentRequestProto> getParserForType() {
-      return PARSER;
-    }
-
-    private void initFields() {
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -25210,25 +28098,33 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto) obj;
 
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
-
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -25252,46 +28148,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ge
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -25299,14 +28210,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code GetCacheContentRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:GetCacheContentRequestProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -25315,40 +28228,32 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -25357,12 +28262,46 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto(this);
         onBuilt();
         return result;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto)other);
@@ -25375,52 +28314,122 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.getDefaultInstance()) return this;
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:GetCacheContentRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:GetCacheContentRequestProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new GetCacheContentRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetCacheContentRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<GetCacheContentRequestProto>() {
+      @java.lang.Override
+      public GetCacheContentRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<GetCacheContentRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<GetCacheContentRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:GetCacheContentRequestProto)
   }
 
-  public interface GetCacheContentResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface GetCacheContentResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:GetCacheContentResponseProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional .CacheEntryList result = 1;
     /**
      * <code>optional .CacheEntryList result = 1;</code>
+     * @return Whether the result field is set.
      */
     boolean hasResult();
     /**
      * <code>optional .CacheEntryList result = 1;</code>
+     * @return The result.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getResult();
     /**
@@ -25432,171 +28441,99 @@ public interface GetCacheContentResponseProtoOrBuilder
    * Protobuf type {@code GetCacheContentResponseProto}
    */
   public static final class GetCacheContentResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements GetCacheContentResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:GetCacheContentResponseProto)
+      GetCacheContentResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use GetCacheContentResponseProto.newBuilder() to construct.
-    private GetCacheContentResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private GetCacheContentResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private GetCacheContentResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final GetCacheContentResponseProto defaultInstance;
-    public static GetCacheContentResponseProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public GetCacheContentResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private GetCacheContentResponseProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private GetCacheContentResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = result_.toBuilder();
-              }
-              result_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(result_);
-                result_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new GetCacheContentResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<GetCacheContentResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<GetCacheContentResponseProto>() {
-      public GetCacheContentResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new GetCacheContentResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<GetCacheContentResponseProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional .CacheEntryList result = 1;
     public static final int RESULT_FIELD_NUMBER = 1;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result_;
     /**
      * <code>optional .CacheEntryList result = 1;</code>
+     * @return Whether the result field is set.
      */
+    @java.lang.Override
     public boolean hasResult() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional .CacheEntryList result = 1;</code>
+     * @return The result.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getResult() {
-      return result_;
+      return result_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance() : result_;
     }
     /**
      * <code>optional .CacheEntryList result = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder getResultOrBuilder() {
-      return result_;
+      return result_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance() : result_;
     }
 
-    private void initFields() {
-      result_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance();
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, result_);
+      if (((bitField0_ & 0x00000001) != 0)) {
+        output.writeMessage(1, getResult());
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, result_);
+          .computeMessageSize(1, getResult());
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -25607,25 +28544,22 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto) obj;
 
-      boolean result = true;
-      result = result && (hasResult() == other.hasResult());
+      if (hasResult() != other.hasResult()) return false;
       if (hasResult()) {
-        result = result && getResult()
-            .equals(other.getResult());
+        if (!getResult()
+            .equals(other.getResult())) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasResult()) {
         hash = (37 * hash) + RESULT_FIELD_NUMBER;
         hash = (53 * hash) + getResult().hashCode();
@@ -25635,6 +28569,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -25658,46 +28603,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ge
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -25705,14 +28665,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code GetCacheContentResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:GetCacheContentResponseProto)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -25725,43 +28687,40 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getResultFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
-        if (resultBuilder_ == null) {
-          result_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance();
-        } else {
-          resultBuilder_.clear();
+        bitField0_ = 0;
+        result_ = null;
+        if (resultBuilder_ != null) {
+          resultBuilder_.dispose();
+          resultBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -25770,23 +28729,59 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.result_ = resultBuilder_ == null
+              ? result_
+              : resultBuilder_.build();
           to_bitField0_ |= 0x00000001;
         }
-        if (resultBuilder_ == null) {
-          result.result_ = result_;
-        } else {
-          result.result_ = resultBuilder_.build();
-        }
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto)other);
@@ -25802,48 +28797,72 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           mergeResult(other.getResult());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                input.readMessage(
+                    getResultFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional .CacheEntryList result = 1;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder> resultBuilder_;
       /**
        * <code>optional .CacheEntryList result = 1;</code>
+       * @return Whether the result field is set.
        */
       public boolean hasResult() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional .CacheEntryList result = 1;</code>
+       * @return The result.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getResult() {
         if (resultBuilder_ == null) {
-          return result_;
+          return result_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance() : result_;
         } else {
           return resultBuilder_.getMessage();
         }
@@ -25857,11 +28876,11 @@ public Builder setResult(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
             throw new NullPointerException();
           }
           result_ = value;
-          onChanged();
         } else {
           resultBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -25871,11 +28890,11 @@ public Builder setResult(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder builderForValue) {
         if (resultBuilder_ == null) {
           result_ = builderForValue.build();
-          onChanged();
         } else {
           resultBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000001;
+        onChanged();
         return this;
       }
       /**
@@ -25883,31 +28902,33 @@ public Builder setResult(
        */
       public Builder mergeResult(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList value) {
         if (resultBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              result_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance()) {
-            result_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.newBuilder(result_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000001) != 0) &&
+            result_ != null &&
+            result_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance()) {
+            getResultBuilder().mergeFrom(value);
           } else {
             result_ = value;
           }
-          onChanged();
         } else {
           resultBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000001;
+        if (result_ != null) {
+          bitField0_ |= 0x00000001;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .CacheEntryList result = 1;</code>
        */
       public Builder clearResult() {
-        if (resultBuilder_ == null) {
-          result_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance();
-          onChanged();
-        } else {
-          resultBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000001);
+        result_ = null;
+        if (resultBuilder_ != null) {
+          resultBuilder_.dispose();
+          resultBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -25925,41 +28946,94 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntr
         if (resultBuilder_ != null) {
           return resultBuilder_.getMessageOrBuilder();
         } else {
-          return result_;
+          return result_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance() : result_;
         }
       }
       /**
        * <code>optional .CacheEntryList result = 1;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder> 
           getResultFieldBuilder() {
         if (resultBuilder_ == null) {
-          resultBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          resultBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder>(
-                  result_,
+                  getResult(),
                   getParentForChildren(),
                   isClean());
           result_ = null;
         }
         return resultBuilder_;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:GetCacheContentResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:GetCacheContentResponseProto)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new GetCacheContentResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetCacheContentResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<GetCacheContentResponseProto>() {
+      @java.lang.Override
+      public GetCacheContentResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<GetCacheContentResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<GetCacheContentResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:GetCacheContentResponseProto)
   }
 
-  public interface CacheEntryListOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface CacheEntryListOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:CacheEntryList)
+      com.google.protobuf.MessageOrBuilder {
 
-    // repeated .CacheEntry entries = 1;
     /**
      * <code>repeated .CacheEntry entries = 1;</code>
      */
@@ -25988,115 +29062,52 @@ org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuil
    * Protobuf type {@code CacheEntryList}
    */
   public static final class CacheEntryList extends
-      com.google.protobuf.GeneratedMessage
-      implements CacheEntryListOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:CacheEntryList)
+      CacheEntryListOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use CacheEntryList.newBuilder() to construct.
-    private CacheEntryList(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private CacheEntryList(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private CacheEntryList(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final CacheEntryList defaultInstance;
-    public static CacheEntryList getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public CacheEntryList getDefaultInstanceForType() {
-      return defaultInstance;
+    private CacheEntryList() {
+      entries_ = java.util.Collections.emptyList();
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private CacheEntryList(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                entries_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              entries_.add(input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.PARSER, extensionRegistry));
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          entries_ = java.util.Collections.unmodifiableList(entries_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new CacheEntryList();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<CacheEntryList> PARSER =
-        new com.google.protobuf.AbstractParser<CacheEntryList>() {
-      public CacheEntryList parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new CacheEntryList(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<CacheEntryList> getParserForType() {
-      return PARSER;
-    }
-
-    // repeated .CacheEntry entries = 1;
     public static final int ENTRIES_FIELD_NUMBER = 1;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> entries_;
     /**
      * <code>repeated .CacheEntry entries = 1;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> getEntriesList() {
       return entries_;
     }
     /**
      * <code>repeated .CacheEntry entries = 1;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder> 
         getEntriesOrBuilderList() {
       return entries_;
@@ -26104,47 +29115,49 @@ public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolP
     /**
      * <code>repeated .CacheEntry entries = 1;</code>
      */
+    @java.lang.Override
     public int getEntriesCount() {
       return entries_.size();
     }
     /**
      * <code>repeated .CacheEntry entries = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getEntries(int index) {
       return entries_.get(index);
     }
     /**
      * <code>repeated .CacheEntry entries = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder getEntriesOrBuilder(
         int index) {
       return entries_.get(index);
     }
 
-    private void initFields() {
-      entries_ = java.util.Collections.emptyList();
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       for (int i = 0; i < entries_.size(); i++) {
         output.writeMessage(1, entries_.get(i));
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
@@ -26153,17 +29166,10 @@ public int getSerializedSize() {
           .computeMessageSize(1, entries_.get(i));
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -26174,22 +29180,19 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList) obj;
 
-      boolean result = true;
-      result = result && getEntriesList()
-          .equals(other.getEntriesList());
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getEntriesList()
+          .equals(other.getEntriesList())) return false;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (getEntriesCount() > 0) {
         hash = (37 * hash) + ENTRIES_FIELD_NUMBER;
         hash = (53 * hash) + getEntriesList().hashCode();
@@ -26199,6 +29202,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -26222,46 +29236,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ca
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -26269,14 +29298,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code CacheEntryList}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:CacheEntryList)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -26285,47 +29316,40 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getEntriesFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         if (entriesBuilder_ == null) {
           entries_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
         } else {
+          entries_ = null;
           entriesBuilder_.clear();
         }
+        bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result = buildPartial();
         if (!result.isInitialized()) {
@@ -26334,11 +29358,18 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntr
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList(this);
-        int from_bitField0_ = bitField0_;
+        buildPartialRepeatedFields(result);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartialRepeatedFields(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result) {
         if (entriesBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
+          if (((bitField0_ & 0x00000001) != 0)) {
             entries_ = java.util.Collections.unmodifiableList(entries_);
             bitField0_ = (bitField0_ & ~0x00000001);
           }
@@ -26346,10 +29377,45 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntr
         } else {
           result.entries_ = entriesBuilder_.build();
         }
-        onBuilt();
-        return result;
       }
 
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result) {
+        int from_bitField0_ = bitField0_;
+      }
+
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList)other);
@@ -26380,7 +29446,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
               entries_ = other.entries_;
               bitField0_ = (bitField0_ & ~0x00000001);
               entriesBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getEntriesFieldBuilder() : null;
             } else {
               entriesBuilder_.addAllMessages(other.entries_);
@@ -26388,43 +29454,71 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           }
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.PARSER,
+                        extensionRegistry);
+                if (entriesBuilder_ == null) {
+                  ensureEntriesIsMutable();
+                  entries_.add(m);
+                } else {
+                  entriesBuilder_.addMessage(m);
+                }
+                break;
+              } // case 10
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // repeated .CacheEntry entries = 1;
       private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> entries_ =
         java.util.Collections.emptyList();
       private void ensureEntriesIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+        if (!((bitField0_ & 0x00000001) != 0)) {
           entries_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry>(entries_);
           bitField0_ |= 0x00000001;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder> entriesBuilder_;
 
       /**
@@ -26556,7 +29650,8 @@ public Builder addAllEntries(
           java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> values) {
         if (entriesBuilder_ == null) {
           ensureEntriesIsMutable();
-          super.addAll(values, entries_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, entries_);
           onChanged();
         } else {
           entriesBuilder_.addAllMessages(values);
@@ -26639,67 +29734,124 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntr
            getEntriesBuilderList() {
         return getEntriesFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder> 
           getEntriesFieldBuilder() {
         if (entriesBuilder_ == null) {
-          entriesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          entriesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder>(
                   entries_,
-                  ((bitField0_ & 0x00000001) == 0x00000001),
+                  ((bitField0_ & 0x00000001) != 0),
                   getParentForChildren(),
                   isClean());
           entries_ = null;
         }
         return entriesBuilder_;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:CacheEntryList)
     }
 
+    // @@protoc_insertion_point(class_scope:CacheEntryList)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList DEFAULT_INSTANCE;
     static {
-      defaultInstance = new CacheEntryList(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheEntryList>
+        PARSER = new com.google.protobuf.AbstractParser<CacheEntryList>() {
+      @java.lang.Override
+      public CacheEntryList parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<CacheEntryList> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<CacheEntryList> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:CacheEntryList)
   }
 
-  public interface CacheEntryOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface CacheEntryOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:CacheEntry)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional bytes file_key = 1;
     /**
      * <code>optional bytes file_key = 1;</code>
+     * @return Whether the fileKey field is set.
      */
     boolean hasFileKey();
     /**
      * <code>optional bytes file_key = 1;</code>
+     * @return The fileKey.
      */
     com.google.protobuf.ByteString getFileKey();
 
-    // optional string file_path = 2;
     /**
      * <code>optional string file_path = 2;</code>
+     * @return Whether the filePath field is set.
      */
     boolean hasFilePath();
     /**
      * <code>optional string file_path = 2;</code>
+     * @return The filePath.
      */
     java.lang.String getFilePath();
     /**
      * <code>optional string file_path = 2;</code>
+     * @return The bytes for filePath.
      */
     com.google.protobuf.ByteString
         getFilePathBytes();
 
-    // optional .CacheTag cache_tag = 3;
     /**
      * <code>optional .CacheTag cache_tag = 3;</code>
+     * @return Whether the cacheTag field is set.
      */
     boolean hasCacheTag();
     /**
      * <code>optional .CacheTag cache_tag = 3;</code>
+     * @return The cacheTag.
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getCacheTag();
     /**
@@ -26707,7 +29859,6 @@ public interface CacheEntryOrBuilder
      */
     org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder getCacheTagOrBuilder();
 
-    // repeated .CacheEntryRange ranges = 4;
     /**
      * <code>repeated .CacheEntryRange ranges = 4;</code>
      */
@@ -26736,155 +29887,76 @@ org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeO
    * Protobuf type {@code CacheEntry}
    */
   public static final class CacheEntry extends
-      com.google.protobuf.GeneratedMessage
-      implements CacheEntryOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:CacheEntry)
+      CacheEntryOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use CacheEntry.newBuilder() to construct.
-    private CacheEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private CacheEntry(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private CacheEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final CacheEntry defaultInstance;
-    public static CacheEntry getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public CacheEntry getDefaultInstanceForType() {
-      return defaultInstance;
+    private CacheEntry() {
+      fileKey_ = com.google.protobuf.ByteString.EMPTY;
+      filePath_ = "";
+      ranges_ = java.util.Collections.emptyList();
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private CacheEntry(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              fileKey_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              filePath_ = input.readBytes();
-              break;
-            }
-            case 26: {
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000004) == 0x00000004)) {
-                subBuilder = cacheTag_.toBuilder();
-              }
-              cacheTag_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(cacheTag_);
-                cacheTag_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000004;
-              break;
-            }
-            case 34: {
-              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
-                ranges_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange>();
-                mutable_bitField0_ |= 0x00000008;
-              }
-              ranges_.add(input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.PARSER, extensionRegistry));
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
-          ranges_ = java.util.Collections.unmodifiableList(ranges_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new CacheEntry();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<CacheEntry> PARSER =
-        new com.google.protobuf.AbstractParser<CacheEntry>() {
-      public CacheEntry parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new CacheEntry(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<CacheEntry> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional bytes file_key = 1;
     public static final int FILE_KEY_FIELD_NUMBER = 1;
-    private com.google.protobuf.ByteString fileKey_;
+    private com.google.protobuf.ByteString fileKey_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>optional bytes file_key = 1;</code>
+     * @return Whether the fileKey field is set.
      */
+    @java.lang.Override
     public boolean hasFileKey() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional bytes file_key = 1;</code>
+     * @return The fileKey.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getFileKey() {
       return fileKey_;
     }
 
-    // optional string file_path = 2;
     public static final int FILE_PATH_FIELD_NUMBER = 2;
-    private java.lang.Object filePath_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object filePath_ = "";
     /**
      * <code>optional string file_path = 2;</code>
+     * @return Whether the filePath field is set.
      */
+    @java.lang.Override
     public boolean hasFilePath() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional string file_path = 2;</code>
+     * @return The filePath.
      */
+    @java.lang.Override
     public java.lang.String getFilePath() {
       java.lang.Object ref = filePath_;
       if (ref instanceof java.lang.String) {
@@ -26901,7 +29973,9 @@ public java.lang.String getFilePath() {
     }
     /**
      * <code>optional string file_path = 2;</code>
+     * @return The bytes for filePath.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getFilePathBytes() {
       java.lang.Object ref = filePath_;
@@ -26916,40 +29990,46 @@ public java.lang.String getFilePath() {
       }
     }
 
-    // optional .CacheTag cache_tag = 3;
     public static final int CACHE_TAG_FIELD_NUMBER = 3;
     private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag cacheTag_;
     /**
      * <code>optional .CacheTag cache_tag = 3;</code>
+     * @return Whether the cacheTag field is set.
      */
+    @java.lang.Override
     public boolean hasCacheTag() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional .CacheTag cache_tag = 3;</code>
+     * @return The cacheTag.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getCacheTag() {
-      return cacheTag_;
+      return cacheTag_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance() : cacheTag_;
     }
     /**
      * <code>optional .CacheTag cache_tag = 3;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder getCacheTagOrBuilder() {
-      return cacheTag_;
+      return cacheTag_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance() : cacheTag_;
     }
 
-    // repeated .CacheEntryRange ranges = 4;
     public static final int RANGES_FIELD_NUMBER = 4;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> ranges_;
     /**
      * <code>repeated .CacheEntryRange ranges = 4;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> getRangesList() {
       return ranges_;
     }
     /**
      * <code>repeated .CacheEntryRange ranges = 4;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder> 
         getRangesOrBuilderList() {
       return ranges_;
@@ -26957,49 +30037,48 @@ public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolP
     /**
      * <code>repeated .CacheEntryRange ranges = 4;</code>
      */
+    @java.lang.Override
     public int getRangesCount() {
       return ranges_.size();
     }
     /**
      * <code>repeated .CacheEntryRange ranges = 4;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getRanges(int index) {
       return ranges_.get(index);
     }
     /**
      * <code>repeated .CacheEntryRange ranges = 4;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder getRangesOrBuilder(
         int index) {
       return ranges_.get(index);
     }
 
-    private void initFields() {
-      fileKey_ = com.google.protobuf.ByteString.EMPTY;
-      filePath_ = "";
-      cacheTag_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance();
-      ranges_ = java.util.Collections.emptyList();
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeBytes(1, fileKey_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getFilePathBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, filePath_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeMessage(3, cacheTag_);
+      if (((bitField0_ & 0x00000004) != 0)) {
+        output.writeMessage(3, getCacheTag());
       }
       for (int i = 0; i < ranges_.size(); i++) {
         output.writeMessage(4, ranges_.get(i));
@@ -27007,40 +30086,32 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(1, fileKey_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getFilePathBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, filePath_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(3, cacheTag_);
+          .computeMessageSize(3, getCacheTag());
       }
       for (int i = 0; i < ranges_.size(); i++) {
         size += com.google.protobuf.CodedOutputStream
           .computeMessageSize(4, ranges_.get(i));
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -27051,37 +30122,34 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry) obj;
 
-      boolean result = true;
-      result = result && (hasFileKey() == other.hasFileKey());
+      if (hasFileKey() != other.hasFileKey()) return false;
       if (hasFileKey()) {
-        result = result && getFileKey()
-            .equals(other.getFileKey());
+        if (!getFileKey()
+            .equals(other.getFileKey())) return false;
       }
-      result = result && (hasFilePath() == other.hasFilePath());
+      if (hasFilePath() != other.hasFilePath()) return false;
       if (hasFilePath()) {
-        result = result && getFilePath()
-            .equals(other.getFilePath());
+        if (!getFilePath()
+            .equals(other.getFilePath())) return false;
       }
-      result = result && (hasCacheTag() == other.hasCacheTag());
+      if (hasCacheTag() != other.hasCacheTag()) return false;
       if (hasCacheTag()) {
-        result = result && getCacheTag()
-            .equals(other.getCacheTag());
+        if (!getCacheTag()
+            .equals(other.getCacheTag())) return false;
       }
-      result = result && getRangesList()
-          .equals(other.getRangesList());
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getRangesList()
+          .equals(other.getRangesList())) return false;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasFileKey()) {
         hash = (37 * hash) + FILE_KEY_FIELD_NUMBER;
         hash = (53 * hash) + getFileKey().hashCode();
@@ -27103,6 +30171,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -27126,46 +30205,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ca
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -27173,14 +30267,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code CacheEntry}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:CacheEntry)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -27193,54 +30289,50 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getCacheTagFieldBuilder();
           getRangesFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         fileKey_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000001);
         filePath_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
-        if (cacheTagBuilder_ == null) {
-          cacheTag_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance();
-        } else {
-          cacheTagBuilder_.clear();
+        cacheTag_ = null;
+        if (cacheTagBuilder_ != null) {
+          cacheTagBuilder_.dispose();
+          cacheTagBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000004);
         if (rangesBuilder_ == null) {
           ranges_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000008);
         } else {
+          ranges_ = null;
           rangesBuilder_.clear();
         }
+        bitField0_ = (bitField0_ & ~0x00000008);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry result = buildPartial();
         if (!result.isInitialized()) {
@@ -27249,28 +30341,18 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntr
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.fileKey_ = fileKey_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.filePath_ = filePath_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        if (cacheTagBuilder_ == null) {
-          result.cacheTag_ = cacheTag_;
-        } else {
-          result.cacheTag_ = cacheTagBuilder_.build();
-        }
+        buildPartialRepeatedFields(result);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartialRepeatedFields(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry result) {
         if (rangesBuilder_ == null) {
-          if (((bitField0_ & 0x00000008) == 0x00000008)) {
+          if (((bitField0_ & 0x00000008) != 0)) {
             ranges_ = java.util.Collections.unmodifiableList(ranges_);
             bitField0_ = (bitField0_ & ~0x00000008);
           }
@@ -27278,11 +30360,61 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntr
         } else {
           result.ranges_ = rangesBuilder_.build();
         }
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
       }
 
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry result) {
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.fileKey_ = fileKey_;
+          to_bitField0_ |= 0x00000001;
+        }
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.filePath_ = filePath_;
+          to_bitField0_ |= 0x00000002;
+        }
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.cacheTag_ = cacheTagBuilder_ == null
+              ? cacheTag_
+              : cacheTagBuilder_.build();
+          to_bitField0_ |= 0x00000004;
+        }
+        result.bitField0_ |= to_bitField0_;
+      }
+
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry)other);
@@ -27298,8 +30430,8 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setFileKey(other.getFileKey());
         }
         if (other.hasFilePath()) {
-          bitField0_ |= 0x00000002;
           filePath_ = other.filePath_;
+          bitField0_ |= 0x00000002;
           onChanged();
         }
         if (other.hasCacheTag()) {
@@ -27324,7 +30456,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
               ranges_ = other.ranges_;
               bitField0_ = (bitField0_ & ~0x00000008);
               rangesBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getRangesFieldBuilder() : null;
             } else {
               rangesBuilder_.addAllMessages(other.ranges_);
@@ -27332,60 +30464,110 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           }
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                fileKey_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                filePath_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 26: {
+                input.readMessage(
+                    getCacheTagFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 26
+              case 34: {
+                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.PARSER,
+                        extensionRegistry);
+                if (rangesBuilder_ == null) {
+                  ensureRangesIsMutable();
+                  ranges_.add(m);
+                } else {
+                  rangesBuilder_.addMessage(m);
+                }
+                break;
+              } // case 34
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional bytes file_key = 1;
       private com.google.protobuf.ByteString fileKey_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>optional bytes file_key = 1;</code>
+       * @return Whether the fileKey field is set.
        */
+      @java.lang.Override
       public boolean hasFileKey() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional bytes file_key = 1;</code>
+       * @return The fileKey.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getFileKey() {
         return fileKey_;
       }
       /**
        * <code>optional bytes file_key = 1;</code>
+       * @param value The fileKey to set.
+       * @return This builder for chaining.
        */
       public Builder setFileKey(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         fileKey_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional bytes file_key = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFileKey() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -27394,23 +30576,27 @@ public Builder clearFileKey() {
         return this;
       }
 
-      // optional string file_path = 2;
       private java.lang.Object filePath_ = "";
       /**
        * <code>optional string file_path = 2;</code>
+       * @return Whether the filePath field is set.
        */
       public boolean hasFilePath() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional string file_path = 2;</code>
+       * @return The filePath.
        */
       public java.lang.String getFilePath() {
         java.lang.Object ref = filePath_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          filePath_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            filePath_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -27418,6 +30604,7 @@ public java.lang.String getFilePath() {
       }
       /**
        * <code>optional string file_path = 2;</code>
+       * @return The bytes for filePath.
        */
       public com.google.protobuf.ByteString
           getFilePathBytes() {
@@ -27434,56 +30621,58 @@ public java.lang.String getFilePath() {
       }
       /**
        * <code>optional string file_path = 2;</code>
+       * @param value The filePath to set.
+       * @return This builder for chaining.
        */
       public Builder setFilePath(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         filePath_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional string file_path = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFilePath() {
-        bitField0_ = (bitField0_ & ~0x00000002);
         filePath_ = getDefaultInstance().getFilePath();
+        bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
       /**
        * <code>optional string file_path = 2;</code>
+       * @param value The bytes for filePath to set.
+       * @return This builder for chaining.
        */
       public Builder setFilePathBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         filePath_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
 
-      // optional .CacheTag cache_tag = 3;
-      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag cacheTag_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag cacheTag_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder> cacheTagBuilder_;
       /**
        * <code>optional .CacheTag cache_tag = 3;</code>
+       * @return Whether the cacheTag field is set.
        */
       public boolean hasCacheTag() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional .CacheTag cache_tag = 3;</code>
+       * @return The cacheTag.
        */
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getCacheTag() {
         if (cacheTagBuilder_ == null) {
-          return cacheTag_;
+          return cacheTag_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance() : cacheTag_;
         } else {
           return cacheTagBuilder_.getMessage();
         }
@@ -27497,11 +30686,11 @@ public Builder setCacheTag(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProt
             throw new NullPointerException();
           }
           cacheTag_ = value;
-          onChanged();
         } else {
           cacheTagBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000004;
+        onChanged();
         return this;
       }
       /**
@@ -27511,11 +30700,11 @@ public Builder setCacheTag(
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder builderForValue) {
         if (cacheTagBuilder_ == null) {
           cacheTag_ = builderForValue.build();
-          onChanged();
         } else {
           cacheTagBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000004;
+        onChanged();
         return this;
       }
       /**
@@ -27523,31 +30712,33 @@ public Builder setCacheTag(
        */
       public Builder mergeCacheTag(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag value) {
         if (cacheTagBuilder_ == null) {
-          if (((bitField0_ & 0x00000004) == 0x00000004) &&
-              cacheTag_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance()) {
-            cacheTag_ =
-              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.newBuilder(cacheTag_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000004) != 0) &&
+            cacheTag_ != null &&
+            cacheTag_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance()) {
+            getCacheTagBuilder().mergeFrom(value);
           } else {
             cacheTag_ = value;
           }
-          onChanged();
         } else {
           cacheTagBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000004;
+        if (cacheTag_ != null) {
+          bitField0_ |= 0x00000004;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .CacheTag cache_tag = 3;</code>
        */
       public Builder clearCacheTag() {
-        if (cacheTagBuilder_ == null) {
-          cacheTag_ = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance();
-          onChanged();
-        } else {
-          cacheTagBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000004);
+        cacheTag_ = null;
+        if (cacheTagBuilder_ != null) {
+          cacheTagBuilder_.dispose();
+          cacheTagBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -27565,19 +30756,20 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagO
         if (cacheTagBuilder_ != null) {
           return cacheTagBuilder_.getMessageOrBuilder();
         } else {
-          return cacheTag_;
+          return cacheTag_ == null ?
+              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance() : cacheTag_;
         }
       }
       /**
        * <code>optional .CacheTag cache_tag = 3;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder> 
           getCacheTagFieldBuilder() {
         if (cacheTagBuilder_ == null) {
-          cacheTagBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          cacheTagBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder>(
-                  cacheTag_,
+                  getCacheTag(),
                   getParentForChildren(),
                   isClean());
           cacheTag_ = null;
@@ -27585,17 +30777,16 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagO
         return cacheTagBuilder_;
       }
 
-      // repeated .CacheEntryRange ranges = 4;
       private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> ranges_ =
         java.util.Collections.emptyList();
       private void ensureRangesIsMutable() {
-        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
+        if (!((bitField0_ & 0x00000008) != 0)) {
           ranges_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange>(ranges_);
           bitField0_ |= 0x00000008;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder> rangesBuilder_;
 
       /**
@@ -27727,7 +30918,8 @@ public Builder addAllRanges(
           java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> values) {
         if (rangesBuilder_ == null) {
           ensureRangesIsMutable();
-          super.addAll(values, ranges_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, ranges_);
           onChanged();
         } else {
           rangesBuilder_.addAllMessages(values);
@@ -27810,66 +31002,126 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntr
            getRangesBuilderList() {
         return getRangesFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder> 
           getRangesFieldBuilder() {
         if (rangesBuilder_ == null) {
-          rangesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          rangesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder>(
                   ranges_,
-                  ((bitField0_ & 0x00000008) == 0x00000008),
+                  ((bitField0_ & 0x00000008) != 0),
                   getParentForChildren(),
                   isClean());
           ranges_ = null;
         }
         return rangesBuilder_;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:CacheEntry)
     }
 
+    // @@protoc_insertion_point(class_scope:CacheEntry)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry DEFAULT_INSTANCE;
     static {
-      defaultInstance = new CacheEntry(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheEntry>
+        PARSER = new com.google.protobuf.AbstractParser<CacheEntry>() {
+      @java.lang.Override
+      public CacheEntry parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<CacheEntry> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<CacheEntry> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:CacheEntry)
   }
 
-  public interface CacheTagOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface CacheTagOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:CacheTag)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string table_name = 1;
     /**
      * <code>optional string table_name = 1;</code>
+     * @return Whether the tableName field is set.
      */
     boolean hasTableName();
     /**
      * <code>optional string table_name = 1;</code>
+     * @return The tableName.
      */
     java.lang.String getTableName();
     /**
      * <code>optional string table_name = 1;</code>
+     * @return The bytes for tableName.
      */
     com.google.protobuf.ByteString
         getTableNameBytes();
 
-    // repeated string partition_desc = 2;
     /**
      * <code>repeated string partition_desc = 2;</code>
+     * @return A list containing the partitionDesc.
      */
     java.util.List<java.lang.String>
-    getPartitionDescList();
+        getPartitionDescList();
     /**
      * <code>repeated string partition_desc = 2;</code>
+     * @return The count of partitionDesc.
      */
     int getPartitionDescCount();
     /**
      * <code>repeated string partition_desc = 2;</code>
+     * @param index The index of the element to return.
+     * @return The partitionDesc at the given index.
      */
     java.lang.String getPartitionDesc(int index);
     /**
      * <code>repeated string partition_desc = 2;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the partitionDesc at the given index.
      */
     com.google.protobuf.ByteString
         getPartitionDescBytes(int index);
@@ -27878,121 +31130,57 @@ public interface CacheTagOrBuilder
    * Protobuf type {@code CacheTag}
    */
   public static final class CacheTag extends
-      com.google.protobuf.GeneratedMessage
-      implements CacheTagOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:CacheTag)
+      CacheTagOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use CacheTag.newBuilder() to construct.
-    private CacheTag(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private CacheTag(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private CacheTag(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final CacheTag defaultInstance;
-    public static CacheTag getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public CacheTag getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
     }
-    private CacheTag(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              tableName_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-                partitionDesc_ = new com.google.protobuf.LazyStringArrayList();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              partitionDesc_.add(input.readBytes());
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-          partitionDesc_ = new com.google.protobuf.UnmodifiableLazyStringList(partitionDesc_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    private CacheTag() {
+      tableName_ = "";
+      partitionDesc_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+    }
+
+    @java.lang.Override
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new CacheTag();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<CacheTag> PARSER =
-        new com.google.protobuf.AbstractParser<CacheTag>() {
-      public CacheTag parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new CacheTag(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<CacheTag> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string table_name = 1;
     public static final int TABLE_NAME_FIELD_NUMBER = 1;
-    private java.lang.Object tableName_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object tableName_ = "";
     /**
      * <code>optional string table_name = 1;</code>
+     * @return Whether the tableName field is set.
      */
+    @java.lang.Override
     public boolean hasTableName() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string table_name = 1;</code>
+     * @return The tableName.
      */
+    @java.lang.Override
     public java.lang.String getTableName() {
       java.lang.Object ref = tableName_;
       if (ref instanceof java.lang.String) {
@@ -28009,7 +31197,9 @@ public java.lang.String getTableName() {
     }
     /**
      * <code>optional string table_name = 1;</code>
+     * @return The bytes for tableName.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getTableNameBytes() {
       java.lang.Object ref = tableName_;
@@ -28024,92 +31214,88 @@ public java.lang.String getTableName() {
       }
     }
 
-    // repeated string partition_desc = 2;
     public static final int PARTITION_DESC_FIELD_NUMBER = 2;
-    private com.google.protobuf.LazyStringList partitionDesc_;
+    @SuppressWarnings("serial")
+    private com.google.protobuf.LazyStringArrayList partitionDesc_ =
+        com.google.protobuf.LazyStringArrayList.emptyList();
     /**
      * <code>repeated string partition_desc = 2;</code>
+     * @return A list containing the partitionDesc.
      */
-    public java.util.List<java.lang.String>
+    public com.google.protobuf.ProtocolStringList
         getPartitionDescList() {
       return partitionDesc_;
     }
     /**
      * <code>repeated string partition_desc = 2;</code>
+     * @return The count of partitionDesc.
      */
     public int getPartitionDescCount() {
       return partitionDesc_.size();
     }
     /**
      * <code>repeated string partition_desc = 2;</code>
+     * @param index The index of the element to return.
+     * @return The partitionDesc at the given index.
      */
     public java.lang.String getPartitionDesc(int index) {
       return partitionDesc_.get(index);
     }
     /**
      * <code>repeated string partition_desc = 2;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the partitionDesc at the given index.
      */
     public com.google.protobuf.ByteString
         getPartitionDescBytes(int index) {
       return partitionDesc_.getByteString(index);
     }
 
-    private void initFields() {
-      tableName_ = "";
-      partitionDesc_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getTableNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, tableName_);
       }
       for (int i = 0; i < partitionDesc_.size(); i++) {
-        output.writeBytes(2, partitionDesc_.getByteString(i));
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, partitionDesc_.getRaw(i));
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getTableNameBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, tableName_);
       }
       {
         int dataSize = 0;
         for (int i = 0; i < partitionDesc_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(partitionDesc_.getByteString(i));
+          dataSize += computeStringSizeNoTag(partitionDesc_.getRaw(i));
         }
         size += dataSize;
         size += 1 * getPartitionDescList().size();
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -28120,27 +31306,24 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag) obj;
 
-      boolean result = true;
-      result = result && (hasTableName() == other.hasTableName());
+      if (hasTableName() != other.hasTableName()) return false;
       if (hasTableName()) {
-        result = result && getTableName()
-            .equals(other.getTableName());
+        if (!getTableName()
+            .equals(other.getTableName())) return false;
       }
-      result = result && getPartitionDescList()
-          .equals(other.getPartitionDescList());
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getPartitionDescList()
+          .equals(other.getPartitionDescList())) return false;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasTableName()) {
         hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
         hash = (53 * hash) + getTableName().hashCode();
@@ -28154,6 +31337,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -28177,46 +31371,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ca
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -28224,14 +31433,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code CacheTag}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:CacheTag)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -28240,44 +31451,36 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         tableName_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
-        partitionDesc_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
+        partitionDesc_ =
+            com.google.protobuf.LazyStringArrayList.emptyList();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag result = buildPartial();
         if (!result.isInitialized()) {
@@ -28286,25 +31489,61 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.tableName_ = tableName_;
           to_bitField0_ |= 0x00000001;
         }
-        result.tableName_ = tableName_;
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          partitionDesc_ = new com.google.protobuf.UnmodifiableLazyStringList(
-              partitionDesc_);
-          bitField0_ = (bitField0_ & ~0x00000002);
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          partitionDesc_.makeImmutable();
+          result.partitionDesc_ = partitionDesc_;
         }
-        result.partitionDesc_ = partitionDesc_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag)other);
@@ -28317,14 +31556,14 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag other) {
         if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance()) return this;
         if (other.hasTableName()) {
-          bitField0_ |= 0x00000001;
           tableName_ = other.tableName_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (!other.partitionDesc_.isEmpty()) {
           if (partitionDesc_.isEmpty()) {
             partitionDesc_ = other.partitionDesc_;
-            bitField0_ = (bitField0_ & ~0x00000002);
+            bitField0_ |= 0x00000002;
           } else {
             ensurePartitionDescIsMutable();
             partitionDesc_.addAll(other.partitionDesc_);
@@ -28332,49 +31571,80 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                tableName_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                com.google.protobuf.ByteString bs = input.readBytes();
+                ensurePartitionDescIsMutable();
+                partitionDesc_.add(bs);
+                break;
+              } // case 18
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string table_name = 1;
       private java.lang.Object tableName_ = "";
       /**
        * <code>optional string table_name = 1;</code>
+       * @return Whether the tableName field is set.
        */
       public boolean hasTableName() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string table_name = 1;</code>
+       * @return The tableName.
        */
       public java.lang.String getTableName() {
         java.lang.Object ref = tableName_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          tableName_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            tableName_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -28382,6 +31652,7 @@ public java.lang.String getTableName() {
       }
       /**
        * <code>optional string table_name = 1;</code>
+       * @return The bytes for tableName.
        */
       public com.google.protobuf.ByteString
           getTableNameBytes() {
@@ -28398,69 +31669,77 @@ public java.lang.String getTableName() {
       }
       /**
        * <code>optional string table_name = 1;</code>
+       * @param value The tableName to set.
+       * @return This builder for chaining.
        */
       public Builder setTableName(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         tableName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string table_name = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearTableName() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         tableName_ = getDefaultInstance().getTableName();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string table_name = 1;</code>
+       * @param value The bytes for tableName to set.
+       * @return This builder for chaining.
        */
       public Builder setTableNameBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         tableName_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // repeated string partition_desc = 2;
-      private com.google.protobuf.LazyStringList partitionDesc_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private com.google.protobuf.LazyStringArrayList partitionDesc_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
       private void ensurePartitionDescIsMutable() {
-        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
+        if (!partitionDesc_.isModifiable()) {
           partitionDesc_ = new com.google.protobuf.LazyStringArrayList(partitionDesc_);
-          bitField0_ |= 0x00000002;
-         }
+        }
+        bitField0_ |= 0x00000002;
       }
       /**
        * <code>repeated string partition_desc = 2;</code>
+       * @return A list containing the partitionDesc.
        */
-      public java.util.List<java.lang.String>
+      public com.google.protobuf.ProtocolStringList
           getPartitionDescList() {
-        return java.util.Collections.unmodifiableList(partitionDesc_);
+        partitionDesc_.makeImmutable();
+        return partitionDesc_;
       }
       /**
        * <code>repeated string partition_desc = 2;</code>
+       * @return The count of partitionDesc.
        */
       public int getPartitionDescCount() {
         return partitionDesc_.size();
       }
       /**
        * <code>repeated string partition_desc = 2;</code>
+       * @param index The index of the element to return.
+       * @return The partitionDesc at the given index.
        */
       public java.lang.String getPartitionDesc(int index) {
         return partitionDesc_.get(index);
       }
       /**
        * <code>repeated string partition_desc = 2;</code>
+       * @param index The index of the value to return.
+       * @return The bytes of the partitionDesc at the given index.
        */
       public com.google.protobuf.ByteString
           getPartitionDescBytes(int index) {
@@ -28468,94 +31747,159 @@ public java.lang.String getPartitionDesc(int index) {
       }
       /**
        * <code>repeated string partition_desc = 2;</code>
+       * @param index The index to set the value at.
+       * @param value The partitionDesc to set.
+       * @return This builder for chaining.
        */
       public Builder setPartitionDesc(
           int index, java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensurePartitionDescIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensurePartitionDescIsMutable();
         partitionDesc_.set(index, value);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string partition_desc = 2;</code>
+       * @param value The partitionDesc to add.
+       * @return This builder for chaining.
        */
       public Builder addPartitionDesc(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensurePartitionDescIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensurePartitionDescIsMutable();
         partitionDesc_.add(value);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string partition_desc = 2;</code>
+       * @param values The partitionDesc to add.
+       * @return This builder for chaining.
        */
       public Builder addAllPartitionDesc(
           java.lang.Iterable<java.lang.String> values) {
         ensurePartitionDescIsMutable();
-        super.addAll(values, partitionDesc_);
+        com.google.protobuf.AbstractMessageLite.Builder.addAll(
+            values, partitionDesc_);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string partition_desc = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearPartitionDesc() {
-        partitionDesc_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
+        partitionDesc_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+        bitField0_ = (bitField0_ & ~0x00000002);;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string partition_desc = 2;</code>
+       * @param value The bytes of the partitionDesc to add.
+       * @return This builder for chaining.
        */
       public Builder addPartitionDescBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensurePartitionDescIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensurePartitionDescIsMutable();
         partitionDesc_.add(value);
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:CacheTag)
     }
 
+    // @@protoc_insertion_point(class_scope:CacheTag)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag DEFAULT_INSTANCE;
     static {
-      defaultInstance = new CacheTag(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheTag>
+        PARSER = new com.google.protobuf.AbstractParser<CacheTag>() {
+      @java.lang.Override
+      public CacheTag parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<CacheTag> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<CacheTag> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:CacheTag)
   }
 
-  public interface CacheEntryRangeOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface CacheEntryRangeOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:CacheEntryRange)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional int64 start = 1;
     /**
      * <code>optional int64 start = 1;</code>
+     * @return Whether the start field is set.
      */
     boolean hasStart();
     /**
      * <code>optional int64 start = 1;</code>
+     * @return The start.
      */
     long getStart();
 
-    // optional int64 end = 2;
     /**
      * <code>optional int64 end = 2;</code>
+     * @return Whether the end field is set.
      */
     boolean hasEnd();
     /**
      * <code>optional int64 end = 2;</code>
+     * @return The end.
      */
     long getEnd();
   }
@@ -28563,186 +31907,118 @@ public interface CacheEntryRangeOrBuilder
    * Protobuf type {@code CacheEntryRange}
    */
   public static final class CacheEntryRange extends
-      com.google.protobuf.GeneratedMessage
-      implements CacheEntryRangeOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:CacheEntryRange)
+      CacheEntryRangeOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use CacheEntryRange.newBuilder() to construct.
-    private CacheEntryRange(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private CacheEntryRange(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private CacheEntryRange(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final CacheEntryRange defaultInstance;
-    public static CacheEntryRange getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public CacheEntryRange getDefaultInstanceForType() {
-      return defaultInstance;
+    private CacheEntryRange() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private CacheEntryRange(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              start_ = input.readInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              end_ = input.readInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new CacheEntryRange();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<CacheEntryRange> PARSER =
-        new com.google.protobuf.AbstractParser<CacheEntryRange>() {
-      public CacheEntryRange parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new CacheEntryRange(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<CacheEntryRange> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional int64 start = 1;
     public static final int START_FIELD_NUMBER = 1;
-    private long start_;
+    private long start_ = 0L;
     /**
      * <code>optional int64 start = 1;</code>
+     * @return Whether the start field is set.
      */
+    @java.lang.Override
     public boolean hasStart() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional int64 start = 1;</code>
+     * @return The start.
      */
+    @java.lang.Override
     public long getStart() {
       return start_;
     }
 
-    // optional int64 end = 2;
     public static final int END_FIELD_NUMBER = 2;
-    private long end_;
+    private long end_ = 0L;
     /**
      * <code>optional int64 end = 2;</code>
+     * @return Whether the end field is set.
      */
+    @java.lang.Override
     public boolean hasEnd() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional int64 end = 2;</code>
+     * @return The end.
      */
+    @java.lang.Override
     public long getEnd() {
       return end_;
     }
 
-    private void initFields() {
-      start_ = 0L;
-      end_ = 0L;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeInt64(1, start_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeInt64(2, end_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(1, start_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(2, end_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -28753,43 +32029,53 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange) obj;
 
-      boolean result = true;
-      result = result && (hasStart() == other.hasStart());
+      if (hasStart() != other.hasStart()) return false;
       if (hasStart()) {
-        result = result && (getStart()
-            == other.getStart());
+        if (getStart()
+            != other.getStart()) return false;
       }
-      result = result && (hasEnd() == other.hasEnd());
+      if (hasEnd() != other.hasEnd()) return false;
       if (hasEnd()) {
-        result = result && (getEnd()
-            == other.getEnd());
+        if (getEnd()
+            != other.getEnd()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasStart()) {
         hash = (37 * hash) + START_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getStart());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getStart());
       }
       if (hasEnd()) {
         hash = (37 * hash) + END_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getEnd());
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getEnd());
       }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -28813,46 +32099,61 @@ public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.Ca
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -28860,14 +32161,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code CacheEntryRange}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:CacheEntryRange)
+        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -28876,44 +32179,35 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         start_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
         end_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange build() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange result = buildPartial();
         if (!result.isInitialized()) {
@@ -28922,23 +32216,61 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntr
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange buildPartial() {
         org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.start_ = start_;
           to_bitField0_ |= 0x00000001;
         }
-        result.start_ = start_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.end_ = end_;
           to_bitField0_ |= 0x00000002;
         }
-        result.end_ = end_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange) {
           return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange)other);
@@ -28957,57 +32289,90 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtoc
           setEnd(other.getEnd());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 8: {
+                start_ = input.readInt64();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 8
+              case 16: {
+                end_ = input.readInt64();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 16
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional int64 start = 1;
       private long start_ ;
       /**
        * <code>optional int64 start = 1;</code>
+       * @return Whether the start field is set.
        */
+      @java.lang.Override
       public boolean hasStart() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional int64 start = 1;</code>
+       * @return The start.
        */
+      @java.lang.Override
       public long getStart() {
         return start_;
       }
       /**
        * <code>optional int64 start = 1;</code>
+       * @param value The start to set.
+       * @return This builder for chaining.
        */
       public Builder setStart(long value) {
-        bitField0_ |= 0x00000001;
+
         start_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 start = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearStart() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -29016,31 +32381,38 @@ public Builder clearStart() {
         return this;
       }
 
-      // optional int64 end = 2;
       private long end_ ;
       /**
        * <code>optional int64 end = 2;</code>
+       * @return Whether the end field is set.
        */
+      @java.lang.Override
       public boolean hasEnd() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional int64 end = 2;</code>
+       * @return The end.
        */
+      @java.lang.Override
       public long getEnd() {
         return end_;
       }
       /**
        * <code>optional int64 end = 2;</code>
+       * @param value The end to set.
+       * @return This builder for chaining.
        */
       public Builder setEnd(long value) {
-        bitField0_ |= 0x00000002;
+
         end_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 end = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearEnd() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -29048,16 +32420,68 @@ public Builder clearEnd() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:CacheEntryRange)
     }
 
+    // @@protoc_insertion_point(class_scope:CacheEntryRange)
+    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange DEFAULT_INSTANCE;
     static {
-      defaultInstance = new CacheEntryRange(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange();
+    }
+
+    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheEntryRange>
+        PARSER = new com.google.protobuf.AbstractParser<CacheEntryRange>() {
+      @java.lang.Override
+      public CacheEntryRange parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<CacheEntryRange> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<CacheEntryRange> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:CacheEntryRange)
   }
 
   /**
@@ -30234,212 +33658,212 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
     // @@protoc_insertion_point(class_scope:LlapManagementProtocol)
   }
 
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_UserPayloadProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_UserPayloadProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_EntityDescriptorProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_EntityDescriptorProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_IOSpecProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_IOSpecProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_GroupInputSpecProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_GroupInputSpecProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_SignableVertexSpec_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_SignableVertexSpec_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_VertexOrBinary_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_VertexOrBinary_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_FragmentRuntimeInfo_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_FragmentRuntimeInfo_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_QueryIdentifierProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_QueryIdentifierProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_NotTezEvent_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_NotTezEvent_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_SubmitWorkRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_SubmitWorkRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_RegisterDagRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_RegisterDagRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_RegisterDagResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_RegisterDagResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_SubmitWorkResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_SubmitWorkResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_SourceStateUpdatedRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_SourceStateUpdatedRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_SourceStateUpdatedResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_SourceStateUpdatedResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_QueryCompleteRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_QueryCompleteRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_QueryCompleteResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_QueryCompleteResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_TerminateFragmentRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_TerminateFragmentRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_TerminateFragmentResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_TerminateFragmentResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_UpdateFragmentRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_UpdateFragmentRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_UpdateFragmentResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_UpdateFragmentResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_GetTokenRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_GetTokenRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_GetTokenResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_GetTokenResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_LlapOutputSocketInitMessage_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_LlapOutputSocketInitMessage_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_PurgeCacheRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_PurgeCacheRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_PurgeCacheResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_PurgeCacheResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_MapEntry_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_MapEntry_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_GetDaemonMetricsRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_GetDaemonMetricsRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_GetDaemonMetricsResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_GetDaemonMetricsResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_SetCapacityRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_SetCapacityRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_SetCapacityResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_SetCapacityResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_EvictEntityRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_EvictEntityRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_TableProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_TableProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_EvictEntityResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_EvictEntityResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_GetCacheContentRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_GetCacheContentRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_GetCacheContentResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_GetCacheContentResponseProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_CacheEntryList_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_CacheEntryList_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_CacheEntry_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_CacheEntry_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_CacheTag_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_CacheTag_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_CacheEntryRange_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_CacheEntryRange_fieldAccessorTable;
 
   public static com.google.protobuf.Descriptors.FileDescriptor
       getDescriptor() {
     return descriptor;
   }
-  private static com.google.protobuf.Descriptors.FileDescriptor
+  private static  com.google.protobuf.Descriptors.FileDescriptor
       descriptor;
   static {
     java.lang.String[] descriptorData = {
@@ -30452,7 +33876,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "-\n\rio_descriptor\030\002 \001(\0132\026.EntityDescripto" +
       "rProto\022\033\n\023physical_edge_count\030\003 \001(\005\"z\n\023G" +
       "roupInputSpecProto\022\022\n\ngroup_name\030\001 \001(\t\022\026" +
-      "\n\016group_vertices\030\002 \003(\t\0227\n\027merged_input_d",
+      "\n\016group_vertices\030\002 \003(\t\0227\n\027merged_input_d" +
       "escriptor\030\003 \001(\0132\026.EntityDescriptorProto\"" +
       "\314\003\n\022SignableVertexSpec\022\014\n\004user\030\001 \001(\t\022\026\n\016" +
       "signatureKeyId\030\002 \001(\003\022/\n\020query_identifier" +
@@ -30462,7 +33886,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "en_identifier\030\010 \001(\t\0224\n\024processor_descrip" +
       "tor\030\t \001(\0132\026.EntityDescriptorProto\022!\n\013inp" +
       "ut_specs\030\n \003(\0132\014.IOSpecProto\022\"\n\014output_s" +
-      "pecs\030\013 \003(\0132\014.IOSpecProto\0221\n\023grouped_inpu",
+      "pecs\030\013 \003(\0132\014.IOSpecProto\0221\n\023grouped_inpu" +
       "t_specs\030\014 \003(\0132\024.GroupInputSpecProto\022\032\n\022v" +
       "ertex_parallelism\030\r \001(\005\022%\n\026is_external_s" +
       "ubmission\030\016 \001(\010:\005false\"K\n\016VertexOrBinary" +
@@ -30472,7 +33896,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "\022-\n%num_self_and_upstream_completed_task" +
       "s\030\002 \001(\005\022\033\n\023within_dag_priority\030\003 \001(\005\022\026\n\016" +
       "dag_start_time\030\004 \001(\003\022 \n\030first_attempt_st" +
-      "art_time\030\005 \001(\003\022\"\n\032current_attempt_start_",
+      "art_time\030\005 \001(\003\022\"\n\032current_attempt_start_" +
       "time\030\006 \001(\003\"d\n\024QueryIdentifierProto\022\035\n\025ap" +
       "plication_id_string\030\001 \001(\t\022\021\n\tdag_index\030\002" +
       " \001(\005\022\032\n\022app_attempt_number\030\003 \001(\005\"l\n\013NotT" +
@@ -30482,7 +33906,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "uestProto\022\"\n\twork_spec\030\001 \001(\0132\017.VertexOrB" +
       "inary\022\033\n\023work_spec_signature\030\002 \001(\014\022\027\n\017fr" +
       "agment_number\030\003 \001(\005\022\026\n\016attempt_number\030\004 " +
-      "\001(\005\022\033\n\023container_id_string\030\005 \001(\t\022\017\n\007am_h",
+      "\001(\005\022\033\n\023container_id_string\030\005 \001(\t\022\017\n\007am_h" +
       "ost\030\006 \001(\t\022\017\n\007am_port\030\007 \001(\005\022\032\n\022credential" +
       "s_binary\030\010 \001(\014\0223\n\025fragment_runtime_info\030" +
       "\t \001(\0132\024.FragmentRuntimeInfo\022\033\n\023initial_e" +
@@ -30492,7 +33916,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "st\030\016 \001(\010:\005false\"t\n\027RegisterDagRequestPro" +
       "to\022\014\n\004user\030\001 \001(\t\022/\n\020query_identifier\030\002 \002" +
       "(\0132\025.QueryIdentifierProto\022\032\n\022credentials" +
-      "_binary\030\003 \001(\014\"\032\n\030RegisterDagResponseProt",
+      "_binary\030\003 \001(\014\"\032\n\030RegisterDagResponseProt" +
       "o\"b\n\027SubmitWorkResponseProto\022/\n\020submissi" +
       "on_state\030\001 \001(\0162\025.SubmissionStateProto\022\026\n" +
       "\016unique_node_id\030\002 \001(\t\"\205\001\n\036SourceStateUpd" +
@@ -30502,7 +33926,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "\037SourceStateUpdatedResponseProto\"e\n\031Quer" +
       "yCompleteRequestProto\022/\n\020query_identifie" +
       "r\030\001 \001(\0132\025.QueryIdentifierProto\022\027\n\014delete" +
-      "_delay\030\002 \001(\003:\0010\"\034\n\032QueryCompleteResponse",
+      "_delay\030\002 \001(\003:\0010\"\034\n\032QueryCompleteResponse" +
       "Proto\"t\n\035TerminateFragmentRequestProto\022/" +
       "\n\020query_identifier\030\001 \001(\0132\025.QueryIdentifi" +
       "erProto\022\"\n\032fragment_identifier_string\030\002 " +
@@ -30512,7 +33936,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "\032fragment_identifier_string\030\002 \001(\t\022\025\n\ris_" +
       "guaranteed\030\003 \001(\010\"D\n\033UpdateFragmentRespon" +
       "seProto\022\016\n\006result\030\001 \001(\010\022\025\n\ris_guaranteed" +
-      "\030\002 \001(\010\"&\n\024GetTokenRequestProto\022\016\n\006app_id",
+      "\030\002 \001(\010\"&\n\024GetTokenRequestProto\022\016\n\006app_id" +
       "\030\001 \001(\t\"&\n\025GetTokenResponseProto\022\r\n\005token" +
       "\030\001 \001(\014\"A\n\033LlapOutputSocketInitMessage\022\023\n" +
       "\013fragment_id\030\001 \002(\t\022\r\n\005token\030\002 \001(\014\"\030\n\026Pur" +
@@ -30522,7 +33946,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "etDaemonMetricsRequestProto\";\n\035GetDaemon" +
       "MetricsResponseProto\022\032\n\007metrics\030\001 \003(\0132\t." +
       "MapEntry\"A\n\027SetCapacityRequestProto\022\023\n\013e" +
-      "xecutorNum\030\001 \001(\005\022\021\n\tqueueSize\030\002 \001(\005\"\032\n\030S",
+      "xecutorNum\030\001 \001(\005\022\021\n\tqueueSize\030\002 \001(\005\"\032\n\030S" +
       "etCapacityResponseProto\"F\n\027EvictEntityRe" +
       "questProto\022\017\n\007db_name\030\001 \002(\t\022\032\n\005table\030\002 \003" +
       "(\0132\013.TableProto\"D\n\nTableProto\022\022\n\ntable_n" +
@@ -30532,7 +33956,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "stProto\"?\n\034GetCacheContentResponseProto\022" +
       "\037\n\006result\030\001 \001(\0132\017.CacheEntryList\".\n\016Cach" +
       "eEntryList\022\034\n\007entries\030\001 \003(\0132\013.CacheEntry" +
-      "\"q\n\nCacheEntry\022\020\n\010file_key\030\001 \001(\014\022\021\n\tfile",
+      "\"q\n\nCacheEntry\022\020\n\010file_key\030\001 \001(\014\022\021\n\tfile" +
       "_path\030\002 \001(\t\022\034\n\tcache_tag\030\003 \001(\0132\t.CacheTa" +
       "g\022 \n\006ranges\030\004 \003(\0132\020.CacheEntryRange\"6\n\010C" +
       "acheTag\022\022\n\ntable_name\030\001 \001(\t\022\026\n\016partition" +
@@ -30542,7 +33966,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "sionStateProto\022\014\n\010ACCEPTED\020\001\022\014\n\010REJECTED" +
       "\020\002\022\021\n\rEVICTED_OTHER\020\0032\337\003\n\022LlapDaemonProt" +
       "ocol\022B\n\013registerDag\022\030.RegisterDagRequest" +
-      "Proto\032\031.RegisterDagResponseProto\022?\n\nsubm",
+      "Proto\032\031.RegisterDagResponseProto\022?\n\nsubm" +
       "itWork\022\027.SubmitWorkRequestProto\032\030.Submit" +
       "WorkResponseProto\022W\n\022sourceStateUpdated\022" +
       "\037.SourceStateUpdatedRequestProto\032 .Sourc" +
@@ -30552,7 +33976,7 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "t\022\036.TerminateFragmentRequestProto\032\037.Term" +
       "inateFragmentResponseProto\022K\n\016updateFrag" +
       "ment\022\033.UpdateFragmentRequestProto\032\034.Upda" +
-      "teFragmentResponseProto2\311\003\n\026LlapManageme",
+      "teFragmentResponseProto2\311\003\n\026LlapManageme" +
       "ntProtocol\022C\n\022getDelegationToken\022\025.GetTo" +
       "kenRequestProto\032\026.GetTokenResponseProto\022" +
       "?\n\npurgeCache\022\027.PurgeCacheRequestProto\032\030" +
@@ -30562,263 +33986,255 @@ public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheC
       "ty\022\030.SetCapacityRequestProto\032\031.SetCapaci" +
       "tyResponseProto\022B\n\013evictEntity\022\030.EvictEn" +
       "tityRequestProto\032\031.EvictEntityResponsePr" +
-      "oto\022N\n\017getCacheContent\022\034.GetCacheContent",
+      "oto\022N\n\017getCacheContent\022\034.GetCacheContent" +
       "RequestProto\032\035.GetCacheContentResponsePr" +
       "otoBH\n&org.apache.hadoop.hive.llap.daemo" +
       "n.rpcB\030LlapDaemonProtocolProtos\210\001\001\240\001\001"
     };
-    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
-      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
-        public com.google.protobuf.ExtensionRegistry assignDescriptors(
-            com.google.protobuf.Descriptors.FileDescriptor root) {
-          descriptor = root;
-          internal_static_UserPayloadProto_descriptor =
-            getDescriptor().getMessageTypes().get(0);
-          internal_static_UserPayloadProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UserPayloadProto_descriptor,
-              new java.lang.String[] { "UserPayload", "Version", });
-          internal_static_EntityDescriptorProto_descriptor =
-            getDescriptor().getMessageTypes().get(1);
-          internal_static_EntityDescriptorProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_EntityDescriptorProto_descriptor,
-              new java.lang.String[] { "ClassName", "UserPayload", "HistoryText", });
-          internal_static_IOSpecProto_descriptor =
-            getDescriptor().getMessageTypes().get(2);
-          internal_static_IOSpecProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_IOSpecProto_descriptor,
-              new java.lang.String[] { "ConnectedVertexName", "IoDescriptor", "PhysicalEdgeCount", });
-          internal_static_GroupInputSpecProto_descriptor =
-            getDescriptor().getMessageTypes().get(3);
-          internal_static_GroupInputSpecProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_GroupInputSpecProto_descriptor,
-              new java.lang.String[] { "GroupName", "GroupVertices", "MergedInputDescriptor", });
-          internal_static_SignableVertexSpec_descriptor =
-            getDescriptor().getMessageTypes().get(4);
-          internal_static_SignableVertexSpec_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_SignableVertexSpec_descriptor,
-              new java.lang.String[] { "User", "SignatureKeyId", "QueryIdentifier", "HiveQueryId", "DagName", "VertexName", "VertexIndex", "TokenIdentifier", "ProcessorDescriptor", "InputSpecs", "OutputSpecs", "GroupedInputSpecs", "VertexParallelism", "IsExternalSubmission", });
-          internal_static_VertexOrBinary_descriptor =
-            getDescriptor().getMessageTypes().get(5);
-          internal_static_VertexOrBinary_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_VertexOrBinary_descriptor,
-              new java.lang.String[] { "Vertex", "VertexBinary", });
-          internal_static_FragmentRuntimeInfo_descriptor =
-            getDescriptor().getMessageTypes().get(6);
-          internal_static_FragmentRuntimeInfo_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_FragmentRuntimeInfo_descriptor,
-              new java.lang.String[] { "NumSelfAndUpstreamTasks", "NumSelfAndUpstreamCompletedTasks", "WithinDagPriority", "DagStartTime", "FirstAttemptStartTime", "CurrentAttemptStartTime", });
-          internal_static_QueryIdentifierProto_descriptor =
-            getDescriptor().getMessageTypes().get(7);
-          internal_static_QueryIdentifierProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_QueryIdentifierProto_descriptor,
-              new java.lang.String[] { "ApplicationIdString", "DagIndex", "AppAttemptNumber", });
-          internal_static_NotTezEvent_descriptor =
-            getDescriptor().getMessageTypes().get(8);
-          internal_static_NotTezEvent_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_NotTezEvent_descriptor,
-              new java.lang.String[] { "InputEventProtoBytes", "VertexName", "DestInputName", "KeyId", });
-          internal_static_SubmitWorkRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(9);
-          internal_static_SubmitWorkRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_SubmitWorkRequestProto_descriptor,
-              new java.lang.String[] { "WorkSpec", "WorkSpecSignature", "FragmentNumber", "AttemptNumber", "ContainerIdString", "AmHost", "AmPort", "CredentialsBinary", "FragmentRuntimeInfo", "InitialEventBytes", "InitialEventSignature", "IsGuaranteed", "Jwt", "IsExternalClientRequest", });
-          internal_static_RegisterDagRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(10);
-          internal_static_RegisterDagRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_RegisterDagRequestProto_descriptor,
-              new java.lang.String[] { "User", "QueryIdentifier", "CredentialsBinary", });
-          internal_static_RegisterDagResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(11);
-          internal_static_RegisterDagResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_RegisterDagResponseProto_descriptor,
-              new java.lang.String[] { });
-          internal_static_SubmitWorkResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(12);
-          internal_static_SubmitWorkResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_SubmitWorkResponseProto_descriptor,
-              new java.lang.String[] { "SubmissionState", "UniqueNodeId", });
-          internal_static_SourceStateUpdatedRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(13);
-          internal_static_SourceStateUpdatedRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_SourceStateUpdatedRequestProto_descriptor,
-              new java.lang.String[] { "QueryIdentifier", "SrcName", "State", });
-          internal_static_SourceStateUpdatedResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(14);
-          internal_static_SourceStateUpdatedResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_SourceStateUpdatedResponseProto_descriptor,
-              new java.lang.String[] { });
-          internal_static_QueryCompleteRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(15);
-          internal_static_QueryCompleteRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_QueryCompleteRequestProto_descriptor,
-              new java.lang.String[] { "QueryIdentifier", "DeleteDelay", });
-          internal_static_QueryCompleteResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(16);
-          internal_static_QueryCompleteResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_QueryCompleteResponseProto_descriptor,
-              new java.lang.String[] { });
-          internal_static_TerminateFragmentRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(17);
-          internal_static_TerminateFragmentRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_TerminateFragmentRequestProto_descriptor,
-              new java.lang.String[] { "QueryIdentifier", "FragmentIdentifierString", });
-          internal_static_TerminateFragmentResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(18);
-          internal_static_TerminateFragmentResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_TerminateFragmentResponseProto_descriptor,
-              new java.lang.String[] { });
-          internal_static_UpdateFragmentRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(19);
-          internal_static_UpdateFragmentRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UpdateFragmentRequestProto_descriptor,
-              new java.lang.String[] { "QueryIdentifier", "FragmentIdentifierString", "IsGuaranteed", });
-          internal_static_UpdateFragmentResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(20);
-          internal_static_UpdateFragmentResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UpdateFragmentResponseProto_descriptor,
-              new java.lang.String[] { "Result", "IsGuaranteed", });
-          internal_static_GetTokenRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(21);
-          internal_static_GetTokenRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_GetTokenRequestProto_descriptor,
-              new java.lang.String[] { "AppId", });
-          internal_static_GetTokenResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(22);
-          internal_static_GetTokenResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_GetTokenResponseProto_descriptor,
-              new java.lang.String[] { "Token", });
-          internal_static_LlapOutputSocketInitMessage_descriptor =
-            getDescriptor().getMessageTypes().get(23);
-          internal_static_LlapOutputSocketInitMessage_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_LlapOutputSocketInitMessage_descriptor,
-              new java.lang.String[] { "FragmentId", "Token", });
-          internal_static_PurgeCacheRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(24);
-          internal_static_PurgeCacheRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_PurgeCacheRequestProto_descriptor,
-              new java.lang.String[] { });
-          internal_static_PurgeCacheResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(25);
-          internal_static_PurgeCacheResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_PurgeCacheResponseProto_descriptor,
-              new java.lang.String[] { "PurgedMemoryBytes", });
-          internal_static_MapEntry_descriptor =
-            getDescriptor().getMessageTypes().get(26);
-          internal_static_MapEntry_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_MapEntry_descriptor,
-              new java.lang.String[] { "Key", "Value", });
-          internal_static_GetDaemonMetricsRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(27);
-          internal_static_GetDaemonMetricsRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_GetDaemonMetricsRequestProto_descriptor,
-              new java.lang.String[] { });
-          internal_static_GetDaemonMetricsResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(28);
-          internal_static_GetDaemonMetricsResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_GetDaemonMetricsResponseProto_descriptor,
-              new java.lang.String[] { "Metrics", });
-          internal_static_SetCapacityRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(29);
-          internal_static_SetCapacityRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_SetCapacityRequestProto_descriptor,
-              new java.lang.String[] { "ExecutorNum", "QueueSize", });
-          internal_static_SetCapacityResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(30);
-          internal_static_SetCapacityResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_SetCapacityResponseProto_descriptor,
-              new java.lang.String[] { });
-          internal_static_EvictEntityRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(31);
-          internal_static_EvictEntityRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_EvictEntityRequestProto_descriptor,
-              new java.lang.String[] { "DbName", "Table", });
-          internal_static_TableProto_descriptor =
-            getDescriptor().getMessageTypes().get(32);
-          internal_static_TableProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_TableProto_descriptor,
-              new java.lang.String[] { "TableName", "PartKey", "PartVal", });
-          internal_static_EvictEntityResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(33);
-          internal_static_EvictEntityResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_EvictEntityResponseProto_descriptor,
-              new java.lang.String[] { "EvictedBytes", });
-          internal_static_GetCacheContentRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(34);
-          internal_static_GetCacheContentRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_GetCacheContentRequestProto_descriptor,
-              new java.lang.String[] { });
-          internal_static_GetCacheContentResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(35);
-          internal_static_GetCacheContentResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_GetCacheContentResponseProto_descriptor,
-              new java.lang.String[] { "Result", });
-          internal_static_CacheEntryList_descriptor =
-            getDescriptor().getMessageTypes().get(36);
-          internal_static_CacheEntryList_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_CacheEntryList_descriptor,
-              new java.lang.String[] { "Entries", });
-          internal_static_CacheEntry_descriptor =
-            getDescriptor().getMessageTypes().get(37);
-          internal_static_CacheEntry_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_CacheEntry_descriptor,
-              new java.lang.String[] { "FileKey", "FilePath", "CacheTag", "Ranges", });
-          internal_static_CacheTag_descriptor =
-            getDescriptor().getMessageTypes().get(38);
-          internal_static_CacheTag_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_CacheTag_descriptor,
-              new java.lang.String[] { "TableName", "PartitionDesc", });
-          internal_static_CacheEntryRange_descriptor =
-            getDescriptor().getMessageTypes().get(39);
-          internal_static_CacheEntryRange_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_CacheEntryRange_descriptor,
-              new java.lang.String[] { "Start", "End", });
-          return null;
-        }
-      };
-    com.google.protobuf.Descriptors.FileDescriptor
+    descriptor = com.google.protobuf.Descriptors.FileDescriptor
       .internalBuildGeneratedFileFrom(descriptorData,
         new com.google.protobuf.Descriptors.FileDescriptor[] {
-        }, assigner);
+        });
+    internal_static_UserPayloadProto_descriptor =
+      getDescriptor().getMessageTypes().get(0);
+    internal_static_UserPayloadProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_UserPayloadProto_descriptor,
+        new java.lang.String[] { "UserPayload", "Version", });
+    internal_static_EntityDescriptorProto_descriptor =
+      getDescriptor().getMessageTypes().get(1);
+    internal_static_EntityDescriptorProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_EntityDescriptorProto_descriptor,
+        new java.lang.String[] { "ClassName", "UserPayload", "HistoryText", });
+    internal_static_IOSpecProto_descriptor =
+      getDescriptor().getMessageTypes().get(2);
+    internal_static_IOSpecProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_IOSpecProto_descriptor,
+        new java.lang.String[] { "ConnectedVertexName", "IoDescriptor", "PhysicalEdgeCount", });
+    internal_static_GroupInputSpecProto_descriptor =
+      getDescriptor().getMessageTypes().get(3);
+    internal_static_GroupInputSpecProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_GroupInputSpecProto_descriptor,
+        new java.lang.String[] { "GroupName", "GroupVertices", "MergedInputDescriptor", });
+    internal_static_SignableVertexSpec_descriptor =
+      getDescriptor().getMessageTypes().get(4);
+    internal_static_SignableVertexSpec_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_SignableVertexSpec_descriptor,
+        new java.lang.String[] { "User", "SignatureKeyId", "QueryIdentifier", "HiveQueryId", "DagName", "VertexName", "VertexIndex", "TokenIdentifier", "ProcessorDescriptor", "InputSpecs", "OutputSpecs", "GroupedInputSpecs", "VertexParallelism", "IsExternalSubmission", });
+    internal_static_VertexOrBinary_descriptor =
+      getDescriptor().getMessageTypes().get(5);
+    internal_static_VertexOrBinary_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_VertexOrBinary_descriptor,
+        new java.lang.String[] { "Vertex", "VertexBinary", });
+    internal_static_FragmentRuntimeInfo_descriptor =
+      getDescriptor().getMessageTypes().get(6);
+    internal_static_FragmentRuntimeInfo_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_FragmentRuntimeInfo_descriptor,
+        new java.lang.String[] { "NumSelfAndUpstreamTasks", "NumSelfAndUpstreamCompletedTasks", "WithinDagPriority", "DagStartTime", "FirstAttemptStartTime", "CurrentAttemptStartTime", });
+    internal_static_QueryIdentifierProto_descriptor =
+      getDescriptor().getMessageTypes().get(7);
+    internal_static_QueryIdentifierProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_QueryIdentifierProto_descriptor,
+        new java.lang.String[] { "ApplicationIdString", "DagIndex", "AppAttemptNumber", });
+    internal_static_NotTezEvent_descriptor =
+      getDescriptor().getMessageTypes().get(8);
+    internal_static_NotTezEvent_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_NotTezEvent_descriptor,
+        new java.lang.String[] { "InputEventProtoBytes", "VertexName", "DestInputName", "KeyId", });
+    internal_static_SubmitWorkRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(9);
+    internal_static_SubmitWorkRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_SubmitWorkRequestProto_descriptor,
+        new java.lang.String[] { "WorkSpec", "WorkSpecSignature", "FragmentNumber", "AttemptNumber", "ContainerIdString", "AmHost", "AmPort", "CredentialsBinary", "FragmentRuntimeInfo", "InitialEventBytes", "InitialEventSignature", "IsGuaranteed", "Jwt", "IsExternalClientRequest", });
+    internal_static_RegisterDagRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(10);
+    internal_static_RegisterDagRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_RegisterDagRequestProto_descriptor,
+        new java.lang.String[] { "User", "QueryIdentifier", "CredentialsBinary", });
+    internal_static_RegisterDagResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(11);
+    internal_static_RegisterDagResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_RegisterDagResponseProto_descriptor,
+        new java.lang.String[] { });
+    internal_static_SubmitWorkResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(12);
+    internal_static_SubmitWorkResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_SubmitWorkResponseProto_descriptor,
+        new java.lang.String[] { "SubmissionState", "UniqueNodeId", });
+    internal_static_SourceStateUpdatedRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(13);
+    internal_static_SourceStateUpdatedRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_SourceStateUpdatedRequestProto_descriptor,
+        new java.lang.String[] { "QueryIdentifier", "SrcName", "State", });
+    internal_static_SourceStateUpdatedResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(14);
+    internal_static_SourceStateUpdatedResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_SourceStateUpdatedResponseProto_descriptor,
+        new java.lang.String[] { });
+    internal_static_QueryCompleteRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(15);
+    internal_static_QueryCompleteRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_QueryCompleteRequestProto_descriptor,
+        new java.lang.String[] { "QueryIdentifier", "DeleteDelay", });
+    internal_static_QueryCompleteResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(16);
+    internal_static_QueryCompleteResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_QueryCompleteResponseProto_descriptor,
+        new java.lang.String[] { });
+    internal_static_TerminateFragmentRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(17);
+    internal_static_TerminateFragmentRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_TerminateFragmentRequestProto_descriptor,
+        new java.lang.String[] { "QueryIdentifier", "FragmentIdentifierString", });
+    internal_static_TerminateFragmentResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(18);
+    internal_static_TerminateFragmentResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_TerminateFragmentResponseProto_descriptor,
+        new java.lang.String[] { });
+    internal_static_UpdateFragmentRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(19);
+    internal_static_UpdateFragmentRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_UpdateFragmentRequestProto_descriptor,
+        new java.lang.String[] { "QueryIdentifier", "FragmentIdentifierString", "IsGuaranteed", });
+    internal_static_UpdateFragmentResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(20);
+    internal_static_UpdateFragmentResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_UpdateFragmentResponseProto_descriptor,
+        new java.lang.String[] { "Result", "IsGuaranteed", });
+    internal_static_GetTokenRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(21);
+    internal_static_GetTokenRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_GetTokenRequestProto_descriptor,
+        new java.lang.String[] { "AppId", });
+    internal_static_GetTokenResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(22);
+    internal_static_GetTokenResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_GetTokenResponseProto_descriptor,
+        new java.lang.String[] { "Token", });
+    internal_static_LlapOutputSocketInitMessage_descriptor =
+      getDescriptor().getMessageTypes().get(23);
+    internal_static_LlapOutputSocketInitMessage_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_LlapOutputSocketInitMessage_descriptor,
+        new java.lang.String[] { "FragmentId", "Token", });
+    internal_static_PurgeCacheRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(24);
+    internal_static_PurgeCacheRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_PurgeCacheRequestProto_descriptor,
+        new java.lang.String[] { });
+    internal_static_PurgeCacheResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(25);
+    internal_static_PurgeCacheResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_PurgeCacheResponseProto_descriptor,
+        new java.lang.String[] { "PurgedMemoryBytes", });
+    internal_static_MapEntry_descriptor =
+      getDescriptor().getMessageTypes().get(26);
+    internal_static_MapEntry_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_MapEntry_descriptor,
+        new java.lang.String[] { "Key", "Value", });
+    internal_static_GetDaemonMetricsRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(27);
+    internal_static_GetDaemonMetricsRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_GetDaemonMetricsRequestProto_descriptor,
+        new java.lang.String[] { });
+    internal_static_GetDaemonMetricsResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(28);
+    internal_static_GetDaemonMetricsResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_GetDaemonMetricsResponseProto_descriptor,
+        new java.lang.String[] { "Metrics", });
+    internal_static_SetCapacityRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(29);
+    internal_static_SetCapacityRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_SetCapacityRequestProto_descriptor,
+        new java.lang.String[] { "ExecutorNum", "QueueSize", });
+    internal_static_SetCapacityResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(30);
+    internal_static_SetCapacityResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_SetCapacityResponseProto_descriptor,
+        new java.lang.String[] { });
+    internal_static_EvictEntityRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(31);
+    internal_static_EvictEntityRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_EvictEntityRequestProto_descriptor,
+        new java.lang.String[] { "DbName", "Table", });
+    internal_static_TableProto_descriptor =
+      getDescriptor().getMessageTypes().get(32);
+    internal_static_TableProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_TableProto_descriptor,
+        new java.lang.String[] { "TableName", "PartKey", "PartVal", });
+    internal_static_EvictEntityResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(33);
+    internal_static_EvictEntityResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_EvictEntityResponseProto_descriptor,
+        new java.lang.String[] { "EvictedBytes", });
+    internal_static_GetCacheContentRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(34);
+    internal_static_GetCacheContentRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_GetCacheContentRequestProto_descriptor,
+        new java.lang.String[] { });
+    internal_static_GetCacheContentResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(35);
+    internal_static_GetCacheContentResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_GetCacheContentResponseProto_descriptor,
+        new java.lang.String[] { "Result", });
+    internal_static_CacheEntryList_descriptor =
+      getDescriptor().getMessageTypes().get(36);
+    internal_static_CacheEntryList_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_CacheEntryList_descriptor,
+        new java.lang.String[] { "Entries", });
+    internal_static_CacheEntry_descriptor =
+      getDescriptor().getMessageTypes().get(37);
+    internal_static_CacheEntry_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_CacheEntry_descriptor,
+        new java.lang.String[] { "FileKey", "FilePath", "CacheTag", "Ranges", });
+    internal_static_CacheTag_descriptor =
+      getDescriptor().getMessageTypes().get(38);
+    internal_static_CacheTag_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_CacheTag_descriptor,
+        new java.lang.String[] { "TableName", "PartitionDesc", });
+    internal_static_CacheEntryRange_descriptor =
+      getDescriptor().getMessageTypes().get(39);
+    internal_static_CacheEntryRange_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_CacheEntryRange_descriptor,
+        new java.lang.String[] { "Start", "End", });
   }
 
   // @@protoc_insertion_point(outer_class_scope)
diff --git a/llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/plugin/rpc/LlapPluginProtocolProtos.java b/llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/plugin/rpc/LlapPluginProtocolProtos.java
index dbcd895f77..ecd660d065 100644
--- a/llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/plugin/rpc/LlapPluginProtocolProtos.java
+++ b/llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/plugin/rpc/LlapPluginProtocolProtos.java
@@ -5,19 +5,27 @@
 
 public final class LlapPluginProtocolProtos {
   private LlapPluginProtocolProtos() {}
+  public static void registerAllExtensions(
+      com.google.protobuf.ExtensionRegistryLite registry) {
+  }
+
   public static void registerAllExtensions(
       com.google.protobuf.ExtensionRegistry registry) {
+    registerAllExtensions(
+        (com.google.protobuf.ExtensionRegistryLite) registry);
   }
-  public interface UpdateQueryRequestProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface UpdateQueryRequestProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:UpdateQueryRequestProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional int32 guaranteed_task_count = 1;
     /**
      * <code>optional int32 guaranteed_task_count = 1;</code>
+     * @return Whether the guaranteedTaskCount field is set.
      */
     boolean hasGuaranteedTaskCount();
     /**
      * <code>optional int32 guaranteed_task_count = 1;</code>
+     * @return The guaranteedTaskCount.
      */
     int getGuaranteedTaskCount();
   }
@@ -25,157 +33,92 @@ public interface UpdateQueryRequestProtoOrBuilder
    * Protobuf type {@code UpdateQueryRequestProto}
    */
   public static final class UpdateQueryRequestProto extends
-      com.google.protobuf.GeneratedMessage
-      implements UpdateQueryRequestProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:UpdateQueryRequestProto)
+      UpdateQueryRequestProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use UpdateQueryRequestProto.newBuilder() to construct.
-    private UpdateQueryRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private UpdateQueryRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private UpdateQueryRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final UpdateQueryRequestProto defaultInstance;
-    public static UpdateQueryRequestProto getDefaultInstance() {
-      return defaultInstance;
+    private UpdateQueryRequestProto() {
     }
 
-    public UpdateQueryRequestProto getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private UpdateQueryRequestProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              guaranteedTaskCount_ = input.readInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new UpdateQueryRequestProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryRequestProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryRequestProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto.class, org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<UpdateQueryRequestProto> PARSER =
-        new com.google.protobuf.AbstractParser<UpdateQueryRequestProto>() {
-      public UpdateQueryRequestProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new UpdateQueryRequestProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<UpdateQueryRequestProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional int32 guaranteed_task_count = 1;
     public static final int GUARANTEED_TASK_COUNT_FIELD_NUMBER = 1;
-    private int guaranteedTaskCount_;
+    private int guaranteedTaskCount_ = 0;
     /**
      * <code>optional int32 guaranteed_task_count = 1;</code>
+     * @return Whether the guaranteedTaskCount field is set.
      */
+    @java.lang.Override
     public boolean hasGuaranteedTaskCount() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional int32 guaranteed_task_count = 1;</code>
+     * @return The guaranteedTaskCount.
      */
+    @java.lang.Override
     public int getGuaranteedTaskCount() {
       return guaranteedTaskCount_;
     }
 
-    private void initFields() {
-      guaranteedTaskCount_ = 0;
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeInt32(1, guaranteedTaskCount_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(1, guaranteedTaskCount_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -186,25 +129,22 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto other = (org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto) obj;
 
-      boolean result = true;
-      result = result && (hasGuaranteedTaskCount() == other.hasGuaranteedTaskCount());
+      if (hasGuaranteedTaskCount() != other.hasGuaranteedTaskCount()) return false;
       if (hasGuaranteedTaskCount()) {
-        result = result && (getGuaranteedTaskCount()
-            == other.getGuaranteedTaskCount());
+        if (getGuaranteedTaskCount()
+            != other.getGuaranteedTaskCount()) return false;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       if (hasGuaranteedTaskCount()) {
         hash = (37 * hash) + GUARANTEED_TASK_COUNT_FIELD_NUMBER;
         hash = (53 * hash) + getGuaranteedTaskCount();
@@ -214,6 +154,17 @@ public int hashCode() {
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -237,46 +188,61 @@ public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.Up
     }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -284,14 +250,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code UpdateQueryRequestProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:UpdateQueryRequestProto)
+        org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryRequestProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryRequestProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -300,42 +268,34 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         guaranteedTaskCount_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryRequestProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto build() {
         org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -344,19 +304,57 @@ public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQue
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto buildPartial() {
         org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto result = new org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.guaranteedTaskCount_ = guaranteedTaskCount_;
           to_bitField0_ |= 0x00000001;
         }
-        result.guaranteedTaskCount_ = guaranteedTaskCount_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto)other);
@@ -372,57 +370,85 @@ public Builder mergeFrom(org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtoc
           setGuaranteedTaskCount(other.getGuaranteedTaskCount());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 8: {
+                guaranteedTaskCount_ = input.readInt32();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 8
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional int32 guaranteed_task_count = 1;
       private int guaranteedTaskCount_ ;
       /**
        * <code>optional int32 guaranteed_task_count = 1;</code>
+       * @return Whether the guaranteedTaskCount field is set.
        */
+      @java.lang.Override
       public boolean hasGuaranteedTaskCount() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional int32 guaranteed_task_count = 1;</code>
+       * @return The guaranteedTaskCount.
        */
+      @java.lang.Override
       public int getGuaranteedTaskCount() {
         return guaranteedTaskCount_;
       }
       /**
        * <code>optional int32 guaranteed_task_count = 1;</code>
+       * @param value The guaranteedTaskCount to set.
+       * @return This builder for chaining.
        */
       public Builder setGuaranteedTaskCount(int value) {
-        bitField0_ |= 0x00000001;
+
         guaranteedTaskCount_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 guaranteed_task_count = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearGuaranteedTaskCount() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -430,145 +456,137 @@ public Builder clearGuaranteedTaskCount() {
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:UpdateQueryRequestProto)
     }
 
+    // @@protoc_insertion_point(class_scope:UpdateQueryRequestProto)
+    private static final org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new UpdateQueryRequestProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<UpdateQueryRequestProto>
+        PARSER = new com.google.protobuf.AbstractParser<UpdateQueryRequestProto>() {
+      @java.lang.Override
+      public UpdateQueryRequestProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<UpdateQueryRequestProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<UpdateQueryRequestProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryRequestProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:UpdateQueryRequestProto)
   }
 
-  public interface UpdateQueryResponseProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface UpdateQueryResponseProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:UpdateQueryResponseProto)
+      com.google.protobuf.MessageOrBuilder {
   }
   /**
    * Protobuf type {@code UpdateQueryResponseProto}
    */
   public static final class UpdateQueryResponseProto extends
-      com.google.protobuf.GeneratedMessage
-      implements UpdateQueryResponseProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:UpdateQueryResponseProto)
+      UpdateQueryResponseProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use UpdateQueryResponseProto.newBuilder() to construct.
-    private UpdateQueryResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private UpdateQueryResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private UpdateQueryResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final UpdateQueryResponseProto defaultInstance;
-    public static UpdateQueryResponseProto getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public UpdateQueryResponseProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private UpdateQueryResponseProto() {
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private UpdateQueryResponseProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new UpdateQueryResponseProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryResponseProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryResponseProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto.class, org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<UpdateQueryResponseProto> PARSER =
-        new com.google.protobuf.AbstractParser<UpdateQueryResponseProto>() {
-      public UpdateQueryResponseProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new UpdateQueryResponseProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<UpdateQueryResponseProto> getParserForType() {
-      return PARSER;
-    }
-
-    private void initFields() {
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
     @java.lang.Override
     public boolean equals(final java.lang.Object obj) {
       if (obj == this) {
@@ -579,25 +597,33 @@ public boolean equals(final java.lang.Object obj) {
       }
       org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto other = (org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto) obj;
 
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
     }
 
-    private int memoizedHashCode = 0;
     @java.lang.Override
     public int hashCode() {
       if (memoizedHashCode != 0) {
         return memoizedHashCode;
       }
       int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (19 * hash) + getDescriptor().hashCode();
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
     }
 
+    public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -621,46 +647,61 @@ public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.Up
     }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -668,14 +709,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code UpdateQueryResponseProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:UpdateQueryResponseProto)
+        org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryResponseProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryResponseProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -684,40 +727,32 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.internal_static_UpdateQueryResponseProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto build() {
         org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -726,12 +761,46 @@ public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQue
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto buildPartial() {
         org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto result = new org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto(this);
         onBuilt();
         return result;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto) {
           return mergeFrom((org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto)other);
@@ -744,40 +813,108 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto other) {
         if (other == org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto.getDefaultInstance()) return this;
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:UpdateQueryResponseProto)
     }
 
+    // @@protoc_insertion_point(class_scope:UpdateQueryResponseProto)
+    private static final org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new UpdateQueryResponseProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto();
+    }
+
+    public static org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<UpdateQueryResponseProto>
+        PARSER = new com.google.protobuf.AbstractParser<UpdateQueryResponseProto>() {
+      @java.lang.Override
+      public UpdateQueryResponseProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<UpdateQueryResponseProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<UpdateQueryResponseProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQueryResponseProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:UpdateQueryResponseProto)
   }
 
   /**
@@ -1012,22 +1149,22 @@ public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQue
     // @@protoc_insertion_point(class_scope:LlapPluginProtocol)
   }
 
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_UpdateQueryRequestProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_UpdateQueryRequestProto_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_UpdateQueryResponseProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_UpdateQueryResponseProto_fieldAccessorTable;
 
   public static com.google.protobuf.Descriptors.FileDescriptor
       getDescriptor() {
     return descriptor;
   }
-  private static com.google.protobuf.Descriptors.FileDescriptor
+  private static  com.google.protobuf.Descriptors.FileDescriptor
       descriptor;
   static {
     java.lang.String[] descriptorData = {
@@ -1039,30 +1176,22 @@ public org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.UpdateQue
       "toBH\n&org.apache.hadoop.hive.llap.plugin" +
       ".rpcB\030LlapPluginProtocolProtos\210\001\001\240\001\001"
     };
-    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
-      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
-        public com.google.protobuf.ExtensionRegistry assignDescriptors(
-            com.google.protobuf.Descriptors.FileDescriptor root) {
-          descriptor = root;
-          internal_static_UpdateQueryRequestProto_descriptor =
-            getDescriptor().getMessageTypes().get(0);
-          internal_static_UpdateQueryRequestProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UpdateQueryRequestProto_descriptor,
-              new java.lang.String[] { "GuaranteedTaskCount", });
-          internal_static_UpdateQueryResponseProto_descriptor =
-            getDescriptor().getMessageTypes().get(1);
-          internal_static_UpdateQueryResponseProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UpdateQueryResponseProto_descriptor,
-              new java.lang.String[] { });
-          return null;
-        }
-      };
-    com.google.protobuf.Descriptors.FileDescriptor
+    descriptor = com.google.protobuf.Descriptors.FileDescriptor
       .internalBuildGeneratedFileFrom(descriptorData,
         new com.google.protobuf.Descriptors.FileDescriptor[] {
-        }, assigner);
+        });
+    internal_static_UpdateQueryRequestProto_descriptor =
+      getDescriptor().getMessageTypes().get(0);
+    internal_static_UpdateQueryRequestProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_UpdateQueryRequestProto_descriptor,
+        new java.lang.String[] { "GuaranteedTaskCount", });
+    internal_static_UpdateQueryResponseProto_descriptor =
+      getDescriptor().getMessageTypes().get(1);
+    internal_static_UpdateQueryResponseProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_UpdateQueryResponseProto_descriptor,
+        new java.lang.String[] { });
   }
 
   // @@protoc_insertion_point(outer_class_scope)
diff --git a/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/hooks/proto/HiveHookEvents.java b/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/hooks/proto/HiveHookEvents.java
index 1f27626ce4..90e2885eb8 100644
--- a/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/hooks/proto/HiveHookEvents.java
+++ b/ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/hooks/proto/HiveHookEvents.java
@@ -5,38 +5,49 @@
 
 public final class HiveHookEvents {
   private HiveHookEvents() {}
+  public static void registerAllExtensions(
+      com.google.protobuf.ExtensionRegistryLite registry) {
+  }
+
   public static void registerAllExtensions(
       com.google.protobuf.ExtensionRegistry registry) {
+    registerAllExtensions(
+        (com.google.protobuf.ExtensionRegistryLite) registry);
   }
-  public interface MapFieldEntryOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface MapFieldEntryOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:MapFieldEntry)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string key = 1;
     /**
      * <code>optional string key = 1;</code>
+     * @return Whether the key field is set.
      */
     boolean hasKey();
     /**
      * <code>optional string key = 1;</code>
+     * @return The key.
      */
     java.lang.String getKey();
     /**
      * <code>optional string key = 1;</code>
+     * @return The bytes for key.
      */
     com.google.protobuf.ByteString
         getKeyBytes();
 
-    // optional string value = 2;
     /**
      * <code>optional string value = 2;</code>
+     * @return Whether the value field is set.
      */
     boolean hasValue();
     /**
      * <code>optional string value = 2;</code>
+     * @return The value.
      */
     java.lang.String getValue();
     /**
      * <code>optional string value = 2;</code>
+     * @return The bytes for value.
      */
     com.google.protobuf.ByteString
         getValueBytes();
@@ -45,115 +56,56 @@ public interface MapFieldEntryOrBuilder
    * Protobuf type {@code MapFieldEntry}
    */
   public static final class MapFieldEntry extends
-      com.google.protobuf.GeneratedMessage
-      implements MapFieldEntryOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:MapFieldEntry)
+      MapFieldEntryOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use MapFieldEntry.newBuilder() to construct.
-    private MapFieldEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private MapFieldEntry(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private MapFieldEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final MapFieldEntry defaultInstance;
-    public static MapFieldEntry getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public MapFieldEntry getDefaultInstanceForType() {
-      return defaultInstance;
+    private MapFieldEntry() {
+      key_ = "";
+      value_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private MapFieldEntry(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              key_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              value_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new MapFieldEntry();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_MapFieldEntry_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_MapFieldEntry_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.class, org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<MapFieldEntry> PARSER =
-        new com.google.protobuf.AbstractParser<MapFieldEntry>() {
-      public MapFieldEntry parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new MapFieldEntry(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<MapFieldEntry> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string key = 1;
     public static final int KEY_FIELD_NUMBER = 1;
-    private java.lang.Object key_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object key_ = "";
     /**
      * <code>optional string key = 1;</code>
+     * @return Whether the key field is set.
      */
+    @java.lang.Override
     public boolean hasKey() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string key = 1;</code>
+     * @return The key.
      */
+    @java.lang.Override
     public java.lang.String getKey() {
       java.lang.Object ref = key_;
       if (ref instanceof java.lang.String) {
@@ -170,7 +122,9 @@ public java.lang.String getKey() {
     }
     /**
      * <code>optional string key = 1;</code>
+     * @return The bytes for key.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getKeyBytes() {
       java.lang.Object ref = key_;
@@ -185,18 +139,22 @@ public java.lang.String getKey() {
       }
     }
 
-    // optional string value = 2;
     public static final int VALUE_FIELD_NUMBER = 2;
-    private java.lang.Object value_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object value_ = "";
     /**
      * <code>optional string value = 2;</code>
+     * @return Whether the value field is set.
      */
+    @java.lang.Override
     public boolean hasValue() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional string value = 2;</code>
+     * @return The value.
      */
+    @java.lang.Override
     public java.lang.String getValue() {
       java.lang.Object ref = value_;
       if (ref instanceof java.lang.String) {
@@ -213,7 +171,9 @@ public java.lang.String getValue() {
     }
     /**
      * <code>optional string value = 2;</code>
+     * @return The bytes for value.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getValueBytes() {
       java.lang.Object ref = value_;
@@ -228,57 +188,101 @@ public java.lang.String getValue() {
       }
     }
 
-    private void initFields() {
-      key_ = "";
-      value_ = "";
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getKeyBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getValueBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, value_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getKeyBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getValueBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, value_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
     @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry other = (org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry) obj;
+
+      if (hasKey() != other.hasKey()) return false;
+      if (hasKey()) {
+        if (!getKey()
+            .equals(other.getKey())) return false;
+      }
+      if (hasValue() != other.hasValue()) return false;
+      if (hasValue()) {
+        if (!getValue()
+            .equals(other.getValue())) return false;
+      }
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
+    }
+
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptor().hashCode();
+      if (hasKey()) {
+        hash = (37 * hash) + KEY_FIELD_NUMBER;
+        hash = (53 * hash) + getKey().hashCode();
+      }
+      if (hasValue()) {
+        hash = (37 * hash) + VALUE_FIELD_NUMBER;
+        hash = (53 * hash) + getValue().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      memoizedHashCode = hash;
+      return hash;
     }
 
+    public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -302,46 +306,61 @@ public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry
     }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -349,14 +368,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code MapFieldEntry}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntryOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:MapFieldEntry)
+        org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntryOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_MapFieldEntry_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_MapFieldEntry_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -365,44 +386,35 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         key_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
         value_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_MapFieldEntry_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry build() {
         org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry result = buildPartial();
         if (!result.isInitialized()) {
@@ -411,23 +423,61 @@ public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry build(
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry buildPartial() {
         org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry result = new org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.key_ = key_;
           to_bitField0_ |= 0x00000001;
         }
-        result.key_ = key_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.value_ = value_;
           to_bitField0_ |= 0x00000002;
         }
-        result.value_ = value_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry) {
           return mergeFrom((org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry)other);
@@ -440,59 +490,89 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry other) {
         if (other == org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.getDefaultInstance()) return this;
         if (other.hasKey()) {
-          bitField0_ |= 0x00000001;
           key_ = other.key_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasValue()) {
-          bitField0_ |= 0x00000002;
           value_ = other.value_;
+          bitField0_ |= 0x00000002;
           onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                key_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                value_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string key = 1;
       private java.lang.Object key_ = "";
       /**
        * <code>optional string key = 1;</code>
+       * @return Whether the key field is set.
        */
       public boolean hasKey() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string key = 1;</code>
+       * @return The key.
        */
       public java.lang.String getKey() {
         java.lang.Object ref = key_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          key_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            key_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -500,6 +580,7 @@ public java.lang.String getKey() {
       }
       /**
        * <code>optional string key = 1;</code>
+       * @return The bytes for key.
        */
       public com.google.protobuf.ByteString
           getKeyBytes() {
@@ -516,57 +597,62 @@ public java.lang.String getKey() {
       }
       /**
        * <code>optional string key = 1;</code>
+       * @param value The key to set.
+       * @return This builder for chaining.
        */
       public Builder setKey(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         key_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string key = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearKey() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         key_ = getDefaultInstance().getKey();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string key = 1;</code>
+       * @param value The bytes for key to set.
+       * @return This builder for chaining.
        */
       public Builder setKeyBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         key_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // optional string value = 2;
       private java.lang.Object value_ = "";
       /**
        * <code>optional string value = 2;</code>
+       * @return Whether the value field is set.
        */
       public boolean hasValue() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional string value = 2;</code>
+       * @return The value.
        */
       public java.lang.String getValue() {
         java.lang.Object ref = value_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          value_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            value_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -574,6 +660,7 @@ public java.lang.String getValue() {
       }
       /**
        * <code>optional string value = 2;</code>
+       * @return The bytes for value.
        */
       public com.google.protobuf.ByteString
           getValueBytes() {
@@ -590,210 +677,288 @@ public java.lang.String getValue() {
       }
       /**
        * <code>optional string value = 2;</code>
+       * @param value The value to set.
+       * @return This builder for chaining.
        */
       public Builder setValue(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         value_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional string value = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearValue() {
-        bitField0_ = (bitField0_ & ~0x00000002);
         value_ = getDefaultInstance().getValue();
+        bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
       /**
        * <code>optional string value = 2;</code>
+       * @param value The bytes for value to set.
+       * @return This builder for chaining.
        */
       public Builder setValueBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         value_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:MapFieldEntry)
     }
 
+    // @@protoc_insertion_point(class_scope:MapFieldEntry)
+    private static final org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry DEFAULT_INSTANCE;
     static {
-      defaultInstance = new MapFieldEntry(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry();
+    }
+
+    public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<MapFieldEntry>
+        PARSER = new com.google.protobuf.AbstractParser<MapFieldEntry>() {
+      @java.lang.Override
+      public MapFieldEntry parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<MapFieldEntry> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<MapFieldEntry> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:MapFieldEntry)
   }
 
-  public interface HiveHookEventProtoOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface HiveHookEventProtoOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:HiveHookEventProto)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string eventType = 1;
     /**
      * <code>optional string eventType = 1;</code>
+     * @return Whether the eventType field is set.
      */
     boolean hasEventType();
     /**
      * <code>optional string eventType = 1;</code>
+     * @return The eventType.
      */
     java.lang.String getEventType();
     /**
      * <code>optional string eventType = 1;</code>
+     * @return The bytes for eventType.
      */
     com.google.protobuf.ByteString
         getEventTypeBytes();
 
-    // optional string hiveQueryId = 2;
     /**
      * <code>optional string hiveQueryId = 2;</code>
+     * @return Whether the hiveQueryId field is set.
      */
     boolean hasHiveQueryId();
     /**
      * <code>optional string hiveQueryId = 2;</code>
+     * @return The hiveQueryId.
      */
     java.lang.String getHiveQueryId();
     /**
      * <code>optional string hiveQueryId = 2;</code>
+     * @return The bytes for hiveQueryId.
      */
     com.google.protobuf.ByteString
         getHiveQueryIdBytes();
 
-    // optional int64 timestamp = 3;
     /**
      * <code>optional int64 timestamp = 3;</code>
+     * @return Whether the timestamp field is set.
      */
     boolean hasTimestamp();
     /**
      * <code>optional int64 timestamp = 3;</code>
+     * @return The timestamp.
      */
     long getTimestamp();
 
-    // optional string executionMode = 4;
     /**
      * <code>optional string executionMode = 4;</code>
+     * @return Whether the executionMode field is set.
      */
     boolean hasExecutionMode();
     /**
      * <code>optional string executionMode = 4;</code>
+     * @return The executionMode.
      */
     java.lang.String getExecutionMode();
     /**
      * <code>optional string executionMode = 4;</code>
+     * @return The bytes for executionMode.
      */
     com.google.protobuf.ByteString
         getExecutionModeBytes();
 
-    // optional string requestUser = 5;
     /**
      * <code>optional string requestUser = 5;</code>
+     * @return Whether the requestUser field is set.
      */
     boolean hasRequestUser();
     /**
      * <code>optional string requestUser = 5;</code>
+     * @return The requestUser.
      */
     java.lang.String getRequestUser();
     /**
      * <code>optional string requestUser = 5;</code>
+     * @return The bytes for requestUser.
      */
     com.google.protobuf.ByteString
         getRequestUserBytes();
 
-    // optional string queue = 6;
     /**
      * <code>optional string queue = 6;</code>
+     * @return Whether the queue field is set.
      */
     boolean hasQueue();
     /**
      * <code>optional string queue = 6;</code>
+     * @return The queue.
      */
     java.lang.String getQueue();
     /**
      * <code>optional string queue = 6;</code>
+     * @return The bytes for queue.
      */
     com.google.protobuf.ByteString
         getQueueBytes();
 
-    // optional string user = 7;
     /**
      * <code>optional string user = 7;</code>
+     * @return Whether the user field is set.
      */
     boolean hasUser();
     /**
      * <code>optional string user = 7;</code>
+     * @return The user.
      */
     java.lang.String getUser();
     /**
      * <code>optional string user = 7;</code>
+     * @return The bytes for user.
      */
     com.google.protobuf.ByteString
         getUserBytes();
 
-    // optional string operationId = 8;
     /**
      * <code>optional string operationId = 8;</code>
+     * @return Whether the operationId field is set.
      */
     boolean hasOperationId();
     /**
      * <code>optional string operationId = 8;</code>
+     * @return The operationId.
      */
     java.lang.String getOperationId();
     /**
      * <code>optional string operationId = 8;</code>
+     * @return The bytes for operationId.
      */
     com.google.protobuf.ByteString
         getOperationIdBytes();
 
-    // repeated string tablesWritten = 9;
     /**
      * <code>repeated string tablesWritten = 9;</code>
+     * @return A list containing the tablesWritten.
      */
     java.util.List<java.lang.String>
-    getTablesWrittenList();
+        getTablesWrittenList();
     /**
      * <code>repeated string tablesWritten = 9;</code>
+     * @return The count of tablesWritten.
      */
     int getTablesWrittenCount();
     /**
      * <code>repeated string tablesWritten = 9;</code>
+     * @param index The index of the element to return.
+     * @return The tablesWritten at the given index.
      */
     java.lang.String getTablesWritten(int index);
     /**
      * <code>repeated string tablesWritten = 9;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the tablesWritten at the given index.
      */
     com.google.protobuf.ByteString
         getTablesWrittenBytes(int index);
 
-    // repeated string tablesRead = 10;
     /**
      * <code>repeated string tablesRead = 10;</code>
+     * @return A list containing the tablesRead.
      */
     java.util.List<java.lang.String>
-    getTablesReadList();
+        getTablesReadList();
     /**
      * <code>repeated string tablesRead = 10;</code>
+     * @return The count of tablesRead.
      */
     int getTablesReadCount();
     /**
      * <code>repeated string tablesRead = 10;</code>
+     * @param index The index of the element to return.
+     * @return The tablesRead at the given index.
      */
     java.lang.String getTablesRead(int index);
     /**
      * <code>repeated string tablesRead = 10;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the tablesRead at the given index.
      */
     com.google.protobuf.ByteString
         getTablesReadBytes(int index);
 
-    // repeated .MapFieldEntry otherInfo = 50;
     /**
      * <code>repeated .MapFieldEntry otherInfo = 50;</code>
      */
@@ -822,178 +987,66 @@ org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntryOrBuilder getO
    * Protobuf type {@code HiveHookEventProto}
    */
   public static final class HiveHookEventProto extends
-      com.google.protobuf.GeneratedMessage
-      implements HiveHookEventProtoOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:HiveHookEventProto)
+      HiveHookEventProtoOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use HiveHookEventProto.newBuilder() to construct.
-    private HiveHookEventProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private HiveHookEventProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private HiveHookEventProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final HiveHookEventProto defaultInstance;
-    public static HiveHookEventProto getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public HiveHookEventProto getDefaultInstanceForType() {
-      return defaultInstance;
+    private HiveHookEventProto() {
+      eventType_ = "";
+      hiveQueryId_ = "";
+      executionMode_ = "";
+      requestUser_ = "";
+      queue_ = "";
+      user_ = "";
+      operationId_ = "";
+      tablesWritten_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+      tablesRead_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+      otherInfo_ = java.util.Collections.emptyList();
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private HiveHookEventProto(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              eventType_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              hiveQueryId_ = input.readBytes();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              timestamp_ = input.readInt64();
-              break;
-            }
-            case 34: {
-              bitField0_ |= 0x00000008;
-              executionMode_ = input.readBytes();
-              break;
-            }
-            case 42: {
-              bitField0_ |= 0x00000010;
-              requestUser_ = input.readBytes();
-              break;
-            }
-            case 50: {
-              bitField0_ |= 0x00000020;
-              queue_ = input.readBytes();
-              break;
-            }
-            case 58: {
-              bitField0_ |= 0x00000040;
-              user_ = input.readBytes();
-              break;
-            }
-            case 66: {
-              bitField0_ |= 0x00000080;
-              operationId_ = input.readBytes();
-              break;
-            }
-            case 74: {
-              if (!((mutable_bitField0_ & 0x00000100) == 0x00000100)) {
-                tablesWritten_ = new com.google.protobuf.LazyStringArrayList();
-                mutable_bitField0_ |= 0x00000100;
-              }
-              tablesWritten_.add(input.readBytes());
-              break;
-            }
-            case 82: {
-              if (!((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
-                tablesRead_ = new com.google.protobuf.LazyStringArrayList();
-                mutable_bitField0_ |= 0x00000200;
-              }
-              tablesRead_.add(input.readBytes());
-              break;
-            }
-            case 402: {
-              if (!((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
-                otherInfo_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry>();
-                mutable_bitField0_ |= 0x00000400;
-              }
-              otherInfo_.add(input.readMessage(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.PARSER, extensionRegistry));
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000100) == 0x00000100)) {
-          tablesWritten_ = new com.google.protobuf.UnmodifiableLazyStringList(tablesWritten_);
-        }
-        if (((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
-          tablesRead_ = new com.google.protobuf.UnmodifiableLazyStringList(tablesRead_);
-        }
-        if (((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
-          otherInfo_ = java.util.Collections.unmodifiableList(otherInfo_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new HiveHookEventProto();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_HiveHookEventProto_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_HiveHookEventProto_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto.class, org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<HiveHookEventProto> PARSER =
-        new com.google.protobuf.AbstractParser<HiveHookEventProto>() {
-      public HiveHookEventProto parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new HiveHookEventProto(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<HiveHookEventProto> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string eventType = 1;
     public static final int EVENTTYPE_FIELD_NUMBER = 1;
-    private java.lang.Object eventType_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object eventType_ = "";
     /**
      * <code>optional string eventType = 1;</code>
+     * @return Whether the eventType field is set.
      */
+    @java.lang.Override
     public boolean hasEventType() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string eventType = 1;</code>
+     * @return The eventType.
      */
+    @java.lang.Override
     public java.lang.String getEventType() {
       java.lang.Object ref = eventType_;
       if (ref instanceof java.lang.String) {
@@ -1010,7 +1063,9 @@ public java.lang.String getEventType() {
     }
     /**
      * <code>optional string eventType = 1;</code>
+     * @return The bytes for eventType.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getEventTypeBytes() {
       java.lang.Object ref = eventType_;
@@ -1025,18 +1080,22 @@ public java.lang.String getEventType() {
       }
     }
 
-    // optional string hiveQueryId = 2;
     public static final int HIVEQUERYID_FIELD_NUMBER = 2;
-    private java.lang.Object hiveQueryId_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object hiveQueryId_ = "";
     /**
      * <code>optional string hiveQueryId = 2;</code>
+     * @return Whether the hiveQueryId field is set.
      */
+    @java.lang.Override
     public boolean hasHiveQueryId() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional string hiveQueryId = 2;</code>
+     * @return The hiveQueryId.
      */
+    @java.lang.Override
     public java.lang.String getHiveQueryId() {
       java.lang.Object ref = hiveQueryId_;
       if (ref instanceof java.lang.String) {
@@ -1053,7 +1112,9 @@ public java.lang.String getHiveQueryId() {
     }
     /**
      * <code>optional string hiveQueryId = 2;</code>
+     * @return The bytes for hiveQueryId.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getHiveQueryIdBytes() {
       java.lang.Object ref = hiveQueryId_;
@@ -1068,34 +1129,41 @@ public java.lang.String getHiveQueryId() {
       }
     }
 
-    // optional int64 timestamp = 3;
     public static final int TIMESTAMP_FIELD_NUMBER = 3;
-    private long timestamp_;
+    private long timestamp_ = 0L;
     /**
      * <code>optional int64 timestamp = 3;</code>
+     * @return Whether the timestamp field is set.
      */
+    @java.lang.Override
     public boolean hasTimestamp() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional int64 timestamp = 3;</code>
+     * @return The timestamp.
      */
+    @java.lang.Override
     public long getTimestamp() {
       return timestamp_;
     }
 
-    // optional string executionMode = 4;
     public static final int EXECUTIONMODE_FIELD_NUMBER = 4;
-    private java.lang.Object executionMode_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object executionMode_ = "";
     /**
      * <code>optional string executionMode = 4;</code>
+     * @return Whether the executionMode field is set.
      */
+    @java.lang.Override
     public boolean hasExecutionMode() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
+      return ((bitField0_ & 0x00000008) != 0);
     }
     /**
      * <code>optional string executionMode = 4;</code>
+     * @return The executionMode.
      */
+    @java.lang.Override
     public java.lang.String getExecutionMode() {
       java.lang.Object ref = executionMode_;
       if (ref instanceof java.lang.String) {
@@ -1112,7 +1180,9 @@ public java.lang.String getExecutionMode() {
     }
     /**
      * <code>optional string executionMode = 4;</code>
+     * @return The bytes for executionMode.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getExecutionModeBytes() {
       java.lang.Object ref = executionMode_;
@@ -1127,18 +1197,22 @@ public java.lang.String getExecutionMode() {
       }
     }
 
-    // optional string requestUser = 5;
     public static final int REQUESTUSER_FIELD_NUMBER = 5;
-    private java.lang.Object requestUser_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object requestUser_ = "";
     /**
      * <code>optional string requestUser = 5;</code>
+     * @return Whether the requestUser field is set.
      */
+    @java.lang.Override
     public boolean hasRequestUser() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+      return ((bitField0_ & 0x00000010) != 0);
     }
     /**
      * <code>optional string requestUser = 5;</code>
+     * @return The requestUser.
      */
+    @java.lang.Override
     public java.lang.String getRequestUser() {
       java.lang.Object ref = requestUser_;
       if (ref instanceof java.lang.String) {
@@ -1155,7 +1229,9 @@ public java.lang.String getRequestUser() {
     }
     /**
      * <code>optional string requestUser = 5;</code>
+     * @return The bytes for requestUser.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getRequestUserBytes() {
       java.lang.Object ref = requestUser_;
@@ -1170,18 +1246,22 @@ public java.lang.String getRequestUser() {
       }
     }
 
-    // optional string queue = 6;
     public static final int QUEUE_FIELD_NUMBER = 6;
-    private java.lang.Object queue_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object queue_ = "";
     /**
      * <code>optional string queue = 6;</code>
+     * @return Whether the queue field is set.
      */
+    @java.lang.Override
     public boolean hasQueue() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+      return ((bitField0_ & 0x00000020) != 0);
     }
     /**
      * <code>optional string queue = 6;</code>
+     * @return The queue.
      */
+    @java.lang.Override
     public java.lang.String getQueue() {
       java.lang.Object ref = queue_;
       if (ref instanceof java.lang.String) {
@@ -1198,7 +1278,9 @@ public java.lang.String getQueue() {
     }
     /**
      * <code>optional string queue = 6;</code>
+     * @return The bytes for queue.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getQueueBytes() {
       java.lang.Object ref = queue_;
@@ -1213,18 +1295,22 @@ public java.lang.String getQueue() {
       }
     }
 
-    // optional string user = 7;
     public static final int USER_FIELD_NUMBER = 7;
-    private java.lang.Object user_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object user_ = "";
     /**
      * <code>optional string user = 7;</code>
+     * @return Whether the user field is set.
      */
+    @java.lang.Override
     public boolean hasUser() {
-      return ((bitField0_ & 0x00000040) == 0x00000040);
+      return ((bitField0_ & 0x00000040) != 0);
     }
     /**
      * <code>optional string user = 7;</code>
+     * @return The user.
      */
+    @java.lang.Override
     public java.lang.String getUser() {
       java.lang.Object ref = user_;
       if (ref instanceof java.lang.String) {
@@ -1241,7 +1327,9 @@ public java.lang.String getUser() {
     }
     /**
      * <code>optional string user = 7;</code>
+     * @return The bytes for user.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getUserBytes() {
       java.lang.Object ref = user_;
@@ -1256,18 +1344,22 @@ public java.lang.String getUser() {
       }
     }
 
-    // optional string operationId = 8;
     public static final int OPERATIONID_FIELD_NUMBER = 8;
-    private java.lang.Object operationId_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object operationId_ = "";
     /**
      * <code>optional string operationId = 8;</code>
+     * @return Whether the operationId field is set.
      */
+    @java.lang.Override
     public boolean hasOperationId() {
-      return ((bitField0_ & 0x00000080) == 0x00000080);
+      return ((bitField0_ & 0x00000080) != 0);
     }
     /**
      * <code>optional string operationId = 8;</code>
+     * @return The operationId.
      */
+    @java.lang.Override
     public java.lang.String getOperationId() {
       java.lang.Object ref = operationId_;
       if (ref instanceof java.lang.String) {
@@ -1284,7 +1376,9 @@ public java.lang.String getOperationId() {
     }
     /**
      * <code>optional string operationId = 8;</code>
+     * @return The bytes for operationId.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getOperationIdBytes() {
       java.lang.Object ref = operationId_;
@@ -1299,78 +1393,94 @@ public java.lang.String getOperationId() {
       }
     }
 
-    // repeated string tablesWritten = 9;
     public static final int TABLESWRITTEN_FIELD_NUMBER = 9;
-    private com.google.protobuf.LazyStringList tablesWritten_;
+    @SuppressWarnings("serial")
+    private com.google.protobuf.LazyStringArrayList tablesWritten_ =
+        com.google.protobuf.LazyStringArrayList.emptyList();
     /**
      * <code>repeated string tablesWritten = 9;</code>
+     * @return A list containing the tablesWritten.
      */
-    public java.util.List<java.lang.String>
+    public com.google.protobuf.ProtocolStringList
         getTablesWrittenList() {
       return tablesWritten_;
     }
     /**
      * <code>repeated string tablesWritten = 9;</code>
+     * @return The count of tablesWritten.
      */
     public int getTablesWrittenCount() {
       return tablesWritten_.size();
     }
     /**
      * <code>repeated string tablesWritten = 9;</code>
+     * @param index The index of the element to return.
+     * @return The tablesWritten at the given index.
      */
     public java.lang.String getTablesWritten(int index) {
       return tablesWritten_.get(index);
     }
     /**
      * <code>repeated string tablesWritten = 9;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the tablesWritten at the given index.
      */
     public com.google.protobuf.ByteString
         getTablesWrittenBytes(int index) {
       return tablesWritten_.getByteString(index);
     }
 
-    // repeated string tablesRead = 10;
     public static final int TABLESREAD_FIELD_NUMBER = 10;
-    private com.google.protobuf.LazyStringList tablesRead_;
+    @SuppressWarnings("serial")
+    private com.google.protobuf.LazyStringArrayList tablesRead_ =
+        com.google.protobuf.LazyStringArrayList.emptyList();
     /**
      * <code>repeated string tablesRead = 10;</code>
+     * @return A list containing the tablesRead.
      */
-    public java.util.List<java.lang.String>
+    public com.google.protobuf.ProtocolStringList
         getTablesReadList() {
       return tablesRead_;
     }
     /**
      * <code>repeated string tablesRead = 10;</code>
+     * @return The count of tablesRead.
      */
     public int getTablesReadCount() {
       return tablesRead_.size();
     }
     /**
      * <code>repeated string tablesRead = 10;</code>
+     * @param index The index of the element to return.
+     * @return The tablesRead at the given index.
      */
     public java.lang.String getTablesRead(int index) {
       return tablesRead_.get(index);
     }
     /**
      * <code>repeated string tablesRead = 10;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the tablesRead at the given index.
      */
     public com.google.protobuf.ByteString
         getTablesReadBytes(int index) {
       return tablesRead_.getByteString(index);
     }
 
-    // repeated .MapFieldEntry otherInfo = 50;
     public static final int OTHERINFO_FIELD_NUMBER = 50;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry> otherInfo_;
     /**
      * <code>repeated .MapFieldEntry otherInfo = 50;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry> getOtherInfoList() {
       return otherInfo_;
     }
     /**
      * <code>repeated .MapFieldEntry otherInfo = 50;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntryOrBuilder> 
         getOtherInfoOrBuilderList() {
       return otherInfo_;
@@ -1378,77 +1488,69 @@ public java.util.List<org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFi
     /**
      * <code>repeated .MapFieldEntry otherInfo = 50;</code>
      */
+    @java.lang.Override
     public int getOtherInfoCount() {
       return otherInfo_.size();
     }
     /**
      * <code>repeated .MapFieldEntry otherInfo = 50;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry getOtherInfo(int index) {
       return otherInfo_.get(index);
     }
     /**
      * <code>repeated .MapFieldEntry otherInfo = 50;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntryOrBuilder getOtherInfoOrBuilder(
         int index) {
       return otherInfo_.get(index);
     }
 
-    private void initFields() {
-      eventType_ = "";
-      hiveQueryId_ = "";
-      timestamp_ = 0L;
-      executionMode_ = "";
-      requestUser_ = "";
-      queue_ = "";
-      user_ = "";
-      operationId_ = "";
-      tablesWritten_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-      tablesRead_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-      otherInfo_ = java.util.Collections.emptyList();
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getEventTypeBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, eventType_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getHiveQueryIdBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, hiveQueryId_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         output.writeInt64(3, timestamp_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeBytes(4, getExecutionModeBytes());
+      if (((bitField0_ & 0x00000008) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, executionMode_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeBytes(5, getRequestUserBytes());
+      if (((bitField0_ & 0x00000010) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, requestUser_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        output.writeBytes(6, getQueueBytes());
+      if (((bitField0_ & 0x00000020) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 6, queue_);
       }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
-        output.writeBytes(7, getUserBytes());
+      if (((bitField0_ & 0x00000040) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 7, user_);
       }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        output.writeBytes(8, getOperationIdBytes());
+      if (((bitField0_ & 0x00000080) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, operationId_);
       }
       for (int i = 0; i < tablesWritten_.size(); i++) {
-        output.writeBytes(9, tablesWritten_.getByteString(i));
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 9, tablesWritten_.getRaw(i));
       }
       for (int i = 0; i < tablesRead_.size(); i++) {
-        output.writeBytes(10, tablesRead_.getByteString(i));
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 10, tablesRead_.getRaw(i));
       }
       for (int i = 0; i < otherInfo_.size(); i++) {
         output.writeMessage(50, otherInfo_.get(i));
@@ -1456,49 +1558,41 @@ public void writeTo(com.google.protobuf.CodedOutputStream output)
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getEventTypeBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, eventType_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getHiveQueryIdBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, hiveQueryId_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(3, timestamp_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(4, getExecutionModeBytes());
+      if (((bitField0_ & 0x00000008) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, executionMode_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(5, getRequestUserBytes());
+      if (((bitField0_ & 0x00000010) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, requestUser_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(6, getQueueBytes());
+      if (((bitField0_ & 0x00000020) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(6, queue_);
       }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(7, getUserBytes());
+      if (((bitField0_ & 0x00000040) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(7, user_);
       }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(8, getOperationIdBytes());
+      if (((bitField0_ & 0x00000080) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, operationId_);
       }
       {
         int dataSize = 0;
         for (int i = 0; i < tablesWritten_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(tablesWritten_.getByteString(i));
+          dataSize += computeStringSizeNoTag(tablesWritten_.getRaw(i));
         }
         size += dataSize;
         size += 1 * getTablesWrittenList().size();
@@ -1506,8 +1600,7 @@ public int getSerializedSize() {
       {
         int dataSize = 0;
         for (int i = 0; i < tablesRead_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(tablesRead_.getByteString(i));
+          dataSize += computeStringSizeNoTag(tablesRead_.getRaw(i));
         }
         size += dataSize;
         size += 1 * getTablesReadList().size();
@@ -1517,17 +1610,138 @@ public int getSerializedSize() {
           .computeMessageSize(50, otherInfo_.get(i));
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
     @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto other = (org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto) obj;
+
+      if (hasEventType() != other.hasEventType()) return false;
+      if (hasEventType()) {
+        if (!getEventType()
+            .equals(other.getEventType())) return false;
+      }
+      if (hasHiveQueryId() != other.hasHiveQueryId()) return false;
+      if (hasHiveQueryId()) {
+        if (!getHiveQueryId()
+            .equals(other.getHiveQueryId())) return false;
+      }
+      if (hasTimestamp() != other.hasTimestamp()) return false;
+      if (hasTimestamp()) {
+        if (getTimestamp()
+            != other.getTimestamp()) return false;
+      }
+      if (hasExecutionMode() != other.hasExecutionMode()) return false;
+      if (hasExecutionMode()) {
+        if (!getExecutionMode()
+            .equals(other.getExecutionMode())) return false;
+      }
+      if (hasRequestUser() != other.hasRequestUser()) return false;
+      if (hasRequestUser()) {
+        if (!getRequestUser()
+            .equals(other.getRequestUser())) return false;
+      }
+      if (hasQueue() != other.hasQueue()) return false;
+      if (hasQueue()) {
+        if (!getQueue()
+            .equals(other.getQueue())) return false;
+      }
+      if (hasUser() != other.hasUser()) return false;
+      if (hasUser()) {
+        if (!getUser()
+            .equals(other.getUser())) return false;
+      }
+      if (hasOperationId() != other.hasOperationId()) return false;
+      if (hasOperationId()) {
+        if (!getOperationId()
+            .equals(other.getOperationId())) return false;
+      }
+      if (!getTablesWrittenList()
+          .equals(other.getTablesWrittenList())) return false;
+      if (!getTablesReadList()
+          .equals(other.getTablesReadList())) return false;
+      if (!getOtherInfoList()
+          .equals(other.getOtherInfoList())) return false;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
+    }
+
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptor().hashCode();
+      if (hasEventType()) {
+        hash = (37 * hash) + EVENTTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getEventType().hashCode();
+      }
+      if (hasHiveQueryId()) {
+        hash = (37 * hash) + HIVEQUERYID_FIELD_NUMBER;
+        hash = (53 * hash) + getHiveQueryId().hashCode();
+      }
+      if (hasTimestamp()) {
+        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getTimestamp());
+      }
+      if (hasExecutionMode()) {
+        hash = (37 * hash) + EXECUTIONMODE_FIELD_NUMBER;
+        hash = (53 * hash) + getExecutionMode().hashCode();
+      }
+      if (hasRequestUser()) {
+        hash = (37 * hash) + REQUESTUSER_FIELD_NUMBER;
+        hash = (53 * hash) + getRequestUser().hashCode();
+      }
+      if (hasQueue()) {
+        hash = (37 * hash) + QUEUE_FIELD_NUMBER;
+        hash = (53 * hash) + getQueue().hashCode();
+      }
+      if (hasUser()) {
+        hash = (37 * hash) + USER_FIELD_NUMBER;
+        hash = (53 * hash) + getUser().hashCode();
+      }
+      if (hasOperationId()) {
+        hash = (37 * hash) + OPERATIONID_FIELD_NUMBER;
+        hash = (53 * hash) + getOperationId().hashCode();
+      }
+      if (getTablesWrittenCount() > 0) {
+        hash = (37 * hash) + TABLESWRITTEN_FIELD_NUMBER;
+        hash = (53 * hash) + getTablesWrittenList().hashCode();
+      }
+      if (getTablesReadCount() > 0) {
+        hash = (37 * hash) + TABLESREAD_FIELD_NUMBER;
+        hash = (53 * hash) + getTablesReadList().hashCode();
+      }
+      if (getOtherInfoCount() > 0) {
+        hash = (37 * hash) + OTHERINFO_FIELD_NUMBER;
+        hash = (53 * hash) + getOtherInfoList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      memoizedHashCode = hash;
+      return hash;
     }
 
+    public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -1551,46 +1765,61 @@ public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEvent
     }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -1598,14 +1827,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code HiveHookEventProto}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProtoOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:HiveHookEventProto)
+        org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProtoOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_HiveHookEventProto_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_HiveHookEventProto_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -1614,67 +1845,52 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getOtherInfoFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         eventType_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
         hiveQueryId_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
         timestamp_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000004);
         executionMode_ = "";
-        bitField0_ = (bitField0_ & ~0x00000008);
         requestUser_ = "";
-        bitField0_ = (bitField0_ & ~0x00000010);
         queue_ = "";
-        bitField0_ = (bitField0_ & ~0x00000020);
         user_ = "";
-        bitField0_ = (bitField0_ & ~0x00000040);
         operationId_ = "";
-        bitField0_ = (bitField0_ & ~0x00000080);
-        tablesWritten_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000100);
-        tablesRead_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000200);
+        tablesWritten_ =
+            com.google.protobuf.LazyStringArrayList.emptyList();
+        tablesRead_ =
+            com.google.protobuf.LazyStringArrayList.emptyList();
         if (otherInfoBuilder_ == null) {
           otherInfo_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000400);
         } else {
+          otherInfo_ = null;
           otherInfoBuilder_.clear();
         }
+        bitField0_ = (bitField0_ & ~0x00000400);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.internal_static_HiveHookEventProto_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto build() {
         org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto result = buildPartial();
         if (!result.isInitialized()) {
@@ -1683,68 +1899,106 @@ public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto b
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto buildPartial() {
         org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto result = new org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto(this);
+        buildPartialRepeatedFields(result);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartialRepeatedFields(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto result) {
+        if (otherInfoBuilder_ == null) {
+          if (((bitField0_ & 0x00000400) != 0)) {
+            otherInfo_ = java.util.Collections.unmodifiableList(otherInfo_);
+            bitField0_ = (bitField0_ & ~0x00000400);
+          }
+          result.otherInfo_ = otherInfo_;
+        } else {
+          result.otherInfo_ = otherInfoBuilder_.build();
+        }
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.eventType_ = eventType_;
           to_bitField0_ |= 0x00000001;
         }
-        result.eventType_ = eventType_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.hiveQueryId_ = hiveQueryId_;
           to_bitField0_ |= 0x00000002;
         }
-        result.hiveQueryId_ = hiveQueryId_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.timestamp_ = timestamp_;
           to_bitField0_ |= 0x00000004;
         }
-        result.timestamp_ = timestamp_;
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
+        if (((from_bitField0_ & 0x00000008) != 0)) {
+          result.executionMode_ = executionMode_;
           to_bitField0_ |= 0x00000008;
         }
-        result.executionMode_ = executionMode_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
+        if (((from_bitField0_ & 0x00000010) != 0)) {
+          result.requestUser_ = requestUser_;
           to_bitField0_ |= 0x00000010;
         }
-        result.requestUser_ = requestUser_;
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
+        if (((from_bitField0_ & 0x00000020) != 0)) {
+          result.queue_ = queue_;
           to_bitField0_ |= 0x00000020;
         }
-        result.queue_ = queue_;
-        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
+        if (((from_bitField0_ & 0x00000040) != 0)) {
+          result.user_ = user_;
           to_bitField0_ |= 0x00000040;
         }
-        result.user_ = user_;
-        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
+        if (((from_bitField0_ & 0x00000080) != 0)) {
+          result.operationId_ = operationId_;
           to_bitField0_ |= 0x00000080;
         }
-        result.operationId_ = operationId_;
-        if (((bitField0_ & 0x00000100) == 0x00000100)) {
-          tablesWritten_ = new com.google.protobuf.UnmodifiableLazyStringList(
-              tablesWritten_);
-          bitField0_ = (bitField0_ & ~0x00000100);
+        if (((from_bitField0_ & 0x00000100) != 0)) {
+          tablesWritten_.makeImmutable();
+          result.tablesWritten_ = tablesWritten_;
         }
-        result.tablesWritten_ = tablesWritten_;
-        if (((bitField0_ & 0x00000200) == 0x00000200)) {
-          tablesRead_ = new com.google.protobuf.UnmodifiableLazyStringList(
-              tablesRead_);
-          bitField0_ = (bitField0_ & ~0x00000200);
-        }
-        result.tablesRead_ = tablesRead_;
-        if (otherInfoBuilder_ == null) {
-          if (((bitField0_ & 0x00000400) == 0x00000400)) {
-            otherInfo_ = java.util.Collections.unmodifiableList(otherInfo_);
-            bitField0_ = (bitField0_ & ~0x00000400);
-          }
-          result.otherInfo_ = otherInfo_;
-        } else {
-          result.otherInfo_ = otherInfoBuilder_.build();
+        if (((from_bitField0_ & 0x00000200) != 0)) {
+          tablesRead_.makeImmutable();
+          result.tablesRead_ = tablesRead_;
         }
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto) {
           return mergeFrom((org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto)other);
@@ -1757,47 +2011,47 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto other) {
         if (other == org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto.getDefaultInstance()) return this;
         if (other.hasEventType()) {
-          bitField0_ |= 0x00000001;
           eventType_ = other.eventType_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasHiveQueryId()) {
-          bitField0_ |= 0x00000002;
           hiveQueryId_ = other.hiveQueryId_;
+          bitField0_ |= 0x00000002;
           onChanged();
         }
         if (other.hasTimestamp()) {
           setTimestamp(other.getTimestamp());
         }
         if (other.hasExecutionMode()) {
-          bitField0_ |= 0x00000008;
           executionMode_ = other.executionMode_;
+          bitField0_ |= 0x00000008;
           onChanged();
         }
         if (other.hasRequestUser()) {
-          bitField0_ |= 0x00000010;
           requestUser_ = other.requestUser_;
+          bitField0_ |= 0x00000010;
           onChanged();
         }
         if (other.hasQueue()) {
-          bitField0_ |= 0x00000020;
           queue_ = other.queue_;
+          bitField0_ |= 0x00000020;
           onChanged();
         }
         if (other.hasUser()) {
-          bitField0_ |= 0x00000040;
           user_ = other.user_;
+          bitField0_ |= 0x00000040;
           onChanged();
         }
         if (other.hasOperationId()) {
-          bitField0_ |= 0x00000080;
           operationId_ = other.operationId_;
+          bitField0_ |= 0x00000080;
           onChanged();
         }
         if (!other.tablesWritten_.isEmpty()) {
           if (tablesWritten_.isEmpty()) {
             tablesWritten_ = other.tablesWritten_;
-            bitField0_ = (bitField0_ & ~0x00000100);
+            bitField0_ |= 0x00000100;
           } else {
             ensureTablesWrittenIsMutable();
             tablesWritten_.addAll(other.tablesWritten_);
@@ -1807,7 +2061,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.Hi
         if (!other.tablesRead_.isEmpty()) {
           if (tablesRead_.isEmpty()) {
             tablesRead_ = other.tablesRead_;
-            bitField0_ = (bitField0_ & ~0x00000200);
+            bitField0_ |= 0x00000200;
           } else {
             ensureTablesReadIsMutable();
             tablesRead_.addAll(other.tablesRead_);
@@ -1833,7 +2087,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.Hi
               otherInfo_ = other.otherInfo_;
               bitField0_ = (bitField0_ & ~0x00000400);
               otherInfoBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getOtherInfoFieldBuilder() : null;
             } else {
               otherInfoBuilder_.addAllMessages(other.otherInfo_);
@@ -1841,49 +2095,134 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.Hi
           }
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                eventType_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                hiveQueryId_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 24: {
+                timestamp_ = input.readInt64();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 24
+              case 34: {
+                executionMode_ = input.readBytes();
+                bitField0_ |= 0x00000008;
+                break;
+              } // case 34
+              case 42: {
+                requestUser_ = input.readBytes();
+                bitField0_ |= 0x00000010;
+                break;
+              } // case 42
+              case 50: {
+                queue_ = input.readBytes();
+                bitField0_ |= 0x00000020;
+                break;
+              } // case 50
+              case 58: {
+                user_ = input.readBytes();
+                bitField0_ |= 0x00000040;
+                break;
+              } // case 58
+              case 66: {
+                operationId_ = input.readBytes();
+                bitField0_ |= 0x00000080;
+                break;
+              } // case 66
+              case 74: {
+                com.google.protobuf.ByteString bs = input.readBytes();
+                ensureTablesWrittenIsMutable();
+                tablesWritten_.add(bs);
+                break;
+              } // case 74
+              case 82: {
+                com.google.protobuf.ByteString bs = input.readBytes();
+                ensureTablesReadIsMutable();
+                tablesRead_.add(bs);
+                break;
+              } // case 82
+              case 402: {
+                org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.PARSER,
+                        extensionRegistry);
+                if (otherInfoBuilder_ == null) {
+                  ensureOtherInfoIsMutable();
+                  otherInfo_.add(m);
+                } else {
+                  otherInfoBuilder_.addMessage(m);
+                }
+                break;
+              } // case 402
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string eventType = 1;
       private java.lang.Object eventType_ = "";
       /**
        * <code>optional string eventType = 1;</code>
+       * @return Whether the eventType field is set.
        */
       public boolean hasEventType() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string eventType = 1;</code>
+       * @return The eventType.
        */
       public java.lang.String getEventType() {
         java.lang.Object ref = eventType_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          eventType_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            eventType_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -1891,6 +2230,7 @@ public java.lang.String getEventType() {
       }
       /**
        * <code>optional string eventType = 1;</code>
+       * @return The bytes for eventType.
        */
       public com.google.protobuf.ByteString
           getEventTypeBytes() {
@@ -1907,57 +2247,62 @@ public java.lang.String getEventType() {
       }
       /**
        * <code>optional string eventType = 1;</code>
+       * @param value The eventType to set.
+       * @return This builder for chaining.
        */
       public Builder setEventType(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         eventType_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string eventType = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearEventType() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         eventType_ = getDefaultInstance().getEventType();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string eventType = 1;</code>
+       * @param value The bytes for eventType to set.
+       * @return This builder for chaining.
        */
       public Builder setEventTypeBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         eventType_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // optional string hiveQueryId = 2;
       private java.lang.Object hiveQueryId_ = "";
       /**
        * <code>optional string hiveQueryId = 2;</code>
+       * @return Whether the hiveQueryId field is set.
        */
       public boolean hasHiveQueryId() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional string hiveQueryId = 2;</code>
+       * @return The hiveQueryId.
        */
       public java.lang.String getHiveQueryId() {
         java.lang.Object ref = hiveQueryId_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          hiveQueryId_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            hiveQueryId_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -1965,6 +2310,7 @@ public java.lang.String getHiveQueryId() {
       }
       /**
        * <code>optional string hiveQueryId = 2;</code>
+       * @return The bytes for hiveQueryId.
        */
       public com.google.protobuf.ByteString
           getHiveQueryIdBytes() {
@@ -1981,65 +2327,73 @@ public java.lang.String getHiveQueryId() {
       }
       /**
        * <code>optional string hiveQueryId = 2;</code>
+       * @param value The hiveQueryId to set.
+       * @return This builder for chaining.
        */
       public Builder setHiveQueryId(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         hiveQueryId_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional string hiveQueryId = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearHiveQueryId() {
-        bitField0_ = (bitField0_ & ~0x00000002);
         hiveQueryId_ = getDefaultInstance().getHiveQueryId();
+        bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
       /**
        * <code>optional string hiveQueryId = 2;</code>
+       * @param value The bytes for hiveQueryId to set.
+       * @return This builder for chaining.
        */
       public Builder setHiveQueryIdBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         hiveQueryId_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
 
-      // optional int64 timestamp = 3;
       private long timestamp_ ;
       /**
        * <code>optional int64 timestamp = 3;</code>
+       * @return Whether the timestamp field is set.
        */
+      @java.lang.Override
       public boolean hasTimestamp() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional int64 timestamp = 3;</code>
+       * @return The timestamp.
        */
+      @java.lang.Override
       public long getTimestamp() {
         return timestamp_;
       }
       /**
        * <code>optional int64 timestamp = 3;</code>
+       * @param value The timestamp to set.
+       * @return This builder for chaining.
        */
       public Builder setTimestamp(long value) {
-        bitField0_ |= 0x00000004;
+
         timestamp_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 timestamp = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearTimestamp() {
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -2048,23 +2402,27 @@ public Builder clearTimestamp() {
         return this;
       }
 
-      // optional string executionMode = 4;
       private java.lang.Object executionMode_ = "";
       /**
        * <code>optional string executionMode = 4;</code>
+       * @return Whether the executionMode field is set.
        */
       public boolean hasExecutionMode() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
+        return ((bitField0_ & 0x00000008) != 0);
       }
       /**
        * <code>optional string executionMode = 4;</code>
+       * @return The executionMode.
        */
       public java.lang.String getExecutionMode() {
         java.lang.Object ref = executionMode_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          executionMode_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            executionMode_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -2072,6 +2430,7 @@ public java.lang.String getExecutionMode() {
       }
       /**
        * <code>optional string executionMode = 4;</code>
+       * @return The bytes for executionMode.
        */
       public com.google.protobuf.ByteString
           getExecutionModeBytes() {
@@ -2088,57 +2447,62 @@ public java.lang.String getExecutionMode() {
       }
       /**
        * <code>optional string executionMode = 4;</code>
+       * @param value The executionMode to set.
+       * @return This builder for chaining.
        */
       public Builder setExecutionMode(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000008;
+        if (value == null) { throw new NullPointerException(); }
         executionMode_ = value;
+        bitField0_ |= 0x00000008;
         onChanged();
         return this;
       }
       /**
        * <code>optional string executionMode = 4;</code>
+       * @return This builder for chaining.
        */
       public Builder clearExecutionMode() {
-        bitField0_ = (bitField0_ & ~0x00000008);
         executionMode_ = getDefaultInstance().getExecutionMode();
+        bitField0_ = (bitField0_ & ~0x00000008);
         onChanged();
         return this;
       }
       /**
        * <code>optional string executionMode = 4;</code>
+       * @param value The bytes for executionMode to set.
+       * @return This builder for chaining.
        */
       public Builder setExecutionModeBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000008;
+        if (value == null) { throw new NullPointerException(); }
         executionMode_ = value;
+        bitField0_ |= 0x00000008;
         onChanged();
         return this;
       }
 
-      // optional string requestUser = 5;
       private java.lang.Object requestUser_ = "";
       /**
        * <code>optional string requestUser = 5;</code>
+       * @return Whether the requestUser field is set.
        */
       public boolean hasRequestUser() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
+        return ((bitField0_ & 0x00000010) != 0);
       }
       /**
        * <code>optional string requestUser = 5;</code>
+       * @return The requestUser.
        */
       public java.lang.String getRequestUser() {
         java.lang.Object ref = requestUser_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          requestUser_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            requestUser_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -2146,6 +2510,7 @@ public java.lang.String getRequestUser() {
       }
       /**
        * <code>optional string requestUser = 5;</code>
+       * @return The bytes for requestUser.
        */
       public com.google.protobuf.ByteString
           getRequestUserBytes() {
@@ -2162,57 +2527,62 @@ public java.lang.String getRequestUser() {
       }
       /**
        * <code>optional string requestUser = 5;</code>
+       * @param value The requestUser to set.
+       * @return This builder for chaining.
        */
       public Builder setRequestUser(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000010;
+        if (value == null) { throw new NullPointerException(); }
         requestUser_ = value;
+        bitField0_ |= 0x00000010;
         onChanged();
         return this;
       }
       /**
        * <code>optional string requestUser = 5;</code>
+       * @return This builder for chaining.
        */
       public Builder clearRequestUser() {
-        bitField0_ = (bitField0_ & ~0x00000010);
         requestUser_ = getDefaultInstance().getRequestUser();
+        bitField0_ = (bitField0_ & ~0x00000010);
         onChanged();
         return this;
       }
       /**
        * <code>optional string requestUser = 5;</code>
+       * @param value The bytes for requestUser to set.
+       * @return This builder for chaining.
        */
       public Builder setRequestUserBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000010;
+        if (value == null) { throw new NullPointerException(); }
         requestUser_ = value;
+        bitField0_ |= 0x00000010;
         onChanged();
         return this;
       }
 
-      // optional string queue = 6;
       private java.lang.Object queue_ = "";
       /**
        * <code>optional string queue = 6;</code>
+       * @return Whether the queue field is set.
        */
       public boolean hasQueue() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
+        return ((bitField0_ & 0x00000020) != 0);
       }
       /**
        * <code>optional string queue = 6;</code>
+       * @return The queue.
        */
       public java.lang.String getQueue() {
         java.lang.Object ref = queue_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          queue_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            queue_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -2220,6 +2590,7 @@ public java.lang.String getQueue() {
       }
       /**
        * <code>optional string queue = 6;</code>
+       * @return The bytes for queue.
        */
       public com.google.protobuf.ByteString
           getQueueBytes() {
@@ -2236,57 +2607,62 @@ public java.lang.String getQueue() {
       }
       /**
        * <code>optional string queue = 6;</code>
+       * @param value The queue to set.
+       * @return This builder for chaining.
        */
       public Builder setQueue(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000020;
+        if (value == null) { throw new NullPointerException(); }
         queue_ = value;
+        bitField0_ |= 0x00000020;
         onChanged();
         return this;
       }
       /**
        * <code>optional string queue = 6;</code>
+       * @return This builder for chaining.
        */
       public Builder clearQueue() {
-        bitField0_ = (bitField0_ & ~0x00000020);
         queue_ = getDefaultInstance().getQueue();
+        bitField0_ = (bitField0_ & ~0x00000020);
         onChanged();
         return this;
       }
       /**
        * <code>optional string queue = 6;</code>
+       * @param value The bytes for queue to set.
+       * @return This builder for chaining.
        */
       public Builder setQueueBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000020;
+        if (value == null) { throw new NullPointerException(); }
         queue_ = value;
+        bitField0_ |= 0x00000020;
         onChanged();
         return this;
       }
 
-      // optional string user = 7;
       private java.lang.Object user_ = "";
       /**
        * <code>optional string user = 7;</code>
+       * @return Whether the user field is set.
        */
       public boolean hasUser() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
+        return ((bitField0_ & 0x00000040) != 0);
       }
       /**
        * <code>optional string user = 7;</code>
+       * @return The user.
        */
       public java.lang.String getUser() {
         java.lang.Object ref = user_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          user_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            user_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -2294,6 +2670,7 @@ public java.lang.String getUser() {
       }
       /**
        * <code>optional string user = 7;</code>
+       * @return The bytes for user.
        */
       public com.google.protobuf.ByteString
           getUserBytes() {
@@ -2310,57 +2687,62 @@ public java.lang.String getUser() {
       }
       /**
        * <code>optional string user = 7;</code>
+       * @param value The user to set.
+       * @return This builder for chaining.
        */
       public Builder setUser(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000040;
+        if (value == null) { throw new NullPointerException(); }
         user_ = value;
+        bitField0_ |= 0x00000040;
         onChanged();
         return this;
       }
       /**
        * <code>optional string user = 7;</code>
+       * @return This builder for chaining.
        */
       public Builder clearUser() {
-        bitField0_ = (bitField0_ & ~0x00000040);
         user_ = getDefaultInstance().getUser();
+        bitField0_ = (bitField0_ & ~0x00000040);
         onChanged();
         return this;
       }
       /**
        * <code>optional string user = 7;</code>
+       * @param value The bytes for user to set.
+       * @return This builder for chaining.
        */
       public Builder setUserBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000040;
+        if (value == null) { throw new NullPointerException(); }
         user_ = value;
+        bitField0_ |= 0x00000040;
         onChanged();
         return this;
       }
 
-      // optional string operationId = 8;
       private java.lang.Object operationId_ = "";
       /**
        * <code>optional string operationId = 8;</code>
+       * @return Whether the operationId field is set.
        */
       public boolean hasOperationId() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
+        return ((bitField0_ & 0x00000080) != 0);
       }
       /**
        * <code>optional string operationId = 8;</code>
+       * @return The operationId.
        */
       public java.lang.String getOperationId() {
         java.lang.Object ref = operationId_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          operationId_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            operationId_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -2368,6 +2750,7 @@ public java.lang.String getOperationId() {
       }
       /**
        * <code>optional string operationId = 8;</code>
+       * @return The bytes for operationId.
        */
       public com.google.protobuf.ByteString
           getOperationIdBytes() {
@@ -2384,69 +2767,77 @@ public java.lang.String getOperationId() {
       }
       /**
        * <code>optional string operationId = 8;</code>
+       * @param value The operationId to set.
+       * @return This builder for chaining.
        */
       public Builder setOperationId(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000080;
+        if (value == null) { throw new NullPointerException(); }
         operationId_ = value;
+        bitField0_ |= 0x00000080;
         onChanged();
         return this;
       }
       /**
        * <code>optional string operationId = 8;</code>
+       * @return This builder for chaining.
        */
       public Builder clearOperationId() {
-        bitField0_ = (bitField0_ & ~0x00000080);
         operationId_ = getDefaultInstance().getOperationId();
+        bitField0_ = (bitField0_ & ~0x00000080);
         onChanged();
         return this;
       }
       /**
        * <code>optional string operationId = 8;</code>
+       * @param value The bytes for operationId to set.
+       * @return This builder for chaining.
        */
       public Builder setOperationIdBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000080;
+        if (value == null) { throw new NullPointerException(); }
         operationId_ = value;
+        bitField0_ |= 0x00000080;
         onChanged();
         return this;
       }
 
-      // repeated string tablesWritten = 9;
-      private com.google.protobuf.LazyStringList tablesWritten_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private com.google.protobuf.LazyStringArrayList tablesWritten_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
       private void ensureTablesWrittenIsMutable() {
-        if (!((bitField0_ & 0x00000100) == 0x00000100)) {
+        if (!tablesWritten_.isModifiable()) {
           tablesWritten_ = new com.google.protobuf.LazyStringArrayList(tablesWritten_);
-          bitField0_ |= 0x00000100;
-         }
+        }
+        bitField0_ |= 0x00000100;
       }
       /**
        * <code>repeated string tablesWritten = 9;</code>
+       * @return A list containing the tablesWritten.
        */
-      public java.util.List<java.lang.String>
+      public com.google.protobuf.ProtocolStringList
           getTablesWrittenList() {
-        return java.util.Collections.unmodifiableList(tablesWritten_);
+        tablesWritten_.makeImmutable();
+        return tablesWritten_;
       }
       /**
        * <code>repeated string tablesWritten = 9;</code>
+       * @return The count of tablesWritten.
        */
       public int getTablesWrittenCount() {
         return tablesWritten_.size();
       }
       /**
        * <code>repeated string tablesWritten = 9;</code>
+       * @param index The index of the element to return.
+       * @return The tablesWritten at the given index.
        */
       public java.lang.String getTablesWritten(int index) {
         return tablesWritten_.get(index);
       }
       /**
        * <code>repeated string tablesWritten = 9;</code>
+       * @param index The index of the value to return.
+       * @return The bytes of the tablesWritten at the given index.
        */
       public com.google.protobuf.ByteString
           getTablesWrittenBytes(int index) {
@@ -2454,92 +2845,109 @@ public java.lang.String getTablesWritten(int index) {
       }
       /**
        * <code>repeated string tablesWritten = 9;</code>
+       * @param index The index to set the value at.
+       * @param value The tablesWritten to set.
+       * @return This builder for chaining.
        */
       public Builder setTablesWritten(
           int index, java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureTablesWrittenIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureTablesWrittenIsMutable();
         tablesWritten_.set(index, value);
+        bitField0_ |= 0x00000100;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string tablesWritten = 9;</code>
+       * @param value The tablesWritten to add.
+       * @return This builder for chaining.
        */
       public Builder addTablesWritten(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureTablesWrittenIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureTablesWrittenIsMutable();
         tablesWritten_.add(value);
+        bitField0_ |= 0x00000100;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string tablesWritten = 9;</code>
+       * @param values The tablesWritten to add.
+       * @return This builder for chaining.
        */
       public Builder addAllTablesWritten(
           java.lang.Iterable<java.lang.String> values) {
         ensureTablesWrittenIsMutable();
-        super.addAll(values, tablesWritten_);
+        com.google.protobuf.AbstractMessageLite.Builder.addAll(
+            values, tablesWritten_);
+        bitField0_ |= 0x00000100;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string tablesWritten = 9;</code>
+       * @return This builder for chaining.
        */
       public Builder clearTablesWritten() {
-        tablesWritten_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000100);
+        tablesWritten_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+        bitField0_ = (bitField0_ & ~0x00000100);;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string tablesWritten = 9;</code>
+       * @param value The bytes of the tablesWritten to add.
+       * @return This builder for chaining.
        */
       public Builder addTablesWrittenBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureTablesWrittenIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureTablesWrittenIsMutable();
         tablesWritten_.add(value);
+        bitField0_ |= 0x00000100;
         onChanged();
         return this;
       }
 
-      // repeated string tablesRead = 10;
-      private com.google.protobuf.LazyStringList tablesRead_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private com.google.protobuf.LazyStringArrayList tablesRead_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
       private void ensureTablesReadIsMutable() {
-        if (!((bitField0_ & 0x00000200) == 0x00000200)) {
+        if (!tablesRead_.isModifiable()) {
           tablesRead_ = new com.google.protobuf.LazyStringArrayList(tablesRead_);
-          bitField0_ |= 0x00000200;
-         }
+        }
+        bitField0_ |= 0x00000200;
       }
       /**
        * <code>repeated string tablesRead = 10;</code>
+       * @return A list containing the tablesRead.
        */
-      public java.util.List<java.lang.String>
+      public com.google.protobuf.ProtocolStringList
           getTablesReadList() {
-        return java.util.Collections.unmodifiableList(tablesRead_);
+        tablesRead_.makeImmutable();
+        return tablesRead_;
       }
       /**
        * <code>repeated string tablesRead = 10;</code>
+       * @return The count of tablesRead.
        */
       public int getTablesReadCount() {
         return tablesRead_.size();
       }
       /**
        * <code>repeated string tablesRead = 10;</code>
+       * @param index The index of the element to return.
+       * @return The tablesRead at the given index.
        */
       public java.lang.String getTablesRead(int index) {
         return tablesRead_.get(index);
       }
       /**
        * <code>repeated string tablesRead = 10;</code>
+       * @param index The index of the value to return.
+       * @return The bytes of the tablesRead at the given index.
        */
       public com.google.protobuf.ByteString
           getTablesReadBytes(int index) {
@@ -2547,74 +2955,83 @@ public java.lang.String getTablesRead(int index) {
       }
       /**
        * <code>repeated string tablesRead = 10;</code>
+       * @param index The index to set the value at.
+       * @param value The tablesRead to set.
+       * @return This builder for chaining.
        */
       public Builder setTablesRead(
           int index, java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureTablesReadIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureTablesReadIsMutable();
         tablesRead_.set(index, value);
+        bitField0_ |= 0x00000200;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string tablesRead = 10;</code>
+       * @param value The tablesRead to add.
+       * @return This builder for chaining.
        */
       public Builder addTablesRead(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureTablesReadIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureTablesReadIsMutable();
         tablesRead_.add(value);
+        bitField0_ |= 0x00000200;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string tablesRead = 10;</code>
+       * @param values The tablesRead to add.
+       * @return This builder for chaining.
        */
       public Builder addAllTablesRead(
           java.lang.Iterable<java.lang.String> values) {
         ensureTablesReadIsMutable();
-        super.addAll(values, tablesRead_);
+        com.google.protobuf.AbstractMessageLite.Builder.addAll(
+            values, tablesRead_);
+        bitField0_ |= 0x00000200;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string tablesRead = 10;</code>
+       * @return This builder for chaining.
        */
       public Builder clearTablesRead() {
-        tablesRead_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000200);
+        tablesRead_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+        bitField0_ = (bitField0_ & ~0x00000200);;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string tablesRead = 10;</code>
+       * @param value The bytes of the tablesRead to add.
+       * @return This builder for chaining.
        */
       public Builder addTablesReadBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureTablesReadIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureTablesReadIsMutable();
         tablesRead_.add(value);
+        bitField0_ |= 0x00000200;
         onChanged();
         return this;
       }
 
-      // repeated .MapFieldEntry otherInfo = 50;
       private java.util.List<org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry> otherInfo_ =
         java.util.Collections.emptyList();
       private void ensureOtherInfoIsMutable() {
-        if (!((bitField0_ & 0x00000400) == 0x00000400)) {
+        if (!((bitField0_ & 0x00000400) != 0)) {
           otherInfo_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry>(otherInfo_);
           bitField0_ |= 0x00000400;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry, org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntryOrBuilder> otherInfoBuilder_;
 
       /**
@@ -2746,7 +3163,8 @@ public Builder addAllOtherInfo(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry> values) {
         if (otherInfoBuilder_ == null) {
           ensureOtherInfoIsMutable();
-          super.addAll(values, otherInfo_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, otherInfo_);
           onChanged();
         } else {
           otherInfoBuilder_.addAllMessages(values);
@@ -2829,48 +3247,100 @@ public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.Builde
            getOtherInfoBuilderList() {
         return getOtherInfoFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry, org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntryOrBuilder> 
           getOtherInfoFieldBuilder() {
         if (otherInfoBuilder_ == null) {
-          otherInfoBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          otherInfoBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry, org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntryOrBuilder>(
                   otherInfo_,
-                  ((bitField0_ & 0x00000400) == 0x00000400),
+                  ((bitField0_ & 0x00000400) != 0),
                   getParentForChildren(),
                   isClean());
           otherInfo_ = null;
         }
         return otherInfoBuilder_;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:HiveHookEventProto)
     }
 
+    // @@protoc_insertion_point(class_scope:HiveHookEventProto)
+    private static final org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto DEFAULT_INSTANCE;
     static {
-      defaultInstance = new HiveHookEventProto(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto();
+    }
+
+    public static org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<HiveHookEventProto>
+        PARSER = new com.google.protobuf.AbstractParser<HiveHookEventProto>() {
+      @java.lang.Override
+      public HiveHookEventProto parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<HiveHookEventProto> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<HiveHookEventProto> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.HiveHookEventProto getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:HiveHookEventProto)
   }
 
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_MapFieldEntry_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_MapFieldEntry_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_HiveHookEventProto_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_HiveHookEventProto_fieldAccessorTable;
 
   public static com.google.protobuf.Descriptors.FileDescriptor
       getDescriptor() {
     return descriptor;
   }
-  private static com.google.protobuf.Descriptors.FileDescriptor
+  private static  com.google.protobuf.Descriptors.FileDescriptor
       descriptor;
   static {
     java.lang.String[] descriptorData = {
@@ -2885,30 +3355,22 @@ public org.apache.hadoop.hive.ql.hooks.proto.HiveHookEvents.MapFieldEntry.Builde
       "%org.apache.hadoop.hive.ql.hooks.protoB\016" +
       "HiveHookEvents"
     };
-    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
-      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
-        public com.google.protobuf.ExtensionRegistry assignDescriptors(
-            com.google.protobuf.Descriptors.FileDescriptor root) {
-          descriptor = root;
-          internal_static_MapFieldEntry_descriptor =
-            getDescriptor().getMessageTypes().get(0);
-          internal_static_MapFieldEntry_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_MapFieldEntry_descriptor,
-              new java.lang.String[] { "Key", "Value", });
-          internal_static_HiveHookEventProto_descriptor =
-            getDescriptor().getMessageTypes().get(1);
-          internal_static_HiveHookEventProto_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_HiveHookEventProto_descriptor,
-              new java.lang.String[] { "EventType", "HiveQueryId", "Timestamp", "ExecutionMode", "RequestUser", "Queue", "User", "OperationId", "TablesWritten", "TablesRead", "OtherInfo", });
-          return null;
-        }
-      };
-    com.google.protobuf.Descriptors.FileDescriptor
+    descriptor = com.google.protobuf.Descriptors.FileDescriptor
       .internalBuildGeneratedFileFrom(descriptorData,
         new com.google.protobuf.Descriptors.FileDescriptor[] {
-        }, assigner);
+        });
+    internal_static_MapFieldEntry_descriptor =
+      getDescriptor().getMessageTypes().get(0);
+    internal_static_MapFieldEntry_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_MapFieldEntry_descriptor,
+        new java.lang.String[] { "Key", "Value", });
+    internal_static_HiveHookEventProto_descriptor =
+      getDescriptor().getMessageTypes().get(1);
+    internal_static_HiveHookEventProto_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_HiveHookEventProto_descriptor,
+        new java.lang.String[] { "EventType", "HiveQueryId", "Timestamp", "ExecutionMode", "RequestUser", "Queue", "User", "OperationId", "TablesWritten", "TablesRead", "OtherInfo", });
   }
 
   // @@protoc_insertion_point(outer_class_scope)
diff --git a/ql/src/gen/protobuf/gen-test/org/apache/hadoop/hive/ql/io/protobuf/SampleProtos.java b/ql/src/gen/protobuf/gen-test/org/apache/hadoop/hive/ql/io/protobuf/SampleProtos.java
index ac75608439..0f7e13c655 100644
--- a/ql/src/gen/protobuf/gen-test/org/apache/hadoop/hive/ql/io/protobuf/SampleProtos.java
+++ b/ql/src/gen/protobuf/gen-test/org/apache/hadoop/hive/ql/io/protobuf/SampleProtos.java
@@ -5,38 +5,49 @@
 
 public final class SampleProtos {
   private SampleProtos() {}
+  public static void registerAllExtensions(
+      com.google.protobuf.ExtensionRegistryLite registry) {
+  }
+
   public static void registerAllExtensions(
       com.google.protobuf.ExtensionRegistry registry) {
+    registerAllExtensions(
+        (com.google.protobuf.ExtensionRegistryLite) registry);
   }
-  public interface MapFieldEntryOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface MapFieldEntryOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:MapFieldEntry)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional string key = 1;
     /**
      * <code>optional string key = 1;</code>
+     * @return Whether the key field is set.
      */
     boolean hasKey();
     /**
      * <code>optional string key = 1;</code>
+     * @return The key.
      */
     java.lang.String getKey();
     /**
      * <code>optional string key = 1;</code>
+     * @return The bytes for key.
      */
     com.google.protobuf.ByteString
         getKeyBytes();
 
-    // optional string value = 2;
     /**
      * <code>optional string value = 2;</code>
+     * @return Whether the value field is set.
      */
     boolean hasValue();
     /**
      * <code>optional string value = 2;</code>
+     * @return The value.
      */
     java.lang.String getValue();
     /**
      * <code>optional string value = 2;</code>
+     * @return The bytes for value.
      */
     com.google.protobuf.ByteString
         getValueBytes();
@@ -45,115 +56,56 @@ public interface MapFieldEntryOrBuilder
    * Protobuf type {@code MapFieldEntry}
    */
   public static final class MapFieldEntry extends
-      com.google.protobuf.GeneratedMessage
-      implements MapFieldEntryOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:MapFieldEntry)
+      MapFieldEntryOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use MapFieldEntry.newBuilder() to construct.
-    private MapFieldEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private MapFieldEntry(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private MapFieldEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final MapFieldEntry defaultInstance;
-    public static MapFieldEntry getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public MapFieldEntry getDefaultInstanceForType() {
-      return defaultInstance;
+    private MapFieldEntry() {
+      key_ = "";
+      value_ = "";
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private MapFieldEntry(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              key_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              value_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new MapFieldEntry();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_MapFieldEntry_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_MapFieldEntry_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.class, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<MapFieldEntry> PARSER =
-        new com.google.protobuf.AbstractParser<MapFieldEntry>() {
-      public MapFieldEntry parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new MapFieldEntry(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<MapFieldEntry> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // optional string key = 1;
     public static final int KEY_FIELD_NUMBER = 1;
-    private java.lang.Object key_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object key_ = "";
     /**
      * <code>optional string key = 1;</code>
+     * @return Whether the key field is set.
      */
+    @java.lang.Override
     public boolean hasKey() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional string key = 1;</code>
+     * @return The key.
      */
+    @java.lang.Override
     public java.lang.String getKey() {
       java.lang.Object ref = key_;
       if (ref instanceof java.lang.String) {
@@ -170,7 +122,9 @@ public java.lang.String getKey() {
     }
     /**
      * <code>optional string key = 1;</code>
+     * @return The bytes for key.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getKeyBytes() {
       java.lang.Object ref = key_;
@@ -185,18 +139,22 @@ public java.lang.String getKey() {
       }
     }
 
-    // optional string value = 2;
     public static final int VALUE_FIELD_NUMBER = 2;
-    private java.lang.Object value_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object value_ = "";
     /**
      * <code>optional string value = 2;</code>
+     * @return Whether the value field is set.
      */
+    @java.lang.Override
     public boolean hasValue() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional string value = 2;</code>
+     * @return The value.
      */
+    @java.lang.Override
     public java.lang.String getValue() {
       java.lang.Object ref = value_;
       if (ref instanceof java.lang.String) {
@@ -213,7 +171,9 @@ public java.lang.String getValue() {
     }
     /**
      * <code>optional string value = 2;</code>
+     * @return The bytes for value.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getValueBytes() {
       java.lang.Object ref = value_;
@@ -228,57 +188,101 @@ public java.lang.String getValue() {
       }
     }
 
-    private void initFields() {
-      key_ = "";
-      value_ = "";
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getKeyBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getValueBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, value_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getKeyBytes());
+      if (((bitField0_ & 0x00000001) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getValueBytes());
+      if (((bitField0_ & 0x00000002) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, value_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
     @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry other = (org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry) obj;
+
+      if (hasKey() != other.hasKey()) return false;
+      if (hasKey()) {
+        if (!getKey()
+            .equals(other.getKey())) return false;
+      }
+      if (hasValue() != other.hasValue()) return false;
+      if (hasValue()) {
+        if (!getValue()
+            .equals(other.getValue())) return false;
+      }
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
+    }
+
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptor().hashCode();
+      if (hasKey()) {
+        hash = (37 * hash) + KEY_FIELD_NUMBER;
+        hash = (53 * hash) + getKey().hashCode();
+      }
+      if (hasValue()) {
+        hash = (37 * hash) + VALUE_FIELD_NUMBER;
+        hash = (53 * hash) + getValue().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      memoizedHashCode = hash;
+      return hash;
     }
 
+    public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -302,46 +306,61 @@ public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry p
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -349,14 +368,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code MapFieldEntry}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:MapFieldEntry)
+        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_MapFieldEntry_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_MapFieldEntry_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -365,44 +386,35 @@ public static final class Builder extends
 
       // Construct using org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.newBuilder()
       private Builder() {
-        maybeForceBuilderInitialization();
+
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
 
+      }
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         key_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
         value_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_MapFieldEntry_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry build() {
         org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry result = buildPartial();
         if (!result.isInitialized()) {
@@ -411,23 +423,61 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry build()
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry buildPartial() {
         org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry result = new org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry(this);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.key_ = key_;
           to_bitField0_ |= 0x00000001;
         }
-        result.key_ = key_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.value_ = value_;
           to_bitField0_ |= 0x00000002;
         }
-        result.value_ = value_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry)other);
@@ -440,59 +490,89 @@ public Builder mergeFrom(com.google.protobuf.Message other) {
       public Builder mergeFrom(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry other) {
         if (other == org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance()) return this;
         if (other.hasKey()) {
-          bitField0_ |= 0x00000001;
           key_ = other.key_;
+          bitField0_ |= 0x00000001;
           onChanged();
         }
         if (other.hasValue()) {
-          bitField0_ |= 0x00000002;
           value_ = other.value_;
+          bitField0_ |= 0x00000002;
           onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                key_ = input.readBytes();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 10
+              case 18: {
+                value_ = input.readBytes();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional string key = 1;
       private java.lang.Object key_ = "";
       /**
        * <code>optional string key = 1;</code>
+       * @return Whether the key field is set.
        */
       public boolean hasKey() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional string key = 1;</code>
+       * @return The key.
        */
       public java.lang.String getKey() {
         java.lang.Object ref = key_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          key_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            key_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -500,6 +580,7 @@ public java.lang.String getKey() {
       }
       /**
        * <code>optional string key = 1;</code>
+       * @return The bytes for key.
        */
       public com.google.protobuf.ByteString
           getKeyBytes() {
@@ -516,57 +597,62 @@ public java.lang.String getKey() {
       }
       /**
        * <code>optional string key = 1;</code>
+       * @param value The key to set.
+       * @return This builder for chaining.
        */
       public Builder setKey(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         key_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional string key = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearKey() {
-        bitField0_ = (bitField0_ & ~0x00000001);
         key_ = getDefaultInstance().getKey();
+        bitField0_ = (bitField0_ & ~0x00000001);
         onChanged();
         return this;
       }
       /**
        * <code>optional string key = 1;</code>
+       * @param value The bytes for key to set.
+       * @return This builder for chaining.
        */
       public Builder setKeyBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
+        if (value == null) { throw new NullPointerException(); }
         key_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
 
-      // optional string value = 2;
       private java.lang.Object value_ = "";
       /**
        * <code>optional string value = 2;</code>
+       * @return Whether the value field is set.
        */
       public boolean hasValue() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional string value = 2;</code>
+       * @return The value.
        */
       public java.lang.String getValue() {
         java.lang.Object ref = value_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          value_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            value_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -574,6 +660,7 @@ public java.lang.String getValue() {
       }
       /**
        * <code>optional string value = 2;</code>
+       * @return The bytes for value.
        */
       public com.google.protobuf.ByteString
           getValueBytes() {
@@ -590,55 +677,108 @@ public java.lang.String getValue() {
       }
       /**
        * <code>optional string value = 2;</code>
+       * @param value The value to set.
+       * @return This builder for chaining.
        */
       public Builder setValue(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         value_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional string value = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearValue() {
-        bitField0_ = (bitField0_ & ~0x00000002);
         value_ = getDefaultInstance().getValue();
+        bitField0_ = (bitField0_ & ~0x00000002);
         onChanged();
         return this;
       }
       /**
        * <code>optional string value = 2;</code>
+       * @param value The bytes for value to set.
+       * @return This builder for chaining.
        */
       public Builder setValueBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
+        if (value == null) { throw new NullPointerException(); }
         value_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:MapFieldEntry)
     }
 
+    // @@protoc_insertion_point(class_scope:MapFieldEntry)
+    private static final org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry DEFAULT_INSTANCE;
     static {
-      defaultInstance = new MapFieldEntry(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry();
+    }
+
+    public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<MapFieldEntry>
+        PARSER = new com.google.protobuf.AbstractParser<MapFieldEntry>() {
+      @java.lang.Override
+      public MapFieldEntry parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<MapFieldEntry> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<MapFieldEntry> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:MapFieldEntry)
   }
 
-  public interface Mesg1OrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface Mesg1OrBuilder extends
+      // @@protoc_insertion_point(interface_extends:Mesg1)
+      com.google.protobuf.MessageOrBuilder {
 
-    // repeated .MapFieldEntry anotherMap = 1;
     /**
      * <code>repeated .MapFieldEntry anotherMap = 1;</code>
      */
@@ -663,13 +803,14 @@ public interface Mesg1OrBuilder
     org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getAnotherMapOrBuilder(
         int index);
 
-    // optional .MapFieldEntry noMap = 2;
     /**
      * <code>optional .MapFieldEntry noMap = 2;</code>
+     * @return Whether the noMap field is set.
      */
     boolean hasNoMap();
     /**
      * <code>optional .MapFieldEntry noMap = 2;</code>
+     * @return The noMap.
      */
     org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry getNoMap();
     /**
@@ -677,17 +818,20 @@ org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getAno
      */
     org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getNoMapOrBuilder();
 
-    // repeated int32 intList = 3;
     /**
      * <code>repeated int32 intList = 3;</code>
+     * @return A list containing the intList.
      */
     java.util.List<java.lang.Integer> getIntListList();
     /**
      * <code>repeated int32 intList = 3;</code>
+     * @return The count of intList.
      */
     int getIntListCount();
     /**
      * <code>repeated int32 intList = 3;</code>
+     * @param index The index of the element to return.
+     * @return The intList at the given index.
      */
     int getIntList(int index);
   }
@@ -695,153 +839,54 @@ org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getAno
    * Protobuf type {@code Mesg1}
    */
   public static final class Mesg1 extends
-      com.google.protobuf.GeneratedMessage
-      implements Mesg1OrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:Mesg1)
+      Mesg1OrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use Mesg1.newBuilder() to construct.
-    private Mesg1(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private Mesg1(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
     }
-    private Mesg1(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final Mesg1 defaultInstance;
-    public static Mesg1 getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public Mesg1 getDefaultInstanceForType() {
-      return defaultInstance;
+    private Mesg1() {
+      anotherMap_ = java.util.Collections.emptyList();
+      intList_ = emptyIntList();
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private Mesg1(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                anotherMap_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              anotherMap_.add(input.readMessage(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.PARSER, extensionRegistry));
-              break;
-            }
-            case 18: {
-              org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = noMap_.toBuilder();
-              }
-              noMap_ = input.readMessage(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(noMap_);
-                noMap_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-            case 24: {
-              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
-                intList_ = new java.util.ArrayList<java.lang.Integer>();
-                mutable_bitField0_ |= 0x00000004;
-              }
-              intList_.add(input.readInt32());
-              break;
-            }
-            case 26: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004) && input.getBytesUntilLimit() > 0) {
-                intList_ = new java.util.ArrayList<java.lang.Integer>();
-                mutable_bitField0_ |= 0x00000004;
-              }
-              while (input.getBytesUntilLimit() > 0) {
-                intList_.add(input.readInt32());
-              }
-              input.popLimit(limit);
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          anotherMap_ = java.util.Collections.unmodifiableList(anotherMap_);
-        }
-        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
-          intList_ = java.util.Collections.unmodifiableList(intList_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new Mesg1();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_Mesg1_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_Mesg1_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.class, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<Mesg1> PARSER =
-        new com.google.protobuf.AbstractParser<Mesg1>() {
-      public Mesg1 parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new Mesg1(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<Mesg1> getParserForType() {
-      return PARSER;
-    }
-
     private int bitField0_;
-    // repeated .MapFieldEntry anotherMap = 1;
     public static final int ANOTHERMAP_FIELD_NUMBER = 1;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry> anotherMap_;
     /**
      * <code>repeated .MapFieldEntry anotherMap = 1;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry> getAnotherMapList() {
       return anotherMap_;
     }
     /**
      * <code>repeated .MapFieldEntry anotherMap = 1;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder> 
         getAnotherMapOrBuilderList() {
       return anotherMap_;
@@ -849,100 +894,110 @@ public java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFiel
     /**
      * <code>repeated .MapFieldEntry anotherMap = 1;</code>
      */
+    @java.lang.Override
     public int getAnotherMapCount() {
       return anotherMap_.size();
     }
     /**
      * <code>repeated .MapFieldEntry anotherMap = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry getAnotherMap(int index) {
       return anotherMap_.get(index);
     }
     /**
      * <code>repeated .MapFieldEntry anotherMap = 1;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getAnotherMapOrBuilder(
         int index) {
       return anotherMap_.get(index);
     }
 
-    // optional .MapFieldEntry noMap = 2;
     public static final int NOMAP_FIELD_NUMBER = 2;
     private org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry noMap_;
     /**
      * <code>optional .MapFieldEntry noMap = 2;</code>
+     * @return Whether the noMap field is set.
      */
+    @java.lang.Override
     public boolean hasNoMap() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional .MapFieldEntry noMap = 2;</code>
+     * @return The noMap.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry getNoMap() {
-      return noMap_;
+      return noMap_ == null ? org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance() : noMap_;
     }
     /**
      * <code>optional .MapFieldEntry noMap = 2;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getNoMapOrBuilder() {
-      return noMap_;
+      return noMap_ == null ? org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance() : noMap_;
     }
 
-    // repeated int32 intList = 3;
     public static final int INTLIST_FIELD_NUMBER = 3;
-    private java.util.List<java.lang.Integer> intList_;
+    @SuppressWarnings("serial")
+    private com.google.protobuf.Internal.IntList intList_ =
+        emptyIntList();
     /**
      * <code>repeated int32 intList = 3;</code>
+     * @return A list containing the intList.
      */
+    @java.lang.Override
     public java.util.List<java.lang.Integer>
         getIntListList() {
       return intList_;
     }
     /**
      * <code>repeated int32 intList = 3;</code>
+     * @return The count of intList.
      */
     public int getIntListCount() {
       return intList_.size();
     }
     /**
      * <code>repeated int32 intList = 3;</code>
+     * @param index The index of the element to return.
+     * @return The intList at the given index.
      */
     public int getIntList(int index) {
-      return intList_.get(index);
+      return intList_.getInt(index);
     }
 
-    private void initFields() {
-      anotherMap_ = java.util.Collections.emptyList();
-      noMap_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance();
-      intList_ = java.util.Collections.emptyList();
-    }
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
       for (int i = 0; i < anotherMap_.size(); i++) {
         output.writeMessage(1, anotherMap_.get(i));
       }
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(2, noMap_);
+      if (((bitField0_ & 0x00000001) != 0)) {
+        output.writeMessage(2, getNoMap());
       }
       for (int i = 0; i < intList_.size(); i++) {
-        output.writeInt32(3, intList_.get(i));
+        output.writeInt32(3, intList_.getInt(i));
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
@@ -950,31 +1005,82 @@ public int getSerializedSize() {
         size += com.google.protobuf.CodedOutputStream
           .computeMessageSize(1, anotherMap_.get(i));
       }
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(2, noMap_);
+          .computeMessageSize(2, getNoMap());
       }
       {
         int dataSize = 0;
         for (int i = 0; i < intList_.size(); i++) {
           dataSize += com.google.protobuf.CodedOutputStream
-            .computeInt32SizeNoTag(intList_.get(i));
+            .computeInt32SizeNoTag(intList_.getInt(i));
         }
         size += dataSize;
         size += 1 * getIntListList().size();
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
     @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 other = (org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1) obj;
+
+      if (!getAnotherMapList()
+          .equals(other.getAnotherMapList())) return false;
+      if (hasNoMap() != other.hasNoMap()) return false;
+      if (hasNoMap()) {
+        if (!getNoMap()
+            .equals(other.getNoMap())) return false;
+      }
+      if (!getIntListList()
+          .equals(other.getIntListList())) return false;
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
+    }
+
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptor().hashCode();
+      if (getAnotherMapCount() > 0) {
+        hash = (37 * hash) + ANOTHERMAP_FIELD_NUMBER;
+        hash = (53 * hash) + getAnotherMapList().hashCode();
+      }
+      if (hasNoMap()) {
+        hash = (37 * hash) + NOMAP_FIELD_NUMBER;
+        hash = (53 * hash) + getNoMap().hashCode();
+      }
+      if (getIntListCount() > 0) {
+        hash = (37 * hash) + INTLIST_FIELD_NUMBER;
+        hash = (53 * hash) + getIntListList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      memoizedHashCode = hash;
+      return hash;
     }
 
+    public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -998,46 +1104,61 @@ public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseFrom
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -1045,14 +1166,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code Mesg1}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:Mesg1)
+        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_Mesg1_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_Mesg1_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -1065,52 +1188,49 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getAnotherMapFieldBuilder();
           getNoMapFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         if (anotherMapBuilder_ == null) {
           anotherMap_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
         } else {
+          anotherMap_ = null;
           anotherMapBuilder_.clear();
         }
-        if (noMapBuilder_ == null) {
-          noMap_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance();
-        } else {
-          noMapBuilder_.clear();
+        bitField0_ = (bitField0_ & ~0x00000001);
+        noMap_ = null;
+        if (noMapBuilder_ != null) {
+          noMapBuilder_.dispose();
+          noMapBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00000002);
-        intList_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000004);
+        intList_ = emptyIntList();
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_Mesg1_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 build() {
         org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 result = buildPartial();
         if (!result.isInitialized()) {
@@ -1119,12 +1239,18 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 build() {
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 buildPartial() {
         org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 result = new org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
+        buildPartialRepeatedFields(result);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartialRepeatedFields(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 result) {
         if (anotherMapBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
+          if (((bitField0_ & 0x00000001) != 0)) {
             anotherMap_ = java.util.Collections.unmodifiableList(anotherMap_);
             bitField0_ = (bitField0_ & ~0x00000001);
           }
@@ -1132,24 +1258,57 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 buildPartial() {
         } else {
           result.anotherMap_ = anotherMapBuilder_.build();
         }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 result) {
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.noMap_ = noMapBuilder_ == null
+              ? noMap_
+              : noMapBuilder_.build();
           to_bitField0_ |= 0x00000001;
         }
-        if (noMapBuilder_ == null) {
-          result.noMap_ = noMap_;
-        } else {
-          result.noMap_ = noMapBuilder_.build();
-        }
-        if (((bitField0_ & 0x00000004) == 0x00000004)) {
-          intList_ = java.util.Collections.unmodifiableList(intList_);
-          bitField0_ = (bitField0_ & ~0x00000004);
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          intList_.makeImmutable();
+          result.intList_ = intList_;
         }
-        result.intList_ = intList_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1)other);
@@ -1180,7 +1339,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg
               anotherMap_ = other.anotherMap_;
               bitField0_ = (bitField0_ & ~0x00000001);
               anotherMapBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getAnotherMapFieldBuilder() : null;
             } else {
               anotherMapBuilder_.addAllMessages(other.anotherMap_);
@@ -1193,7 +1352,8 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg
         if (!other.intList_.isEmpty()) {
           if (intList_.isEmpty()) {
             intList_ = other.intList_;
-            bitField0_ = (bitField0_ & ~0x00000004);
+            intList_.makeImmutable();
+            bitField0_ |= 0x00000004;
           } else {
             ensureIntListIsMutable();
             intList_.addAll(other.intList_);
@@ -1201,43 +1361,94 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg
           onChanged();
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 10: {
+                org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.PARSER,
+                        extensionRegistry);
+                if (anotherMapBuilder_ == null) {
+                  ensureAnotherMapIsMutable();
+                  anotherMap_.add(m);
+                } else {
+                  anotherMapBuilder_.addMessage(m);
+                }
+                break;
+              } // case 10
+              case 18: {
+                input.readMessage(
+                    getNoMapFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 18
+              case 24: {
+                int v = input.readInt32();
+                ensureIntListIsMutable();
+                intList_.addInt(v);
+                break;
+              } // case 24
+              case 26: {
+                int length = input.readRawVarint32();
+                int limit = input.pushLimit(length);
+                ensureIntListIsMutable();
+                while (input.getBytesUntilLimit() > 0) {
+                  intList_.addInt(input.readInt32());
+                }
+                input.popLimit(limit);
+                break;
+              } // case 26
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // repeated .MapFieldEntry anotherMap = 1;
       private java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry> anotherMap_ =
         java.util.Collections.emptyList();
       private void ensureAnotherMapIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+        if (!((bitField0_ & 0x00000001) != 0)) {
           anotherMap_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry>(anotherMap_);
           bitField0_ |= 0x00000001;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder> anotherMapBuilder_;
 
       /**
@@ -1369,7 +1580,8 @@ public Builder addAllAnotherMap(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry> values) {
         if (anotherMapBuilder_ == null) {
           ensureAnotherMapIsMutable();
-          super.addAll(values, anotherMap_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, anotherMap_);
           onChanged();
         } else {
           anotherMapBuilder_.addAllMessages(values);
@@ -1452,14 +1664,14 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder
            getAnotherMapBuilderList() {
         return getAnotherMapFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder> 
           getAnotherMapFieldBuilder() {
         if (anotherMapBuilder_ == null) {
-          anotherMapBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          anotherMapBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder>(
                   anotherMap_,
-                  ((bitField0_ & 0x00000001) == 0x00000001),
+                  ((bitField0_ & 0x00000001) != 0),
                   getParentForChildren(),
                   isClean());
           anotherMap_ = null;
@@ -1467,22 +1679,23 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder
         return anotherMapBuilder_;
       }
 
-      // optional .MapFieldEntry noMap = 2;
-      private org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry noMap_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry noMap_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder> noMapBuilder_;
       /**
        * <code>optional .MapFieldEntry noMap = 2;</code>
+       * @return Whether the noMap field is set.
        */
       public boolean hasNoMap() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional .MapFieldEntry noMap = 2;</code>
+       * @return The noMap.
        */
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry getNoMap() {
         if (noMapBuilder_ == null) {
-          return noMap_;
+          return noMap_ == null ? org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance() : noMap_;
         } else {
           return noMapBuilder_.getMessage();
         }
@@ -1496,11 +1709,11 @@ public Builder setNoMap(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFi
             throw new NullPointerException();
           }
           noMap_ = value;
-          onChanged();
         } else {
           noMapBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00000002;
+        onChanged();
         return this;
       }
       /**
@@ -1510,11 +1723,11 @@ public Builder setNoMap(
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder builderForValue) {
         if (noMapBuilder_ == null) {
           noMap_ = builderForValue.build();
-          onChanged();
         } else {
           noMapBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00000002;
+        onChanged();
         return this;
       }
       /**
@@ -1522,31 +1735,33 @@ public Builder setNoMap(
        */
       public Builder mergeNoMap(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry value) {
         if (noMapBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002) &&
-              noMap_ != org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance()) {
-            noMap_ =
-              org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.newBuilder(noMap_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00000002) != 0) &&
+            noMap_ != null &&
+            noMap_ != org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance()) {
+            getNoMapBuilder().mergeFrom(value);
           } else {
             noMap_ = value;
           }
-          onChanged();
         } else {
           noMapBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000002;
+        if (noMap_ != null) {
+          bitField0_ |= 0x00000002;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .MapFieldEntry noMap = 2;</code>
        */
       public Builder clearNoMap() {
-        if (noMapBuilder_ == null) {
-          noMap_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance();
-          onChanged();
-        } else {
-          noMapBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00000002);
+        noMap_ = null;
+        if (noMapBuilder_ != null) {
+          noMapBuilder_.dispose();
+          noMapBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -1564,19 +1779,20 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder
         if (noMapBuilder_ != null) {
           return noMapBuilder_.getMessageOrBuilder();
         } else {
-          return noMap_;
+          return noMap_ == null ?
+              org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.getDefaultInstance() : noMap_;
         }
       }
       /**
        * <code>optional .MapFieldEntry noMap = 2;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder> 
           getNoMapFieldBuilder() {
         if (noMapBuilder_ == null) {
-          noMapBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          noMapBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder>(
-                  noMap_,
+                  getNoMap(),
                   getParentForChildren(),
                   isClean());
           noMap_ = null;
@@ -1584,242 +1800,328 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder
         return noMapBuilder_;
       }
 
-      // repeated int32 intList = 3;
-      private java.util.List<java.lang.Integer> intList_ = java.util.Collections.emptyList();
+      private com.google.protobuf.Internal.IntList intList_ = emptyIntList();
       private void ensureIntListIsMutable() {
-        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
-          intList_ = new java.util.ArrayList<java.lang.Integer>(intList_);
-          bitField0_ |= 0x00000004;
-         }
+        if (!intList_.isModifiable()) {
+          intList_ = makeMutableCopy(intList_);
+        }
+        bitField0_ |= 0x00000004;
       }
       /**
        * <code>repeated int32 intList = 3;</code>
+       * @return A list containing the intList.
        */
       public java.util.List<java.lang.Integer>
           getIntListList() {
-        return java.util.Collections.unmodifiableList(intList_);
+        intList_.makeImmutable();
+        return intList_;
       }
       /**
        * <code>repeated int32 intList = 3;</code>
+       * @return The count of intList.
        */
       public int getIntListCount() {
         return intList_.size();
       }
       /**
        * <code>repeated int32 intList = 3;</code>
+       * @param index The index of the element to return.
+       * @return The intList at the given index.
        */
       public int getIntList(int index) {
-        return intList_.get(index);
+        return intList_.getInt(index);
       }
       /**
        * <code>repeated int32 intList = 3;</code>
+       * @param index The index to set the value at.
+       * @param value The intList to set.
+       * @return This builder for chaining.
        */
       public Builder setIntList(
           int index, int value) {
+
         ensureIntListIsMutable();
-        intList_.set(index, value);
+        intList_.setInt(index, value);
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>repeated int32 intList = 3;</code>
+       * @param value The intList to add.
+       * @return This builder for chaining.
        */
       public Builder addIntList(int value) {
+
         ensureIntListIsMutable();
-        intList_.add(value);
+        intList_.addInt(value);
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>repeated int32 intList = 3;</code>
+       * @param values The intList to add.
+       * @return This builder for chaining.
        */
       public Builder addAllIntList(
           java.lang.Iterable<? extends java.lang.Integer> values) {
         ensureIntListIsMutable();
-        super.addAll(values, intList_);
+        com.google.protobuf.AbstractMessageLite.Builder.addAll(
+            values, intList_);
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>repeated int32 intList = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearIntList() {
-        intList_ = java.util.Collections.emptyList();
+        intList_ = emptyIntList();
         bitField0_ = (bitField0_ & ~0x00000004);
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:Mesg1)
     }
 
+    // @@protoc_insertion_point(class_scope:Mesg1)
+    private static final org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 DEFAULT_INSTANCE;
     static {
-      defaultInstance = new Mesg1(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1();
+    }
+
+    public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<Mesg1>
+        PARSER = new com.google.protobuf.AbstractParser<Mesg1>() {
+      @java.lang.Override
+      public Mesg1 parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<Mesg1> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<Mesg1> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:Mesg1)
   }
 
-  public interface AllTypesOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
+  public interface AllTypesOrBuilder extends
+      // @@protoc_insertion_point(interface_extends:AllTypes)
+      com.google.protobuf.MessageOrBuilder {
 
-    // optional double doubleType = 1;
     /**
      * <code>optional double doubleType = 1;</code>
+     * @return Whether the doubleType field is set.
      */
     boolean hasDoubleType();
     /**
      * <code>optional double doubleType = 1;</code>
+     * @return The doubleType.
      */
     double getDoubleType();
 
-    // optional float floatType = 2;
     /**
      * <code>optional float floatType = 2;</code>
+     * @return Whether the floatType field is set.
      */
     boolean hasFloatType();
     /**
      * <code>optional float floatType = 2;</code>
+     * @return The floatType.
      */
     float getFloatType();
 
-    // optional int32 int32Type = 3;
     /**
      * <code>optional int32 int32Type = 3;</code>
+     * @return Whether the int32Type field is set.
      */
     boolean hasInt32Type();
     /**
      * <code>optional int32 int32Type = 3;</code>
+     * @return The int32Type.
      */
     int getInt32Type();
 
-    // optional int64 int64Type = 4;
     /**
      * <code>optional int64 int64Type = 4;</code>
+     * @return Whether the int64Type field is set.
      */
     boolean hasInt64Type();
     /**
      * <code>optional int64 int64Type = 4;</code>
+     * @return The int64Type.
      */
     long getInt64Type();
 
-    // optional uint32 uint32Type = 5;
     /**
      * <code>optional uint32 uint32Type = 5;</code>
+     * @return Whether the uint32Type field is set.
      */
     boolean hasUint32Type();
     /**
      * <code>optional uint32 uint32Type = 5;</code>
+     * @return The uint32Type.
      */
     int getUint32Type();
 
-    // optional uint64 uint64Type = 6;
     /**
      * <code>optional uint64 uint64Type = 6;</code>
+     * @return Whether the uint64Type field is set.
      */
     boolean hasUint64Type();
     /**
      * <code>optional uint64 uint64Type = 6;</code>
+     * @return The uint64Type.
      */
     long getUint64Type();
 
-    // optional sint32 sint32Type = 7;
     /**
      * <code>optional sint32 sint32Type = 7;</code>
+     * @return Whether the sint32Type field is set.
      */
     boolean hasSint32Type();
     /**
      * <code>optional sint32 sint32Type = 7;</code>
+     * @return The sint32Type.
      */
     int getSint32Type();
 
-    // optional sint64 sint64Type = 8;
     /**
      * <code>optional sint64 sint64Type = 8;</code>
+     * @return Whether the sint64Type field is set.
      */
     boolean hasSint64Type();
     /**
      * <code>optional sint64 sint64Type = 8;</code>
+     * @return The sint64Type.
      */
     long getSint64Type();
 
-    // optional fixed32 fixed32Type = 9;
     /**
      * <code>optional fixed32 fixed32Type = 9;</code>
+     * @return Whether the fixed32Type field is set.
      */
     boolean hasFixed32Type();
     /**
      * <code>optional fixed32 fixed32Type = 9;</code>
+     * @return The fixed32Type.
      */
     int getFixed32Type();
 
-    // optional fixed64 fixed64Type = 10;
     /**
      * <code>optional fixed64 fixed64Type = 10;</code>
+     * @return Whether the fixed64Type field is set.
      */
     boolean hasFixed64Type();
     /**
      * <code>optional fixed64 fixed64Type = 10;</code>
+     * @return The fixed64Type.
      */
     long getFixed64Type();
 
-    // optional sfixed32 sfixed32Type = 11;
     /**
      * <code>optional sfixed32 sfixed32Type = 11;</code>
+     * @return Whether the sfixed32Type field is set.
      */
     boolean hasSfixed32Type();
     /**
      * <code>optional sfixed32 sfixed32Type = 11;</code>
+     * @return The sfixed32Type.
      */
     int getSfixed32Type();
 
-    // optional sfixed64 sfixed64Type = 12;
     /**
      * <code>optional sfixed64 sfixed64Type = 12;</code>
+     * @return Whether the sfixed64Type field is set.
      */
     boolean hasSfixed64Type();
     /**
      * <code>optional sfixed64 sfixed64Type = 12;</code>
+     * @return The sfixed64Type.
      */
     long getSfixed64Type();
 
-    // optional bool boolType = 13;
     /**
      * <code>optional bool boolType = 13;</code>
+     * @return Whether the boolType field is set.
      */
     boolean hasBoolType();
     /**
      * <code>optional bool boolType = 13;</code>
+     * @return The boolType.
      */
     boolean getBoolType();
 
-    // optional string stringType = 14;
     /**
      * <code>optional string stringType = 14;</code>
+     * @return Whether the stringType field is set.
      */
     boolean hasStringType();
     /**
      * <code>optional string stringType = 14;</code>
+     * @return The stringType.
      */
     java.lang.String getStringType();
     /**
      * <code>optional string stringType = 14;</code>
+     * @return The bytes for stringType.
      */
     com.google.protobuf.ByteString
         getStringTypeBytes();
 
-    // optional bytes bytesType = 15;
     /**
      * <code>optional bytes bytesType = 15;</code>
+     * @return Whether the bytesType field is set.
      */
     boolean hasBytesType();
     /**
      * <code>optional bytes bytesType = 15;</code>
+     * @return The bytesType.
      */
     com.google.protobuf.ByteString getBytesType();
 
-    // repeated .MapFieldEntry mapType = 16;
     /**
      * <code>repeated .MapFieldEntry mapType = 16;</code>
      */
@@ -1844,33 +2146,39 @@ public interface AllTypesOrBuilder
     org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getMapTypeOrBuilder(
         int index);
 
-    // repeated string stringListType = 17;
     /**
      * <code>repeated string stringListType = 17;</code>
+     * @return A list containing the stringListType.
      */
     java.util.List<java.lang.String>
-    getStringListTypeList();
+        getStringListTypeList();
     /**
      * <code>repeated string stringListType = 17;</code>
+     * @return The count of stringListType.
      */
     int getStringListTypeCount();
     /**
      * <code>repeated string stringListType = 17;</code>
+     * @param index The index of the element to return.
+     * @return The stringListType at the given index.
      */
     java.lang.String getStringListType(int index);
     /**
      * <code>repeated string stringListType = 17;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the stringListType at the given index.
      */
     com.google.protobuf.ByteString
         getStringListTypeBytes(int index);
 
-    // optional .Mesg1 messageType = 18;
     /**
      * <code>optional .Mesg1 messageType = 18;</code>
+     * @return Whether the messageType field is set.
      */
     boolean hasMessageType();
     /**
      * <code>optional .Mesg1 messageType = 18;</code>
+     * @return The messageType.
      */
     org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 getMessageType();
     /**
@@ -1878,7 +2186,6 @@ org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getMap
      */
     org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder getMessageTypeOrBuilder();
 
-    // repeated .Mesg1 messageListType = 19;
     /**
      * <code>repeated .Mesg1 messageListType = 19;</code>
      */
@@ -1903,13 +2210,14 @@ org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getMap
     org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder getMessageListTypeOrBuilder(
         int index);
 
-    // optional .AllTypes.Enum1 enumType = 20;
     /**
      * <code>optional .AllTypes.Enum1 enumType = 20;</code>
+     * @return Whether the enumType field is set.
      */
     boolean hasEnumType();
     /**
      * <code>optional .AllTypes.Enum1 enumType = 20;</code>
+     * @return The enumType.
      */
     org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 getEnumType();
   }
@@ -1917,224 +2225,44 @@ org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder getMessageList
    * Protobuf type {@code AllTypes}
    */
   public static final class AllTypes extends
-      com.google.protobuf.GeneratedMessage
-      implements AllTypesOrBuilder {
+      com.google.protobuf.GeneratedMessageV3 implements
+      // @@protoc_insertion_point(message_implements:AllTypes)
+      AllTypesOrBuilder {
+  private static final long serialVersionUID = 0L;
     // Use AllTypes.newBuilder() to construct.
-    private AllTypes(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+    private AllTypes(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
       super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private AllTypes(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final AllTypes defaultInstance;
-    public static AllTypes getDefaultInstance() {
-      return defaultInstance;
     }
-
-    public AllTypes getDefaultInstanceForType() {
-      return defaultInstance;
+    private AllTypes() {
+      stringType_ = "";
+      bytesType_ = com.google.protobuf.ByteString.EMPTY;
+      mapType_ = java.util.Collections.emptyList();
+      stringListType_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+      messageListType_ = java.util.Collections.emptyList();
+      enumType_ = 1;
     }
 
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
     @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private AllTypes(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 9: {
-              bitField0_ |= 0x00000001;
-              doubleType_ = input.readDouble();
-              break;
-            }
-            case 21: {
-              bitField0_ |= 0x00000002;
-              floatType_ = input.readFloat();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              int32Type_ = input.readInt32();
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              int64Type_ = input.readInt64();
-              break;
-            }
-            case 40: {
-              bitField0_ |= 0x00000010;
-              uint32Type_ = input.readUInt32();
-              break;
-            }
-            case 48: {
-              bitField0_ |= 0x00000020;
-              uint64Type_ = input.readUInt64();
-              break;
-            }
-            case 56: {
-              bitField0_ |= 0x00000040;
-              sint32Type_ = input.readSInt32();
-              break;
-            }
-            case 64: {
-              bitField0_ |= 0x00000080;
-              sint64Type_ = input.readSInt64();
-              break;
-            }
-            case 77: {
-              bitField0_ |= 0x00000100;
-              fixed32Type_ = input.readFixed32();
-              break;
-            }
-            case 81: {
-              bitField0_ |= 0x00000200;
-              fixed64Type_ = input.readFixed64();
-              break;
-            }
-            case 93: {
-              bitField0_ |= 0x00000400;
-              sfixed32Type_ = input.readSFixed32();
-              break;
-            }
-            case 97: {
-              bitField0_ |= 0x00000800;
-              sfixed64Type_ = input.readSFixed64();
-              break;
-            }
-            case 104: {
-              bitField0_ |= 0x00001000;
-              boolType_ = input.readBool();
-              break;
-            }
-            case 114: {
-              bitField0_ |= 0x00002000;
-              stringType_ = input.readBytes();
-              break;
-            }
-            case 122: {
-              bitField0_ |= 0x00004000;
-              bytesType_ = input.readBytes();
-              break;
-            }
-            case 130: {
-              if (!((mutable_bitField0_ & 0x00008000) == 0x00008000)) {
-                mapType_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry>();
-                mutable_bitField0_ |= 0x00008000;
-              }
-              mapType_.add(input.readMessage(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.PARSER, extensionRegistry));
-              break;
-            }
-            case 138: {
-              if (!((mutable_bitField0_ & 0x00010000) == 0x00010000)) {
-                stringListType_ = new com.google.protobuf.LazyStringArrayList();
-                mutable_bitField0_ |= 0x00010000;
-              }
-              stringListType_.add(input.readBytes());
-              break;
-            }
-            case 146: {
-              org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder subBuilder = null;
-              if (((bitField0_ & 0x00008000) == 0x00008000)) {
-                subBuilder = messageType_.toBuilder();
-              }
-              messageType_ = input.readMessage(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(messageType_);
-                messageType_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00008000;
-              break;
-            }
-            case 154: {
-              if (!((mutable_bitField0_ & 0x00040000) == 0x00040000)) {
-                messageListType_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1>();
-                mutable_bitField0_ |= 0x00040000;
-              }
-              messageListType_.add(input.readMessage(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.PARSER, extensionRegistry));
-              break;
-            }
-            case 160: {
-              int rawValue = input.readEnum();
-              org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 value = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(20, rawValue);
-              } else {
-                bitField0_ |= 0x00010000;
-                enumType_ = value;
-              }
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00008000) == 0x00008000)) {
-          mapType_ = java.util.Collections.unmodifiableList(mapType_);
-        }
-        if (((mutable_bitField0_ & 0x00010000) == 0x00010000)) {
-          stringListType_ = new com.google.protobuf.UnmodifiableLazyStringList(stringListType_);
-        }
-        if (((mutable_bitField0_ & 0x00040000) == 0x00040000)) {
-          messageListType_ = java.util.Collections.unmodifiableList(messageListType_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
+    @SuppressWarnings({"unused"})
+    protected java.lang.Object newInstance(
+        UnusedPrivateParameter unused) {
+      return new AllTypes();
     }
+
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
       return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_AllTypes_descriptor;
     }
 
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+    @java.lang.Override
+    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
         internalGetFieldAccessorTable() {
       return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_AllTypes_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
               org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.class, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<AllTypes> PARSER =
-        new com.google.protobuf.AbstractParser<AllTypes>() {
-      public AllTypes parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new AllTypes(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<AllTypes> getParserForType() {
-      return PARSER;
-    }
-
     /**
      * Protobuf enum {@code AllTypes.Enum1}
      */
@@ -2143,11 +2271,11 @@ public enum Enum1
       /**
        * <code>VAL1 = 1;</code>
        */
-      VAL1(0, 1),
+      VAL1(1),
       /**
        * <code>VAL2 = 2;</code>
        */
-      VAL2(1, 2),
+      VAL2(2),
       ;
 
       /**
@@ -2160,9 +2288,25 @@ public enum Enum1
       public static final int VAL2_VALUE = 2;
 
 
-      public final int getNumber() { return value; }
+      public final int getNumber() {
+        return value;
+      }
 
+      /**
+       * @param value The numeric wire value of the corresponding enum entry.
+       * @return The enum associated with the given numeric wire value.
+       * @deprecated Use {@link #forNumber(int)} instead.
+       */
+      @java.lang.Deprecated
       public static Enum1 valueOf(int value) {
+        return forNumber(value);
+      }
+
+      /**
+       * @param value The numeric wire value of the corresponding enum entry.
+       * @return The enum associated with the given numeric wire value.
+       */
+      public static Enum1 forNumber(int value) {
         switch (value) {
           case 1: return VAL1;
           case 2: return VAL2;
@@ -2174,17 +2318,17 @@ public static Enum1 valueOf(int value) {
           internalGetValueMap() {
         return internalValueMap;
       }
-      private static com.google.protobuf.Internal.EnumLiteMap<Enum1>
-          internalValueMap =
+      private static final com.google.protobuf.Internal.EnumLiteMap<
+          Enum1> internalValueMap =
             new com.google.protobuf.Internal.EnumLiteMap<Enum1>() {
               public Enum1 findValueByNumber(int number) {
-                return Enum1.valueOf(number);
+                return Enum1.forNumber(number);
               }
             };
 
       public final com.google.protobuf.Descriptors.EnumValueDescriptor
           getValueDescriptor() {
-        return getDescriptor().getValues().get(index);
+        return getDescriptor().getValues().get(ordinal());
       }
       public final com.google.protobuf.Descriptors.EnumDescriptor
           getDescriptorForType() {
@@ -2206,11 +2350,9 @@ public static Enum1 valueOf(
         return VALUES[desc.getIndex()];
       }
 
-      private final int index;
       private final int value;
 
-      private Enum1(int index, int value) {
-        this.index = index;
+      private Enum1(int value) {
         this.value = value;
       }
 
@@ -2218,226 +2360,269 @@ private Enum1(int index, int value) {
     }
 
     private int bitField0_;
-    // optional double doubleType = 1;
     public static final int DOUBLETYPE_FIELD_NUMBER = 1;
-    private double doubleType_;
+    private double doubleType_ = 0D;
     /**
      * <code>optional double doubleType = 1;</code>
+     * @return Whether the doubleType field is set.
      */
+    @java.lang.Override
     public boolean hasDoubleType() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
+      return ((bitField0_ & 0x00000001) != 0);
     }
     /**
      * <code>optional double doubleType = 1;</code>
+     * @return The doubleType.
      */
+    @java.lang.Override
     public double getDoubleType() {
       return doubleType_;
     }
 
-    // optional float floatType = 2;
     public static final int FLOATTYPE_FIELD_NUMBER = 2;
-    private float floatType_;
+    private float floatType_ = 0F;
     /**
      * <code>optional float floatType = 2;</code>
+     * @return Whether the floatType field is set.
      */
+    @java.lang.Override
     public boolean hasFloatType() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
+      return ((bitField0_ & 0x00000002) != 0);
     }
     /**
      * <code>optional float floatType = 2;</code>
+     * @return The floatType.
      */
+    @java.lang.Override
     public float getFloatType() {
       return floatType_;
     }
 
-    // optional int32 int32Type = 3;
     public static final int INT32TYPE_FIELD_NUMBER = 3;
-    private int int32Type_;
+    private int int32Type_ = 0;
     /**
      * <code>optional int32 int32Type = 3;</code>
+     * @return Whether the int32Type field is set.
      */
+    @java.lang.Override
     public boolean hasInt32Type() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000004) != 0);
     }
     /**
      * <code>optional int32 int32Type = 3;</code>
+     * @return The int32Type.
      */
+    @java.lang.Override
     public int getInt32Type() {
       return int32Type_;
     }
 
-    // optional int64 int64Type = 4;
     public static final int INT64TYPE_FIELD_NUMBER = 4;
-    private long int64Type_;
+    private long int64Type_ = 0L;
     /**
      * <code>optional int64 int64Type = 4;</code>
+     * @return Whether the int64Type field is set.
      */
+    @java.lang.Override
     public boolean hasInt64Type() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
+      return ((bitField0_ & 0x00000008) != 0);
     }
     /**
      * <code>optional int64 int64Type = 4;</code>
+     * @return The int64Type.
      */
+    @java.lang.Override
     public long getInt64Type() {
       return int64Type_;
     }
 
-    // optional uint32 uint32Type = 5;
     public static final int UINT32TYPE_FIELD_NUMBER = 5;
-    private int uint32Type_;
+    private int uint32Type_ = 0;
     /**
      * <code>optional uint32 uint32Type = 5;</code>
+     * @return Whether the uint32Type field is set.
      */
+    @java.lang.Override
     public boolean hasUint32Type() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+      return ((bitField0_ & 0x00000010) != 0);
     }
     /**
      * <code>optional uint32 uint32Type = 5;</code>
+     * @return The uint32Type.
      */
+    @java.lang.Override
     public int getUint32Type() {
       return uint32Type_;
     }
 
-    // optional uint64 uint64Type = 6;
     public static final int UINT64TYPE_FIELD_NUMBER = 6;
-    private long uint64Type_;
+    private long uint64Type_ = 0L;
     /**
      * <code>optional uint64 uint64Type = 6;</code>
+     * @return Whether the uint64Type field is set.
      */
+    @java.lang.Override
     public boolean hasUint64Type() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+      return ((bitField0_ & 0x00000020) != 0);
     }
     /**
      * <code>optional uint64 uint64Type = 6;</code>
+     * @return The uint64Type.
      */
+    @java.lang.Override
     public long getUint64Type() {
       return uint64Type_;
     }
 
-    // optional sint32 sint32Type = 7;
     public static final int SINT32TYPE_FIELD_NUMBER = 7;
-    private int sint32Type_;
+    private int sint32Type_ = 0;
     /**
      * <code>optional sint32 sint32Type = 7;</code>
+     * @return Whether the sint32Type field is set.
      */
+    @java.lang.Override
     public boolean hasSint32Type() {
-      return ((bitField0_ & 0x00000040) == 0x00000040);
+      return ((bitField0_ & 0x00000040) != 0);
     }
     /**
      * <code>optional sint32 sint32Type = 7;</code>
+     * @return The sint32Type.
      */
+    @java.lang.Override
     public int getSint32Type() {
       return sint32Type_;
     }
 
-    // optional sint64 sint64Type = 8;
     public static final int SINT64TYPE_FIELD_NUMBER = 8;
-    private long sint64Type_;
+    private long sint64Type_ = 0L;
     /**
      * <code>optional sint64 sint64Type = 8;</code>
+     * @return Whether the sint64Type field is set.
      */
+    @java.lang.Override
     public boolean hasSint64Type() {
-      return ((bitField0_ & 0x00000080) == 0x00000080);
+      return ((bitField0_ & 0x00000080) != 0);
     }
     /**
      * <code>optional sint64 sint64Type = 8;</code>
+     * @return The sint64Type.
      */
+    @java.lang.Override
     public long getSint64Type() {
       return sint64Type_;
     }
 
-    // optional fixed32 fixed32Type = 9;
     public static final int FIXED32TYPE_FIELD_NUMBER = 9;
-    private int fixed32Type_;
+    private int fixed32Type_ = 0;
     /**
      * <code>optional fixed32 fixed32Type = 9;</code>
+     * @return Whether the fixed32Type field is set.
      */
+    @java.lang.Override
     public boolean hasFixed32Type() {
-      return ((bitField0_ & 0x00000100) == 0x00000100);
+      return ((bitField0_ & 0x00000100) != 0);
     }
     /**
      * <code>optional fixed32 fixed32Type = 9;</code>
+     * @return The fixed32Type.
      */
+    @java.lang.Override
     public int getFixed32Type() {
       return fixed32Type_;
     }
 
-    // optional fixed64 fixed64Type = 10;
     public static final int FIXED64TYPE_FIELD_NUMBER = 10;
-    private long fixed64Type_;
+    private long fixed64Type_ = 0L;
     /**
      * <code>optional fixed64 fixed64Type = 10;</code>
+     * @return Whether the fixed64Type field is set.
      */
+    @java.lang.Override
     public boolean hasFixed64Type() {
-      return ((bitField0_ & 0x00000200) == 0x00000200);
+      return ((bitField0_ & 0x00000200) != 0);
     }
     /**
      * <code>optional fixed64 fixed64Type = 10;</code>
+     * @return The fixed64Type.
      */
+    @java.lang.Override
     public long getFixed64Type() {
       return fixed64Type_;
     }
 
-    // optional sfixed32 sfixed32Type = 11;
     public static final int SFIXED32TYPE_FIELD_NUMBER = 11;
-    private int sfixed32Type_;
+    private int sfixed32Type_ = 0;
     /**
      * <code>optional sfixed32 sfixed32Type = 11;</code>
+     * @return Whether the sfixed32Type field is set.
      */
+    @java.lang.Override
     public boolean hasSfixed32Type() {
-      return ((bitField0_ & 0x00000400) == 0x00000400);
+      return ((bitField0_ & 0x00000400) != 0);
     }
     /**
      * <code>optional sfixed32 sfixed32Type = 11;</code>
+     * @return The sfixed32Type.
      */
+    @java.lang.Override
     public int getSfixed32Type() {
       return sfixed32Type_;
     }
 
-    // optional sfixed64 sfixed64Type = 12;
     public static final int SFIXED64TYPE_FIELD_NUMBER = 12;
-    private long sfixed64Type_;
+    private long sfixed64Type_ = 0L;
     /**
      * <code>optional sfixed64 sfixed64Type = 12;</code>
+     * @return Whether the sfixed64Type field is set.
      */
+    @java.lang.Override
     public boolean hasSfixed64Type() {
-      return ((bitField0_ & 0x00000800) == 0x00000800);
+      return ((bitField0_ & 0x00000800) != 0);
     }
     /**
      * <code>optional sfixed64 sfixed64Type = 12;</code>
+     * @return The sfixed64Type.
      */
+    @java.lang.Override
     public long getSfixed64Type() {
       return sfixed64Type_;
     }
 
-    // optional bool boolType = 13;
     public static final int BOOLTYPE_FIELD_NUMBER = 13;
-    private boolean boolType_;
+    private boolean boolType_ = false;
     /**
      * <code>optional bool boolType = 13;</code>
+     * @return Whether the boolType field is set.
      */
+    @java.lang.Override
     public boolean hasBoolType() {
-      return ((bitField0_ & 0x00001000) == 0x00001000);
+      return ((bitField0_ & 0x00001000) != 0);
     }
     /**
      * <code>optional bool boolType = 13;</code>
+     * @return The boolType.
      */
+    @java.lang.Override
     public boolean getBoolType() {
       return boolType_;
     }
 
-    // optional string stringType = 14;
     public static final int STRINGTYPE_FIELD_NUMBER = 14;
-    private java.lang.Object stringType_;
+    @SuppressWarnings("serial")
+    private volatile java.lang.Object stringType_ = "";
     /**
      * <code>optional string stringType = 14;</code>
+     * @return Whether the stringType field is set.
      */
+    @java.lang.Override
     public boolean hasStringType() {
-      return ((bitField0_ & 0x00002000) == 0x00002000);
+      return ((bitField0_ & 0x00002000) != 0);
     }
     /**
      * <code>optional string stringType = 14;</code>
+     * @return The stringType.
      */
+    @java.lang.Override
     public java.lang.String getStringType() {
       java.lang.Object ref = stringType_;
       if (ref instanceof java.lang.String) {
@@ -2454,7 +2639,9 @@ public java.lang.String getStringType() {
     }
     /**
      * <code>optional string stringType = 14;</code>
+     * @return The bytes for stringType.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString
         getStringTypeBytes() {
       java.lang.Object ref = stringType_;
@@ -2469,34 +2656,39 @@ public java.lang.String getStringType() {
       }
     }
 
-    // optional bytes bytesType = 15;
     public static final int BYTESTYPE_FIELD_NUMBER = 15;
-    private com.google.protobuf.ByteString bytesType_;
+    private com.google.protobuf.ByteString bytesType_ = com.google.protobuf.ByteString.EMPTY;
     /**
      * <code>optional bytes bytesType = 15;</code>
+     * @return Whether the bytesType field is set.
      */
+    @java.lang.Override
     public boolean hasBytesType() {
-      return ((bitField0_ & 0x00004000) == 0x00004000);
+      return ((bitField0_ & 0x00004000) != 0);
     }
     /**
      * <code>optional bytes bytesType = 15;</code>
+     * @return The bytesType.
      */
+    @java.lang.Override
     public com.google.protobuf.ByteString getBytesType() {
       return bytesType_;
     }
 
-    // repeated .MapFieldEntry mapType = 16;
     public static final int MAPTYPE_FIELD_NUMBER = 16;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry> mapType_;
     /**
      * <code>repeated .MapFieldEntry mapType = 16;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry> getMapTypeList() {
       return mapType_;
     }
     /**
      * <code>repeated .MapFieldEntry mapType = 16;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder> 
         getMapTypeOrBuilderList() {
       return mapType_;
@@ -2504,87 +2696,103 @@ public java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFiel
     /**
      * <code>repeated .MapFieldEntry mapType = 16;</code>
      */
+    @java.lang.Override
     public int getMapTypeCount() {
       return mapType_.size();
     }
     /**
      * <code>repeated .MapFieldEntry mapType = 16;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry getMapType(int index) {
       return mapType_.get(index);
     }
     /**
      * <code>repeated .MapFieldEntry mapType = 16;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder getMapTypeOrBuilder(
         int index) {
       return mapType_.get(index);
     }
 
-    // repeated string stringListType = 17;
     public static final int STRINGLISTTYPE_FIELD_NUMBER = 17;
-    private com.google.protobuf.LazyStringList stringListType_;
+    @SuppressWarnings("serial")
+    private com.google.protobuf.LazyStringArrayList stringListType_ =
+        com.google.protobuf.LazyStringArrayList.emptyList();
     /**
      * <code>repeated string stringListType = 17;</code>
+     * @return A list containing the stringListType.
      */
-    public java.util.List<java.lang.String>
+    public com.google.protobuf.ProtocolStringList
         getStringListTypeList() {
       return stringListType_;
     }
     /**
      * <code>repeated string stringListType = 17;</code>
+     * @return The count of stringListType.
      */
     public int getStringListTypeCount() {
       return stringListType_.size();
     }
     /**
      * <code>repeated string stringListType = 17;</code>
+     * @param index The index of the element to return.
+     * @return The stringListType at the given index.
      */
     public java.lang.String getStringListType(int index) {
       return stringListType_.get(index);
     }
     /**
      * <code>repeated string stringListType = 17;</code>
+     * @param index The index of the value to return.
+     * @return The bytes of the stringListType at the given index.
      */
     public com.google.protobuf.ByteString
         getStringListTypeBytes(int index) {
       return stringListType_.getByteString(index);
     }
 
-    // optional .Mesg1 messageType = 18;
     public static final int MESSAGETYPE_FIELD_NUMBER = 18;
     private org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 messageType_;
     /**
      * <code>optional .Mesg1 messageType = 18;</code>
+     * @return Whether the messageType field is set.
      */
+    @java.lang.Override
     public boolean hasMessageType() {
-      return ((bitField0_ & 0x00008000) == 0x00008000);
+      return ((bitField0_ & 0x00008000) != 0);
     }
     /**
      * <code>optional .Mesg1 messageType = 18;</code>
+     * @return The messageType.
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 getMessageType() {
-      return messageType_;
+      return messageType_ == null ? org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance() : messageType_;
     }
     /**
      * <code>optional .Mesg1 messageType = 18;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder getMessageTypeOrBuilder() {
-      return messageType_;
+      return messageType_ == null ? org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance() : messageType_;
     }
 
-    // repeated .Mesg1 messageListType = 19;
     public static final int MESSAGELISTTYPE_FIELD_NUMBER = 19;
+    @SuppressWarnings("serial")
     private java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1> messageListType_;
     /**
      * <code>repeated .Mesg1 messageListType = 19;</code>
      */
+    @java.lang.Override
     public java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1> getMessageListTypeList() {
       return messageListType_;
     }
     /**
      * <code>repeated .Mesg1 messageListType = 19;</code>
      */
+    @java.lang.Override
     public java.util.List<? extends org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder> 
         getMessageListTypeOrBuilderList() {
       return messageListType_;
@@ -2592,199 +2800,183 @@ public java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1>
     /**
      * <code>repeated .Mesg1 messageListType = 19;</code>
      */
+    @java.lang.Override
     public int getMessageListTypeCount() {
       return messageListType_.size();
     }
     /**
      * <code>repeated .Mesg1 messageListType = 19;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 getMessageListType(int index) {
       return messageListType_.get(index);
     }
     /**
      * <code>repeated .Mesg1 messageListType = 19;</code>
      */
+    @java.lang.Override
     public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder getMessageListTypeOrBuilder(
         int index) {
       return messageListType_.get(index);
     }
 
-    // optional .AllTypes.Enum1 enumType = 20;
     public static final int ENUMTYPE_FIELD_NUMBER = 20;
-    private org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 enumType_;
+    private int enumType_ = 1;
     /**
      * <code>optional .AllTypes.Enum1 enumType = 20;</code>
+     * @return Whether the enumType field is set.
      */
-    public boolean hasEnumType() {
-      return ((bitField0_ & 0x00010000) == 0x00010000);
+    @java.lang.Override public boolean hasEnumType() {
+      return ((bitField0_ & 0x00010000) != 0);
     }
     /**
      * <code>optional .AllTypes.Enum1 enumType = 20;</code>
+     * @return The enumType.
      */
-    public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 getEnumType() {
-      return enumType_;
-    }
-
-    private void initFields() {
-      doubleType_ = 0D;
-      floatType_ = 0F;
-      int32Type_ = 0;
-      int64Type_ = 0L;
-      uint32Type_ = 0;
-      uint64Type_ = 0L;
-      sint32Type_ = 0;
-      sint64Type_ = 0L;
-      fixed32Type_ = 0;
-      fixed64Type_ = 0L;
-      sfixed32Type_ = 0;
-      sfixed64Type_ = 0L;
-      boolType_ = false;
-      stringType_ = "";
-      bytesType_ = com.google.protobuf.ByteString.EMPTY;
-      mapType_ = java.util.Collections.emptyList();
-      stringListType_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-      messageType_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance();
-      messageListType_ = java.util.Collections.emptyList();
-      enumType_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.VAL1;
+    @java.lang.Override public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 getEnumType() {
+      org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 result = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.forNumber(enumType_);
+      return result == null ? org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.VAL1 : result;
     }
+
     private byte memoizedIsInitialized = -1;
+    @java.lang.Override
     public final boolean isInitialized() {
       byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
+      if (isInitialized == 1) return true;
+      if (isInitialized == 0) return false;
 
       memoizedIsInitialized = 1;
       return true;
     }
 
+    @java.lang.Override
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         output.writeDouble(1, doubleType_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         output.writeFloat(2, floatType_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         output.writeInt32(3, int32Type_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+      if (((bitField0_ & 0x00000008) != 0)) {
         output.writeInt64(4, int64Type_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+      if (((bitField0_ & 0x00000010) != 0)) {
         output.writeUInt32(5, uint32Type_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
+      if (((bitField0_ & 0x00000020) != 0)) {
         output.writeUInt64(6, uint64Type_);
       }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
+      if (((bitField0_ & 0x00000040) != 0)) {
         output.writeSInt32(7, sint32Type_);
       }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
+      if (((bitField0_ & 0x00000080) != 0)) {
         output.writeSInt64(8, sint64Type_);
       }
-      if (((bitField0_ & 0x00000100) == 0x00000100)) {
+      if (((bitField0_ & 0x00000100) != 0)) {
         output.writeFixed32(9, fixed32Type_);
       }
-      if (((bitField0_ & 0x00000200) == 0x00000200)) {
+      if (((bitField0_ & 0x00000200) != 0)) {
         output.writeFixed64(10, fixed64Type_);
       }
-      if (((bitField0_ & 0x00000400) == 0x00000400)) {
+      if (((bitField0_ & 0x00000400) != 0)) {
         output.writeSFixed32(11, sfixed32Type_);
       }
-      if (((bitField0_ & 0x00000800) == 0x00000800)) {
+      if (((bitField0_ & 0x00000800) != 0)) {
         output.writeSFixed64(12, sfixed64Type_);
       }
-      if (((bitField0_ & 0x00001000) == 0x00001000)) {
+      if (((bitField0_ & 0x00001000) != 0)) {
         output.writeBool(13, boolType_);
       }
-      if (((bitField0_ & 0x00002000) == 0x00002000)) {
-        output.writeBytes(14, getStringTypeBytes());
+      if (((bitField0_ & 0x00002000) != 0)) {
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 14, stringType_);
       }
-      if (((bitField0_ & 0x00004000) == 0x00004000)) {
+      if (((bitField0_ & 0x00004000) != 0)) {
         output.writeBytes(15, bytesType_);
       }
       for (int i = 0; i < mapType_.size(); i++) {
         output.writeMessage(16, mapType_.get(i));
       }
       for (int i = 0; i < stringListType_.size(); i++) {
-        output.writeBytes(17, stringListType_.getByteString(i));
+        com.google.protobuf.GeneratedMessageV3.writeString(output, 17, stringListType_.getRaw(i));
       }
-      if (((bitField0_ & 0x00008000) == 0x00008000)) {
-        output.writeMessage(18, messageType_);
+      if (((bitField0_ & 0x00008000) != 0)) {
+        output.writeMessage(18, getMessageType());
       }
       for (int i = 0; i < messageListType_.size(); i++) {
         output.writeMessage(19, messageListType_.get(i));
       }
-      if (((bitField0_ & 0x00010000) == 0x00010000)) {
-        output.writeEnum(20, enumType_.getNumber());
+      if (((bitField0_ & 0x00010000) != 0)) {
+        output.writeEnum(20, enumType_);
       }
       getUnknownFields().writeTo(output);
     }
 
-    private int memoizedSerializedSize = -1;
+    @java.lang.Override
     public int getSerializedSize() {
-      int size = memoizedSerializedSize;
+      int size = memoizedSize;
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+      if (((bitField0_ & 0x00000001) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeDoubleSize(1, doubleType_);
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+      if (((bitField0_ & 0x00000002) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeFloatSize(2, floatType_);
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+      if (((bitField0_ & 0x00000004) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(3, int32Type_);
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+      if (((bitField0_ & 0x00000008) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeInt64Size(4, int64Type_);
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+      if (((bitField0_ & 0x00000010) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeUInt32Size(5, uint32Type_);
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
+      if (((bitField0_ & 0x00000020) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeUInt64Size(6, uint64Type_);
       }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
+      if (((bitField0_ & 0x00000040) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeSInt32Size(7, sint32Type_);
       }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
+      if (((bitField0_ & 0x00000080) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeSInt64Size(8, sint64Type_);
       }
-      if (((bitField0_ & 0x00000100) == 0x00000100)) {
+      if (((bitField0_ & 0x00000100) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeFixed32Size(9, fixed32Type_);
       }
-      if (((bitField0_ & 0x00000200) == 0x00000200)) {
+      if (((bitField0_ & 0x00000200) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeFixed64Size(10, fixed64Type_);
       }
-      if (((bitField0_ & 0x00000400) == 0x00000400)) {
+      if (((bitField0_ & 0x00000400) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeSFixed32Size(11, sfixed32Type_);
       }
-      if (((bitField0_ & 0x00000800) == 0x00000800)) {
+      if (((bitField0_ & 0x00000800) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeSFixed64Size(12, sfixed64Type_);
       }
-      if (((bitField0_ & 0x00001000) == 0x00001000)) {
+      if (((bitField0_ & 0x00001000) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBoolSize(13, boolType_);
       }
-      if (((bitField0_ & 0x00002000) == 0x00002000)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(14, getStringTypeBytes());
+      if (((bitField0_ & 0x00002000) != 0)) {
+        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(14, stringType_);
       }
-      if (((bitField0_ & 0x00004000) == 0x00004000)) {
+      if (((bitField0_ & 0x00004000) != 0)) {
         size += com.google.protobuf.CodedOutputStream
           .computeBytesSize(15, bytesType_);
       }
@@ -2795,36 +2987,245 @@ public int getSerializedSize() {
       {
         int dataSize = 0;
         for (int i = 0; i < stringListType_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(stringListType_.getByteString(i));
+          dataSize += computeStringSizeNoTag(stringListType_.getRaw(i));
         }
         size += dataSize;
         size += 2 * getStringListTypeList().size();
       }
-      if (((bitField0_ & 0x00008000) == 0x00008000)) {
+      if (((bitField0_ & 0x00008000) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(18, messageType_);
+          .computeMessageSize(18, getMessageType());
       }
       for (int i = 0; i < messageListType_.size(); i++) {
         size += com.google.protobuf.CodedOutputStream
           .computeMessageSize(19, messageListType_.get(i));
       }
-      if (((bitField0_ & 0x00010000) == 0x00010000)) {
+      if (((bitField0_ & 0x00010000) != 0)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeEnumSize(20, enumType_.getNumber());
+          .computeEnumSize(20, enumType_);
       }
       size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
+      memoizedSize = size;
       return size;
     }
 
-    private static final long serialVersionUID = 0L;
     @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes other = (org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes) obj;
+
+      if (hasDoubleType() != other.hasDoubleType()) return false;
+      if (hasDoubleType()) {
+        if (java.lang.Double.doubleToLongBits(getDoubleType())
+            != java.lang.Double.doubleToLongBits(
+                other.getDoubleType())) return false;
+      }
+      if (hasFloatType() != other.hasFloatType()) return false;
+      if (hasFloatType()) {
+        if (java.lang.Float.floatToIntBits(getFloatType())
+            != java.lang.Float.floatToIntBits(
+                other.getFloatType())) return false;
+      }
+      if (hasInt32Type() != other.hasInt32Type()) return false;
+      if (hasInt32Type()) {
+        if (getInt32Type()
+            != other.getInt32Type()) return false;
+      }
+      if (hasInt64Type() != other.hasInt64Type()) return false;
+      if (hasInt64Type()) {
+        if (getInt64Type()
+            != other.getInt64Type()) return false;
+      }
+      if (hasUint32Type() != other.hasUint32Type()) return false;
+      if (hasUint32Type()) {
+        if (getUint32Type()
+            != other.getUint32Type()) return false;
+      }
+      if (hasUint64Type() != other.hasUint64Type()) return false;
+      if (hasUint64Type()) {
+        if (getUint64Type()
+            != other.getUint64Type()) return false;
+      }
+      if (hasSint32Type() != other.hasSint32Type()) return false;
+      if (hasSint32Type()) {
+        if (getSint32Type()
+            != other.getSint32Type()) return false;
+      }
+      if (hasSint64Type() != other.hasSint64Type()) return false;
+      if (hasSint64Type()) {
+        if (getSint64Type()
+            != other.getSint64Type()) return false;
+      }
+      if (hasFixed32Type() != other.hasFixed32Type()) return false;
+      if (hasFixed32Type()) {
+        if (getFixed32Type()
+            != other.getFixed32Type()) return false;
+      }
+      if (hasFixed64Type() != other.hasFixed64Type()) return false;
+      if (hasFixed64Type()) {
+        if (getFixed64Type()
+            != other.getFixed64Type()) return false;
+      }
+      if (hasSfixed32Type() != other.hasSfixed32Type()) return false;
+      if (hasSfixed32Type()) {
+        if (getSfixed32Type()
+            != other.getSfixed32Type()) return false;
+      }
+      if (hasSfixed64Type() != other.hasSfixed64Type()) return false;
+      if (hasSfixed64Type()) {
+        if (getSfixed64Type()
+            != other.getSfixed64Type()) return false;
+      }
+      if (hasBoolType() != other.hasBoolType()) return false;
+      if (hasBoolType()) {
+        if (getBoolType()
+            != other.getBoolType()) return false;
+      }
+      if (hasStringType() != other.hasStringType()) return false;
+      if (hasStringType()) {
+        if (!getStringType()
+            .equals(other.getStringType())) return false;
+      }
+      if (hasBytesType() != other.hasBytesType()) return false;
+      if (hasBytesType()) {
+        if (!getBytesType()
+            .equals(other.getBytesType())) return false;
+      }
+      if (!getMapTypeList()
+          .equals(other.getMapTypeList())) return false;
+      if (!getStringListTypeList()
+          .equals(other.getStringListTypeList())) return false;
+      if (hasMessageType() != other.hasMessageType()) return false;
+      if (hasMessageType()) {
+        if (!getMessageType()
+            .equals(other.getMessageType())) return false;
+      }
+      if (!getMessageListTypeList()
+          .equals(other.getMessageListTypeList())) return false;
+      if (hasEnumType() != other.hasEnumType()) return false;
+      if (hasEnumType()) {
+        if (enumType_ != other.enumType_) return false;
+      }
+      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
+      return true;
+    }
+
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptor().hashCode();
+      if (hasDoubleType()) {
+        hash = (37 * hash) + DOUBLETYPE_FIELD_NUMBER;
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            java.lang.Double.doubleToLongBits(getDoubleType()));
+      }
+      if (hasFloatType()) {
+        hash = (37 * hash) + FLOATTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + java.lang.Float.floatToIntBits(
+            getFloatType());
+      }
+      if (hasInt32Type()) {
+        hash = (37 * hash) + INT32TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getInt32Type();
+      }
+      if (hasInt64Type()) {
+        hash = (37 * hash) + INT64TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getInt64Type());
+      }
+      if (hasUint32Type()) {
+        hash = (37 * hash) + UINT32TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getUint32Type();
+      }
+      if (hasUint64Type()) {
+        hash = (37 * hash) + UINT64TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getUint64Type());
+      }
+      if (hasSint32Type()) {
+        hash = (37 * hash) + SINT32TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getSint32Type();
+      }
+      if (hasSint64Type()) {
+        hash = (37 * hash) + SINT64TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getSint64Type());
+      }
+      if (hasFixed32Type()) {
+        hash = (37 * hash) + FIXED32TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getFixed32Type();
+      }
+      if (hasFixed64Type()) {
+        hash = (37 * hash) + FIXED64TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getFixed64Type());
+      }
+      if (hasSfixed32Type()) {
+        hash = (37 * hash) + SFIXED32TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getSfixed32Type();
+      }
+      if (hasSfixed64Type()) {
+        hash = (37 * hash) + SFIXED64TYPE_FIELD_NUMBER;
+        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
+            getSfixed64Type());
+      }
+      if (hasBoolType()) {
+        hash = (37 * hash) + BOOLTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
+            getBoolType());
+      }
+      if (hasStringType()) {
+        hash = (37 * hash) + STRINGTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getStringType().hashCode();
+      }
+      if (hasBytesType()) {
+        hash = (37 * hash) + BYTESTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getBytesType().hashCode();
+      }
+      if (getMapTypeCount() > 0) {
+        hash = (37 * hash) + MAPTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getMapTypeList().hashCode();
+      }
+      if (getStringListTypeCount() > 0) {
+        hash = (37 * hash) + STRINGLISTTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getStringListTypeList().hashCode();
+      }
+      if (hasMessageType()) {
+        hash = (37 * hash) + MESSAGETYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getMessageType().hashCode();
+      }
+      if (getMessageListTypeCount() > 0) {
+        hash = (37 * hash) + MESSAGELISTTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getMessageListTypeList().hashCode();
+      }
+      if (hasEnumType()) {
+        hash = (37 * hash) + ENUMTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + enumType_;
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      memoizedHashCode = hash;
+      return hash;
     }
 
+    public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseFrom(
+        java.nio.ByteBuffer data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseFrom(
+        java.nio.ByteBuffer data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseFrom(
         com.google.protobuf.ByteString data)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -2848,46 +3249,61 @@ public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseF
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
+
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseDelimitedFrom(java.io.InputStream input)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input);
     }
+
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseDelimitedFrom(
         java.io.InputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseFrom(
         com.google.protobuf.CodedInputStream input)
         throws java.io.IOException {
-      return PARSER.parseFrom(input);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input);
     }
     public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parseFrom(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+      return com.google.protobuf.GeneratedMessageV3
+          .parseWithIOException(PARSER, input, extensionRegistry);
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
+    @java.lang.Override
     public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder() {
+      return DEFAULT_INSTANCE.toBuilder();
+    }
     public static Builder newBuilder(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes prototype) {
-      return newBuilder().mergeFrom(prototype);
+      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
+    }
+    @java.lang.Override
+    public Builder toBuilder() {
+      return this == DEFAULT_INSTANCE
+          ? new Builder() : new Builder().mergeFrom(this);
     }
-    public Builder toBuilder() { return newBuilder(this); }
 
     @java.lang.Override
     protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
       Builder builder = new Builder(parent);
       return builder;
     }
@@ -2895,14 +3311,16 @@ protected Builder newBuilderForType(
      * Protobuf type {@code AllTypes}
      */
     public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypesOrBuilder {
+        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
+        // @@protoc_insertion_point(builder_implements:AllTypes)
+        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypesOrBuilder {
       public static final com.google.protobuf.Descriptors.Descriptor
           getDescriptor() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_AllTypes_descriptor;
       }
 
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      @java.lang.Override
+      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
           internalGetFieldAccessorTable() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_AllTypes_fieldAccessorTable
             .ensureFieldAccessorsInitialized(
@@ -2915,91 +3333,74 @@ private Builder() {
       }
 
       private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
         super(parent);
         maybeForceBuilderInitialization();
       }
       private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        if (com.google.protobuf.GeneratedMessageV3
+                .alwaysUseFieldBuilders) {
           getMapTypeFieldBuilder();
           getMessageTypeFieldBuilder();
           getMessageListTypeFieldBuilder();
         }
       }
-      private static Builder create() {
-        return new Builder();
-      }
-
+      @java.lang.Override
       public Builder clear() {
         super.clear();
+        bitField0_ = 0;
         doubleType_ = 0D;
-        bitField0_ = (bitField0_ & ~0x00000001);
         floatType_ = 0F;
-        bitField0_ = (bitField0_ & ~0x00000002);
         int32Type_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000004);
         int64Type_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000008);
         uint32Type_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000010);
         uint64Type_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000020);
         sint32Type_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000040);
         sint64Type_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000080);
         fixed32Type_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000100);
         fixed64Type_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000200);
         sfixed32Type_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000400);
         sfixed64Type_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000800);
         boolType_ = false;
-        bitField0_ = (bitField0_ & ~0x00001000);
         stringType_ = "";
-        bitField0_ = (bitField0_ & ~0x00002000);
         bytesType_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00004000);
         if (mapTypeBuilder_ == null) {
           mapType_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00008000);
         } else {
+          mapType_ = null;
           mapTypeBuilder_.clear();
         }
-        stringListType_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00010000);
-        if (messageTypeBuilder_ == null) {
-          messageType_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance();
-        } else {
-          messageTypeBuilder_.clear();
+        bitField0_ = (bitField0_ & ~0x00008000);
+        stringListType_ =
+            com.google.protobuf.LazyStringArrayList.emptyList();
+        messageType_ = null;
+        if (messageTypeBuilder_ != null) {
+          messageTypeBuilder_.dispose();
+          messageTypeBuilder_ = null;
         }
-        bitField0_ = (bitField0_ & ~0x00020000);
         if (messageListTypeBuilder_ == null) {
           messageListType_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00040000);
         } else {
+          messageListType_ = null;
           messageListTypeBuilder_.clear();
         }
-        enumType_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.VAL1;
-        bitField0_ = (bitField0_ & ~0x00080000);
+        bitField0_ = (bitField0_ & ~0x00040000);
+        enumType_ = 1;
         return this;
       }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
+      @java.lang.Override
       public com.google.protobuf.Descriptors.Descriptor
           getDescriptorForType() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.internal_static_AllTypes_descriptor;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes getDefaultInstanceForType() {
         return org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.getDefaultInstance();
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes build() {
         org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes result = buildPartial();
         if (!result.isInitialized()) {
@@ -3008,111 +3409,149 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes build() {
         return result;
       }
 
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes buildPartial() {
         org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes result = new org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes(this);
+        buildPartialRepeatedFields(result);
+        if (bitField0_ != 0) { buildPartial0(result); }
+        onBuilt();
+        return result;
+      }
+
+      private void buildPartialRepeatedFields(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes result) {
+        if (mapTypeBuilder_ == null) {
+          if (((bitField0_ & 0x00008000) != 0)) {
+            mapType_ = java.util.Collections.unmodifiableList(mapType_);
+            bitField0_ = (bitField0_ & ~0x00008000);
+          }
+          result.mapType_ = mapType_;
+        } else {
+          result.mapType_ = mapTypeBuilder_.build();
+        }
+        if (messageListTypeBuilder_ == null) {
+          if (((bitField0_ & 0x00040000) != 0)) {
+            messageListType_ = java.util.Collections.unmodifiableList(messageListType_);
+            bitField0_ = (bitField0_ & ~0x00040000);
+          }
+          result.messageListType_ = messageListType_;
+        } else {
+          result.messageListType_ = messageListTypeBuilder_.build();
+        }
+      }
+
+      private void buildPartial0(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes result) {
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((from_bitField0_ & 0x00000001) != 0)) {
+          result.doubleType_ = doubleType_;
           to_bitField0_ |= 0x00000001;
         }
-        result.doubleType_ = doubleType_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000002) != 0)) {
+          result.floatType_ = floatType_;
           to_bitField0_ |= 0x00000002;
         }
-        result.floatType_ = floatType_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((from_bitField0_ & 0x00000004) != 0)) {
+          result.int32Type_ = int32Type_;
           to_bitField0_ |= 0x00000004;
         }
-        result.int32Type_ = int32Type_;
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
+        if (((from_bitField0_ & 0x00000008) != 0)) {
+          result.int64Type_ = int64Type_;
           to_bitField0_ |= 0x00000008;
         }
-        result.int64Type_ = int64Type_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
+        if (((from_bitField0_ & 0x00000010) != 0)) {
+          result.uint32Type_ = uint32Type_;
           to_bitField0_ |= 0x00000010;
         }
-        result.uint32Type_ = uint32Type_;
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
+        if (((from_bitField0_ & 0x00000020) != 0)) {
+          result.uint64Type_ = uint64Type_;
           to_bitField0_ |= 0x00000020;
         }
-        result.uint64Type_ = uint64Type_;
-        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
+        if (((from_bitField0_ & 0x00000040) != 0)) {
+          result.sint32Type_ = sint32Type_;
           to_bitField0_ |= 0x00000040;
         }
-        result.sint32Type_ = sint32Type_;
-        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
+        if (((from_bitField0_ & 0x00000080) != 0)) {
+          result.sint64Type_ = sint64Type_;
           to_bitField0_ |= 0x00000080;
         }
-        result.sint64Type_ = sint64Type_;
-        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
+        if (((from_bitField0_ & 0x00000100) != 0)) {
+          result.fixed32Type_ = fixed32Type_;
           to_bitField0_ |= 0x00000100;
         }
-        result.fixed32Type_ = fixed32Type_;
-        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
+        if (((from_bitField0_ & 0x00000200) != 0)) {
+          result.fixed64Type_ = fixed64Type_;
           to_bitField0_ |= 0x00000200;
         }
-        result.fixed64Type_ = fixed64Type_;
-        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
+        if (((from_bitField0_ & 0x00000400) != 0)) {
+          result.sfixed32Type_ = sfixed32Type_;
           to_bitField0_ |= 0x00000400;
         }
-        result.sfixed32Type_ = sfixed32Type_;
-        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
+        if (((from_bitField0_ & 0x00000800) != 0)) {
+          result.sfixed64Type_ = sfixed64Type_;
           to_bitField0_ |= 0x00000800;
         }
-        result.sfixed64Type_ = sfixed64Type_;
-        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
+        if (((from_bitField0_ & 0x00001000) != 0)) {
+          result.boolType_ = boolType_;
           to_bitField0_ |= 0x00001000;
         }
-        result.boolType_ = boolType_;
-        if (((from_bitField0_ & 0x00002000) == 0x00002000)) {
+        if (((from_bitField0_ & 0x00002000) != 0)) {
+          result.stringType_ = stringType_;
           to_bitField0_ |= 0x00002000;
         }
-        result.stringType_ = stringType_;
-        if (((from_bitField0_ & 0x00004000) == 0x00004000)) {
+        if (((from_bitField0_ & 0x00004000) != 0)) {
+          result.bytesType_ = bytesType_;
           to_bitField0_ |= 0x00004000;
         }
-        result.bytesType_ = bytesType_;
-        if (mapTypeBuilder_ == null) {
-          if (((bitField0_ & 0x00008000) == 0x00008000)) {
-            mapType_ = java.util.Collections.unmodifiableList(mapType_);
-            bitField0_ = (bitField0_ & ~0x00008000);
-          }
-          result.mapType_ = mapType_;
-        } else {
-          result.mapType_ = mapTypeBuilder_.build();
-        }
-        if (((bitField0_ & 0x00010000) == 0x00010000)) {
-          stringListType_ = new com.google.protobuf.UnmodifiableLazyStringList(
-              stringListType_);
-          bitField0_ = (bitField0_ & ~0x00010000);
+        if (((from_bitField0_ & 0x00010000) != 0)) {
+          stringListType_.makeImmutable();
+          result.stringListType_ = stringListType_;
         }
-        result.stringListType_ = stringListType_;
-        if (((from_bitField0_ & 0x00020000) == 0x00020000)) {
+        if (((from_bitField0_ & 0x00020000) != 0)) {
+          result.messageType_ = messageTypeBuilder_ == null
+              ? messageType_
+              : messageTypeBuilder_.build();
           to_bitField0_ |= 0x00008000;
         }
-        if (messageTypeBuilder_ == null) {
-          result.messageType_ = messageType_;
-        } else {
-          result.messageType_ = messageTypeBuilder_.build();
-        }
-        if (messageListTypeBuilder_ == null) {
-          if (((bitField0_ & 0x00040000) == 0x00040000)) {
-            messageListType_ = java.util.Collections.unmodifiableList(messageListType_);
-            bitField0_ = (bitField0_ & ~0x00040000);
-          }
-          result.messageListType_ = messageListType_;
-        } else {
-          result.messageListType_ = messageListTypeBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00080000) == 0x00080000)) {
+        if (((from_bitField0_ & 0x00080000) != 0)) {
+          result.enumType_ = enumType_;
           to_bitField0_ |= 0x00010000;
         }
-        result.enumType_ = enumType_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        result.bitField0_ |= to_bitField0_;
       }
 
+      @java.lang.Override
+      public Builder clone() {
+        return super.clone();
+      }
+      @java.lang.Override
+      public Builder setField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.setField(field, value);
+      }
+      @java.lang.Override
+      public Builder clearField(
+          com.google.protobuf.Descriptors.FieldDescriptor field) {
+        return super.clearField(field);
+      }
+      @java.lang.Override
+      public Builder clearOneof(
+          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
+        return super.clearOneof(oneof);
+      }
+      @java.lang.Override
+      public Builder setRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          int index, java.lang.Object value) {
+        return super.setRepeatedField(field, index, value);
+      }
+      @java.lang.Override
+      public Builder addRepeatedField(
+          com.google.protobuf.Descriptors.FieldDescriptor field,
+          java.lang.Object value) {
+        return super.addRepeatedField(field, value);
+      }
+      @java.lang.Override
       public Builder mergeFrom(com.google.protobuf.Message other) {
         if (other instanceof org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes) {
           return mergeFrom((org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes)other);
@@ -3164,8 +3603,8 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllT
           setBoolType(other.getBoolType());
         }
         if (other.hasStringType()) {
-          bitField0_ |= 0x00002000;
           stringType_ = other.stringType_;
+          bitField0_ |= 0x00002000;
           onChanged();
         }
         if (other.hasBytesType()) {
@@ -3190,7 +3629,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllT
               mapType_ = other.mapType_;
               bitField0_ = (bitField0_ & ~0x00008000);
               mapTypeBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getMapTypeFieldBuilder() : null;
             } else {
               mapTypeBuilder_.addAllMessages(other.mapType_);
@@ -3200,7 +3639,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllT
         if (!other.stringListType_.isEmpty()) {
           if (stringListType_.isEmpty()) {
             stringListType_ = other.stringListType_;
-            bitField0_ = (bitField0_ & ~0x00010000);
+            bitField0_ |= 0x00010000;
           } else {
             ensureStringListTypeIsMutable();
             stringListType_.addAll(other.stringListType_);
@@ -3229,7 +3668,7 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllT
               messageListType_ = other.messageListType_;
               bitField0_ = (bitField0_ & ~0x00040000);
               messageListTypeBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                    getMessageListTypeFieldBuilder() : null;
             } else {
               messageListTypeBuilder_.addAllMessages(other.messageListType_);
@@ -3240,57 +3679,206 @@ public Builder mergeFrom(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllT
           setEnumType(other.getEnumType());
         }
         this.mergeUnknownFields(other.getUnknownFields());
+        onChanged();
         return this;
       }
 
+      @java.lang.Override
       public final boolean isInitialized() {
         return true;
       }
 
+      @java.lang.Override
       public Builder mergeFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws java.io.IOException {
-        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes parsedMessage = null;
+        if (extensionRegistry == null) {
+          throw new java.lang.NullPointerException();
+        }
         try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+          boolean done = false;
+          while (!done) {
+            int tag = input.readTag();
+            switch (tag) {
+              case 0:
+                done = true;
+                break;
+              case 9: {
+                doubleType_ = input.readDouble();
+                bitField0_ |= 0x00000001;
+                break;
+              } // case 9
+              case 21: {
+                floatType_ = input.readFloat();
+                bitField0_ |= 0x00000002;
+                break;
+              } // case 21
+              case 24: {
+                int32Type_ = input.readInt32();
+                bitField0_ |= 0x00000004;
+                break;
+              } // case 24
+              case 32: {
+                int64Type_ = input.readInt64();
+                bitField0_ |= 0x00000008;
+                break;
+              } // case 32
+              case 40: {
+                uint32Type_ = input.readUInt32();
+                bitField0_ |= 0x00000010;
+                break;
+              } // case 40
+              case 48: {
+                uint64Type_ = input.readUInt64();
+                bitField0_ |= 0x00000020;
+                break;
+              } // case 48
+              case 56: {
+                sint32Type_ = input.readSInt32();
+                bitField0_ |= 0x00000040;
+                break;
+              } // case 56
+              case 64: {
+                sint64Type_ = input.readSInt64();
+                bitField0_ |= 0x00000080;
+                break;
+              } // case 64
+              case 77: {
+                fixed32Type_ = input.readFixed32();
+                bitField0_ |= 0x00000100;
+                break;
+              } // case 77
+              case 81: {
+                fixed64Type_ = input.readFixed64();
+                bitField0_ |= 0x00000200;
+                break;
+              } // case 81
+              case 93: {
+                sfixed32Type_ = input.readSFixed32();
+                bitField0_ |= 0x00000400;
+                break;
+              } // case 93
+              case 97: {
+                sfixed64Type_ = input.readSFixed64();
+                bitField0_ |= 0x00000800;
+                break;
+              } // case 97
+              case 104: {
+                boolType_ = input.readBool();
+                bitField0_ |= 0x00001000;
+                break;
+              } // case 104
+              case 114: {
+                stringType_ = input.readBytes();
+                bitField0_ |= 0x00002000;
+                break;
+              } // case 114
+              case 122: {
+                bytesType_ = input.readBytes();
+                bitField0_ |= 0x00004000;
+                break;
+              } // case 122
+              case 130: {
+                org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.PARSER,
+                        extensionRegistry);
+                if (mapTypeBuilder_ == null) {
+                  ensureMapTypeIsMutable();
+                  mapType_.add(m);
+                } else {
+                  mapTypeBuilder_.addMessage(m);
+                }
+                break;
+              } // case 130
+              case 138: {
+                com.google.protobuf.ByteString bs = input.readBytes();
+                ensureStringListTypeIsMutable();
+                stringListType_.add(bs);
+                break;
+              } // case 138
+              case 146: {
+                input.readMessage(
+                    getMessageTypeFieldBuilder().getBuilder(),
+                    extensionRegistry);
+                bitField0_ |= 0x00020000;
+                break;
+              } // case 146
+              case 154: {
+                org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 m =
+                    input.readMessage(
+                        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.PARSER,
+                        extensionRegistry);
+                if (messageListTypeBuilder_ == null) {
+                  ensureMessageListTypeIsMutable();
+                  messageListType_.add(m);
+                } else {
+                  messageListTypeBuilder_.addMessage(m);
+                }
+                break;
+              } // case 154
+              case 160: {
+                int tmpRaw = input.readEnum();
+                org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 tmpValue =
+                    org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.forNumber(tmpRaw);
+                if (tmpValue == null) {
+                  mergeUnknownVarintField(20, tmpRaw);
+                } else {
+                  enumType_ = tmpRaw;
+                  bitField0_ |= 0x00080000;
+                }
+                break;
+              } // case 160
+              default: {
+                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
+                  done = true; // was an endgroup tag
+                }
+                break;
+              } // default:
+            } // switch (tag)
+          } // while (!done)
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes) e.getUnfinishedMessage();
-          throw e;
+          throw e.unwrapIOException();
         } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
+          onChanged();
+        } // finally
         return this;
       }
       private int bitField0_;
 
-      // optional double doubleType = 1;
       private double doubleType_ ;
       /**
        * <code>optional double doubleType = 1;</code>
+       * @return Whether the doubleType field is set.
        */
+      @java.lang.Override
       public boolean hasDoubleType() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000001) != 0);
       }
       /**
        * <code>optional double doubleType = 1;</code>
+       * @return The doubleType.
        */
+      @java.lang.Override
       public double getDoubleType() {
         return doubleType_;
       }
       /**
        * <code>optional double doubleType = 1;</code>
+       * @param value The doubleType to set.
+       * @return This builder for chaining.
        */
       public Builder setDoubleType(double value) {
-        bitField0_ |= 0x00000001;
+
         doubleType_ = value;
+        bitField0_ |= 0x00000001;
         onChanged();
         return this;
       }
       /**
        * <code>optional double doubleType = 1;</code>
+       * @return This builder for chaining.
        */
       public Builder clearDoubleType() {
         bitField0_ = (bitField0_ & ~0x00000001);
@@ -3299,31 +3887,38 @@ public Builder clearDoubleType() {
         return this;
       }
 
-      // optional float floatType = 2;
       private float floatType_ ;
       /**
        * <code>optional float floatType = 2;</code>
+       * @return Whether the floatType field is set.
        */
+      @java.lang.Override
       public boolean hasFloatType() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000002) != 0);
       }
       /**
        * <code>optional float floatType = 2;</code>
+       * @return The floatType.
        */
+      @java.lang.Override
       public float getFloatType() {
         return floatType_;
       }
       /**
        * <code>optional float floatType = 2;</code>
+       * @param value The floatType to set.
+       * @return This builder for chaining.
        */
       public Builder setFloatType(float value) {
-        bitField0_ |= 0x00000002;
+
         floatType_ = value;
+        bitField0_ |= 0x00000002;
         onChanged();
         return this;
       }
       /**
        * <code>optional float floatType = 2;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFloatType() {
         bitField0_ = (bitField0_ & ~0x00000002);
@@ -3332,31 +3927,38 @@ public Builder clearFloatType() {
         return this;
       }
 
-      // optional int32 int32Type = 3;
       private int int32Type_ ;
       /**
        * <code>optional int32 int32Type = 3;</code>
+       * @return Whether the int32Type field is set.
        */
+      @java.lang.Override
       public boolean hasInt32Type() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+        return ((bitField0_ & 0x00000004) != 0);
       }
       /**
        * <code>optional int32 int32Type = 3;</code>
+       * @return The int32Type.
        */
+      @java.lang.Override
       public int getInt32Type() {
         return int32Type_;
       }
       /**
        * <code>optional int32 int32Type = 3;</code>
+       * @param value The int32Type to set.
+       * @return This builder for chaining.
        */
       public Builder setInt32Type(int value) {
-        bitField0_ |= 0x00000004;
+
         int32Type_ = value;
+        bitField0_ |= 0x00000004;
         onChanged();
         return this;
       }
       /**
        * <code>optional int32 int32Type = 3;</code>
+       * @return This builder for chaining.
        */
       public Builder clearInt32Type() {
         bitField0_ = (bitField0_ & ~0x00000004);
@@ -3365,31 +3967,38 @@ public Builder clearInt32Type() {
         return this;
       }
 
-      // optional int64 int64Type = 4;
       private long int64Type_ ;
       /**
        * <code>optional int64 int64Type = 4;</code>
+       * @return Whether the int64Type field is set.
        */
+      @java.lang.Override
       public boolean hasInt64Type() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
+        return ((bitField0_ & 0x00000008) != 0);
       }
       /**
        * <code>optional int64 int64Type = 4;</code>
+       * @return The int64Type.
        */
+      @java.lang.Override
       public long getInt64Type() {
         return int64Type_;
       }
       /**
        * <code>optional int64 int64Type = 4;</code>
+       * @param value The int64Type to set.
+       * @return This builder for chaining.
        */
       public Builder setInt64Type(long value) {
-        bitField0_ |= 0x00000008;
+
         int64Type_ = value;
+        bitField0_ |= 0x00000008;
         onChanged();
         return this;
       }
       /**
        * <code>optional int64 int64Type = 4;</code>
+       * @return This builder for chaining.
        */
       public Builder clearInt64Type() {
         bitField0_ = (bitField0_ & ~0x00000008);
@@ -3398,31 +4007,38 @@ public Builder clearInt64Type() {
         return this;
       }
 
-      // optional uint32 uint32Type = 5;
       private int uint32Type_ ;
       /**
        * <code>optional uint32 uint32Type = 5;</code>
+       * @return Whether the uint32Type field is set.
        */
+      @java.lang.Override
       public boolean hasUint32Type() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
+        return ((bitField0_ & 0x00000010) != 0);
       }
       /**
        * <code>optional uint32 uint32Type = 5;</code>
+       * @return The uint32Type.
        */
+      @java.lang.Override
       public int getUint32Type() {
         return uint32Type_;
       }
       /**
        * <code>optional uint32 uint32Type = 5;</code>
+       * @param value The uint32Type to set.
+       * @return This builder for chaining.
        */
       public Builder setUint32Type(int value) {
-        bitField0_ |= 0x00000010;
+
         uint32Type_ = value;
+        bitField0_ |= 0x00000010;
         onChanged();
         return this;
       }
       /**
        * <code>optional uint32 uint32Type = 5;</code>
+       * @return This builder for chaining.
        */
       public Builder clearUint32Type() {
         bitField0_ = (bitField0_ & ~0x00000010);
@@ -3431,31 +4047,38 @@ public Builder clearUint32Type() {
         return this;
       }
 
-      // optional uint64 uint64Type = 6;
       private long uint64Type_ ;
       /**
        * <code>optional uint64 uint64Type = 6;</code>
+       * @return Whether the uint64Type field is set.
        */
+      @java.lang.Override
       public boolean hasUint64Type() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
+        return ((bitField0_ & 0x00000020) != 0);
       }
       /**
        * <code>optional uint64 uint64Type = 6;</code>
+       * @return The uint64Type.
        */
+      @java.lang.Override
       public long getUint64Type() {
         return uint64Type_;
       }
       /**
        * <code>optional uint64 uint64Type = 6;</code>
+       * @param value The uint64Type to set.
+       * @return This builder for chaining.
        */
       public Builder setUint64Type(long value) {
-        bitField0_ |= 0x00000020;
+
         uint64Type_ = value;
+        bitField0_ |= 0x00000020;
         onChanged();
         return this;
       }
       /**
        * <code>optional uint64 uint64Type = 6;</code>
+       * @return This builder for chaining.
        */
       public Builder clearUint64Type() {
         bitField0_ = (bitField0_ & ~0x00000020);
@@ -3464,31 +4087,38 @@ public Builder clearUint64Type() {
         return this;
       }
 
-      // optional sint32 sint32Type = 7;
       private int sint32Type_ ;
       /**
        * <code>optional sint32 sint32Type = 7;</code>
+       * @return Whether the sint32Type field is set.
        */
+      @java.lang.Override
       public boolean hasSint32Type() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
+        return ((bitField0_ & 0x00000040) != 0);
       }
       /**
        * <code>optional sint32 sint32Type = 7;</code>
+       * @return The sint32Type.
        */
+      @java.lang.Override
       public int getSint32Type() {
         return sint32Type_;
       }
       /**
        * <code>optional sint32 sint32Type = 7;</code>
+       * @param value The sint32Type to set.
+       * @return This builder for chaining.
        */
       public Builder setSint32Type(int value) {
-        bitField0_ |= 0x00000040;
+
         sint32Type_ = value;
+        bitField0_ |= 0x00000040;
         onChanged();
         return this;
       }
       /**
        * <code>optional sint32 sint32Type = 7;</code>
+       * @return This builder for chaining.
        */
       public Builder clearSint32Type() {
         bitField0_ = (bitField0_ & ~0x00000040);
@@ -3497,31 +4127,38 @@ public Builder clearSint32Type() {
         return this;
       }
 
-      // optional sint64 sint64Type = 8;
       private long sint64Type_ ;
       /**
        * <code>optional sint64 sint64Type = 8;</code>
+       * @return Whether the sint64Type field is set.
        */
+      @java.lang.Override
       public boolean hasSint64Type() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
+        return ((bitField0_ & 0x00000080) != 0);
       }
       /**
        * <code>optional sint64 sint64Type = 8;</code>
+       * @return The sint64Type.
        */
+      @java.lang.Override
       public long getSint64Type() {
         return sint64Type_;
       }
       /**
        * <code>optional sint64 sint64Type = 8;</code>
+       * @param value The sint64Type to set.
+       * @return This builder for chaining.
        */
       public Builder setSint64Type(long value) {
-        bitField0_ |= 0x00000080;
+
         sint64Type_ = value;
+        bitField0_ |= 0x00000080;
         onChanged();
         return this;
       }
       /**
        * <code>optional sint64 sint64Type = 8;</code>
+       * @return This builder for chaining.
        */
       public Builder clearSint64Type() {
         bitField0_ = (bitField0_ & ~0x00000080);
@@ -3530,31 +4167,38 @@ public Builder clearSint64Type() {
         return this;
       }
 
-      // optional fixed32 fixed32Type = 9;
       private int fixed32Type_ ;
       /**
        * <code>optional fixed32 fixed32Type = 9;</code>
+       * @return Whether the fixed32Type field is set.
        */
+      @java.lang.Override
       public boolean hasFixed32Type() {
-        return ((bitField0_ & 0x00000100) == 0x00000100);
+        return ((bitField0_ & 0x00000100) != 0);
       }
       /**
        * <code>optional fixed32 fixed32Type = 9;</code>
+       * @return The fixed32Type.
        */
+      @java.lang.Override
       public int getFixed32Type() {
         return fixed32Type_;
       }
       /**
        * <code>optional fixed32 fixed32Type = 9;</code>
+       * @param value The fixed32Type to set.
+       * @return This builder for chaining.
        */
       public Builder setFixed32Type(int value) {
-        bitField0_ |= 0x00000100;
+
         fixed32Type_ = value;
+        bitField0_ |= 0x00000100;
         onChanged();
         return this;
       }
       /**
        * <code>optional fixed32 fixed32Type = 9;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFixed32Type() {
         bitField0_ = (bitField0_ & ~0x00000100);
@@ -3563,31 +4207,38 @@ public Builder clearFixed32Type() {
         return this;
       }
 
-      // optional fixed64 fixed64Type = 10;
       private long fixed64Type_ ;
       /**
        * <code>optional fixed64 fixed64Type = 10;</code>
+       * @return Whether the fixed64Type field is set.
        */
+      @java.lang.Override
       public boolean hasFixed64Type() {
-        return ((bitField0_ & 0x00000200) == 0x00000200);
+        return ((bitField0_ & 0x00000200) != 0);
       }
       /**
        * <code>optional fixed64 fixed64Type = 10;</code>
+       * @return The fixed64Type.
        */
+      @java.lang.Override
       public long getFixed64Type() {
         return fixed64Type_;
       }
       /**
        * <code>optional fixed64 fixed64Type = 10;</code>
+       * @param value The fixed64Type to set.
+       * @return This builder for chaining.
        */
       public Builder setFixed64Type(long value) {
-        bitField0_ |= 0x00000200;
+
         fixed64Type_ = value;
+        bitField0_ |= 0x00000200;
         onChanged();
         return this;
       }
       /**
        * <code>optional fixed64 fixed64Type = 10;</code>
+       * @return This builder for chaining.
        */
       public Builder clearFixed64Type() {
         bitField0_ = (bitField0_ & ~0x00000200);
@@ -3596,31 +4247,38 @@ public Builder clearFixed64Type() {
         return this;
       }
 
-      // optional sfixed32 sfixed32Type = 11;
       private int sfixed32Type_ ;
       /**
        * <code>optional sfixed32 sfixed32Type = 11;</code>
+       * @return Whether the sfixed32Type field is set.
        */
+      @java.lang.Override
       public boolean hasSfixed32Type() {
-        return ((bitField0_ & 0x00000400) == 0x00000400);
+        return ((bitField0_ & 0x00000400) != 0);
       }
       /**
        * <code>optional sfixed32 sfixed32Type = 11;</code>
+       * @return The sfixed32Type.
        */
+      @java.lang.Override
       public int getSfixed32Type() {
         return sfixed32Type_;
       }
       /**
        * <code>optional sfixed32 sfixed32Type = 11;</code>
+       * @param value The sfixed32Type to set.
+       * @return This builder for chaining.
        */
       public Builder setSfixed32Type(int value) {
-        bitField0_ |= 0x00000400;
+
         sfixed32Type_ = value;
+        bitField0_ |= 0x00000400;
         onChanged();
         return this;
       }
       /**
        * <code>optional sfixed32 sfixed32Type = 11;</code>
+       * @return This builder for chaining.
        */
       public Builder clearSfixed32Type() {
         bitField0_ = (bitField0_ & ~0x00000400);
@@ -3629,31 +4287,38 @@ public Builder clearSfixed32Type() {
         return this;
       }
 
-      // optional sfixed64 sfixed64Type = 12;
       private long sfixed64Type_ ;
       /**
        * <code>optional sfixed64 sfixed64Type = 12;</code>
+       * @return Whether the sfixed64Type field is set.
        */
+      @java.lang.Override
       public boolean hasSfixed64Type() {
-        return ((bitField0_ & 0x00000800) == 0x00000800);
+        return ((bitField0_ & 0x00000800) != 0);
       }
       /**
        * <code>optional sfixed64 sfixed64Type = 12;</code>
+       * @return The sfixed64Type.
        */
+      @java.lang.Override
       public long getSfixed64Type() {
         return sfixed64Type_;
       }
       /**
        * <code>optional sfixed64 sfixed64Type = 12;</code>
+       * @param value The sfixed64Type to set.
+       * @return This builder for chaining.
        */
       public Builder setSfixed64Type(long value) {
-        bitField0_ |= 0x00000800;
+
         sfixed64Type_ = value;
+        bitField0_ |= 0x00000800;
         onChanged();
         return this;
       }
       /**
        * <code>optional sfixed64 sfixed64Type = 12;</code>
+       * @return This builder for chaining.
        */
       public Builder clearSfixed64Type() {
         bitField0_ = (bitField0_ & ~0x00000800);
@@ -3662,31 +4327,38 @@ public Builder clearSfixed64Type() {
         return this;
       }
 
-      // optional bool boolType = 13;
       private boolean boolType_ ;
       /**
        * <code>optional bool boolType = 13;</code>
+       * @return Whether the boolType field is set.
        */
+      @java.lang.Override
       public boolean hasBoolType() {
-        return ((bitField0_ & 0x00001000) == 0x00001000);
+        return ((bitField0_ & 0x00001000) != 0);
       }
       /**
        * <code>optional bool boolType = 13;</code>
+       * @return The boolType.
        */
+      @java.lang.Override
       public boolean getBoolType() {
         return boolType_;
       }
       /**
        * <code>optional bool boolType = 13;</code>
+       * @param value The boolType to set.
+       * @return This builder for chaining.
        */
       public Builder setBoolType(boolean value) {
-        bitField0_ |= 0x00001000;
+
         boolType_ = value;
+        bitField0_ |= 0x00001000;
         onChanged();
         return this;
       }
       /**
        * <code>optional bool boolType = 13;</code>
+       * @return This builder for chaining.
        */
       public Builder clearBoolType() {
         bitField0_ = (bitField0_ & ~0x00001000);
@@ -3695,23 +4367,27 @@ public Builder clearBoolType() {
         return this;
       }
 
-      // optional string stringType = 14;
       private java.lang.Object stringType_ = "";
       /**
        * <code>optional string stringType = 14;</code>
+       * @return Whether the stringType field is set.
        */
       public boolean hasStringType() {
-        return ((bitField0_ & 0x00002000) == 0x00002000);
+        return ((bitField0_ & 0x00002000) != 0);
       }
       /**
        * <code>optional string stringType = 14;</code>
+       * @return The stringType.
        */
       public java.lang.String getStringType() {
         java.lang.Object ref = stringType_;
         if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          stringType_ = s;
+          com.google.protobuf.ByteString bs =
+              (com.google.protobuf.ByteString) ref;
+          java.lang.String s = bs.toStringUtf8();
+          if (bs.isValidUtf8()) {
+            stringType_ = s;
+          }
           return s;
         } else {
           return (java.lang.String) ref;
@@ -3719,6 +4395,7 @@ public java.lang.String getStringType() {
       }
       /**
        * <code>optional string stringType = 14;</code>
+       * @return The bytes for stringType.
        */
       public com.google.protobuf.ByteString
           getStringTypeBytes() {
@@ -3735,68 +4412,73 @@ public java.lang.String getStringType() {
       }
       /**
        * <code>optional string stringType = 14;</code>
+       * @param value The stringType to set.
+       * @return This builder for chaining.
        */
       public Builder setStringType(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00002000;
+        if (value == null) { throw new NullPointerException(); }
         stringType_ = value;
+        bitField0_ |= 0x00002000;
         onChanged();
         return this;
       }
       /**
        * <code>optional string stringType = 14;</code>
+       * @return This builder for chaining.
        */
       public Builder clearStringType() {
-        bitField0_ = (bitField0_ & ~0x00002000);
         stringType_ = getDefaultInstance().getStringType();
+        bitField0_ = (bitField0_ & ~0x00002000);
         onChanged();
         return this;
       }
       /**
        * <code>optional string stringType = 14;</code>
+       * @param value The bytes for stringType to set.
+       * @return This builder for chaining.
        */
       public Builder setStringTypeBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00002000;
+        if (value == null) { throw new NullPointerException(); }
         stringType_ = value;
+        bitField0_ |= 0x00002000;
         onChanged();
         return this;
       }
 
-      // optional bytes bytesType = 15;
       private com.google.protobuf.ByteString bytesType_ = com.google.protobuf.ByteString.EMPTY;
       /**
        * <code>optional bytes bytesType = 15;</code>
+       * @return Whether the bytesType field is set.
        */
+      @java.lang.Override
       public boolean hasBytesType() {
-        return ((bitField0_ & 0x00004000) == 0x00004000);
+        return ((bitField0_ & 0x00004000) != 0);
       }
       /**
        * <code>optional bytes bytesType = 15;</code>
+       * @return The bytesType.
        */
+      @java.lang.Override
       public com.google.protobuf.ByteString getBytesType() {
         return bytesType_;
       }
       /**
        * <code>optional bytes bytesType = 15;</code>
+       * @param value The bytesType to set.
+       * @return This builder for chaining.
        */
       public Builder setBytesType(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00004000;
+        if (value == null) { throw new NullPointerException(); }
         bytesType_ = value;
+        bitField0_ |= 0x00004000;
         onChanged();
         return this;
       }
       /**
        * <code>optional bytes bytesType = 15;</code>
+       * @return This builder for chaining.
        */
       public Builder clearBytesType() {
         bitField0_ = (bitField0_ & ~0x00004000);
@@ -3805,17 +4487,16 @@ public Builder clearBytesType() {
         return this;
       }
 
-      // repeated .MapFieldEntry mapType = 16;
       private java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry> mapType_ =
         java.util.Collections.emptyList();
       private void ensureMapTypeIsMutable() {
-        if (!((bitField0_ & 0x00008000) == 0x00008000)) {
+        if (!((bitField0_ & 0x00008000) != 0)) {
           mapType_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry>(mapType_);
           bitField0_ |= 0x00008000;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder> mapTypeBuilder_;
 
       /**
@@ -3947,7 +4628,8 @@ public Builder addAllMapType(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry> values) {
         if (mapTypeBuilder_ == null) {
           ensureMapTypeIsMutable();
-          super.addAll(values, mapType_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, mapType_);
           onChanged();
         } else {
           mapTypeBuilder_.addAllMessages(values);
@@ -4030,14 +4712,14 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder
            getMapTypeBuilderList() {
         return getMapTypeFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder> 
           getMapTypeFieldBuilder() {
         if (mapTypeBuilder_ == null) {
-          mapTypeBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          mapTypeBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntryOrBuilder>(
                   mapType_,
-                  ((bitField0_ & 0x00008000) == 0x00008000),
+                  ((bitField0_ & 0x00008000) != 0),
                   getParentForChildren(),
                   isClean());
           mapType_ = null;
@@ -4045,35 +4727,42 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.MapFieldEntry.Builder
         return mapTypeBuilder_;
       }
 
-      // repeated string stringListType = 17;
-      private com.google.protobuf.LazyStringList stringListType_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private com.google.protobuf.LazyStringArrayList stringListType_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
       private void ensureStringListTypeIsMutable() {
-        if (!((bitField0_ & 0x00010000) == 0x00010000)) {
+        if (!stringListType_.isModifiable()) {
           stringListType_ = new com.google.protobuf.LazyStringArrayList(stringListType_);
-          bitField0_ |= 0x00010000;
-         }
+        }
+        bitField0_ |= 0x00010000;
       }
       /**
        * <code>repeated string stringListType = 17;</code>
+       * @return A list containing the stringListType.
        */
-      public java.util.List<java.lang.String>
+      public com.google.protobuf.ProtocolStringList
           getStringListTypeList() {
-        return java.util.Collections.unmodifiableList(stringListType_);
+        stringListType_.makeImmutable();
+        return stringListType_;
       }
       /**
        * <code>repeated string stringListType = 17;</code>
+       * @return The count of stringListType.
        */
       public int getStringListTypeCount() {
         return stringListType_.size();
       }
       /**
        * <code>repeated string stringListType = 17;</code>
+       * @param index The index of the element to return.
+       * @return The stringListType at the given index.
        */
       public java.lang.String getStringListType(int index) {
         return stringListType_.get(index);
       }
       /**
        * <code>repeated string stringListType = 17;</code>
+       * @param index The index of the value to return.
+       * @return The bytes of the stringListType at the given index.
        */
       public com.google.protobuf.ByteString
           getStringListTypeBytes(int index) {
@@ -4081,79 +4770,90 @@ public java.lang.String getStringListType(int index) {
       }
       /**
        * <code>repeated string stringListType = 17;</code>
+       * @param index The index to set the value at.
+       * @param value The stringListType to set.
+       * @return This builder for chaining.
        */
       public Builder setStringListType(
           int index, java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureStringListTypeIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureStringListTypeIsMutable();
         stringListType_.set(index, value);
+        bitField0_ |= 0x00010000;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string stringListType = 17;</code>
+       * @param value The stringListType to add.
+       * @return This builder for chaining.
        */
       public Builder addStringListType(
           java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureStringListTypeIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureStringListTypeIsMutable();
         stringListType_.add(value);
+        bitField0_ |= 0x00010000;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string stringListType = 17;</code>
+       * @param values The stringListType to add.
+       * @return This builder for chaining.
        */
       public Builder addAllStringListType(
           java.lang.Iterable<java.lang.String> values) {
         ensureStringListTypeIsMutable();
-        super.addAll(values, stringListType_);
+        com.google.protobuf.AbstractMessageLite.Builder.addAll(
+            values, stringListType_);
+        bitField0_ |= 0x00010000;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string stringListType = 17;</code>
+       * @return This builder for chaining.
        */
       public Builder clearStringListType() {
-        stringListType_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00010000);
+        stringListType_ =
+          com.google.protobuf.LazyStringArrayList.emptyList();
+        bitField0_ = (bitField0_ & ~0x00010000);;
         onChanged();
         return this;
       }
       /**
        * <code>repeated string stringListType = 17;</code>
+       * @param value The bytes of the stringListType to add.
+       * @return This builder for chaining.
        */
       public Builder addStringListTypeBytes(
           com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureStringListTypeIsMutable();
+        if (value == null) { throw new NullPointerException(); }
+        ensureStringListTypeIsMutable();
         stringListType_.add(value);
+        bitField0_ |= 0x00010000;
         onChanged();
         return this;
       }
 
-      // optional .Mesg1 messageType = 18;
-      private org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 messageType_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
+      private org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 messageType_;
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder> messageTypeBuilder_;
       /**
        * <code>optional .Mesg1 messageType = 18;</code>
+       * @return Whether the messageType field is set.
        */
       public boolean hasMessageType() {
-        return ((bitField0_ & 0x00020000) == 0x00020000);
+        return ((bitField0_ & 0x00020000) != 0);
       }
       /**
        * <code>optional .Mesg1 messageType = 18;</code>
+       * @return The messageType.
        */
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 getMessageType() {
         if (messageTypeBuilder_ == null) {
-          return messageType_;
+          return messageType_ == null ? org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance() : messageType_;
         } else {
           return messageTypeBuilder_.getMessage();
         }
@@ -4167,11 +4867,11 @@ public Builder setMessageType(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos
             throw new NullPointerException();
           }
           messageType_ = value;
-          onChanged();
         } else {
           messageTypeBuilder_.setMessage(value);
         }
         bitField0_ |= 0x00020000;
+        onChanged();
         return this;
       }
       /**
@@ -4181,11 +4881,11 @@ public Builder setMessageType(
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder builderForValue) {
         if (messageTypeBuilder_ == null) {
           messageType_ = builderForValue.build();
-          onChanged();
         } else {
           messageTypeBuilder_.setMessage(builderForValue.build());
         }
         bitField0_ |= 0x00020000;
+        onChanged();
         return this;
       }
       /**
@@ -4193,31 +4893,33 @@ public Builder setMessageType(
        */
       public Builder mergeMessageType(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1 value) {
         if (messageTypeBuilder_ == null) {
-          if (((bitField0_ & 0x00020000) == 0x00020000) &&
-              messageType_ != org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance()) {
-            messageType_ =
-              org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.newBuilder(messageType_).mergeFrom(value).buildPartial();
+          if (((bitField0_ & 0x00020000) != 0) &&
+            messageType_ != null &&
+            messageType_ != org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance()) {
+            getMessageTypeBuilder().mergeFrom(value);
           } else {
             messageType_ = value;
           }
-          onChanged();
         } else {
           messageTypeBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00020000;
+        if (messageType_ != null) {
+          bitField0_ |= 0x00020000;
+          onChanged();
+        }
         return this;
       }
       /**
        * <code>optional .Mesg1 messageType = 18;</code>
        */
       public Builder clearMessageType() {
-        if (messageTypeBuilder_ == null) {
-          messageType_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance();
-          onChanged();
-        } else {
-          messageTypeBuilder_.clear();
-        }
         bitField0_ = (bitField0_ & ~0x00020000);
+        messageType_ = null;
+        if (messageTypeBuilder_ != null) {
+          messageTypeBuilder_.dispose();
+          messageTypeBuilder_ = null;
+        }
+        onChanged();
         return this;
       }
       /**
@@ -4235,19 +4937,20 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder getMess
         if (messageTypeBuilder_ != null) {
           return messageTypeBuilder_.getMessageOrBuilder();
         } else {
-          return messageType_;
+          return messageType_ == null ?
+              org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.getDefaultInstance() : messageType_;
         }
       }
       /**
        * <code>optional .Mesg1 messageType = 18;</code>
        */
-      private com.google.protobuf.SingleFieldBuilder<
+      private com.google.protobuf.SingleFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder> 
           getMessageTypeFieldBuilder() {
         if (messageTypeBuilder_ == null) {
-          messageTypeBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+          messageTypeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
               org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder>(
-                  messageType_,
+                  getMessageType(),
                   getParentForChildren(),
                   isClean());
           messageType_ = null;
@@ -4255,17 +4958,16 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder getMess
         return messageTypeBuilder_;
       }
 
-      // repeated .Mesg1 messageListType = 19;
       private java.util.List<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1> messageListType_ =
         java.util.Collections.emptyList();
       private void ensureMessageListTypeIsMutable() {
-        if (!((bitField0_ & 0x00040000) == 0x00040000)) {
+        if (!((bitField0_ & 0x00040000) != 0)) {
           messageListType_ = new java.util.ArrayList<org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1>(messageListType_);
           bitField0_ |= 0x00040000;
          }
       }
 
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder> messageListTypeBuilder_;
 
       /**
@@ -4397,7 +5099,8 @@ public Builder addAllMessageListType(
           java.lang.Iterable<? extends org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1> values) {
         if (messageListTypeBuilder_ == null) {
           ensureMessageListTypeIsMutable();
-          super.addAll(values, messageListType_);
+          com.google.protobuf.AbstractMessageLite.Builder.addAll(
+              values, messageListType_);
           onChanged();
         } else {
           messageListTypeBuilder_.addAllMessages(values);
@@ -4480,14 +5183,14 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder addMessa
            getMessageListTypeBuilderList() {
         return getMessageListTypeFieldBuilder().getBuilderList();
       }
-      private com.google.protobuf.RepeatedFieldBuilder<
+      private com.google.protobuf.RepeatedFieldBuilderV3<
           org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder> 
           getMessageListTypeFieldBuilder() {
         if (messageListTypeBuilder_ == null) {
-          messageListTypeBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+          messageListTypeBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
               org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder, org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1OrBuilder>(
                   messageListType_,
-                  ((bitField0_ & 0x00040000) == 0x00040000),
+                  ((bitField0_ & 0x00040000) != 0),
                   getParentForChildren(),
                   isClean());
           messageListType_ = null;
@@ -4495,74 +5198,132 @@ public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.Mesg1.Builder addMessa
         return messageListTypeBuilder_;
       }
 
-      // optional .AllTypes.Enum1 enumType = 20;
-      private org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 enumType_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.VAL1;
+      private int enumType_ = 1;
       /**
        * <code>optional .AllTypes.Enum1 enumType = 20;</code>
+       * @return Whether the enumType field is set.
        */
-      public boolean hasEnumType() {
-        return ((bitField0_ & 0x00080000) == 0x00080000);
+      @java.lang.Override public boolean hasEnumType() {
+        return ((bitField0_ & 0x00080000) != 0);
       }
       /**
        * <code>optional .AllTypes.Enum1 enumType = 20;</code>
+       * @return The enumType.
        */
+      @java.lang.Override
       public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 getEnumType() {
-        return enumType_;
+        org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 result = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.forNumber(enumType_);
+        return result == null ? org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.VAL1 : result;
       }
       /**
        * <code>optional .AllTypes.Enum1 enumType = 20;</code>
+       * @param value The enumType to set.
+       * @return This builder for chaining.
        */
       public Builder setEnumType(org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1 value) {
         if (value == null) {
           throw new NullPointerException();
         }
         bitField0_ |= 0x00080000;
-        enumType_ = value;
+        enumType_ = value.getNumber();
         onChanged();
         return this;
       }
       /**
        * <code>optional .AllTypes.Enum1 enumType = 20;</code>
+       * @return This builder for chaining.
        */
       public Builder clearEnumType() {
         bitField0_ = (bitField0_ & ~0x00080000);
-        enumType_ = org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes.Enum1.VAL1;
+        enumType_ = 1;
         onChanged();
         return this;
       }
+      @java.lang.Override
+      public final Builder setUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.setUnknownFields(unknownFields);
+      }
+
+      @java.lang.Override
+      public final Builder mergeUnknownFields(
+          final com.google.protobuf.UnknownFieldSet unknownFields) {
+        return super.mergeUnknownFields(unknownFields);
+      }
+
 
       // @@protoc_insertion_point(builder_scope:AllTypes)
     }
 
+    // @@protoc_insertion_point(class_scope:AllTypes)
+    private static final org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes DEFAULT_INSTANCE;
     static {
-      defaultInstance = new AllTypes(true);
-      defaultInstance.initFields();
+      DEFAULT_INSTANCE = new org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes();
+    }
+
+    public static org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes getDefaultInstance() {
+      return DEFAULT_INSTANCE;
+    }
+
+    @java.lang.Deprecated public static final com.google.protobuf.Parser<AllTypes>
+        PARSER = new com.google.protobuf.AbstractParser<AllTypes>() {
+      @java.lang.Override
+      public AllTypes parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        Builder builder = newBuilder();
+        try {
+          builder.mergeFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          throw e.setUnfinishedMessage(builder.buildPartial());
+        } catch (com.google.protobuf.UninitializedMessageException e) {
+          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
+        } catch (java.io.IOException e) {
+          throw new com.google.protobuf.InvalidProtocolBufferException(e)
+              .setUnfinishedMessage(builder.buildPartial());
+        }
+        return builder.buildPartial();
+      }
+    };
+
+    public static com.google.protobuf.Parser<AllTypes> parser() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<AllTypes> getParserForType() {
+      return PARSER;
+    }
+
+    @java.lang.Override
+    public org.apache.hadoop.hive.ql.io.protobuf.SampleProtos.AllTypes getDefaultInstanceForType() {
+      return DEFAULT_INSTANCE;
     }
 
-    // @@protoc_insertion_point(class_scope:AllTypes)
   }
 
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_MapFieldEntry_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_MapFieldEntry_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_Mesg1_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_Mesg1_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
+  private static final com.google.protobuf.Descriptors.Descriptor
     internal_static_AllTypes_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+  private static final 
+    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
       internal_static_AllTypes_fieldAccessorTable;
 
   public static com.google.protobuf.Descriptors.FileDescriptor
       getDescriptor() {
     return descriptor;
   }
-  private static com.google.protobuf.Descriptors.FileDescriptor
+  private static  com.google.protobuf.Descriptors.FileDescriptor
       descriptor;
   static {
     java.lang.String[] descriptorData = {
@@ -4575,7 +5336,7 @@ public Builder clearEnumType() {
       "pe\030\004 \001(\003\022\022\n\nuint32Type\030\005 \001(\r\022\022\n\nuint64Ty" +
       "pe\030\006 \001(\004\022\022\n\nsint32Type\030\007 \001(\021\022\022\n\nsint64Ty" +
       "pe\030\010 \001(\022\022\023\n\013fixed32Type\030\t \001(\007\022\023\n\013fixed64" +
-      "Type\030\n \001(\006\022\024\n\014sfixed32Type\030\013 \001(\017\022\024\n\014sfix",
+      "Type\030\n \001(\006\022\024\n\014sfixed32Type\030\013 \001(\017\022\024\n\014sfix" +
       "ed64Type\030\014 \001(\020\022\020\n\010boolType\030\r \001(\010\022\022\n\nstri" +
       "ngType\030\016 \001(\t\022\021\n\tbytesType\030\017 \001(\014\022\037\n\007mapTy" +
       "pe\030\020 \003(\0132\016.MapFieldEntry\022\026\n\016stringListTy" +
@@ -4585,36 +5346,28 @@ public Builder clearEnumType() {
       "L1\020\001\022\010\n\004VAL2\020\002B5\n%org.apache.hadoop.hive" +
       ".ql.io.protobufB\014SampleProtos"
     };
-    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
-      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
-        public com.google.protobuf.ExtensionRegistry assignDescriptors(
-            com.google.protobuf.Descriptors.FileDescriptor root) {
-          descriptor = root;
-          internal_static_MapFieldEntry_descriptor =
-            getDescriptor().getMessageTypes().get(0);
-          internal_static_MapFieldEntry_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_MapFieldEntry_descriptor,
-              new java.lang.String[] { "Key", "Value", });
-          internal_static_Mesg1_descriptor =
-            getDescriptor().getMessageTypes().get(1);
-          internal_static_Mesg1_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_Mesg1_descriptor,
-              new java.lang.String[] { "AnotherMap", "NoMap", "IntList", });
-          internal_static_AllTypes_descriptor =
-            getDescriptor().getMessageTypes().get(2);
-          internal_static_AllTypes_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_AllTypes_descriptor,
-              new java.lang.String[] { "DoubleType", "FloatType", "Int32Type", "Int64Type", "Uint32Type", "Uint64Type", "Sint32Type", "Sint64Type", "Fixed32Type", "Fixed64Type", "Sfixed32Type", "Sfixed64Type", "BoolType", "StringType", "BytesType", "MapType", "StringListType", "MessageType", "MessageListType", "EnumType", });
-          return null;
-        }
-      };
-    com.google.protobuf.Descriptors.FileDescriptor
+    descriptor = com.google.protobuf.Descriptors.FileDescriptor
       .internalBuildGeneratedFileFrom(descriptorData,
         new com.google.protobuf.Descriptors.FileDescriptor[] {
-        }, assigner);
+        });
+    internal_static_MapFieldEntry_descriptor =
+      getDescriptor().getMessageTypes().get(0);
+    internal_static_MapFieldEntry_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_MapFieldEntry_descriptor,
+        new java.lang.String[] { "Key", "Value", });
+    internal_static_Mesg1_descriptor =
+      getDescriptor().getMessageTypes().get(1);
+    internal_static_Mesg1_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_Mesg1_descriptor,
+        new java.lang.String[] { "AnotherMap", "NoMap", "IntList", });
+    internal_static_AllTypes_descriptor =
+      getDescriptor().getMessageTypes().get(2);
+    internal_static_AllTypes_fieldAccessorTable = new
+      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
+        internal_static_AllTypes_descriptor,
+        new java.lang.String[] { "DoubleType", "FloatType", "Int32Type", "Int64Type", "Uint32Type", "Uint64Type", "Sint32Type", "Sint64Type", "Fixed32Type", "Fixed64Type", "Sfixed32Type", "Sfixed64Type", "BoolType", "StringType", "BytesType", "MapType", "StringListType", "MessageType", "MessageListType", "EnumType", });
   }
 
   // @@protoc_insertion_point(outer_class_scope)
