diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index d66c0f3dd6..dc98b3fd8e 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -312,7 +312,7 @@ public static enum ConfVars {
         "When hive.exec.mode.local.auto is true, the number of tasks should less than this for local mode."),
 
     DROPIGNORESNONEXISTENT("hive.exec.drop.ignorenonexistent", true,
-        "Do not report an error if DROP TABLE/VIEW specifies a non-existent table/view"),
+        "Do not report an error if DROP TABLE/VIEW/Index specifies a non-existent table/view/index"),
 
     HIVEIGNOREMAPJOINHINT("hive.ignore.mapjoin.hint", true, "Ignore the mapjoin hint"),
 
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
index 3b5f65f31e..4dbd26a483 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
@@ -29,26 +29,33 @@
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.Index;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.PartitionEventType;
 import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.metastore.events.AddIndexEvent;
 import org.apache.hadoop.hive.metastore.events.AddPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.AlterIndexEvent;
 import org.apache.hadoop.hive.metastore.events.AlterPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.AlterTableEvent;
 import org.apache.hadoop.hive.metastore.events.ConfigChangeEvent;
 import org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent;
 import org.apache.hadoop.hive.metastore.events.CreateTableEvent;
 import org.apache.hadoop.hive.metastore.events.DropDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.DropIndexEvent;
 import org.apache.hadoop.hive.metastore.events.DropPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.DropTableEvent;
 import org.apache.hadoop.hive.metastore.events.ListenerEvent;
 import org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent;
+import org.apache.hadoop.hive.metastore.events.PreAddIndexEvent;
 import org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.PreAlterIndexEvent;
 import org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.PreAlterTableEvent;
 import org.apache.hadoop.hive.metastore.events.PreCreateDatabaseEvent;
 import org.apache.hadoop.hive.metastore.events.PreCreateTableEvent;
 import org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.PreDropIndexEvent;
 import org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.PreDropTableEvent;
 import org.apache.hadoop.hive.metastore.events.PreEventContext;
@@ -183,12 +190,35 @@ private void validateDropDb(Database expectedDb, Database actualDb) {
     assertEquals(expectedDb, actualDb);
   }
 
+  private void validateIndex(Index expectedIndex, Index actualIndex) {
+    assertEquals(expectedIndex.getDbName(), actualIndex.getDbName());
+    assertEquals(expectedIndex.getIndexName(), actualIndex.getIndexName());
+    assertEquals(expectedIndex.getIndexHandlerClass(), actualIndex.getIndexHandlerClass());
+    assertEquals(expectedIndex.getOrigTableName(), actualIndex.getOrigTableName());
+    assertEquals(expectedIndex.getIndexTableName(), actualIndex.getIndexTableName());
+    assertEquals(expectedIndex.getSd().getLocation(), actualIndex.getSd().getLocation());
+  }
+
+  private void validateAddIndex(Index expectedIndex, Index actualIndex) {
+    validateIndex(expectedIndex, actualIndex);
+  }
+
+  private void validateAlterIndex(Index expectedOldIndex, Index actualOldIndex,
+      Index expectedNewIndex, Index actualNewIndex) {
+    validateIndex(expectedOldIndex, actualOldIndex);
+    validateIndex(expectedNewIndex, actualNewIndex);
+  }
+
+  private void validateDropIndex(Index expectedIndex, Index actualIndex) {
+    validateIndex(expectedIndex, actualIndex);
+  }
+
   public void testListener() throws Exception {
     int listSize = 0;
 
     List<ListenerEvent> notifyList = DummyListener.notifyList;
-    assertEquals(notifyList.size(), listSize);
     List<PreEventContext> preNotifyList = DummyPreListener.notifyList;
+    assertEquals(notifyList.size(), listSize);
     assertEquals(preNotifyList.size(), listSize);
 
     driver.run("create database " + dbName);
@@ -216,6 +246,48 @@ public void testListener() throws Exception {
     assert tblEvent.getStatus();
     validateCreateTable(tbl, tblEvent.getTable());
 
+    driver.run("create index tmptbl_i on table tmptbl(a) as 'compact' " +
+        "WITH DEFERRED REBUILD IDXPROPERTIES ('prop1'='val1', 'prop2'='val2')");
+    listSize += 2;  // creates index table internally
+    assertEquals(notifyList.size(), listSize);
+
+    AddIndexEvent addIndexEvent = (AddIndexEvent)notifyList.get(listSize - 1);
+    assert addIndexEvent.getStatus();
+    PreAddIndexEvent preAddIndexEvent = (PreAddIndexEvent)(preNotifyList.get(preNotifyList.size() - 3));
+
+    Index oldIndex = msc.getIndex(dbName, "tmptbl", "tmptbl_i");
+
+    validateAddIndex(oldIndex, addIndexEvent.getIndex());
+
+    validateAddIndex(oldIndex, preAddIndexEvent.getIndex());
+
+    driver.run("alter index tmptbl_i on tmptbl set IDXPROPERTIES " +
+        "('prop1'='val1_new', 'prop3'='val3')");
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+
+    Index newIndex = msc.getIndex(dbName, "tmptbl", "tmptbl_i");
+
+    AlterIndexEvent alterIndexEvent = (AlterIndexEvent) notifyList.get(listSize - 1);
+    assert alterIndexEvent.getStatus();
+    validateAlterIndex(oldIndex, alterIndexEvent.getOldIndex(),
+        newIndex, alterIndexEvent.getNewIndex());
+
+    PreAlterIndexEvent preAlterIndexEvent = (PreAlterIndexEvent) (preNotifyList.get(preNotifyList.size() - 1));
+    validateAlterIndex(oldIndex, preAlterIndexEvent.getOldIndex(),
+        newIndex, preAlterIndexEvent.getNewIndex());
+
+    driver.run("drop index tmptbl_i on tmptbl");
+    listSize++;
+    assertEquals(notifyList.size(), listSize);
+
+    DropIndexEvent dropIndexEvent = (DropIndexEvent) notifyList.get(listSize - 1);
+    assert dropIndexEvent.getStatus();
+    validateDropIndex(newIndex, dropIndexEvent.getIndex());
+
+    PreDropIndexEvent preDropIndexEvent = (PreDropIndexEvent) (preNotifyList.get(preNotifyList.size() - 1));
+    validateDropIndex(newIndex, preDropIndexEvent.getIndex());
+
     driver.run("alter table tmptbl add partition (b='2011')");
     listSize++;
     assertEquals(notifyList.size(), listSize);
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
index f5e35b8064..a035ff1437 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
@@ -559,7 +559,7 @@ public void clearTestSideEffects() throws Exception {
          List<Index> indexes = db.getIndexes(dbName, tblName, (short)-1);
           if (indexes != null && indexes.size() > 0) {
             for (Index index : indexes) {
-              db.dropIndex(dbName, tblName, index.getIndexName(), true);
+              db.dropIndex(dbName, tblName, index.getIndexName(), true, true);
             }
           }
         }
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
index 8c18760c87..3cf43a96ce 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
@@ -148,6 +148,8 @@
 import org.apache.hadoop.hive.metastore.api.UnknownTableException;
 import org.apache.hadoop.hive.metastore.api.UnlockRequest;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
+import org.apache.hadoop.hive.metastore.events.AddIndexEvent;
+import org.apache.hadoop.hive.metastore.events.AlterIndexEvent;
 import org.apache.hadoop.hive.metastore.events.AddPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.AlterPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.AlterTableEvent;
@@ -155,17 +157,21 @@
 import org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent;
 import org.apache.hadoop.hive.metastore.events.CreateTableEvent;
 import org.apache.hadoop.hive.metastore.events.DropDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.DropIndexEvent;
 import org.apache.hadoop.hive.metastore.events.DropPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.DropTableEvent;
 import org.apache.hadoop.hive.metastore.events.EventCleanerTask;
 import org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent;
+import org.apache.hadoop.hive.metastore.events.PreAddIndexEvent;
 import org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.PreAlterIndexEvent;
 import org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.PreAlterTableEvent;
 import org.apache.hadoop.hive.metastore.events.PreAuthorizationCallEvent;
 import org.apache.hadoop.hive.metastore.events.PreCreateDatabaseEvent;
 import org.apache.hadoop.hive.metastore.events.PreCreateTableEvent;
 import org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.PreDropIndexEvent;
 import org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.PreDropTableEvent;
 import org.apache.hadoop.hive.metastore.events.PreEventContext;
@@ -1429,17 +1435,15 @@ private boolean is_table_exists(RawStore ms, String dbname, String name)
       return (ms.getTable(dbname, name) != null);
     }
 
-    private void drop_table_core(final RawStore ms, final String dbname, final String name,
-        final boolean deleteData, final EnvironmentContext envContext)
-        throws NoSuchObjectException, MetaException, IOException,
-        InvalidObjectException, InvalidInputException {
+    private boolean drop_table_core(final RawStore ms, final String dbname, final String name,
+        final boolean deleteData, final EnvironmentContext envContext,
+        final String indexName) throws NoSuchObjectException,
+        MetaException, IOException, InvalidObjectException, InvalidInputException {
       boolean success = false;
       boolean isExternal = false;
       Path tblPath = null;
       List<Path> partPaths = null;
       Table tbl = null;
-      isExternal = false;
-      boolean isIndexTable = false;
       try {
         ms.openTransaction();
         // drop any partitions
@@ -1453,8 +1457,8 @@ private void drop_table_core(final RawStore ms, final String dbname, final Strin
 
         firePreEvent(new PreDropTableEvent(tbl, deleteData, this));
 
-        isIndexTable = isIndexTable(tbl);
-        if (isIndexTable) {
+        boolean isIndexTable = isIndexTable(tbl);
+        if (indexName == null && isIndexTable) {
           throw new RuntimeException(
               "The table " + name + " is an index table. Please do drop index instead.");
         }
@@ -1476,7 +1480,8 @@ private void drop_table_core(final RawStore ms, final String dbname, final Strin
         if (tbl.getSd().getLocation() != null) {
           tblPath = new Path(tbl.getSd().getLocation());
           if (!wh.isWritable(tblPath.getParent())) {
-            throw new MetaException("Table metadata not deleted since " +
+            String target = indexName == null ? "Table" : "Index table";
+            throw new MetaException(target + " metadata not deleted since " +
                 tblPath.getParent() + " is not writable by " +
                 hiveConf.getUser());
           }
@@ -1487,17 +1492,17 @@ private void drop_table_core(final RawStore ms, final String dbname, final Strin
             tbl.getPartitionKeys(), deleteData && !isExternal);
 
         if (!ms.dropTable(dbname, name)) {
-          throw new MetaException("Unable to drop table");
+          String tableName = dbname + "." + name;
+          throw new MetaException(indexName == null ? "Unable to drop table " + tableName:
+              "Unable to drop index table " + tableName + " for index " + indexName);
         }
         success = ms.commitTransaction();
       } finally {
         if (!success) {
           ms.rollbackTransaction();
         } else if (deleteData && !isExternal) {
-          boolean ifPurge = false;
-          if (envContext != null){
-            ifPurge = Boolean.parseBoolean(envContext.getProperties().get("ifPurge"));
-          }
+          boolean ifPurge = envContext != null &&
+              Boolean.parseBoolean(envContext.getProperties().get("ifPurge"));
           // Delete the data in the partitions which have other locations
           deletePartitionData(partPaths, ifPurge);
           // Delete the data in the table
@@ -1510,6 +1515,7 @@ private void drop_table_core(final RawStore ms, final String dbname, final Strin
           listener.onDropTable(dropTableEvent);
         }
       }
+      return success;
     }
 
     /**
@@ -1653,8 +1659,7 @@ public void drop_table_with_environment_context(final String dbname, final Strin
       boolean success = false;
       Exception ex = null;
       try {
-        drop_table_core(getMS(), dbname, name, deleteData, envContext);
-        success = true;
+        success = drop_table_core(getMS(), dbname, name, deleteData, envContext, null);
       } catch (IOException e) {
         ex = e;
         throw new MetaException(e.getMessage());
@@ -3209,7 +3214,12 @@ public void alter_index(final String dbname, final String base_table_name,
 
       boolean success = false;
       Exception ex = null;
+      Index oldIndex = null;
       try {
+        oldIndex = get_index_by_name(dbname, base_table_name, index_name);
+
+        firePreEvent(new PreAlterIndexEvent(oldIndex, newIndex, this));
+
         getMS().alterIndex(dbname, base_table_name, index_name, newIndex);
         success = true;
       } catch (InvalidObjectException e) {
@@ -3226,6 +3236,10 @@ public void alter_index(final String dbname, final String base_table_name,
         }
       } finally {
         endFunction("alter_index", success, ex, base_table_name);
+        for (MetaStoreEventListener listener : listeners) {
+          AlterIndexEvent alterIndexEvent = new AlterIndexEvent(oldIndex, newIndex, success, this);
+          listener.onAlterIndex(alterIndexEvent);
+        }
       }
       return;
     }
@@ -3771,6 +3785,8 @@ private Index add_index_core(final RawStore ms, final Index index, final Table i
 
       try {
         ms.openTransaction();
+        firePreEvent(new PreAddIndexEvent(index, this));
+
         Index old_index = null;
         try {
           old_index = get_index_by_name(index.getDbName(), index
@@ -3818,6 +3834,10 @@ private Index add_index_core(final RawStore ms, final Index index, final Table i
           }
           ms.rollbackTransaction();
         }
+        for (MetaStoreEventListener listener : listeners) {
+          AddIndexEvent addIndexEvent = new AddIndexEvent(index, success, this);
+          listener.onAddIndex(addIndexEvent);
+        }
       }
     }
 
@@ -3852,16 +3872,17 @@ private boolean drop_index_by_name_core(final RawStore ms,
         MetaException, TException, IOException, InvalidObjectException, InvalidInputException {
 
       boolean success = false;
+      Index index = null;
       Path tblPath = null;
       List<Path> partPaths = null;
       try {
         ms.openTransaction();
 
         // drop the underlying index table
-        Index index = get_index_by_name(dbName, tblName, indexName);
-        if (index == null) {
-          throw new NoSuchObjectException(indexName + " doesn't exist");
-        }
+        index = get_index_by_name(dbName, tblName, indexName);  // throws exception if not exists
+
+        firePreEvent(new PreDropIndexEvent(index, this));
+
         ms.dropIndex(dbName, tblName, indexName);
 
         String idxTblName = index.getIndexTableName();
@@ -3882,26 +3903,29 @@ private boolean drop_index_by_name_core(final RawStore ms,
           }
 
           // Drop the partitions and get a list of partition locations which need to be deleted
-          partPaths = dropPartitionsAndGetLocations(ms, dbName, idxTblName, tblPath,
+          partPaths = dropPartitionsAndGetLocations(ms, qualified[0], qualified[1], tblPath,
               tbl.getPartitionKeys(), deleteData);
 
-          if (!ms.dropTable(dbName, idxTblName)) {
+          if (!ms.dropTable(qualified[0], qualified[1])) {
             throw new MetaException("Unable to drop underlying data table "
-                + idxTblName + " for index " + idxTblName);
+                + qualified[0] + "." + qualified[1] + " for index " + indexName);
           }
         }
         success = ms.commitTransaction();
       } finally {
         if (!success) {
           ms.rollbackTransaction();
-          return false;
         } else if (deleteData && tblPath != null) {
           deletePartitionData(partPaths);
           deleteTableData(tblPath);
           // ok even if the data is not deleted
         }
+        for (MetaStoreEventListener listener : listeners) {
+          DropIndexEvent dropIndexEvent = new DropIndexEvent(index, success, this);
+          listener.onDropIndex(dropIndexEvent);
+        }
       }
-      return true;
+      return success;
     }
 
     @Override
@@ -3920,7 +3944,7 @@ public Index get_index_by_name(final String dbName, final String tblName,
         ex = e;
         rethrowException(e);
       } finally {
-        endFunction("drop_index_by_name", ret != null, ex, tblName);
+        endFunction("get_index_by_name", ret != null, ex, tblName);
       }
       return ret;
     }
@@ -5746,7 +5770,7 @@ public static void startMetaStore(int port, HadoopThriftAuthBridge bridge,
     try {
       isMetaStoreRemote = true;
       // Server will create new threads up to max as necessary. After an idle
-      // period, it will destory threads to keep the number of threads in the
+      // period, it will destroy threads to keep the number of threads in the
       // pool to min.
       int minWorkerThreads = conf.getIntVar(HiveConf.ConfVars.METASTORESERVERMINTHREADS);
       int maxWorkerThreads = conf.getIntVar(HiveConf.ConfVars.METASTORESERVERMAXTHREADS);
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreEventListener.java b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreEventListener.java
index ec1dca22b2..792ef422ca 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreEventListener.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreEventListener.java
@@ -21,6 +21,8 @@
 import org.apache.hadoop.conf.Configurable;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.events.AddIndexEvent;
+import org.apache.hadoop.hive.metastore.events.AlterIndexEvent;
 import org.apache.hadoop.hive.metastore.events.AddPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.AlterPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.AlterTableEvent;
@@ -28,6 +30,7 @@
 import org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent;
 import org.apache.hadoop.hive.metastore.events.CreateTableEvent;
 import org.apache.hadoop.hive.metastore.events.DropDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.DropIndexEvent;
 import org.apache.hadoop.hive.metastore.events.DropPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.DropTableEvent;
 import org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent;
@@ -117,7 +120,27 @@ public void onDropDatabase (DropDatabaseEvent dbEvent) throws MetaException {
    * @throws MetaException
    */
   public void onLoadPartitionDone(LoadPartitionDoneEvent partSetDoneEvent) throws MetaException {
+  }
+
+  /**
+   * @param indexEvent index event
+   * @throws MetaException
+   */
+  public void onAddIndex(AddIndexEvent indexEvent) throws MetaException {
+  }
+
+  /**
+   * @param indexEvent index event
+   * @throws MetaException
+   */
+  public void onDropIndex(DropIndexEvent indexEvent) throws MetaException {
+  }
 
+  /**
+   * @param indexEvent index event
+   * @throws MetaException
+   */
+  public void onAlterIndex(AlterIndexEvent indexEvent) throws MetaException {
   }
 
   @Override
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index 637a39a569..004966d336 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -64,7 +64,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.api.AggrStats;
-import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
 import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
 import org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc;
 import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;
@@ -92,7 +91,6 @@
 import org.apache.hadoop.hive.metastore.api.ResourceUri;
 import org.apache.hadoop.hive.metastore.api.Role;
 import org.apache.hadoop.hive.metastore.api.SerDeInfo;
-import org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest;
 import org.apache.hadoop.hive.metastore.api.SkewedInfo;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.metastore.api.Table;
@@ -197,7 +195,7 @@ public Configuration getConf() {
   }
 
   /**
-   * Called whenever this object is instantiated using ReflectionUils, and also
+   * Called whenever this object is instantiated using ReflectionUtils, and also
    * on connection retries. In cases of connection retries, conf will usually
    * contain modified values.
    */
@@ -2665,7 +2663,7 @@ public void alterTable(String dbname, String name, Table newTable)
         throw new MetaException("table " + name + " doesn't exist");
       }
 
-      // For now only alter name, owner, paramters, cols, bucketcols are allowed
+      // For now only alter name, owner, parameters, cols, bucketcols are allowed
       oldt.setDatabase(newt.getDatabase());
       oldt.setTableName(newt.getTableName().toLowerCase());
       oldt.setParameters(newt.getParameters());
@@ -2708,7 +2706,7 @@ public void alterIndex(String dbname, String baseTblName, String name, Index new
         throw new MetaException("index " + name + " doesn't exist");
       }
 
-      // For now only alter paramters are allowed
+      // For now only alter parameters are allowed
       oldi.setParameters(newi.getParameters());
 
       // commit the changes
@@ -2878,7 +2876,7 @@ private void preDropStorageDescriptor(MStorageDescriptor msd) {
     MColumnDescriptor mcd = msd.getCD();
     // Because there is a 1-N relationship between CDs and SDs,
     // we must set the SD's CD to null first before dropping the storage descriptor
-    // to satisfy foriegn key constraints.
+    // to satisfy foreign key constraints.
     msd.setCD(null);
     removeUnusedColumnDescriptor(mcd);
   }
@@ -3019,19 +3017,26 @@ public Index getIndex(String dbName, String origTableName, String indexName)
   }
 
   private Index convertToIndex(MIndex mIndex) throws MetaException {
-    if(mIndex == null) {
+    if (mIndex == null) {
       return null;
     }
 
+    MTable origTable = mIndex.getOrigTable();
+    MTable indexTable = mIndex.getIndexTable();
+
+    String[] qualified = MetaStoreUtils.getQualifiedName(
+        origTable.getDatabase().getName(), indexTable.getTableName());
+    String indexTableName = qualified[0] + "." + qualified[1];
+
     return new Index(
     mIndex.getIndexName(),
     mIndex.getIndexHandlerClass(),
-    mIndex.getOrigTable().getDatabase().getName(),
-    mIndex.getOrigTable().getTableName(),
+    origTable.getDatabase().getName(),
+    origTable.getTableName(),
     mIndex.getCreateTime(),
     mIndex.getLastAccessTime(),
-    mIndex.getIndexTable().getTableName(),
-    this.convertToStorageDescriptor(mIndex.getSd()),
+    indexTableName,
+    convertToStorageDescriptor(mIndex.getSd()),
     mIndex.getParameters(),
     mIndex.getDeferredRebuild());
 
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/events/AddIndexEvent.java b/metastore/src/java/org/apache/hadoop/hive/metastore/events/AddIndexEvent.java
new file mode 100644
index 0000000000..43ac0aa68d
--- /dev/null
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/events/AddIndexEvent.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore.events;
+
+import org.apache.hadoop.hive.metastore.HiveMetaStore;
+import org.apache.hadoop.hive.metastore.api.Index;
+
+public class AddIndexEvent extends ListenerEvent {
+
+  private final Index index;
+
+  public AddIndexEvent(Index index, boolean status, HiveMetaStore.HMSHandler handler) {
+    super(status, handler);
+    this.index = index;
+  }
+
+  public Index getIndex() {
+    return index;
+  }
+}
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/events/AlterIndexEvent.java b/metastore/src/java/org/apache/hadoop/hive/metastore/events/AlterIndexEvent.java
new file mode 100644
index 0000000000..4a49700455
--- /dev/null
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/events/AlterIndexEvent.java
@@ -0,0 +1,43 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore.events;
+
+import org.apache.hadoop.hive.metastore.HiveMetaStore;
+import org.apache.hadoop.hive.metastore.api.Index;
+
+public class AlterIndexEvent extends ListenerEvent {
+
+  private final Index newIndex;
+  private final Index oldIndex;
+
+  public AlterIndexEvent(Index oldIndex, Index newIndex, boolean status,
+      HiveMetaStore.HMSHandler handler) {
+    super(status, handler);
+    this.oldIndex = oldIndex;
+    this.newIndex = newIndex;
+  }
+
+  public Index getOldIndex() {
+    return oldIndex;
+  }
+
+  public Index getNewIndex() {
+    return newIndex;
+  }
+}
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/events/DropIndexEvent.java b/metastore/src/java/org/apache/hadoop/hive/metastore/events/DropIndexEvent.java
new file mode 100644
index 0000000000..06f2302a81
--- /dev/null
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/events/DropIndexEvent.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore.events;
+
+import org.apache.hadoop.hive.metastore.HiveMetaStore;
+import org.apache.hadoop.hive.metastore.api.Index;
+
+public class DropIndexEvent extends ListenerEvent {
+
+  private final Index index;
+
+  public DropIndexEvent(Index index, boolean status, HiveMetaStore.HMSHandler handler) {
+    super(status, handler);
+    this.index = index;
+  }
+
+  public Index getIndex() {
+    return index;
+  }
+}
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddIndexEvent.java b/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddIndexEvent.java
new file mode 100644
index 0000000000..baa04a5866
--- /dev/null
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAddIndexEvent.java
@@ -0,0 +1,37 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore.events;
+
+import org.apache.hadoop.hive.metastore.HiveMetaStore;
+import org.apache.hadoop.hive.metastore.api.Index;
+
+public class PreAddIndexEvent extends PreEventContext {
+
+private final Index table;
+
+  public PreAddIndexEvent(Index table, HiveMetaStore.HMSHandler handler) {
+    super(PreEventType.ADD_INDEX, handler);
+    this.table = table;
+  }
+
+  public Index getIndex() {
+    return table;
+  }
+}
+
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAlterIndexEvent.java b/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAlterIndexEvent.java
new file mode 100644
index 0000000000..97cf305671
--- /dev/null
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreAlterIndexEvent.java
@@ -0,0 +1,43 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore.events;
+
+import org.apache.hadoop.hive.metastore.HiveMetaStore;
+import org.apache.hadoop.hive.metastore.api.Index;
+
+public class PreAlterIndexEvent extends PreEventContext {
+
+  private final Index newIndex;
+  private final Index oldIndex;
+
+  public PreAlterIndexEvent(Index oldIndex, Index newIndex, HiveMetaStore.HMSHandler handler) {
+    super(PreEventType.ALTER_INDEX, handler);
+    this.oldIndex = oldIndex;
+    this.newIndex = newIndex;
+  }
+
+  public Index getOldIndex() {
+    return oldIndex;
+  }
+
+  public Index getNewIndex() {
+    return newIndex;
+  }
+}
+
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreDropIndexEvent.java b/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreDropIndexEvent.java
new file mode 100644
index 0000000000..437e5c1468
--- /dev/null
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreDropIndexEvent.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore.events;
+
+import org.apache.hadoop.hive.metastore.HiveMetaStore;
+import org.apache.hadoop.hive.metastore.api.Index;
+
+public class PreDropIndexEvent extends PreEventContext {
+
+  private final Index index;
+
+  public PreDropIndexEvent(Index index, HiveMetaStore.HMSHandler handler) {
+    super(PreEventType.DROP_INDEX, handler);
+    this.index = index;
+  }
+
+  public Index getIndex() {
+    return index;
+  }
+}
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreEventContext.java b/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreEventContext.java
index dbc3247ea0..ee24a35f96 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreEventContext.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/events/PreEventContext.java
@@ -39,7 +39,10 @@ public static enum PreEventType {
     LOAD_PARTITION_DONE,
     AUTHORIZATION_API_CALL,
     READ_TABLE,
-    READ_DATABASE
+    READ_DATABASE,
+    ADD_INDEX,
+    ALTER_INDEX,
+    DROP_INDEX
   }
 
   private final PreEventType eventType;
diff --git a/metastore/src/test/org/apache/hadoop/hive/metastore/DummyListener.java b/metastore/src/test/org/apache/hadoop/hive/metastore/DummyListener.java
index 37daf52cd7..a3b16d049c 100644
--- a/metastore/src/test/org/apache/hadoop/hive/metastore/DummyListener.java
+++ b/metastore/src/test/org/apache/hadoop/hive/metastore/DummyListener.java
@@ -23,13 +23,16 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.events.AddIndexEvent;
 import org.apache.hadoop.hive.metastore.events.AddPartitionEvent;
+import org.apache.hadoop.hive.metastore.events.AlterIndexEvent;
 import org.apache.hadoop.hive.metastore.events.AlterPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.AlterTableEvent;
 import org.apache.hadoop.hive.metastore.events.ConfigChangeEvent;
 import org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent;
 import org.apache.hadoop.hive.metastore.events.CreateTableEvent;
 import org.apache.hadoop.hive.metastore.events.DropDatabaseEvent;
+import org.apache.hadoop.hive.metastore.events.DropIndexEvent;
 import org.apache.hadoop.hive.metastore.events.DropPartitionEvent;
 import org.apache.hadoop.hive.metastore.events.DropTableEvent;
 import org.apache.hadoop.hive.metastore.events.ListenerEvent;
@@ -60,52 +63,70 @@ public DummyListener(Configuration config) {
 
   @Override
   public void onConfigChange(ConfigChangeEvent configChange) {
-    notifyList.add(configChange);
+    addEvent(configChange);
   }
 
   @Override
   public void onAddPartition(AddPartitionEvent partition) throws MetaException {
-    notifyList.add(partition);
+    addEvent(partition);
   }
 
   @Override
   public void onCreateDatabase(CreateDatabaseEvent db) throws MetaException {
-    notifyList.add(db);
+    addEvent(db);
   }
 
   @Override
   public void onCreateTable(CreateTableEvent table) throws MetaException {
-    notifyList.add(table);
+    addEvent(table);
   }
 
   @Override
   public void onDropDatabase(DropDatabaseEvent db) throws MetaException {
-    notifyList.add(db);
+    addEvent(db);
   }
 
   @Override
   public void onDropPartition(DropPartitionEvent partition) throws MetaException {
-    notifyList.add(partition);
+    addEvent(partition);
   }
 
   @Override
   public void onDropTable(DropTableEvent table) throws MetaException {
-    notifyList.add(table);
+    addEvent(table);
   }
 
   @Override
   public void onAlterTable(AlterTableEvent event) throws MetaException {
-    notifyList.add(event);
+    addEvent(event);
   }
 
   @Override
   public void onAlterPartition(AlterPartitionEvent event) throws MetaException {
-    notifyList.add(event);
+    addEvent(event);
   }
 
   @Override
   public void onLoadPartitionDone(LoadPartitionDoneEvent partEvent) throws MetaException {
-    notifyList.add(partEvent);
+    addEvent(partEvent);
   }
 
+  @Override
+  public void onAddIndex(AddIndexEvent indexEvent) throws MetaException {
+    addEvent(indexEvent);
+  }
+
+  @Override
+  public void onDropIndex(DropIndexEvent indexEvent) throws MetaException {
+    addEvent(indexEvent);
+  }
+
+  @Override
+  public void onAlterIndex(AlterIndexEvent indexEvent) throws MetaException {
+    addEvent(indexEvent);
+  }
+
+  private void addEvent(ListenerEvent event) {
+    notifyList.add(event);
+  }
 }
diff --git a/metastore/src/test/org/apache/hadoop/hive/metastore/DummyPreListener.java b/metastore/src/test/org/apache/hadoop/hive/metastore/DummyPreListener.java
index 9a943b2b2e..7ff6f92952 100644
--- a/metastore/src/test/org/apache/hadoop/hive/metastore/DummyPreListener.java
+++ b/metastore/src/test/org/apache/hadoop/hive/metastore/DummyPreListener.java
@@ -30,7 +30,7 @@
  *
  * DummyPreListener.
  *
- * An implemenation of MetaStorePreEventListener which stores the Events it's seen in a list.
+ * An implementation of MetaStorePreEventListener which stores the Events it's seen in a list.
  */
 public class DummyPreListener extends MetaStorePreEventListener {
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
index 0ef0e7e7e9..3a6eb396f8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
@@ -156,7 +156,6 @@
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
-import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe;
 import org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
@@ -781,7 +780,7 @@ private int alterDatabase(AlterDatabaseDesc alterDbDesc) throws HiveException {
   }
 
   private int dropIndex(Hive db, DropIndexDesc dropIdx) throws HiveException {
-    db.dropIndex(dropIdx.getTableName(), dropIdx.getIndexName(), true);
+    db.dropIndex(dropIdx.getTableName(), dropIdx.getIndexName(), dropIdx.isThrowException(), true);
     return 0;
   }
 
@@ -2204,7 +2203,7 @@ private int showDatabases(Hive db, ShowDatabasesDesc showDatabasesDesc) throws H
    *           Throws this exception if an unexpected error occurs.
    */
   private int showTables(Hive db, ShowTablesDesc showTbls) throws HiveException {
-    // get the tables for the desired pattenn - populate the output stream
+    // get the tables for the desired pattern - populate the output stream
     List<String> tbls = null;
     String dbName = showTbls.getDbName();
 
@@ -3915,7 +3914,7 @@ private int createTable(Hive db, CreateTableDesc crtTbl) throws HiveException {
     tbl.setInputFormatClass(crtTbl.getInputFormat());
     tbl.setOutputFormatClass(crtTbl.getOutputFormat());
 
-    // only persist input/ouput format to metadata when it is explicitly specified.
+    // only persist input/output format to metadata when it is explicitly specified.
     // Otherwise, load lazily via StorageHandler at query time.
     if (crtTbl.getInputFormat() != null && !crtTbl.getInputFormat().isEmpty()) {
       tbl.getTTable().getSd().setInputFormat(tbl.getInputFormatClass().getName());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
index cbbe78139e..b90062704e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
@@ -313,7 +313,7 @@ public void dropDatabase(String name, boolean deleteData, boolean ignoreUnknownD
    * @param name
    * @param deleteData
    * @param ignoreUnknownDb if true, will ignore NoSuchObjectException
-   * @param cascade           if true, delete all tables on the DB if exists. Othewise, the query
+   * @param cascade         if true, delete all tables on the DB if exists. Otherwise, the query
    *                        will fail if table still exists.
    * @throws HiveException
    * @throws NoSuchObjectException
@@ -331,7 +331,7 @@ public void dropDatabase(String name, boolean deleteData, boolean ignoreUnknownD
 
 
   /**
-   * Creates a table metdata and the directory for the table data
+   * Creates a table metadata and the directory for the table data
    *
    * @param tableName
    *          name of the table
@@ -355,7 +355,7 @@ public void createTable(String tableName, List<String> columns,
   }
 
   /**
-   * Creates a table metdata and the directory for the table data
+   * Creates a table metadata and the directory for the table data
    *
    * @param tableName
    *          name of the table
@@ -885,16 +885,21 @@ public Index getIndex(String dbName, String baseTableName,
     }
   }
 
-  public boolean dropIndex(String baseTableName, String index_name, boolean deleteData) throws HiveException {
+  public boolean dropIndex(String baseTableName, String index_name,
+      boolean throwException, boolean deleteData) throws HiveException {
     String[] names = Utilities.getDbTableName(baseTableName);
-    return dropIndex(names[0], names[1], index_name, deleteData);
+    return dropIndex(names[0], names[1], index_name, throwException, deleteData);
   }
 
-  public boolean dropIndex(String db_name, String tbl_name, String index_name, boolean deleteData) throws HiveException {
+  public boolean dropIndex(String db_name, String tbl_name, String index_name,
+      boolean throwException, boolean deleteData) throws HiveException {
     try {
       return getMSC().dropIndex(db_name, tbl_name, index_name, deleteData);
     } catch (NoSuchObjectException e) {
-      throw new HiveException("Partition or table doesn't exist. " + e.getMessage(), e);
+      if (throwException) {
+        throw new HiveException("Index " + index_name + " doesn't exist. ", e);
+      }
+      return false;
     } catch (Exception e) {
       throw new HiveException(e.getMessage(), e);
     }
@@ -2861,10 +2866,6 @@ public GetOpenTxnsInfoResponse showTransactions() throws HiveException {
     }
   }
 
-  public static String[] getQualifiedNames(String qualifiedName) {
-    return qualifiedName.split("\\.");
-  }
-
   public void createFunction(Function func) throws HiveException {
     try {
       getMSC().createFunction(func);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 7af34ce5a4..5b8f3015bb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -53,6 +53,7 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Index;
 import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.metastore.api.SkewedInfo;
 import org.apache.hadoop.hive.ql.Driver;
@@ -214,24 +215,6 @@ public static String getTypeName(ASTNode node) throws SemanticException {
     return typeName;
   }
 
-  static class TablePartition {
-    String tableName;
-    HashMap<String, String> partSpec = null;
-
-    public TablePartition() {
-    }
-
-    public TablePartition(ASTNode tblPart) throws SemanticException {
-      tableName = getDotName((getQualifiedTableName((ASTNode) tblPart.getChild(0))));
-      if (tblPart.getChildCount() > 1) {
-        ASTNode part = (ASTNode) tblPart.getChild(1);
-        if (part.getToken().getType() == HiveParser.TOK_PARTSPEC) {
-          this.partSpec = DDLSemanticAnalyzer.getPartSpec(part);
-        }
-      }
-    }
-  }
-
   public DDLSemanticAnalyzer(HiveConf conf) throws SemanticException {
     this(conf, createHiveDB(conf));
   }
@@ -1139,20 +1122,25 @@ private void analyzeDropIndex(ASTNode ast) throws SemanticException {
     // configured not to ignore this
     boolean throwException =
         !ifExists && !HiveConf.getBoolVar(conf, ConfVars.DROPIGNORESNONEXISTENT);
-    if (throwException) {
-      try {
-        Index idx = db.getIndex(tableName, indexName);
-      } catch (HiveException e) {
+    Table tbl = getTable(tableName, false);
+    if (throwException && tbl == null) {
+      throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tableName));
+    }
+    try {
+      Index idx = db.getIndex(tableName, indexName);
+    } catch (HiveException e) {
+      if (!(e.getCause() instanceof NoSuchObjectException)) {
+        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg("dropping index"), e);
+      }
+      if (throwException) {
         throw new SemanticException(ErrorMsg.INVALID_INDEX.getMsg(indexName));
       }
     }
-
-    Table tbl = getTable(tableName, false);
     if (tbl != null) {
-      inputs.add(new ReadEntity(getTable(tableName)));
+      inputs.add(new ReadEntity(tbl));
     }
 
-    DropIndexDesc dropIdxDesc = new DropIndexDesc(indexName, tableName);
+    DropIndexDesc dropIdxDesc = new DropIndexDesc(indexName, tableName, throwException);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         dropIdxDesc), conf));
   }
@@ -2809,7 +2797,7 @@ private Partition getPartitionForOutput(Table tab, Map<String, String> currentPa
    * @param ast
    *          The parsed command tree.
    * @throws SemanticException
-   *           Parsin failed
+   *           Parsing failed
    */
   private void analyzeAlterTableTouch(String[] qualified, CommonTree ast)
       throws SemanticException {
@@ -2933,8 +2921,8 @@ private List<Map<String, String>> getPartitionSpecs(CommonTree ast)
    *
    * @param ast Tree to extract partitions from.
    * @param tab Table.
-   * @param result Map of partitions by prefix length. Most of the time prefix length will
-   *               be the same for all partition specs, so we can just OR the expressions.
+   * @return    Map of partitions by prefix length. Most of the time prefix length will
+   *            be the same for all partition specs, so we can just OR the expressions.
    */
   private Map<Integer, List<ExprNodeGenericFuncDesc>> getFullPartitionSpecs(
       CommonTree ast, Table tab, boolean canGroupExprs) throws SemanticException {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/DropIndexDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/DropIndexDesc.java
index 5ef14efa94..0d5844aea7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/DropIndexDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/DropIndexDesc.java
@@ -24,15 +24,17 @@ public class DropIndexDesc {
   private String indexName;
   
   private String tableName;
+
+  private boolean throwException;
   
   /**
    * @param indexName
    * @param tableName
    */
-  public DropIndexDesc(String indexName, String tableName) {
-    super();
+  public DropIndexDesc(String indexName, String tableName, boolean throwException) {
     this.indexName = indexName;
     this.tableName = tableName;
+    this.throwException = throwException;
   }
 
   /**
@@ -63,4 +65,11 @@ public void setTableName(String tableName) {
     this.tableName = tableName;
   }
 
+  public boolean isThrowException() {
+    return throwException;
+  }
+
+  public void setThrowException(boolean throwException) {
+    this.throwException = throwException;
+  }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHive.java b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHive.java
index 153908cc53..da7d082902 100755
--- a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHive.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHive.java
@@ -550,7 +550,7 @@ public void testIndex() throws Throwable {
             index.getIndexName());
         assertEquals("Table names don't match for index: " + indexName, tableName,
             index.getOrigTableName());
-        assertEquals("Index table names didn't match for index: " + indexName, indexTableName,
+        assertEquals("Index table names didn't match for index: " + indexName, qIndexTableName,
             index.getIndexTableName());
         assertEquals("Index handler classes didn't match for index: " + indexName,
             indexHandlerClass, index.getIndexHandlerClass());
@@ -564,7 +564,7 @@ public void testIndex() throws Throwable {
 
       // Drop index
       try {
-        hm.dropIndex(MetaStoreUtils.DEFAULT_DATABASE_NAME, tableName, indexName, true);
+        hm.dropIndex(MetaStoreUtils.DEFAULT_DATABASE_NAME, tableName, indexName, false, true);
       } catch (HiveException e) {
         System.err.println(StringUtils.stringifyException(e));
         assertTrue("Unable to drop index: " + indexName, false);
diff --git a/ql/src/test/queries/clientpositive/drop_index.q b/ql/src/test/queries/clientpositive/drop_index.q
index 54ef823c9c..e03856c74d 100644
--- a/ql/src/test/queries/clientpositive/drop_index.q
+++ b/ql/src/test/queries/clientpositive/drop_index.q
@@ -1,3 +1,2 @@
-SET hive.exec.drop.ignorenonexistent=false;
 DROP INDEX IF EXISTS UnknownIndex ON src;
 DROP INDEX IF EXISTS UnknownIndex ON UnknownTable;
diff --git a/ql/src/test/results/clientnegative/alter_concatenate_indexed_table.q.out b/ql/src/test/results/clientnegative/alter_concatenate_indexed_table.q.out
index 19fbf51a76..7461ba322e 100644
--- a/ql/src/test/results/clientnegative/alter_concatenate_indexed_table.q.out
+++ b/ql/src/test/results/clientnegative/alter_concatenate_indexed_table.q.out
@@ -76,5 +76,5 @@ PREHOOK: query: show indexes on src_rc_concatenate_test
 PREHOOK: type: SHOWINDEXES
 POSTHOOK: query: show indexes on src_rc_concatenate_test
 POSTHOOK: type: SHOWINDEXES
-src_rc_concatenate_test_index	src_rc_concatenate_test	key                 	default__src_rc_concatenate_test_src_rc_concatenate_test_index__	compact             	
+src_rc_concatenate_test_index	src_rc_concatenate_test	key                 	default.default__src_rc_concatenate_test_src_rc_concatenate_test_index__	compact             	
 FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: can not do merge because source table default.src_rc_concatenate_test is indexed.
diff --git a/ql/src/test/results/clientpositive/alter_concatenate_indexed_table.q.out b/ql/src/test/results/clientpositive/alter_concatenate_indexed_table.q.out
index ffcbcf9d89..7f8458d68b 100644
--- a/ql/src/test/results/clientpositive/alter_concatenate_indexed_table.q.out
+++ b/ql/src/test/results/clientpositive/alter_concatenate_indexed_table.q.out
@@ -76,7 +76,7 @@ PREHOOK: query: show indexes on src_rc_concatenate_test
 PREHOOK: type: SHOWINDEXES
 POSTHOOK: query: show indexes on src_rc_concatenate_test
 POSTHOOK: type: SHOWINDEXES
-src_rc_concatenate_test_index	src_rc_concatenate_test	key                 	default__src_rc_concatenate_test_src_rc_concatenate_test_index__	compact             	
+src_rc_concatenate_test_index	src_rc_concatenate_test	key                 	default.default__src_rc_concatenate_test_src_rc_concatenate_test_index__	compact             	
 PREHOOK: query: alter table src_rc_concatenate_test concatenate
 PREHOOK: type: ALTER_TABLE_MERGE
 PREHOOK: Input: default@src_rc_concatenate_test
@@ -215,7 +215,7 @@ PREHOOK: query: show indexes on src_rc_concatenate_test_part
 PREHOOK: type: SHOWINDEXES
 POSTHOOK: query: show indexes on src_rc_concatenate_test_part
 POSTHOOK: type: SHOWINDEXES
-src_rc_concatenate_test_part_index	src_rc_concatenate_test_part	key                 	default__src_rc_concatenate_test_part_src_rc_concatenate_test_part_index__	compact             	
+src_rc_concatenate_test_part_index	src_rc_concatenate_test_part	key                 	default.default__src_rc_concatenate_test_part_src_rc_concatenate_test_part_index__	compact             	
 PREHOOK: query: alter table src_rc_concatenate_test_part partition (ds='2011') concatenate
 PREHOOK: type: ALTER_PARTITION_MERGE
 PREHOOK: Input: default@src_rc_concatenate_test_part
diff --git a/ql/src/test/results/clientpositive/index_auth.q.out b/ql/src/test/results/clientpositive/index_auth.q.out
index 385b639f92..39a1096902 100644
--- a/ql/src/test/results/clientpositive/index_auth.q.out
+++ b/ql/src/test/results/clientpositive/index_auth.q.out
@@ -24,7 +24,7 @@ PREHOOK: query: SHOW INDEXES ON foobar
 PREHOOK: type: SHOWINDEXES
 POSTHOOK: query: SHOW INDEXES ON foobar
 POSTHOOK: type: SHOWINDEXES
-srcpart_auth_index  	foobar              	key                 	default__foobar_srcpart_auth_index__	bitmap              	
+srcpart_auth_index  	foobar              	key                 	default.default__foobar_srcpart_auth_index__	bitmap              	
 PREHOOK: query: grant select on table foobar to user hive_test_user
 PREHOOK: type: GRANT_PRIVILEGE
 PREHOOK: Output: default@foobar
diff --git a/ql/src/test/results/clientpositive/index_auto.q.out b/ql/src/test/results/clientpositive/index_auto.q.out
index d1f5fa2139..124a9d20fe 100644
--- a/ql/src/test/results/clientpositive/index_auto.q.out
+++ b/ql/src/test/results/clientpositive/index_auto.q.out
@@ -146,7 +146,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_index__
+            alias: default.default__src_src_index__
             filterExpr: ((key > 80) and (key < 100)) (type: boolean)
             Filter Operator
               predicate: ((key > 80) and (key < 100)) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_auto_file_format.q.out b/ql/src/test/results/clientpositive/index_auto_file_format.q.out
index eca656f62d..85a57f346d 100644
--- a/ql/src/test/results/clientpositive/index_auto_file_format.q.out
+++ b/ql/src/test/results/clientpositive/index_auto_file_format.q.out
@@ -40,7 +40,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_index__
+            alias: default.default__src_src_index__
             filterExpr: (key = 86) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -156,7 +156,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_index__
+            alias: default.default__src_src_index__
             filterExpr: (key = 86) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_auto_mult_tables.q.out b/ql/src/test/results/clientpositive/index_auto_mult_tables.q.out
index ab0f06f072..e5dbf7b7ce 100644
--- a/ql/src/test/results/clientpositive/index_auto_mult_tables.q.out
+++ b/ql/src/test/results/clientpositive/index_auto_mult_tables.q.out
@@ -203,7 +203,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__srcpart_srcpart_index__
+            alias: default.default__srcpart_srcpart_index__
             filterExpr: (((((key > 70) and (key < 90)) and (key > 80)) and (key < 100)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
             Filter Operator
               predicate: (((((key > 70) and (key < 90)) and (key > 80)) and (key < 100)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
@@ -295,7 +295,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_index__
+            alias: default.default__src_src_index__
             filterExpr: (((((key > 80) and (key < 100)) and (key > 70)) and (key < 90)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
             Filter Operator
               predicate: (((((key > 80) and (key < 100)) and (key > 70)) and (key < 90)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_auto_mult_tables_compact.q.out b/ql/src/test/results/clientpositive/index_auto_mult_tables_compact.q.out
index 20fa238b14..5f767e1a1d 100644
--- a/ql/src/test/results/clientpositive/index_auto_mult_tables_compact.q.out
+++ b/ql/src/test/results/clientpositive/index_auto_mult_tables_compact.q.out
@@ -213,7 +213,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__srcpart_srcpart_index__
+            alias: default.default__srcpart_srcpart_index__
             filterExpr: ((((key > 70) and (key < 90)) and (key > 80)) and (key < 100)) (type: boolean)
             Filter Operator
               predicate: ((((key > 70) and (key < 90)) and (key > 80)) and (key < 100)) (type: boolean)
@@ -323,7 +323,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_index__
+            alias: default.default__src_src_index__
             filterExpr: ((((key > 80) and (key < 100)) and (key > 70)) and (key < 90)) (type: boolean)
             Filter Operator
               predicate: ((((key > 80) and (key < 100)) and (key > 70)) and (key < 90)) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_auto_multiple.q.out b/ql/src/test/results/clientpositive/index_auto_multiple.q.out
index 2408d481d0..722ee6e4c9 100644
--- a/ql/src/test/results/clientpositive/index_auto_multiple.q.out
+++ b/ql/src/test/results/clientpositive/index_auto_multiple.q.out
@@ -60,7 +60,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_key_index__
+            alias: default.default__src_src_key_index__
             filterExpr: (key = 86) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_auto_partitioned.q.out b/ql/src/test/results/clientpositive/index_auto_partitioned.q.out
index da57bea59e..4777c53ceb 100644
--- a/ql/src/test/results/clientpositive/index_auto_partitioned.q.out
+++ b/ql/src/test/results/clientpositive/index_auto_partitioned.q.out
@@ -54,7 +54,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__srcpart_src_part_index__
+            alias: default.default__srcpart_src_part_index__
             filterExpr: ((key = 86) and (ds = '2008-04-09')) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_auto_self_join.q.out b/ql/src/test/results/clientpositive/index_auto_self_join.q.out
index 94ded26844..2bc80f462e 100644
--- a/ql/src/test/results/clientpositive/index_auto_self_join.q.out
+++ b/ql/src/test/results/clientpositive/index_auto_self_join.q.out
@@ -123,7 +123,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_index__
+            alias: default.default__src_src_index__
             filterExpr: (((key > 70) and (key < 90)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
             Filter Operator
               predicate: (((key > 70) and (key < 90)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
@@ -216,7 +216,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_index__
+            alias: default.default__src_src_index__
             filterExpr: (((key > 80) and (key < 100)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
             Filter Operator
               predicate: (((key > 80) and (key < 100)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_auto_update.q.out b/ql/src/test/results/clientpositive/index_auto_update.q.out
index 85f26ac963..0f996b5c69 100644
--- a/ql/src/test/results/clientpositive/index_auto_update.q.out
+++ b/ql/src/test/results/clientpositive/index_auto_update.q.out
@@ -222,7 +222,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__temp_temp_index__
+            alias: default.default__temp_temp_index__
             filterExpr: (key = 86) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_bitmap_auto_partitioned.q.out b/ql/src/test/results/clientpositive/index_bitmap_auto_partitioned.q.out
index 473df0a50d..052acbbbea 100644
--- a/ql/src/test/results/clientpositive/index_bitmap_auto_partitioned.q.out
+++ b/ql/src/test/results/clientpositive/index_bitmap_auto_partitioned.q.out
@@ -52,7 +52,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__srcpart_src_part_index__
+            alias: default.default__srcpart_src_part_index__
             filterExpr: ((key = 86) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
             Filter Operator
               predicate: ((key = 86) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_bitmap_compression.q.out b/ql/src/test/results/clientpositive/index_bitmap_compression.q.out
index b5a172f20b..f1362590a9 100644
--- a/ql/src/test/results/clientpositive/index_bitmap_compression.q.out
+++ b/ql/src/test/results/clientpositive/index_bitmap_compression.q.out
@@ -38,7 +38,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_index__
+            alias: default.default__src_src_index__
             filterExpr: (((key > 80) and (key < 100)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
             Filter Operator
               predicate: (((key > 80) and (key < 100)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_compression.q.out b/ql/src/test/results/clientpositive/index_compression.q.out
index 241c2e9c03..b44bce89c3 100644
--- a/ql/src/test/results/clientpositive/index_compression.q.out
+++ b/ql/src/test/results/clientpositive/index_compression.q.out
@@ -42,7 +42,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_src_index__
+            alias: default.default__src_src_index__
             filterExpr: ((key > 80) and (key < 100)) (type: boolean)
             Filter Operator
               predicate: ((key > 80) and (key < 100)) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/index_serde.q.out b/ql/src/test/results/clientpositive/index_serde.q.out
index 5b4cbfcc53..c0fc7e529a 100644
--- a/ql/src/test/results/clientpositive/index_serde.q.out
+++ b/ql/src/test/results/clientpositive/index_serde.q.out
@@ -134,7 +134,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__doctors_doctors_index__
+            alias: default.default__doctors_doctors_index__
             filterExpr: (number > 6) (type: boolean)
             Filter Operator
               predicate: (number > 6) (type: boolean)
diff --git a/ql/src/test/results/clientpositive/ql_rewrite_gbtoidx.q.out b/ql/src/test/results/clientpositive/ql_rewrite_gbtoidx.q.out
index f7da06c1ce..65b40caa02 100644
--- a/ql/src/test/results/clientpositive/ql_rewrite_gbtoidx.q.out
+++ b/ql/src/test/results/clientpositive/ql_rewrite_gbtoidx.q.out
@@ -257,7 +257,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__lineitem_lineitem_lshipdate_idx__
+            alias: default.default__lineitem_lineitem_lshipdate_idx__
             Statistics: Num rows: 95 Data size: 8675 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: l_shipdate (type: string), _count_of_l_shipdate (type: bigint)
@@ -584,7 +584,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__lineitem_lineitem_lshipdate_idx__
+            alias: default.default__lineitem_lineitem_lshipdate_idx__
             Statistics: Num rows: 95 Data size: 8675 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: l_shipdate (type: string), _count_of_l_shipdate (type: bigint)
@@ -762,10 +762,10 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: lastyear:default__lineitem_lineitem_lshipdate_idx__
+            alias: thisyear:default.default__lineitem_lineitem_lshipdate_idx__
             Statistics: Num rows: 95 Data size: 8675 Basic stats: COMPLETE Column stats: NONE
             Filter Operator
-              predicate: (year(l_shipdate) = 1997) (type: boolean)
+              predicate: (year(l_shipdate) = 1998) (type: boolean)
               Statistics: Num rows: 47 Data size: 4291 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: l_shipdate (type: string), _count_of_l_shipdate (type: bigint)
@@ -846,10 +846,10 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: thisyear:default__lineitem_lineitem_lshipdate_idx__
+            alias: lastyear:default.default__lineitem_lineitem_lshipdate_idx__
             Statistics: Num rows: 95 Data size: 8675 Basic stats: COMPLETE Column stats: NONE
             Filter Operator
-              predicate: (year(l_shipdate) = 1998) (type: boolean)
+              predicate: (year(l_shipdate) = 1997) (type: boolean)
               Statistics: Num rows: 47 Data size: 4291 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: l_shipdate (type: string), _count_of_l_shipdate (type: bigint)
@@ -916,7 +916,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: null-subquery1:dummy-subquery1:default__lineitem_lineitem_lshipdate_idx__
+            alias: null-subquery1:dummy-subquery1:default.default__lineitem_lineitem_lshipdate_idx__
             Statistics: Num rows: 95 Data size: 8675 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: l_shipdate (type: string), _count_of_l_shipdate (type: bigint)
@@ -1095,7 +1095,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__tbl_tbl_key_idx__
+            alias: default.default__tbl_tbl_key_idx__
             Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
             Select Operator
               expressions: key (type: int), _count_of_key (type: bigint)
@@ -2364,7 +2364,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__tblpart_tbl_part_index__
+            alias: default.default__tblpart_tbl_part_index__
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             Filter Operator
               predicate: (key < 10) (type: boolean)
@@ -2565,7 +2565,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__tbl_tbl_key_idx__
+            alias: default.default__tbl_tbl_key_idx__
             Statistics: Num rows: 6 Data size: 430 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: key (type: int), _count_of_key (type: bigint)
diff --git a/ql/src/test/results/clientpositive/show_indexes_edge_cases.q.out b/ql/src/test/results/clientpositive/show_indexes_edge_cases.q.out
index cc6a4058a0..eb2f3bcdee 100644
--- a/ql/src/test/results/clientpositive/show_indexes_edge_cases.q.out
+++ b/ql/src/test/results/clientpositive/show_indexes_edge_cases.q.out
@@ -118,10 +118,10 @@ PREHOOK: query: SHOW INDEXES ON show_idx_full
 PREHOOK: type: SHOWINDEXES
 POSTHOOK: query: SHOW INDEXES ON show_idx_full
 POSTHOOK: type: SHOWINDEXES
-idx_1               	show_idx_full       	key                 	default__show_idx_full_idx_1__	compact             	
-idx_2               	show_idx_full       	value1              	default__show_idx_full_idx_2__	compact             	
-idx_comment         	show_idx_full       	value2              	default__show_idx_full_idx_comment__	compact             	index comment       
-idx_compound        	show_idx_full       	key, value1         	default__show_idx_full_idx_compound__	compact             	
+idx_1               	show_idx_full       	key                 	default.default__show_idx_full_idx_1__	compact             	
+idx_2               	show_idx_full       	value1              	default.default__show_idx_full_idx_2__	compact             	
+idx_comment         	show_idx_full       	value2              	default.default__show_idx_full_idx_comment__	compact             	index comment       
+idx_compound        	show_idx_full       	key, value1         	default.default__show_idx_full_idx_compound__	compact             	
 PREHOOK: query: EXPLAIN SHOW INDEXES ON show_idx_empty
 PREHOOK: type: SHOWINDEXES
 POSTHOOK: query: EXPLAIN SHOW INDEXES ON show_idx_empty
diff --git a/ql/src/test/results/clientpositive/show_indexes_syntax.q.out b/ql/src/test/results/clientpositive/show_indexes_syntax.q.out
index bc96359b27..e3cdd09b60 100644
--- a/ql/src/test/results/clientpositive/show_indexes_syntax.q.out
+++ b/ql/src/test/results/clientpositive/show_indexes_syntax.q.out
@@ -53,7 +53,7 @@ PREHOOK: query: SHOW INDEX ON show_idx_t1
 PREHOOK: type: SHOWINDEXES
 POSTHOOK: query: SHOW INDEX ON show_idx_t1
 POSTHOOK: type: SHOWINDEXES
-idx_t1              	show_idx_t1         	key                 	default__show_idx_t1_idx_t1__	compact             	
+idx_t1              	show_idx_t1         	key                 	default.default__show_idx_t1_idx_t1__	compact             	
 PREHOOK: query: EXPLAIN
 SHOW INDEXES ON show_idx_t1
 PREHOOK: type: SHOWINDEXES
@@ -79,7 +79,7 @@ PREHOOK: query: SHOW INDEXES ON show_idx_t1
 PREHOOK: type: SHOWINDEXES
 POSTHOOK: query: SHOW INDEXES ON show_idx_t1
 POSTHOOK: type: SHOWINDEXES
-idx_t1              	show_idx_t1         	key                 	default__show_idx_t1_idx_t1__	compact             	
+idx_t1              	show_idx_t1         	key                 	default.default__show_idx_t1_idx_t1__	compact             	
 PREHOOK: query: EXPLAIN
 SHOW FORMATTED INDEXES ON show_idx_t1
 PREHOOK: type: SHOWINDEXES
@@ -108,7 +108,7 @@ POSTHOOK: type: SHOWINDEXES
 idx_name            	tab_name            	col_names           	idx_tab_name        	idx_type            	comment             
 	 	 	 	 	 
 	 	 	 	 	 
-idx_t1              	show_idx_t1         	key                 	default__show_idx_t1_idx_t1__	compact             	
+idx_t1              	show_idx_t1         	key                 	default.default__show_idx_t1_idx_t1__	compact             	
 PREHOOK: query: DROP TABLE show_idx_t1
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@show_idx_t1
diff --git a/ql/src/test/results/clientpositive/union_view.q.out b/ql/src/test/results/clientpositive/union_view.q.out
index bf31828ad6..ce987c3290 100644
--- a/ql/src/test/results/clientpositive/union_view.q.out
+++ b/ql/src/test/results/clientpositive/union_view.q.out
@@ -54,7 +54,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_1_src_union_1_key_idx__
+            alias: default.default__src_union_1_src_union_1_key_idx__
             filterExpr: ((key = 86) and (ds = '1')) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -113,7 +113,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_2_src_union_2_key_idx__
+            alias: default.default__src_union_2_src_union_2_key_idx__
             filterExpr: ((key = 86) and (ds = '2')) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -172,7 +172,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_3_src_union_3_key_idx__
+            alias: default.default__src_union_3_src_union_3_key_idx__
             filterExpr: ((key = 86) and (ds = '3')) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -236,7 +236,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_1_src_union_1_key_idx__
+            alias: default.default__src_union_1_src_union_1_key_idx__
             filterExpr: (ds = '1') (type: boolean)
             Select Operator
               expressions: _bucketname (type: string), _offsets (type: array<bigint>)
@@ -307,7 +307,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_2_src_union_2_key_idx__
+            alias: default.default__src_union_2_src_union_2_key_idx__
             filterExpr: (ds = '2') (type: boolean)
             Select Operator
               expressions: _bucketname (type: string), _offsets (type: array<bigint>)
@@ -378,7 +378,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_3_src_union_3_key_idx__
+            alias: default.default__src_union_3_src_union_3_key_idx__
             filterExpr: (ds = '3') (type: boolean)
             Select Operator
               expressions: _bucketname (type: string), _offsets (type: array<bigint>)
@@ -452,7 +452,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_1_src_union_1_key_idx__
+            alias: default.default__src_union_1_src_union_1_key_idx__
             filterExpr: ((key = 86) and (ds = '1')) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -517,7 +517,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_2_src_union_2_key_idx__
+            alias: default.default__src_union_2_src_union_2_key_idx__
             filterExpr: ((key = 86) and (ds = '2')) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -582,7 +582,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_3_src_union_3_key_idx__
+            alias: default.default__src_union_3_src_union_3_key_idx__
             filterExpr: ((key = 86) and (ds = '3')) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -651,7 +651,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_2_src_union_2_key_idx__
+            alias: default.default__src_union_2_src_union_2_key_idx__
             filterExpr: (key = 86) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -757,7 +757,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_1_src_union_1_key_idx__
+            alias: default.default__src_union_1_src_union_1_key_idx__
             filterExpr: (key = 86) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -781,7 +781,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_3_src_union_3_key_idx__
+            alias: default.default__src_union_3_src_union_3_key_idx__
             filterExpr: (key = 86) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -828,7 +828,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_1_src_union_1_key_idx__
+            alias: default.default__src_union_1_src_union_1_key_idx__
             filterExpr: (ds = '1') (type: boolean)
             Select Operator
               expressions: _bucketname (type: string), _offsets (type: array<bigint>)
@@ -903,7 +903,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_2_src_union_2_key_idx__
+            alias: default.default__src_union_2_src_union_2_key_idx__
             filterExpr: (ds = '2') (type: boolean)
             Select Operator
               expressions: _bucketname (type: string), _offsets (type: array<bigint>)
@@ -978,7 +978,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_3_src_union_3_key_idx__
+            alias: default.default__src_union_3_src_union_3_key_idx__
             filterExpr: (ds = '3') (type: boolean)
             Select Operator
               expressions: _bucketname (type: string), _offsets (type: array<bigint>)
@@ -1056,7 +1056,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_3_src_union_3_key_idx__
+            alias: default.default__src_union_3_src_union_3_key_idx__
             filterExpr: ((key = 86) and (ds = '4')) (type: boolean)
             Filter Operator
               predicate: (key = 86) (type: boolean)
@@ -1122,7 +1122,7 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            alias: default__src_union_3_src_union_3_key_idx__
+            alias: default.default__src_union_3_src_union_3_key_idx__
             filterExpr: (ds = '4') (type: boolean)
             Select Operator
               expressions: _bucketname (type: string), _offsets (type: array<bigint>)
