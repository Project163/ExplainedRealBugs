diff --git a/CHANGES.txt b/CHANGES.txt
index 2cb9e08f73..1041f68fcf 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -390,6 +390,9 @@ Trunk -  Unreleased
     HIVE-1320. NPE with lineage in a query of union all on joins
     (Ashish Thusoo via Ning Zhang)
 
+    HIVE-1321. bugs with temp directories, trailing blank fields in HBase bulk load
+    (John Sichi via namit)
+
 Release 0.5.0 -  Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java
index a7664737bb..2973eddda1 100644
--- a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java
+++ b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java
@@ -55,7 +55,7 @@
  * loading.  Until HBASE-1861 is implemented, it can only be used
  * for loading a table with a single column family.
  */
-public class HiveHFileOutputFormat extends 
+public class HiveHFileOutputFormat extends
     HFileOutputFormat implements
     HiveOutputFormat<ImmutableBytesWritable, KeyValue> {
 
@@ -74,7 +74,7 @@ ImmutableBytesWritable, KeyValue> getFileWriter(
       throw new IOException(ex);
     }
   }
-  
+
   @Override
   public RecordWriter getHiveRecordWriter(
     final JobConf jc, final Path finalOutPath,
@@ -98,13 +98,14 @@ public RecordWriter getHiveRecordWriter(
     setOutputPath(job, finalOutPath);
 
     // Create the HFile writer
-    org.apache.hadoop.mapreduce.TaskAttemptContext tac =
+    final org.apache.hadoop.mapreduce.TaskAttemptContext tac =
       new TaskAttemptContext(job.getConfiguration(), new TaskAttemptID()) {
         @Override
         public void progress() {
           progressable.progress();
         }
       };
+    final Path outputdir = FileOutputFormat.getOutputPath(tac);
     final org.apache.hadoop.mapreduce.RecordWriter<
       ImmutableBytesWritable, KeyValue> fileWriter = getFileWriter(tac);
 
@@ -126,7 +127,7 @@ public void progress() {
     }
 
     return new RecordWriter() {
-      
+
       @Override
       public void close(boolean abort) throws IOException {
         try {
@@ -138,10 +139,22 @@ public void close(boolean abort) throws IOException {
           // to the location specified by the user.  There should
           // actually only be one (each reducer produces one HFile),
           // but we don't know what its name is.
-          Path outputdir = FileOutputFormat.getOutputPath(job);
           FileSystem fs = outputdir.getFileSystem(jc);
           fs.mkdirs(columnFamilyPath);
-          Path srcDir = new Path(outputdir, columnFamilyName);
+          Path srcDir = outputdir;
+          for (;;) {
+            FileStatus [] files = fs.listStatus(srcDir);
+            if ((files == null) || (files.length == 0)) {
+              throw new IOException("No files found in " + srcDir);
+            }
+            if (files.length != 1) {
+              throw new IOException("Multiple files found in " + srcDir);
+            }
+            srcDir = files[0].getPath();
+            if (srcDir.getName().equals(columnFamilyName)) {
+              break;
+            }
+          }
           for (FileStatus regionFile : fs.listStatus(srcDir)) {
             fs.rename(
               regionFile.getPath(),
@@ -163,14 +176,25 @@ public void write(Writable w) throws IOException {
         // Decompose the incoming text row into fields.
         String s = ((Text) w).toString();
         String [] fields = s.split("\u0001");
-        assert(fields.length == (columnMap.size() + 1));
+        assert(fields.length <= (columnMap.size() + 1));
         // First field is the row key.
         byte [] rowKeyBytes = Bytes.toBytes(fields[0]);
         // Remaining fields are cells addressed by column name within row.
         for (Map.Entry<byte [], Integer> entry : columnMap.entrySet()) {
           byte [] columnNameBytes = entry.getKey();
           int iColumn = entry.getValue();
-          byte [] valBytes = Bytes.toBytes(fields[iColumn]);
+          String val;
+          if (iColumn >= fields.length) {
+            // trailing blank field
+            val = "";
+          } else {
+            val = fields[iColumn];
+            if ("\\N".equals(val)) {
+              // omit nulls
+              continue;
+            }
+          }
+          byte [] valBytes = Bytes.toBytes(val);
           KeyValue kv = new KeyValue(
             rowKeyBytes,
             columnFamilyNameBytes,
diff --git a/hbase-handler/src/test/queries/hbase_bulk.m b/hbase-handler/src/test/queries/hbase_bulk.m
index a241df2e72..0ac3de6f48 100644
--- a/hbase-handler/src/test/queries/hbase_bulk.m
+++ b/hbase-handler/src/test/queries/hbase_bulk.m
@@ -38,8 +38,12 @@ set total.order.partitioner.natural.order=false;
 set total.order.partitioner.path=/tmp/hbpartition.lst;
 
 -- this should produce three files in /tmp/hbsort/cf
+-- include some trailing blanks and nulls to make sure we handle them correctly
 insert overwrite table hbsort
-select distinct value, key, key+1
+select distinct value,
+  case when key=103 then cast(null as string) else key end,
+  case when key=103 then ''
+       else cast(key+1 as string) end
 from src
 cluster by value;
 
diff --git a/hbase-handler/src/test/results/hbase_bulk.m.out b/hbase-handler/src/test/results/hbase_bulk.m.out
index 8205b10976..6254195589 100644
--- a/hbase-handler/src/test/results/hbase_bulk.m.out
+++ b/hbase-handler/src/test/results/hbase_bulk.m.out
@@ -69,18 +69,26 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@hbpartition
 POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-           1            1                139 hdfs://localhost:59616/tmp/hbpartitions
+           1            1                139 hdfs://localhost:57034/tmp/hbpartitions
 PREHOOK: query: -- this should produce three files in /tmp/hbsort/cf
+-- include some trailing blanks and nulls to make sure we handle them correctly
 insert overwrite table hbsort
-select distinct value, key, key+1
+select distinct value,
+  case when key=103 then cast(null as string) else key end,
+  case when key=103 then ''
+       else cast(key+1 as string) end
 from src
 cluster by value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Output: default@hbsort
 POSTHOOK: query: -- this should produce three files in /tmp/hbsort/cf
+-- include some trailing blanks and nulls to make sure we handle them correctly
 insert overwrite table hbsort
-select distinct value, key, key+1
+select distinct value,
+  case when key=103 then cast(null as string) else key end,
+  case when key=103 then ''
+       else cast(key+1 as string) end
 from src
 cluster by value
 POSTHOOK: type: QUERY
@@ -88,9 +96,9 @@ POSTHOOK: Input: default@src
 POSTHOOK: Output: default@hbsort
 POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.val SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-           1            3              23267 hdfs://localhost:59616/tmp/hbsort/cf
+           1            3              23227 hdfs://localhost:57034/tmp/hbsort/cf
 PREHOOK: query: -- To get the files out to your local filesystem for loading into
 -- HBase, run mkdir -p /tmp/blah/cf, then uncomment and
 -- semicolon-terminate the line below before running this test:
@@ -108,7 +116,7 @@ POSTHOOK: type: DROPTABLE
 POSTHOOK: Output: default@hbsort
 POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.val SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: drop table hbpartition
 PREHOOK: type: DROPTABLE
@@ -117,5 +125,5 @@ POSTHOOK: type: DROPTABLE
 POSTHOOK: Output: default@hbpartition
 POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.val SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
