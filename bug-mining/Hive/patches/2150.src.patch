diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 370f6e4d4f..bf9d4c16a1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -39,12 +39,19 @@
 import org.apache.hadoop.hive.common.ValidTxnList;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Schema;
-import org.apache.hadoop.hive.ql.exec.*;
+import org.apache.hadoop.hive.ql.exec.ConditionalTask;
+import org.apache.hadoop.hive.ql.exec.FetchTask;
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.exec.TaskResult;
+import org.apache.hadoop.hive.ql.exec.TaskRunner;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;
 import org.apache.hadoop.hive.ql.hooks.Entity;
 import org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;
@@ -55,15 +62,39 @@
 import org.apache.hadoop.hive.ql.hooks.PreExecute;
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
-import org.apache.hadoop.hive.ql.lockmgr.*;
+import org.apache.hadoop.hive.ql.lockmgr.HiveLock;
+import org.apache.hadoop.hive.ql.lockmgr.HiveLockMode;
+import org.apache.hadoop.hive.ql.lockmgr.HiveLockObj;
+import org.apache.hadoop.hive.ql.lockmgr.HiveLockObject;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData;
+import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;
+import org.apache.hadoop.hive.ql.lockmgr.LockException;
+import org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory;
 import org.apache.hadoop.hive.ql.log.PerfLogger;
-import org.apache.hadoop.hive.ql.metadata.*;
+import org.apache.hadoop.hive.ql.metadata.AuthorizationException;
+import org.apache.hadoop.hive.ql.metadata.DummyPartition;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter;
 import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;
 import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatter;
 import org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner;
-import org.apache.hadoop.hive.ql.parse.*;
+import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
+import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHook;
+import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
+import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl;
+import org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer;
+import org.apache.hadoop.hive.ql.parse.ParseContext;
+import org.apache.hadoop.hive.ql.parse.ParseDriver;
+import org.apache.hadoop.hive.ql.parse.ParseUtils;
+import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.parse.VariableSubstitution;
 import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
@@ -704,7 +735,7 @@ private List<HivePrivilegeObject> getHivePrivObjects(HashSet<? extends Entity> p
       case DUMMYPARTITION:
       case PARTITION:
         // not currently handled
-        break;
+        continue;
         default:
           throw new AssertionError("Unexpected object type");
       }
diff --git a/ql/src/test/queries/clientpositive/authorization_parts.q b/ql/src/test/queries/clientpositive/authorization_parts.q
new file mode 100644
index 0000000000..bee091b1d3
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/authorization_parts.q
@@ -0,0 +1,19 @@
+set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
+set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
+set hive.security.authorization.enabled=true;
+
+dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_uri_add_part1;
+dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_uri_add_part2;
+
+
+
+
+-- check add partition without insert privilege
+create table tpart(i int, j int) partitioned by (k string);
+
+alter table tpart add partition (k = '1') location '${system:test.tmp.dir}/a_uri_add_part1/';
+alter table tpart add partition (k = '2') location '${system:test.tmp.dir}/a_uri_add_part2/';
+
+select count(*) from tpart;
+
+analyze table tpart partition (k) compute statistics;
diff --git a/ql/src/test/results/clientpositive/authorization_parts.q.out b/ql/src/test/results/clientpositive/authorization_parts.q.out
new file mode 100644
index 0000000000..6c70744073
--- /dev/null
+++ b/ql/src/test/results/clientpositive/authorization_parts.q.out
@@ -0,0 +1,56 @@
+PREHOOK: query: -- check add partition without insert privilege
+create table tpart(i int, j int) partitioned by (k string)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: -- check add partition without insert privilege
+create table tpart(i int, j int) partitioned by (k string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@tpart
+#### A masked pattern was here ####
+PREHOOK: type: ALTERTABLE_ADDPARTS
+#### A masked pattern was here ####
+PREHOOK: Output: default@tpart
+#### A masked pattern was here ####
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+#### A masked pattern was here ####
+POSTHOOK: Output: default@tpart
+POSTHOOK: Output: default@tpart@k=1
+#### A masked pattern was here ####
+PREHOOK: type: ALTERTABLE_ADDPARTS
+#### A masked pattern was here ####
+PREHOOK: Output: default@tpart
+#### A masked pattern was here ####
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+#### A masked pattern was here ####
+POSTHOOK: Output: default@tpart
+POSTHOOK: Output: default@tpart@k=2
+PREHOOK: query: select count(*) from tpart
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tpart
+PREHOOK: Input: default@tpart@k=1
+PREHOOK: Input: default@tpart@k=2
+#### A masked pattern was here ####
+POSTHOOK: query: select count(*) from tpart
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tpart
+POSTHOOK: Input: default@tpart@k=1
+POSTHOOK: Input: default@tpart@k=2
+#### A masked pattern was here ####
+0
+PREHOOK: query: analyze table tpart partition (k) compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tpart
+PREHOOK: Input: default@tpart@k=1
+PREHOOK: Input: default@tpart@k=2
+PREHOOK: Output: default@tpart
+PREHOOK: Output: default@tpart@k=1
+PREHOOK: Output: default@tpart@k=2
+POSTHOOK: query: analyze table tpart partition (k) compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tpart
+POSTHOOK: Input: default@tpart@k=1
+POSTHOOK: Input: default@tpart@k=2
+POSTHOOK: Output: default@tpart
+POSTHOOK: Output: default@tpart@k=1
+POSTHOOK: Output: default@tpart@k=2
