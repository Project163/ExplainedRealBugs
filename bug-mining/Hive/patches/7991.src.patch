diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
index 079c844a5c..e8e2163cd0 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
@@ -21,6 +21,8 @@
 
 import java.util.List;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.SchemaParser;
 import org.apache.iceberg.catalog.TableIdentifier;
@@ -201,8 +203,12 @@ public static Schema readSchema(Configuration conf) {
   }
 
   public static String[] selectedColumns(Configuration conf) {
-    String[] readColumns = conf.getStrings(InputFormatConfig.SELECTED_COLUMNS);
-    return readColumns != null && readColumns.length > 0 ? readColumns : null;
+    String readColumns = conf.get(InputFormatConfig.SELECTED_COLUMNS);
+    if (readColumns == null || readColumns.isEmpty()) {
+      return null;
+    }
+
+    return readColumns.split(conf.get(serdeConstants.COLUMN_NAME_DELIMITER, String.valueOf(SerDeUtils.COMMA)));
   }
 
   private static Schema schema(Configuration conf, String key) {
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java
index 5974e7f2b9..9a966598e8 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java
@@ -65,8 +65,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
       }
     }
 
-    String[] selectedColumns = ColumnProjectionUtils.getReadColumnNames(job);
-    job.setStrings(InputFormatConfig.SELECTED_COLUMNS, selectedColumns);
+    job.set(InputFormatConfig.SELECTED_COLUMNS, job.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, ""));
 
     String location = job.get(InputFormatConfig.TABLE_LOCATION);
     return Arrays.stream(super.getSplits(job, numSplits))
@@ -77,8 +76,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
   @Override
   public RecordReader<Void, Container<Record>> getRecordReader(InputSplit split, JobConf job,
                                                                Reporter reporter) throws IOException {
-    String[] selectedColumns = ColumnProjectionUtils.getReadColumnNames(job);
-    job.setStrings(InputFormatConfig.SELECTED_COLUMNS, selectedColumns);
+    job.set(InputFormatConfig.SELECTED_COLUMNS, job.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, ""));
     return super.getRecordReader(split, job, reporter);
   }
 
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
index 948d896a63..dd4ffbe27e 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
@@ -54,6 +54,8 @@
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.exceptions.NoSuchTableException;
 import org.apache.iceberg.hive.HiveSchemaUtil;
+import org.apache.iceberg.mr.Catalogs;
+import org.apache.iceberg.mr.InputFormatConfig;
 import org.apache.iceberg.mr.TestHelper;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
@@ -587,6 +589,26 @@ public void testInsertOverwriteNonPartitionedTable() throws IOException {
     HiveIcebergTestUtils.validateData(table, ImmutableList.of(), 0);
   }
 
+
+  @Test
+  public void testSpecialCharacters() {
+    TableIdentifier table = TableIdentifier.of("default", "tar,! ,get");
+    // note: the Chinese character seems to be accepted in the column name, but not
+    // in the table name - this is the case for both Iceberg and standard Hive tables.
+    shell.executeStatement(String.format(
+        "CREATE TABLE `%s` (id bigint, `dep,! 是,t` string) STORED BY ICEBERG STORED AS %s %s TBLPROPERTIES ('%s'='%s')",
+        table.name(), fileFormat, testTables.locationForCreateTableSQL(table),
+        InputFormatConfig.CATALOG_NAME, Catalogs.ICEBERG_DEFAULT_CATALOG_NAME));
+    shell.executeStatement(String.format("INSERT INTO `%s` VALUES (1, 'moon'), (2, 'star')", table.name()));
+
+    List<Object[]> result = shell.executeStatement(String.format(
+        "SELECT `dep,! 是,t`, id FROM `%s` ORDER BY id", table.name()));
+
+    Assert.assertEquals(2, result.size());
+    Assert.assertArrayEquals(new Object[]{"moon", 1L}, result.get(0));
+    Assert.assertArrayEquals(new Object[]{"star", 2L}, result.get(1));
+  }
+
   @Test
   public void testInsertOverwritePartitionedTable() throws IOException {
     TableIdentifier target = TableIdentifier.of("default", "target");
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
index 559b4b5cdc..6dff5332e7 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
@@ -25,6 +25,7 @@
 import java.util.Set;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hive.common.util.HiveStringUtils;
 import org.slf4j.Logger;
@@ -202,7 +203,7 @@ public static Set<String> getNestedColumnPaths(Configuration conf) {
   public static String[] getReadColumnNames(Configuration conf) {
     String colNames = conf.get(READ_COLUMN_NAMES_CONF_STR, READ_COLUMN_IDS_CONF_STR_DEFAULT);
     if (colNames != null && !colNames.isEmpty()) {
-      return colNames.split(",");
+      return colNames.split(conf.get(serdeConstants.COLUMN_NAME_DELIMITER, String.valueOf(SerDeUtils.COMMA)));
     }
     return new String[] {};
   }
@@ -228,17 +229,12 @@ private static void setReadNestedColumnPathConf(
 
   private static void appendReadColumnNames(Configuration conf, List<String> cols) {
     String old = conf.get(READ_COLUMN_NAMES_CONF_STR, "");
-    StringBuilder result = new StringBuilder(old);
-    boolean first = old.isEmpty();
-    for(String col: cols) {
-      if (first) {
-        first = false;
-      } else {
-        result.append(',');
-      }
-      result.append(col);
+    String delim = conf.get(serdeConstants.COLUMN_NAME_DELIMITER, String.valueOf(SerDeUtils.COMMA));
+    String result = String.join(delim, cols);
+    if (!old.isEmpty()) {
+      result = old + delim + result;
     }
-    conf.set(READ_COLUMN_NAMES_CONF_STR, result.toString());
+    conf.set(READ_COLUMN_NAMES_CONF_STR, result);
   }
 
   private static String toReadColumnIDString(List<Integer> ids) {
