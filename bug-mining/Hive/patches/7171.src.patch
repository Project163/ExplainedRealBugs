diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
index 7528f27f8c..406bec6ae8 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
@@ -256,6 +256,40 @@ public void testCreateFunctionWithFunctionBinaryJarsOnHDFS() throws Throwable {
     assertThat(jars, containsInAnyOrder(expectedDependenciesNames.toArray()));
   }
 
+  @Test
+  public void testIncrementalCreateFunctionWithFunctionBinaryJarsOnHDFS() throws Throwable {
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    replica.load(replicatedDbName, bootStrapDump.dumpLocation)
+            .run("REPL STATUS " + replicatedDbName)
+            .verifyResult(bootStrapDump.lastReplicationId);
+
+    Dependencies dependencies = dependencies("ivy://io.github.myui:hivemall:0.4.0-2", primary);
+    String jarSubString = dependencies.toJarSubSql();
+
+    primary.run("CREATE FUNCTION " + primaryDbName
+            + ".anotherFunction as 'hivemall.tools.string.StopwordUDF' "
+            + "using " + jarSubString);
+
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, bootStrapDump.lastReplicationId);
+
+    replica.load(replicatedDbName, tuple.dumpLocation)
+            .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "*'")
+            .verifyResult(replicatedDbName + ".anotherFunction");
+
+    FileStatus[] fileStatuses = replica.miniDFSCluster.getFileSystem().globStatus(
+            new Path(
+                    replica.functionsRoot + "/" + replicatedDbName.toLowerCase() + "/anotherfunction/*/*")
+            , path -> path.toString().endsWith("jar"));
+    List<String> expectedDependenciesNames = dependencies.jarNames();
+    assertThat(fileStatuses.length, is(equalTo(expectedDependenciesNames.size())));
+    List<String> jars = Arrays.stream(fileStatuses).map(f -> {
+        String[] splits = f.getPath().toString().split("/");
+        return splits[splits.length - 1];
+    }).collect(Collectors.toList());
+
+    assertThat(jars, containsInAnyOrder(expectedDependenciesNames.toArray()));
+  }
+
   static class Dependencies {
     private final List<Path> fullQualifiedJarPaths;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CreateFunctionHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CreateFunctionHandler.java
index 5954e1578e..c9e1041fc1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CreateFunctionHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CreateFunctionHandler.java
@@ -39,7 +39,7 @@ CreateFunctionMessage eventMessage(String stringRepresentation) {
 
   @Override
   public void handle(Context withinContext) throws Exception {
-    LOG.info("Processing#{} CREATE_MESSAGE message : {}", fromEventId(), eventMessageAsJSON);
+    LOG.info("Processing#{} CREATE_FUNCTION message : {}", fromEventId(), eventMessageAsJSON);
     Path metadataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);
     FileSystem fileSystem = metadataPath.getFileSystem(withinContext.hiveConf);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java
index b68e8874c5..6258c9e30c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java
@@ -22,6 +22,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.ReplChangeManager;
 import org.apache.hadoop.hive.metastore.api.Function;
+import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.ResourceUri;
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.ReplicationSpec;
@@ -47,7 +48,7 @@ public FunctionSerializer(Function function, HiveConf hiveConf) {
 
   @Override
   public void writeTo(JsonWriter writer, ReplicationSpec additionalPropertiesProvider)
-      throws SemanticException, IOException {
+      throws SemanticException, IOException, MetaException {
     TSerializer serializer = new TSerializer(new TJSONProtocol.Factory());
     List<ResourceUri> resourceUris = new ArrayList<>();
     for (ResourceUri uri : function.getResourceUris()) {
@@ -55,6 +56,8 @@ public void writeTo(JsonWriter writer, ReplicationSpec additionalPropertiesProvi
       if ("hdfs".equals(inputPath.toUri().getScheme())) {
         FileSystem fileSystem = inputPath.getFileSystem(hiveConf);
         Path qualifiedUri = PathBuilder.fullyQualifiedHDFSUri(inputPath, fileSystem);
+        // Initialize ReplChangeManager instance since we will require it to encode file URI.
+        ReplChangeManager.getInstance(hiveConf);
         String checkSum = ReplChangeManager.checksumFor(qualifiedUri, fileSystem);
         String newFileUri = ReplChangeManager.encodeFileUri(qualifiedUri.toString(), checkSum, null);
         resourceUris.add(new ResourceUri(uri.getResourceType(), newFileUri));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/JsonWriter.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/JsonWriter.java
index e20be68b6b..4be00b63bc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/JsonWriter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/JsonWriter.java
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.ql.parse.ReplicationSpec;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.codehaus.jackson.JsonFactory;
@@ -50,6 +51,6 @@ public void close() throws IOException {
   public interface Serializer {
     String UTF_8 = "UTF-8";
     void writeTo(JsonWriter writer, ReplicationSpec additionalPropertiesProvider) throws
-        SemanticException, IOException;
+        SemanticException, IOException, MetaException;
   }
 }
diff --git a/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java b/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java
index b5fc994fcd..d6c6d50ca0 100644
--- a/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java
+++ b/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java
@@ -356,6 +356,9 @@ public static FileInfo getFileInfo(Path src, String checksumString, String srcCM
   // Currently using fileuri#checksum#cmrooturi#subdirs as the format
   public static String encodeFileUri(String fileUriStr, String fileChecksum, String encodedSubDir)
           throws IOException {
+    if (instance == null) {
+      throw new IllegalStateException("Uninitialized ReplChangeManager instance.");
+    }
     String encodedUri = fileUriStr;
     if ((fileChecksum != null) && (cmroot != null)) {
       encodedUri = encodedUri + URI_FRAGMENT_SEPARATOR + fileChecksum
