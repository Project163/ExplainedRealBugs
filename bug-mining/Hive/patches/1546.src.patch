diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
index 6e9c4cd32c..3a2f711ada 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
@@ -69,6 +69,7 @@
 import org.apache.tez.dag.api.ProcessorDescriptor;
 import org.apache.tez.dag.api.Vertex;
 import org.apache.tez.runtime.library.input.ShuffledMergedInputLegacy;
+import org.apache.tez.runtime.library.input.ShuffledUnorderedKVInput;
 import org.apache.tez.runtime.library.output.OnFileSortedOutput;
 import org.apache.tez.runtime.library.output.OnFileUnorderedKVOutput;
 import org.apache.tez.mapreduce.hadoop.InputSplitInfo;
@@ -168,7 +169,7 @@ public static Edge createEdge(JobConf vConf, Vertex v, JobConf wConf, Vertex w,
     case BROADCAST_EDGE:
       dataMovementType = DataMovementType.BROADCAST;
       logicalOutputClass = OnFileUnorderedKVOutput.class;
-      logicalInputClass = MRInput.class;
+      logicalInputClass = ShuffledUnorderedKVInput.class;
       break;
 
     case SIMPLE_EDGE:
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java
index 6a83d37d49..9463b63b88 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java
@@ -4,6 +4,8 @@
 import java.util.List;
 import java.util.Stack;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
 import org.apache.hadoop.hive.ql.exec.MapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
@@ -20,6 +22,8 @@
 
 public class ReduceSinkMapJoinProc implements NodeProcessor {
 
+  protected transient Log LOG = LogFactory.getLog(this.getClass().getName());
+
   /* (non-Javadoc)
    * This processor addresses the RS-MJ case that occurs in tez on the small/hash
    * table side of things. The connection between the work that RS will be a part of
@@ -37,9 +41,16 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,
     context.currentRootOperator = null;
 
     MapJoinOperator mapJoinOp = (MapJoinOperator)nd;
+    Operator<? extends OperatorDesc> childOp = mapJoinOp.getChildOperators().get(0);
 
-    Operator<? extends OperatorDesc>childOp = mapJoinOp.getChildOperators().get(0);
     ReduceSinkOperator parentRS = (ReduceSinkOperator)stack.get(stack.size() - 2);
+
+    // remember the original parent list before we start modifying it.
+    if (!context.mapJoinParentMap.containsKey(mapJoinOp)) {
+      List<Operator<?>> parents = new ArrayList(mapJoinOp.getParentOperators());
+      context.mapJoinParentMap.put(mapJoinOp, parents);
+    }
+
     while (childOp != null) {
       if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof FileSinkOperator)) {
         /*
@@ -54,6 +65,15 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,
 
         BaseWork myWork = context.operatorWorkMap.get(childOp);
         BaseWork parentWork = context.operatorWorkMap.get(parentRS);
+          
+        // set the link between mapjoin and parent vertex
+        int pos = context.mapJoinParentMap.get(mapJoinOp).indexOf(parentRS);
+        if (pos == -1) {
+          throw new SemanticException("Cannot find position of parent in mapjoin");
+        }
+        LOG.debug("Mapjoin "+mapJoinOp+", pos: "+pos+" --> "+parentWork.getName());
+        mapJoinOp.getConf().getParentToInput().put(pos, parentWork.getName());
+
         if (myWork != null) {
           // link the work with the work associated with the reduce sink that triggered this rule
           TezWork tezWork = context.currentTask.getWork();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java
index a53bd5a770..088fe792e0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java
@@ -26,6 +26,7 @@
 import java.util.Set;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.exec.MapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
@@ -89,6 +90,9 @@ public class GenTezProcContext implements NodeProcessorCtx{
   // a map that maintains operator (file-sink or reduce-sink) to work mapping
   public final Map<Operator<?>, BaseWork> operatorWorkMap;
 
+  // we need to keep the original list of operators in the map join to know
+  // what position in the mapjoin the different parent work items will have.
+  public final Map<MapJoinOperator, List<Operator<?>>> mapJoinParentMap;
 
   @SuppressWarnings("unchecked")
   public GenTezProcContext(HiveConf conf, ParseContext parseContext,
@@ -106,5 +110,6 @@ public GenTezProcContext(HiveConf conf, ParseContext parseContext,
     this.rootOperators = rootOperators;
     this.linkOpWithWorkMap = new HashMap<Operator<?>, List<BaseWork>>();
     this.operatorWorkMap = new HashMap<Operator<?>, BaseWork>();
+    this.mapJoinParentMap = new HashMap<MapJoinOperator, List<Operator<?>>>();
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
index e609633953..cf0ca5705b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
@@ -47,6 +47,9 @@ public class MapJoinDesc extends JoinDesc implements Serializable {
 
   private transient String bigTableAlias;
 
+  // for tez. used to remember which position maps to which logical input
+  private Map<Integer, String> parentToInput = new HashMap<Integer, String>();
+
   // table alias (small) --> input file name (big) --> target file names (small)
   private Map<String, Map<String, List<String>>> aliasBucketFileNameMapping;
   private Map<String, Integer> bigTableBucketNumMapping;
@@ -74,6 +77,7 @@ public MapJoinDesc(MapJoinDesc clone) {
     this.bigTableBucketNumMapping = clone.bigTableBucketNumMapping;
     this.bigTablePartSpecToFileMapping = clone.bigTablePartSpecToFileMapping;
     this.dumpFilePrefix = clone.dumpFilePrefix;
+    this.parentToInput = clone.parentToInput;
   }
 
   public MapJoinDesc(final Map<Byte, List<ExprNodeDesc>> keys,
@@ -106,6 +110,14 @@ private void initRetainExprList() {
     }
   }
 
+  public Map<Integer, String> getParentToInput() {
+    return parentToInput;
+  }
+
+  public void setParentToInput(Map<Integer, String> parentToInput) {
+    this.parentToInput = parentToInput;
+  }
+
   public Map<Byte, List<Integer>> getRetainList() {
     return retainList;
   }
