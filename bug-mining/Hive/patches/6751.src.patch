diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
index 9d35805330..765a647275 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
@@ -32,9 +32,9 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.JoinUtil;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
@@ -163,7 +163,7 @@ public GetAdaptor(MapJoinKey key) {
     }
 
     @Override
-    public JoinUtil.JoinResult setFromVector(VectorHashKeyWrapper kw,
+    public JoinUtil.JoinResult setFromVector(VectorHashKeyWrapperBase kw,
         VectorExpressionWriter[] keyOutputWriters, VectorHashKeyWrapperBatch keyWrapperBatch)
         throws HiveException {
       if (currentKey == null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
index 027e39a8da..13f1702d7e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
@@ -39,10 +39,10 @@
 import org.apache.hadoop.hive.ql.exec.JoinUtil.JoinResult;
 import org.apache.hadoop.hive.ql.exec.SerializationUtilities;
 import org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.KeyValueHelper;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.exec.vector.rowbytescontainer.VectorRowBytesContainer;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.HiveUtils;
@@ -812,7 +812,7 @@ public GetAdaptor() {
     }
 
     @Override
-    public JoinUtil.JoinResult setFromVector(VectorHashKeyWrapper kw,
+    public JoinUtil.JoinResult setFromVector(VectorHashKeyWrapperBase kw,
         VectorExpressionWriter[] keyOutputWriters, VectorHashKeyWrapperBatch keyWrapperBatch)
         throws HiveException {
       if (nulls == null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
index 033bbdb6a5..b632e1de89 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
@@ -30,9 +30,9 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.JoinUtil;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
@@ -519,7 +519,7 @@ public GetAdaptor() {
     }
 
     @Override
-    public JoinUtil.JoinResult setFromVector(VectorHashKeyWrapper kw,
+    public JoinUtil.JoinResult setFromVector(VectorHashKeyWrapperBase kw,
         VectorExpressionWriter[] keyOutputWriters, VectorHashKeyWrapperBatch keyWrapperBatch)
         throws HiveException {
       if (nulls == null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
index 6504a5f7b0..2e3716c8b1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
@@ -24,9 +24,9 @@
 import java.util.HashSet;
 import java.util.List;
 
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
@@ -118,7 +118,7 @@ public static MapJoinKey readFromVector(Output output, MapJoinKey key, Object[]
    * Serializes row to output for vectorized path.
    * @param byteStream Output to reuse. Can be null, in that case a new one would be created.
    */
-  public static Output serializeVector(Output byteStream, VectorHashKeyWrapper kw,
+  public static Output serializeVector(Output byteStream, VectorHashKeyWrapperBase kw,
       VectorExpressionWriter[] keyOutputWriters, VectorHashKeyWrapperBatch keyWrapperBatch,
       boolean[] nulls, boolean[] sortableSortOrders, byte[] nullMarkers, byte[] notNullMarkers)
               throws HiveException, SerDeException {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
index 5c750a38a0..555ccdf6cb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
@@ -25,10 +25,10 @@
 import java.util.List;
 
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
@@ -149,7 +149,7 @@ protected boolean[] getNulls() {
     return nulls;
   }
 
-  public void readFromVector(VectorHashKeyWrapper kw, VectorExpressionWriter[] keyOutputWriters,
+  public void readFromVector(VectorHashKeyWrapperBase kw, VectorExpressionWriter[] keyOutputWriters,
       VectorHashKeyWrapperBatch keyWrapperBatch) throws HiveException {
     if (key == null || key.length != keyOutputWriters.length) {
       key = new Object[keyOutputWriters.length];
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainer.java
index b0c757434b..2c4229f23b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainer.java
@@ -24,9 +24,9 @@
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.JoinUtil;
 import org.apache.hadoop.hive.common.MemoryEstimate;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper;
-import org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -43,7 +43,7 @@ public interface ReusableGetAdaptor {
      * Changes current rows to which adaptor is referring to the rows corresponding to
      * the key represented by a VHKW object, and writers and batch used to interpret it.
      */
-    JoinUtil.JoinResult setFromVector(VectorHashKeyWrapper kw, VectorExpressionWriter[] keyOutputWriters,
+    JoinUtil.JoinResult setFromVector(VectorHashKeyWrapperBase kw, VectorExpressionWriter[] keyOutputWriters,
         VectorHashKeyWrapperBatch keyWrapperBatch) throws HiveException;
 
     /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java
index 7758ac4fea..7ada2bf727 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java
@@ -37,34 +37,34 @@ public class VectorColumnSetInfo {
   /**
    * indices of LONG primitive keys.
    */
-  protected int[] longIndices;
+  public int[] longIndices;
 
   /**
    * indices of DOUBLE primitive keys.
    */
-  protected int[] doubleIndices;
+  public int[] doubleIndices;
 
   /**
    * indices of string (byte[]) primitive keys.
    */
-  protected int[] stringIndices;
+  public int[] stringIndices;
 
   /**
    * indices of decimal primitive keys.
    */
-  protected int[] decimalIndices;
+  public int[] decimalIndices;
 
   /**
    * indices of TIMESTAMP primitive keys.
    */
-  protected int[] timestampIndices;
+  public int[] timestampIndices;
 
   /**
    * indices of INTERVAL_DAY_TIME primitive keys.
    */
-  protected int[] intervalDayTimeIndices;
+  public int[] intervalDayTimeIndices;
 
-  final protected int keyCount;
+  final public int keyCount;
   private int addKeyIndex;
 
   private int addLongIndex;
@@ -77,9 +77,9 @@ public class VectorColumnSetInfo {
   // Given the keyIndex these arrays return:
   //   The ColumnVector.Type,
   //   The type specific index into longIndices, doubleIndices, etc...
-  protected TypeInfo[] typeInfos;
-  protected ColumnVector.Type[] columnVectorTypes;
-  protected int[] columnTypeSpecificIndices;
+  public TypeInfo[] typeInfos;
+  public ColumnVector.Type[] columnVectorTypes;
+  public int[] columnTypeSpecificIndices;
 
   protected VectorColumnSetInfo(int keyCount) {
     this.keyCount = keyCount;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
index 43f1162087..7816cbbf15 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
@@ -44,6 +44,8 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.GroupByDesc;
@@ -453,7 +455,7 @@ public void close(boolean aborted) throws HiveException {
       if (!aborted && sumBatchSize == 0 && GroupByOperator.shouldEmitSummaryRow(conf)) {
         // in case the empty grouping set is preset; but no output has done
         // the "summary row" still needs to be emitted
-        VectorHashKeyWrapper kw = keyWrappersBatch.getVectorHashKeyWrappers()[0];
+        VectorHashKeyWrapperBase kw = keyWrappersBatch.getVectorHashKeyWrappers()[0];
         kw.setNull();
         int pos = conf.getGroupingSetPosition();
         if (pos >= 0) {
@@ -481,13 +483,13 @@ private void prepareBatchAggregationBufferSets(VectorizedRowBatch batch) throws
 
       // We now have to probe the global hash and find-or-allocate
       // the aggregation buffers to use for each key present in the batch
-      VectorHashKeyWrapper[] keyWrappers = keyWrappersBatch.getVectorHashKeyWrappers();
+      VectorHashKeyWrapperBase[] keyWrappers = keyWrappersBatch.getVectorHashKeyWrappers();
 
       final int n = keyExpressions.length == 0 ? 1 : batch.size;
       // note - the row mapping is not relevant when aggregationBatchInfo::getDistinctBufferSetCount() == 1
 
       for (int i=0; i < n; ++i) {
-        VectorHashKeyWrapper kw = keyWrappers[i];
+        VectorHashKeyWrapperBase kw = keyWrappers[i];
         VectorAggregationBufferRow aggregationBuffer = mapKeysAggregationBuffers.get(kw);
         if (null == aggregationBuffer) {
           // the probe failed, we must allocate a set of aggregation buffers
@@ -564,7 +566,7 @@ private void flush(boolean all) throws HiveException {
       while(iter.hasNext()) {
         Map.Entry<KeyWrapper, VectorAggregationBufferRow> pair = iter.next();
 
-        writeSingleRow((VectorHashKeyWrapper) pair.getKey(), pair.getValue());
+        writeSingleRow((VectorHashKeyWrapperBase) pair.getKey(), pair.getValue());
 
         if (!all) {
           iter.remove();
@@ -659,13 +661,13 @@ private class ProcessingModeStreaming extends ProcessingModeBase {
     /**
      * The current key, used in streaming mode
      */
-    private VectorHashKeyWrapper streamingKey;
+    private VectorHashKeyWrapperBase streamingKey;
 
     /**
      * The keys that needs to be flushed at the end of the current batch
      */
-    private final VectorHashKeyWrapper[] keysToFlush =
-        new VectorHashKeyWrapper[VectorizedRowBatch.DEFAULT_SIZE];
+    private final VectorHashKeyWrapperBase[] keysToFlush =
+        new VectorHashKeyWrapperBase[VectorizedRowBatch.DEFAULT_SIZE];
 
     /**
      * The aggregates that needs to be flushed at the end of the current batch
@@ -723,9 +725,9 @@ public void doProcessBatch(VectorizedRowBatch batch, boolean isFirstGroupingSet,
         keyWrappersBatch.evaluateBatchGroupingSets(batch, currentGroupingSetsOverrideIsNulls);
       }
 
-      VectorHashKeyWrapper[] batchKeys = keyWrappersBatch.getVectorHashKeyWrappers();
+      VectorHashKeyWrapperBase[] batchKeys = keyWrappersBatch.getVectorHashKeyWrappers();
 
-      final VectorHashKeyWrapper prevKey = streamingKey;
+      final VectorHashKeyWrapperBase prevKey = streamingKey;
       if (streamingKey == null) {
         // This is the first batch we process after switching from hash mode
         currentStreamingAggregators = streamAggregationBufferRowPool.getFromPool();
@@ -760,7 +762,7 @@ public void doProcessBatch(VectorizedRowBatch batch, boolean isFirstGroupingSet,
       }
 
       if (streamingKey != prevKey) {
-        streamingKey = (VectorHashKeyWrapper) streamingKey.copyKey();
+        streamingKey = (VectorHashKeyWrapperBase) streamingKey.copyKey();
       }
     }
 
@@ -1127,7 +1129,7 @@ public void process(Object row, int tag) throws HiveException {
    * @param agg
    * @throws HiveException
    */
-  private void writeSingleRow(VectorHashKeyWrapper kw, VectorAggregationBufferRow agg)
+  private void writeSingleRow(VectorHashKeyWrapperBase kw, VectorAggregationBufferRow agg)
       throws HiveException {
 
     int colNum = 0;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinOperator.java
index a84bd721e6..2d8e1d7cf9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinOperator.java
@@ -31,6 +31,8 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.MapJoinDesc;
@@ -69,7 +71,7 @@ public class VectorMapJoinOperator extends VectorMapJoinBaseOperator {
   // for the inner-loop supper.processOp callbacks
   //
   private transient int batchIndex;
-  private transient VectorHashKeyWrapper[] keyValues;
+  private transient VectorHashKeyWrapperBase[] keyValues;
   private transient VectorHashKeyWrapperBatch keyWrapperBatch;
   private transient VectorExpressionWriter[] keyOutputWriters;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
index 35f810fa14..c13510e489 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
@@ -30,6 +30,8 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
@@ -87,14 +89,14 @@ public class VectorSMBMapJoinOperator extends SMBMapJoinOperator
 
   private transient int batchIndex = -1;
 
-  private transient VectorHashKeyWrapper[] keyValues;
+  private transient VectorHashKeyWrapperBase[] keyValues;
 
   private transient SMBJoinKeyEvaluator keyEvaluator;
 
   private transient VectorExpressionWriter[] valueWriters;
 
   private interface SMBJoinKeyEvaluator {
-    List<Object> evaluate(VectorHashKeyWrapper kw) throws HiveException;
+    List<Object> evaluate(VectorHashKeyWrapperBase kw) throws HiveException;
 }
 
   /** Kryo ctor. */
@@ -193,7 +195,7 @@ public SMBJoinKeyEvaluator init() {
       }
 
       @Override
-      public List<Object> evaluate(VectorHashKeyWrapper kw) throws HiveException {
+      public List<Object> evaluate(VectorHashKeyWrapperBase kw) throws HiveException {
         for(int i = 0; i < keyExpressions.length; ++i) {
           key.set(i, keyWrapperBatch.getWritableKeyValue(kw, i, keyOutputWriters[i]));
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBase.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBase.java
new file mode 100644
index 0000000000..8bf2ccb164
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBase.java
@@ -0,0 +1,223 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.wrapper;
+
+import org.apache.hive.common.util.Murmur3;
+
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+import org.apache.hadoop.hive.ql.exec.KeyWrapper;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorColumnSetInfo;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+
+/**
+ * A hash map key wrapper for vectorized processing.
+ * It stores the key values as primitives in arrays for each supported primitive type.
+ * This works in conjunction with
+ * {@link org.apache.hadoop.hive.ql.exec.VectorHashKeyWrapperBatch VectorHashKeyWrapperBatch}
+ * to hash vectorized processing units (batches).
+ */
+public abstract class VectorHashKeyWrapperBase extends KeyWrapper {
+
+  public static final class HashContext {
+    private final Murmur3.IncrementalHash32 bytesHash = new Murmur3.IncrementalHash32();
+
+    public static Murmur3.IncrementalHash32 getBytesHash(HashContext ctx) {
+      if (ctx == null) {
+        return new Murmur3.IncrementalHash32();
+      }
+      return ctx.bytesHash;
+    }
+  }
+
+  protected int hashcode;
+
+  protected VectorHashKeyWrapperBase() {
+    hashcode = 0;
+  }
+
+  @Override
+  public void getNewKey(Object row, ObjectInspector rowInspector) throws HiveException {
+    throw new HiveException("Should not be called");
+  }
+
+  @Override
+  public void setHashKey() {
+    throw new RuntimeException("Not implemented");
+  }
+
+  @Override
+  public int hashCode() {
+    return hashcode;
+  }
+
+  @Override
+  public boolean equals(Object that) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  @Override
+  protected Object clone() {
+    throw new RuntimeException("Not implemented");
+  }
+
+  @Override
+  public KeyWrapper copyKey() {
+    return (KeyWrapper) clone();
+  }
+
+  @Override
+  public void copyKey(KeyWrapper oldWrapper) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public Object[] getKeyArray() {
+    throw new UnsupportedOperationException();
+  }
+
+  public void assignLong(int keyIndex, int index, long v) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  // FIXME: isNull is not updated; which might cause problems
+  @Deprecated
+  public void assignLong(int index, long v) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignNullLong(int keyIndex, int index) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignDouble(int index, double d) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignNullDouble(int keyIndex, int index) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignString(int index, byte[] bytes, int start, int length) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignNullString(int keyIndex, int index) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignDecimal(int index, HiveDecimalWritable value) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignNullDecimal(int keyIndex, int index) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignTimestamp(int index, Timestamp value) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignTimestamp(int index, TimestampColumnVector colVector, int elementNum) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignNullTimestamp(int keyIndex, int index) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignIntervalDayTime(int index, HiveIntervalDayTime value) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignIntervalDayTime(int index, IntervalDayTimeColumnVector colVector, int elementNum) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void assignNullIntervalDayTime(int keyIndex, int index) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  /*
+   * This method is mainly intended for debug display purposes.
+   */
+  public String stringifyKeys(VectorColumnSetInfo columnSetInfo)
+  {
+    throw new RuntimeException("Not implemented");
+  }
+
+  @Override
+  public String toString()
+  {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public long getLongValue(int i) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public double getDoubleValue(int i) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public byte[] getBytes(int i) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public int getByteStart(int i) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public int getByteLength(int i) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public HiveDecimalWritable getDecimal(int i) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public Timestamp getTimestamp(int i) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public HiveIntervalDayTime getIntervalDayTime(int i) {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public int getVariableSize() {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void clearIsNull() {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public void setNull() {
+    throw new RuntimeException("Not implemented");
+  }
+
+  public boolean isNull(int keyIndex) {
+    throw new RuntimeException("Not implemented");
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBatch.java
similarity index 96%
rename from ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java
rename to ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBatch.java
index 689d3c3243..dd31991d03 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBatch.java
@@ -16,13 +16,22 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.exec.vector;
+package org.apache.hadoop.hive.ql.exec.vector.wrapper;
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector.Type;
+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorColumnSetInfo;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
 /**
@@ -53,7 +62,7 @@ public VectorHashKeyWrapperBatch(int keyCount) {
    * Always clone the key wrapper to obtain an immutable keywrapper suitable
    * to use a key in a HashMap.
    */
-  private VectorHashKeyWrapper[] vectorHashKeyWrappers;
+  private VectorHashKeyWrapperBase[] vectorHashKeyWrappers;
 
   /**
    * The fixed size of the key wrappers.
@@ -63,7 +72,7 @@ public VectorHashKeyWrapperBatch(int keyCount) {
   /**
    * Shared hashcontext for all keys in this batch
    */
-  private final VectorHashKeyWrapper.HashContext hashCtx = new VectorHashKeyWrapper.HashContext();
+  private final VectorHashKeyWrapperBase.HashContext hashCtx = new VectorHashKeyWrapperBase.HashContext();
 
    /**
    * Returns the compiled fixed size for the key wrappers.
@@ -76,7 +85,7 @@ public int getKeysFixedSize() {
   /**
    * Accessor for the batch-sized array of key wrappers.
    */
-  public VectorHashKeyWrapper[] getVectorHashKeyWrappers() {
+  public VectorHashKeyWrapperBase[] getVectorHashKeyWrappers() {
     return vectorHashKeyWrappers;
   }
 
@@ -904,7 +913,7 @@ public static VectorHashKeyWrapperBatch compileKeyWrapperBatch(VectorExpression[
     compiledKeyWrapperBatch.finishAdding();
 
     compiledKeyWrapperBatch.vectorHashKeyWrappers =
-        new VectorHashKeyWrapper[VectorizedRowBatch.DEFAULT_SIZE];
+        new VectorHashKeyWrapperBase[VectorizedRowBatch.DEFAULT_SIZE];
     for(int i=0;i<VectorizedRowBatch.DEFAULT_SIZE; ++i) {
       compiledKeyWrapperBatch.vectorHashKeyWrappers[i] =
           compiledKeyWrapperBatch.allocateKeyWrapper();
@@ -934,8 +943,8 @@ public static VectorHashKeyWrapperBatch compileKeyWrapperBatch(VectorExpression[
     return compiledKeyWrapperBatch;
   }
 
-  public VectorHashKeyWrapper allocateKeyWrapper() {
-    return VectorHashKeyWrapper.allocate(hashCtx,
+  public VectorHashKeyWrapperBase allocateKeyWrapper() {
+    return VectorHashKeyWrapperFactory.allocate(hashCtx,
         longIndices.length,
         doubleIndices.length,
         stringIndices.length,
@@ -949,7 +958,7 @@ public VectorHashKeyWrapper allocateKeyWrapper() {
    * Get the row-mode writable object value of a key from a key wrapper
    * @param keyOutputWriter
    */
-  public Object getWritableKeyValue(VectorHashKeyWrapper kw, int keyIndex,
+  public Object getWritableKeyValue(VectorHashKeyWrapperBase kw, int keyIndex,
       VectorExpressionWriter keyOutputWriter)
     throws HiveException {
 
@@ -988,7 +997,7 @@ public Object getWritableKeyValue(VectorHashKeyWrapper kw, int keyIndex,
     }
   }
 
-  public void setLongValue(VectorHashKeyWrapper kw, int keyIndex, Long value)
+  public void setLongValue(VectorHashKeyWrapperBase kw, int keyIndex, Long value)
     throws HiveException {
 
     if (columnVectorTypes[keyIndex] != Type.LONG) {
@@ -1004,7 +1013,7 @@ public void setLongValue(VectorHashKeyWrapper kw, int keyIndex, Long value)
   }
 
   public void assignRowColumn(VectorizedRowBatch batch, int batchIndex, int keyIndex,
-      VectorHashKeyWrapper kw)
+      VectorHashKeyWrapperBase kw)
     throws HiveException {
 
     ColumnVector colVector = batch.cols[keyIndex];
@@ -1057,7 +1066,7 @@ public int getVariableSize(int batchSize) {
     int variableSize = 0;
     if ( 0 < stringIndices.length) {
       for (int k=0; k<batchSize; ++k) {
-        VectorHashKeyWrapper hkw = vectorHashKeyWrappers[k];
+        VectorHashKeyWrapperBase hkw = vectorHashKeyWrappers[k];
         variableSize += hkw.getVariableSize();
       }
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperEmpty.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperEmpty.java
new file mode 100644
index 0000000000..63fc0daeb1
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperEmpty.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.wrapper;
+
+import org.apache.hadoop.hive.ql.exec.vector.VectorColumnSetInfo;
+import org.apache.hive.common.util.HashCodeUtil;
+
+// No need to override assigns - all assign ops will fail due to 0 key size.
+public final class VectorHashKeyWrapperEmpty extends VectorHashKeyWrapperBase {
+
+  public final static VectorHashKeyWrapperBase EMPTY_KEY_WRAPPER = new VectorHashKeyWrapperEmpty();
+
+  private static final int emptyHashcode =
+      HashCodeUtil.calculateLongHashCode(88L);
+
+  private VectorHashKeyWrapperEmpty() {
+    super();
+  }
+
+  @Override
+  public void setHashKey() {
+    hashcode = emptyHashcode;
+  }
+
+  @Override
+  protected Object clone() {
+    // immutable
+    return this;
+  }
+
+  @Override
+  public boolean equals(Object that) {
+    if (that == this) {
+      // should only be one object
+      return true;
+    }
+    return super.equals(that);
+  }
+
+  public String stringifyKeys(VectorColumnSetInfo columnSetInfo)
+  {
+    return "";
+  }
+
+  @Override
+  public String toString()
+  {
+    return "nulls []";
+  }
+
+  @Override
+  public int getVariableSize() {
+    return 0;
+  }
+
+  @Override
+  public void clearIsNull() {
+    // Nothing to do.
+  }
+
+  @Override
+  public void setNull() {
+    // Nothing to do.
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperFactory.java
new file mode 100644
index 0000000000..5645c47c2d
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperFactory.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.wrapper;
+
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase.HashContext;
+
+public class VectorHashKeyWrapperFactory {
+
+  public static VectorHashKeyWrapperBase allocate(HashContext ctx, int longValuesCount,
+      int doubleValuesCount, int byteValuesCount, int decimalValuesCount, int timestampValuesCount,
+      int intervalDayTimeValuesCount, int keyCount) {
+
+    final int nonLongBytesCount =
+        doubleValuesCount + decimalValuesCount +
+        timestampValuesCount + intervalDayTimeValuesCount;
+
+    /*
+     * Add more special cases as desired.
+     * FUTURE: Consider writing a init time "classifier" that returns an enum so we don't have to
+     * FUTURE: analyze these counts over and over...
+     */
+    if (nonLongBytesCount == 0) {
+      if (byteValuesCount == 0) {
+        if (longValuesCount == 1) {
+          return new VectorHashKeyWrapperSingleLong();
+        } else if (longValuesCount == 2) {
+          return new VectorHashKeyWrapperTwoLong();
+        } else if (longValuesCount == 0) {
+          return VectorHashKeyWrapperEmpty.EMPTY_KEY_WRAPPER;
+        }
+      }
+    }
+
+    // Fall through to use the general wrapper.
+    return new VectorHashKeyWrapperGeneral(ctx, longValuesCount, doubleValuesCount, byteValuesCount,
+        decimalValuesCount, timestampValuesCount, intervalDayTimeValuesCount,
+        keyCount);
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperGeneral.java
similarity index 87%
rename from ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java
rename to ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperGeneral.java
index 38c31a516a..8fe53e7bc3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperGeneral.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.exec.vector;
+package org.apache.hadoop.hive.ql.exec.vector.wrapper;
 
 import org.apache.hadoop.hive.serde2.io.DateWritableV2;
 import org.apache.hive.common.util.Murmur3;
@@ -28,6 +28,9 @@
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.KeyWrapper;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorColumnSetInfo;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
@@ -44,18 +47,7 @@
  * {@link org.apache.hadoop.hive.ql.exec.VectorHashKeyWrapperBatch VectorHashKeyWrapperBatch}
  * to hash vectorized processing units (batches).
  */
-public class VectorHashKeyWrapper extends KeyWrapper {
-
-  public static final class HashContext {
-    private final Murmur3.IncrementalHash32 bytesHash = new Murmur3.IncrementalHash32();
-
-    public static Murmur3.IncrementalHash32 getBytesHash(HashContext ctx) {
-      if (ctx == null) {
-        return new Murmur3.IncrementalHash32();
-      }
-      return ctx.bytesHash;
-    }
-  }
+public class VectorHashKeyWrapperGeneral extends VectorHashKeyWrapperBase {
 
   private static final int[] EMPTY_INT_ARRAY = new int[0];
   private static final long[] EMPTY_LONG_ARRAY = new long[0];
@@ -65,8 +57,6 @@ public static Murmur3.IncrementalHash32 getBytesHash(HashContext ctx) {
   private static final Timestamp[] EMPTY_TIMESTAMP_ARRAY = new Timestamp[0];
   private static final HiveIntervalDayTime[] EMPTY_INTERVAL_DAY_TIME_ARRAY = new HiveIntervalDayTime[0];
 
-  public static final VectorHashKeyWrapper EMPTY_KEY_WRAPPER = new EmptyVectorHashKeyWrapper();
-
   private long[] longValues;
   private double[] doubleValues;
 
@@ -82,20 +72,22 @@ public static Murmur3.IncrementalHash32 getBytesHash(HashContext ctx) {
   private HiveIntervalDayTime[] intervalDayTimeValues;
   private static HiveIntervalDayTime ZERO_INTERVALDAYTIME= new HiveIntervalDayTime(0, 0);
 
+  private HashContext hashCtx;
+
+  private int keyCount;
+
   // NOTE: The null array is indexed by keyIndex, which is not available internally.  The mapping
   //       from a long, double, etc index to key index is kept once in the separate
   //       VectorColumnSetInfo object.
-  private boolean[] isNull;
-
-  private int hashcode;
+  protected boolean[] isNull;
 
-  private HashContext hashCtx;
-
-  private VectorHashKeyWrapper(HashContext ctx, int longValuesCount, int doubleValuesCount,
+  public VectorHashKeyWrapperGeneral(HashContext ctx, int longValuesCount, int doubleValuesCount,
           int byteValuesCount, int decimalValuesCount, int timestampValuesCount,
           int intervalDayTimeValuesCount,
           int keyCount) {
+    super();
     hashCtx = ctx;
+    this.keyCount = keyCount;
     longValues = longValuesCount > 0 ? new long[longValuesCount] : EMPTY_LONG_ARRAY;
     doubleValues = doubleValuesCount > 0 ? new double[doubleValuesCount] : EMPTY_DOUBLE_ARRAY;
     decimalValues = decimalValuesCount > 0 ? new HiveDecimalWritable[decimalValuesCount] : EMPTY_DECIMAL_ARRAY;
@@ -120,27 +112,10 @@ private VectorHashKeyWrapper(HashContext ctx, int longValuesCount, int doubleVal
       intervalDayTimeValues[i] = new HiveIntervalDayTime();
     }
     isNull = new boolean[keyCount];
-    hashcode = 0;
-  }
-
-  private VectorHashKeyWrapper() {
-  }
-
-  public static VectorHashKeyWrapper allocate(HashContext ctx, int longValuesCount, int doubleValuesCount,
-      int byteValuesCount, int decimalValuesCount, int timestampValuesCount,
-      int intervalDayTimeValuesCount, int keyCount) {
-    if ((longValuesCount + doubleValuesCount + byteValuesCount + decimalValuesCount
-        + timestampValuesCount + intervalDayTimeValuesCount) == 0) {
-      return EMPTY_KEY_WRAPPER;
-    }
-    return new VectorHashKeyWrapper(ctx, longValuesCount, doubleValuesCount, byteValuesCount,
-        decimalValuesCount, timestampValuesCount, intervalDayTimeValuesCount,
-        keyCount);
   }
 
-  @Override
-  public void getNewKey(Object row, ObjectInspector rowInspector) throws HiveException {
-    throw new HiveException("Should not be called");
+  private VectorHashKeyWrapperGeneral() {
+    super();
   }
 
   @Override
@@ -192,8 +167,8 @@ public int hashCode() {
 
   @Override
   public boolean equals(Object that) {
-    if (that instanceof VectorHashKeyWrapper) {
-      VectorHashKeyWrapper keyThat = (VectorHashKeyWrapper)that;
+    if (that instanceof VectorHashKeyWrapperGeneral) {
+      VectorHashKeyWrapperGeneral keyThat = (VectorHashKeyWrapperGeneral)that;
       // not comparing hashCtx - irrelevant
       return hashcode == keyThat.hashcode &&
           Arrays.equals(longValues, keyThat.longValues) &&
@@ -208,7 +183,7 @@ public boolean equals(Object that) {
     return false;
   }
 
-  private boolean bytesEquals(VectorHashKeyWrapper keyThat) {
+  private boolean bytesEquals(VectorHashKeyWrapperGeneral keyThat) {
     //By the time we enter here the byteValues.lentgh and isNull must have already been compared
     for (int i = 0; i < byteValues.length; ++i) {
       // the byte comparison is potentially expensive so is better to branch on null
@@ -229,13 +204,14 @@ private boolean bytesEquals(VectorHashKeyWrapper keyThat) {
 
   @Override
   protected Object clone() {
-    VectorHashKeyWrapper clone = new VectorHashKeyWrapper();
+    VectorHashKeyWrapperGeneral clone = new VectorHashKeyWrapperGeneral();
     duplicateTo(clone);
     return clone;
   }
 
-  public void duplicateTo(VectorHashKeyWrapper clone) {
+  private void duplicateTo(VectorHashKeyWrapperGeneral clone) {
     clone.hashCtx = hashCtx;
+    clone.keyCount = keyCount;
     clone.longValues = (longValues.length > 0) ? longValues.clone() : EMPTY_LONG_ARRAY;
     clone.doubleValues = (doubleValues.length > 0) ? doubleValues.clone() : EMPTY_DOUBLE_ARRAY;
     clone.isNull = isNull.clone();
@@ -289,20 +265,6 @@ public void duplicateTo(VectorHashKeyWrapper clone) {
   }
 
   @Override
-  public KeyWrapper copyKey() {
-    return (KeyWrapper) clone();
-  }
-
-  @Override
-  public void copyKey(KeyWrapper oldWrapper) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public Object[] getKeyArray() {
-    throw new UnsupportedOperationException();
-  }
-
   public void assignLong(int keyIndex, int index, long v) {
     isNull[keyIndex] = false;
     longValues[index] = v;
@@ -310,24 +272,29 @@ public void assignLong(int keyIndex, int index, long v) {
 
   // FIXME: isNull is not updated; which might cause problems
   @Deprecated
+  @Override
   public void assignLong(int index, long v) {
     longValues[index] = v;
   }
 
+  @Override
   public void assignNullLong(int keyIndex, int index) {
     isNull[keyIndex] = true;
     longValues[index] = 0; // assign 0 to simplify hashcode
   }
 
+  @Override
   public void assignDouble(int index, double d) {
     doubleValues[index] = d;
   }
 
+  @Override
   public void assignNullDouble(int keyIndex, int index) {
     isNull[keyIndex] = true;
     doubleValues[index] = 0; // assign 0 to simplify hashcode
   }
 
+  @Override
   public void assignString(int index, byte[] bytes, int start, int length) {
     Preconditions.checkState(bytes != null);
     byteValues[index] = bytes;
@@ -335,6 +302,7 @@ public void assignString(int index, byte[] bytes, int start, int length) {
     byteLengths[index] = length;
   }
 
+  @Override
   public void assignNullString(int keyIndex, int index) {
     isNull[keyIndex] = true;
     byteValues[index] = null;
@@ -343,15 +311,18 @@ public void assignNullString(int keyIndex, int index) {
     byteLengths[index] = -1;
   }
 
+  @Override
   public void assignDecimal(int index, HiveDecimalWritable value) {
     decimalValues[index].set(value);
   }
 
+  @Override
   public void assignNullDecimal(int keyIndex, int index) {
     isNull[keyIndex] = true;
     decimalValues[index].set(HiveDecimal.ZERO); // assign 0 to simplify hashcode
   }
 
+  @Override
   public void assignTimestamp(int index, Timestamp value) {
     // Do not assign the input value object to the timestampValues array element.
     // Always copy value using set* methods.
@@ -359,10 +330,12 @@ public void assignTimestamp(int index, Timestamp value) {
     timestampValues[index].setNanos(value.getNanos());
   }
 
+  @Override
   public void assignTimestamp(int index, TimestampColumnVector colVector, int elementNum) {
     colVector.timestampUpdate(timestampValues[index], elementNum);
   }
 
+  @Override
   public void assignNullTimestamp(int keyIndex, int index) {
     isNull[keyIndex] = true;
     // assign 0 to simplify hashcode
@@ -370,14 +343,17 @@ public void assignNullTimestamp(int keyIndex, int index) {
     timestampValues[index].setNanos(ZERO_TIMESTAMP.getNanos());
   }
 
+  @Override
   public void assignIntervalDayTime(int index, HiveIntervalDayTime value) {
     intervalDayTimeValues[index].set(value);
   }
 
+  @Override
   public void assignIntervalDayTime(int index, IntervalDayTimeColumnVector colVector, int elementNum) {
     intervalDayTimeValues[index].set(colVector.asScratchIntervalDayTime(elementNum));
   }
 
+  @Override
   public void assignNullIntervalDayTime(int keyIndex, int index) {
     isNull[keyIndex] = true;
     intervalDayTimeValues[index].set(ZERO_INTERVALDAYTIME); // assign 0 to simplify hashcode
@@ -386,6 +362,7 @@ public void assignNullIntervalDayTime(int keyIndex, int index) {
   /*
    * This method is mainly intended for debug display purposes.
    */
+  @Override
   public String stringifyKeys(VectorColumnSetInfo columnSetInfo)
   {
     StringBuilder sb = new StringBuilder();
@@ -605,78 +582,68 @@ public String toString()
     return sb.toString();
   }
 
+  @Override
   public long getLongValue(int i) {
     return longValues[i];
   }
 
+  @Override
   public double getDoubleValue(int i) {
     return doubleValues[i];
   }
 
+  @Override
   public byte[] getBytes(int i) {
     return byteValues[i];
   }
 
+  @Override
   public int getByteStart(int i) {
     return byteStarts[i];
   }
 
+  @Override
   public int getByteLength(int i) {
     return byteLengths[i];
   }
 
-  public int getVariableSize() {
-    int variableSize = 0;
-    for (int i=0; i<byteLengths.length; ++i) {
-      JavaDataModel model = JavaDataModel.get();
-      variableSize += model.lengthForByteArrayOfSize(byteLengths[i]);
-    }
-    return variableSize;
-  }
-
+  @Override
   public HiveDecimalWritable getDecimal(int i) {
     return decimalValues[i];
   }
 
+  @Override
   public Timestamp getTimestamp(int i) {
     return timestampValues[i];
   }
 
+  @Override
   public HiveIntervalDayTime getIntervalDayTime(int i) {
     return intervalDayTimeValues[i];
   }
 
+  @Override
+  public int getVariableSize() {
+    int variableSize = 0;
+    for (int i=0; i<byteLengths.length; ++i) {
+      JavaDataModel model = JavaDataModel.get();
+      variableSize += model.lengthForByteArrayOfSize(byteLengths[i]);
+    }
+    return variableSize;
+  }
+
+  @Override
   public void clearIsNull() {
     Arrays.fill(isNull, false);
   }
 
+  @Override
   public void setNull() {
     Arrays.fill(isNull, true);
   }
 
+  @Override
   public boolean isNull(int keyIndex) {
     return isNull[keyIndex];
   }
-
-  public static final class EmptyVectorHashKeyWrapper extends VectorHashKeyWrapper {
-    private EmptyVectorHashKeyWrapper() {
-      super(null, 0, 0, 0, 0, 0, 0, /* keyCount */ 0);
-      // no need to override assigns - all assign ops will fail due to 0 size
-    }
-
-    @Override
-    protected Object clone() {
-      // immutable
-      return this;
-    }
-
-    @Override
-    public boolean equals(Object that) {
-      if (that == this) {
-        // should only be one object
-        return true;
-      }
-      return super.equals(that);
-    }
-  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperSingleBase.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperSingleBase.java
new file mode 100644
index 0000000000..b34ac6ba73
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperSingleBase.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.wrapper;
+
+import org.apache.hive.common.util.HashCodeUtil;
+
+public abstract class VectorHashKeyWrapperSingleBase extends VectorHashKeyWrapperBase {
+
+  protected boolean isNull0;
+
+  protected static final int nullHashcode =
+      HashCodeUtil.calculateLongHashCode(238322L);
+
+  protected VectorHashKeyWrapperSingleBase() {
+    super();
+    isNull0 = false;
+  }
+
+  @Override
+  public void clearIsNull() {
+    isNull0 = false;
+  }
+
+  @Override
+  public void setNull() {
+    isNull0 = true;
+  }
+
+  @Override
+  public boolean isNull(int keyIndex) {
+    if (keyIndex == 0) {
+      return isNull0;
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperSingleLong.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperSingleLong.java
new file mode 100644
index 0000000000..7229836400
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperSingleLong.java
@@ -0,0 +1,131 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.wrapper;
+
+import org.apache.hadoop.hive.ql.exec.vector.VectorColumnSetInfo;
+import org.apache.hive.common.util.HashCodeUtil;
+
+public class VectorHashKeyWrapperSingleLong extends VectorHashKeyWrapperSingleBase {
+
+  private long longValue0;
+
+  protected VectorHashKeyWrapperSingleLong() {
+    super();
+    longValue0 = 0;
+  }
+
+  @Override
+  public void setHashKey() {
+    hashcode =
+        isNull0 ?
+            nullHashcode :
+            HashCodeUtil.calculateLongHashCode(longValue0);
+  }
+
+  @Override
+  public boolean equals(Object that) {
+    if (that instanceof VectorHashKeyWrapperSingleLong) {
+      VectorHashKeyWrapperSingleLong keyThat = (VectorHashKeyWrapperSingleLong) that;
+      return
+          isNull0 == keyThat.isNull0 &&
+          longValue0 == keyThat.longValue0;
+    }
+    return false;
+  }
+
+  @Override
+  protected Object clone() {
+    VectorHashKeyWrapperSingleLong clone = new VectorHashKeyWrapperSingleLong();
+    clone.isNull0 = isNull0;
+    clone.longValue0 = longValue0;
+    clone.hashcode = hashcode;
+    return clone;
+  }
+
+  public void assignLong(int keyIndex, int index, long v) {
+    if (keyIndex == 0 && index == 0) {
+      isNull0 = false;
+      longValue0 = v;
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+
+  // FIXME: isNull is not updated; which might cause problems
+  @Deprecated
+  public void assignLong(int index, long v) {
+    if (index == 0) {
+      longValue0 = v;
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+
+  public void assignNullLong(int keyIndex, int index) {
+    if (keyIndex == 0 && index == 0) {
+      isNull0 = true;
+      longValue0 = 0;
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+
+  /*
+   * This method is mainly intended for debug display purposes.
+   */
+  @Override
+  public String stringifyKeys(VectorColumnSetInfo columnSetInfo)
+  {
+    StringBuilder sb = new StringBuilder();
+    sb.append("longs [");
+    if (!isNull0) {
+      sb.append(longValue0);
+    } else {
+      sb.append("null");
+    }
+    sb.append("]");
+    return sb.toString();
+  }
+
+  @Override
+  public String toString()
+  {
+    StringBuilder sb = new StringBuilder();
+    sb.append("longs [");
+    sb.append(longValue0);
+    sb.append("], nulls [");
+    sb.append(isNull0);
+    sb.append("]");
+    return sb.toString();
+  }
+
+  @Override
+  public long getLongValue(int i) {
+    if (i == 0) {
+      return longValue0;
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+
+  @Override
+  public int getVariableSize() {
+    return 0;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperTwoBase.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperTwoBase.java
new file mode 100644
index 0000000000..292b9a813a
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperTwoBase.java
@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.wrapper;
+
+import org.apache.hive.common.util.HashCodeUtil;
+
+public abstract class VectorHashKeyWrapperTwoBase extends VectorHashKeyWrapperBase {
+
+  protected boolean isNull0;
+  protected boolean isNull1;
+
+  protected static final int null0Hashcode =
+      HashCodeUtil.calculateLongHashCode(8893L);
+  protected static final int null1Hashcode =
+      HashCodeUtil.calculateLongHashCode(255533L);
+  protected static final int twoNullHashcode =
+      HashCodeUtil.calculateLongHashCode(7566L);
+
+  protected VectorHashKeyWrapperTwoBase() {
+    super();
+    isNull0 = false;
+    isNull1 = false;
+  }
+
+  @Override
+  public void clearIsNull() {
+    isNull0 = false;
+    isNull1 = false;
+  }
+
+  @Override
+  public void setNull() {
+    isNull0 = true;
+    isNull1 = true;
+  }
+
+  @Override
+  public boolean isNull(int keyIndex) {
+    if (keyIndex == 0) {
+      return isNull0;
+    } else if (keyIndex == 1) {
+      return isNull1;
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperTwoLong.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperTwoLong.java
new file mode 100644
index 0000000000..165272887e
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperTwoLong.java
@@ -0,0 +1,170 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.wrapper;
+
+import org.apache.hadoop.hive.ql.exec.vector.VectorColumnSetInfo;
+import org.apache.hive.common.util.HashCodeUtil;
+
+public class VectorHashKeyWrapperTwoLong extends VectorHashKeyWrapperTwoBase {
+
+  private long longValue0;
+  private long longValue1;
+
+  protected VectorHashKeyWrapperTwoLong() {
+    super();
+    longValue0 = 0;
+    longValue1 = 0;
+  }
+
+  @Override
+  public void setHashKey() {
+    if (isNull0 || isNull1) {
+      hashcode =
+          (isNull0 && isNull1 ?
+            twoNullHashcode :
+            (isNull0 ?
+                null0Hashcode ^
+                HashCodeUtil.calculateLongHashCode(longValue1) :
+                HashCodeUtil.calculateLongHashCode(longValue0) ^
+                null1Hashcode));
+    } else {
+      hashcode =
+          HashCodeUtil.calculateLongHashCode(longValue0) >>> 16 ^
+          HashCodeUtil.calculateLongHashCode(longValue1);
+    }
+  }
+
+  @Override
+  public boolean equals(Object that) {
+    if (that instanceof VectorHashKeyWrapperTwoLong) {
+      VectorHashKeyWrapperTwoLong keyThat = (VectorHashKeyWrapperTwoLong) that;
+      return
+          isNull0 == keyThat.isNull0 &&
+          longValue0 == keyThat.longValue0 &&
+          isNull1 == keyThat.isNull1 &&
+          longValue1 == keyThat.longValue1;
+    }
+    return false;
+  }
+
+  @Override
+  protected Object clone() {
+    VectorHashKeyWrapperTwoLong clone = new VectorHashKeyWrapperTwoLong();
+    clone.isNull0 = isNull0;
+    clone.longValue0 = longValue0;
+    clone.isNull1 = isNull1;
+    clone.longValue1 = longValue1;
+    clone.hashcode = hashcode;
+    return clone;
+  }
+
+  @Override
+  public void assignLong(int keyIndex, int index, long v) {
+    if (keyIndex == 0 && index == 0) {
+      isNull0 = false;
+      longValue0 = v;
+    } else if (keyIndex == 1 && index == 1) {
+      isNull1 = false;
+      longValue1 = v;
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+
+  // FIXME: isNull is not updated; which might cause problems
+  @Deprecated
+  @Override
+  public void assignLong(int index, long v) {
+    if (index == 0) {
+      longValue0 = v;
+    } else if (index == 1) {
+      longValue1 = v;
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+
+  @Override
+  public void assignNullLong(int keyIndex, int index) {
+    if (keyIndex == 0 && index == 0) {
+      isNull0 = true;
+      longValue0 = 0;   // Assign 0 to make equals simple.
+    } else if (keyIndex == 1 && index == 1) {
+      isNull1 = true;
+      longValue1 = 0;   // Assign 0 to make equals simple.
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+
+  /*
+   * This method is mainly intended for debug display purposes.
+   */
+  @Override
+  public String stringifyKeys(VectorColumnSetInfo columnSetInfo)
+  {
+    StringBuilder sb = new StringBuilder();
+    sb.append("longs [");
+    if (!isNull0) {
+      sb.append(longValue0);
+    } else {
+      sb.append("null");
+    }
+    sb.append(", ");
+    if (!isNull1) {
+      sb.append(longValue1);
+    } else {
+      sb.append("null");
+    }
+    sb.append("]");
+    return sb.toString();
+  }
+
+  @Override
+  public String toString()
+  {
+    StringBuilder sb = new StringBuilder();
+    sb.append("longs [");
+    sb.append(longValue0);
+    sb.append(", ");
+    sb.append(longValue1);
+    sb.append("], nulls [");
+    sb.append(isNull0);
+    sb.append(", ");
+    sb.append(isNull1);
+    sb.append("]");
+    return sb.toString();
+  }
+
+  @Override
+  public long getLongValue(int i) {
+    if (i == 0) {
+      return longValue0;
+    } else if (i == 1) {
+      return longValue1;
+    } else {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+  }
+
+  @Override
+  public int getVariableSize() {
+    return 0;
+  }
+}
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorHashKeyWrapperBatch.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorHashKeyWrapperBatch.java
index e349fbd384..d50f6e6595 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorHashKeyWrapperBatch.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorHashKeyWrapperBatch.java
@@ -29,6 +29,8 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
+import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
@@ -73,8 +75,8 @@ public void testVectorHashKeyWrapperBatch() throws HiveException {
     batch.size = 3;
 
     vhkwb.evaluateBatch(batch);
-    VectorHashKeyWrapper[] vhkwArray = vhkwb.getVectorHashKeyWrappers();
-    VectorHashKeyWrapper vhk = vhkwArray[0];
+    VectorHashKeyWrapperBase[] vhkwArray = vhkwb.getVectorHashKeyWrappers();
+    VectorHashKeyWrapperBase vhk = vhkwArray[0];
     assertTrue(vhk.isNull(0));
     vhk = vhkwArray[1];
     assertFalse(vhk.isNull(0));
