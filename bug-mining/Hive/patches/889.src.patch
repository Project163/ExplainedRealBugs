diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
index 73575e580e..569a79d828 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
@@ -215,6 +215,10 @@ public enum ErrorMsg {
   UDAF_INVALID_LOCATION(10128, "Not yet supported place for UDAF"),
   DROP_PARTITION_NON_STRING_PARTCOLS_NONEQUALITY(10129,
     "Drop partitions for a non string partition columns is not allowed using non-equality"),
+  NUM_BUCKETS_CHANGE_NOT_ALLOWED(10130, "Changing the number of buckets for a " +
+    "partitioned table is not allowed. It may lead to wrong results for " +
+    "older partitions"),
+
 
   SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),
   SCRIPT_IO_ERROR(20001, "An error occurred while reading or writing to your custom script. "
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 182faf2223..d43afdcc31 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -1251,17 +1251,17 @@ private void analyzeAlterTablePartMergeFiles(ASTNode tablePartAST, ASTNode ast,
   private void analyzeAlterTableClusterSort(ASTNode ast)
       throws SemanticException {
     String tableName = getUnescapedName((ASTNode)ast.getChild(0));
+    Table tab = null;
 
     try {
-      Table tab = db.getTable(db.getCurrentDatabase(), tableName, false);
-      if (tab != null) {
-        inputs.add(new ReadEntity(tab));
-        outputs.add(new WriteEntity(tab));
-      }
+      tab = db.getTable(db.getCurrentDatabase(), tableName, true);
     } catch (HiveException e) {
       throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tableName));
     }
 
+    inputs.add(new ReadEntity(tab));
+    outputs.add(new WriteEntity(tab));
+
     if (ast.getChildCount() == 1) {
       // This means that we want to turn off bucketing
       AlterTableDesc alterTblDesc = new AlterTableDesc(tableName, -1,
@@ -1282,6 +1282,25 @@ private void analyzeAlterTableClusterSort(ASTNode ast)
       if (numBuckets <= 0) {
         throw new SemanticException(ErrorMsg.INVALID_BUCKET_NUMBER.getMsg());
       }
+
+      // If the table is partitioned, the number of buckets cannot be changed
+      // (unless the table is empty).
+      // The hive code uses bucket information from the table, and changing the
+      // number of buckets can lead to wrong results for bucketed join/sampling
+      // etc. This should be fixed as part of HIVE-3283.
+      // Once the above jira is fixed, this error check/message should be removed
+      if (tab.isPartitioned()) {
+        try {
+          List<String> partitionNames = db.getPartitionNames(tableName, (short)1);
+          if ((partitionNames != null) && (!partitionNames.isEmpty())) {
+            throw new
+              SemanticException(ErrorMsg.NUM_BUCKETS_CHANGE_NOT_ALLOWED.getMsg());
+          }
+        } catch (HiveException e) {
+          throw new SemanticException(e.getMessage());
+        }
+      }
+
       AlterTableDesc alterTblDesc = new AlterTableDesc(tableName, numBuckets,
           bucketCols, sortCols);
       rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
diff --git a/ql/src/test/queries/clientnegative/alter_numbuckets_partitioned_table.q b/ql/src/test/queries/clientnegative/alter_numbuckets_partitioned_table.q
new file mode 100644
index 0000000000..355a1eca21
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/alter_numbuckets_partitioned_table.q
@@ -0,0 +1,9 @@
+
+create table tst1(key string, value string) partitioned by (ds string) clustered by (key) into 10 buckets;
+
+alter table tst1 clustered by (key) into 8 buckets;
+
+set hive.enforce.bucketing=true;
+insert overwrite table tst1 partition (ds='1') select key, value from src;
+
+alter table tst1 clustered by (key) into 12 buckets;
diff --git a/ql/src/test/queries/clientpositive/bucket_groupby.q b/ql/src/test/queries/clientpositive/bucket_groupby.q
index ddce79700b..a531ba70a7 100644
--- a/ql/src/test/queries/clientpositive/bucket_groupby.q
+++ b/ql/src/test/queries/clientpositive/bucket_groupby.q
@@ -1,5 +1,6 @@
 create table clustergroupby(key string, value string) partitioned by(ds string);
 describe extended clustergroupby;
+alter table clustergroupby clustered by (key) into 1 buckets;
 
 insert overwrite table clustergroupby partition (ds='100') select key, value from src sort by key;
 
@@ -7,7 +8,6 @@ explain
 select key, count(1) from clustergroupby where ds='100' group by key limit 10;
 select key, count(1) from clustergroupby where ds='100' group by key limit 10;
 
-alter table clustergroupby clustered by (key) into 1 buckets;
 describe extended clustergroupby;
 insert overwrite table clustergroupby partition (ds='101') select key, value from src distribute by key;
 
@@ -41,6 +41,10 @@ select key, count(1) from clustergroupby  group by key;
 explain
 select key, count(1) from clustergroupby  group by key, 3;
 
+-- number of buckets cannot be changed, so drop the table
+drop table clustergroupby;
+create table clustergroupby(key string, value string) partitioned by(ds string);
+
 --sort columns--
 alter table clustergroupby clustered by (value) sorted by (key, value) into 1 buckets;
 describe extended clustergroupby;
@@ -56,6 +60,9 @@ explain
 select key, count(1) from clustergroupby  where ds='102'  group by key, value limit 10;
 select key, count(1) from clustergroupby  where ds='102'  group by key, value limit 10;
 
+-- number of buckets cannot be changed, so drop the table
+drop table clustergroupby;
+create table clustergroupby(key string, value string) partitioned by(ds string);
 
 alter table clustergroupby clustered by (value, key) sorted by (key) into 1 buckets;
 describe extended clustergroupby;
diff --git a/ql/src/test/queries/clientpositive/exim_04_evolved_parts.q b/ql/src/test/queries/clientpositive/exim_04_evolved_parts.q
index 941cb9aca6..ae1ed6fc42 100644
--- a/ql/src/test/queries/clientpositive/exim_04_evolved_parts.q
+++ b/ql/src/test/queries/clientpositive/exim_04_evolved_parts.q
@@ -9,14 +9,14 @@ create table exim_employee (emp_id int comment 'employee id', emp_name string, e
  row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" with serdeproperties ('serialization.format'='1')
  stored as rcfile;
 
+alter table exim_employee add columns (emp_dept int);
+alter table exim_employee clustered by (emp_sex, emp_dept) sorted by (emp_id desc) into 5 buckets;
 alter table exim_employee add partition (emp_country='in', emp_state='tn');
 
-alter table exim_employee add columns (emp_dept int);
 alter table exim_employee set serde "org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe" with serdeproperties ('serialization.format'='2');
 alter table exim_employee set fileformat 
 	inputformat "org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat" 
 	outputformat "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat";
-alter table exim_employee clustered by (emp_sex, emp_dept) sorted by (emp_id desc) into 5 buckets;
 
 alter table exim_employee add partition (emp_country='in', emp_state='ka');
 dfs -mkdir ../build/ql/test/data/exports/exim_employee/temp;
diff --git a/ql/src/test/results/clientnegative/alter_numbuckets_partitioned_table.q.out b/ql/src/test/results/clientnegative/alter_numbuckets_partitioned_table.q.out
new file mode 100644
index 0000000000..8eba68a67b
--- /dev/null
+++ b/ql/src/test/results/clientnegative/alter_numbuckets_partitioned_table.q.out
@@ -0,0 +1,24 @@
+PREHOOK: query: create table tst1(key string, value string) partitioned by (ds string) clustered by (key) into 10 buckets
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table tst1(key string, value string) partitioned by (ds string) clustered by (key) into 10 buckets
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tst1
+PREHOOK: query: alter table tst1 clustered by (key) into 8 buckets
+PREHOOK: type: ALTERTABLE_CLUSTER_SORT
+PREHOOK: Input: default@tst1
+PREHOOK: Output: default@tst1
+POSTHOOK: query: alter table tst1 clustered by (key) into 8 buckets
+POSTHOOK: type: ALTERTABLE_CLUSTER_SORT
+POSTHOOK: Input: default@tst1
+POSTHOOK: Output: default@tst1
+PREHOOK: query: insert overwrite table tst1 partition (ds='1') select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@tst1@ds=1
+POSTHOOK: query: insert overwrite table tst1 partition (ds='1') select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@tst1@ds=1
+POSTHOOK: Lineage: tst1 PARTITION(ds=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tst1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+FAILED: SemanticException [Error 10130]: Changing the number of buckets for a partitioned table is not allowed. It may lead to wrong results for older partitions
diff --git a/ql/src/test/results/clientpositive/bucket_groupby.q.out b/ql/src/test/results/clientpositive/bucket_groupby.q.out
index 43e398f9da..8e8b04faaf 100644
--- a/ql/src/test/results/clientpositive/bucket_groupby.q.out
+++ b/ql/src/test/results/clientpositive/bucket_groupby.q.out
@@ -12,6 +12,14 @@ value	string
 ds	string	
 	 	 
 #### A masked pattern was here ####
+PREHOOK: query: alter table clustergroupby clustered by (key) into 1 buckets
+PREHOOK: type: ALTERTABLE_CLUSTER_SORT
+PREHOOK: Input: default@clustergroupby
+PREHOOK: Output: default@clustergroupby
+POSTHOOK: query: alter table clustergroupby clustered by (key) into 1 buckets
+POSTHOOK: type: ALTERTABLE_CLUSTER_SORT
+POSTHOOK: Input: default@clustergroupby
+POSTHOOK: Output: default@clustergroupby
 PREHOOK: query: insert overwrite table clustergroupby partition (ds='100') select key, value from src sort by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -52,7 +60,7 @@ STAGE PLANS:
               Group By Operator
                 aggregations:
                       expr: count(1)
-                bucketGroup: false
+                bucketGroup: true
                 keys:
                       expr: key
                       type: string
@@ -120,16 +128,6 @@ POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.Field
 111	1
 113	2
 114	1
-PREHOOK: query: alter table clustergroupby clustered by (key) into 1 buckets
-PREHOOK: type: ALTERTABLE_CLUSTER_SORT
-PREHOOK: Input: default@clustergroupby
-PREHOOK: Output: default@clustergroupby
-POSTHOOK: query: alter table clustergroupby clustered by (key) into 1 buckets
-POSTHOOK: type: ALTERTABLE_CLUSTER_SORT
-POSTHOOK: Input: default@clustergroupby
-POSTHOOK: Output: default@clustergroupby
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: describe extended clustergroupby
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: describe extended clustergroupby
@@ -702,7 +700,7 @@ STAGE PLANS:
               Group By Operator
                 aggregations:
                       expr: count(1)
-                bucketGroup: false
+                bucketGroup: true
                 keys:
                       expr: key
                       type: string
@@ -1104,7 +1102,7 @@ STAGE PLANS:
               Group By Operator
                 aggregations:
                       expr: count(1)
-                bucketGroup: false
+                bucketGroup: true
                 keys:
                       expr: key
                       type: string
@@ -1159,6 +1157,29 @@ STAGE PLANS:
       limit: -1
 
 
+PREHOOK: query: -- number of buckets cannot be changed, so drop the table
+drop table clustergroupby
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@clustergroupby
+PREHOOK: Output: default@clustergroupby
+POSTHOOK: query: -- number of buckets cannot be changed, so drop the table
+drop table clustergroupby
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@clustergroupby
+POSTHOOK: Output: default@clustergroupby
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: create table clustergroupby(key string, value string) partitioned by(ds string)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table clustergroupby(key string, value string) partitioned by(ds string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@clustergroupby
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: --sort columns--
 alter table clustergroupby clustered by (value) sorted by (key, value) into 1 buckets
 PREHOOK: type: ALTERTABLE_CLUSTER_SORT
@@ -1528,6 +1549,33 @@ POSTHOOK: Lineage: clustergroupby PARTITION(ds=102).value SIMPLE [(src)src.Field
 111	1
 113	2
 114	1
+PREHOOK: query: -- number of buckets cannot be changed, so drop the table
+drop table clustergroupby
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@clustergroupby
+PREHOOK: Output: default@clustergroupby
+POSTHOOK: query: -- number of buckets cannot be changed, so drop the table
+drop table clustergroupby
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@clustergroupby
+POSTHOOK: Output: default@clustergroupby
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=102).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=102).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: create table clustergroupby(key string, value string) partitioned by(ds string)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table clustergroupby(key string, value string) partitioned by(ds string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@clustergroupby
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=102).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: clustergroupby PARTITION(ds=102).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: alter table clustergroupby clustered by (value, key) sorted by (key) into 1 buckets
 PREHOOK: type: ALTERTABLE_CLUSTER_SORT
 PREHOOK: Input: default@clustergroupby
diff --git a/ql/src/test/results/clientpositive/exim_04_evolved_parts.q.out b/ql/src/test/results/clientpositive/exim_04_evolved_parts.q.out
index d78ca85132..e5108333f2 100644
--- a/ql/src/test/results/clientpositive/exim_04_evolved_parts.q.out
+++ b/ql/src/test/results/clientpositive/exim_04_evolved_parts.q.out
@@ -13,13 +13,6 @@ POSTHOOK: query: create table exim_employee (emp_id int comment 'employee id', e
  stored as rcfile
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@exim_employee
-PREHOOK: query: alter table exim_employee add partition (emp_country='in', emp_state='tn')
-PREHOOK: type: ALTERTABLE_ADDPARTS
-PREHOOK: Input: default@exim_employee
-POSTHOOK: query: alter table exim_employee add partition (emp_country='in', emp_state='tn')
-POSTHOOK: type: ALTERTABLE_ADDPARTS
-POSTHOOK: Input: default@exim_employee
-POSTHOOK: Output: default@exim_employee@emp_country=in/emp_state=tn
 PREHOOK: query: alter table exim_employee add columns (emp_dept int)
 PREHOOK: type: ALTERTABLE_ADDCOLS
 PREHOOK: Input: default@exim_employee
@@ -28,6 +21,21 @@ POSTHOOK: query: alter table exim_employee add columns (emp_dept int)
 POSTHOOK: type: ALTERTABLE_ADDCOLS
 POSTHOOK: Input: default@exim_employee
 POSTHOOK: Output: default@exim_employee
+PREHOOK: query: alter table exim_employee clustered by (emp_sex, emp_dept) sorted by (emp_id desc) into 5 buckets
+PREHOOK: type: ALTERTABLE_CLUSTER_SORT
+PREHOOK: Input: default@exim_employee
+PREHOOK: Output: default@exim_employee
+POSTHOOK: query: alter table exim_employee clustered by (emp_sex, emp_dept) sorted by (emp_id desc) into 5 buckets
+POSTHOOK: type: ALTERTABLE_CLUSTER_SORT
+POSTHOOK: Input: default@exim_employee
+POSTHOOK: Output: default@exim_employee
+PREHOOK: query: alter table exim_employee add partition (emp_country='in', emp_state='tn')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: default@exim_employee
+POSTHOOK: query: alter table exim_employee add partition (emp_country='in', emp_state='tn')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: default@exim_employee
+POSTHOOK: Output: default@exim_employee@emp_country=in/emp_state=tn
 PREHOOK: query: alter table exim_employee set serde "org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe" with serdeproperties ('serialization.format'='2')
 PREHOOK: type: ALTERTABLE_SERIALIZER
 PREHOOK: Input: default@exim_employee
@@ -48,14 +56,6 @@ POSTHOOK: query: alter table exim_employee set fileformat
 POSTHOOK: type: ALTERTABLE_FILEFORMAT
 POSTHOOK: Input: default@exim_employee
 POSTHOOK: Output: default@exim_employee
-PREHOOK: query: alter table exim_employee clustered by (emp_sex, emp_dept) sorted by (emp_id desc) into 5 buckets
-PREHOOK: type: ALTERTABLE_CLUSTER_SORT
-PREHOOK: Input: default@exim_employee
-PREHOOK: Output: default@exim_employee
-POSTHOOK: query: alter table exim_employee clustered by (emp_sex, emp_dept) sorted by (emp_id desc) into 5 buckets
-POSTHOOK: type: ALTERTABLE_CLUSTER_SORT
-POSTHOOK: Input: default@exim_employee
-POSTHOOK: Output: default@exim_employee
 PREHOOK: query: alter table exim_employee add partition (emp_country='in', emp_state='ka')
 PREHOOK: type: ALTERTABLE_ADDPARTS
 PREHOOK: Input: default@exim_employee
@@ -118,6 +118,7 @@ emp_id	int	employee id
 emp_name	string	
 emp_dob	string	employee date of birth
 emp_sex	string	M/F
+emp_dept	int	
 emp_country	string	2-char code
 emp_state	string	2-char code
 	 	 
