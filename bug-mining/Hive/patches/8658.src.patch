diff --git a/iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java b/iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
index 7d49069b3f..84ad202375 100644
--- a/iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
+++ b/iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
@@ -85,6 +85,8 @@ public class HiveTableOperations extends BaseMetastoreTableOperations {
   private static final String NO_LOCK_EXPECTED_VALUE = "expected_parameter_value";
   private static final long HIVE_TABLE_PROPERTY_MAX_SIZE_DEFAULT = 32672;
 
+  private static final String HIVE_ICEBERG_STORAGE_HANDLER = "org.apache.iceberg.mr.hive.HiveIcebergStorageHandler";
+
   private static final BiMap<String, String> ICEBERG_TO_HMS_TRANSLATION = ImmutableBiMap.of(
       // gc.enabled in Iceberg and external.table.purge in Hive are meant to do the same things but with different names
       GC_ENABLED, "external.table.purge",
@@ -388,8 +390,11 @@ private void setHmsTableParameters(String newMetadataLocation, Table tbl, TableM
 
     // If needed set the 'storage_handler' property to enable query from Hive
     if (hiveEngineEnabled) {
-      parameters.put(hive_metastoreConstants.META_TABLE_STORAGE,
-          "org.apache.iceberg.mr.hive.HiveIcebergStorageHandler");
+      String storageHandler = parameters.get(hive_metastoreConstants.META_TABLE_STORAGE);
+      // Check if META_TABLE_STORAGE is not present or is not an instance of ICEBERG_STORAGE_HANDLER
+      if (storageHandler == null || !isHiveIcebergStorageHandler(storageHandler)) {
+        parameters.put(hive_metastoreConstants.META_TABLE_STORAGE, HIVE_ICEBERG_STORAGE_HANDLER);
+      }
     } else {
       parameters.remove(hive_metastoreConstants.META_TABLE_STORAGE);
     }
@@ -591,4 +596,19 @@ HiveLock lockObject(TableMetadata metadata) {
       return new NoLock();
     }
   }
+
+  /**
+   * Checks if the storage_handler property is already set to HIVE_ICEBERG_STORAGE_HANDLER.
+   * @param storageHandler Storage Handler class
+   * @return true if the storage_handler property is set to HIVE_ICEBERG_STORAGE_HANDLER
+   */
+  private static boolean isHiveIcebergStorageHandler(String storageHandler) {
+    try {
+      Class<?> storageHandlerClass = Class.forName(storageHandler);
+      Class<?> icebergStorageHandlerClass = Class.forName(HIVE_ICEBERG_STORAGE_HANDLER);
+      return icebergStorageHandlerClass.isAssignableFrom(storageHandlerClass);
+    } catch (ClassNotFoundException e) {
+      throw new RuntimeException("Error checking storage handler class", e);
+    }
+  }
 }
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
index 831edd83d0..819ad62685 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
@@ -85,6 +85,7 @@ private InputFormatConfig() {
   public static final String CATALOG_WAREHOUSE_TEMPLATE = "iceberg.catalog.%s.warehouse";
   public static final String CATALOG_CLASS_TEMPLATE = "iceberg.catalog.%s.catalog-impl";
   public static final String CATALOG_DEFAULT_CONFIG_PREFIX = "iceberg.catalog-default.";
+  public static final String QUERY_FILTERS = "iceberg.query.filters";
 
   public enum InMemoryDataModel {
     PIG,
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java
index 242d8d87e5..dd329c122a 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java
@@ -102,6 +102,19 @@ static Expression icebergDataFilterFromHiveConf(Configuration conf) {
     if (hiveFilter != null) {
       ExprNodeGenericFuncDesc exprNodeDesc = SerializationUtilities
           .deserializeObject(hiveFilter, ExprNodeGenericFuncDesc.class);
+      return getFilterExpr(conf, exprNodeDesc);
+    }
+    return null;
+  }
+
+  /**
+   * getFilterExpr extracts search argument from ExprNodeGenericFuncDesc and returns Iceberg Filter Expression
+   * @param conf - job conf
+   * @param exprNodeDesc - Describes a GenericFunc node
+   * @return Iceberg Filter Expression
+   */
+  static Expression getFilterExpr(Configuration conf, ExprNodeGenericFuncDesc exprNodeDesc) {
+    if (exprNodeDesc != null) {
       SearchArgument sarg = ConvertAstToSearchArg.create(conf, exprNodeDesc);
       try {
         return HiveIcebergFilterFactory.generateFilterExpression(sarg);
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java
index 7f4b9e12c3..b4d5ce98f5 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java
@@ -66,6 +66,7 @@
 import org.apache.iceberg.Table;
 import org.apache.iceberg.Transaction;
 import org.apache.iceberg.exceptions.NotFoundException;
+import org.apache.iceberg.expressions.Expression;
 import org.apache.iceberg.expressions.Expressions;
 import org.apache.iceberg.hadoop.Util;
 import org.apache.iceberg.io.FileIO;
@@ -421,11 +422,19 @@ private void commitTable(FileIO io, ExecutorService executor, OutputTable output
     Table table = null;
     String branchName = null;
 
+    Expression filterExpr = Expressions.alwaysTrue();
+
     for (JobContext jobContext : outputTable.jobContexts) {
       JobConf conf = jobContext.getJobConf();
       table = Optional.ofNullable(table).orElse(Catalogs.loadTable(conf, catalogProperties));
       branchName = conf.get(InputFormatConfig.OUTPUT_TABLE_SNAPSHOT_REF);
 
+      Expression jobContextFilterExpr = (Expression) SessionStateUtil.getResource(conf, InputFormatConfig.QUERY_FILTERS)
+          .orElse(Expressions.alwaysTrue());
+      if (!filterExpr.equals(jobContextFilterExpr)) {
+        filterExpr = Expressions.and(filterExpr, jobContextFilterExpr);
+      }
+      LOG.debug("Filter Expression :{}", filterExpr);
       LOG.info("Committing job has started for table: {}, using location: {}",
           table, generateJobLocation(outputTable.table.location(), conf, jobContext.getJobID()));
 
@@ -458,7 +467,7 @@ private void commitTable(FileIO io, ExecutorService executor, OutputTable output
                 .map(String::valueOf).collect(Collectors.joining(",")));
       } else {
         Long snapshotId = getSnapshotId(outputTable.table, branchName);
-        commitWrite(table, branchName, snapshotId, startTime, filesForCommit, operation);
+        commitWrite(table, branchName, snapshotId, startTime, filesForCommit, operation, filterExpr);
       }
     } else {
 
@@ -483,12 +492,13 @@ private Long getSnapshotId(Table table, String branchName) {
   /**
    * Creates and commits an Iceberg change with the provided data and delete files.
    * If there are no delete files then an Iceberg 'append' is created, otherwise Iceberg 'overwrite' is created.
-   * @param table The table we are changing
-   * @param startTime The start time of the commit - used only for logging
-   * @param results The object containing the new files we would like to add to the table
+   * @param table      The table we are changing
+   * @param startTime  The start time of the commit - used only for logging
+   * @param results    The object containing the new files we would like to add to the table
+   * @param filterExpr Filter expression for conflict detection filter
    */
   private void commitWrite(Table table, String branchName, Long snapshotId, long startTime,
-      FilesForCommit results, Operation operation) {
+      FilesForCommit results, Operation operation, Expression filterExpr) {
 
     if (!results.replacedDataFiles().isEmpty()) {
       OverwriteFiles write = table.newOverwrite();
@@ -501,6 +511,7 @@ private void commitWrite(Table table, String branchName, Long snapshotId, long s
       if (snapshotId != null) {
         write.validateFromSnapshot(snapshotId);
       }
+      write.conflictDetectionFilter(filterExpr);
       write.validateNoConflictingData();
       write.validateNoConflictingDeletes();
       write.commit();
@@ -525,6 +536,8 @@ private void commitWrite(Table table, String branchName, Long snapshotId, long s
       if (snapshotId != null) {
         write.validateFromSnapshot(snapshotId);
       }
+      write.conflictDetectionFilter(filterExpr);
+
       if (!results.dataFiles().isEmpty()) {
         write.validateDeletedFiles();
         write.validateNoConflictingDeleteFiles();
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
index cd50aa929c..7fcaebbfca 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
@@ -384,9 +384,14 @@ public DecomposedPredicate decomposePredicate(JobConf jobConf, Deserializer dese
       }
     }
     predicate.pushedPredicate = (ExprNodeGenericFuncDesc) pushedPredicate;
+    Expression filterExpr = (Expression) HiveIcebergInputFormat.getFilterExpr(conf, predicate.pushedPredicate);
+    if (filterExpr != null) {
+      SessionStateUtil.addResource(conf, InputFormatConfig.QUERY_FILTERS, filterExpr);
+    }
     return predicate;
   }
 
+
   @Override
   public boolean canProvideBasicStatistics() {
     return true;
@@ -748,8 +753,7 @@ public void storageHandlerCommit(Properties commitProperties, Operation operatio
     if (jobContextList.isEmpty()) {
       return;
     }
-
-    HiveIcebergOutputCommitter committer = new HiveIcebergOutputCommitter();
+    HiveIcebergOutputCommitter committer = getOutputCommitter();
     try {
       committer.commitJobs(jobContextList, operation);
     } catch (Throwable e) {
@@ -769,6 +773,10 @@ public void storageHandlerCommit(Properties commitProperties, Operation operatio
     }
   }
 
+  public HiveIcebergOutputCommitter getOutputCommitter() {
+    return new HiveIcebergOutputCommitter();
+  }
+
   @Override
   public boolean isAllowedAlterOperation(AlterTableType opType) {
     return HiveIcebergMetaHook.SUPPORTED_ALTER_OPS.contains(opType);
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerStub.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerStub.java
new file mode 100644
index 0000000000..69d61a58dc
--- /dev/null
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerStub.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.mr.hive;
+
+import java.util.concurrent.Phaser;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * HiveIcebergStorageHandlerStub is used only for unit tests.
+ * Currently, we use it to achieve a specific thread interleaving to simulate conflicts in concurrent writes
+ * deterministically.
+ */
+public class HiveIcebergStorageHandlerStub extends HiveIcebergStorageHandler {
+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergStorageHandlerStub.class);
+
+  @Override
+  public HiveIcebergOutputCommitter getOutputCommitter() {
+
+    try {
+      LOG.debug(" Using HiveIcebergStorageHandlerStub for unit tests");
+      if (TestUtilPhaser.isInstantiated()) {
+        Phaser testUtilPhaser = TestUtilPhaser.getInstance().getPhaser();
+        LOG.debug("Activating the Phaser Barrier for thread: {} ", Thread.currentThread().getName());
+        testUtilPhaser.arriveAndAwaitAdvance();
+        LOG.debug("Breaking the Phaser Barrier and deregistering the phaser for thread: {} ",
+            Thread.currentThread().getName());
+      }
+    } catch (Exception e) {
+      throw new RuntimeException("Phaser failed: ", e);
+    }
+
+    return new HiveIcebergOutputCommitter();
+  }
+
+}
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerWithEngineBase.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerWithEngineBase.java
index b5c9e2942b..8466810624 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerWithEngineBase.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerWithEngineBase.java
@@ -165,7 +165,7 @@ public static Collection<Object[]> parameters() {
   public TemporaryFolder temp = new TemporaryFolder();
 
   @Rule
-  public Timeout timeout = new Timeout(400_000, TimeUnit.MILLISECONDS);
+  public Timeout timeout = new Timeout(500_000, TimeUnit.MILLISECONDS);
 
   @BeforeClass
   public static void beforeClass() {
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestConflictingDataFiles.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestConflictingDataFiles.java
new file mode 100644
index 0000000000..d89a909998
--- /dev/null
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestConflictingDataFiles.java
@@ -0,0 +1,241 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.apache.iceberg.mr.hive;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.Executors;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.catalog.TableIdentifier;
+import org.apache.iceberg.data.Record;
+import org.apache.iceberg.exceptions.ValidationException;
+import org.apache.iceberg.mr.TestHelper;
+import org.apache.iceberg.relocated.com.google.common.base.Throwables;
+import org.apache.iceberg.util.Tasks;
+import org.junit.After;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerTestUtils.init;
+
+public class TestConflictingDataFiles extends HiveIcebergStorageHandlerWithEngineBase {
+
+  private final String storageHandlerStub = "'org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerStub'";
+
+  @Before
+  public void setUpTables() {
+    PartitionSpec spec =
+        PartitionSpec.builderFor(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA).identity("last_name")
+            .bucket("customer_id", 16).build();
+
+    // create and insert an initial batch of records
+    testTables.createTable(shell, "customers", HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, spec, fileFormat,
+        HiveIcebergStorageHandlerTestUtils.OTHER_CUSTOMER_RECORDS_2, 2, Collections.emptyMap(), storageHandlerStub);
+    // insert one more batch so that we have multiple data files within the same partition
+    shell.executeStatement(testTables.getInsertQuery(HiveIcebergStorageHandlerTestUtils.OTHER_CUSTOMER_RECORDS_1,
+        TableIdentifier.of("default", "customers"), false));
+    TestUtilPhaser.getInstance();
+  }
+
+  @After
+  public void destroyTestSetUp() {
+    TestUtilPhaser.destroyInstance();
+  }
+
+  @Test
+  public void testSingleFilterUpdate() {
+    String[] singleFilterQuery = new String[] { "UPDATE customers SET first_name='Changed' WHERE  last_name='Taylor'",
+        "UPDATE customers SET first_name='Changed' WHERE  last_name='Donnel'" };
+
+    try {
+      Tasks.range(2).executeWith(Executors.newFixedThreadPool(2)).run(i -> {
+        TestUtilPhaser.getInstance().getPhaser().register();
+        init(shell, testTables, temp, executionEngine);
+        HiveConf.setBoolVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, isVectorized);
+        HiveConf.setVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_FETCH_TASK_CONVERSION, "none");
+        HiveConf.setVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES,
+            RETRY_STRATEGIES_WITHOUT_WRITE_CONFLICT);
+        shell.executeStatement(singleFilterQuery[i]);
+        shell.closeSession();
+      });
+      List<Object[]> objects =
+          shell.executeStatement("SELECT * FROM customers ORDER BY customer_id, last_name, first_name");
+      Assert.assertEquals(12, objects.size());
+      List<Record> expected = TestHelper.RecordsBuilder.newInstance(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA)
+          .add(1L, "Joanna", "Pierce")
+          .add(1L, "Changed", "Taylor")
+          .add(2L, "Changed", "Donnel")
+          .add(2L, "Susan", "Morrison")
+          .add(2L, "Bob", "Silver")
+          .add(2L, "Joanna", "Silver")
+          .add(3L, "Marci", "Barna")
+          .add(3L, "Blake", "Burr")
+          .add(3L, "Trudy", "Henderson")
+          .add(3L, "Trudy", "Johnson")
+          .add(4L, "Laci", "Zold")
+          .add(5L, "Peti", "Rozsaszin").build();
+      HiveIcebergTestUtils.validateData(expected,
+          HiveIcebergTestUtils.valueForRow(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, objects), 0);
+
+    } catch (Throwable ex) {
+      Throwable cause = Throwables.getRootCause(ex);
+      Assert.fail(String.valueOf(cause));
+    }
+  }
+
+  @Test
+  public void testMultiFiltersUpdate() {
+
+    String[] multiFilterQuery =
+        new String[] { "UPDATE customers SET first_name='Changed' WHERE  last_name='Henderson' OR last_name='Johnson'",
+            "UPDATE customers SET first_name='Changed' WHERE  last_name='Taylor' AND customer_id=1" };
+
+    try {
+      Tasks.range(2).executeWith(Executors.newFixedThreadPool(2)).run(i -> {
+        TestUtilPhaser.getInstance().getPhaser().register();
+        init(shell, testTables, temp, executionEngine);
+        HiveConf.setBoolVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, isVectorized);
+        HiveConf.setVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_FETCH_TASK_CONVERSION, "none");
+        HiveConf.setVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES,
+            RETRY_STRATEGIES_WITHOUT_WRITE_CONFLICT);
+        shell.executeStatement(multiFilterQuery[i]);
+        shell.closeSession();
+      });
+    } catch (Throwable ex) {
+      // If retry succeeds then it should not throw an ValidationException.
+      Throwable cause = Throwables.getRootCause(ex);
+      Assert.assertTrue(cause instanceof ValidationException);
+      if (cause.getMessage().matches("^Found.*conflicting.*files(.*)")) {
+        Assert.fail();
+      }
+    }
+
+    List<Object[]> objects =
+        shell.executeStatement("SELECT * FROM customers ORDER BY customer_id, last_name, first_name");
+    Assert.assertEquals(12, objects.size());
+    List<Record> expected = TestHelper.RecordsBuilder.newInstance(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA)
+        .add(1L, "Joanna", "Pierce")
+        .add(1L, "Changed", "Taylor")
+        .add(2L, "Jake", "Donnel")
+        .add(2L, "Susan", "Morrison")
+        .add(2L, "Bob", "Silver")
+        .add(2L, "Joanna", "Silver")
+        .add(3L, "Marci", "Barna")
+        .add(3L, "Blake", "Burr")
+        .add(3L, "Changed", "Henderson")
+        .add(3L, "Changed", "Johnson")
+        .add(4L, "Laci", "Zold")
+        .add(5L, "Peti", "Rozsaszin")
+        .build();
+    HiveIcebergTestUtils.validateData(expected,
+        HiveIcebergTestUtils.valueForRow(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, objects), 0);
+  }
+
+  @Test
+  public void testDeleteFilters() {
+    String[] sql = new String[] { "DELETE FROM customers WHERE  last_name='Taylor'",
+        "DELETE FROM customers WHERE last_name='Donnel'",
+        "DELETE FROM customers WHERE last_name='Henderson' OR last_name='Johnson'" };
+
+    try {
+      Tasks.range(3).executeWith(Executors.newFixedThreadPool(3)).run(i -> {
+        TestUtilPhaser.getInstance().getPhaser().register();
+        init(shell, testTables, temp, executionEngine);
+        HiveConf.setBoolVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, isVectorized);
+        HiveConf.setVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_FETCH_TASK_CONVERSION, "none");
+        HiveConf.setVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES,
+            RETRY_STRATEGIES_WITHOUT_WRITE_CONFLICT);
+        shell.executeStatement(sql[i]);
+        shell.closeSession();
+      });
+    } catch (Throwable ex) {
+      Throwable cause = Throwables.getRootCause(ex);
+      Assert.assertTrue(cause instanceof ValidationException);
+      if (cause.getMessage().matches("^Found.*conflicting.*files(.*)")) {
+        Assert.fail();
+      }
+    }
+
+    List<Object[]> objects =
+        shell.executeStatement("SELECT * FROM customers ORDER BY customer_id, last_name, first_name");
+    Assert.assertEquals(8, objects.size());
+    List<Record> expected = TestHelper.RecordsBuilder.newInstance(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA)
+        .add(1L, "Joanna", "Pierce")
+        .add(2L, "Susan", "Morrison")
+        .add(2L, "Bob", "Silver")
+        .add(2L, "Joanna", "Silver")
+        .add(3L, "Marci", "Barna")
+        .add(3L, "Blake", "Burr")
+        .add(4L, "Laci", "Zold")
+        .add(5L, "Peti", "Rozsaszin")
+        .build();
+    HiveIcebergTestUtils.validateData(expected,
+        HiveIcebergTestUtils.valueForRow(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, objects), 0);
+    TestUtilPhaser.destroyInstance();
+  }
+
+  @Test
+  public void testConflictingUpdates() {
+    String[] singleFilterQuery = new String[] { "UPDATE customers SET first_name='Changed' WHERE  last_name='Taylor'",
+        "UPDATE customers SET first_name='Changed' WHERE  last_name='Taylor'" };
+
+    try {
+      Tasks.range(2).executeWith(Executors.newFixedThreadPool(2)).run(i -> {
+        TestUtilPhaser.getInstance().getPhaser().register();
+        init(shell, testTables, temp, executionEngine);
+        HiveConf.setBoolVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, isVectorized);
+        HiveConf.setVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_FETCH_TASK_CONVERSION, "none");
+        HiveConf.setVar(shell.getHiveConf(), HiveConf.ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES,
+            RETRY_STRATEGIES_WITHOUT_WRITE_CONFLICT);
+        shell.executeStatement(singleFilterQuery[i]);
+        shell.closeSession();
+      });
+    } catch (Throwable ex) {
+      // Since there is a conflict it should throw ValidationException
+      Throwable cause = Throwables.getRootCause(ex);
+      Assert.assertTrue(cause instanceof ValidationException);
+      Assert.assertTrue(cause.getMessage().matches("^Found.*conflicting" + ".*files(.*)"));
+    }
+
+    List<Object[]> objects =
+        shell.executeStatement("SELECT * FROM customers ORDER BY customer_id, last_name, first_name");
+    Assert.assertEquals(12, objects.size());
+    List<Record> expected = TestHelper.RecordsBuilder.newInstance(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA)
+        .add(1L, "Joanna", "Pierce")
+        .add(1L, "Changed", "Taylor")
+        .add(2L, "Jake", "Donnel")
+        .add(2L, "Susan", "Morrison")
+        .add(2L, "Bob", "Silver")
+        .add(2L, "Joanna", "Silver")
+        .add(3L, "Marci", "Barna")
+        .add(3L, "Blake", "Burr")
+        .add(3L, "Trudy", "Henderson")
+        .add(3L, "Trudy", "Johnson")
+        .add(4L, "Laci", "Zold")
+        .add(5L, "Peti", "Rozsaszin")
+        .build();
+    HiveIcebergTestUtils.validateData(expected,
+        HiveIcebergTestUtils.valueForRow(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, objects), 0);
+
+  }
+}
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestTables.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestTables.java
index 656f16b4a1..e331aebbd1 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestTables.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestTables.java
@@ -30,6 +30,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.metastore.HiveMetaHook;
 import org.apache.iceberg.CatalogProperties;
 import org.apache.iceberg.CatalogUtil;
 import org.apache.iceberg.FileFormat;
@@ -337,6 +338,28 @@ public Table createTable(TestHiveShell shell, String tableName, Schema schema, P
    */
   public Table createTable(TestHiveShell shell, String tableName, Schema schema, PartitionSpec spec,
       FileFormat fileFormat, List<Record> records, Integer formatVersion, Map<String, String> tblProperties) {
+    return createTable(shell, tableName, schema, spec, fileFormat, records, formatVersion, tblProperties,
+        HiveMetaHook.ICEBERG);
+  }
+
+  /**
+   * Creates a partitioned Hive test table using Hive SQL. The table will be in the 'default' database.
+   * The table will be populated with the provided List of {@link Record}s using a Hive insert statement.
+   * @param shell The HiveShell used for Hive table creation
+   * @param tableName The name of the test table
+   * @param schema The schema used for the table creation
+   * @param spec The partition specification for the table
+   * @param fileFormat The file format used for writing the data
+   * @param records The records with which the table is populated
+   * @param formatVersion The version of the spec the table should use (format-version)
+   * @param tblProperties Additional table properties
+   * @param storageHandler Storage Handler to be used
+   * @return The created table
+   * @throws IOException If there is an error writing data
+   */
+  public Table createTable(TestHiveShell shell, String tableName, Schema schema, PartitionSpec spec,
+      FileFormat fileFormat, List<Record> records, Integer formatVersion, Map<String, String> tblProperties,
+      String storageHandler) {
     TableIdentifier identifier = TableIdentifier.of("default", tableName);
     String tblProps = propertiesForCreateTableSQL(ImmutableMap.<String, String>builder().putAll(tblProperties)
         .put(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.toString())
@@ -345,7 +368,8 @@ public Table createTable(TestHiveShell shell, String tableName, Schema schema, P
         .put(TableProperties.FORMAT_VERSION, Integer.toString(formatVersion))
         .build());
     shell.executeStatement("CREATE EXTERNAL TABLE " + identifier +
-        " STORED BY ICEBERG " + locationForCreateTableSQL(identifier) + tblProps);
+        " STORED BY " + storageHandler + " " +
+        locationForCreateTableSQL(identifier) + tblProps);
 
     if (records != null && !records.isEmpty()) {
       String query = getInsertQuery(records, identifier, false);
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestUtilPhaser.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestUtilPhaser.java
new file mode 100644
index 0000000000..169ae9a1b1
--- /dev/null
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestUtilPhaser.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.mr.hive;
+
+import java.util.concurrent.Phaser;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class TestUtilPhaser {
+
+  private static final Logger LOG = LoggerFactory.getLogger(TestUtilPhaser.class);
+  private static TestUtilPhaser instance;
+  private final Phaser phaser;
+
+  private TestUtilPhaser() {
+    phaser = new Phaser();
+  }
+
+  public static synchronized TestUtilPhaser getInstance() {
+    if (instance == null) {
+      LOG.info("UnitTestConcurrency: Instantiating the Phaser barrier");
+      instance = new TestUtilPhaser();
+    }
+    return instance;
+  }
+
+  public Phaser getPhaser() {
+    return phaser;
+  }
+
+  public static synchronized boolean isInstantiated() {
+    return instance != null;
+  }
+
+  public static synchronized void destroyInstance() {
+    if (instance != null) {
+      instance.getPhaser().forceTermination();
+      LOG.info("UnitTestConcurrency: Destroying the Phaser barrier");
+      instance = null;
+    }
+  }
+}
