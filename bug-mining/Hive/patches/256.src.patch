diff --git a/CHANGES.txt b/CHANGES.txt
index 90b0071633..84285d4129 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -137,6 +137,9 @@ Trunk -  Unreleased
     HIVE-716. Web interface wait/notify, interface changes
     (Edward Capriolo via namit)
 
+    HIVE-831. Remove ASTPartitionPruner (missed this in HIVE-844)
+    (Namit Jain via rmurthy)
+
 Release 0.4.0 -  Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 60df1a14ad..46694e4980 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -91,27 +91,27 @@ public static enum ConfVars {
 
     // session identifier
     HIVESESSIONID("hive.session.id", ""),
-    
+
     // query being executed (multiple per session)
     HIVEQUERYSTRING("hive.query.string", ""),
-    
+
     // id of query being executed (multiple per session)
     HIVEQUERYID("hive.query.id", ""),
-    
+
     // id of the mapred plan being executed (multiple per query)
     HIVEPLANID("hive.query.planid", ""),
     // max jobname length
     HIVEJOBNAMELENGTH("hive.jobname.length", 50),
-    
+
     // hive jar
-    HIVEJAR("hive.jar.path", ""), 
+    HIVEJAR("hive.jar.path", ""),
     HIVEAUXJARS("hive.aux.jars.path", ""),
-    
+
     // hive added files and jars
     HIVEADDEDFILES("hive.added.files.path", ""),
     HIVEADDEDJARS("hive.added.jars.path", ""),
     HIVEADDEDARCHIVES("hive.added.archives.path", ""),
-   
+
     // for hive script operator
     HIVETABLENAME("hive.table.name", ""),
     HIVEPARTITIONNAME("hive.partition.name", ""),
@@ -126,19 +126,19 @@ public static enum ConfVars {
     HIVEGROUPBYMAPINTERVAL("hive.groupby.mapaggr.checkinterval", 100000),
     HIVEMAPAGGRHASHMEMORY("hive.map.aggr.hash.percentmemory", (float)0.5),
     HIVEMAPAGGRHASHMINREDUCTION("hive.map.aggr.hash.min.reduction", (float)0.5),
-    
+
     // Default file format for CREATE TABLE statement
     // Options: TextFile, SequenceFile
     HIVEDEFAULTFILEFORMAT("hive.default.fileformat", "TextFile"),
-    
+
     //Location of Hive run time structured log file
     HIVEHISTORYFILELOC("hive.querylog.location",  "/tmp/"+System.getProperty("user.name")),
-    
+
     // Default serde and record reader for user scripts
     HIVESCRIPTSERDE("hive.script.serde", "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"),
     HIVESCRIPTRECORDREADER("hive.script.recordreader", "org.apache.hadoop.hive.ql.exec.TextRecordReader"),
     HIVESCRIPTRECORDWRITER("hive.script.recordwriter", "org.apache.hadoop.hive.ql.exec.TextRecordWriter"),
-    
+
     // HWI
     HIVEHWILISTENHOST("hive.hwi.listen.host","0.0.0.0"),
     HIVEHWILISTENPORT("hive.hwi.listen.port","9999"),
@@ -156,7 +156,7 @@ public static enum ConfVars {
     HIVEMERGEMAPFILES("hive.merge.mapfiles", true),
     HIVEMERGEMAPREDFILES("hive.merge.mapredfiles", false),
     HIVEMERGEMAPFILESSIZE("hive.merge.size.per.task", (long)(256*1000*1000)),
-      
+
     HIVESENDHEARTBEAT("hive.heartbeat.interval", 1000),
 
     HIVEJOBPROGRESS("hive.task.progress", false),
@@ -165,9 +165,8 @@ public static enum ConfVars {
 
     // Optimizer
     HIVEOPTCP("hive.optimize.cp", true), // column pruner
-    HIVEOPTPPD("hive.optimize.ppd", true), // predicate pushdown
-    HIVEOPTPPR("hive.optimize.pruner", true); // partition pruner
-    
+    HIVEOPTPPD("hive.optimize.ppd", true); // predicate pushdown
+
     public final String varname;
     public final String defaultVal;
     public final int defaultIntVal;
@@ -235,7 +234,7 @@ public static int getIntVar(Configuration conf, ConfVars var) {
     assert(var.valClass == Integer.class);
     return conf.getInt(var.varname, var.defaultIntVal);
   }
-  
+
   public int getIntVar(ConfVars var) {
     return getIntVar(this, var);
   }
@@ -294,7 +293,7 @@ public void logVars(PrintStream ps) {
   public HiveConf() {
     super();
   }
-  
+
   public HiveConf(Class<?> cls) {
     super();
     initialize(cls);
@@ -318,11 +317,11 @@ private Properties getUnderlyingProps() {
 
   private void initialize(Class<?> cls) {
     hiveJar = (new JobConf(cls)).getJar();
-    
+
     // preserve the original configuration
     origProp = getUnderlyingProps();
-    
-    // let's add the hive configuration 
+
+    // let's add the hive configuration
     URL hconfurl = getClassLoader().getResource("hive-default.xml");
     if(hconfurl == null) {
       l4j.debug("hive-default.xml not found.");
@@ -336,10 +335,10 @@ private void initialize(Class<?> cls) {
       addResource(hsiteurl);
     }
 
-    // if hadoop configuration files are already in our path - then define 
+    // if hadoop configuration files are already in our path - then define
     // the containing directory as the configuration directory
     URL hadoopconfurl = getClassLoader().getResource("hadoop-default.xml");
-    if(hadoopconfurl == null) 
+    if(hadoopconfurl == null)
       hadoopconfurl = getClassLoader().getResource("hadoop-site.xml");
     if(hadoopconfurl != null) {
       String conffile = hadoopconfurl.getPath();
@@ -353,7 +352,7 @@ private void initialize(Class<?> cls) {
     if(hiveJar == null) {
       hiveJar = this.get(ConfVars.HIVEJAR.varname);
     }
-    
+
     if(auxJars == null) {
       auxJars = this.get(ConfVars.HIVEAUXJARS.varname);
     }
@@ -405,7 +404,7 @@ public void setAuxJars(String auxJars) {
     this.auxJars = auxJars;
     setVar(this, ConfVars.HIVEAUXJARS, auxJars);
   }
-  
+
   /**
    * @return the user name set in hadoop.job.ugi param or the current user from System
    * @throws IOException
@@ -421,7 +420,7 @@ public String getUser() throws IOException {
       throw (IOException)new IOException().initCause(e);
     }
   }
-  
+
   public static String getColumnInternalName(int pos){
     return "_col"+pos;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
index 39c4f74501..fed133446e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -58,6 +58,7 @@
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx;
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRMapJoinCtx;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner;
 
 /**
  * General utility common functions for the Processor to convert operator into map-reduce tasks
@@ -86,7 +87,7 @@ public static void initPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx) t
     opTaskMap.put(reducer, currTask);
     plan.setReducer(reducer);
     reduceSinkDesc desc = (reduceSinkDesc)op.getConf();
-    
+
     plan.setNumReduceTasks(desc.getNumReducers());
 
     List<Task<? extends Serializable>> rootTasks = opProcCtx.getRootTasks();
@@ -117,7 +118,7 @@ public static void initPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx) t
    * @param pos position of the parent
    */
   public static void initMapJoinPlan(Operator<? extends Serializable> op, GenMRProcContext opProcCtx, boolean readInputMapJoin, boolean readInputUnion,
-      boolean setReducer, int pos) 
+      boolean setReducer, int pos)
     throws SemanticException {
     Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = opProcCtx.getMapCurrCtx();
     assert (((pos == -1) && (readInputMapJoin)) || (pos != -1));
@@ -137,39 +138,39 @@ public static void initMapJoinPlan(Operator<? extends Serializable> op, GenMRPro
       if (setReducer) {
         Operator<? extends Serializable> reducer = op.getChildOperators().get(0);
         plan.setReducer(reducer);
-        opTaskMap.put(reducer, currTask);      
+        opTaskMap.put(reducer, currTask);
         if (reducer.getClass() == JoinOperator.class)
           plan.setNeedsTagging(true);
-        reduceSinkDesc desc = (reduceSinkDesc)op.getConf();      
+        reduceSinkDesc desc = (reduceSinkDesc)op.getConf();
         plan.setNumReduceTasks(desc.getNumReducers());
       }
-      else 
+      else
         opTaskMap.put(op, currTask);
 
       if (!readInputUnion) {
         GenMRMapJoinCtx mjCtx = opProcCtx.getMapJoinCtx(currMapJoinOp);
         String taskTmpDir;
-        tableDesc tt_desc; 
+        tableDesc tt_desc;
         Operator<? extends Serializable> rootOp;
 
         if (mjCtx.getOldMapJoin() == null) {
           taskTmpDir = mjCtx.getTaskTmpDir();
-          tt_desc = mjCtx.getTTDesc(); 
+          tt_desc = mjCtx.getTTDesc();
           rootOp = mjCtx.getRootMapJoinOp();
         }
         else {
           GenMRMapJoinCtx oldMjCtx = opProcCtx.getMapJoinCtx(mjCtx.getOldMapJoin());
           taskTmpDir = oldMjCtx.getTaskTmpDir();
-          tt_desc = oldMjCtx.getTTDesc(); 
+          tt_desc = oldMjCtx.getTTDesc();
           rootOp = oldMjCtx.getRootMapJoinOp();
         }
-      
+
         setTaskPlan(taskTmpDir, taskTmpDir, rootOp, plan, local, tt_desc);
       }
       else {
         initUnionPlan(opProcCtx, currTask, false);
       }
-        
+
       opProcCtx.setCurrMapJoinOp(null);
     }
     else {
@@ -177,14 +178,14 @@ public static void initMapJoinPlan(Operator<? extends Serializable> op, GenMRPro
 
       // The map is overloaded to keep track of mapjoins also
       opTaskMap.put(op, currTask);
-      
+
       List<Task<? extends Serializable>> rootTasks = opProcCtx.getRootTasks();
       rootTasks.add(currTask);
-      
+
       assert currTopOp != null;
       List<Operator<? extends Serializable>> seenOps = opProcCtx.getSeenOps();
       String currAliasId = opProcCtx.getCurrAliasId();
-      
+
       seenOps.add(currTopOp);
       boolean local = (pos == desc.getPosBigTable()) ? false : true;
       setTaskPlan(currAliasId, currTopOp, plan, local, opProcCtx);
@@ -197,7 +198,7 @@ public static void initMapJoinPlan(Operator<? extends Serializable> op, GenMRPro
 
   /**
    * Initialize the current union plan.
-   * 
+   *
    * @param op the reduce sink operator encountered
    * @param opProcCtx processing context
    */
@@ -212,13 +213,13 @@ public static void initUnionPlan(ReduceSinkOperator op, GenMRProcContext opProcC
     opTaskMap.put(reducer, currTask);
     plan.setReducer(reducer);
     reduceSinkDesc desc = (reduceSinkDesc)op.getConf();
-    
+
     plan.setNumReduceTasks(desc.getNumReducers());
 
     if (reducer.getClass() == JoinOperator.class)
       plan.setNeedsTagging(true);
 
-    initUnionPlan(opProcCtx, currTask, false); 
+    initUnionPlan(opProcCtx, currTask, false);
   }
 
   /*
@@ -233,15 +234,15 @@ public static void initUnionPlan(GenMRProcContext opProcCtx, Task<? extends Seri
     assert uCtx != null;
 
     List<String>    taskTmpDirLst = uCtx.getTaskTmpDir();
-    List<tableDesc> tt_descLst    = uCtx.getTTDesc(); 
+    List<tableDesc> tt_descLst    = uCtx.getTTDesc();
     assert !taskTmpDirLst.isEmpty() && !tt_descLst.isEmpty();
     assert taskTmpDirLst.size() == tt_descLst.size();
     int size = taskTmpDirLst.size();
     assert local == false;
-    
+
     for (int pos = 0; pos < size; pos++) {
-      String taskTmpDir = taskTmpDirLst.get(pos); 
-      tableDesc tt_desc = tt_descLst.get(pos); 
+      String taskTmpDir = taskTmpDirLst.get(pos);
+      tableDesc tt_desc = tt_descLst.get(pos);
       if (plan.getPathToAliases().get(taskTmpDir) == null) {
         plan.getPathToAliases().put(taskTmpDir, new ArrayList<String>());
         plan.getPathToAliases().get(taskTmpDir).add(taskTmpDir);
@@ -260,27 +261,27 @@ public static void initUnionPlan(GenMRProcContext opProcCtx, Task<? extends Seri
    * @param pos position of the parent in the stack
    */
   public static void joinPlan(Operator<? extends Serializable> op,
-                              Task<? extends Serializable> oldTask, 
-                              Task<? extends Serializable> task, 
-                              GenMRProcContext opProcCtx, 
+                              Task<? extends Serializable> oldTask,
+                              Task<? extends Serializable> task,
+                              GenMRProcContext opProcCtx,
                               int pos, boolean split,
-                              boolean readMapJoinData, 
+                              boolean readMapJoinData,
                               boolean readUnionData) throws SemanticException {
     Task<? extends Serializable> currTask = task;
     mapredWork plan = (mapredWork) currTask.getWork();
     Operator<? extends Serializable> currTopOp = opProcCtx.getCurrTopOp();
     List<Task<? extends Serializable>> parTasks = null;
-      
+
     // terminate the old task and make current task dependent on it
     if (split) {
       assert oldTask != null;
       splitTasks((ReduceSinkOperator)op, oldTask, currTask, opProcCtx, true, false, 0);
-    } 
+    }
     else {
       if ((oldTask != null) && (oldTask.getParentTasks() != null) && !oldTask.getParentTasks().isEmpty()) {
         parTasks = new ArrayList<Task<? extends Serializable>>();
         parTasks.addAll(oldTask.getParentTasks());
-        
+
         Object[] parTaskArr = parTasks.toArray();
         for (int i = 0; i < parTaskArr.length; i++)
           ((Task<? extends Serializable>)parTaskArr[i]).removeDependentTask(oldTask);
@@ -290,7 +291,7 @@ public static void joinPlan(Operator<? extends Serializable> op,
     if (currTopOp != null) {
       List<Operator<? extends Serializable>> seenOps = opProcCtx.getSeenOps();
       String                                 currAliasId = opProcCtx.getCurrAliasId();
-      
+
       if (!seenOps.contains(currTopOp)) {
         seenOps.add(currTopOp);
         boolean local = false;
@@ -308,13 +309,13 @@ else if (opProcCtx.getCurrMapJoinOp() != null) {
       }
       else {
         GenMRMapJoinCtx mjCtx = opProcCtx.getMapJoinCtx(mjOp);
-      
+
         // In case of map-join followed by map-join, the file needs to be obtained from the old map join
         MapJoinOperator oldMapJoin = mjCtx.getOldMapJoin();
         String          taskTmpDir = null;
-        tableDesc       tt_desc    = null; 
+        tableDesc       tt_desc    = null;
         Operator<? extends Serializable> rootOp = null;
-      
+
         if (oldMapJoin == null) {
           taskTmpDir = mjCtx.getTaskTmpDir();
           tt_desc    = mjCtx.getTTDesc();
@@ -327,21 +328,21 @@ else if (opProcCtx.getCurrMapJoinOp() != null) {
           tt_desc    = oldMjCtx.getTTDesc();
           rootOp     = oldMjCtx.getRootMapJoinOp();
         }
-      
+
         boolean local = ((pos == -1) || (pos == ((mapJoinDesc)mjOp.getConf()).getPosBigTable())) ? false : true;
         setTaskPlan(taskTmpDir, taskTmpDir, rootOp, plan, local, tt_desc);
       }
       opProcCtx.setCurrMapJoinOp(null);
-      
+
       if ((oldTask != null) && (parTasks != null)) {
-        for (Task<? extends Serializable> parTask : parTasks) 
+        for (Task<? extends Serializable> parTask : parTasks)
           parTask.addDependentTask(currTask);
       }
-      
+
       if (opProcCtx.getRootTasks().contains(currTask))
         opProcCtx.getRootTasks().remove(currTask);
     }
-    
+
     opProcCtx.setCurrTask(currTask);
   }
 
@@ -350,9 +351,9 @@ else if (opProcCtx.getCurrMapJoinOp() != null) {
    * @param op the reduce sink operator encountered
    * @param opProcCtx processing context
    */
-  public static void splitPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx) 
+  public static void splitPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx)
     throws SemanticException {
-    // Generate a new task              
+    // Generate a new task
     mapredWork cplan = getMapRedWork();
     ParseContext parseCtx = opProcCtx.getParseCtx();
     Task<? extends Serializable> redTask = TaskFactory.get(cplan, parseCtx.getConf());
@@ -361,7 +362,7 @@ public static void splitPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx)
     // Add the reducer
     cplan.setReducer(reducer);
     reduceSinkDesc desc = (reduceSinkDesc)op.getConf();
-    
+
     cplan.setNumReduceTasks(new Integer(desc.getNumReducers()));
 
     HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap = opProcCtx.getOpTaskMap();
@@ -381,7 +382,7 @@ public static void splitPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx)
    * @param opProcCtx processing context
    */
   public static void setTaskPlan(String alias_id, Operator<? extends Serializable> topOp,
-      mapredWork plan, boolean local, GenMRProcContext opProcCtx) 
+      mapredWork plan, boolean local, GenMRProcContext opProcCtx)
     throws SemanticException {
     ParseContext parseCtx = opProcCtx.getParseCtx();
     Set<ReadEntity> inputs = opProcCtx.getInputs();
@@ -393,26 +394,18 @@ public static void setTaskPlan(String alias_id, Operator<? extends Serializable>
     tableDesc  tblDesc = null;
 
     PrunedPartitionList partsList = null;
-    
+
     try {
-      if (!opProcCtx.getConf().getBoolVar(HiveConf.ConfVars.HIVEOPTPPD) ||
-          !opProcCtx.getConf().getBoolVar(HiveConf.ConfVars.HIVEOPTPPR)) {
-        partsList = parseCtx.getAliasToPruner().get(alias_id).prune();
-      }
-      else {
-        partsList = org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.prune(
-                                                    parseCtx.getTopToTable().get(topOp), 
-                                                    parseCtx.getOpToPartPruner().get(topOp),
-                                                    opProcCtx.getConf(),
-                                                    alias_id);
-      }
+      partsList = PartitionPruner.prune(parseCtx.getTopToTable().get(topOp),
+                                        parseCtx.getOpToPartPruner().get(topOp),
+                                        opProcCtx.getConf(), alias_id);
     } catch (SemanticException e) {
       throw e;
     } catch (HiveException e) {
       LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
       throw new SemanticException(e.getMessage(), e);
     }
-    
+
     // Generate the map work for this alias_id
     Set<Partition> parts = null;
     // pass both confirmed and unknown partitions through the map-reduce framework
@@ -429,7 +422,7 @@ public static void setTaskPlan(String alias_id, Operator<? extends Serializable>
     }
     plan.getAliasToPartnInfo().put(alias_id, aliasPartnDesc);
     SamplePruner samplePruner = parseCtx.getAliasToSamplePruner().get(alias_id);
-    
+
     for (Partition part : parts) {
       if (part.getTable().isPartitioned())
         inputs.add(new ReadEntity(part));
@@ -445,7 +438,7 @@ public static void setTaskPlan(String alias_id, Operator<? extends Serializable>
       else {
         paths = part.getPath();
       }
-      
+
       // is it a partitioned table ?
       if (!part.getTable().isPartitioned()) {
         assert ((tblDir == null) && (tblDesc == null));
@@ -457,12 +450,12 @@ public static void setTaskPlan(String alias_id, Operator<? extends Serializable>
       for (Path p: paths) {
         String path = p.toString();
         LOG.debug("Adding " + path + " of table" + alias_id);
-        
+
         partDir.add(p);
         partDesc.add(Utilities.getPartitionDesc(part));
       }
     }
-    
+
     Iterator<Path>          iterPath      = partDir.iterator();
     Iterator<partitionDesc> iterPartnDesc = partDesc.iterator();
 
@@ -514,7 +507,7 @@ public static void setTaskPlan(String alias_id, Operator<? extends Serializable>
    * @param tt_desc  table descriptor
    */
   public static void setTaskPlan(String path, String alias, Operator<? extends Serializable> topOp,
-                                 mapredWork plan, boolean local, tableDesc tt_desc) 
+                                 mapredWork plan, boolean local, tableDesc tt_desc)
     throws SemanticException {
 
     if (!local) {
@@ -531,7 +524,7 @@ public static void setTaskPlan(String path, String alias, Operator<? extends Ser
         localPlan = new mapredLocalWork(
                                         new LinkedHashMap<String, Operator<? extends Serializable>>(),
                                         new LinkedHashMap<String, fetchWork>());
-      
+
       assert localPlan.getAliasToWork().get(alias) == null;
       assert localPlan.getAliasToFetchWork().get(alias) == null;
       localPlan.getAliasToWork().put(alias, topOp);
@@ -548,7 +541,7 @@ public static void setTaskPlan(String path, String alias, Operator<? extends Ser
   public static void setKeyAndValueDesc(mapredWork plan, Operator<? extends Serializable> topOp) {
     if (topOp == null)
       return;
-    
+
     if (topOp instanceof ReduceSinkOperator) {
       ReduceSinkOperator rs = (ReduceSinkOperator)topOp;
       plan.setKeyDesc(rs.getConf().getKeySerializeInfo());
@@ -559,7 +552,7 @@ public static void setKeyAndValueDesc(mapredWork plan, Operator<? extends Serial
       }
       tagToSchema.set(tag, rs.getConf().getValueSerializeInfo());
     } else {
-      List<Operator<? extends Serializable>> children = topOp.getChildOperators(); 
+      List<Operator<? extends Serializable>> children = topOp.getChildOperators();
       if (children != null) {
         for(Operator<? extends Serializable> op: children) {
           setKeyAndValueDesc(plan, op);
@@ -589,7 +582,7 @@ public static mapredWork getMapRedWork() {
    * @param parseCtx parse context
    */
   @SuppressWarnings("nls")
-  private static Operator<? extends Serializable> putOpInsertMap(Operator<? extends Serializable> op, RowResolver rr, ParseContext parseCtx) 
+  private static Operator<? extends Serializable> putOpInsertMap(Operator<? extends Serializable> op, RowResolver rr, ParseContext parseCtx)
   {
     OpParseContext ctx = new OpParseContext(rr);
     parseCtx.getOpParseCtx().put(op, ctx);
@@ -607,13 +600,13 @@ private static Operator<? extends Serializable> putOpInsertMap(Operator<? extend
    * @param pos position of the parent
    **/
   public static void splitTasks(Operator<? extends Serializable> op,
-                                 Task<? extends Serializable> parentTask, 
-                                 Task<? extends Serializable> childTask, 
+                                 Task<? extends Serializable> parentTask,
+                                 Task<? extends Serializable> childTask,
                                  GenMRProcContext opProcCtx, boolean setReducer,
                                  boolean local, int posn) throws SemanticException {
     mapredWork plan = (mapredWork) childTask.getWork();
     Operator<? extends Serializable> currTopOp = opProcCtx.getCurrTopOp();
-    
+
     ParseContext parseCtx = opProcCtx.getParseCtx();
     parentTask.addDependentTask(childTask);
 
@@ -625,11 +618,11 @@ public static void splitTasks(Operator<? extends Serializable> op,
     // generate the temporary file
     Context baseCtx = parseCtx.getContext();
     String taskTmpDir = baseCtx.getMRTmpFileURI();
-    
+
     Operator<? extends Serializable> parent = op.getParentOperators().get(posn);
-    tableDesc tt_desc = 
-      PlanUtils.getIntermediateFileTableDesc(PlanUtils.getFieldSchemasFromRowSchema(parent.getSchema(), "temporarycol")); 
-    
+    tableDesc tt_desc =
+      PlanUtils.getIntermediateFileTableDesc(PlanUtils.getFieldSchemasFromRowSchema(parent.getSchema(), "temporarycol"));
+
     // Create a file sink operator for this file name
     boolean compressIntermediate = parseCtx.getConf().getBoolVar(HiveConf.ConfVars.COMPRESSINTERMEDIATE);
     fileSinkDesc desc = new fileSinkDesc(taskTmpDir, tt_desc, compressIntermediate);
@@ -638,7 +631,7 @@ public static void splitTasks(Operator<? extends Serializable> op,
       desc.setCompressType(parseCtx.getConf().getVar(HiveConf.ConfVars.COMPRESSINTERMEDIATETYPE));
     }
     Operator<? extends Serializable> fs_op = putOpInsertMap(OperatorFactory.get(desc, parent.getSchema()), null, parseCtx);
-    
+
     // replace the reduce child with this operator
     List<Operator<? extends Serializable>> childOpList = parent.getChildOperators();
     for (int pos = 0; pos < childOpList.size(); pos++) {
@@ -647,29 +640,29 @@ public static void splitTasks(Operator<? extends Serializable> op,
         break;
       }
     }
-    
+
     List<Operator<? extends Serializable>> parentOpList = new ArrayList<Operator<? extends Serializable>>();
     parentOpList.add(parent);
     fs_op.setParentOperators(parentOpList);
 
     // create a dummy tableScan operator on top of op
-    Operator<? extends Serializable> ts_op = 
+    Operator<? extends Serializable> ts_op =
       putOpInsertMap(OperatorFactory.get(tableScanDesc.class, parent.getSchema()), null, parseCtx);
-    
+
     childOpList = new ArrayList<Operator<? extends Serializable>>();
     childOpList.add(op);
     ts_op.setChildOperators(childOpList);
     op.getParentOperators().set(posn, ts_op);
-   
+
     Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = opProcCtx.getMapCurrCtx();
     mapCurrCtx.put(ts_op, new GenMapRedCtx(childTask, null, null));
-    
+
     String streamDesc = taskTmpDir;
     mapredWork cplan = (mapredWork) childTask.getWork();
 
     if (setReducer) {
       Operator<? extends Serializable> reducer = op.getChildOperators().get(0);
-    
+
       if (reducer.getClass() == JoinOperator.class) {
         String origStreamDesc;
         streamDesc = "$INTNAME";
@@ -684,7 +677,7 @@ public static void splitTasks(Operator<? extends Serializable> op,
       if (reducer.getClass() == JoinOperator.class)
         cplan.setNeedsTagging(true);
     }
-        
+
     // Add the path to alias mapping
     setTaskPlan(taskTmpDir, streamDesc, ts_op, cplan, local, tt_desc);
 
@@ -703,12 +696,12 @@ public static void splitTasks(Operator<? extends Serializable> op,
       opProcCtx.setMapJoinCtx(mjOp, mjCtx);
       opProcCtx.getMapCurrCtx().put(parent, new GenMapRedCtx(childTask, null, null));
     }
-    
+
     currTopOp = null;
     String currAliasId = null;
-    
+
     opProcCtx.setCurrTopOp(currTopOp);
     opProcCtx.setCurrAliasId(currAliasId);
     opProcCtx.setCurrTask(childTask);
-  }    
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
index f4cad79405..6a2de4bd41 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
@@ -37,7 +37,7 @@ public class Optimizer {
 
 	/**
 	 * create the list of transformations
-	 * @param hiveConf 
+	 * @param hiveConf
 	 */
   public void initialize(HiveConf hiveConf) {
     transformations = new ArrayList<Transform>();
@@ -46,9 +46,7 @@ public void initialize(HiveConf hiveConf) {
     }
     if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD)) {
       transformations.add(new PredicatePushDown());
-      if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPR)) {
-        transformations.add(new PartitionPruner());
-      }
+      transformations.add(new PartitionPruner());
     }
     transformations.add(new UnionProcessor());
     transformations.add(new MapJoinProcessor());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
index e56d55c86a..02c2502df1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
@@ -49,6 +49,8 @@
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.exprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@@ -107,6 +109,11 @@ public static boolean onlyContainsPartnCols(Table tab, exprNodeDesc expr) {
       return tab.isPartitionKey(colName);
     }
 
+    // It cannot contain a non-deterministic function
+    if ((expr instanceof exprNodeGenericFuncDesc) &&
+        !FunctionRegistry.isDeterministic(((exprNodeGenericFuncDesc)expr).getGenericUDF()))
+      return false;
+
     // All columns of the expression must be parttioned columns
     List<exprNodeDesc> children = expr.getChildren();
     if (children != null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTPartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTPartitionPruner.java
deleted file mode 100644
index a5166b04dc..0000000000
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTPartitionPruner.java
+++ /dev/null
@@ -1,427 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.parse;
-
-import java.util.*;
-
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc;
-import org.apache.hadoop.hive.ql.plan.exprNodeConstantDesc;
-import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
-import org.apache.hadoop.hive.ql.plan.exprNodeFieldDesc;
-import org.apache.hadoop.hive.ql.plan.exprNodeGenericFuncDesc;
-import org.apache.hadoop.hive.ql.plan.exprNodeNullDesc;
-import org.apache.hadoop.hive.ql.udf.UDFOPAnd;
-import org.apache.hadoop.hive.ql.udf.UDFOPNot;
-import org.apache.hadoop.hive.ql.udf.UDFOPOr;
-import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.apache.hadoop.hive.ql.udf.UDFType;
-import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-public class ASTPartitionPruner {
-    
-  // The log
-  @SuppressWarnings("nls")
-  private static final Log LOG = LogFactory.getLog("hive.ql.parse.PartitionPruner");
- 
-  private String tableAlias;
-
-  private QBMetaData metaData;
-  
-  private Table tab;
-
-  private exprNodeDesc prunerExpr;
-  
-  private HiveConf conf;
-  
-  // is set to true if the expression only contains partitioning columns and not any other column reference.
-  // This is used to optimize select * from table where ... scenario, when the where condition only references
-  // partitioning columns - the partitions are identified and streamed directly to the client without requiring 
-  // a map-reduce job
-  private boolean onlyContainsPartCols;
-
-  public ASTPartitionPruner() {  
-  }
-  
-  /** Creates a new instance of PartitionPruner */
-  public ASTPartitionPruner(String tableAlias, QBMetaData metaData, HiveConf conf) {
-    this.tableAlias = tableAlias;
-    this.metaData = metaData;
-    this.tab = metaData.getTableForAlias(tableAlias);
-    this.prunerExpr = null;
-    this.conf = conf;
-    onlyContainsPartCols = true;
-  }
-
-  public boolean onlyContainsPartitionCols() {
-    return onlyContainsPartCols;
-  }
-  
-  /** Class to store the return result of genExprNodeDesc.
-   * 
-   *  TODO: In the future when we refactor the PartitionPruner code, we should
-   *  use the same code (GraphWalker) as it is now in TypeCheckProcFactory. 
-   *  We should use NULL to represent a table name node, and the DOT operator
-   *  should descend into the sub tree for 2 levels in order to find out the
-   *  table name.  The benefit is that we get rid of another concept class -
-   *  here it is ExprNodeTempDesc - the return value of a branch in the 
-   *  Expression Syntax Tree, which is different from the value of a branch in
-   *  the Expression Evaluation Tree.  
-   *     
-   */
-  static class ExprNodeTempDesc {
-
-    public ExprNodeTempDesc(exprNodeDesc desc) {
-      isTableName = false;
-      this.desc = desc;
-    }
-    
-    public ExprNodeTempDesc(String tableName) {
-      isTableName = true;
-      this.tableName = tableName;
-    }
-    
-    public boolean getIsTableName() {
-      return isTableName;
-    }
-    
-    public exprNodeDesc getDesc() {
-      return desc;
-    }
-    
-    public String getTableName() {
-      return tableName;
-    }
-    
-    boolean isTableName;
-    exprNodeDesc desc;
-    String tableName;
-    
-    public String toString() {
-      if (isTableName) {
-        return "Table:" + tableName;
-      } else {
-        return "Desc: " + desc;
-      }
-    }
-  };
-  
-  static ExprNodeTempDesc genSimpleExprNodeDesc(ASTNode expr) throws SemanticException {
-    exprNodeDesc desc = null;
-    switch(expr.getType()) {
-      case HiveParser.TOK_NULL:
-        desc = new exprNodeNullDesc();
-        break;
-      case HiveParser.Identifier:
-        // This is the case for an XPATH element (like "c" in "a.b.c.d")
-        desc = new exprNodeConstantDesc(
-            TypeInfoFactory.stringTypeInfo, 
-                SemanticAnalyzer.unescapeIdentifier(expr.getText()));
-        break;
-      case HiveParser.Number:
-        Number v = null;
-        try {
-          v = Double.valueOf(expr.getText());
-          v = Long.valueOf(expr.getText());
-          v = Integer.valueOf(expr.getText());
-        } catch (NumberFormatException e) {
-          // do nothing here, we will throw an exception in the following block
-        }
-        if (v == null) {
-          throw new SemanticException(ErrorMsg.INVALID_NUMERICAL_CONSTANT.getMsg(expr));
-        }
-        desc = new exprNodeConstantDesc(v);
-        break;
-      case HiveParser.StringLiteral:
-        desc = new exprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, BaseSemanticAnalyzer.unescapeSQLString(expr.getText()));
-        break;
-      case HiveParser.TOK_CHARSETLITERAL:
-        desc = new exprNodeConstantDesc(BaseSemanticAnalyzer.charSetString(expr.getChild(0).getText(), expr.getChild(1).getText()));
-        break;
-      case HiveParser.KW_TRUE:
-        desc = new exprNodeConstantDesc(Boolean.TRUE);
-        break;
-      case HiveParser.KW_FALSE:
-        desc = new exprNodeConstantDesc(Boolean.FALSE);
-        break;
-    }
-    return desc == null ? null : new ExprNodeTempDesc(desc);
-  }
-  
-  /**
-   * We use exprNodeConstantDesc(class,null) to represent unknown values.
-   * Except UDFOPAnd, UDFOPOr, and UDFOPNot, all UDFs are assumed to return unknown values 
-   * if any of the arguments are unknown.  
-   *  
-   * @param expr
-   * @return The expression desc, will NEVER be null.
-   * @throws SemanticException
-   */
-  @SuppressWarnings("nls")
-  private ExprNodeTempDesc genExprNodeDesc(ASTNode expr)
-  throws SemanticException {
-    //  We recursively create the exprNodeDesc.  Base cases:  when we encounter 
-    //  a column ref, we convert that into an exprNodeColumnDesc;  when we encounter 
-    //  a constant, we convert that into an exprNodeConstantDesc.  For others we just 
-    //  build the exprNodeFuncDesc with recursively built children.
-
-    //  Is this a simple expr node (not a TOK_COLREF or a TOK_FUNCTION or an operator)?
-    ExprNodeTempDesc tempDesc = genSimpleExprNodeDesc(expr);
-    if (tempDesc != null) {
-      return tempDesc;
-    }
-
-    int tokType = expr.getType();
-    switch (tokType) {
-      case HiveParser.TOK_TABLE_OR_COL: {
-        String tableOrCol = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText());
-        
-        if (metaData.getAliasToTable().get(tableOrCol.toLowerCase()) != null) {
-          // It's a table name
-          tempDesc = new ExprNodeTempDesc(tableOrCol);
-        } else {
-          // It's a column
-          String colName = tableOrCol;
-          String tabAlias = SemanticAnalyzer.getTabAliasForCol(this.metaData, colName, (ASTNode)expr.getChild(0));
-          LOG.debug("getTableColumnDesc(" + tabAlias + ", " + colName);
-          tempDesc = getTableColumnDesc(tabAlias, colName);
-        }
-        break;
-      }
-      
-
-      default: {
-        
-        boolean isFunction = (expr.getType() == HiveParser.TOK_FUNCTION);
-        
-        // Create all children
-        int childrenBegin = (isFunction ? 1 : 0);
-        ArrayList<ExprNodeTempDesc> tempChildren = new ArrayList<ExprNodeTempDesc>(expr.getChildCount() - childrenBegin);
-        for (int ci=childrenBegin; ci<expr.getChildCount(); ci++) {
-          ExprNodeTempDesc child = genExprNodeDesc((ASTNode)expr.getChild(ci));
-          tempChildren.add(child);
-        }
-
-        // Is it a special case: table DOT column?
-        if (expr.getType() == HiveParser.DOT && tempChildren.get(0).getIsTableName()) {
-          String tabAlias = tempChildren.get(0).getTableName();
-          String colName = ((exprNodeConstantDesc) tempChildren.get(1).getDesc()).getValue().toString();
-          tempDesc = getTableColumnDesc(tabAlias, colName);
-          
-        } else {
-        
-          ArrayList<exprNodeDesc> children = new ArrayList<exprNodeDesc>(expr.getChildCount() - childrenBegin);
-          for (int ci=0; ci<tempChildren.size(); ci++) {
-            children.add(tempChildren.get(ci).getDesc());
-          }
-          
-          // Create function desc
-          exprNodeDesc desc = null;
-          try {
-            desc = TypeCheckProcFactory.DefaultExprProcessor.getXpathOrFuncExprNodeDesc(expr, isFunction, children);
-          } catch (UDFArgumentTypeException e) {
-            throw new SemanticException(ErrorMsg.INVALID_ARGUMENT_TYPE
-                .getMsg(expr.getChild(childrenBegin + e.getArgumentId()), e.getMessage()));
-          } catch (UDFArgumentLengthException e) {
-            throw new SemanticException(ErrorMsg.INVALID_ARGUMENT_LENGTH
-                .getMsg(expr, e.getMessage()));
-          } catch (UDFArgumentException e) {
-            throw new SemanticException(ErrorMsg.INVALID_ARGUMENT
-                .getMsg(expr, e.getMessage()));
-          }
-          
-          if (FunctionRegistry.isOpAndOrNot(desc)) {
-            // do nothing because "And" and "Or" and "Not" supports null value evaluation
-            // NOTE: In the future all UDFs that treats null value as UNKNOWN (both in parameters and return 
-            // values) should derive from a common base class UDFNullAsUnknown, so instead of listing the classes
-            // here we would test whether a class is derived from that base class. 
-          } else if (mightBeUnknown(desc) ||
-                     ((desc instanceof exprNodeGenericFuncDesc) && 
-                         !FunctionRegistry.isDeterministic(((exprNodeGenericFuncDesc)desc).getGenericUDF())))
-          {
-            // If its a non-deterministic UDF or if any child is null, set this node to null
-            LOG.trace("Pruner function might be unknown: " + expr.toStringTree());
-            desc = new exprNodeConstantDesc(desc.getTypeInfo(), null);
-          }
-
-          tempDesc = new ExprNodeTempDesc(desc);
-        }
-        break;
-      }
-    }
-    return tempDesc;
-  }
-
-  private ExprNodeTempDesc getTableColumnDesc(String tabAlias, String colName) {
-    ExprNodeTempDesc desc;
-    try {
-      Table t = this.metaData.getTableForAlias(tabAlias);
-      if (t.isPartitionKey(colName)) {
-        // Set value to null if it's not partition column
-        if (tabAlias.equalsIgnoreCase(tableAlias)) {
-          desc = new ExprNodeTempDesc(new exprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, 
-                                                             colName, tabAlias, true)); 
-        } else {                
-          desc = new ExprNodeTempDesc(new exprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, null));
-        }
-      } else {
-        TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromObjectInspector(
-            this.metaData.getTableForAlias(tabAlias).getDeserializer().getObjectInspector());
-        desc = new ExprNodeTempDesc(
-            new exprNodeConstantDesc(((StructTypeInfo)typeInfo).getStructFieldTypeInfo(colName), null));
-        onlyContainsPartCols = false;
-      }
-    } catch (SerDeException e){
-      throw new RuntimeException(e);
-    }
-    return desc;
-  }  
-  
-  public static boolean mightBeUnknown(exprNodeDesc desc) {
-    if (desc instanceof exprNodeConstantDesc) {
-      exprNodeConstantDesc d = (exprNodeConstantDesc)desc;
-      return d.getValue() == null;
-    } else if (desc instanceof exprNodeNullDesc) {
-      return false;
-    } else if (desc instanceof exprNodeFieldDesc) {
-      exprNodeFieldDesc d = (exprNodeFieldDesc)desc;
-      return mightBeUnknown(d.getDesc());
-    } else if (desc instanceof exprNodeGenericFuncDesc) {
-      exprNodeGenericFuncDesc d = (exprNodeGenericFuncDesc)desc;
-      for(int i=0; i<d.getChildren().size(); i++) {
-        if (mightBeUnknown(d.getChildExprs().get(i))) {
-          return true;
-        }
-      }
-      return false;
-    } else if (desc instanceof exprNodeColumnDesc) {
-      return false;
-    }
-    return false;
-  }
-  
-  public boolean hasPartitionPredicate(ASTNode expr) {
-
-    int tokType = expr.getType();
-    boolean hasPPred = false;
-    switch (tokType) {
-      case HiveParser.TOK_TABLE_OR_COL: {
-        // Must be a column. If it's a table then it's already processed by "case DOT" below. 
-        String colName = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText());
-
-        return tab.isPartitionKey(colName);
-      }
-      case HiveParser.DOT: {
-        assert(expr.getChildCount() == 2);
-        ASTNode left = (ASTNode)expr.getChild(0);
-        ASTNode right = (ASTNode)expr.getChild(1);
-        
-        if (left.getType() == HiveParser.TOK_TABLE_OR_COL) {
-          // Is "left" a table?
-          String tableOrCol = BaseSemanticAnalyzer.unescapeIdentifier(left.getChild(0).getText());
-          if (metaData.getAliasToTable().get(tableOrCol.toLowerCase()) != null) {
-            // "left" is a table
-            String colName = BaseSemanticAnalyzer.unescapeIdentifier(right.getText());
-            return tableAlias.equalsIgnoreCase(tableOrCol) && tab.isPartitionKey(colName);
-          }
-        }
-        // else fall through to default
-      }
-      default: {
-        boolean isFunction = (expr.getType() == HiveParser.TOK_FUNCTION);
-        
-        // Create all children
-        int childrenBegin = (isFunction ? 1 : 0);
-        for (int ci=childrenBegin; ci<expr.getChildCount(); ci++) {
-          hasPPred = (hasPPred || hasPartitionPredicate((ASTNode)expr.getChild(ci)));
-        }
-        break;
-      }
-    }
-
-    return hasPPred;
-  }
-
-  /** Add an expression */
-  @SuppressWarnings("nls")
-  public void addExpression(ASTNode expr) throws SemanticException {
-    LOG.debug("adding pruning Tree = " + expr.toStringTree());
-    ExprNodeTempDesc temp = genExprNodeDesc(expr);
-    LOG.debug("new pruning Tree = " + temp);
-    exprNodeDesc desc = temp.getDesc();
-    // Ignore null constant expressions
-    if (!(desc instanceof exprNodeConstantDesc) || ((exprNodeConstantDesc)desc).getValue() != null ) {
-      LOG.trace("adding pruning expr = " + desc);
-      if (this.prunerExpr == null)
-        this.prunerExpr = desc;
-      else
-        this.prunerExpr = TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("OR", this.prunerExpr, desc);
-    }
-  }
-
-  /** 
-   * Add an expression from the JOIN condition. Since these expressions will be used for all the where clauses, they 
-   * are always ANDed. Then we walk through the remaining filters (in the where clause) and OR them with the existing
-   * condition.
-   */
-  @SuppressWarnings("nls")
-  public void addJoinOnExpression(ASTNode expr) throws SemanticException {
-    LOG.trace("adding pruning Tree = " + expr.toStringTree());
-    exprNodeDesc desc = genExprNodeDesc(expr).getDesc();
-    // Ignore null constant expressions
-    if (!(desc instanceof exprNodeConstantDesc) || ((exprNodeConstantDesc)desc).getValue() != null ) {
-      LOG.trace("adding pruning expr = " + desc);
-      if (this.prunerExpr == null)
-        this.prunerExpr = desc;
-      else
-        this.prunerExpr = TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("AND", this.prunerExpr, desc);
-    }
-  }
-
-  /** 
-   * From the table metadata prune the partitions to return the partitions.
-   * Evaluate the parition pruner for each partition and return confirmed and unknown partitions separately
-   */
-  @SuppressWarnings("nls")
-  public PrunedPartitionList prune() throws HiveException {
-    return org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.prune(this.tab,
-        this.prunerExpr, conf, this.tableAlias);
-  }
-
-  public Table getTable() {
-    return this.tab;
-  }
-}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseContext.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseContext.java
index 39f89854f7..1759467a54 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseContext.java
@@ -43,13 +43,12 @@
  * populated. Note that since the parse context contains the operator tree, it
  * can be easily retrieved by the next optimization step or finally for task
  * generation after the plan has been completely optimized.
- * 
+ *
  **/
 
 public class ParseContext {
   private QB qb;
   private ASTNode ast;
-  private HashMap<String, ASTPartitionPruner> aliasToPruner;
   private HashMap<TableScanOperator, exprNodeDesc> opToPartPruner;
   private HashMap<String, SamplePruner> aliasToSamplePruner;
   private HashMap<String, Operator<? extends Serializable>> topOps;
@@ -68,20 +67,18 @@ public class ParseContext {
 
   // is set to true if the expression only contains partitioning columns and not any other column reference.
   // This is used to optimize select * from table where ... scenario, when the where condition only references
-  // partitioning columns - the partitions are identified and streamed directly to the client without requiring 
+  // partitioning columns - the partitions are identified and streamed directly to the client without requiring
   // a map-reduce job
   private boolean hasNonPartCols;
-  
-  public ParseContext() {  
+
+  public ParseContext() {
   }
-  
+
   /**
    * @param qb
    *          current QB
    * @param ast
    *          current parse tree
-   * @param aliasToPruner
-   *          partition pruner list
    * @param opToPartPruner
    *          map from table scan operator to partition pruner
    * @param aliasToSamplePruner
@@ -107,7 +104,6 @@ public ParseContext() {
    *          list of map join operators with no reducer
    */
   public ParseContext(HiveConf conf, QB qb, ASTNode ast,
-      HashMap<String, ASTPartitionPruner> aliasToPruner,
       HashMap<TableScanOperator, exprNodeDesc> opToPartPruner,
       HashMap<String, SamplePruner> aliasToSamplePruner,
       HashMap<String, Operator<? extends Serializable>> topOps,
@@ -121,7 +117,6 @@ public ParseContext(HiveConf conf, QB qb, ASTNode ast,
     this.conf = conf;
     this.qb = qb;
     this.ast = ast;
-    this.aliasToPruner = aliasToPruner;
     this.opToPartPruner = opToPartPruner;
     this.aliasToSamplePruner = aliasToSamplePruner;
     this.joinContext = joinContext;
@@ -199,21 +194,6 @@ public void setParseTree(ASTNode ast) {
     this.ast = ast;
   }
 
-  /**
-   * @return the aliasToPruner
-   */
-  public HashMap<String, ASTPartitionPruner> getAliasToPruner() {
-    return aliasToPruner;
-  }
-
-  /**
-   * @param aliasToPruner
-   *          the aliasToPruner to set
-   */
-  public void setAliasToPruner(HashMap<String, ASTPartitionPruner> aliasToPruner) {
-    this.aliasToPruner = aliasToPruner;
-  }
-
   /**
    * @return the opToPartPruner
    */
@@ -351,7 +331,7 @@ public int getDestTableId() {
   public void setDestTableId(int destTableId) {
     this.destTableId = destTableId;
   }
-  
+
   public UnionProcContext getUCtx() {
     return uCtx;
   }
@@ -388,7 +368,7 @@ public void setListMapJoinOpsNoReducer(
       List<MapJoinOperator> listMapJoinOpsNoReducer) {
     this.listMapJoinOpsNoReducer = listMapJoinOpsNoReducer;
   }
-  
+
   /**
    * Sets the hasNonPartCols flag
    * @param val
@@ -396,7 +376,7 @@ public void setListMapJoinOpsNoReducer(
   public void setHasNonPartCols(boolean val) {
     this.hasNonPartCols = val;
   }
-  
+
   /**
    * Gets the value of the hasNonPartCols flag
    */
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 1a482f1e2d..0b080474a7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -148,7 +148,6 @@
  */
 
 public class SemanticAnalyzer extends BaseSemanticAnalyzer {
-  private HashMap<String, org.apache.hadoop.hive.ql.parse.ASTPartitionPruner> aliasToPruner;
   private HashMap<TableScanOperator, exprNodeDesc> opToPartPruner;
   private HashMap<String, SamplePruner> aliasToSamplePruner;
   private HashMap<String, Operator<? extends Serializable>> topOps;
@@ -182,7 +181,6 @@ public SemanticAnalyzer(HiveConf conf) throws SemanticException {
 
     super(conf);
 
-    this.aliasToPruner = new HashMap<String, org.apache.hadoop.hive.ql.parse.ASTPartitionPruner>();
     this.opToPartPruner = new HashMap<TableScanOperator, exprNodeDesc>();
     this.aliasToSamplePruner = new HashMap<String, SamplePruner>();
     this.topOps = new HashMap<String, Operator<? extends Serializable>>();
@@ -204,7 +202,6 @@ public SemanticAnalyzer(HiveConf conf) throws SemanticException {
   @Override
   protected void reset() {
     super.reset();
-    this.aliasToPruner.clear();
     this.loadTableWork.clear();
     this.loadFileWork.clear();
     this.topOps.clear();
@@ -220,7 +217,6 @@ protected void reset() {
   }
 
   public void init(ParseContext pctx) {
-    aliasToPruner = pctx.getAliasToPruner();
     opToPartPruner = pctx.getOpToPartPruner();
     aliasToSamplePruner = pctx.getAliasToSamplePruner();
     topOps = pctx.getTopOps();
@@ -238,8 +234,9 @@ public void init(ParseContext pctx) {
   }
 
   public ParseContext getParseContext() {
-    return new ParseContext(conf, qb, ast, aliasToPruner, opToPartPruner, aliasToSamplePruner, topOps,
-                            topSelOps, opParseCtx, joinContext, topToTable, loadTableWork, loadFileWork, ctx, idToTableNameMap, destTableId, uCtx,
+    return new ParseContext(conf, qb, ast, opToPartPruner, aliasToSamplePruner, topOps,
+                            topSelOps, opParseCtx, joinContext, topToTable, loadTableWork,
+                            loadFileWork, ctx, idToTableNameMap, destTableId, uCtx,
                             listMapJoinOpsNoReducer);
   }
 
@@ -590,109 +587,6 @@ else if (qbp.getOrderByForClause(ctx_1.dest) != null) {
     }
   }
 
-  private void genPartitionPruners(QBExpr qbexpr) throws SemanticException {
-    if (qbexpr.getOpcode() == QBExpr.Opcode.NULLOP) {
-      genPartitionPruners(qbexpr.getQB());
-    } else {
-      genPartitionPruners(qbexpr.getQBExpr1());
-      genPartitionPruners(qbexpr.getQBExpr2());
-    }
-  }
-
-  /**
-   * Generate partition pruners. The filters can occur in the where clause and in the JOIN conditions. First, walk over the
-   * filters in the join condition and AND them, since all of them are needed. Then for each where clause, traverse the
-   * filter.
-   * Note that, currently we do not propagate filters over subqueries. For eg: if the query is of the type:
-   * select ... FROM t1 JOIN (select ... t2) x where x.partition
-   * we will not recognize that x.partition condition introduces a parition pruner on t2
-   *
-   */
-  @SuppressWarnings("nls")
-  private void genPartitionPruners(QB qb) throws SemanticException {
-    Map<String, Boolean> joinPartnPruner = new HashMap<String, Boolean>();
-    QBParseInfo qbp = qb.getParseInfo();
-
-    // Recursively prune subqueries
-    for (String alias : qb.getSubqAliases()) {
-      QBExpr qbexpr = qb.getSubqForAlias(alias);
-      genPartitionPruners(qbexpr);
-    }
-
-    for (String alias : qb.getTabAliases()) {
-      String alias_id = (qb.getId() == null ? alias : qb.getId() + ":" + alias);
-
-      org.apache.hadoop.hive.ql.parse.ASTPartitionPruner pruner =
-        new org.apache.hadoop.hive.ql.parse.ASTPartitionPruner(alias, qb.getMetaData(), conf);
-
-      // Pass each where clause to the pruner
-      for(String clause: qbp.getClauseNames()) {
-
-        ASTNode whexp = (ASTNode)qbp.getWhrForClause(clause);
-        if (whexp != null) {
-          pruner.addExpression((ASTNode)whexp.getChild(0));
-        }
-      }
-
-      // Add the pruner to the list
-      this.aliasToPruner.put(alias_id, pruner);
-    }
-
-    if (!qb.getTabAliases().isEmpty() && qb.getQbJoinTree() != null) {
-      int pos = 0;
-      for (String alias : qb.getQbJoinTree().getBaseSrc()) {
-        if (alias != null) {
-          String alias_id = (qb.getId() == null ? alias : qb.getId() + ":" + alias);
-          org.apache.hadoop.hive.ql.parse.ASTPartitionPruner pruner =
-            this.aliasToPruner.get(alias_id);
-          if(pruner == null) {
-            // this means that the alias is a subquery
-            pos++;
-            continue;
-          }
-          Vector<ASTNode> filters = qb.getQbJoinTree().getFilters().get(pos);
-          for (ASTNode cond : filters) {
-            pruner.addJoinOnExpression(cond);
-            if (pruner.hasPartitionPredicate(cond))
-              joinPartnPruner.put(alias_id, new Boolean(true));
-          }
-          if (qb.getQbJoinTree().getJoinSrc() != null) {
-            filters = qb.getQbJoinTree().getFilters().get(0);
-            for (ASTNode cond : filters) {
-              pruner.addJoinOnExpression(cond);
-              if (pruner.hasPartitionPredicate(cond))
-                joinPartnPruner.put(alias_id, new Boolean(true));
-            }
-          }
-        }
-        pos++;
-      }
-    }
-
-    // Do old-style partition pruner check only if the new partition pruner
-    // is not enabled.
-    if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEOPTPPD)
-        || !HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEOPTPPR)) {
-      for (String alias : qb.getTabAliases()) {
-        String alias_id = (qb.getId() == null ? alias : qb.getId() + ":" + alias);
-        org.apache.hadoop.hive.ql.parse.ASTPartitionPruner pruner = this.aliasToPruner.get(alias_id);
-        if (joinPartnPruner.get(alias_id) == null) {
-          // Pass each where clause to the pruner
-           for(String clause: qbp.getClauseNames()) {
-
-             ASTNode whexp = (ASTNode)qbp.getWhrForClause(clause);
-             if (pruner.getTable().isPartitioned() &&
-                 conf.getVar(HiveConf.ConfVars.HIVEMAPREDMODE).equalsIgnoreCase("strict") &&
-                 (whexp == null || !pruner.hasPartitionPredicate((ASTNode)whexp.getChild(0)))) {
-               throw new SemanticException(ErrorMsg.NO_PARTITION_PREDICATE.getMsg(whexp != null ? whexp : qbp.getSelForClause(clause),
-                                                                                  " for Alias " + alias + " Table " + pruner.getTable().getName()));
-             }
-           }
-        }
-      }
-    }
-  }
-
   private void genSamplePruners(QBExpr qbexpr) throws SemanticException {
     if (qbexpr.getOpcode() == QBExpr.Opcode.NULLOP) {
       genSamplePruners(qbexpr.getQB());
@@ -4336,68 +4230,6 @@ private void genMapRedTasks(QB qb) throws SemanticException {
 
     QBParseInfo qbParseInfo = qb.getParseInfo();
 
-    // In case of a select, use a fetch task instead of a move task
-    if (qb.getIsQuery()) {
-      if ((!loadTableWork.isEmpty()) || (loadFileWork.size() != 1))
-        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
-      String cols = loadFileWork.get(0).getColumns();
-      String colTypes = loadFileWork.get(0).getColumnTypes();
-
-      fetch = new fetchWork(new Path(loadFileWork.get(0).getSourceDir()).toString(),
-                            new tableDesc(LazySimpleSerDe.class, TextInputFormat.class,
-                                           IgnoreKeyTextOutputFormat.class,
-                                           Utilities.makeProperties(
-                                            org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT, "" + Utilities.ctrlaCode,
-                                            org.apache.hadoop.hive.serde.Constants.LIST_COLUMNS, cols,
-                                            org.apache.hadoop.hive.serde.Constants.LIST_COLUMN_TYPES, colTypes)),
-                            qb.getParseInfo().getOuterQueryLimit());
-
-      fetchTask = TaskFactory.get(fetch, this.conf);
-      setFetchTask(fetchTask);
-    }
-    else {
-      // First we generate the move work as this needs to be made dependent on all
-      // the tasks that have a file sink operation
-      List<moveWork>  mv = new ArrayList<moveWork>();
-      for (loadTableDesc ltd : loadTableWork)
-        mvTask.add(TaskFactory.get(new moveWork(ltd, null, false), this.conf));
-      for (loadFileDesc lfd : loadFileWork)
-        mvTask.add(TaskFactory.get(new moveWork(null, lfd, false), this.conf));
-    }
-
-    // generate map reduce plans
-    GenMRProcContext procCtx =
-      new GenMRProcContext(
-        conf, new HashMap<Operator<? extends Serializable>, Task<? extends Serializable>>(),
-        new ArrayList<Operator<? extends Serializable>>(),
-        getParseContext(), mvTask, this.rootTasks,
-        new LinkedHashMap<Operator<? extends Serializable>, GenMapRedCtx>(),
-        inputs, outputs);
-
-    // create a walker which walks the tree in a DFS manner while maintaining the operator stack.
-    // The dispatcher generates the plan from the operator tree
-    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();
-    opRules.put(new RuleRegExp(new String("R1"), "TS%"), new GenMRTableScan1());
-    opRules.put(new RuleRegExp(new String("R2"), "TS%.*RS%"), new GenMRRedSink1());
-    opRules.put(new RuleRegExp(new String("R3"), "RS%.*RS%"), new GenMRRedSink2());
-    opRules.put(new RuleRegExp(new String("R4"), "FS%"), new GenMRFileSink1());
-    opRules.put(new RuleRegExp(new String("R5"), "UNION%"), new GenMRUnion1());
-    opRules.put(new RuleRegExp(new String("R6"), "UNION%.*RS%"), new GenMRRedSink3());
-    opRules.put(new RuleRegExp(new String("R6"), "MAPJOIN%.*RS%"), new GenMRRedSink4());
-    opRules.put(new RuleRegExp(new String("R7"), "TS%.*MAPJOIN%"), MapJoinFactory.getTableScanMapJoin());
-    opRules.put(new RuleRegExp(new String("R8"), "RS%.*MAPJOIN%"), MapJoinFactory.getReduceSinkMapJoin());
-    opRules.put(new RuleRegExp(new String("R9"), "UNION%.*MAPJOIN%"), MapJoinFactory.getUnionMapJoin());
-    opRules.put(new RuleRegExp(new String("R10"), "MAPJOIN%.*MAPJOIN%"), MapJoinFactory.getMapJoinMapJoin());
-    opRules.put(new RuleRegExp(new String("R11"), "MAPJOIN%SEL%"), MapJoinFactory.getMapJoin());
-
-    // The dispatcher fires the processor corresponding to the closest matching rule and passes the context along
-    Dispatcher disp = new DefaultRuleDispatcher(new GenMROperator(), opRules, procCtx);
-
-    GraphWalker ogw = new GenMapRedWalker(disp);
-    ArrayList<Node> topNodes = new ArrayList<Node>();
-    topNodes.addAll(this.topOps.values());
-    ogw.startWalking(topNodes, null);
-
     // Does this query need reduce job
     if (qb.isSelectStarQuery()
         && qbParseInfo.getDestToClusterBy().isEmpty()
@@ -4412,6 +4244,7 @@ private void genMapRedTasks(QB qb) throws SemanticException {
         if (qbParseInfo.getDestToWhereExpr().isEmpty()) {
           fetch = new fetchWork(tab.getPath().toString(), Utilities.getTableDesc(tab), qb.getParseInfo().getOuterQueryLimit());
           noMapRed = true;
+          inputs.add(new ReadEntity(tab));
         }
       }
       else {
@@ -4424,7 +4257,7 @@ private void genMapRedTasks(QB qb) throws SemanticException {
 
             PrunedPartitionList partsList = null;
             try {
-              partsList = PartitionPruner.prune(topToTable.get(ts), opToPartPruner.get(ts), conf, null);
+              partsList = PartitionPruner.prune(topToTable.get(ts), opToPartPruner.get(ts), conf, (String)topOps.keySet().toArray()[0]);
             } catch (HiveException e) {
               // Has to use full name to make sure it does not conflict with org.apache.commons.lang.StringUtils
               LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
@@ -4442,6 +4275,7 @@ private void genMapRedTasks(QB qb) throws SemanticException {
                 Partition part = iterParts.next();
                 listP.add(part.getPartitionPath().toString());
                 partP.add(Utilities.getPartitionDesc(part));
+                inputs.add(new ReadEntity(part));
               }
 
               fetch = new fetchWork(listP, partP, qb.getParseInfo().getOuterQueryLimit());
@@ -4461,6 +4295,68 @@ private void genMapRedTasks(QB qb) throws SemanticException {
       }
     }
 
+    // In case of a select, use a fetch task instead of a move task
+    if (qb.getIsQuery()) {
+      if ((!loadTableWork.isEmpty()) || (loadFileWork.size() != 1))
+        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
+      String cols = loadFileWork.get(0).getColumns();
+      String colTypes = loadFileWork.get(0).getColumnTypes();
+
+      fetch = new fetchWork(new Path(loadFileWork.get(0).getSourceDir()).toString(),
+                            new tableDesc(LazySimpleSerDe.class, TextInputFormat.class,
+                                           IgnoreKeyTextOutputFormat.class,
+                                           Utilities.makeProperties(
+                                            org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT, "" + Utilities.ctrlaCode,
+                                            org.apache.hadoop.hive.serde.Constants.LIST_COLUMNS, cols,
+                                            org.apache.hadoop.hive.serde.Constants.LIST_COLUMN_TYPES, colTypes)),
+                            qb.getParseInfo().getOuterQueryLimit());
+
+      fetchTask = TaskFactory.get(fetch, this.conf);
+      setFetchTask(fetchTask);
+    }
+    else {
+      // First we generate the move work as this needs to be made dependent on all
+      // the tasks that have a file sink operation
+      List<moveWork>  mv = new ArrayList<moveWork>();
+      for (loadTableDesc ltd : loadTableWork)
+        mvTask.add(TaskFactory.get(new moveWork(ltd, null, false), this.conf));
+      for (loadFileDesc lfd : loadFileWork)
+        mvTask.add(TaskFactory.get(new moveWork(null, lfd, false), this.conf));
+    }
+
+    // generate map reduce plans
+    GenMRProcContext procCtx =
+      new GenMRProcContext(
+        conf, new HashMap<Operator<? extends Serializable>, Task<? extends Serializable>>(),
+        new ArrayList<Operator<? extends Serializable>>(),
+        getParseContext(), mvTask, this.rootTasks,
+        new LinkedHashMap<Operator<? extends Serializable>, GenMapRedCtx>(),
+        inputs, outputs);
+
+    // create a walker which walks the tree in a DFS manner while maintaining the operator stack.
+    // The dispatcher generates the plan from the operator tree
+    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();
+    opRules.put(new RuleRegExp(new String("R1"), "TS%"), new GenMRTableScan1());
+    opRules.put(new RuleRegExp(new String("R2"), "TS%.*RS%"), new GenMRRedSink1());
+    opRules.put(new RuleRegExp(new String("R3"), "RS%.*RS%"), new GenMRRedSink2());
+    opRules.put(new RuleRegExp(new String("R4"), "FS%"), new GenMRFileSink1());
+    opRules.put(new RuleRegExp(new String("R5"), "UNION%"), new GenMRUnion1());
+    opRules.put(new RuleRegExp(new String("R6"), "UNION%.*RS%"), new GenMRRedSink3());
+    opRules.put(new RuleRegExp(new String("R6"), "MAPJOIN%.*RS%"), new GenMRRedSink4());
+    opRules.put(new RuleRegExp(new String("R7"), "TS%.*MAPJOIN%"), MapJoinFactory.getTableScanMapJoin());
+    opRules.put(new RuleRegExp(new String("R8"), "RS%.*MAPJOIN%"), MapJoinFactory.getReduceSinkMapJoin());
+    opRules.put(new RuleRegExp(new String("R9"), "UNION%.*MAPJOIN%"), MapJoinFactory.getUnionMapJoin());
+    opRules.put(new RuleRegExp(new String("R10"), "MAPJOIN%.*MAPJOIN%"), MapJoinFactory.getMapJoinMapJoin());
+    opRules.put(new RuleRegExp(new String("R11"), "MAPJOIN%SEL%"), MapJoinFactory.getMapJoin());
+
+    // The dispatcher fires the processor corresponding to the closest matching rule and passes the context along
+    Dispatcher disp = new DefaultRuleDispatcher(new GenMROperator(), opRules, procCtx);
+
+    GraphWalker ogw = new GenMapRedWalker(disp);
+    ArrayList<Node> topNodes = new ArrayList<Node>();
+    topNodes.addAll(this.topOps.values());
+    ogw.startWalking(topNodes, null);
+
     // reduce sink does not have any kids - since the plan by now has been broken up into multiple
     // tasks, iterate over all tasks.
     // For each task, go over all operators recursively
@@ -4606,7 +4502,7 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
     genPlan(qb);
 
 
-    ParseContext pCtx = new ParseContext(conf, qb, ast, aliasToPruner, opToPartPruner, aliasToSamplePruner, topOps,
+    ParseContext pCtx = new ParseContext(conf, qb, ast, opToPartPruner, aliasToSamplePruner, topOps,
                                          topSelOps, opParseCtx, joinContext, topToTable, loadTableWork, loadFileWork,
                                          ctx, idToTableNameMap, destTableId, uCtx, listMapJoinOpsNoReducer);
 
@@ -4617,10 +4513,6 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
     init(pCtx);
     qb = pCtx.getQB();
 
-    // Do any partition pruning using ASTPartitionPruner
-    genPartitionPruners(qb);
-    LOG.info("Completed partition pruning");
-
     // Do any sample pruning
     genSamplePruners(qb);
     LOG.info("Completed sample pruning");
diff --git a/ql/src/test/queries/clientpositive/input42.q b/ql/src/test/queries/clientpositive/input42.q
index ea48754ff9..dfe5a3094b 100644
--- a/ql/src/test/queries/clientpositive/input42.q
+++ b/ql/src/test/queries/clientpositive/input42.q
@@ -8,3 +8,9 @@ explain extended
 select * from srcpart a where a.ds='2008-04-08' and key < 200;
 
 select * from srcpart a where a.ds='2008-04-08' and key < 200;
+
+
+explain extended
+select * from srcpart a where a.ds='2008-04-08' and rand(100) < 0.1;
+
+select * from srcpart a where a.ds='2008-04-08' and rand(100) < 0.1;
diff --git a/ql/src/test/results/clientpositive/input.q.out b/ql/src/test/results/clientpositive/input.q.out
index 3ed119bcbd..ffcdde2cb9 100644
--- a/ql/src/test/results/clientpositive/input.q.out
+++ b/ql/src/test/results/clientpositive/input.q.out
@@ -4,17 +4,17 @@ ABSTRACT SYNTAX TREE:
   (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x)))))
 
 STAGE DEPENDENCIES:
-  Stage-2 is a root stage
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-2
+  Stage: Stage-0
     Fetch Operator
       limit: -1
 
 
 query: SELECT x.* FROM SRC x
 Input: default/src
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/1897097966/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/405351724/10000
 238	val_238
 86	val_86
 311	val_311
diff --git a/ql/src/test/results/clientpositive/input0.q.out b/ql/src/test/results/clientpositive/input0.q.out
index 20c30c5946..41ee582efd 100644
--- a/ql/src/test/results/clientpositive/input0.q.out
+++ b/ql/src/test/results/clientpositive/input0.q.out
@@ -4,17 +4,17 @@ ABSTRACT SYNTAX TREE:
   (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))))
 
 STAGE DEPENDENCIES:
-  Stage-2 is a root stage
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-2
+  Stage: Stage-0
     Fetch Operator
       limit: -1
 
 
 query: SELECT * FROM src
 Input: default/src
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/687640224/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/724542712/10000
 238	val_238
 86	val_86
 311	val_311
diff --git a/ql/src/test/results/clientpositive/input42.q.out b/ql/src/test/results/clientpositive/input42.q.out
index 857fa4c981..21d4cfddbd 100644
--- a/ql/src/test/results/clientpositive/input42.q.out
+++ b/ql/src/test/results/clientpositive/input42.q.out
@@ -4,10 +4,10 @@ ABSTRACT SYNTAX TREE:
   (TOK_QUERY (TOK_FROM (TOK_TABREF srcpart a)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (. (TOK_TABLE_OR_COL a) ds) '2008-04-08'))))
 
 STAGE DEPENDENCIES:
-  Stage-2 is a root stage
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-2
+  Stage: Stage-0
     Fetch Operator
       limit: -1
 
@@ -15,7 +15,7 @@ STAGE PLANS:
 query: select * from srcpart a where a.ds='2008-04-08'
 Input: default/srcpart/ds=2008-04-08/hr=11
 Input: default/srcpart/ds=2008-04-08/hr=12
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/503307728/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/2090238290/10000
 238	val_238	2008-04-08	11
 86	val_86	2008-04-08	11
 311	val_311	2008-04-08	11
@@ -1056,7 +1056,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 0
-                    directory: file:/data/users/njain/hive4/hive4/build/ql/tmp/82878543/10001
+                    directory: file:/data/users/njain/hive1/hive1/build/ql/tmp/1636549090/10001
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -1066,10 +1066,10 @@ STAGE PLANS:
                           columns.types string:string:string:string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [a]
-        file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 [a]
+        file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [a]
+        file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 [a]
       Path -> Partition:
-        file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
             partition values:
               ds 2008-04-08
@@ -1088,10 +1088,10 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
-        file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
           Partition
             partition values:
               ds 2008-04-08
@@ -1110,7 +1110,7 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive4/hive4/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
 
@@ -1121,7 +1121,7 @@ STAGE PLANS:
 query: select * from srcpart a where a.ds='2008-04-08' and key < 200
 Input: default/srcpart/ds=2008-04-08/hr=11
 Input: default/srcpart/ds=2008-04-08/hr=12
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/1821159430/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/2029733644/10000
 86	val_86	2008-04-08	11
 27	val_27	2008-04-08	11
 165	val_165	2008-04-08	11
@@ -1500,3 +1500,199 @@ Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/1821159430/10000
 90	val_90	2008-04-08	12
 169	val_169	2008-04-08	12
 97	val_97	2008-04-08	12
+query: explain extended
+select * from srcpart a where a.ds='2008-04-08' and rand(100) < 0.1
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF srcpart a)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (= (. (TOK_TABLE_OR_COL a) ds) '2008-04-08') (< (TOK_FUNCTION rand 100) 0.1)))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        a 
+          TableScan
+            alias: a
+            Filter Operator
+              isSamplingPred: false
+              predicate:
+                  expr: ((ds = '2008-04-08') and (rand(100) < 0.1))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                      expr: ds
+                      type: string
+                      expr: hr
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  directory: file:/data/users/njain/hive1/hive1/build/ql/tmp/1619555939/10001
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      properties:
+                        columns _col0,_col1,_col2,_col3
+                        serialization.format 1
+                        columns.types string:string:string:string
+      Needs Tagging: false
+      Path -> Alias:
+        file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [a]
+        file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 [a]
+      Path -> Partition:
+        file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+          Partition
+            partition values:
+              ds 2008-04-08
+              hr 11
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                name srcpart
+                columns.types string:string
+                serialization.ddl struct srcpart { string key, string value}
+                serialization.format 1
+                columns key,value
+                partition_columns ds/hr
+                bucket_count -1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                file.inputformat org.apache.hadoop.mapred.TextInputFormat
+                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                location file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: srcpart
+        file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+          Partition
+            partition values:
+              ds 2008-04-08
+              hr 12
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                name srcpart
+                columns.types string:string
+                serialization.ddl struct srcpart { string key, string value}
+                serialization.format 1
+                columns key,value
+                partition_columns ds/hr
+                bucket_count -1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                file.inputformat org.apache.hadoop.mapred.TextInputFormat
+                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                location file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/srcpart
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: srcpart
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+query: select * from srcpart a where a.ds='2008-04-08' and rand(100) < 0.1
+Input: default/srcpart/ds=2008-04-08/hr=11
+Input: default/srcpart/ds=2008-04-08/hr=12
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/774585255/10000
+145	val_145	2008-04-08	11
+417	val_417	2008-04-08	11
+292	val_292	2008-04-08	11
+446	val_446	2008-04-08	11
+394	val_394	2008-04-08	11
+195	val_195	2008-04-08	11
+455	val_455	2008-04-08	11
+170	val_170	2008-04-08	11
+72	val_72	2008-04-08	11
+239	val_239	2008-04-08	11
+176	val_176	2008-04-08	11
+65	val_65	2008-04-08	11
+181	val_181	2008-04-08	11
+404	val_404	2008-04-08	11
+470	val_470	2008-04-08	11
+85	val_85	2008-04-08	11
+118	val_118	2008-04-08	11
+15	val_15	2008-04-08	11
+369	val_369	2008-04-08	11
+143	val_143	2008-04-08	11
+472	val_472	2008-04-08	11
+489	val_489	2008-04-08	11
+78	val_78	2008-04-08	11
+223	val_223	2008-04-08	11
+453	val_453	2008-04-08	11
+76	val_76	2008-04-08	11
+69	val_69	2008-04-08	11
+368	val_368	2008-04-08	11
+113	val_113	2008-04-08	11
+256	val_256	2008-04-08	11
+70	val_70	2008-04-08	11
+487	val_487	2008-04-08	11
+128	val_128	2008-04-08	11
+466	val_466	2008-04-08	11
+483	val_483	2008-04-08	11
+53	val_53	2008-04-08	11
+406	val_406	2008-04-08	11
+401	val_401	2008-04-08	11
+424	val_424	2008-04-08	11
+164	val_164	2008-04-08	11
+424	val_424	2008-04-08	11
+491	val_491	2008-04-08	11
+237	val_237	2008-04-08	11
+444	val_444	2008-04-08	11
+298	val_298	2008-04-08	11
+341	val_341	2008-04-08	11
+97	val_97	2008-04-08	11
+145	val_145	2008-04-08	12
+417	val_417	2008-04-08	12
+292	val_292	2008-04-08	12
+446	val_446	2008-04-08	12
+394	val_394	2008-04-08	12
+195	val_195	2008-04-08	12
+455	val_455	2008-04-08	12
+170	val_170	2008-04-08	12
+72	val_72	2008-04-08	12
+239	val_239	2008-04-08	12
+176	val_176	2008-04-08	12
+65	val_65	2008-04-08	12
+181	val_181	2008-04-08	12
+404	val_404	2008-04-08	12
+470	val_470	2008-04-08	12
+85	val_85	2008-04-08	12
+118	val_118	2008-04-08	12
+15	val_15	2008-04-08	12
+369	val_369	2008-04-08	12
+143	val_143	2008-04-08	12
+472	val_472	2008-04-08	12
+489	val_489	2008-04-08	12
+78	val_78	2008-04-08	12
+223	val_223	2008-04-08	12
+453	val_453	2008-04-08	12
+76	val_76	2008-04-08	12
+69	val_69	2008-04-08	12
+368	val_368	2008-04-08	12
+113	val_113	2008-04-08	12
+256	val_256	2008-04-08	12
+70	val_70	2008-04-08	12
+487	val_487	2008-04-08	12
+128	val_128	2008-04-08	12
+466	val_466	2008-04-08	12
+483	val_483	2008-04-08	12
+53	val_53	2008-04-08	12
+406	val_406	2008-04-08	12
+401	val_401	2008-04-08	12
+424	val_424	2008-04-08	12
+164	val_164	2008-04-08	12
+424	val_424	2008-04-08	12
+491	val_491	2008-04-08	12
+237	val_237	2008-04-08	12
+444	val_444	2008-04-08	12
+298	val_298	2008-04-08	12
+341	val_341	2008-04-08	12
+97	val_97	2008-04-08	12
diff --git a/ql/src/test/results/clientpositive/input_limit.q.out b/ql/src/test/results/clientpositive/input_limit.q.out
index 62c5171367..b486dc5f8f 100644
--- a/ql/src/test/results/clientpositive/input_limit.q.out
+++ b/ql/src/test/results/clientpositive/input_limit.q.out
@@ -4,17 +4,17 @@ ABSTRACT SYNTAX TREE:
   (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_LIMIT 20)))
 
 STAGE DEPENDENCIES:
-  Stage-2 is a root stage
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-2
+  Stage: Stage-0
     Fetch Operator
       limit: 20
 
 
 query: SELECT x.* FROM SRC x LIMIT 20
 Input: default/src
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/738165830/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/197174171/10000
 238	val_238
 86	val_86
 311	val_311
diff --git a/ql/src/test/results/clientpositive/input_part0.q.out b/ql/src/test/results/clientpositive/input_part0.q.out
index a6c37c05d2..f8b7f75d08 100644
--- a/ql/src/test/results/clientpositive/input_part0.q.out
+++ b/ql/src/test/results/clientpositive/input_part0.q.out
@@ -4,10 +4,10 @@ ABSTRACT SYNTAX TREE:
   (TOK_QUERY (TOK_FROM (TOK_TABREF SRCPART x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL x) ds) '2008-04-08'))))
 
 STAGE DEPENDENCIES:
-  Stage-2 is a root stage
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-2
+  Stage: Stage-0
     Fetch Operator
       limit: -1
 
@@ -15,7 +15,7 @@ STAGE PLANS:
 query: SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08'
 Input: default/srcpart/ds=2008-04-08/hr=11
 Input: default/srcpart/ds=2008-04-08/hr=12
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/1270624914/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/299966053/10000
 238	val_238	2008-04-08	11
 86	val_86	2008-04-08	11
 311	val_311	2008-04-08	11
diff --git a/ql/src/test/results/clientpositive/input_part3.q.out b/ql/src/test/results/clientpositive/input_part3.q.out
index 148ee65a57..537b736fbe 100644
--- a/ql/src/test/results/clientpositive/input_part3.q.out
+++ b/ql/src/test/results/clientpositive/input_part3.q.out
@@ -4,17 +4,17 @@ ABSTRACT SYNTAX TREE:
   (TOK_QUERY (TOK_FROM (TOK_TABREF SRCPART x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_WHERE (and (= (. (TOK_TABLE_OR_COL x) ds) '2008-04-08') (= (. (TOK_TABLE_OR_COL x) hr) 11)))))
 
 STAGE DEPENDENCIES:
-  Stage-2 is a root stage
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-2
+  Stage: Stage-0
     Fetch Operator
       limit: -1
 
 
 query: SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08' and x.hr = 11
 Input: default/srcpart/ds=2008-04-08/hr=11
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/1609442613/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/647682821/10000
 238	val_238	2008-04-08	11
 86	val_86	2008-04-08	11
 311	val_311	2008-04-08	11
diff --git a/ql/src/test/results/clientpositive/input_part4.q.out b/ql/src/test/results/clientpositive/input_part4.q.out
index e93be53fbf..ea1816f4b4 100644
--- a/ql/src/test/results/clientpositive/input_part4.q.out
+++ b/ql/src/test/results/clientpositive/input_part4.q.out
@@ -4,13 +4,13 @@ ABSTRACT SYNTAX TREE:
   (TOK_QUERY (TOK_FROM (TOK_TABREF SRCPART x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_WHERE (and (= (. (TOK_TABLE_OR_COL x) ds) '2008-04-08') (= (. (TOK_TABLE_OR_COL x) hr) 15)))))
 
 STAGE DEPENDENCIES:
-  Stage-2 is a root stage
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-2
+  Stage: Stage-0
     Fetch Operator
       limit: -1
 
 
 query: SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08' and x.hr = 15
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/1460662833/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/1328250162/10000
diff --git a/ql/src/test/results/clientpositive/input_part8.q.out b/ql/src/test/results/clientpositive/input_part8.q.out
index 1485540acd..5cc2037d0f 100644
--- a/ql/src/test/results/clientpositive/input_part8.q.out
+++ b/ql/src/test/results/clientpositive/input_part8.q.out
@@ -4,10 +4,10 @@ ABSTRACT SYNTAX TREE:
   (TOK_QUERY (TOK_FROM (TOK_TABREF SRCPART x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_WHERE (= (TOK_TABLE_OR_COL ds) '2008-04-08')) (TOK_LIMIT 10)))
 
 STAGE DEPENDENCIES:
-  Stage-2 is a root stage
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-2
+  Stage: Stage-0
     Fetch Operator
       limit: 10
 
@@ -15,7 +15,7 @@ STAGE PLANS:
 query: SELECT x.* FROM SRCPART x WHERE ds = '2008-04-08' LIMIT 10
 Input: default/srcpart/ds=2008-04-08/hr=11
 Input: default/srcpart/ds=2008-04-08/hr=12
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/695989191/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/735532208/10000
 238	val_238	2008-04-08	11
 86	val_86	2008-04-08	11
 311	val_311	2008-04-08	11
diff --git a/ql/src/test/results/clientpositive/regex_col.q.out b/ql/src/test/results/clientpositive/regex_col.q.out
index 277b0eeccf..bd605ea7e1 100644
--- a/ql/src/test/results/clientpositive/regex_col.q.out
+++ b/ql/src/test/results/clientpositive/regex_col.q.out
@@ -4,10 +4,10 @@ ABSTRACT SYNTAX TREE:
   (TOK_QUERY (TOK_FROM (TOK_TABREF srcpart)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))))
 
 STAGE DEPENDENCIES:
-  Stage-2 is a root stage
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-2
+  Stage: Stage-0
     Fetch Operator
       limit: -1
 
@@ -263,7 +263,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive4/hive4/build/ql/tmp/1306057238/10002 
+        file:/data/users/njain/hive1/hive1/build/ql/tmp/938805392/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -298,7 +298,7 @@ Input: default/srcpart/ds=2008-04-08/hr=11
 Input: default/srcpart/ds=2008-04-08/hr=12
 Input: default/srcpart/ds=2008-04-09/hr=11
 Input: default/srcpart/ds=2008-04-09/hr=12
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/1294377711/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/404701516/10000
 2008-04-08	11
 2008-04-08	11
 2008-04-08	11
@@ -474,7 +474,7 @@ Input: default/srcpart/ds=2008-04-08/hr=11
 Input: default/srcpart/ds=2008-04-08/hr=12
 Input: default/srcpart/ds=2008-04-09/hr=11
 Input: default/srcpart/ds=2008-04-09/hr=12
-Output: file:/data/users/njain/hive4/hive4/build/ql/tmp/1866969869/10000
+Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/2025278770/10000
 0	val_0
 0	val_0
 0	val_0
