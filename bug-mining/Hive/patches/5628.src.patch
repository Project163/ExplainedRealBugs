diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index da48a7ccbd..bf6583e8e2 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -2832,6 +2832,15 @@ public static enum ConfVars {
         "1. chosen : use VectorUDFAdaptor for a small set of UDFs that were choosen for good performance\n" +
         "2. all    : use VectorUDFAdaptor for all UDFs"
     ),
+    HIVE_VECTORIZATION_COMPLEX_TYPES_ENABLED("hive.vectorized.complex.types.enabled", true,
+        "This flag should be set to true to enable vectorization\n" +
+        "of expressions with complex types.\n" +
+        "The default value is true."),
+    HIVE_VECTORIZATION_GROUPBY_COMPLEX_TYPES_ENABLED("hive.vectorized.groupby.complex.types.enabled", true,
+        "This flag should be set to true to enable group by vectorization\n" +
+        "of aggregations that use complex types.\n",
+        "For example, AVG uses a complex type (STRUCT) for partial aggregation results" +
+        "The default value is true."),
 
     HIVE_TYPE_CHECK_ON_INSERT("hive.typecheck.on.insert", true, "This property has been extended to control "
         + "whether to check, convert, and normalize partition value to conform to its column type in "
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java.orig b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java.orig
new file mode 100644
index 0000000000..da48a7ccbd
--- /dev/null
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java.orig
@@ -0,0 +1,4717 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.conf;
+
+import com.google.common.base.Joiner;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.FileUtils;
+import org.apache.hadoop.hive.common.classification.InterfaceAudience;
+import org.apache.hadoop.hive.common.classification.InterfaceAudience.LimitedPrivate;
+import org.apache.hadoop.hive.conf.Validator.PatternSet;
+import org.apache.hadoop.hive.conf.Validator.RangeValidator;
+import org.apache.hadoop.hive.conf.Validator.RatioValidator;
+import org.apache.hadoop.hive.conf.Validator.SizeValidator;
+import org.apache.hadoop.hive.conf.Validator.StringSet;
+import org.apache.hadoop.hive.conf.Validator.TimeValidator;
+import org.apache.hadoop.hive.conf.Validator.WritableDirectoryValidator;
+import org.apache.hadoop.hive.shims.Utils;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.util.Shell;
+import org.apache.hive.common.HiveCompat;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.security.auth.login.LoginException;
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.PrintStream;
+import java.io.UnsupportedEncodingException;
+import java.net.URI;
+import java.net.URL;
+import java.net.URLDecoder;
+import java.net.URLEncoder;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Properties;
+import java.util.Set;
+import java.util.concurrent.TimeUnit;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Hive Configuration.
+ */
+public class HiveConf extends Configuration {
+  protected String hiveJar;
+  protected Properties origProp;
+  protected String auxJars;
+  private static final Logger l4j = LoggerFactory.getLogger(HiveConf.class);
+  private static boolean loadMetastoreConfig = false;
+  private static boolean loadHiveServer2Config = false;
+  private static URL hiveDefaultURL = null;
+  private static URL hiveSiteURL = null;
+  private static URL hivemetastoreSiteUrl = null;
+  private static URL hiveServer2SiteUrl = null;
+
+  private static byte[] confVarByteArray = null;
+
+
+  private static final Map<String, ConfVars> vars = new HashMap<String, ConfVars>();
+  private static final Map<String, ConfVars> metaConfs = new HashMap<String, ConfVars>();
+  private final List<String> restrictList = new ArrayList<String>();
+  private final Set<String> hiddenSet = new HashSet<String>();
+
+  private Pattern modWhiteListPattern = null;
+  private volatile boolean isSparkConfigUpdated = false;
+  private static final int LOG_PREFIX_LENGTH = 64;
+
+  public boolean getSparkConfigUpdated() {
+    return isSparkConfigUpdated;
+  }
+
+  public void setSparkConfigUpdated(boolean isSparkConfigUpdated) {
+    this.isSparkConfigUpdated = isSparkConfigUpdated;
+  }
+
+  public interface EncoderDecoder<K, V> {
+    V encode(K key);
+    K decode(V value);
+  }
+
+  public static class URLEncoderDecoder implements EncoderDecoder<String, String> {
+    private static final String UTF_8 = "UTF-8";
+    @Override
+    public String encode(String key) {
+      try {
+        return URLEncoder.encode(key, UTF_8);
+      } catch (UnsupportedEncodingException e) {
+        return key;
+      }
+    }
+
+    @Override
+    public String decode(String value) {
+      try {
+        return URLDecoder.decode(value, UTF_8);
+      } catch (UnsupportedEncodingException e) {
+        return value;
+      }
+    }
+  }
+  public static class EncoderDecoderFactory {
+    public static final URLEncoderDecoder URL_ENCODER_DECODER = new URLEncoderDecoder();
+  }
+
+  static {
+    ClassLoader classLoader = Thread.currentThread().getContextClassLoader();
+    if (classLoader == null) {
+      classLoader = HiveConf.class.getClassLoader();
+    }
+
+    hiveDefaultURL = classLoader.getResource("hive-default.xml");
+
+    // Look for hive-site.xml on the CLASSPATH and log its location if found.
+    hiveSiteURL = findConfigFile(classLoader, "hive-site.xml", true);
+    hivemetastoreSiteUrl = findConfigFile(classLoader, "hivemetastore-site.xml", false);
+    hiveServer2SiteUrl = findConfigFile(classLoader, "hiveserver2-site.xml", false);
+
+    for (ConfVars confVar : ConfVars.values()) {
+      vars.put(confVar.varname, confVar);
+    }
+
+    Set<String> llapDaemonConfVarsSetLocal = new LinkedHashSet<>();
+    populateLlapDaemonVarsSet(llapDaemonConfVarsSetLocal);
+    llapDaemonVarsSet = Collections.unmodifiableSet(llapDaemonConfVarsSetLocal);
+  }
+
+  private static URL findConfigFile(ClassLoader classLoader, String name, boolean doLog) {
+    URL result = classLoader.getResource(name);
+    if (result == null) {
+      String confPath = System.getenv("HIVE_CONF_DIR");
+      result = checkConfigFile(new File(confPath, name));
+      if (result == null) {
+        String homePath = System.getenv("HIVE_HOME");
+        String nameInConf = "conf" + File.pathSeparator + name;
+        result = checkConfigFile(new File(homePath, nameInConf));
+        if (result == null) {
+          URI jarUri = null;
+          try {
+            jarUri = HiveConf.class.getProtectionDomain().getCodeSource().getLocation().toURI();
+          } catch (Throwable e) {
+            if (l4j.isInfoEnabled()) {
+              l4j.info("Cannot get jar URI", e);
+            }
+            System.err.println("Cannot get jar URI: " + e.getMessage());
+          }
+          result = checkConfigFile(new File(new File(jarUri).getParentFile(), nameInConf));
+        }
+      }
+    }
+    if (doLog && l4j.isInfoEnabled()) {
+      l4j.info("Found configuration file " + result);
+    }
+    return result;
+  }
+
+  private static URL checkConfigFile(File f) {
+    try {
+      return (f.exists() && f.isFile()) ? f.toURI().toURL() : null;
+    } catch (Throwable e) {
+      if (l4j.isInfoEnabled()) {
+        l4j.info("Error looking for config " + f, e);
+      }
+      System.err.println("Error looking for config " + f + ": " + e.getMessage());
+      return null;
+    }
+  }
+
+
+
+
+  @InterfaceAudience.Private
+  public static final String PREFIX_LLAP = "llap.";
+  @InterfaceAudience.Private
+  public static final String PREFIX_HIVE_LLAP = "hive.llap.";
+
+  /**
+   * Metastore related options that the db is initialized against. When a conf
+   * var in this is list is changed, the metastore instance for the CLI will
+   * be recreated so that the change will take effect.
+   */
+  public static final HiveConf.ConfVars[] metaVars = {
+      HiveConf.ConfVars.METASTOREWAREHOUSE,
+      HiveConf.ConfVars.REPLDIR,
+      HiveConf.ConfVars.METASTOREURIS,
+      HiveConf.ConfVars.METASTORE_SERVER_PORT,
+      HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES,
+      HiveConf.ConfVars.METASTORETHRIFTFAILURERETRIES,
+      HiveConf.ConfVars.METASTORE_CLIENT_CONNECT_RETRY_DELAY,
+      HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT,
+      HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_LIFETIME,
+      HiveConf.ConfVars.METASTOREPWD,
+      HiveConf.ConfVars.METASTORECONNECTURLHOOK,
+      HiveConf.ConfVars.METASTORECONNECTURLKEY,
+      HiveConf.ConfVars.METASTORESERVERMINTHREADS,
+      HiveConf.ConfVars.METASTORESERVERMAXTHREADS,
+      HiveConf.ConfVars.METASTORE_TCP_KEEP_ALIVE,
+      HiveConf.ConfVars.METASTORE_INT_ORIGINAL,
+      HiveConf.ConfVars.METASTORE_INT_ARCHIVED,
+      HiveConf.ConfVars.METASTORE_INT_EXTRACTED,
+      HiveConf.ConfVars.METASTORE_KERBEROS_KEYTAB_FILE,
+      HiveConf.ConfVars.METASTORE_KERBEROS_PRINCIPAL,
+      HiveConf.ConfVars.METASTORE_USE_THRIFT_SASL,
+      HiveConf.ConfVars.METASTORE_TOKEN_SIGNATURE,
+      HiveConf.ConfVars.METASTORE_CACHE_PINOBJTYPES,
+      HiveConf.ConfVars.METASTORE_CONNECTION_POOLING_TYPE,
+      HiveConf.ConfVars.METASTORE_VALIDATE_TABLES,
+      HiveConf.ConfVars.METASTORE_DATANUCLEUS_INIT_COL_INFO,
+      HiveConf.ConfVars.METASTORE_VALIDATE_COLUMNS,
+      HiveConf.ConfVars.METASTORE_VALIDATE_CONSTRAINTS,
+      HiveConf.ConfVars.METASTORE_STORE_MANAGER_TYPE,
+      HiveConf.ConfVars.METASTORE_AUTO_CREATE_ALL,
+      HiveConf.ConfVars.METASTORE_TRANSACTION_ISOLATION,
+      HiveConf.ConfVars.METASTORE_CACHE_LEVEL2,
+      HiveConf.ConfVars.METASTORE_CACHE_LEVEL2_TYPE,
+      HiveConf.ConfVars.METASTORE_IDENTIFIER_FACTORY,
+      HiveConf.ConfVars.METASTORE_PLUGIN_REGISTRY_BUNDLE_CHECK,
+      HiveConf.ConfVars.METASTORE_AUTHORIZATION_STORAGE_AUTH_CHECKS,
+      HiveConf.ConfVars.METASTORE_BATCH_RETRIEVE_MAX,
+      HiveConf.ConfVars.METASTORE_EVENT_LISTENERS,
+      HiveConf.ConfVars.METASTORE_TRANSACTIONAL_EVENT_LISTENERS,
+      HiveConf.ConfVars.METASTORE_EVENT_CLEAN_FREQ,
+      HiveConf.ConfVars.METASTORE_EVENT_EXPIRY_DURATION,
+      HiveConf.ConfVars.METASTORE_EVENT_MESSAGE_FACTORY,
+      HiveConf.ConfVars.METASTORE_FILTER_HOOK,
+      HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL,
+      HiveConf.ConfVars.METASTORE_END_FUNCTION_LISTENERS,
+      HiveConf.ConfVars.METASTORE_PART_INHERIT_TBL_PROPS,
+      HiveConf.ConfVars.METASTORE_BATCH_RETRIEVE_OBJECTS_MAX,
+      HiveConf.ConfVars.METASTORE_INIT_HOOKS,
+      HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS,
+      HiveConf.ConfVars.HMSHANDLERATTEMPTS,
+      HiveConf.ConfVars.HMSHANDLERINTERVAL,
+      HiveConf.ConfVars.HMSHANDLERFORCERELOADCONF,
+      HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN,
+      HiveConf.ConfVars.METASTORE_ORM_RETRIEVE_MAPNULLS_AS_EMPTY_STRINGS,
+      HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES,
+      HiveConf.ConfVars.USERS_IN_ADMIN_ROLE,
+      HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+      HiveConf.ConfVars.HIVE_TXN_MANAGER,
+      HiveConf.ConfVars.HIVE_TXN_TIMEOUT,
+      HiveConf.ConfVars.HIVE_TXN_OPERATIONAL_PROPERTIES,
+      HiveConf.ConfVars.HIVE_TXN_HEARTBEAT_THREADPOOL_SIZE,
+      HiveConf.ConfVars.HIVE_TXN_MAX_OPEN_BATCH,
+      HiveConf.ConfVars.HIVE_TXN_RETRYABLE_SQLEX_REGEX,
+      HiveConf.ConfVars.HIVE_METASTORE_STATS_NDV_TUNER,
+      HiveConf.ConfVars.HIVE_METASTORE_STATS_NDV_DENSITY_FUNCTION,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_ENABLED,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_SIZE,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_MAX_PARTITIONS,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_FPP,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_MAX_VARIANCE,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_TTL,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_MAX_WRITER_WAIT,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_MAX_READER_WAIT,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_MAX_FULL,
+      HiveConf.ConfVars.METASTORE_AGGREGATE_STATS_CACHE_CLEAN_UNTIL,
+      HiveConf.ConfVars.METASTORE_FASTPATH,
+      HiveConf.ConfVars.METASTORE_HBASE_CATALOG_CACHE_SIZE,
+      HiveConf.ConfVars.METASTORE_HBASE_AGGREGATE_STATS_CACHE_SIZE,
+      HiveConf.ConfVars.METASTORE_HBASE_AGGREGATE_STATS_CACHE_MAX_PARTITIONS,
+      HiveConf.ConfVars.METASTORE_HBASE_AGGREGATE_STATS_CACHE_FALSE_POSITIVE_PROBABILITY,
+      HiveConf.ConfVars.METASTORE_HBASE_AGGREGATE_STATS_CACHE_MAX_VARIANCE,
+      HiveConf.ConfVars.METASTORE_HBASE_CACHE_TIME_TO_LIVE,
+      HiveConf.ConfVars.METASTORE_HBASE_CACHE_MAX_WRITER_WAIT,
+      HiveConf.ConfVars.METASTORE_HBASE_CACHE_MAX_READER_WAIT,
+      HiveConf.ConfVars.METASTORE_HBASE_CACHE_MAX_FULL,
+      HiveConf.ConfVars.METASTORE_HBASE_CACHE_CLEAN_UNTIL,
+      HiveConf.ConfVars.METASTORE_HBASE_CONNECTION_CLASS,
+      HiveConf.ConfVars.METASTORE_HBASE_AGGR_STATS_CACHE_ENTRIES,
+      HiveConf.ConfVars.METASTORE_HBASE_AGGR_STATS_MEMORY_TTL,
+      HiveConf.ConfVars.METASTORE_HBASE_AGGR_STATS_INVALIDATOR_FREQUENCY,
+      HiveConf.ConfVars.METASTORE_HBASE_AGGR_STATS_HBASE_TTL,
+      HiveConf.ConfVars.METASTORE_HBASE_FILE_METADATA_THREADS
+      };
+
+  /**
+   * User configurable Metastore vars
+   */
+  public static final HiveConf.ConfVars[] metaConfVars = {
+      HiveConf.ConfVars.METASTORE_TRY_DIRECT_SQL,
+      HiveConf.ConfVars.METASTORE_TRY_DIRECT_SQL_DDL,
+      HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT,
+      HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN,
+      HiveConf.ConfVars.METASTORE_CAPABILITY_CHECK
+  };
+
+  static {
+    for (ConfVars confVar : metaConfVars) {
+      metaConfs.put(confVar.varname, confVar);
+    }
+  }
+
+  public static final String HIVE_LLAP_DAEMON_SERVICE_PRINCIPAL_NAME = "hive.llap.daemon.service.principal";
+  public static final String HIVE_SERVER2_AUTHENTICATION_LDAP_USERMEMBERSHIPKEY_NAME =
+      "hive.server2.authentication.ldap.userMembershipKey";
+
+  /**
+   * dbVars are the parameters can be set per database. If these
+   * parameters are set as a database property, when switching to that
+   * database, the HiveConf variable will be changed. The change of these
+   * parameters will effectively change the DFS and MapReduce clusters
+   * for different databases.
+   */
+  public static final HiveConf.ConfVars[] dbVars = {
+    HiveConf.ConfVars.HADOOPBIN,
+    HiveConf.ConfVars.METASTOREWAREHOUSE,
+    HiveConf.ConfVars.SCRATCHDIR
+  };
+
+  /**
+   * Variables used by LLAP daemons.
+   * TODO: Eventually auto-populate this based on prefixes. The conf variables
+   * will need to be renamed for this.
+   */
+  private static final Set<String> llapDaemonVarsSet;
+
+  private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal) {
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_IO_ENABLED.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_IO_MEMORY_MODE.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_ALLOCATOR_MIN_ALLOC.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_ALLOCATOR_MAX_ALLOC.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_ALLOCATOR_ARENA_COUNT.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_ALLOCATOR_DIRECT.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_USE_LRFU.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_LRFU_LAMBDA.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_CACHE_ALLOW_SYNTHETIC_FILEID.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_IO_USE_FILEID_PATH.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_IO_DECODING_METRICS_PERCENTILE_INTERVALS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_ORC_ENABLE_TIME_COUNTERS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_KERBEROS_PRINCIPAL.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_KERBEROS_KEYTAB_FILE.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_ZKSM_KERBEROS_PRINCIPAL.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_ZKSM_KERBEROS_KEYTAB_FILE.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_ZKSM_ZK_CONNECTION_STRING.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_SECURITY_ACL.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_MANAGEMENT_ACL.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_SECURITY_ACL_DENY.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_MANAGEMENT_ACL_DENY.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DELEGATION_TOKEN_LIFETIME.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_MANAGEMENT_RPC_PORT.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_WEB_AUTO_AUTH.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_RPC_NUM_HANDLERS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_WORK_DIRS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_YARN_SHUFFLE_PORT.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_SHUFFLE_DIR_WATCHER_ENABLED.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_AM_LIVENESS_HEARTBEAT_INTERVAL_MS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_AM_LIVENESS_CONNECTION_TIMEOUT_MS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_AM_LIVENESS_CONNECTION_SLEEP_BETWEEN_RETRIES_MS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_RPC_PORT.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_XMX_HEADROOM.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_VCPUS_PER_INSTANCE.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_NUM_FILE_CLEANER_THREADS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_FILE_CLEANUP_DELAY_SECONDS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_SERVICE_REFRESH_INTERVAL.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_ALLOW_PERMANENT_FNS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_DOWNLOAD_PERMANENT_FNS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_TASK_SCHEDULER_WAIT_QUEUE_SIZE.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_WAIT_QUEUE_COMPARATOR_CLASS_NAME.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_TASK_SCHEDULER_ENABLE_PREEMPTION.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_TASK_PREEMPTION_METRICS_INTERVALS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_WEB_PORT.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_WEB_SSL.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_CONTAINER_ID.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_VALIDATE_ACLS.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_LOGGER.varname);
+    llapDaemonVarsSetLocal.add(ConfVars.LLAP_DAEMON_AM_USE_FQDN.varname);
+  }
+
+  /**
+   * Get a set containing configuration parameter names used by LLAP Server isntances
+   * @return an unmodifiable set containing llap ConfVars
+   */
+  public static final Set<String> getLlapDaemonConfVars() {
+    return llapDaemonVarsSet;
+  }
+
+
+  /**
+   * ConfVars.
+   *
+   * These are the default configuration properties for Hive. Each HiveConf
+   * object is initialized as follows:
+   *
+   * 1) Hadoop configuration properties are applied.
+   * 2) ConfVar properties with non-null values are overlayed.
+   * 3) hive-site.xml properties are overlayed.
+   *
+   * WARNING: think twice before adding any Hadoop configuration properties
+   * with non-null values to this list as they will override any values defined
+   * in the underlying Hadoop configuration.
+   */
+  public static enum ConfVars {
+    // QL execution stuff
+    SCRIPTWRAPPER("hive.exec.script.wrapper", null, ""),
+    PLAN("hive.exec.plan", "", ""),
+    STAGINGDIR("hive.exec.stagingdir", ".hive-staging",
+        "Directory name that will be created inside table locations in order to support HDFS encryption. " +
+        "This is replaces ${hive.exec.scratchdir} for query results with the exception of read-only tables. " +
+        "In all cases ${hive.exec.scratchdir} is still used for other temporary files, such as job plans."),
+    SCRATCHDIR("hive.exec.scratchdir", "/tmp/hive",
+        "HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. " +
+        "For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/<username> is created, " +
+        "with ${hive.scratch.dir.permission}."),
+    REPLDIR("hive.repl.rootdir","/user/hive/repl/",
+        "HDFS root dir for all replication dumps."),
+    REPLCMENABLED("hive.repl.cm.enabled", false,
+        "Turn on ChangeManager, so delete files will go to cmrootdir."),
+    REPLCMDIR("hive.repl.cmrootdir","/user/hive/cmroot/",
+        "Root dir for ChangeManager, used for deleted files."),
+    REPLCMRETIAN("hive.repl.cm.retain","24h",
+        new TimeValidator(TimeUnit.HOURS),
+        "Time to retain removed files in cmrootdir."),
+    REPLCMINTERVAL("hive.repl.cm.interval","3600s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Inteval for cmroot cleanup thread."),
+    REPL_FUNCTIONS_ROOT_DIR("hive.repl.replica.functions.root.dir","/user/hive/repl/functions/",
+        "Root directory on the replica warehouse where the repl sub-system will store jars from the primary warehouse"),
+    LOCALSCRATCHDIR("hive.exec.local.scratchdir",
+        "${system:java.io.tmpdir}" + File.separator + "${system:user.name}",
+        "Local scratch space for Hive jobs"),
+    DOWNLOADED_RESOURCES_DIR("hive.downloaded.resources.dir",
+        "${system:java.io.tmpdir}" + File.separator + "${hive.session.id}_resources",
+        "Temporary local directory for added resources in the remote file system."),
+    SCRATCHDIRPERMISSION("hive.scratch.dir.permission", "700",
+        "The permission for the user specific scratch directories that get created."),
+    SUBMITVIACHILD("hive.exec.submitviachild", false, ""),
+    SUBMITLOCALTASKVIACHILD("hive.exec.submit.local.task.via.child", true,
+        "Determines whether local tasks (typically mapjoin hashtable generation phase) runs in \n" +
+        "separate JVM (true recommended) or not. \n" +
+        "Avoids the overhead of spawning new JVM, but can lead to out-of-memory issues."),
+    SCRIPTERRORLIMIT("hive.exec.script.maxerrsize", 100000,
+        "Maximum number of bytes a script is allowed to emit to standard error (per map-reduce task). \n" +
+        "This prevents runaway scripts from filling logs partitions to capacity"),
+    ALLOWPARTIALCONSUMP("hive.exec.script.allow.partial.consumption", false,
+        "When enabled, this option allows a user script to exit successfully without consuming \n" +
+        "all the data from the standard input."),
+    STREAMREPORTERPERFIX("stream.stderr.reporter.prefix", "reporter:",
+        "Streaming jobs that log to standard error with this prefix can log counter or status information."),
+    STREAMREPORTERENABLED("stream.stderr.reporter.enabled", true,
+        "Enable consumption of status and counter messages for streaming jobs."),
+    COMPRESSRESULT("hive.exec.compress.output", false,
+        "This controls whether the final outputs of a query (to a local/HDFS file or a Hive table) is compressed. \n" +
+        "The compression codec and other options are determined from Hadoop config variables mapred.output.compress*"),
+    COMPRESSINTERMEDIATE("hive.exec.compress.intermediate", false,
+        "This controls whether intermediate files produced by Hive between multiple map-reduce jobs are compressed. \n" +
+        "The compression codec and other options are determined from Hadoop config variables mapred.output.compress*"),
+    COMPRESSINTERMEDIATECODEC("hive.intermediate.compression.codec", "", ""),
+    COMPRESSINTERMEDIATETYPE("hive.intermediate.compression.type", "", ""),
+    BYTESPERREDUCER("hive.exec.reducers.bytes.per.reducer", (long) (256 * 1000 * 1000),
+        "size per reducer.The default is 256Mb, i.e if the input size is 1G, it will use 4 reducers."),
+    MAXREDUCERS("hive.exec.reducers.max", 1009,
+        "max number of reducers will be used. If the one specified in the configuration parameter mapred.reduce.tasks is\n" +
+        "negative, Hive will use this one as the max number of reducers when automatically determine number of reducers."),
+    PREEXECHOOKS("hive.exec.pre.hooks", "",
+        "Comma-separated list of pre-execution hooks to be invoked for each statement. \n" +
+        "A pre-execution hook is specified as the name of a Java class which implements the \n" +
+        "org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext interface."),
+    POSTEXECHOOKS("hive.exec.post.hooks", "",
+        "Comma-separated list of post-execution hooks to be invoked for each statement. \n" +
+        "A post-execution hook is specified as the name of a Java class which implements the \n" +
+        "org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext interface."),
+    ONFAILUREHOOKS("hive.exec.failure.hooks", "",
+        "Comma-separated list of on-failure hooks to be invoked for each statement. \n" +
+        "An on-failure hook is specified as the name of Java class which implements the \n" +
+        "org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext interface."),
+    QUERYREDACTORHOOKS("hive.exec.query.redactor.hooks", "",
+        "Comma-separated list of hooks to be invoked for each query which can \n" +
+        "tranform the query before it's placed in the job.xml file. Must be a Java class which \n" +
+        "extends from the org.apache.hadoop.hive.ql.hooks.Redactor abstract class."),
+    CLIENTSTATSPUBLISHERS("hive.client.stats.publishers", "",
+        "Comma-separated list of statistics publishers to be invoked on counters on each job. \n" +
+        "A client stats publisher is specified as the name of a Java class which implements the \n" +
+        "org.apache.hadoop.hive.ql.stats.ClientStatsPublisher interface."),
+    ATSHOOKQUEUECAPACITY("hive.ats.hook.queue.capacity", 64,
+        "Queue size for the ATS Hook executor. If the number of outstanding submissions \n" +
+        "to the ATS executor exceed this amount, the Hive ATS Hook will not try to log queries to ATS."),
+    EXECPARALLEL("hive.exec.parallel", false, "Whether to execute jobs in parallel"),
+    EXECPARALLETHREADNUMBER("hive.exec.parallel.thread.number", 8,
+        "How many jobs at most can be executed in parallel"),
+    HIVESPECULATIVEEXECREDUCERS("hive.mapred.reduce.tasks.speculative.execution", true,
+        "Whether speculative execution for reducers should be turned on. "),
+    HIVECOUNTERSPULLINTERVAL("hive.exec.counters.pull.interval", 1000L,
+        "The interval with which to poll the JobTracker for the counters the running job. \n" +
+        "The smaller it is the more load there will be on the jobtracker, the higher it is the less granular the caught will be."),
+    DYNAMICPARTITIONING("hive.exec.dynamic.partition", true,
+        "Whether or not to allow dynamic partitions in DML/DDL."),
+    DYNAMICPARTITIONINGMODE("hive.exec.dynamic.partition.mode", "strict",
+        "In strict mode, the user must specify at least one static partition\n" +
+        "in case the user accidentally overwrites all partitions.\n" +
+        "In nonstrict mode all partitions are allowed to be dynamic."),
+    DYNAMICPARTITIONMAXPARTS("hive.exec.max.dynamic.partitions", 1000,
+        "Maximum number of dynamic partitions allowed to be created in total."),
+    DYNAMICPARTITIONMAXPARTSPERNODE("hive.exec.max.dynamic.partitions.pernode", 100,
+        "Maximum number of dynamic partitions allowed to be created in each mapper/reducer node."),
+    MAXCREATEDFILES("hive.exec.max.created.files", 100000L,
+        "Maximum number of HDFS files created by all mappers/reducers in a MapReduce job."),
+    DEFAULTPARTITIONNAME("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__",
+        "The default partition name in case the dynamic partition column value is null/empty string or any other values that cannot be escaped. \n" +
+        "This value must not contain any special character used in HDFS URI (e.g., ':', '%', '/' etc). \n" +
+        "The user has to be aware that the dynamic partition value should not contain this value to avoid confusions."),
+    DEFAULT_ZOOKEEPER_PARTITION_NAME("hive.lockmgr.zookeeper.default.partition.name", "__HIVE_DEFAULT_ZOOKEEPER_PARTITION__", ""),
+
+    // Whether to show a link to the most failed task + debugging tips
+    SHOW_JOB_FAIL_DEBUG_INFO("hive.exec.show.job.failure.debug.info", true,
+        "If a job fails, whether to provide a link in the CLI to the task with the\n" +
+        "most failures, along with debugging hints if applicable."),
+    JOB_DEBUG_CAPTURE_STACKTRACES("hive.exec.job.debug.capture.stacktraces", true,
+        "Whether or not stack traces parsed from the task logs of a sampled failed task \n" +
+        "for each failed job should be stored in the SessionState"),
+    JOB_DEBUG_TIMEOUT("hive.exec.job.debug.timeout", 30000, ""),
+    TASKLOG_DEBUG_TIMEOUT("hive.exec.tasklog.debug.timeout", 20000, ""),
+    OUTPUT_FILE_EXTENSION("hive.output.file.extension", null,
+        "String used as a file extension for output files. \n" +
+        "If not set, defaults to the codec extension for text files (e.g. \".gz\"), or no extension otherwise."),
+
+    HIVE_IN_TEST("hive.in.test", false, "internal usage only, true in test mode", true),
+    HIVE_IN_TEST_SHORT_LOGS("hive.in.test.short.logs", false,
+        "internal usage only, used only in test mode. If set true, when requesting the " +
+        "operation logs the short version (generated by LogDivertAppenderForTest) will be " +
+        "returned"),
+    HIVE_IN_TEST_REMOVE_LOGS("hive.in.test.remove.logs", true,
+        "internal usage only, used only in test mode. If set false, the operation logs, and the " +
+        "operation log directory will not be removed, so they can be found after the test runs."),
+
+    HIVE_IN_TEZ_TEST("hive.in.tez.test", false, "internal use only, true when in testing tez",
+        true),
+
+    LOCALMODEAUTO("hive.exec.mode.local.auto", false,
+        "Let Hive determine whether to run in local mode automatically"),
+    LOCALMODEMAXBYTES("hive.exec.mode.local.auto.inputbytes.max", 134217728L,
+        "When hive.exec.mode.local.auto is true, input bytes should less than this for local mode."),
+    LOCALMODEMAXINPUTFILES("hive.exec.mode.local.auto.input.files.max", 4,
+        "When hive.exec.mode.local.auto is true, the number of tasks should less than this for local mode."),
+
+    DROPIGNORESNONEXISTENT("hive.exec.drop.ignorenonexistent", true,
+        "Do not report an error if DROP TABLE/VIEW/Index/Function specifies a non-existent table/view/index/function"),
+
+    HIVEIGNOREMAPJOINHINT("hive.ignore.mapjoin.hint", true, "Ignore the mapjoin hint"),
+
+    HIVE_FILE_MAX_FOOTER("hive.file.max.footer", 100,
+        "maximum number of lines for footer user can define for a table file"),
+
+    HIVE_RESULTSET_USE_UNIQUE_COLUMN_NAMES("hive.resultset.use.unique.column.names", true,
+        "Make column names unique in the result set by qualifying column names with table alias if needed.\n" +
+        "Table alias will be added to column names for queries of type \"select *\" or \n" +
+        "if query explicitly uses table alias \"select r1.x..\"."),
+
+    // Hadoop Configuration Properties
+    // Properties with null values are ignored and exist only for the purpose of giving us
+    // a symbolic name to reference in the Hive source code. Properties with non-null
+    // values will override any values set in the underlying Hadoop configuration.
+    HADOOPBIN("hadoop.bin.path", findHadoopBinary(), "", true),
+    YARNBIN("yarn.bin.path", findYarnBinary(), "", true),
+    HIVE_FS_HAR_IMPL("fs.har.impl", "org.apache.hadoop.hive.shims.HiveHarFileSystem",
+        "The implementation for accessing Hadoop Archives. Note that this won't be applicable to Hadoop versions less than 0.20"),
+    MAPREDMAXSPLITSIZE(FileInputFormat.SPLIT_MAXSIZE, 256000000L, "", true),
+    MAPREDMINSPLITSIZE(FileInputFormat.SPLIT_MINSIZE, 1L, "", true),
+    MAPREDMINSPLITSIZEPERNODE(CombineFileInputFormat.SPLIT_MINSIZE_PERNODE, 1L, "", true),
+    MAPREDMINSPLITSIZEPERRACK(CombineFileInputFormat.SPLIT_MINSIZE_PERRACK, 1L, "", true),
+    // The number of reduce tasks per job. Hadoop sets this value to 1 by default
+    // By setting this property to -1, Hive will automatically determine the correct
+    // number of reducers.
+    HADOOPNUMREDUCERS("mapreduce.job.reduces", -1, "", true),
+
+    // Metastore stuff. Be sure to update HiveConf.metaVars when you add something here!
+    METASTOREDBTYPE("hive.metastore.db.type", "DERBY", new StringSet("DERBY", "ORACLE", "MYSQL", "MSSQL", "POSTGRES"),
+        "Type of database used by the metastore. Information schema & JDBCStorageHandler depend on it."),
+    METASTOREWAREHOUSE("hive.metastore.warehouse.dir", "/user/hive/warehouse",
+        "location of default database for the warehouse"),
+    METASTOREURIS("hive.metastore.uris", "",
+        "Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore."),
+
+    METASTORE_CAPABILITY_CHECK("hive.metastore.client.capability.check", true,
+        "Whether to check client capabilities for potentially breaking API usage."),
+    METASTORE_FASTPATH("hive.metastore.fastpath", false,
+        "Used to avoid all of the proxies and object copies in the metastore.  Note, if this is " +
+            "set, you MUST use a local metastore (hive.metastore.uris must be empty) otherwise " +
+            "undefined and most likely undesired behavior will result"),
+    METASTORE_FS_HANDLER_THREADS_COUNT("hive.metastore.fshandler.threads", 15,
+        "Number of threads to be allocated for metastore handler for fs operations."),
+    METASTORE_HBASE_CATALOG_CACHE_SIZE("hive.metastore.hbase.catalog.cache.size", 50000, "Maximum number of " +
+        "objects we will place in the hbase metastore catalog cache.  The objects will be divided up by " +
+        "types that we need to cache."),
+    METASTORE_HBASE_AGGREGATE_STATS_CACHE_SIZE("hive.metastore.hbase.aggregate.stats.cache.size", 10000,
+        "Maximum number of aggregate stats nodes that we will place in the hbase metastore aggregate stats cache."),
+    METASTORE_HBASE_AGGREGATE_STATS_CACHE_MAX_PARTITIONS("hive.metastore.hbase.aggregate.stats.max.partitions", 10000,
+        "Maximum number of partitions that are aggregated per cache node."),
+    METASTORE_HBASE_AGGREGATE_STATS_CACHE_FALSE_POSITIVE_PROBABILITY("hive.metastore.hbase.aggregate.stats.false.positive.probability",
+        (float) 0.01, "Maximum false positive probability for the Bloom Filter used in each aggregate stats cache node (default 1%)."),
+    METASTORE_HBASE_AGGREGATE_STATS_CACHE_MAX_VARIANCE("hive.metastore.hbase.aggregate.stats.max.variance", (float) 0.1,
+        "Maximum tolerable variance in number of partitions between a cached node and our request (default 10%)."),
+    METASTORE_HBASE_CACHE_TIME_TO_LIVE("hive.metastore.hbase.cache.ttl", "600s", new TimeValidator(TimeUnit.SECONDS),
+        "Number of seconds for a cached node to be active in the cache before they become stale."),
+    METASTORE_HBASE_CACHE_MAX_WRITER_WAIT("hive.metastore.hbase.cache.max.writer.wait", "5000ms", new TimeValidator(TimeUnit.MILLISECONDS),
+        "Number of milliseconds a writer will wait to acquire the writelock before giving up."),
+    METASTORE_HBASE_CACHE_MAX_READER_WAIT("hive.metastore.hbase.cache.max.reader.wait", "1000ms", new TimeValidator(TimeUnit.MILLISECONDS),
+         "Number of milliseconds a reader will wait to acquire the readlock before giving up."),
+    METASTORE_HBASE_CACHE_MAX_FULL("hive.metastore.hbase.cache.max.full", (float) 0.9,
+         "Maximum cache full % after which the cache cleaner thread kicks in."),
+    METASTORE_HBASE_CACHE_CLEAN_UNTIL("hive.metastore.hbase.cache.clean.until", (float) 0.8,
+          "The cleaner thread cleans until cache reaches this % full size."),
+    METASTORE_HBASE_CONNECTION_CLASS("hive.metastore.hbase.connection.class",
+        "org.apache.hadoop.hive.metastore.hbase.VanillaHBaseConnection",
+        "Class used to connection to HBase"),
+    METASTORE_HBASE_AGGR_STATS_CACHE_ENTRIES("hive.metastore.hbase.aggr.stats.cache.entries",
+        10000, "How many in stats objects to cache in memory"),
+    METASTORE_HBASE_AGGR_STATS_MEMORY_TTL("hive.metastore.hbase.aggr.stats.memory.ttl", "60s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Number of seconds stats objects live in memory after they are read from HBase."),
+    METASTORE_HBASE_AGGR_STATS_INVALIDATOR_FREQUENCY(
+        "hive.metastore.hbase.aggr.stats.invalidator.frequency", "5s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "How often the stats cache scans its HBase entries and looks for expired entries"),
+    METASTORE_HBASE_AGGR_STATS_HBASE_TTL("hive.metastore.hbase.aggr.stats.hbase.ttl", "604800s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Number of seconds stats entries live in HBase cache after they are created.  They may be" +
+            " invalided by updates or partition drops before this.  Default is one week."),
+    METASTORE_HBASE_FILE_METADATA_THREADS("hive.metastore.hbase.file.metadata.threads", 1,
+        "Number of threads to use to read file metadata in background to cache it."),
+
+    METASTORETHRIFTCONNECTIONRETRIES("hive.metastore.connect.retries", 3,
+        "Number of retries while opening a connection to metastore"),
+    METASTORETHRIFTFAILURERETRIES("hive.metastore.failure.retries", 1,
+        "Number of retries upon failure of Thrift metastore calls"),
+    METASTORE_SERVER_PORT("hive.metastore.port", 9083, "Hive metastore listener port"),
+    METASTORE_CLIENT_CONNECT_RETRY_DELAY("hive.metastore.client.connect.retry.delay", "1s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Number of seconds for the client to wait between consecutive connection attempts"),
+    METASTORE_CLIENT_SOCKET_TIMEOUT("hive.metastore.client.socket.timeout", "600s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "MetaStore Client socket timeout in seconds"),
+    METASTORE_CLIENT_SOCKET_LIFETIME("hive.metastore.client.socket.lifetime", "0s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "MetaStore Client socket lifetime in seconds. After this time is exceeded, client\n" +
+        "reconnects on the next MetaStore operation. A value of 0s means the connection\n" +
+        "has an infinite lifetime."),
+    METASTOREPWD("javax.jdo.option.ConnectionPassword", "mine",
+        "password to use against metastore database"),
+    METASTORECONNECTURLHOOK("hive.metastore.ds.connection.url.hook", "",
+        "Name of the hook to use for retrieving the JDO connection URL. If empty, the value in javax.jdo.option.ConnectionURL is used"),
+    METASTOREMULTITHREADED("javax.jdo.option.Multithreaded", true,
+        "Set this to true if multiple threads access metastore through JDO concurrently."),
+    METASTORECONNECTURLKEY("javax.jdo.option.ConnectionURL",
+        "jdbc:derby:;databaseName=metastore_db;create=true",
+        "JDBC connect string for a JDBC metastore.\n" +
+        "To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.\n" +
+        "For example, jdbc:postgresql://myhost/db?ssl=true for postgres database."),
+    METASTORE_DBACCESS_SSL_PROPS("hive.metastore.dbaccess.ssl.properties", "",
+           "Comma-separated SSL properties for metastore to access database when JDO connection URL\n" +
+           "enables SSL access. e.g. javax.net.ssl.trustStore=/tmp/truststore,javax.net.ssl.trustStorePassword=pwd."),
+    HMSHANDLERATTEMPTS("hive.hmshandler.retry.attempts", 10,
+        "The number of times to retry a HMSHandler call if there were a connection error."),
+    HMSHANDLERINTERVAL("hive.hmshandler.retry.interval", "2000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS), "The time between HMSHandler retry attempts on failure."),
+    HMSHANDLERFORCERELOADCONF("hive.hmshandler.force.reload.conf", false,
+        "Whether to force reloading of the HMSHandler configuration (including\n" +
+        "the connection URL, before the next metastore query that accesses the\n" +
+        "datastore. Once reloaded, this value is reset to false. Used for\n" +
+        "testing only."),
+    METASTORESERVERMAXMESSAGESIZE("hive.metastore.server.max.message.size", 100*1024*1024L,
+        "Maximum message size in bytes a HMS will accept."),
+    METASTORESERVERMINTHREADS("hive.metastore.server.min.threads", 200,
+        "Minimum number of worker threads in the Thrift server's pool."),
+    METASTORESERVERMAXTHREADS("hive.metastore.server.max.threads", 1000,
+        "Maximum number of worker threads in the Thrift server's pool."),
+    METASTORE_TCP_KEEP_ALIVE("hive.metastore.server.tcp.keepalive", true,
+        "Whether to enable TCP keepalive for the metastore server. Keepalive will prevent accumulation of half-open connections."),
+
+    METASTORE_INT_ORIGINAL("hive.metastore.archive.intermediate.original",
+        "_INTERMEDIATE_ORIGINAL",
+        "Intermediate dir suffixes used for archiving. Not important what they\n" +
+        "are, as long as collisions are avoided"),
+    METASTORE_INT_ARCHIVED("hive.metastore.archive.intermediate.archived",
+        "_INTERMEDIATE_ARCHIVED", ""),
+    METASTORE_INT_EXTRACTED("hive.metastore.archive.intermediate.extracted",
+        "_INTERMEDIATE_EXTRACTED", ""),
+    METASTORE_KERBEROS_KEYTAB_FILE("hive.metastore.kerberos.keytab.file", "",
+        "The path to the Kerberos Keytab file containing the metastore Thrift server's service principal."),
+    METASTORE_KERBEROS_PRINCIPAL("hive.metastore.kerberos.principal",
+        "hive-metastore/_HOST@EXAMPLE.COM",
+        "The service principal for the metastore Thrift server. \n" +
+        "The special string _HOST will be replaced automatically with the correct host name."),
+    METASTORE_USE_THRIFT_SASL("hive.metastore.sasl.enabled", false,
+        "If true, the metastore Thrift interface will be secured with SASL. Clients must authenticate with Kerberos."),
+    METASTORE_USE_THRIFT_FRAMED_TRANSPORT("hive.metastore.thrift.framed.transport.enabled", false,
+        "If true, the metastore Thrift interface will use TFramedTransport. When false (default) a standard TTransport is used."),
+    METASTORE_USE_THRIFT_COMPACT_PROTOCOL("hive.metastore.thrift.compact.protocol.enabled", false,
+        "If true, the metastore Thrift interface will use TCompactProtocol. When false (default) TBinaryProtocol will be used.\n" +
+        "Setting it to true will break compatibility with older clients running TBinaryProtocol."),
+    METASTORE_TOKEN_SIGNATURE("hive.metastore.token.signature", "",
+        "The delegation token service name to match when selecting a token from the current user's tokens."),
+    METASTORE_CLUSTER_DELEGATION_TOKEN_STORE_CLS("hive.cluster.delegation.token.store.class",
+        "org.apache.hadoop.hive.thrift.MemoryTokenStore",
+        "The delegation token store implementation. Set to org.apache.hadoop.hive.thrift.ZooKeeperTokenStore for load-balanced cluster."),
+    METASTORE_CLUSTER_DELEGATION_TOKEN_STORE_ZK_CONNECTSTR(
+        "hive.cluster.delegation.token.store.zookeeper.connectString", "",
+        "The ZooKeeper token store connect string. You can re-use the configuration value\n" +
+        "set in hive.zookeeper.quorum, by leaving this parameter unset."),
+    METASTORE_CLUSTER_DELEGATION_TOKEN_STORE_ZK_ZNODE(
+        "hive.cluster.delegation.token.store.zookeeper.znode", "/hivedelegation",
+        "The root path for token store data. Note that this is used by both HiveServer2 and\n" +
+        "MetaStore to store delegation Token. One directory gets created for each of them.\n" +
+        "The final directory names would have the servername appended to it (HIVESERVER2,\n" +
+        "METASTORE)."),
+    METASTORE_CLUSTER_DELEGATION_TOKEN_STORE_ZK_ACL(
+        "hive.cluster.delegation.token.store.zookeeper.acl", "",
+        "ACL for token store entries. Comma separated list of ACL entries. For example:\n" +
+        "sasl:hive/host1@MY.DOMAIN:cdrwa,sasl:hive/host2@MY.DOMAIN:cdrwa\n" +
+        "Defaults to all permissions for the hiveserver2/metastore process user."),
+    METASTORE_CACHE_PINOBJTYPES("hive.metastore.cache.pinobjtypes", "Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order",
+        "List of comma separated metastore object types that should be pinned in the cache"),
+    METASTORE_CONNECTION_POOLING_TYPE("datanucleus.connectionPoolingType", "HikariCP", new StringSet("BONECP", "DBCP",
+      "HikariCP", "NONE"),
+        "Specify connection pool library for datanucleus"),
+    METASTORE_CONNECTION_POOLING_MAX_CONNECTIONS("datanucleus.connectionPool.maxPoolSize", 10,
+      "Specify the maximum number of connections in the connection pool. Note: The configured size will be used by\n" +
+        "2 connection pools (TxnHandler and ObjectStore). When configuring the max connection pool size, it is\n" +
+        "recommended to take into account the number of metastore instances and the number of HiveServer2 instances\n" +
+        "configured with embedded metastore. To get optimal performance, set config to meet the following condition\n"+
+        "(2 * pool_size * metastore_instances + 2 * pool_size * HS2_instances_with_embedded_metastore) = \n" +
+        "(2 * physical_core_count + hard_disk_count)."),
+    // Workaround for DN bug on Postgres:
+    // http://www.datanucleus.org/servlet/forum/viewthread_thread,7985_offset
+    METASTORE_DATANUCLEUS_INIT_COL_INFO("datanucleus.rdbms.initializeColumnInfo", "NONE",
+        "initializeColumnInfo setting for DataNucleus; set to NONE at least on Postgres."),
+    METASTORE_VALIDATE_TABLES("datanucleus.schema.validateTables", false,
+        "validates existing schema against code. turn this on if you want to verify existing schema"),
+    METASTORE_VALIDATE_COLUMNS("datanucleus.schema.validateColumns", false,
+        "validates existing schema against code. turn this on if you want to verify existing schema"),
+    METASTORE_VALIDATE_CONSTRAINTS("datanucleus.schema.validateConstraints", false,
+        "validates existing schema against code. turn this on if you want to verify existing schema"),
+    METASTORE_STORE_MANAGER_TYPE("datanucleus.storeManagerType", "rdbms", "metadata store type"),
+    METASTORE_AUTO_CREATE_ALL("datanucleus.schema.autoCreateAll", false,
+        "Auto creates necessary schema on a startup if one doesn't exist. Set this to false, after creating it once."
+        + "To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not "
+        + "recommended for production use cases, run schematool command instead." ),
+    METASTORE_SCHEMA_VERIFICATION("hive.metastore.schema.verification", true,
+        "Enforce metastore schema version consistency.\n" +
+        "True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic\n" +
+        "      schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures\n" +
+        "      proper metastore schema migration. (Default)\n" +
+        "False: Warn if the version information stored in metastore doesn't match with one from in Hive jars."),
+    METASTORE_SCHEMA_VERIFICATION_RECORD_VERSION("hive.metastore.schema.verification.record.version", false,
+      "When true the current MS version is recorded in the VERSION table. If this is disabled and verification is\n" +
+      " enabled the MS will be unusable."),
+    METASTORE_SCHEMA_INFO_CLASS("hive.metastore.schema.info.class",
+        "org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo",
+        "Fully qualified class name for the metastore schema information class \n"
+        + "which is used by schematool to fetch the schema information.\n"
+        + " This class should implement the IMetaStoreSchemaInfo interface"),
+    METASTORE_TRANSACTION_ISOLATION("datanucleus.transactionIsolation", "read-committed",
+        "Default transaction isolation level for identity generation."),
+    METASTORE_CACHE_LEVEL2("datanucleus.cache.level2", false,
+        "Use a level 2 cache. Turn this off if metadata is changed independently of Hive metastore server"),
+    METASTORE_CACHE_LEVEL2_TYPE("datanucleus.cache.level2.type", "none", ""),
+    METASTORE_IDENTIFIER_FACTORY("datanucleus.identifierFactory", "datanucleus1",
+        "Name of the identifier factory to use when generating table/column names etc. \n" +
+        "'datanucleus1' is used for backward compatibility with DataNucleus v1"),
+    METASTORE_USE_LEGACY_VALUE_STRATEGY("datanucleus.rdbms.useLegacyNativeValueStrategy", true, ""),
+    METASTORE_PLUGIN_REGISTRY_BUNDLE_CHECK("datanucleus.plugin.pluginRegistryBundleCheck", "LOG",
+        "Defines what happens when plugin bundles are found and are duplicated [EXCEPTION|LOG|NONE]"),
+    METASTORE_BATCH_RETRIEVE_MAX("hive.metastore.batch.retrieve.max", 300,
+        "Maximum number of objects (tables/partitions) can be retrieved from metastore in one batch. \n" +
+        "The higher the number, the less the number of round trips is needed to the Hive metastore server, \n" +
+        "but it may also cause higher memory requirement at the client side."),
+    METASTORE_BATCH_RETRIEVE_OBJECTS_MAX(
+        "hive.metastore.batch.retrieve.table.partition.max", 1000,
+        "Maximum number of objects that metastore internally retrieves in one batch."),
+
+    METASTORE_INIT_HOOKS("hive.metastore.init.hooks", "",
+        "A comma separated list of hooks to be invoked at the beginning of HMSHandler initialization. \n" +
+        "An init hook is specified as the name of Java class which extends org.apache.hadoop.hive.metastore.MetaStoreInitListener."),
+    METASTORE_PRE_EVENT_LISTENERS("hive.metastore.pre.event.listeners", "",
+        "List of comma separated listeners for metastore events."),
+    METASTORE_EVENT_LISTENERS("hive.metastore.event.listeners", "",
+        "A comma separated list of Java classes that implement the org.apache.hadoop.hive.metastore.MetaStoreEventListener" +
+            " interface. The metastore event and corresponding listener method will be invoked in separate JDO transactions. " +
+            "Alternatively, configure hive.metastore.transactional.event.listeners to ensure both are invoked in same JDO transaction."),
+    METASTORE_TRANSACTIONAL_EVENT_LISTENERS("hive.metastore.transactional.event.listeners", "",
+        "A comma separated list of Java classes that implement the org.apache.hadoop.hive.metastore.MetaStoreEventListener" +
+            " interface. Both the metastore event and corresponding listener method will be invoked in the same JDO transaction."),
+    METASTORE_EVENT_DB_LISTENER_TTL("hive.metastore.event.db.listener.timetolive", "86400s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "time after which events will be removed from the database listener queue"),
+    METASTORE_AUTHORIZATION_STORAGE_AUTH_CHECKS("hive.metastore.authorization.storage.checks", false,
+        "Should the metastore do authorization checks against the underlying storage (usually hdfs) \n" +
+        "for operations like drop-partition (disallow the drop-partition if the user in\n" +
+        "question doesn't have permissions to delete the corresponding directory\n" +
+        "on the storage)."),
+    METASTORE_AUTHORIZATION_EXTERNALTABLE_DROP_CHECK("hive.metastore.authorization.storage.check.externaltable.drop", true,
+        "Should StorageBasedAuthorization check permission of the storage before dropping external table.\n" +
+        "StorageBasedAuthorization already does this check for managed table. For external table however,\n" +
+        "anyone who has read permission of the directory could drop external table, which is surprising.\n" +
+        "The flag is set to false by default to maintain backward compatibility."),
+    METASTORE_EVENT_CLEAN_FREQ("hive.metastore.event.clean.freq", "0s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Frequency at which timer task runs to purge expired events in metastore."),
+    METASTORE_EVENT_EXPIRY_DURATION("hive.metastore.event.expiry.duration", "0s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Duration after which events expire from events table"),
+    METASTORE_EVENT_MESSAGE_FACTORY("hive.metastore.event.message.factory",
+        "org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory",
+        "Factory class for making encoding and decoding messages in the events generated."),
+    METASTORE_EXECUTE_SET_UGI("hive.metastore.execute.setugi", true,
+        "In unsecure mode, setting this property to true will cause the metastore to execute DFS operations using \n" +
+        "the client's reported user and group permissions. Note that this property must be set on \n" +
+        "both the client and server sides. Further note that its best effort. \n" +
+        "If client sets its to true and server sets it to false, client setting will be ignored."),
+    METASTORE_PARTITION_NAME_WHITELIST_PATTERN("hive.metastore.partition.name.whitelist.pattern", "",
+        "Partition names will be checked against this regex pattern and rejected if not matched."),
+
+    METASTORE_INTEGER_JDO_PUSHDOWN("hive.metastore.integral.jdo.pushdown", false,
+        "Allow JDO query pushdown for integral partition columns in metastore. Off by default. This\n" +
+        "improves metastore perf for integral columns, especially if there's a large number of partitions.\n" +
+        "However, it doesn't work correctly with integral values that are not normalized (e.g. have\n" +
+        "leading zeroes, like 0012). If metastore direct SQL is enabled and works, this optimization\n" +
+        "is also irrelevant."),
+    METASTORE_TRY_DIRECT_SQL("hive.metastore.try.direct.sql", true,
+        "Whether the Hive metastore should try to use direct SQL queries instead of the\n" +
+        "DataNucleus for certain read paths. This can improve metastore performance when\n" +
+        "fetching many partitions or column statistics by orders of magnitude; however, it\n" +
+        "is not guaranteed to work on all RDBMS-es and all versions. In case of SQL failures,\n" +
+        "the metastore will fall back to the DataNucleus, so it's safe even if SQL doesn't\n" +
+        "work for all queries on your datastore. If all SQL queries fail (for example, your\n" +
+        "metastore is backed by MongoDB), you might want to disable this to save the\n" +
+        "try-and-fall-back cost."),
+    METASTORE_DIRECT_SQL_PARTITION_BATCH_SIZE("hive.metastore.direct.sql.batch.size", 0,
+        "Batch size for partition and other object retrieval from the underlying DB in direct\n" +
+        "SQL. For some DBs like Oracle and MSSQL, there are hardcoded or perf-based limitations\n" +
+        "that necessitate this. For DBs that can handle the queries, this isn't necessary and\n" +
+        "may impede performance. -1 means no batching, 0 means automatic batching."),
+    METASTORE_TRY_DIRECT_SQL_DDL("hive.metastore.try.direct.sql.ddl", true,
+        "Same as hive.metastore.try.direct.sql, for read statements within a transaction that\n" +
+        "modifies metastore data. Due to non-standard behavior in Postgres, if a direct SQL\n" +
+        "select query has incorrect syntax or something similar inside a transaction, the\n" +
+        "entire transaction will fail and fall-back to DataNucleus will not be possible. You\n" +
+        "should disable the usage of direct SQL inside transactions if that happens in your case."),
+    METASTORE_DIRECT_SQL_MAX_QUERY_LENGTH("hive.direct.sql.max.query.length", 100, "The maximum\n" +
+        " size of a query string (in KB)."),
+    METASTORE_DIRECT_SQL_MAX_ELEMENTS_IN_CLAUSE("hive.direct.sql.max.elements.in.clause", 1000,
+        "The maximum number of values in a IN clause. Once exceeded, it will be broken into\n" +
+        " multiple OR separated IN clauses."),
+    METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE("hive.direct.sql.max.elements.values.clause",
+        1000, "The maximum number of values in a VALUES clause for INSERT statement."),
+    METASTORE_ORM_RETRIEVE_MAPNULLS_AS_EMPTY_STRINGS("hive.metastore.orm.retrieveMapNullsAsEmptyStrings",false,
+        "Thrift does not support nulls in maps, so any nulls present in maps retrieved from ORM must " +
+        "either be pruned or converted to empty strings. Some backing dbs such as Oracle persist empty strings " +
+        "as nulls, so we should set this parameter if we wish to reverse that behaviour. For others, " +
+        "pruning is the correct behaviour"),
+    METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES(
+        "hive.metastore.disallow.incompatible.col.type.changes", true,
+        "If true (default is false), ALTER TABLE operations which change the type of a\n" +
+        "column (say STRING) to an incompatible type (say MAP) are disallowed.\n" +
+        "RCFile default SerDe (ColumnarSerDe) serializes the values in such a way that the\n" +
+        "datatypes can be converted from string to any type. The map is also serialized as\n" +
+        "a string, which can be read as a string as well. However, with any binary\n" +
+        "serialization, this is not true. Blocking the ALTER TABLE prevents ClassCastExceptions\n" +
+        "when subsequently trying to access old partitions.\n" +
+        "\n" +
+        "Primitive types like INT, STRING, BIGINT, etc., are compatible with each other and are\n" +
+        "not blocked.\n" +
+        "\n" +
+        "See HIVE-4409 for more details."),
+    METASTORE_LIMIT_PARTITION_REQUEST("hive.metastore.limit.partition.request", -1,
+        "This limits the number of partitions that can be requested from the metastore for a given table.\n" +
+            "The default value \"-1\" means no limit."),
+
+    NEWTABLEDEFAULTPARA("hive.table.parameters.default", "",
+        "Default property values for newly created tables"),
+    DDL_CTL_PARAMETERS_WHITELIST("hive.ddl.createtablelike.properties.whitelist", "",
+        "Table Properties to copy over when executing a Create Table Like."),
+    METASTORE_RAW_STORE_IMPL("hive.metastore.rawstore.impl", "org.apache.hadoop.hive.metastore.ObjectStore",
+        "Name of the class that implements org.apache.hadoop.hive.metastore.rawstore interface. \n" +
+        "This class is used to store and retrieval of raw metadata objects such as table, database"),
+    METASTORE_CACHED_RAW_STORE_IMPL("hive.metastore.cached.rawstore.impl", "org.apache.hadoop.hive.metastore.ObjectStore",
+        "Name of the wrapped RawStore class"),
+    METASTORE_CACHED_RAW_STORE_CACHE_UPDATE_FREQUENCY(
+        "hive.metastore.cached.rawstore.cache.update.frequency", "60", new TimeValidator(
+            TimeUnit.SECONDS),
+        "The time after which metastore cache is updated from metastore DB."),
+    METASTORE_TXN_STORE_IMPL("hive.metastore.txn.store.impl",
+        "org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler",
+        "Name of class that implements org.apache.hadoop.hive.metastore.txn.TxnStore.  This " +
+        "class is used to store and retrieve transactions and locks"),
+    METASTORE_CONNECTION_DRIVER("javax.jdo.option.ConnectionDriverName", "org.apache.derby.jdbc.EmbeddedDriver",
+        "Driver class name for a JDBC metastore"),
+    METASTORE_MANAGER_FACTORY_CLASS("javax.jdo.PersistenceManagerFactoryClass",
+        "org.datanucleus.api.jdo.JDOPersistenceManagerFactory",
+        "class implementing the jdo persistence"),
+    METASTORE_EXPRESSION_PROXY_CLASS("hive.metastore.expression.proxy",
+        "org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore", ""),
+    METASTORE_DETACH_ALL_ON_COMMIT("javax.jdo.option.DetachAllOnCommit", true,
+        "Detaches all objects from session so that they can be used after transaction is committed"),
+    METASTORE_NON_TRANSACTIONAL_READ("javax.jdo.option.NonTransactionalRead", true,
+        "Reads outside of transactions"),
+    METASTORE_CONNECTION_USER_NAME("javax.jdo.option.ConnectionUserName", "APP",
+        "Username to use against metastore database"),
+    METASTORE_END_FUNCTION_LISTENERS("hive.metastore.end.function.listeners", "",
+        "List of comma separated listeners for the end of metastore functions."),
+    METASTORE_PART_INHERIT_TBL_PROPS("hive.metastore.partition.inherit.table.properties", "",
+        "List of comma separated keys occurring in table properties which will get inherited to newly created partitions. \n" +
+        "* implies all the keys will get inherited."),
+    METASTORE_FILTER_HOOK("hive.metastore.filter.hook", "org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl",
+        "Metastore hook class for filtering the metadata read results. If hive.security.authorization.manager"
+        + "is set to instance of HiveAuthorizerFactory, then this value is ignored."),
+    FIRE_EVENTS_FOR_DML("hive.metastore.dml.events", false, "If true, the metastore will be asked" +
+        " to fire events for DML operations"),
+    METASTORE_CLIENT_DROP_PARTITIONS_WITH_EXPRESSIONS("hive.metastore.client.drop.partitions.using.expressions", true,
+        "Choose whether dropping partitions with HCatClient pushes the partition-predicate to the metastore, " +
+            "or drops partitions iteratively"),
+
+    METASTORE_AGGREGATE_STATS_CACHE_ENABLED("hive.metastore.aggregate.stats.cache.enabled", true,
+        "Whether aggregate stats caching is enabled or not."),
+    METASTORE_AGGREGATE_STATS_CACHE_SIZE("hive.metastore.aggregate.stats.cache.size", 10000,
+        "Maximum number of aggregate stats nodes that we will place in the metastore aggregate stats cache."),
+    METASTORE_AGGREGATE_STATS_CACHE_MAX_PARTITIONS("hive.metastore.aggregate.stats.cache.max.partitions", 10000,
+        "Maximum number of partitions that are aggregated per cache node."),
+    METASTORE_AGGREGATE_STATS_CACHE_FPP("hive.metastore.aggregate.stats.cache.fpp", (float) 0.01,
+        "Maximum false positive probability for the Bloom Filter used in each aggregate stats cache node (default 1%)."),
+    METASTORE_AGGREGATE_STATS_CACHE_MAX_VARIANCE("hive.metastore.aggregate.stats.cache.max.variance", (float) 0.01,
+        "Maximum tolerable variance in number of partitions between a cached node and our request (default 1%)."),
+    METASTORE_AGGREGATE_STATS_CACHE_TTL("hive.metastore.aggregate.stats.cache.ttl", "600s", new TimeValidator(TimeUnit.SECONDS),
+        "Number of seconds for a cached node to be active in the cache before they become stale."),
+    METASTORE_AGGREGATE_STATS_CACHE_MAX_WRITER_WAIT("hive.metastore.aggregate.stats.cache.max.writer.wait", "5000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "Number of milliseconds a writer will wait to acquire the writelock before giving up."),
+    METASTORE_AGGREGATE_STATS_CACHE_MAX_READER_WAIT("hive.metastore.aggregate.stats.cache.max.reader.wait", "1000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "Number of milliseconds a reader will wait to acquire the readlock before giving up."),
+    METASTORE_AGGREGATE_STATS_CACHE_MAX_FULL("hive.metastore.aggregate.stats.cache.max.full", (float) 0.9,
+        "Maximum cache full % after which the cache cleaner thread kicks in."),
+    METASTORE_AGGREGATE_STATS_CACHE_CLEAN_UNTIL("hive.metastore.aggregate.stats.cache.clean.until", (float) 0.8,
+        "The cleaner thread cleans until cache reaches this % full size."),
+    METASTORE_METRICS("hive.metastore.metrics.enabled", false, "Enable metrics on the metastore."),
+    METASTORE_INIT_METADATA_COUNT_ENABLED("hive.metastore.initial.metadata.count.enabled", true,
+      "Enable a metadata count at metastore startup for metrics."),
+
+    // Metastore SSL settings
+    HIVE_METASTORE_USE_SSL("hive.metastore.use.SSL", false,
+        "Set this to true for using SSL encryption in HMS server."),
+    HIVE_METASTORE_SSL_KEYSTORE_PATH("hive.metastore.keystore.path", "",
+        "Metastore SSL certificate keystore location."),
+    HIVE_METASTORE_SSL_KEYSTORE_PASSWORD("hive.metastore.keystore.password", "",
+        "Metastore SSL certificate keystore password."),
+    HIVE_METASTORE_SSL_TRUSTSTORE_PATH("hive.metastore.truststore.path", "",
+        "Metastore SSL certificate truststore location."),
+    HIVE_METASTORE_SSL_TRUSTSTORE_PASSWORD("hive.metastore.truststore.password", "",
+        "Metastore SSL certificate truststore password."),
+
+    // Parameters for exporting metadata on table drop (requires the use of the)
+    // org.apache.hadoop.hive.ql.parse.MetaDataExportListener preevent listener
+    METADATA_EXPORT_LOCATION("hive.metadata.export.location", "",
+        "When used in conjunction with the org.apache.hadoop.hive.ql.parse.MetaDataExportListener pre event listener, \n" +
+        "it is the location to which the metadata will be exported. The default is an empty string, which results in the \n" +
+        "metadata being exported to the current user's home directory on HDFS."),
+    MOVE_EXPORTED_METADATA_TO_TRASH("hive.metadata.move.exported.metadata.to.trash", true,
+        "When used in conjunction with the org.apache.hadoop.hive.ql.parse.MetaDataExportListener pre event listener, \n" +
+        "this setting determines if the metadata that is exported will subsequently be moved to the user's trash directory \n" +
+        "alongside the dropped table data. This ensures that the metadata will be cleaned up along with the dropped table data."),
+
+    // CLI
+    CLIIGNOREERRORS("hive.cli.errors.ignore", false, ""),
+    CLIPRINTCURRENTDB("hive.cli.print.current.db", false,
+        "Whether to include the current database in the Hive prompt."),
+    CLIPROMPT("hive.cli.prompt", "hive",
+        "Command line prompt configuration value. Other hiveconf can be used in this configuration value. \n" +
+        "Variable substitution will only be invoked at the Hive CLI startup."),
+    CLIPRETTYOUTPUTNUMCOLS("hive.cli.pretty.output.num.cols", -1,
+        "The number of columns to use when formatting output generated by the DESCRIBE PRETTY table_name command.\n" +
+        "If the value of this property is -1, then Hive will use the auto-detected terminal width."),
+
+    HIVE_METASTORE_FS_HANDLER_CLS("hive.metastore.fs.handler.class", "org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl", ""),
+
+    // Things we log in the jobconf
+
+    // session identifier
+    HIVESESSIONID("hive.session.id", "", ""),
+    // whether session is running in silent mode or not
+    HIVESESSIONSILENT("hive.session.silent", false, ""),
+
+    HIVE_SESSION_HISTORY_ENABLED("hive.session.history.enabled", false,
+        "Whether to log Hive query, query plan, runtime statistics etc."),
+
+    HIVEQUERYSTRING("hive.query.string", "",
+        "Query being executed (might be multiple per a session)"),
+
+    HIVEQUERYID("hive.query.id", "",
+        "ID for query being executed (might be multiple per a session)"),
+
+    HIVEJOBNAMELENGTH("hive.jobname.length", 50, "max jobname length"),
+
+    // hive jar
+    HIVEJAR("hive.jar.path", "",
+        "The location of hive_cli.jar that is used when submitting jobs in a separate jvm."),
+    HIVEAUXJARS("hive.aux.jars.path", "",
+        "The location of the plugin jars that contain implementations of user defined functions and serdes."),
+
+    // reloadable jars
+    HIVERELOADABLEJARS("hive.reloadable.aux.jars.path", "",
+        "The locations of the plugin jars, which can be a comma-separated folders or jars. Jars can be renewed\n"
+        + "by executing reload command. And these jars can be "
+            + "used as the auxiliary classes like creating a UDF or SerDe."),
+
+    // hive added files and jars
+    HIVEADDEDFILES("hive.added.files.path", "", "This an internal parameter."),
+    HIVEADDEDJARS("hive.added.jars.path", "", "This an internal parameter."),
+    HIVEADDEDARCHIVES("hive.added.archives.path", "", "This an internal parameter."),
+
+    HIVE_CURRENT_DATABASE("hive.current.database", "", "Database name used by current session. Internal usage only.", true),
+
+    // for hive script operator
+    HIVES_AUTO_PROGRESS_TIMEOUT("hive.auto.progress.timeout", "0s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "How long to run autoprogressor for the script/UDTF operators.\n" +
+        "Set to 0 for forever."),
+    HIVESCRIPTAUTOPROGRESS("hive.script.auto.progress", false,
+        "Whether Hive Transform/Map/Reduce Clause should automatically send progress information to TaskTracker \n" +
+        "to avoid the task getting killed because of inactivity.  Hive sends progress information when the script is \n" +
+        "outputting to stderr.  This option removes the need of periodically producing stderr messages, \n" +
+        "but users should be cautious because this may prevent infinite loops in the scripts to be killed by TaskTracker."),
+    HIVESCRIPTIDENVVAR("hive.script.operator.id.env.var", "HIVE_SCRIPT_OPERATOR_ID",
+        "Name of the environment variable that holds the unique script operator ID in the user's \n" +
+        "transform function (the custom mapper/reducer that the user has specified in the query)"),
+    HIVESCRIPTTRUNCATEENV("hive.script.operator.truncate.env", false,
+        "Truncate each environment variable for external script in scripts operator to 20KB (to fit system limits)"),
+    HIVESCRIPT_ENV_BLACKLIST("hive.script.operator.env.blacklist",
+        "hive.txn.valid.txns,hive.script.operator.env.blacklist",
+        "Comma separated list of keys from the configuration file not to convert to environment " +
+        "variables when envoking the script operator"),
+    HIVE_STRICT_CHECKS_LARGE_QUERY("hive.strict.checks.large.query", false,
+        "Enabling strict large query checks disallows the following:\n" +
+        "  Orderby without limit.\n" +
+        "  No partition being picked up for a query against partitioned table.\n" +
+        "Note that these checks currently do not consider data size, only the query pattern."),
+    HIVE_STRICT_CHECKS_TYPE_SAFETY("hive.strict.checks.type.safety", true,
+        "Enabling strict type safety checks disallows the following:\n" +
+        "  Comparing bigints and strings.\n" +
+        "  Comparing bigints and doubles."),
+    HIVE_STRICT_CHECKS_CARTESIAN("hive.strict.checks.cartesian.product", true,
+        "Enabling strict Cartesian join checks disallows the following:\n" +
+        "  Cartesian product (cross join)."),
+    HIVE_STRICT_CHECKS_BUCKETING("hive.strict.checks.bucketing", true,
+        "Enabling strict bucketing checks disallows the following:\n" +
+        "  Load into bucketed tables."),
+
+    @Deprecated
+    HIVEMAPREDMODE("hive.mapred.mode", null,
+        "Deprecated; use hive.strict.checks.* settings instead."),
+    HIVEALIAS("hive.alias", "", ""),
+    HIVEMAPSIDEAGGREGATE("hive.map.aggr", true, "Whether to use map-side aggregation in Hive Group By queries"),
+    HIVEGROUPBYSKEW("hive.groupby.skewindata", false, "Whether there is skew in data to optimize group by queries"),
+    HIVEJOINEMITINTERVAL("hive.join.emit.interval", 1000,
+        "How many rows in the right-most join operand Hive should buffer before emitting the join result."),
+    HIVEJOINCACHESIZE("hive.join.cache.size", 25000,
+        "How many rows in the joining tables (except the streaming table) should be cached in memory."),
+    HIVE_PUSH_RESIDUAL_INNER("hive.join.inner.residual", false,
+        "Whether to push non-equi filter predicates within inner joins. This can improve efficiency in "
+        + "the evaluation of certain joins, since we will not be emitting rows which are thrown away by "
+        + "a Filter operator straight away. However, currently vectorization does not support them, thus "
+        + "enabling it is only recommended when vectorization is disabled."),
+
+    // CBO related
+    HIVE_CBO_ENABLED("hive.cbo.enable", true, "Flag to control enabling Cost Based Optimizations using Calcite framework."),
+    HIVE_CBO_CNF_NODES_LIMIT("hive.cbo.cnf.maxnodes", -1, "When converting to conjunctive normal form (CNF), fail if" +
+        "the expression exceeds this threshold; the threshold is expressed in terms of number of nodes (leaves and" +
+        "interior nodes). -1 to not set up a threshold."),
+    HIVE_CBO_RETPATH_HIVEOP("hive.cbo.returnpath.hiveop", false, "Flag to control calcite plan to hive operator conversion"),
+    HIVE_CBO_EXTENDED_COST_MODEL("hive.cbo.costmodel.extended", false, "Flag to control enabling the extended cost model based on"
+                                 + "CPU, IO and cardinality. Otherwise, the cost model is based on cardinality."),
+    HIVE_CBO_COST_MODEL_CPU("hive.cbo.costmodel.cpu", "0.000001", "Default cost of a comparison"),
+    HIVE_CBO_COST_MODEL_NET("hive.cbo.costmodel.network", "150.0", "Default cost of a transfering a byte over network;"
+                                                                  + " expressed as multiple of CPU cost"),
+    HIVE_CBO_COST_MODEL_LFS_WRITE("hive.cbo.costmodel.local.fs.write", "4.0", "Default cost of writing a byte to local FS;"
+                                                                             + " expressed as multiple of NETWORK cost"),
+    HIVE_CBO_COST_MODEL_LFS_READ("hive.cbo.costmodel.local.fs.read", "4.0", "Default cost of reading a byte from local FS;"
+                                                                           + " expressed as multiple of NETWORK cost"),
+    HIVE_CBO_COST_MODEL_HDFS_WRITE("hive.cbo.costmodel.hdfs.write", "10.0", "Default cost of writing a byte to HDFS;"
+                                                                 + " expressed as multiple of Local FS write cost"),
+    HIVE_CBO_COST_MODEL_HDFS_READ("hive.cbo.costmodel.hdfs.read", "1.5", "Default cost of reading a byte from HDFS;"
+                                                                 + " expressed as multiple of Local FS read cost"),
+    HIVE_CBO_SHOW_WARNINGS("hive.cbo.show.warnings", true,
+         "Toggle display of CBO warnings like missing column stats"),
+    AGGR_JOIN_TRANSPOSE("hive.transpose.aggr.join", false, "push aggregates through join"),
+    SEMIJOIN_CONVERSION("hive.optimize.semijoin.conversion", true, "convert group by followed by inner equi join into semijoin"),
+    HIVE_COLUMN_ALIGNMENT("hive.order.columnalignment", true, "Flag to control whether we want to try to align" +
+        "columns in operators such as Aggregate or Join so that we try to reduce the number of shuffling stages"),
+
+    // materialized views
+    HIVE_MATERIALIZED_VIEW_ENABLE_AUTO_REWRITING("hive.materializedview.rewriting", false,
+        "Whether to try to rewrite queries using the materialized views enabled for rewriting"),
+    HIVE_MATERIALIZED_VIEW_FILE_FORMAT("hive.materializedview.fileformat", "ORC",
+        new StringSet("none", "TextFile", "SequenceFile", "RCfile", "ORC"),
+        "Default file format for CREATE MATERIALIZED VIEW statement"),
+    HIVE_MATERIALIZED_VIEW_SERDE("hive.materializedview.serde",
+        "org.apache.hadoop.hive.ql.io.orc.OrcSerde", "Default SerDe used for materialized views"),
+
+    // hive.mapjoin.bucket.cache.size has been replaced by hive.smbjoin.cache.row,
+    // need to remove by hive .13. Also, do not change default (see SMB operator)
+    HIVEMAPJOINBUCKETCACHESIZE("hive.mapjoin.bucket.cache.size", 100, ""),
+
+    HIVEMAPJOINUSEOPTIMIZEDTABLE("hive.mapjoin.optimized.hashtable", true,
+        "Whether Hive should use memory-optimized hash table for MapJoin.\n" +
+        "Only works on Tez and Spark, because memory-optimized hashtable cannot be serialized."),
+    HIVEMAPJOINOPTIMIZEDTABLEPROBEPERCENT("hive.mapjoin.optimized.hashtable.probe.percent",
+        (float) 0.5, "Probing space percentage of the optimized hashtable"),
+    HIVEUSEHYBRIDGRACEHASHJOIN("hive.mapjoin.hybridgrace.hashtable", true, "Whether to use hybrid" +
+        "grace hash join as the join method for mapjoin. Tez only."),
+    HIVEHYBRIDGRACEHASHJOINMEMCHECKFREQ("hive.mapjoin.hybridgrace.memcheckfrequency", 1024, "For " +
+        "hybrid grace hash join, how often (how many rows apart) we check if memory is full. " +
+        "This number should be power of 2."),
+    HIVEHYBRIDGRACEHASHJOINMINWBSIZE("hive.mapjoin.hybridgrace.minwbsize", 524288, "For hybrid grace" +
+        "Hash join, the minimum write buffer size used by optimized hashtable. Default is 512 KB."),
+    HIVEHYBRIDGRACEHASHJOINMINNUMPARTITIONS("hive.mapjoin.hybridgrace.minnumpartitions", 16, "For" +
+        "Hybrid grace hash join, the minimum number of partitions to create."),
+    HIVEHASHTABLEWBSIZE("hive.mapjoin.optimized.hashtable.wbsize", 8 * 1024 * 1024,
+        "Optimized hashtable (see hive.mapjoin.optimized.hashtable) uses a chain of buffers to\n" +
+        "store data. This is one buffer size. HT may be slightly faster if this is larger, but for small\n" +
+        "joins unnecessary memory will be allocated and then trimmed."),
+    HIVEHYBRIDGRACEHASHJOINBLOOMFILTER("hive.mapjoin.hybridgrace.bloomfilter", true, "Whether to " +
+        "use BloomFilter in Hybrid grace hash join to minimize unnecessary spilling."),
+
+    HIVESMBJOINCACHEROWS("hive.smbjoin.cache.rows", 10000,
+        "How many rows with the same key value should be cached in memory per smb joined table."),
+    HIVEGROUPBYMAPINTERVAL("hive.groupby.mapaggr.checkinterval", 100000,
+        "Number of rows after which size of the grouping keys/aggregation classes is performed"),
+    HIVEMAPAGGRHASHMEMORY("hive.map.aggr.hash.percentmemory", (float) 0.5,
+        "Portion of total memory to be used by map-side group aggregation hash table"),
+    HIVEMAPJOINFOLLOWEDBYMAPAGGRHASHMEMORY("hive.mapjoin.followby.map.aggr.hash.percentmemory", (float) 0.3,
+        "Portion of total memory to be used by map-side group aggregation hash table, when this group by is followed by map join"),
+    HIVEMAPAGGRMEMORYTHRESHOLD("hive.map.aggr.hash.force.flush.memory.threshold", (float) 0.9,
+        "The max memory to be used by map-side group aggregation hash table.\n" +
+        "If the memory usage is higher than this number, force to flush data"),
+    HIVEMAPAGGRHASHMINREDUCTION("hive.map.aggr.hash.min.reduction", (float) 0.5,
+        "Hash aggregation will be turned off if the ratio between hash  table size and input rows is bigger than this number. \n" +
+        "Set to 1 to make sure hash aggregation is never turned off."),
+    HIVEMULTIGROUPBYSINGLEREDUCER("hive.multigroupby.singlereducer", true,
+        "Whether to optimize multi group by query to generate single M/R  job plan. If the multi group by query has \n" +
+        "common group by keys, it will be optimized to generate single M/R job."),
+    HIVE_MAP_GROUPBY_SORT("hive.map.groupby.sorted", true,
+        "If the bucketing/sorting properties of the table exactly match the grouping key, whether to perform \n" +
+        "the group by in the mapper by using BucketizedHiveInputFormat. The only downside to this\n" +
+        "is that it limits the number of mappers to the number of files."),
+    HIVE_GROUPBY_POSITION_ALIAS("hive.groupby.position.alias", false,
+        "Whether to enable using Column Position Alias in Group By"),
+    HIVE_ORDERBY_POSITION_ALIAS("hive.orderby.position.alias", true,
+        "Whether to enable using Column Position Alias in Order By"),
+    @Deprecated
+    HIVE_GROUPBY_ORDERBY_POSITION_ALIAS("hive.groupby.orderby.position.alias", false,
+        "Whether to enable using Column Position Alias in Group By or Order By (deprecated).\n" +
+        "Use " + HIVE_ORDERBY_POSITION_ALIAS.varname + " or " + HIVE_GROUPBY_POSITION_ALIAS.varname + " instead"),
+    HIVE_NEW_JOB_GROUPING_SET_CARDINALITY("hive.new.job.grouping.set.cardinality", 30,
+        "Whether a new map-reduce job should be launched for grouping sets/rollups/cubes.\n" +
+        "For a query like: select a, b, c, count(1) from T group by a, b, c with rollup;\n" +
+        "4 rows are created per row: (a, b, c), (a, b, null), (a, null, null), (null, null, null).\n" +
+        "This can lead to explosion across map-reduce boundary if the cardinality of T is very high,\n" +
+        "and map-side aggregation does not do a very good job. \n" +
+        "\n" +
+        "This parameter decides if Hive should add an additional map-reduce job. If the grouping set\n" +
+        "cardinality (4 in the example above), is more than this value, a new MR job is added under the\n" +
+        "assumption that the original group by will reduce the data size."),
+    HIVE_GROUPBY_LIMIT_EXTRASTEP("hive.groupby.limit.extrastep", true, "This parameter decides if Hive should \n" +
+        "create new MR job for sorting final output"),
+
+    // Max file num and size used to do a single copy (after that, distcp is used)
+    HIVE_EXEC_COPYFILE_MAXNUMFILES("hive.exec.copyfile.maxnumfiles", 1L,
+        "Maximum number of files Hive uses to do sequential HDFS copies between directories." +
+        "Distributed copies (distcp) will be used instead for larger numbers of files so that copies can be done faster."),
+    HIVE_EXEC_COPYFILE_MAXSIZE("hive.exec.copyfile.maxsize", 32L * 1024 * 1024 /*32M*/,
+        "Maximum file size (in bytes) that Hive uses to do single HDFS copies between directories." +
+        "Distributed copies (distcp) will be used instead for bigger files so that copies can be done faster."),
+
+    // for hive udtf operator
+    HIVEUDTFAUTOPROGRESS("hive.udtf.auto.progress", false,
+        "Whether Hive should automatically send progress information to TaskTracker \n" +
+        "when using UDTF's to prevent the task getting killed because of inactivity.  Users should be cautious \n" +
+        "because this may prevent TaskTracker from killing tasks with infinite loops."),
+
+    HIVEDEFAULTFILEFORMAT("hive.default.fileformat", "TextFile", new StringSet("TextFile", "SequenceFile", "RCfile", "ORC", "parquet"),
+        "Default file format for CREATE TABLE statement. Users can explicitly override it by CREATE TABLE ... STORED AS [FORMAT]"),
+    HIVEDEFAULTMANAGEDFILEFORMAT("hive.default.fileformat.managed", "none",
+        new StringSet("none", "TextFile", "SequenceFile", "RCfile", "ORC", "parquet"),
+        "Default file format for CREATE TABLE statement applied to managed tables only. External tables will be \n" +
+        "created with format specified by hive.default.fileformat. Leaving this null will result in using hive.default.fileformat \n" +
+        "for all tables."),
+    HIVEQUERYRESULTFILEFORMAT("hive.query.result.fileformat", "SequenceFile", new StringSet("TextFile", "SequenceFile", "RCfile", "Llap"),
+        "Default file format for storing result of the query."),
+    HIVECHECKFILEFORMAT("hive.fileformat.check", true, "Whether to check file format or not when loading data files"),
+
+    // default serde for rcfile
+    HIVEDEFAULTRCFILESERDE("hive.default.rcfile.serde",
+        "org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe",
+        "The default SerDe Hive will use for the RCFile format"),
+
+    HIVEDEFAULTSERDE("hive.default.serde",
+        "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
+        "The default SerDe Hive will use for storage formats that do not specify a SerDe."),
+
+    SERDESUSINGMETASTOREFORSCHEMA("hive.serdes.using.metastore.for.schema",
+        "org.apache.hadoop.hive.ql.io.orc.OrcSerde," +
+        "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe," +
+        "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe," +
+        "org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe," +
+        "org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe," +
+        "org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe," +
+        "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe," +
+        "org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe",
+        "SerDes retrieving schema from metastore. This is an internal parameter."),
+
+    HIVEHISTORYFILELOC("hive.querylog.location",
+        "${system:java.io.tmpdir}" + File.separator + "${system:user.name}",
+        "Location of Hive run time structured log file"),
+
+    HIVE_LOG_INCREMENTAL_PLAN_PROGRESS("hive.querylog.enable.plan.progress", true,
+        "Whether to log the plan's progress every time a job's progress is checked.\n" +
+        "These logs are written to the location specified by hive.querylog.location"),
+
+    HIVE_LOG_INCREMENTAL_PLAN_PROGRESS_INTERVAL("hive.querylog.plan.progress.interval", "60000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "The interval to wait between logging the plan's progress.\n" +
+        "If there is a whole number percentage change in the progress of the mappers or the reducers,\n" +
+        "the progress is logged regardless of this value.\n" +
+        "The actual interval will be the ceiling of (this value divided by the value of\n" +
+        "hive.exec.counters.pull.interval) multiplied by the value of hive.exec.counters.pull.interval\n" +
+        "I.e. if it is not divide evenly by the value of hive.exec.counters.pull.interval it will be\n" +
+        "logged less frequently than specified.\n" +
+        "This only has an effect if hive.querylog.enable.plan.progress is set to true."),
+
+    HIVESCRIPTSERDE("hive.script.serde", "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
+        "The default SerDe for transmitting input data to and reading output data from the user scripts. "),
+    HIVESCRIPTRECORDREADER("hive.script.recordreader",
+        "org.apache.hadoop.hive.ql.exec.TextRecordReader",
+        "The default record reader for reading data from the user scripts. "),
+    HIVESCRIPTRECORDWRITER("hive.script.recordwriter",
+        "org.apache.hadoop.hive.ql.exec.TextRecordWriter",
+        "The default record writer for writing data to the user scripts. "),
+    HIVESCRIPTESCAPE("hive.transform.escape.input", false,
+        "This adds an option to escape special chars (newlines, carriage returns and\n" +
+        "tabs) when they are passed to the user script. This is useful if the Hive tables\n" +
+        "can contain data that contains special characters."),
+    HIVEBINARYRECORDMAX("hive.binary.record.max.length", 1000,
+        "Read from a binary stream and treat each hive.binary.record.max.length bytes as a record. \n" +
+        "The last record before the end of stream can have less than hive.binary.record.max.length bytes"),
+
+    HIVEHADOOPMAXMEM("hive.mapred.local.mem", 0, "mapper/reducer memory in local mode"),
+
+    //small table file size
+    HIVESMALLTABLESFILESIZE("hive.mapjoin.smalltable.filesize", 25000000L,
+        "The threshold for the input file size of the small tables; if the file size is smaller \n" +
+        "than this threshold, it will try to convert the common join into map join"),
+
+
+    HIVE_SCHEMA_EVOLUTION("hive.exec.schema.evolution", true,
+        "Use schema evolution to convert self-describing file format's data to the schema desired by the reader."),
+
+    HIVE_TRANSACTIONAL_TABLE_SCAN("hive.transactional.table.scan", false,
+        "internal usage only -- do transaction (ACID) table scan.", true),
+
+    HIVE_TRANSACTIONAL_NUM_EVENTS_IN_MEMORY("hive.transactional.events.mem", 10000000,
+        "Vectorized ACID readers can often load all the delete events from all the delete deltas\n"
+        + "into memory to optimize for performance. To prevent out-of-memory errors, this is a rough heuristic\n"
+        + "that limits the total number of delete events that can be loaded into memory at once.\n"
+        + "Roughly it has been set to 10 million delete events per bucket (~160 MB).\n"),
+
+    HIVESAMPLERANDOMNUM("hive.sample.seednumber", 0,
+        "A number used to percentage sampling. By changing this number, user will change the subsets of data sampled."),
+
+    // test mode in hive mode
+    HIVETESTMODE("hive.test.mode", false,
+        "Whether Hive is running in test mode. If yes, it turns on sampling and prefixes the output tablename.",
+        false),
+    HIVETESTMODEPREFIX("hive.test.mode.prefix", "test_",
+        "In test mode, specfies prefixes for the output table", false),
+    HIVETESTMODESAMPLEFREQ("hive.test.mode.samplefreq", 32,
+        "In test mode, specfies sampling frequency for table, which is not bucketed,\n" +
+        "For example, the following query:\n" +
+        "  INSERT OVERWRITE TABLE dest SELECT col1 from src\n" +
+        "would be converted to\n" +
+        "  INSERT OVERWRITE TABLE test_dest\n" +
+        "  SELECT col1 from src TABLESAMPLE (BUCKET 1 out of 32 on rand(1))", false),
+    HIVETESTMODENOSAMPLE("hive.test.mode.nosamplelist", "",
+        "In test mode, specifies comma separated table names which would not apply sampling", false),
+    HIVETESTMODEDUMMYSTATAGGR("hive.test.dummystats.aggregator", "", "internal variable for test", false),
+    HIVETESTMODEDUMMYSTATPUB("hive.test.dummystats.publisher", "", "internal variable for test", false),
+    HIVETESTCURRENTTIMESTAMP("hive.test.currenttimestamp", null, "current timestamp for test", false),
+    HIVETESTMODEROLLBACKTXN("hive.test.rollbacktxn", false, "For testing only.  Will mark every ACID transaction aborted", false),
+    HIVETESTMODEFAILCOMPACTION("hive.test.fail.compaction", false, "For testing only.  Will cause CompactorMR to fail.", false),
+    HIVETESTMODEFAILHEARTBEATER("hive.test.fail.heartbeater", false, "For testing only.  Will cause Heartbeater to fail.", false),
+
+    HIVEMERGEMAPFILES("hive.merge.mapfiles", true,
+        "Merge small files at the end of a map-only job"),
+    HIVEMERGEMAPREDFILES("hive.merge.mapredfiles", false,
+        "Merge small files at the end of a map-reduce job"),
+    HIVEMERGETEZFILES("hive.merge.tezfiles", false, "Merge small files at the end of a Tez DAG"),
+    HIVEMERGESPARKFILES("hive.merge.sparkfiles", false, "Merge small files at the end of a Spark DAG Transformation"),
+    HIVEMERGEMAPFILESSIZE("hive.merge.size.per.task", (long) (256 * 1000 * 1000),
+        "Size of merged files at the end of the job"),
+    HIVEMERGEMAPFILESAVGSIZE("hive.merge.smallfiles.avgsize", (long) (16 * 1000 * 1000),
+        "When the average output file size of a job is less than this number, Hive will start an additional \n" +
+        "map-reduce job to merge the output files into bigger files. This is only done for map-only jobs \n" +
+        "if hive.merge.mapfiles is true, and for map-reduce jobs if hive.merge.mapredfiles is true."),
+    HIVEMERGERCFILEBLOCKLEVEL("hive.merge.rcfile.block.level", true, ""),
+    HIVEMERGEORCFILESTRIPELEVEL("hive.merge.orcfile.stripe.level", true,
+        "When hive.merge.mapfiles, hive.merge.mapredfiles or hive.merge.tezfiles is enabled\n" +
+        "while writing a table with ORC file format, enabling this config will do stripe-level\n" +
+        "fast merge for small ORC files. Note that enabling this config will not honor the\n" +
+        "padding tolerance config (hive.exec.orc.block.padding.tolerance)."),
+
+    HIVEUSEEXPLICITRCFILEHEADER("hive.exec.rcfile.use.explicit.header", true,
+        "If this is set the header for RCFiles will simply be RCF.  If this is not\n" +
+        "set the header will be that borrowed from sequence files, e.g. SEQ- followed\n" +
+        "by the input and output RCFile formats."),
+    HIVEUSERCFILESYNCCACHE("hive.exec.rcfile.use.sync.cache", true, ""),
+
+    HIVE_RCFILE_RECORD_INTERVAL("hive.io.rcfile.record.interval", Integer.MAX_VALUE, ""),
+    HIVE_RCFILE_COLUMN_NUMBER_CONF("hive.io.rcfile.column.number.conf", 0, ""),
+    HIVE_RCFILE_TOLERATE_CORRUPTIONS("hive.io.rcfile.tolerate.corruptions", false, ""),
+    HIVE_RCFILE_RECORD_BUFFER_SIZE("hive.io.rcfile.record.buffer.size", 4194304, ""),   // 4M
+
+    PARQUET_MEMORY_POOL_RATIO("parquet.memory.pool.ratio", 0.5f,
+        "Maximum fraction of heap that can be used by Parquet file writers in one task.\n" +
+        "It is for avoiding OutOfMemory error in tasks. Work with Parquet 1.6.0 and above.\n" +
+        "This config parameter is defined in Parquet, so that it does not start with 'hive.'."),
+    @Deprecated
+    HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION("hive.parquet.timestamp.skip.conversion", true,
+        "Current Hive implementation of parquet stores timestamps to UTC, this flag allows skipping of the conversion" +
+            "on reading parquet files from other tools"),
+    HIVE_PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE("hive.parquet.mr.int96.enable.utc.write.zone", false,
+        "Enable this variable to use UTC as the default timezone for new Parquet tables."),
+    HIVE_INT_TIMESTAMP_CONVERSION_IN_SECONDS("hive.int.timestamp.conversion.in.seconds", false,
+        "Boolean/tinyint/smallint/int/bigint value is interpreted as milliseconds during the timestamp conversion.\n" +
+        "Set this flag to true to interpret the value as seconds to be consistent with float/double." ),
+
+    HIVE_ORC_BASE_DELTA_RATIO("hive.exec.orc.base.delta.ratio", 8, "The ratio of base writer and\n" +
+        "delta writer in terms of STRIPE_SIZE and BUFFER_SIZE."),
+    HIVE_ORC_SPLIT_STRATEGY("hive.exec.orc.split.strategy", "HYBRID", new StringSet("HYBRID", "BI", "ETL"),
+        "This is not a user level config. BI strategy is used when the requirement is to spend less time in split generation" +
+        " as opposed to query execution (split generation does not read or cache file footers)." +
+        " ETL strategy is used when spending little more time in split generation is acceptable" +
+        " (split generation reads and caches file footers). HYBRID chooses between the above strategies" +
+        " based on heuristics."),
+
+    HIVE_ORC_MS_FOOTER_CACHE_ENABLED("hive.orc.splits.ms.footer.cache.enabled", false,
+        "Whether to enable using file metadata cache in metastore for ORC file footers."),
+    HIVE_ORC_MS_FOOTER_CACHE_PPD("hive.orc.splits.ms.footer.cache.ppd.enabled", true,
+        "Whether to enable file footer cache PPD (hive.orc.splits.ms.footer.cache.enabled\n" +
+        "must also be set to true for this to work)."),
+
+    HIVE_ORC_INCLUDE_FILE_FOOTER_IN_SPLITS("hive.orc.splits.include.file.footer", false,
+        "If turned on splits generated by orc will include metadata about the stripes in the file. This\n" +
+        "data is read remotely (from the client or HS2 machine) and sent to all the tasks."),
+    HIVE_ORC_SPLIT_DIRECTORY_BATCH_MS("hive.orc.splits.directory.batch.ms", 0,
+        "How long, in ms, to wait to batch input directories for processing during ORC split\n" +
+        "generation. 0 means process directories individually. This can increase the number of\n" +
+        "metastore calls if metastore metadata cache is used."),
+    HIVE_ORC_INCLUDE_FILE_ID_IN_SPLITS("hive.orc.splits.include.fileid", true,
+        "Include file ID in splits on file systems that support it."),
+    HIVE_ORC_ALLOW_SYNTHETIC_FILE_ID_IN_SPLITS("hive.orc.splits.allow.synthetic.fileid", true,
+        "Allow synthetic file ID in splits on file systems that don't have a native one."),
+    HIVE_ORC_CACHE_STRIPE_DETAILS_MEMORY_SIZE("hive.orc.cache.stripe.details.mem.size", "256Mb",
+        new SizeValidator(), "Maximum size of orc splits cached in the client."),
+    HIVE_ORC_COMPUTE_SPLITS_NUM_THREADS("hive.orc.compute.splits.num.threads", 10,
+        "How many threads orc should use to create splits in parallel."),
+    HIVE_ORC_CACHE_USE_SOFT_REFERENCES("hive.orc.cache.use.soft.references", false,
+        "By default, the cache that ORC input format uses to store orc file footer use hard\n" +
+        "references for the cached object. Setting this to true can help avoid out of memory\n" +
+        "issues under memory pressure (in some cases) at the cost of slight unpredictability in\n" +
+        "overall query performance."),
+
+    HIVE_LAZYSIMPLE_EXTENDED_BOOLEAN_LITERAL("hive.lazysimple.extended_boolean_literal", false,
+        "LazySimpleSerde uses this property to determine if it treats 'T', 't', 'F', 'f',\n" +
+        "'1', and '0' as extened, legal boolean literal, in addition to 'TRUE' and 'FALSE'.\n" +
+        "The default is false, which means only 'TRUE' and 'FALSE' are treated as legal\n" +
+        "boolean literal."),
+
+    HIVESKEWJOIN("hive.optimize.skewjoin", false,
+        "Whether to enable skew join optimization. \n" +
+        "The algorithm is as follows: At runtime, detect the keys with a large skew. Instead of\n" +
+        "processing those keys, store them temporarily in an HDFS directory. In a follow-up map-reduce\n" +
+        "job, process those skewed keys. The same key need not be skewed for all the tables, and so,\n" +
+        "the follow-up map-reduce job (for the skewed keys) would be much faster, since it would be a\n" +
+        "map-join."),
+    HIVEDYNAMICPARTITIONHASHJOIN("hive.optimize.dynamic.partition.hashjoin", false,
+        "Whether to enable dynamically partitioned hash join optimization. \n" +
+        "This setting is also dependent on enabling hive.auto.convert.join"),
+    HIVECONVERTJOIN("hive.auto.convert.join", true,
+        "Whether Hive enables the optimization about converting common join into mapjoin based on the input file size"),
+    HIVECONVERTJOINNOCONDITIONALTASK("hive.auto.convert.join.noconditionaltask", true,
+        "Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. \n" +
+        "If this parameter is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than the\n" +
+        "specified size, the join is directly converted to a mapjoin (there is no conditional task)."),
+
+    HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD("hive.auto.convert.join.noconditionaltask.size",
+        10000000L,
+        "If hive.auto.convert.join.noconditionaltask is off, this parameter does not take affect. \n" +
+        "However, if it is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than this size, \n" +
+        "the join is directly converted to a mapjoin(there is no conditional task). The default is 10MB"),
+    HIVECONVERTJOINUSENONSTAGED("hive.auto.convert.join.use.nonstaged", false,
+        "For conditional joins, if input stream from a small alias can be directly applied to join operator without \n" +
+        "filtering or projection, the alias need not to be pre-staged in distributed cache via mapred local task.\n" +
+        "Currently, this is not working with vectorization or tez execution engine."),
+    HIVESKEWJOINKEY("hive.skewjoin.key", 100000,
+        "Determine if we get a skew key in join. If we see more than the specified number of rows with the same key in join operator,\n" +
+        "we think the key as a skew join key. "),
+    HIVESKEWJOINMAPJOINNUMMAPTASK("hive.skewjoin.mapjoin.map.tasks", 10000,
+        "Determine the number of map task used in the follow up map join job for a skew join.\n" +
+        "It should be used together with hive.skewjoin.mapjoin.min.split to perform a fine grained control."),
+    HIVESKEWJOINMAPJOINMINSPLIT("hive.skewjoin.mapjoin.min.split", 33554432L,
+        "Determine the number of map task at most used in the follow up map join job for a skew join by specifying \n" +
+        "the minimum split size. It should be used together with hive.skewjoin.mapjoin.map.tasks to perform a fine grained control."),
+
+    HIVESENDHEARTBEAT("hive.heartbeat.interval", 1000,
+        "Send a heartbeat after this interval - used by mapjoin and filter operators"),
+    HIVELIMITMAXROWSIZE("hive.limit.row.max.size", 100000L,
+        "When trying a smaller subset of data for simple LIMIT, how much size we need to guarantee each row to have at least."),
+    HIVELIMITOPTLIMITFILE("hive.limit.optimize.limit.file", 10,
+        "When trying a smaller subset of data for simple LIMIT, maximum number of files we can sample."),
+    HIVELIMITOPTENABLE("hive.limit.optimize.enable", false,
+        "Whether to enable to optimization to trying a smaller subset of data for simple LIMIT first."),
+    HIVELIMITOPTMAXFETCH("hive.limit.optimize.fetch.max", 50000,
+        "Maximum number of rows allowed for a smaller subset of data for simple LIMIT, if it is a fetch query. \n" +
+        "Insert queries are not restricted by this limit."),
+    HIVELIMITPUSHDOWNMEMORYUSAGE("hive.limit.pushdown.memory.usage", 0.1f, new RatioValidator(),
+        "The fraction of available memory to be used for buffering rows in Reducesink operator for limit pushdown optimization."),
+
+    @Deprecated
+    HIVELIMITTABLESCANPARTITION("hive.limit.query.max.table.partition", -1,
+        "This controls how many partitions can be scanned for each partitioned table.\n" +
+        "The default value \"-1\" means no limit. (DEPRECATED: Please use " + ConfVars.METASTORE_LIMIT_PARTITION_REQUEST + " in the metastore instead.)"),
+
+    HIVECONVERTJOINMAXENTRIESHASHTABLE("hive.auto.convert.join.hashtable.max.entries", 40000000L,
+        "If hive.auto.convert.join.noconditionaltask is off, this parameter does not take affect. \n" +
+        "However, if it is on, and the predicated number of entries in hashtable for a given join \n" +
+        "input is larger than this number, the join will not be converted to a mapjoin. \n" +
+        "The value \"-1\" means no limit."),
+    HIVEHASHTABLEKEYCOUNTADJUSTMENT("hive.hashtable.key.count.adjustment", 1.0f,
+        "Adjustment to mapjoin hashtable size derived from table and column statistics; the estimate" +
+        " of the number of keys is divided by this value. If the value is 0, statistics are not used" +
+        "and hive.hashtable.initialCapacity is used instead."),
+    HIVEHASHTABLETHRESHOLD("hive.hashtable.initialCapacity", 100000, "Initial capacity of " +
+        "mapjoin hashtable if statistics are absent, or if hive.hashtable.key.count.adjustment is set to 0"),
+    HIVEHASHTABLELOADFACTOR("hive.hashtable.loadfactor", (float) 0.75, ""),
+    HIVEHASHTABLEFOLLOWBYGBYMAXMEMORYUSAGE("hive.mapjoin.followby.gby.localtask.max.memory.usage", (float) 0.55,
+        "This number means how much memory the local task can take to hold the key/value into an in-memory hash table \n" +
+        "when this map join is followed by a group by. If the local task's memory usage is more than this number, \n" +
+        "the local task will abort by itself. It means the data of the small table is too large to be held in memory."),
+    HIVEHASHTABLEMAXMEMORYUSAGE("hive.mapjoin.localtask.max.memory.usage", (float) 0.90,
+        "This number means how much memory the local task can take to hold the key/value into an in-memory hash table. \n" +
+        "If the local task's memory usage is more than this number, the local task will abort by itself. \n" +
+        "It means the data of the small table is too large to be held in memory."),
+    HIVEHASHTABLESCALE("hive.mapjoin.check.memory.rows", (long)100000,
+        "The number means after how many rows processed it needs to check the memory usage"),
+
+    HIVEDEBUGLOCALTASK("hive.debug.localtask",false, ""),
+
+    HIVEINPUTFORMAT("hive.input.format", "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat",
+        "The default input format. Set this to HiveInputFormat if you encounter problems with CombineHiveInputFormat."),
+    HIVETEZINPUTFORMAT("hive.tez.input.format", "org.apache.hadoop.hive.ql.io.HiveInputFormat",
+        "The default input format for tez. Tez groups splits in the AM."),
+
+    HIVETEZCONTAINERSIZE("hive.tez.container.size", -1,
+        "By default Tez will spawn containers of the size of a mapper. This can be used to overwrite."),
+    HIVETEZCPUVCORES("hive.tez.cpu.vcores", -1,
+        "By default Tez will ask for however many cpus map-reduce is configured to use per container.\n" +
+        "This can be used to overwrite."),
+    HIVETEZJAVAOPTS("hive.tez.java.opts", null,
+        "By default Tez will use the Java options from map tasks. This can be used to overwrite."),
+    HIVETEZLOGLEVEL("hive.tez.log.level", "INFO",
+        "The log level to use for tasks executing as part of the DAG.\n" +
+        "Used only if hive.tez.java.opts is used to configure Java options."),
+    HIVETEZHS2USERACCESS("hive.tez.hs2.user.access", true,
+        "Whether to grant access to the hs2/hive user for queries"),
+    HIVEQUERYNAME ("hive.query.name", null,
+        "This named is used by Tez to set the dag name. This name in turn will appear on \n" +
+        "the Tez UI representing the work that was done."),
+
+    HIVEOPTIMIZEBUCKETINGSORTING("hive.optimize.bucketingsorting", true,
+        "Don't create a reducer for enforcing \n" +
+        "bucketing/sorting for queries of the form: \n" +
+        "insert overwrite table T2 select * from T1;\n" +
+        "where T1 and T2 are bucketed/sorted by the same keys into the same number of buckets."),
+    HIVEPARTITIONER("hive.mapred.partitioner", "org.apache.hadoop.hive.ql.io.DefaultHivePartitioner", ""),
+    HIVEENFORCESORTMERGEBUCKETMAPJOIN("hive.enforce.sortmergebucketmapjoin", false,
+        "If the user asked for sort-merge bucketed map-side join, and it cannot be performed, should the query fail or not ?"),
+    HIVEENFORCEBUCKETMAPJOIN("hive.enforce.bucketmapjoin", false,
+        "If the user asked for bucketed map-side join, and it cannot be performed, \n" +
+        "should the query fail or not ? For example, if the buckets in the tables being joined are\n" +
+        "not a multiple of each other, bucketed map-side join cannot be performed, and the\n" +
+        "query will fail if hive.enforce.bucketmapjoin is set to true."),
+
+    HIVE_AUTO_SORTMERGE_JOIN("hive.auto.convert.sortmerge.join", false,
+        "Will the join be automatically converted to a sort-merge join, if the joined tables pass the criteria for sort-merge join."),
+    HIVE_AUTO_SORTMERGE_JOIN_REDUCE("hive.auto.convert.sortmerge.join.reduce.side", true,
+        "Whether hive.auto.convert.sortmerge.join (if enabled) should be applied to reduce side."),
+    HIVE_AUTO_SORTMERGE_JOIN_BIGTABLE_SELECTOR(
+        "hive.auto.convert.sortmerge.join.bigtable.selection.policy",
+        "org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ",
+        "The policy to choose the big table for automatic conversion to sort-merge join. \n" +
+        "By default, the table with the largest partitions is assigned the big table. All policies are:\n" +
+        ". based on position of the table - the leftmost table is selected\n" +
+        "org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSMJ.\n" +
+        ". based on total size (all the partitions selected in the query) of the table \n" +
+        "org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ.\n" +
+        ". based on average size (all the partitions selected in the query) of the table \n" +
+        "org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ.\n" +
+        "New policies can be added in future."),
+    HIVE_AUTO_SORTMERGE_JOIN_TOMAPJOIN(
+        "hive.auto.convert.sortmerge.join.to.mapjoin", false,
+        "If hive.auto.convert.sortmerge.join is set to true, and a join was converted to a sort-merge join, \n" +
+        "this parameter decides whether each table should be tried as a big table, and effectively a map-join should be\n" +
+        "tried. That would create a conditional task with n+1 children for a n-way join (1 child for each table as the\n" +
+        "big table), and the backup task will be the sort-merge join. In some cases, a map-join would be faster than a\n" +
+        "sort-merge join, if there is no advantage of having the output bucketed and sorted. For example, if a very big sorted\n" +
+        "and bucketed table with few files (say 10 files) are being joined with a very small sorter and bucketed table\n" +
+        "with few files (10 files), the sort-merge join will only use 10 mappers, and a simple map-only join might be faster\n" +
+        "if the complete small table can fit in memory, and a map-join can be performed."),
+
+    HIVESCRIPTOPERATORTRUST("hive.exec.script.trust", false, ""),
+    HIVEROWOFFSET("hive.exec.rowoffset", false,
+        "Whether to provide the row offset virtual column"),
+
+    // Optimizer
+    HIVEOPTINDEXFILTER("hive.optimize.index.filter", false,
+        "Whether to enable automatic use of indexes"),
+    HIVEINDEXAUTOUPDATE("hive.optimize.index.autoupdate", false,
+        "Whether to update stale indexes automatically"),
+    HIVEOPTPPD("hive.optimize.ppd", true,
+        "Whether to enable predicate pushdown"),
+    HIVEOPTPPD_WINDOWING("hive.optimize.ppd.windowing", true,
+        "Whether to enable predicate pushdown through windowing"),
+    HIVEPPDRECOGNIZETRANSITIVITY("hive.ppd.recognizetransivity", true,
+        "Whether to transitively replicate predicate filters over equijoin conditions."),
+    HIVEPPDREMOVEDUPLICATEFILTERS("hive.ppd.remove.duplicatefilters", true,
+        "During query optimization, filters may be pushed down in the operator tree. \n" +
+        "If this config is true only pushed down filters remain in the operator tree, \n" +
+        "and the original filter is removed. If this config is false, the original filter \n" +
+        "is also left in the operator tree at the original place."),
+    HIVEPOINTLOOKUPOPTIMIZER("hive.optimize.point.lookup", true,
+         "Whether to transform OR clauses in Filter operators into IN clauses"),
+    HIVEPOINTLOOKUPOPTIMIZERMIN("hive.optimize.point.lookup.min", 31,
+             "Minimum number of OR clauses needed to transform into IN clauses"),
+    HIVECOUNTDISTINCTOPTIMIZER("hive.optimize.countdistinct", true,
+                 "Whether to transform count distinct into two stages"),
+   HIVEPARTITIONCOLUMNSEPARATOR("hive.optimize.partition.columns.separate", true,
+            "Extract partition columns from IN clauses"),
+    // Constant propagation optimizer
+    HIVEOPTCONSTANTPROPAGATION("hive.optimize.constant.propagation", true, "Whether to enable constant propagation optimizer"),
+    HIVEIDENTITYPROJECTREMOVER("hive.optimize.remove.identity.project", true, "Removes identity project from operator tree"),
+    HIVEMETADATAONLYQUERIES("hive.optimize.metadataonly", false,
+        "Whether to eliminate scans of the tables from which no columns are selected. Note\n" +
+        "that, when selecting from empty tables with data files, this can produce incorrect\n" +
+        "results, so it's disabled by default. It works correctly for normal tables."),
+    HIVENULLSCANOPTIMIZE("hive.optimize.null.scan", true, "Dont scan relations which are guaranteed to not generate any rows"),
+    HIVEOPTPPD_STORAGE("hive.optimize.ppd.storage", true,
+        "Whether to push predicates down to storage handlers"),
+    HIVEOPTGROUPBY("hive.optimize.groupby", true,
+        "Whether to enable the bucketed group by from bucketed partitions/tables."),
+    HIVEOPTBUCKETMAPJOIN("hive.optimize.bucketmapjoin", false,
+        "Whether to try bucket mapjoin"),
+    HIVEOPTSORTMERGEBUCKETMAPJOIN("hive.optimize.bucketmapjoin.sortedmerge", false,
+        "Whether to try sorted bucket merge map join"),
+    HIVEOPTREDUCEDEDUPLICATION("hive.optimize.reducededuplication", true,
+        "Remove extra map-reduce jobs if the data is already clustered by the same key which needs to be used again. \n" +
+        "This should always be set to true. Since it is a new feature, it has been made configurable."),
+    HIVEOPTREDUCEDEDUPLICATIONMINREDUCER("hive.optimize.reducededuplication.min.reducer", 4,
+        "Reduce deduplication merges two RSs by moving key/parts/reducer-num of the child RS to parent RS. \n" +
+        "That means if reducer-num of the child RS is fixed (order by or forced bucketing) and small, it can make very slow, single MR.\n" +
+        "The optimization will be automatically disabled if number of reducers would be less than specified value."),
+
+    HIVEOPTSORTDYNAMICPARTITION("hive.optimize.sort.dynamic.partition", false,
+        "When enabled dynamic partitioning column will be globally sorted.\n" +
+        "This way we can keep only one record writer open for each partition value\n" +
+        "in the reducer thereby reducing the memory pressure on reducers."),
+
+    HIVESAMPLINGFORORDERBY("hive.optimize.sampling.orderby", false, "Uses sampling on order-by clause for parallel execution."),
+    HIVESAMPLINGNUMBERFORORDERBY("hive.optimize.sampling.orderby.number", 1000, "Total number of samples to be obtained."),
+    HIVESAMPLINGPERCENTFORORDERBY("hive.optimize.sampling.orderby.percent", 0.1f, new RatioValidator(),
+        "Probability with which a row will be chosen."),
+    HIVEOPTIMIZEDISTINCTREWRITE("hive.optimize.distinct.rewrite", true, "When applicable this "
+        + "optimization rewrites distinct aggregates from a single stage to multi-stage "
+        + "aggregation. This may not be optimal in all cases. Ideally, whether to trigger it or "
+        + "not should be cost based decision. Until Hive formalizes cost model for this, this is config driven."),
+    // whether to optimize union followed by select followed by filesink
+    // It creates sub-directories in the final output, so should not be turned on in systems
+    // where MAPREDUCE-1501 is not present
+    HIVE_OPTIMIZE_UNION_REMOVE("hive.optimize.union.remove", false,
+        "Whether to remove the union and push the operators between union and the filesink above union. \n" +
+        "This avoids an extra scan of the output by union. This is independently useful for union\n" +
+        "queries, and specially useful when hive.optimize.skewjoin.compiletime is set to true, since an\n" +
+        "extra union is inserted.\n" +
+        "\n" +
+        "The merge is triggered if either of hive.merge.mapfiles or hive.merge.mapredfiles is set to true.\n" +
+        "If the user has set hive.merge.mapfiles to true and hive.merge.mapredfiles to false, the idea was the\n" +
+        "number of reducers are few, so the number of files anyway are small. However, with this optimization,\n" +
+        "we are increasing the number of files possibly by a big margin. So, we merge aggressively."),
+    HIVEOPTCORRELATION("hive.optimize.correlation", false, "exploit intra-query correlations."),
+
+    HIVE_OPTIMIZE_LIMIT_TRANSPOSE("hive.optimize.limittranspose", false,
+        "Whether to push a limit through left/right outer join or union. If the value is true and the size of the outer\n" +
+        "input is reduced enough (as specified in hive.optimize.limittranspose.reduction), the limit is pushed\n" +
+        "to the outer input or union; to remain semantically correct, the limit is kept on top of the join or the union too."),
+    HIVE_OPTIMIZE_LIMIT_TRANSPOSE_REDUCTION_PERCENTAGE("hive.optimize.limittranspose.reductionpercentage", 1.0f,
+        "When hive.optimize.limittranspose is true, this variable specifies the minimal reduction of the\n" +
+        "size of the outer input of the join or input of the union that we should get in order to apply the rule."),
+    HIVE_OPTIMIZE_LIMIT_TRANSPOSE_REDUCTION_TUPLES("hive.optimize.limittranspose.reductiontuples", (long) 0,
+        "When hive.optimize.limittranspose is true, this variable specifies the minimal reduction in the\n" +
+        "number of tuples of the outer input of the join or the input of the union that you should get in order to apply the rule."),
+
+    HIVE_OPTIMIZE_REDUCE_WITH_STATS("hive.optimize.filter.stats.reduction", false, "Whether to simplify comparison\n" +
+        "expressions in filter operators using column stats"),
+
+    HIVE_OPTIMIZE_SKEWJOIN_COMPILETIME("hive.optimize.skewjoin.compiletime", false,
+        "Whether to create a separate plan for skewed keys for the tables in the join.\n" +
+        "This is based on the skewed keys stored in the metadata. At compile time, the plan is broken\n" +
+        "into different joins: one for the skewed keys, and the other for the remaining keys. And then,\n" +
+        "a union is performed for the 2 joins generated above. So unless the same skewed key is present\n" +
+        "in both the joined tables, the join for the skewed key will be performed as a map-side join.\n" +
+        "\n" +
+        "The main difference between this parameter and hive.optimize.skewjoin is that this parameter\n" +
+        "uses the skew information stored in the metastore to optimize the plan at compile time itself.\n" +
+        "If there is no skew information in the metadata, this parameter will not have any affect.\n" +
+        "Both hive.optimize.skewjoin.compiletime and hive.optimize.skewjoin should be set to true.\n" +
+        "Ideally, hive.optimize.skewjoin should be renamed as hive.optimize.skewjoin.runtime, but not doing\n" +
+        "so for backward compatibility.\n" +
+        "\n" +
+        "If the skew information is correctly stored in the metadata, hive.optimize.skewjoin.compiletime\n" +
+        "would change the query plan to take care of it, and hive.optimize.skewjoin will be a no-op."),
+
+    HIVE_SHARED_WORK_OPTIMIZATION("hive.optimize.shared.work", true,
+        "Whether to enable shared work optimizer. The optimizer finds scan operator over the same table\n" +
+        "and follow-up operators in the query plan and merges them if they meet some preconditions."),
+
+    // CTE
+    HIVE_CTE_MATERIALIZE_THRESHOLD("hive.optimize.cte.materialize.threshold", -1,
+        "If the number of references to a CTE clause exceeds this threshold, Hive will materialize it\n" +
+        "before executing the main query block. -1 will disable this feature."),
+
+    // Indexes
+    HIVEOPTINDEXFILTER_COMPACT_MINSIZE("hive.optimize.index.filter.compact.minsize", (long) 5 * 1024 * 1024 * 1024,
+        "Minimum size (in bytes) of the inputs on which a compact index is automatically used."), // 5G
+    HIVEOPTINDEXFILTER_COMPACT_MAXSIZE("hive.optimize.index.filter.compact.maxsize", (long) -1,
+        "Maximum size (in bytes) of the inputs on which a compact index is automatically used.  A negative number is equivalent to infinity."), // infinity
+    HIVE_INDEX_COMPACT_QUERY_MAX_ENTRIES("hive.index.compact.query.max.entries", (long) 10000000,
+        "The maximum number of index entries to read during a query that uses the compact index. Negative value is equivalent to infinity."), // 10M
+    HIVE_INDEX_COMPACT_QUERY_MAX_SIZE("hive.index.compact.query.max.size", (long) 10 * 1024 * 1024 * 1024,
+        "The maximum number of bytes that a query using the compact index can read. Negative value is equivalent to infinity."), // 10G
+    HIVE_INDEX_COMPACT_BINARY_SEARCH("hive.index.compact.binary.search", true,
+        "Whether or not to use a binary search to find the entries in an index table that match the filter, where possible"),
+
+    // Statistics
+    HIVESTATSAUTOGATHER("hive.stats.autogather", true,
+        "A flag to gather statistics (only basic) automatically during the INSERT OVERWRITE command."),
+    HIVESTATSCOLAUTOGATHER("hive.stats.column.autogather", false,
+        "A flag to gather column statistics automatically."),
+    HIVESTATSDBCLASS("hive.stats.dbclass", "fs", new PatternSet("custom", "fs"),
+        "The storage that stores temporary Hive statistics. In filesystem based statistics collection ('fs'), \n" +
+        "each task writes statistics it has collected in a file on the filesystem, which will be aggregated \n" +
+        "after the job has finished. Supported values are fs (filesystem) and custom as defined in StatsSetupConst.java."), // StatsSetupConst.StatDB
+    HIVE_STATS_DEFAULT_PUBLISHER("hive.stats.default.publisher", "",
+        "The Java class (implementing the StatsPublisher interface) that is used by default if hive.stats.dbclass is custom type."),
+    HIVE_STATS_DEFAULT_AGGREGATOR("hive.stats.default.aggregator", "",
+        "The Java class (implementing the StatsAggregator interface) that is used by default if hive.stats.dbclass is custom type."),
+    HIVE_STATS_ATOMIC("hive.stats.atomic", false,
+        "whether to update metastore stats only if all stats are available"),
+    CLIENT_STATS_COUNTERS("hive.client.stats.counters", "",
+        "Subset of counters that should be of interest for hive.client.stats.publishers (when one wants to limit their publishing). \n" +
+        "Non-display names should be used"),
+    //Subset of counters that should be of interest for hive.client.stats.publishers (when one wants to limit their publishing). Non-display names should be used".
+    HIVE_STATS_RELIABLE("hive.stats.reliable", false,
+        "Whether queries will fail because stats cannot be collected completely accurately. \n" +
+        "If this is set to true, reading/writing from/into a partition may fail because the stats\n" +
+        "could not be computed accurately."),
+    HIVE_STATS_COLLECT_PART_LEVEL_STATS("hive.analyze.stmt.collect.partlevel.stats", true,
+        "analyze table T compute statistics for columns. Queries like these should compute partition"
+        + "level stats for partitioned table even when no part spec is specified."),
+    HIVE_STATS_GATHER_NUM_THREADS("hive.stats.gather.num.threads", 10,
+        "Number of threads used by partialscan/noscan analyze command for partitioned tables.\n" +
+        "This is applicable only for file formats that implement StatsProvidingRecordReader (like ORC)."),
+    // Collect table access keys information for operators that can benefit from bucketing
+    HIVE_STATS_COLLECT_TABLEKEYS("hive.stats.collect.tablekeys", false,
+        "Whether join and group by keys on tables are derived and maintained in the QueryPlan.\n" +
+        "This is useful to identify how tables are accessed and to determine if they should be bucketed."),
+    // Collect column access information
+    HIVE_STATS_COLLECT_SCANCOLS("hive.stats.collect.scancols", false,
+        "Whether column accesses are tracked in the QueryPlan.\n" +
+        "This is useful to identify how tables are accessed and to determine if there are wasted columns that can be trimmed."),
+    // standard error allowed for ndv estimates. A lower value indicates higher accuracy and a
+    // higher compute cost.
+    HIVE_STATS_NDV_ERROR("hive.stats.ndv.error", (float)20.0,
+        "Standard error expressed in percentage. Provides a tradeoff between accuracy and compute cost. \n" +
+        "A lower value for error indicates higher accuracy and a higher compute cost."),
+    HIVE_METASTORE_STATS_NDV_TUNER("hive.metastore.stats.ndv.tuner", (float)0.0,
+         "Provides a tunable parameter between the lower bound and the higher bound of ndv for aggregate ndv across all the partitions. \n" +
+         "The lower bound is equal to the maximum of ndv of all the partitions. The higher bound is equal to the sum of ndv of all the partitions.\n" +
+         "Its value should be between 0.0 (i.e., choose lower bound) and 1.0 (i.e., choose higher bound)"),
+    HIVE_METASTORE_STATS_NDV_DENSITY_FUNCTION("hive.metastore.stats.ndv.densityfunction", false,
+        "Whether to use density function to estimate the NDV for the whole table based on the NDV of partitions"),
+    HIVE_STATS_KEY_PREFIX("hive.stats.key.prefix", "", "", true), // internal usage only
+    // if length of variable length data type cannot be determined this length will be used.
+    HIVE_STATS_MAX_VARIABLE_LENGTH("hive.stats.max.variable.length", 100,
+        "To estimate the size of data flowing through operators in Hive/Tez(for reducer estimation etc.),\n" +
+        "average row size is multiplied with the total number of rows coming out of each operator.\n" +
+        "Average row size is computed from average column size of all columns in the row. In the absence\n" +
+        "of column statistics, for variable length columns (like string, bytes etc.), this value will be\n" +
+        "used. For fixed length columns their corresponding Java equivalent sizes are used\n" +
+        "(float - 4 bytes, double - 8 bytes etc.)."),
+    // if number of elements in list cannot be determined, this value will be used
+    HIVE_STATS_LIST_NUM_ENTRIES("hive.stats.list.num.entries", 10,
+        "To estimate the size of data flowing through operators in Hive/Tez(for reducer estimation etc.),\n" +
+        "average row size is multiplied with the total number of rows coming out of each operator.\n" +
+        "Average row size is computed from average column size of all columns in the row. In the absence\n" +
+        "of column statistics and for variable length complex columns like list, the average number of\n" +
+        "entries/values can be specified using this config."),
+    // if number of elements in map cannot be determined, this value will be used
+    HIVE_STATS_MAP_NUM_ENTRIES("hive.stats.map.num.entries", 10,
+        "To estimate the size of data flowing through operators in Hive/Tez(for reducer estimation etc.),\n" +
+        "average row size is multiplied with the total number of rows coming out of each operator.\n" +
+        "Average row size is computed from average column size of all columns in the row. In the absence\n" +
+        "of column statistics and for variable length complex columns like map, the average number of\n" +
+        "entries/values can be specified using this config."),
+    // statistics annotation fetches stats for each partition, which can be expensive. turning
+    // this off will result in basic sizes being fetched from namenode instead
+    HIVE_STATS_FETCH_PARTITION_STATS("hive.stats.fetch.partition.stats", true,
+        "Annotation of operator tree with statistics information requires partition level basic\n" +
+        "statistics like number of rows, data size and file size. Partition statistics are fetched from\n" +
+        "metastore. Fetching partition statistics for each needed partition can be expensive when the\n" +
+        "number of partitions is high. This flag can be used to disable fetching of partition statistics\n" +
+        "from metastore. When this flag is disabled, Hive will make calls to filesystem to get file sizes\n" +
+        "and will estimate the number of rows from row schema."),
+    // statistics annotation fetches column statistics for all required columns which can
+    // be very expensive sometimes
+    HIVE_STATS_FETCH_COLUMN_STATS("hive.stats.fetch.column.stats", false,
+        "Annotation of operator tree with statistics information requires column statistics.\n" +
+        "Column statistics are fetched from metastore. Fetching column statistics for each needed column\n" +
+        "can be expensive when the number of columns is high. This flag can be used to disable fetching\n" +
+        "of column statistics from metastore."),
+    // in the absence of column statistics, the estimated number of rows/data size that will
+    // be emitted from join operator will depend on this factor
+    HIVE_STATS_JOIN_FACTOR("hive.stats.join.factor", (float) 1.1,
+        "Hive/Tez optimizer estimates the data size flowing through each of the operators. JOIN operator\n" +
+        "uses column statistics to estimate the number of rows flowing out of it and hence the data size.\n" +
+        "In the absence of column statistics, this factor determines the amount of rows that flows out\n" +
+        "of JOIN operator."),
+    HIVE_STATS_CORRELATED_MULTI_KEY_JOINS("hive.stats.correlated.multi.key.joins", false,
+        "When estimating output rows for a join involving multiple columns, the default behavior assumes" +
+        "the columns are independent. Setting this flag to true will cause the estimator to assume" +
+        "the columns are correlated."),
+    // in the absence of uncompressed/raw data size, total file size will be used for statistics
+    // annotation. But the file may be compressed, encoded and serialized which may be lesser in size
+    // than the actual uncompressed/raw data size. This factor will be multiplied to file size to estimate
+    // the raw data size.
+    HIVE_STATS_DESERIALIZATION_FACTOR("hive.stats.deserialization.factor", (float) 1.0,
+        "Hive/Tez optimizer estimates the data size flowing through each of the operators. In the absence\n" +
+        "of basic statistics like number of rows and data size, file size is used to estimate the number\n" +
+        "of rows and data size. Since files in tables/partitions are serialized (and optionally\n" +
+        "compressed) the estimates of number of rows and data size cannot be reliably determined.\n" +
+        "This factor is multiplied with the file size to account for serialization and compression."),
+    HIVE_STATS_IN_CLAUSE_FACTOR("hive.stats.filter.in.factor", (float) 1.0,
+        "Currently column distribution is assumed to be uniform. This can lead to overestimation/underestimation\n" +
+        "in the number of rows filtered by a certain operator, which in turn might lead to overprovision or\n" +
+        "underprovision of resources. This factor is applied to the cardinality estimation of IN clauses in\n" +
+        "filter operators."),
+
+    // Concurrency
+    HIVE_SUPPORT_CONCURRENCY("hive.support.concurrency", false,
+        "Whether Hive supports concurrency control or not. \n" +
+        "A ZooKeeper instance must be up and running when using zookeeper Hive lock manager "),
+    HIVE_LOCK_MANAGER("hive.lock.manager", "org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager", ""),
+    HIVE_LOCK_NUMRETRIES("hive.lock.numretries", 100,
+        "The number of times you want to try to get all the locks"),
+    HIVE_UNLOCK_NUMRETRIES("hive.unlock.numretries", 10,
+        "The number of times you want to retry to do one unlock"),
+    HIVE_LOCK_SLEEP_BETWEEN_RETRIES("hive.lock.sleep.between.retries", "60s",
+        new TimeValidator(TimeUnit.SECONDS, 0L, false, Long.MAX_VALUE, false),
+        "The maximum sleep time between various retries"),
+    HIVE_LOCK_MAPRED_ONLY("hive.lock.mapred.only.operation", false,
+        "This param is to control whether or not only do lock on queries\n" +
+        "that need to execute at least one mapred job."),
+    HIVE_LOCK_QUERY_STRING_MAX_LENGTH("hive.lock.query.string.max.length", 1000000,
+        "The maximum length of the query string to store in the lock.\n" +
+        "The default value is 1000000, since the data limit of a znode is 1MB"),
+
+     // Zookeeper related configs
+    HIVE_ZOOKEEPER_QUORUM("hive.zookeeper.quorum", "",
+        "List of ZooKeeper servers to talk to. This is needed for: \n" +
+        "1. Read/write locks - when hive.lock.manager is set to \n" +
+        "org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager, \n" +
+        "2. When HiveServer2 supports service discovery via Zookeeper.\n" +
+        "3. For delegation token storage if zookeeper store is used, if\n" +
+        "hive.cluster.delegation.token.store.zookeeper.connectString is not set\n" +
+        "4. LLAP daemon registry service"),
+
+    HIVE_ZOOKEEPER_CLIENT_PORT("hive.zookeeper.client.port", "2181",
+        "The port of ZooKeeper servers to talk to.\n" +
+        "If the list of Zookeeper servers specified in hive.zookeeper.quorum\n" +
+        "does not contain port numbers, this value is used."),
+    HIVE_ZOOKEEPER_SESSION_TIMEOUT("hive.zookeeper.session.timeout", "1200000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "ZooKeeper client's session timeout (in milliseconds). The client is disconnected, and as a result, all locks released, \n" +
+        "if a heartbeat is not sent in the timeout."),
+    HIVE_ZOOKEEPER_NAMESPACE("hive.zookeeper.namespace", "hive_zookeeper_namespace",
+        "The parent node under which all ZooKeeper nodes are created."),
+    HIVE_ZOOKEEPER_CLEAN_EXTRA_NODES("hive.zookeeper.clean.extra.nodes", false,
+        "Clean extra nodes at the end of the session."),
+    HIVE_ZOOKEEPER_CONNECTION_MAX_RETRIES("hive.zookeeper.connection.max.retries", 3,
+        "Max number of times to retry when connecting to the ZooKeeper server."),
+    HIVE_ZOOKEEPER_CONNECTION_BASESLEEPTIME("hive.zookeeper.connection.basesleeptime", "1000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "Initial amount of time (in milliseconds) to wait between retries\n" +
+        "when connecting to the ZooKeeper server when using ExponentialBackoffRetry policy."),
+
+    // Transactions
+    HIVE_TXN_MANAGER("hive.txn.manager",
+        "org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager",
+        "Set to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager as part of turning on Hive\n" +
+        "transactions, which also requires appropriate settings for hive.compactor.initiator.on,\n" +
+        "hive.compactor.worker.threads, hive.support.concurrency (true),\n" +
+        "and hive.exec.dynamic.partition.mode (nonstrict).\n" +
+        "The default DummyTxnManager replicates pre-Hive-0.13 behavior and provides\n" +
+        "no transactions."),
+    HIVE_TXN_STRICT_LOCKING_MODE("hive.txn.strict.locking.mode", true, "In strict mode non-ACID\n" +
+        "resources use standard R/W lock semantics, e.g. INSERT will acquire exclusive lock.\n" +
+        "In nonstrict mode, for non-ACID resources, INSERT will only acquire shared lock, which\n" +
+        "allows two concurrent writes to the same partition but still lets lock manager prevent\n" +
+        "DROP TABLE etc. when the table is being written to"),
+    HIVE_TXN_TIMEOUT("hive.txn.timeout", "300s", new TimeValidator(TimeUnit.SECONDS),
+        "time after which transactions are declared aborted if the client has not sent a heartbeat."),
+    HIVE_TXN_HEARTBEAT_THREADPOOL_SIZE("hive.txn.heartbeat.threadpool.size", 5, "The number of " +
+        "threads to use for heartbeating. For Hive CLI, 1 is enough. For HiveServer2, we need a few"),
+    TXN_MGR_DUMP_LOCK_STATE_ON_ACQUIRE_TIMEOUT("hive.txn.manager.dump.lock.state.on.acquire.timeout", false,
+      "Set this to true so that when attempt to acquire a lock on resource times out, the current state" +
+        " of the lock manager is dumped to log file.  This is for debugging.  See also " +
+        "hive.lock.numretries and hive.lock.sleep.between.retries."),
+
+    HIVE_TXN_OPERATIONAL_PROPERTIES("hive.txn.operational.properties", 0,
+        "Sets the operational properties that control the appropriate behavior for various\n"
+        + "versions of the Hive ACID subsystem. Setting it to zero will turn on the legacy mode\n"
+        + "for ACID, while setting it to one will enable a split-update feature found in the newer\n"
+        + "version of Hive ACID subsystem. Mostly it is intended to be used as an internal property\n"
+        + "for future versions of ACID. (See HIVE-14035 for details.)"),
+
+    HIVE_MAX_OPEN_TXNS("hive.max.open.txns", 100000, "Maximum number of open transactions. If \n" +
+        "current open transactions reach this limit, future open transaction requests will be \n" +
+        "rejected, until this number goes below the limit."),
+    HIVE_COUNT_OPEN_TXNS_INTERVAL("hive.count.open.txns.interval", "1s",
+        new TimeValidator(TimeUnit.SECONDS), "Time in seconds between checks to count open transactions."),
+
+    HIVE_TXN_MAX_OPEN_BATCH("hive.txn.max.open.batch", 1000,
+        "Maximum number of transactions that can be fetched in one call to open_txns().\n" +
+        "This controls how many transactions streaming agents such as Flume or Storm open\n" +
+        "simultaneously. The streaming agent then writes that number of entries into a single\n" +
+        "file (per Flume agent or Storm bolt). Thus increasing this value decreases the number\n" +
+        "of delta files created by streaming agents. But it also increases the number of open\n" +
+        "transactions that Hive has to track at any given time, which may negatively affect\n" +
+        "read performance."),
+
+    HIVE_TXN_RETRYABLE_SQLEX_REGEX("hive.txn.retryable.sqlex.regex", "", "Comma separated list\n" +
+        "of regular expression patterns for SQL state, error code, and error message of\n" +
+        "retryable SQLExceptions, that's suitable for the metastore DB.\n" +
+        "For example: Can't serialize.*,40001$,^Deadlock,.*ORA-08176.*\n" +
+        "The string that the regex will be matched against is of the following form, where ex is a SQLException:\n" +
+        "ex.getMessage() + \" (SQLState=\" + ex.getSQLState() + \", ErrorCode=\" + ex.getErrorCode() + \")\""),
+
+    HIVE_COMPACTOR_INITIATOR_ON("hive.compactor.initiator.on", false,
+        "Whether to run the initiator and cleaner threads on this metastore instance or not.\n" +
+        "Set this to true on one instance of the Thrift metastore service as part of turning\n" +
+        "on Hive transactions. For a complete list of parameters required for turning on\n" +
+        "transactions, see hive.txn.manager."),
+
+    HIVE_COMPACTOR_WORKER_THREADS("hive.compactor.worker.threads", 0,
+        "How many compactor worker threads to run on this metastore instance. Set this to a\n" +
+        "positive number on one or more instances of the Thrift metastore service as part of\n" +
+        "turning on Hive transactions. For a complete list of parameters required for turning\n" +
+        "on transactions, see hive.txn.manager.\n" +
+        "Worker threads spawn MapReduce jobs to do compactions. They do not do the compactions\n" +
+        "themselves. Increasing the number of worker threads will decrease the time it takes\n" +
+        "tables or partitions to be compacted once they are determined to need compaction.\n" +
+        "It will also increase the background load on the Hadoop cluster as more MapReduce jobs\n" +
+        "will be running in the background."),
+
+    HIVE_COMPACTOR_WORKER_TIMEOUT("hive.compactor.worker.timeout", "86400s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Time in seconds after which a compaction job will be declared failed and the\n" +
+        "compaction re-queued."),
+
+    HIVE_COMPACTOR_CHECK_INTERVAL("hive.compactor.check.interval", "300s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Time in seconds between checks to see if any tables or partitions need to be\n" +
+        "compacted. This should be kept high because each check for compaction requires\n" +
+        "many calls against the NameNode.\n" +
+        "Decreasing this value will reduce the time it takes for compaction to be started\n" +
+        "for a table or partition that requires compaction. However, checking if compaction\n" +
+        "is needed requires several calls to the NameNode for each table or partition that\n" +
+        "has had a transaction done on it since the last major compaction. So decreasing this\n" +
+        "value will increase the load on the NameNode."),
+
+    HIVE_COMPACTOR_DELTA_NUM_THRESHOLD("hive.compactor.delta.num.threshold", 10,
+        "Number of delta directories in a table or partition that will trigger a minor\n" +
+        "compaction."),
+
+    HIVE_COMPACTOR_DELTA_PCT_THRESHOLD("hive.compactor.delta.pct.threshold", 0.1f,
+        "Percentage (fractional) size of the delta files relative to the base that will trigger\n" +
+        "a major compaction. (1.0 = 100%, so the default 0.1 = 10%.)"),
+    COMPACTOR_MAX_NUM_DELTA("hive.compactor.max.num.delta", 500, "Maximum number of delta files that " +
+      "the compactor will attempt to handle in a single job."),
+
+    HIVE_COMPACTOR_ABORTEDTXN_THRESHOLD("hive.compactor.abortedtxn.threshold", 1000,
+        "Number of aborted transactions involving a given table or partition that will trigger\n" +
+        "a major compaction."),
+
+    COMPACTOR_INITIATOR_FAILED_THRESHOLD("hive.compactor.initiator.failed.compacts.threshold", 2,
+      new RangeValidator(1, 20), "Number of consecutive compaction failures (per table/partition) " +
+      "after which automatic compactions will not be scheduled any more.  Note that this must be less " +
+      "than hive.compactor.history.retention.failed."),
+
+    HIVE_COMPACTOR_CLEANER_RUN_INTERVAL("hive.compactor.cleaner.run.interval", "5000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS), "Time between runs of the cleaner thread"),
+    COMPACTOR_JOB_QUEUE("hive.compactor.job.queue", "", "Used to specify name of Hadoop queue to which\n" +
+      "Compaction jobs will be submitted.  Set to empty string to let Hadoop choose the queue."),
+
+    COMPACTOR_HISTORY_RETENTION_SUCCEEDED("hive.compactor.history.retention.succeeded", 3,
+      new RangeValidator(0, 100), "Determines how many successful compaction records will be " +
+      "retained in compaction history for a given table/partition."),
+
+    COMPACTOR_HISTORY_RETENTION_FAILED("hive.compactor.history.retention.failed", 3,
+      new RangeValidator(0, 100), "Determines how many failed compaction records will be " +
+      "retained in compaction history for a given table/partition."),
+
+    COMPACTOR_HISTORY_RETENTION_ATTEMPTED("hive.compactor.history.retention.attempted", 2,
+      new RangeValidator(0, 100), "Determines how many attempted compaction records will be " +
+      "retained in compaction history for a given table/partition."),
+
+    COMPACTOR_HISTORY_REAPER_INTERVAL("hive.compactor.history.reaper.interval", "2m",
+      new TimeValidator(TimeUnit.MILLISECONDS), "Determines how often compaction history reaper runs"),
+
+    HIVE_TIMEDOUT_TXN_REAPER_START("hive.timedout.txn.reaper.start", "100s",
+      new TimeValidator(TimeUnit.MILLISECONDS), "Time delay of 1st reaper run after metastore start"),
+    HIVE_TIMEDOUT_TXN_REAPER_INTERVAL("hive.timedout.txn.reaper.interval", "180s",
+      new TimeValidator(TimeUnit.MILLISECONDS), "Time interval describing how often the reaper runs"),
+    WRITE_SET_REAPER_INTERVAL("hive.writeset.reaper.interval", "60s",
+      new TimeValidator(TimeUnit.MILLISECONDS), "Frequency of WriteSet reaper runs"),
+
+    MERGE_CARDINALITY_VIOLATION_CHECK("hive.merge.cardinality.check", true,
+      "Set to true to ensure that each SQL Merge statement ensures that for each row in the target\n" +
+        "table there is at most 1 matching row in the source table per SQL Specification."),
+
+    // For Druid storage handler
+    HIVE_DRUID_INDEXING_GRANULARITY("hive.druid.indexer.segments.granularity", "DAY",
+            new PatternSet("YEAR", "MONTH", "WEEK", "DAY", "HOUR", "MINUTE", "SECOND"),
+            "Granularity for the segments created by the Druid storage handler"
+    ),
+    HIVE_DRUID_MAX_PARTITION_SIZE("hive.druid.indexer.partition.size.max", 5000000,
+            "Maximum number of records per segment partition"
+    ),
+    HIVE_DRUID_MAX_ROW_IN_MEMORY("hive.druid.indexer.memory.rownum.max", 75000,
+            "Maximum number of records in memory while storing data in Druid"
+    ),
+    HIVE_DRUID_BROKER_DEFAULT_ADDRESS("hive.druid.broker.address.default", "localhost:8082",
+            "Address of the Druid broker. If we are querying Druid from Hive, this address needs to be\n"
+                    +
+                    "declared"
+    ),
+    HIVE_DRUID_COORDINATOR_DEFAULT_ADDRESS("hive.druid.coordinator.address.default", "localhost:8081",
+            "Address of the Druid coordinator. It is used to check the load status of newly created segments"
+    ),
+    HIVE_DRUID_SELECT_DISTRIBUTE("hive.druid.select.distribute", true,
+        "If it is set to true, we distribute the execution of Druid Select queries. Concretely, we retrieve\n" +
+        "the result for Select queries directly from the Druid nodes containing the segments data.\n" +
+        "In particular, first we contact the Druid broker node to obtain the nodes containing the segments\n" +
+        "for the given query, and then we contact those nodes to retrieve the results for the query.\n" +
+        "If it is set to false, we do not execute the Select queries in a distributed fashion. Instead, results\n" +
+        "for those queries are returned by the Druid broker node."),
+    HIVE_DRUID_SELECT_THRESHOLD("hive.druid.select.threshold", 10000,
+        "Takes only effect when hive.druid.select.distribute is set to false. \n" +
+        "When we can split a Select query, this is the maximum number of rows that we try to retrieve\n" +
+        "per query. In order to do that, we obtain the estimated size for the complete result. If the\n" +
+        "number of records of the query results is larger than this threshold, we split the query in\n" +
+        "total number of rows/threshold parts across the time dimension. Note that we assume the\n" +
+        "records to be split uniformly across the time dimension."),
+    HIVE_DRUID_NUM_HTTP_CONNECTION("hive.druid.http.numConnection", 20, "Number of connections used by\n" +
+        "the HTTP client."),
+    HIVE_DRUID_HTTP_READ_TIMEOUT("hive.druid.http.read.timeout", "PT1M", "Read timeout period for the HTTP\n" +
+        "client in ISO8601 format (for example P2W, P3M, PT1H30M, PT0.750S), default is period of 1 minute."),
+    HIVE_DRUID_SLEEP_TIME("hive.druid.sleep.time", "PT10S",
+            "Sleep time between retries in ISO8601 format (for example P2W, P3M, PT1H30M, PT0.750S), default is period of 10 seconds."
+    ),
+    HIVE_DRUID_BASE_PERSIST_DIRECTORY("hive.druid.basePersistDirectory", "",
+            "Local temporary directory used to persist intermediate indexing state, will default to JVM system property java.io.tmpdir."
+    ),
+    DRUID_SEGMENT_DIRECTORY("hive.druid.storage.storageDirectory", "/druid/segments"
+            , "druid deep storage location."),
+    DRUID_METADATA_BASE("hive.druid.metadata.base", "druid", "Default prefix for metadata tables"),
+    DRUID_METADATA_DB_TYPE("hive.druid.metadata.db.type", "mysql",
+            new PatternSet("mysql", "postgresql"), "Type of the metadata database."
+    ),
+    DRUID_METADATA_DB_USERNAME("hive.druid.metadata.username", "",
+            "Username to connect to Type of the metadata DB."
+    ),
+    DRUID_METADATA_DB_PASSWORD("hive.druid.metadata.password", "",
+            "Password to connect to Type of the metadata DB."
+    ),
+    DRUID_METADATA_DB_URI("hive.druid.metadata.uri", "",
+            "URI to connect to the database (for example jdbc:mysql://hostname:port/DBName)."
+    ),
+    DRUID_WORKING_DIR("hive.druid.working.directory", "/tmp/workingDirectory",
+            "Default hdfs working directory used to store some intermediate metadata"
+    ),
+    HIVE_DRUID_MAX_TRIES("hive.druid.maxTries", 5, "Maximum number of retries before giving up"),
+    HIVE_DRUID_PASSIVE_WAIT_TIME("hive.druid.passiveWaitTimeMs", 30000,
+            "Wait time in ms default to 30 seconds."
+    ),
+    HIVE_DRUID_BITMAP_FACTORY_TYPE("hive.druid.bitmap.type", "roaring", new PatternSet("roaring", "concise"), "Coding algorithm use to encode the bitmaps"),
+    // For HBase storage handler
+    HIVE_HBASE_WAL_ENABLED("hive.hbase.wal.enabled", true,
+        "Whether writes to HBase should be forced to the write-ahead log. \n" +
+        "Disabling this improves HBase write performance at the risk of lost writes in case of a crash."),
+    HIVE_HBASE_GENERATE_HFILES("hive.hbase.generatehfiles", false,
+        "True when HBaseStorageHandler should generate hfiles instead of operate against the online table."),
+    HIVE_HBASE_SNAPSHOT_NAME("hive.hbase.snapshot.name", null, "The HBase table snapshot name to use."),
+    HIVE_HBASE_SNAPSHOT_RESTORE_DIR("hive.hbase.snapshot.restoredir", "/tmp", "The directory in which to " +
+        "restore the HBase table snapshot."),
+
+    // For har files
+    HIVEARCHIVEENABLED("hive.archive.enabled", false, "Whether archiving operations are permitted"),
+
+    HIVEOPTGBYUSINGINDEX("hive.optimize.index.groupby", false,
+        "Whether to enable optimization of group-by queries using Aggregate indexes."),
+
+    HIVEFETCHTASKCONVERSION("hive.fetch.task.conversion", "more", new StringSet("none", "minimal", "more"),
+        "Some select queries can be converted to single FETCH task minimizing latency.\n" +
+        "Currently the query should be single sourced not having any subquery and should not have\n" +
+        "any aggregations or distincts (which incurs RS), lateral views and joins.\n" +
+        "0. none : disable hive.fetch.task.conversion\n" +
+        "1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n" +
+        "2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)"
+    ),
+    HIVEFETCHTASKCONVERSIONTHRESHOLD("hive.fetch.task.conversion.threshold", 1073741824L,
+        "Input threshold for applying hive.fetch.task.conversion. If target table is native, input length\n" +
+        "is calculated by summation of file lengths. If it's not native, storage handler for the table\n" +
+        "can optionally implement org.apache.hadoop.hive.ql.metadata.InputEstimator interface."),
+
+    HIVEFETCHTASKAGGR("hive.fetch.task.aggr", false,
+        "Aggregation queries with no group-by clause (for example, select count(*) from src) execute\n" +
+        "final aggregations in single reduce task. If this is set true, Hive delegates final aggregation\n" +
+        "stage to fetch task, possibly decreasing the query time."),
+
+    HIVEOPTIMIZEMETADATAQUERIES("hive.compute.query.using.stats", true,
+        "When set to true Hive will answer a few queries like count(1) purely using stats\n" +
+        "stored in metastore. For basic stats collection turn on the config hive.stats.autogather to true.\n" +
+        "For more advanced stats collection need to run analyze table queries."),
+
+    // Serde for FetchTask
+    HIVEFETCHOUTPUTSERDE("hive.fetch.output.serde", "org.apache.hadoop.hive.serde2.DelimitedJSONSerDe",
+        "The SerDe used by FetchTask to serialize the fetch output."),
+
+    HIVEEXPREVALUATIONCACHE("hive.cache.expr.evaluation", true,
+        "If true, the evaluation result of a deterministic expression referenced twice or more\n" +
+        "will be cached.\n" +
+        "For example, in a filter condition like '.. where key + 10 = 100 or key + 10 = 0'\n" +
+        "the expression 'key + 10' will be evaluated/cached once and reused for the following\n" +
+        "expression ('key + 10 = 0'). Currently, this is applied only to expressions in select\n" +
+        "or filter operators."),
+
+    // Hive Variables
+    HIVEVARIABLESUBSTITUTE("hive.variable.substitute", true,
+        "This enables substitution using syntax like ${var} ${system:var} and ${env:var}."),
+    HIVEVARIABLESUBSTITUTEDEPTH("hive.variable.substitute.depth", 40,
+        "The maximum replacements the substitution engine will do."),
+
+    HIVECONFVALIDATION("hive.conf.validation", true,
+        "Enables type checking for registered Hive configurations"),
+
+    SEMANTIC_ANALYZER_HOOK("hive.semantic.analyzer.hook", "", ""),
+    HIVE_TEST_AUTHORIZATION_SQLSTD_HS2_MODE(
+        "hive.test.authz.sstd.hs2.mode", false, "test hs2 mode from .q tests", true),
+    HIVE_AUTHORIZATION_ENABLED("hive.security.authorization.enabled", false,
+        "enable or disable the Hive client authorization"),
+    HIVE_AUTHORIZATION_MANAGER("hive.security.authorization.manager",
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory",
+        "The Hive client authorization manager class name. The user defined authorization class should implement \n" +
+        "interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider."),
+    HIVE_AUTHENTICATOR_MANAGER("hive.security.authenticator.manager",
+        "org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator",
+        "hive client authenticator manager class name. The user defined authenticator should implement \n" +
+        "interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider."),
+    HIVE_METASTORE_AUTHORIZATION_MANAGER("hive.security.metastore.authorization.manager",
+        "org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider",
+        "Names of authorization manager classes (comma separated) to be used in the metastore\n" +
+        "for authorization. The user defined authorization class should implement interface\n" +
+        "org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.\n" +
+        "All authorization manager classes have to successfully authorize the metastore API\n" +
+        "call for the command execution to be allowed."),
+    HIVE_METASTORE_AUTHORIZATION_AUTH_READS("hive.security.metastore.authorization.auth.reads", true,
+        "If this is true, metastore authorizer authorizes read actions on database, table"),
+    HIVE_METASTORE_AUTHENTICATOR_MANAGER("hive.security.metastore.authenticator.manager",
+        "org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator",
+        "authenticator manager class name to be used in the metastore for authentication. \n" +
+        "The user defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider."),
+    HIVE_AUTHORIZATION_TABLE_USER_GRANTS("hive.security.authorization.createtable.user.grants", "",
+        "the privileges automatically granted to some users whenever a table gets created.\n" +
+        "An example like \"userX,userY:select;userZ:create\" will grant select privilege to userX and userY,\n" +
+        "and grant create privilege to userZ whenever a new table created."),
+    HIVE_AUTHORIZATION_TABLE_GROUP_GRANTS("hive.security.authorization.createtable.group.grants",
+        "",
+        "the privileges automatically granted to some groups whenever a table gets created.\n" +
+        "An example like \"groupX,groupY:select;groupZ:create\" will grant select privilege to groupX and groupY,\n" +
+        "and grant create privilege to groupZ whenever a new table created."),
+    HIVE_AUTHORIZATION_TABLE_ROLE_GRANTS("hive.security.authorization.createtable.role.grants", "",
+        "the privileges automatically granted to some roles whenever a table gets created.\n" +
+        "An example like \"roleX,roleY:select;roleZ:create\" will grant select privilege to roleX and roleY,\n" +
+        "and grant create privilege to roleZ whenever a new table created."),
+    HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS("hive.security.authorization.createtable.owner.grants",
+        "",
+        "The privileges automatically granted to the owner whenever a table gets created.\n" +
+        "An example like \"select,drop\" will grant select and drop privilege to the owner\n" +
+        "of the table. Note that the default gives the creator of a table no access to the\n" +
+        "table (but see HIVE-8067)."),
+    HIVE_AUTHORIZATION_TASK_FACTORY("hive.security.authorization.task.factory",
+        "org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl",
+        "Authorization DDL task factory implementation"),
+
+    // if this is not set default value is set during config initialization
+    // Default value can't be set in this constructor as it would refer names in other ConfVars
+    // whose constructor would not have been called
+    HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST(
+        "hive.security.authorization.sqlstd.confwhitelist", "",
+        "List of comma separated Java regexes. Configurations parameters that match these\n" +
+        "regexes can be modified by user when SQL standard authorization is enabled.\n" +
+        "To get the default value, use the 'set <param>' command.\n" +
+        "Note that the hive.conf.restricted.list checks are still enforced after the white list\n" +
+        "check"),
+
+    HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST_APPEND(
+        "hive.security.authorization.sqlstd.confwhitelist.append", "",
+        "List of comma separated Java regexes, to be appended to list set in\n" +
+        "hive.security.authorization.sqlstd.confwhitelist. Using this list instead\n" +
+        "of updating the original list means that you can append to the defaults\n" +
+        "set by SQL standard authorization instead of replacing it entirely."),
+
+    HIVE_CLI_PRINT_HEADER("hive.cli.print.header", false, "Whether to print the names of the columns in query output."),
+
+    HIVE_CLI_TEZ_SESSION_ASYNC("hive.cli.tez.session.async", true, "Whether to start Tez\n" +
+        "session in background when running CLI with Tez, allowing CLI to be available earlier."),
+
+    HIVE_ERROR_ON_EMPTY_PARTITION("hive.error.on.empty.partition", false,
+        "Whether to throw an exception if dynamic partition insert generates empty results."),
+
+    HIVE_INDEX_COMPACT_FILE("hive.index.compact.file", "", "internal variable"),
+    HIVE_INDEX_BLOCKFILTER_FILE("hive.index.blockfilter.file", "", "internal variable"),
+    HIVE_INDEX_IGNORE_HDFS_LOC("hive.index.compact.file.ignore.hdfs", false,
+        "When true the HDFS location stored in the index file will be ignored at runtime.\n" +
+        "If the data got moved or the name of the cluster got changed, the index data should still be usable."),
+
+    HIVE_EXIM_URI_SCHEME_WL("hive.exim.uri.scheme.whitelist", "hdfs,pfile,file,s3,s3a",
+        "A comma separated list of acceptable URI schemes for import and export."),
+    // temporary variable for testing. This is added just to turn off this feature in case of a bug in
+    // deployment. It has not been documented in hive-default.xml intentionally, this should be removed
+    // once the feature is stable
+    HIVE_EXIM_RESTRICT_IMPORTS_INTO_REPLICATED_TABLES("hive.exim.strict.repl.tables",true,
+        "Parameter that determines if 'regular' (non-replication) export dumps can be\n" +
+        "imported on to tables that are the target of replication. If this parameter is\n" +
+        "set, regular imports will check if the destination table(if it exists) has a " +
+        "'repl.last.id' set on it. If so, it will fail."),
+    HIVE_REPL_TASK_FACTORY("hive.repl.task.factory",
+        "org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory",
+        "Parameter that can be used to override which ReplicationTaskFactory will be\n" +
+        "used to instantiate ReplicationTask events. Override for third party repl plugins"),
+    HIVE_MAPPER_CANNOT_SPAN_MULTIPLE_PARTITIONS("hive.mapper.cannot.span.multiple.partitions", false, ""),
+    HIVE_REWORK_MAPREDWORK("hive.rework.mapredwork", false,
+        "should rework the mapred work or not.\n" +
+        "This is first introduced by SymlinkTextInputFormat to replace symlink files with real paths at compile time."),
+    HIVE_CONCATENATE_CHECK_INDEX ("hive.exec.concatenate.check.index", true,
+        "If this is set to true, Hive will throw error when doing\n" +
+        "'alter table tbl_name [partSpec] concatenate' on a table/partition\n" +
+        "that has indexes on it. The reason the user want to set this to true\n" +
+        "is because it can help user to avoid handling all index drop, recreation,\n" +
+        "rebuild work. This is very helpful for tables with thousands of partitions."),
+    HIVE_IO_EXCEPTION_HANDLERS("hive.io.exception.handlers", "",
+        "A list of io exception handler class names. This is used\n" +
+        "to construct a list exception handlers to handle exceptions thrown\n" +
+        "by record readers"),
+
+    // logging configuration
+    HIVE_LOG4J_FILE("hive.log4j.file", "",
+        "Hive log4j configuration file.\n" +
+        "If the property is not set, then logging will be initialized using hive-log4j2.properties found on the classpath.\n" +
+        "If the property is set, the value must be a valid URI (java.net.URI, e.g. \"file:///tmp/my-logging.xml\"), \n" +
+        "which you can then extract a URL from and pass to PropertyConfigurator.configure(URL)."),
+    HIVE_EXEC_LOG4J_FILE("hive.exec.log4j.file", "",
+        "Hive log4j configuration file for execution mode(sub command).\n" +
+        "If the property is not set, then logging will be initialized using hive-exec-log4j2.properties found on the classpath.\n" +
+        "If the property is set, the value must be a valid URI (java.net.URI, e.g. \"file:///tmp/my-logging.xml\"), \n" +
+        "which you can then extract a URL from and pass to PropertyConfigurator.configure(URL)."),
+    HIVE_ASYNC_LOG_ENABLED("hive.async.log.enabled", true,
+        "Whether to enable Log4j2's asynchronous logging. Asynchronous logging can give\n" +
+        " significant performance improvement as logging will be handled in separate thread\n" +
+        " that uses LMAX disruptor queue for buffering log messages.\n" +
+        " Refer https://logging.apache.org/log4j/2.x/manual/async.html for benefits and\n" +
+        " drawbacks."),
+
+    HIVE_LOG_EXPLAIN_OUTPUT("hive.log.explain.output", false,
+        "Whether to log explain output for every query.\n" +
+        "When enabled, will log EXPLAIN EXTENDED output for the query at INFO log4j log level."),
+    HIVE_EXPLAIN_USER("hive.explain.user", true,
+        "Whether to show explain result at user level.\n" +
+        "When enabled, will log EXPLAIN output for the query at user level. Tez only."),
+    HIVE_SPARK_EXPLAIN_USER("hive.spark.explain.user", false,
+        "Whether to show explain result at user level.\n" +
+        "When enabled, will log EXPLAIN output for the query at user level. Spark only."),
+
+    // prefix used to auto generated column aliases (this should be started with '_')
+    HIVE_AUTOGEN_COLUMNALIAS_PREFIX_LABEL("hive.autogen.columnalias.prefix.label", "_c",
+        "String used as a prefix when auto generating column alias.\n" +
+        "By default the prefix label will be appended with a column position number to form the column alias. \n" +
+        "Auto generation would happen if an aggregate function is used in a select clause without an explicit alias."),
+    HIVE_AUTOGEN_COLUMNALIAS_PREFIX_INCLUDEFUNCNAME(
+        "hive.autogen.columnalias.prefix.includefuncname", false,
+        "Whether to include function name in the column alias auto generated by Hive."),
+    HIVE_METRICS_CLASS("hive.service.metrics.class",
+        "org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics",
+        new StringSet(
+            "org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics",
+            "org.apache.hadoop.hive.common.metrics.LegacyMetrics"),
+        "Hive metrics subsystem implementation class."),
+    HIVE_CODAHALE_METRICS_REPORTER_CLASSES("hive.service.metrics.codahale.reporter.classes",
+        "org.apache.hadoop.hive.common.metrics.metrics2.JsonFileMetricsReporter, " +
+            "org.apache.hadoop.hive.common.metrics.metrics2.JmxMetricsReporter",
+            "Comma separated list of reporter implementation classes for metric class "
+                + "org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics. Overrides "
+                + "HIVE_METRICS_REPORTER conf if present"),
+    @Deprecated
+    HIVE_METRICS_REPORTER("hive.service.metrics.reporter", "",
+        "Reporter implementations for metric class "
+            + "org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics;" +
+        "Deprecated, use HIVE_CODAHALE_METRICS_REPORTER_CLASSES instead. This configuraiton will be"
+            + " overridden by HIVE_CODAHALE_METRICS_REPORTER_CLASSES if present. " +
+            "Comma separated list of JMX, CONSOLE, JSON_FILE, HADOOP2"),
+    HIVE_METRICS_JSON_FILE_LOCATION("hive.service.metrics.file.location", "/tmp/report.json",
+        "For metric class org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics JSON_FILE reporter, the location of local JSON metrics file.  " +
+        "This file will get overwritten at every interval."),
+    HIVE_METRICS_JSON_FILE_INTERVAL("hive.service.metrics.file.frequency", "5000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "For metric class org.apache.hadoop.hive.common.metrics.metrics2.JsonFileMetricsReporter, " +
+        "the frequency of updating JSON metrics file."),
+    HIVE_METRICS_HADOOP2_INTERVAL("hive.service.metrics.hadoop2.frequency", "30s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "For metric class org.apache.hadoop.hive.common.metrics.metrics2.Metrics2Reporter, " +
+        "the frequency of updating the HADOOP2 metrics system."),
+    HIVE_METRICS_HADOOP2_COMPONENT_NAME("hive.service.metrics.hadoop2.component",
+        "hive",
+        "Component name to provide to Hadoop2 Metrics system. Ideally 'hivemetastore' for the MetaStore " +
+        " and and 'hiveserver2' for HiveServer2."
+        ),
+    HIVE_PERF_LOGGER("hive.exec.perf.logger", "org.apache.hadoop.hive.ql.log.PerfLogger",
+        "The class responsible for logging client side performance metrics. \n" +
+        "Must be a subclass of org.apache.hadoop.hive.ql.log.PerfLogger"),
+    HIVE_START_CLEANUP_SCRATCHDIR("hive.start.cleanup.scratchdir", false,
+        "To cleanup the Hive scratchdir when starting the Hive Server"),
+    HIVE_SCRATCH_DIR_LOCK("hive.scratchdir.lock", false,
+        "To hold a lock file in scratchdir to prevent to be removed by cleardanglingscratchdir"),
+    HIVE_INSERT_INTO_MULTILEVEL_DIRS("hive.insert.into.multilevel.dirs", false,
+        "Where to insert into multilevel directories like\n" +
+        "\"insert directory '/HIVEFT25686/chinna/' from table\""),
+    HIVE_INSERT_INTO_EXTERNAL_TABLES("hive.insert.into.external.tables", true,
+        "whether insert into external tables is allowed"),
+    HIVE_TEMPORARY_TABLE_STORAGE(
+        "hive.exec.temporary.table.storage", "default", new StringSet("memory",
+         "ssd", "default"), "Define the storage policy for temporary tables." +
+         "Choices between memory, ssd and default"),
+    HIVE_QUERY_LIFETIME_HOOKS("hive.query.lifetime.hooks", "",
+        "A comma separated list of hooks which implement QueryLifeTimeHook. These will be triggered" +
+            " before/after query compilation and before/after query execution, in the order specified." +
+        "Implementations of QueryLifeTimeHookWithParseHooks can also be specified in this list. If they are" +
+        "specified then they will be invoked in the same places as QueryLifeTimeHooks and will be invoked during pre " +
+         "and post query parsing"),
+    HIVE_DRIVER_RUN_HOOKS("hive.exec.driver.run.hooks", "",
+        "A comma separated list of hooks which implement HiveDriverRunHook. Will be run at the beginning " +
+        "and end of Driver.run, these will be run in the order specified."),
+    HIVE_DDL_OUTPUT_FORMAT("hive.ddl.output.format", null,
+        "The data format to use for DDL output.  One of \"text\" (for human\n" +
+        "readable text) or \"json\" (for a json object)."),
+    HIVE_ENTITY_SEPARATOR("hive.entity.separator", "@",
+        "Separator used to construct names of tables and partitions. For example, dbname@tablename@partitionname"),
+    HIVE_CAPTURE_TRANSFORM_ENTITY("hive.entity.capture.transform", false,
+        "Compiler to capture transform URI referred in the query"),
+    HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY("hive.display.partition.cols.separately", true,
+        "In older Hive version (0.10 and earlier) no distinction was made between\n" +
+        "partition columns or non-partition columns while displaying columns in describe\n" +
+        "table. From 0.12 onwards, they are displayed separately. This flag will let you\n" +
+        "get old behavior, if desired. See, test-case in patch for HIVE-6689."),
+
+    HIVE_SSL_PROTOCOL_BLACKLIST("hive.ssl.protocol.blacklist", "SSLv2,SSLv3",
+        "SSL Versions to disable for all Hive Servers"),
+
+     // HiveServer2 specific configs
+    HIVE_SERVER2_CLEAR_DANGLING_SCRATCH_DIR("hive.server2.clear.dangling.scratchdir", false,
+        "Clear dangling scratch dir periodically in HS2"),
+    HIVE_SERVER2_CLEAR_DANGLING_SCRATCH_DIR_INTERVAL("hive.server2.clear.dangling.scratchdir.interval",
+        "1800s", new TimeValidator(TimeUnit.SECONDS),
+        "Interval to clear dangling scratch dir periodically in HS2"),
+    HIVE_SERVER2_SLEEP_INTERVAL_BETWEEN_START_ATTEMPTS("hive.server2.sleep.interval.between.start.attempts",
+        "60s", new TimeValidator(TimeUnit.MILLISECONDS, 0l, true, Long.MAX_VALUE, true),
+        "Amount of time to sleep between HiveServer2 start attempts. Primarily meant for tests"),
+    HIVE_SERVER2_MAX_START_ATTEMPTS("hive.server2.max.start.attempts", 30L, new RangeValidator(0L, null),
+        "Number of times HiveServer2 will attempt to start before exiting. The sleep interval between retries" +
+        " is determined by " + ConfVars.HIVE_SERVER2_SLEEP_INTERVAL_BETWEEN_START_ATTEMPTS.varname +
+        "\n The default of 30 will keep trying for 30 minutes."),
+    HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY("hive.server2.support.dynamic.service.discovery", false,
+        "Whether HiveServer2 supports dynamic service discovery for its clients. " +
+        "To support this, each instance of HiveServer2 currently uses ZooKeeper to register itself, " +
+        "when it is brought up. JDBC/ODBC clients should use the ZooKeeper ensemble: " +
+        "hive.zookeeper.quorum in their connection string."),
+    HIVE_SERVER2_ZOOKEEPER_NAMESPACE("hive.server2.zookeeper.namespace", "hiveserver2",
+        "The parent node in ZooKeeper used by HiveServer2 when supporting dynamic service discovery."),
+    HIVE_SERVER2_ZOOKEEPER_PUBLISH_CONFIGS("hive.server2.zookeeper.publish.configs", true,
+        "Whether we should publish HiveServer2's configs to ZooKeeper."),
+
+    // HiveServer2 global init file location
+    HIVE_SERVER2_GLOBAL_INIT_FILE_LOCATION("hive.server2.global.init.file.location", "${env:HIVE_CONF_DIR}",
+        "Either the location of a HS2 global init file or a directory containing a .hiverc file. If the \n" +
+        "property is set, the value must be a valid path to an init file or directory where the init file is located."),
+    HIVE_SERVER2_TRANSPORT_MODE("hive.server2.transport.mode", "binary", new StringSet("binary", "http"),
+        "Transport mode of HiveServer2."),
+    HIVE_SERVER2_THRIFT_BIND_HOST("hive.server2.thrift.bind.host", "",
+        "Bind host on which to run the HiveServer2 Thrift service."),
+    HIVE_SERVER2_PARALLEL_COMPILATION("hive.driver.parallel.compilation", false, "Whether to\n" +
+        "enable parallel compilation of the queries between sessions and within the same session on HiveServer2. The default is false."),
+    HIVE_SERVER2_COMPILE_LOCK_TIMEOUT("hive.server2.compile.lock.timeout", "0s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Number of seconds a request will wait to acquire the compile lock before giving up. " +
+        "Setting it to 0s disables the timeout."),
+    HIVE_SERVER2_PARALLEL_OPS_IN_SESSION("hive.server2.parallel.ops.in.session", true,
+        "Whether to allow several parallel operations (such as SQL statements) in one session."),
+
+    // HiveServer2 WebUI
+    HIVE_SERVER2_WEBUI_BIND_HOST("hive.server2.webui.host", "0.0.0.0", "The host address the HiveServer2 WebUI will listen on"),
+    HIVE_SERVER2_WEBUI_PORT("hive.server2.webui.port", 10002, "The port the HiveServer2 WebUI will listen on. This can be"
+        + "set to 0 or a negative integer to disable the web UI"),
+    HIVE_SERVER2_WEBUI_MAX_THREADS("hive.server2.webui.max.threads", 50, "The max HiveServer2 WebUI threads"),
+    HIVE_SERVER2_WEBUI_USE_SSL("hive.server2.webui.use.ssl", false,
+        "Set this to true for using SSL encryption for HiveServer2 WebUI."),
+    HIVE_SERVER2_WEBUI_SSL_KEYSTORE_PATH("hive.server2.webui.keystore.path", "",
+        "SSL certificate keystore location for HiveServer2 WebUI."),
+    HIVE_SERVER2_WEBUI_SSL_KEYSTORE_PASSWORD("hive.server2.webui.keystore.password", "",
+        "SSL certificate keystore password for HiveServer2 WebUI."),
+    HIVE_SERVER2_WEBUI_USE_SPNEGO("hive.server2.webui.use.spnego", false,
+        "If true, the HiveServer2 WebUI will be secured with SPNEGO. Clients must authenticate with Kerberos."),
+    HIVE_SERVER2_WEBUI_SPNEGO_KEYTAB("hive.server2.webui.spnego.keytab", "",
+        "The path to the Kerberos Keytab file containing the HiveServer2 WebUI SPNEGO service principal."),
+    HIVE_SERVER2_WEBUI_SPNEGO_PRINCIPAL("hive.server2.webui.spnego.principal",
+        "HTTP/_HOST@EXAMPLE.COM", "The HiveServer2 WebUI SPNEGO service principal.\n" +
+        "The special string _HOST will be replaced automatically with \n" +
+        "the value of hive.server2.webui.host or the correct host name."),
+    HIVE_SERVER2_WEBUI_MAX_HISTORIC_QUERIES("hive.server2.webui.max.historic.queries", 25,
+        "The maximum number of past queries to show in HiverSever2 WebUI."),
+
+    // Tez session settings
+    HIVE_SERVER2_TEZ_DEFAULT_QUEUES("hive.server2.tez.default.queues", "",
+        "A list of comma separated values corresponding to YARN queues of the same name.\n" +
+        "When HiveServer2 is launched in Tez mode, this configuration needs to be set\n" +
+        "for multiple Tez sessions to run in parallel on the cluster."),
+    HIVE_SERVER2_TEZ_SESSIONS_PER_DEFAULT_QUEUE("hive.server2.tez.sessions.per.default.queue", 1,
+        "A positive integer that determines the number of Tez sessions that should be\n" +
+        "launched on each of the queues specified by \"hive.server2.tez.default.queues\".\n" +
+        "Determines the parallelism on each queue."),
+    HIVE_SERVER2_TEZ_INITIALIZE_DEFAULT_SESSIONS("hive.server2.tez.initialize.default.sessions",
+        false,
+        "This flag is used in HiveServer2 to enable a user to use HiveServer2 without\n" +
+        "turning on Tez for HiveServer2. The user could potentially want to run queries\n" +
+        "over Tez without the pool of sessions."),
+    HIVE_SERVER2_TEZ_SESSION_LIFETIME("hive.server2.tez.session.lifetime", "162h",
+        new TimeValidator(TimeUnit.HOURS),
+        "The lifetime of the Tez sessions launched by HS2 when default sessions are enabled.\n" +
+        "Set to 0 to disable session expiration."),
+    HIVE_SERVER2_TEZ_SESSION_LIFETIME_JITTER("hive.server2.tez.session.lifetime.jitter", "3h",
+        new TimeValidator(TimeUnit.HOURS),
+        "The jitter for Tez session lifetime; prevents all the sessions from restarting at once."),
+    HIVE_SERVER2_TEZ_SESSION_MAX_INIT_THREADS("hive.server2.tez.sessions.init.threads", 16,
+        "If hive.server2.tez.initialize.default.sessions is enabled, the maximum number of\n" +
+        "threads to use to initialize the default sessions."),
+    HIVE_SERVER2_TEZ_SESSION_RESTRICTED_CONFIGS("hive.server2.tez.sessions.restricted.configs", "",
+    "The configuration settings that cannot be set when submitting jobs to HiveServer2. If\n" +
+    "any of these are set to values different from those in the server configuration, an\n" +
+    "exception will be thrown."),
+    HIVE_SERVER2_TEZ_SESSION_CUSTOM_QUEUE_ALLOWED("hive.server2.tez.sessions.custom.queue.allowed",
+      "true", new StringSet("true", "false", "ignore"),
+      "Whether Tez session pool should allow submitting queries to custom queues. The options\n" +
+      "are true, false (error out), ignore (accept the query but ignore the queue setting)."),
+
+    // Operation log configuration
+    HIVE_SERVER2_LOGGING_OPERATION_ENABLED("hive.server2.logging.operation.enabled", true,
+        "When true, HS2 will save operation logs and make them available for clients"),
+    HIVE_SERVER2_LOGGING_OPERATION_LOG_LOCATION("hive.server2.logging.operation.log.location",
+        "${system:java.io.tmpdir}" + File.separator + "${system:user.name}" + File.separator +
+            "operation_logs",
+        "Top level directory where operation logs are stored if logging functionality is enabled"),
+    HIVE_SERVER2_LOGGING_OPERATION_LEVEL("hive.server2.logging.operation.level", "EXECUTION",
+        new StringSet("NONE", "EXECUTION", "PERFORMANCE", "VERBOSE"),
+        "HS2 operation logging mode available to clients to be set at session level.\n" +
+        "For this to work, hive.server2.logging.operation.enabled should be set to true.\n" +
+        "  NONE: Ignore any logging\n" +
+        "  EXECUTION: Log completion of tasks\n" +
+        "  PERFORMANCE: Execution + Performance logs \n" +
+        "  VERBOSE: All logs" ),
+
+    // Enable metric collection for HiveServer2
+    HIVE_SERVER2_METRICS_ENABLED("hive.server2.metrics.enabled", false, "Enable metrics on the HiveServer2."),
+
+    // http (over thrift) transport settings
+    HIVE_SERVER2_THRIFT_HTTP_PORT("hive.server2.thrift.http.port", 10001,
+        "Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'http'."),
+    HIVE_SERVER2_THRIFT_HTTP_PATH("hive.server2.thrift.http.path", "cliservice",
+        "Path component of URL endpoint when in HTTP mode."),
+    HIVE_SERVER2_THRIFT_MAX_MESSAGE_SIZE("hive.server2.thrift.max.message.size", 100*1024*1024,
+        "Maximum message size in bytes a HS2 server will accept."),
+    HIVE_SERVER2_THRIFT_HTTP_MAX_IDLE_TIME("hive.server2.thrift.http.max.idle.time", "1800s",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "Maximum idle time for a connection on the server when in HTTP mode."),
+    HIVE_SERVER2_THRIFT_HTTP_WORKER_KEEPALIVE_TIME("hive.server2.thrift.http.worker.keepalive.time", "60s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Keepalive time for an idle http worker thread. When the number of workers exceeds min workers, " +
+        "excessive threads are killed after this time interval."),
+    HIVE_SERVER2_THRIFT_HTTP_REQUEST_HEADER_SIZE("hive.server2.thrift.http.request.header.size", 6*1024,
+        "Request header size in bytes, when using HTTP transport mode. Jetty defaults used."),
+    HIVE_SERVER2_THRIFT_HTTP_RESPONSE_HEADER_SIZE("hive.server2.thrift.http.response.header.size", 6*1024,
+        "Response header size in bytes, when using HTTP transport mode. Jetty defaults used."),
+
+    // Cookie based authentication when using HTTP Transport
+    HIVE_SERVER2_THRIFT_HTTP_COOKIE_AUTH_ENABLED("hive.server2.thrift.http.cookie.auth.enabled", true,
+        "When true, HiveServer2 in HTTP transport mode, will use cookie based authentication mechanism."),
+    HIVE_SERVER2_THRIFT_HTTP_COOKIE_MAX_AGE("hive.server2.thrift.http.cookie.max.age", "86400s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Maximum age in seconds for server side cookie used by HS2 in HTTP mode."),
+    HIVE_SERVER2_THRIFT_HTTP_COOKIE_DOMAIN("hive.server2.thrift.http.cookie.domain", null,
+        "Domain for the HS2 generated cookies"),
+    HIVE_SERVER2_THRIFT_HTTP_COOKIE_PATH("hive.server2.thrift.http.cookie.path", null,
+        "Path for the HS2 generated cookies"),
+    @Deprecated
+    HIVE_SERVER2_THRIFT_HTTP_COOKIE_IS_SECURE("hive.server2.thrift.http.cookie.is.secure", true,
+        "Deprecated: Secure attribute of the HS2 generated cookie (this is automatically enabled for SSL enabled HiveServer2)."),
+    HIVE_SERVER2_THRIFT_HTTP_COOKIE_IS_HTTPONLY("hive.server2.thrift.http.cookie.is.httponly", true,
+        "HttpOnly attribute of the HS2 generated cookie."),
+
+    // binary transport settings
+    HIVE_SERVER2_THRIFT_PORT("hive.server2.thrift.port", 10000,
+        "Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'binary'."),
+    HIVE_SERVER2_THRIFT_SASL_QOP("hive.server2.thrift.sasl.qop", "auth",
+        new StringSet("auth", "auth-int", "auth-conf"),
+        "Sasl QOP value; set it to one of following values to enable higher levels of\n" +
+        "protection for HiveServer2 communication with clients.\n" +
+        "Setting hadoop.rpc.protection to a higher level than HiveServer2 does not\n" +
+        "make sense in most situations. HiveServer2 ignores hadoop.rpc.protection in favor\n" +
+        "of hive.server2.thrift.sasl.qop.\n" +
+        "  \"auth\" - authentication only (default)\n" +
+        "  \"auth-int\" - authentication plus integrity protection\n" +
+        "  \"auth-conf\" - authentication plus integrity and confidentiality protection\n" +
+        "This is applicable only if HiveServer2 is configured to use Kerberos authentication."),
+    HIVE_SERVER2_THRIFT_MIN_WORKER_THREADS("hive.server2.thrift.min.worker.threads", 5,
+        "Minimum number of Thrift worker threads"),
+    HIVE_SERVER2_THRIFT_MAX_WORKER_THREADS("hive.server2.thrift.max.worker.threads", 500,
+        "Maximum number of Thrift worker threads"),
+    HIVE_SERVER2_THRIFT_LOGIN_BEBACKOFF_SLOT_LENGTH(
+        "hive.server2.thrift.exponential.backoff.slot.length", "100ms",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "Binary exponential backoff slot time for Thrift clients during login to HiveServer2,\n" +
+        "for retries until hitting Thrift client timeout"),
+    HIVE_SERVER2_THRIFT_LOGIN_TIMEOUT("hive.server2.thrift.login.timeout", "20s",
+        new TimeValidator(TimeUnit.SECONDS), "Timeout for Thrift clients during login to HiveServer2"),
+    HIVE_SERVER2_THRIFT_WORKER_KEEPALIVE_TIME("hive.server2.thrift.worker.keepalive.time", "60s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Keepalive time (in seconds) for an idle worker thread. When the number of workers exceeds min workers, " +
+        "excessive threads are killed after this time interval."),
+
+    // Configuration for async thread pool in SessionManager
+    HIVE_SERVER2_ASYNC_EXEC_THREADS("hive.server2.async.exec.threads", 100,
+        "Number of threads in the async thread pool for HiveServer2"),
+    HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT("hive.server2.async.exec.shutdown.timeout", "10s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "How long HiveServer2 shutdown will wait for async threads to terminate."),
+    HIVE_SERVER2_ASYNC_EXEC_WAIT_QUEUE_SIZE("hive.server2.async.exec.wait.queue.size", 100,
+        "Size of the wait queue for async thread pool in HiveServer2.\n" +
+        "After hitting this limit, the async thread pool will reject new requests."),
+    HIVE_SERVER2_ASYNC_EXEC_KEEPALIVE_TIME("hive.server2.async.exec.keepalive.time", "10s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Time that an idle HiveServer2 async thread (from the thread pool) will wait for a new task\n" +
+        "to arrive before terminating"),
+    HIVE_SERVER2_ASYNC_EXEC_ASYNC_COMPILE("hive.server2.async.exec.async.compile", false,
+        "Whether to enable compiling async query asynchronously. If enabled, it is unknown if the query will have any resultset before compilation completed."),
+    HIVE_SERVER2_LONG_POLLING_TIMEOUT("hive.server2.long.polling.timeout", "5000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "Time that HiveServer2 will wait before responding to asynchronous calls that use long polling"),
+
+    HIVE_SESSION_IMPL_CLASSNAME("hive.session.impl.classname", null, "Classname for custom implementation of hive session"),
+    HIVE_SESSION_IMPL_WITH_UGI_CLASSNAME("hive.session.impl.withugi.classname", null, "Classname for custom implementation of hive session with UGI"),
+
+    // HiveServer2 auth configuration
+    HIVE_SERVER2_AUTHENTICATION("hive.server2.authentication", "NONE",
+      new StringSet("NOSASL", "NONE", "LDAP", "KERBEROS", "PAM", "CUSTOM"),
+        "Client authentication types.\n" +
+        "  NONE: no authentication check\n" +
+        "  LDAP: LDAP/AD based authentication\n" +
+        "  KERBEROS: Kerberos/GSSAPI authentication\n" +
+        "  CUSTOM: Custom authentication provider\n" +
+        "          (Use with property hive.server2.custom.authentication.class)\n" +
+        "  PAM: Pluggable authentication module\n" +
+        "  NOSASL:  Raw transport"),
+    HIVE_SERVER2_ALLOW_USER_SUBSTITUTION("hive.server2.allow.user.substitution", true,
+        "Allow alternate user to be specified as part of HiveServer2 open connection request."),
+    HIVE_SERVER2_KERBEROS_KEYTAB("hive.server2.authentication.kerberos.keytab", "",
+        "Kerberos keytab file for server principal"),
+    HIVE_SERVER2_KERBEROS_PRINCIPAL("hive.server2.authentication.kerberos.principal", "",
+        "Kerberos server principal"),
+    HIVE_SERVER2_SPNEGO_KEYTAB("hive.server2.authentication.spnego.keytab", "",
+        "keytab file for SPNego principal, optional,\n" +
+        "typical value would look like /etc/security/keytabs/spnego.service.keytab,\n" +
+        "This keytab would be used by HiveServer2 when Kerberos security is enabled and \n" +
+        "HTTP transport mode is used.\n" +
+        "This needs to be set only if SPNEGO is to be used in authentication.\n" +
+        "SPNego authentication would be honored only if valid\n" +
+        "  hive.server2.authentication.spnego.principal\n" +
+        "and\n" +
+        "  hive.server2.authentication.spnego.keytab\n" +
+        "are specified."),
+    HIVE_SERVER2_SPNEGO_PRINCIPAL("hive.server2.authentication.spnego.principal", "",
+        "SPNego service principal, optional,\n" +
+        "typical value would look like HTTP/_HOST@EXAMPLE.COM\n" +
+        "SPNego service principal would be used by HiveServer2 when Kerberos security is enabled\n" +
+        "and HTTP transport mode is used.\n" +
+        "This needs to be set only if SPNEGO is to be used in authentication."),
+    HIVE_SERVER2_PLAIN_LDAP_URL("hive.server2.authentication.ldap.url", null,
+        "LDAP connection URL(s),\n" +
+         "this value could contain URLs to mutiple LDAP servers instances for HA,\n" +
+         "each LDAP URL is separated by a SPACE character. URLs are used in the \n" +
+         " order specified until a connection is successful."),
+    HIVE_SERVER2_PLAIN_LDAP_BASEDN("hive.server2.authentication.ldap.baseDN", null, "LDAP base DN"),
+    HIVE_SERVER2_PLAIN_LDAP_DOMAIN("hive.server2.authentication.ldap.Domain", null, ""),
+    HIVE_SERVER2_PLAIN_LDAP_GROUPDNPATTERN("hive.server2.authentication.ldap.groupDNPattern", null,
+        "COLON-separated list of patterns to use to find DNs for group entities in this directory.\n" +
+        "Use %s where the actual group name is to be substituted for.\n" +
+        "For example: CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com."),
+    HIVE_SERVER2_PLAIN_LDAP_GROUPFILTER("hive.server2.authentication.ldap.groupFilter", null,
+        "COMMA-separated list of LDAP Group names (short name not full DNs).\n" +
+        "For example: HiveAdmins,HadoopAdmins,Administrators"),
+    HIVE_SERVER2_PLAIN_LDAP_USERDNPATTERN("hive.server2.authentication.ldap.userDNPattern", null,
+        "COLON-separated list of patterns to use to find DNs for users in this directory.\n" +
+        "Use %s where the actual group name is to be substituted for.\n" +
+        "For example: CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com."),
+    HIVE_SERVER2_PLAIN_LDAP_USERFILTER("hive.server2.authentication.ldap.userFilter", null,
+        "COMMA-separated list of LDAP usernames (just short names, not full DNs).\n" +
+        "For example: hiveuser,impalauser,hiveadmin,hadoopadmin"),
+    HIVE_SERVER2_PLAIN_LDAP_GUIDKEY("hive.server2.authentication.ldap.guidKey", "uid",
+        "LDAP attribute name whose values are unique in this LDAP server.\n" +
+        "For example: uid or CN."),
+    HIVE_SERVER2_PLAIN_LDAP_GROUPMEMBERSHIP_KEY("hive.server2.authentication.ldap.groupMembershipKey", "member",
+        "LDAP attribute name on the group object that contains the list of distinguished names\n" +
+        "for the user, group, and contact objects that are members of the group.\n" +
+        "For example: member, uniqueMember or memberUid"),
+    HIVE_SERVER2_PLAIN_LDAP_USERMEMBERSHIP_KEY(HIVE_SERVER2_AUTHENTICATION_LDAP_USERMEMBERSHIPKEY_NAME, null,
+        "LDAP attribute name on the user object that contains groups of which the user is\n" +
+        "a direct member, except for the primary group, which is represented by the\n" +
+        "primaryGroupId.\n" +
+        "For example: memberOf"),
+    HIVE_SERVER2_PLAIN_LDAP_GROUPCLASS_KEY("hive.server2.authentication.ldap.groupClassKey", "groupOfNames",
+        "LDAP attribute name on the group entry that is to be used in LDAP group searches.\n" +
+        "For example: group, groupOfNames or groupOfUniqueNames."),
+    HIVE_SERVER2_PLAIN_LDAP_CUSTOMLDAPQUERY("hive.server2.authentication.ldap.customLDAPQuery", null,
+        "A full LDAP query that LDAP Atn provider uses to execute against LDAP Server.\n" +
+        "If this query returns a null resultset, the LDAP Provider fails the Authentication\n" +
+        "request, succeeds if the user is part of the resultset." +
+        "For example: (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) \n" +
+        "(&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com)" +
+        "(memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com))))"),
+    HIVE_SERVER2_CUSTOM_AUTHENTICATION_CLASS("hive.server2.custom.authentication.class", null,
+        "Custom authentication class. Used when property\n" +
+        "'hive.server2.authentication' is set to 'CUSTOM'. Provided class\n" +
+        "must be a proper implementation of the interface\n" +
+        "org.apache.hive.service.auth.PasswdAuthenticationProvider. HiveServer2\n" +
+        "will call its Authenticate(user, passed) method to authenticate requests.\n" +
+        "The implementation may optionally implement Hadoop's\n" +
+        "org.apache.hadoop.conf.Configurable class to grab Hive's Configuration object."),
+    HIVE_SERVER2_PAM_SERVICES("hive.server2.authentication.pam.services", null,
+      "List of the underlying pam services that should be used when auth type is PAM\n" +
+      "A file with the same name must exist in /etc/pam.d"),
+
+    HIVE_SERVER2_ENABLE_DOAS("hive.server2.enable.doAs", true,
+        "Setting this property to true will have HiveServer2 execute\n" +
+        "Hive operations as the user making the calls to it."),
+    HIVE_DISTCP_DOAS_USER("hive.distcp.privileged.doAs","hdfs",
+        "This property allows privileged distcp executions done by hive\n" +
+        "to run as this user. Typically, it should be the user you\n" +
+        "run the namenode as, such as the 'hdfs' user."),
+    HIVE_SERVER2_TABLE_TYPE_MAPPING("hive.server2.table.type.mapping", "CLASSIC", new StringSet("CLASSIC", "HIVE"),
+        "This setting reflects how HiveServer2 will report the table types for JDBC and other\n" +
+        "client implementations that retrieve the available tables and supported table types\n" +
+        "  HIVE : Exposes Hive's native table types like MANAGED_TABLE, EXTERNAL_TABLE, VIRTUAL_VIEW\n" +
+        "  CLASSIC : More generic types like TABLE and VIEW"),
+    HIVE_SERVER2_SESSION_HOOK("hive.server2.session.hook", "", ""),
+
+    // SSL settings
+    HIVE_SERVER2_USE_SSL("hive.server2.use.SSL", false,
+        "Set this to true for using SSL encryption in HiveServer2."),
+    HIVE_SERVER2_SSL_KEYSTORE_PATH("hive.server2.keystore.path", "",
+        "SSL certificate keystore location."),
+    HIVE_SERVER2_SSL_KEYSTORE_PASSWORD("hive.server2.keystore.password", "",
+        "SSL certificate keystore password."),
+    HIVE_SERVER2_MAP_FAIR_SCHEDULER_QUEUE("hive.server2.map.fair.scheduler.queue", true,
+        "If the YARN fair scheduler is configured and HiveServer2 is running in non-impersonation mode,\n" +
+        "this setting determines the user for fair scheduler queue mapping.\n" +
+        "If set to true (default), the logged-in user determines the fair scheduler queue\n" +
+        "for submitted jobs, so that map reduce resource usage can be tracked by user.\n" +
+        "If set to false, all Hive jobs go to the 'hive' user's queue."),
+    HIVE_SERVER2_BUILTIN_UDF_WHITELIST("hive.server2.builtin.udf.whitelist", "",
+        "Comma separated list of builtin udf names allowed in queries.\n" +
+        "An empty whitelist allows all builtin udfs to be executed. " +
+        " The udf black list takes precedence over udf white list"),
+    HIVE_SERVER2_BUILTIN_UDF_BLACKLIST("hive.server2.builtin.udf.blacklist", "",
+         "Comma separated list of udfs names. These udfs will not be allowed in queries." +
+         " The udf black list takes precedence over udf white list"),
+     HIVE_ALLOW_UDF_LOAD_ON_DEMAND("hive.allow.udf.load.on.demand", false,
+         "Whether enable loading UDFs from metastore on demand; this is mostly relevant for\n" +
+         "HS2 and was the default behavior before Hive 1.2. Off by default."),
+
+    HIVE_SERVER2_SESSION_CHECK_INTERVAL("hive.server2.session.check.interval", "6h",
+        new TimeValidator(TimeUnit.MILLISECONDS, 3000l, true, null, false),
+        "The check interval for session/operation timeout, which can be disabled by setting to zero or negative value."),
+    HIVE_SERVER2_CLOSE_SESSION_ON_DISCONNECT("hive.server2.close.session.on.disconnect", true,
+      "Session will be closed when connection is closed. Set this to false to have session outlive its parent connection."),
+    HIVE_SERVER2_IDLE_SESSION_TIMEOUT("hive.server2.idle.session.timeout", "7d",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "Session will be closed when it's not accessed for this duration, which can be disabled by setting to zero or negative value."),
+    HIVE_SERVER2_IDLE_OPERATION_TIMEOUT("hive.server2.idle.operation.timeout", "5d",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "Operation will be closed when it's not accessed for this duration of time, which can be disabled by setting to zero value.\n" +
+        "  With positive value, it's checked for operations in terminal state only (FINISHED, CANCELED, CLOSED, ERROR).\n" +
+        "  With negative value, it's checked for all of the operations regardless of state."),
+    HIVE_SERVER2_IDLE_SESSION_CHECK_OPERATION("hive.server2.idle.session.check.operation", true,
+        "Session will be considered to be idle only if there is no activity, and there is no pending operation.\n" +
+        " This setting takes effect only if session idle timeout (hive.server2.idle.session.timeout) and checking\n" +
+        "(hive.server2.session.check.interval) are enabled."),
+    HIVE_SERVER2_THRIFT_CLIENT_RETRY_LIMIT("hive.server2.thrift.client.retry.limit", 1,"Number of retries upon " +
+      "failure of Thrift HiveServer2 calls"),
+    HIVE_SERVER2_THRIFT_CLIENT_CONNECTION_RETRY_LIMIT("hive.server2.thrift.client.connect.retry.limit", 1,"Number of " +
+      "retries while opening a connection to HiveServe2"),
+    HIVE_SERVER2_THRIFT_CLIENT_RETRY_DELAY_SECONDS("hive.server2.thrift.client.retry.delay.seconds", "1s",
+      new TimeValidator(TimeUnit.SECONDS), "Number of seconds for the HiveServer2 thrift client to wait between " +
+      "consecutive connection attempts. Also specifies the time to wait between retrying thrift calls upon failures"),
+    HIVE_SERVER2_THRIFT_CLIENT_USER("hive.server2.thrift.client.user", "anonymous","Username to use against thrift" +
+      " client"),
+    HIVE_SERVER2_THRIFT_CLIENT_PASSWORD("hive.server2.thrift.client.password", "anonymous","Password to use against " +
+      "thrift client"),
+
+    // ResultSet serialization settings
+    HIVE_SERVER2_THRIFT_RESULTSET_SERIALIZE_IN_TASKS("hive.server2.thrift.resultset.serialize.in.tasks", false,
+      "Whether we should serialize the Thrift structures used in JDBC ResultSet RPC in task nodes.\n " +
+      "We use SequenceFile and ThriftJDBCBinarySerDe to read and write the final results if this is true."),
+    // TODO: Make use of this config to configure fetch size
+    HIVE_SERVER2_THRIFT_RESULTSET_MAX_FETCH_SIZE("hive.server2.thrift.resultset.max.fetch.size",
+        10000, "Max number of rows sent in one Fetch RPC call by the server to the client."),
+    HIVE_SERVER2_THRIFT_RESULTSET_DEFAULT_FETCH_SIZE("hive.server2.thrift.resultset.default.fetch.size", 1000,
+        "The number of rows sent in one Fetch RPC call by the server to the client, if not\n" +
+        "specified by the client."),
+    HIVE_SERVER2_XSRF_FILTER_ENABLED("hive.server2.xsrf.filter.enabled",false,
+        "If enabled, HiveServer2 will block any requests made to it over http " +
+        "if an X-XSRF-HEADER header is not present"),
+    HIVE_SECURITY_COMMAND_WHITELIST("hive.security.command.whitelist", "set,reset,dfs,add,list,delete,reload,compile",
+        "Comma separated list of non-SQL Hive commands users are authorized to execute"),
+    HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH("hive.server2.job.credential.provider.path", "",
+        "If set, this configuration property should provide a comma-separated list of URLs that indicates the type and " +
+        "location of providers to be used by hadoop credential provider API. It provides HiveServer2 the ability to provide job-specific " +
+        "credential providers for jobs run using MR and Spark execution engines. This functionality has not been tested against Tez."),
+    HIVE_MOVE_FILES_THREAD_COUNT("hive.mv.files.thread", 15, new  SizeValidator(0L, true, 1024L, true), "Number of threads"
+         + " used to move files in move task. Set it to 0 to disable multi-threaded file moves. This parameter is also used by"
+         + " MSCK to check tables."),
+    HIVE_LOAD_DYNAMIC_PARTITIONS_THREAD_COUNT("hive.load.dynamic.partitions.thread", 15,
+        new  SizeValidator(1L, true, 1024L, true),
+        "Number of threads used to load dynamic partitions."),
+    // If this is set all move tasks at the end of a multi-insert query will only begin once all
+    // outputs are ready
+    HIVE_MULTI_INSERT_MOVE_TASKS_SHARE_DEPENDENCIES(
+        "hive.multi.insert.move.tasks.share.dependencies", false,
+        "If this is set all move tasks for tables/partitions (not directories) at the end of a\n" +
+        "multi-insert query will only begin once the dependencies for all these move tasks have been\n" +
+        "met.\n" +
+        "Advantages: If concurrency is enabled, the locks will only be released once the query has\n" +
+        "            finished, so with this config enabled, the time when the table/partition is\n" +
+        "            generated will be much closer to when the lock on it is released.\n" +
+        "Disadvantages: If concurrency is not enabled, with this disabled, the tables/partitions which\n" +
+        "               are produced by this query and finish earlier will be available for querying\n" +
+        "               much earlier.  Since the locks are only released once the query finishes, this\n" +
+        "               does not apply if concurrency is enabled."),
+
+    HIVE_INFER_BUCKET_SORT("hive.exec.infer.bucket.sort", false,
+        "If this is set, when writing partitions, the metadata will include the bucketing/sorting\n" +
+        "properties with which the data was written if any (this will not overwrite the metadata\n" +
+        "inherited from the table if the table is bucketed/sorted)"),
+
+    HIVE_INFER_BUCKET_SORT_NUM_BUCKETS_POWER_TWO(
+        "hive.exec.infer.bucket.sort.num.buckets.power.two", false,
+        "If this is set, when setting the number of reducers for the map reduce task which writes the\n" +
+        "final output files, it will choose a number which is a power of two, unless the user specifies\n" +
+        "the number of reducers to use using mapred.reduce.tasks.  The number of reducers\n" +
+        "may be set to a power of two, only to be followed by a merge task meaning preventing\n" +
+        "anything from being inferred.\n" +
+        "With hive.exec.infer.bucket.sort set to true:\n" +
+        "Advantages:  If this is not set, the number of buckets for partitions will seem arbitrary,\n" +
+        "             which means that the number of mappers used for optimized joins, for example, will\n" +
+        "             be very low.  With this set, since the number of buckets used for any partition is\n" +
+        "             a power of two, the number of mappers used for optimized joins will be the least\n" +
+        "             number of buckets used by any partition being joined.\n" +
+        "Disadvantages: This may mean a much larger or much smaller number of reducers being used in the\n" +
+        "               final map reduce job, e.g. if a job was originally going to take 257 reducers,\n" +
+        "               it will now take 512 reducers, similarly if the max number of reducers is 511,\n" +
+        "               and a job was going to use this many, it will now use 256 reducers."),
+
+    HIVEOPTLISTBUCKETING("hive.optimize.listbucketing", false,
+        "Enable list bucketing optimizer. Default value is false so that we disable it by default."),
+
+    // Allow TCP Keep alive socket option for for HiveServer or a maximum timeout for the socket.
+    SERVER_READ_SOCKET_TIMEOUT("hive.server.read.socket.timeout", "10s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Timeout for the HiveServer to close the connection if no response from the client. By default, 10 seconds."),
+    SERVER_TCP_KEEP_ALIVE("hive.server.tcp.keepalive", true,
+        "Whether to enable TCP keepalive for the Hive Server. Keepalive will prevent accumulation of half-open connections."),
+
+    HIVE_DECODE_PARTITION_NAME("hive.decode.partition.name", false,
+        "Whether to show the unquoted partition names in query results."),
+
+    HIVE_EXECUTION_ENGINE("hive.execution.engine", "mr", new StringSet("mr", "tez", "spark"),
+        "Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR\n" +
+        "remains the default engine for historical reasons, it is itself a historical engine\n" +
+        "and is deprecated in Hive 2 line. It may be removed without further warning."),
+
+    HIVE_EXECUTION_MODE("hive.execution.mode", "container", new StringSet("container", "llap"),
+        "Chooses whether query fragments will run in container or in llap"),
+
+    HIVE_JAR_DIRECTORY("hive.jar.directory", null,
+        "This is the location hive in tez mode will look for to find a site wide \n" +
+        "installed hive instance."),
+    HIVE_USER_INSTALL_DIR("hive.user.install.directory", "/user/",
+        "If hive (in tez mode only) cannot find a usable hive jar in \"hive.jar.directory\", \n" +
+        "it will upload the hive jar to \"hive.user.install.directory/user.name\"\n" +
+        "and use it to run queries."),
+
+    // Vectorization enabled
+    HIVE_VECTORIZATION_ENABLED("hive.vectorized.execution.enabled", false,
+        "This flag should be set to true to enable vectorized mode of query execution.\n" +
+        "The default value is false."),
+    HIVE_VECTORIZATION_REDUCE_ENABLED("hive.vectorized.execution.reduce.enabled", true,
+        "This flag should be set to true to enable vectorized mode of the reduce-side of query execution.\n" +
+        "The default value is true."),
+    HIVE_VECTORIZATION_REDUCE_GROUPBY_ENABLED("hive.vectorized.execution.reduce.groupby.enabled", true,
+        "This flag should be set to true to enable vectorized mode of the reduce-side GROUP BY query execution.\n" +
+        "The default value is true."),
+    HIVE_VECTORIZATION_MAPJOIN_NATIVE_ENABLED("hive.vectorized.execution.mapjoin.native.enabled", true,
+         "This flag should be set to true to enable native (i.e. non-pass through) vectorization\n" +
+         "of queries using MapJoin.\n" +
+         "The default value is true."),
+    HIVE_VECTORIZATION_MAPJOIN_NATIVE_MULTIKEY_ONLY_ENABLED("hive.vectorized.execution.mapjoin.native.multikey.only.enabled", false,
+         "This flag should be set to true to restrict use of native vector map join hash tables to\n" +
+         "the MultiKey in queries using MapJoin.\n" +
+         "The default value is false."),
+    HIVE_VECTORIZATION_MAPJOIN_NATIVE_MINMAX_ENABLED("hive.vectorized.execution.mapjoin.minmax.enabled", false,
+         "This flag should be set to true to enable vector map join hash tables to\n" +
+         "use max / max filtering for integer join queries using MapJoin.\n" +
+         "The default value is false."),
+    HIVE_VECTORIZATION_MAPJOIN_NATIVE_OVERFLOW_REPEATED_THRESHOLD("hive.vectorized.execution.mapjoin.overflow.repeated.threshold", -1,
+         "The number of small table rows for a match in vector map join hash tables\n" +
+         "where we use the repeated field optimization in overflow vectorized row batch for join queries using MapJoin.\n" +
+         "A value of -1 means do use the join result optimization.  Otherwise, threshold value can be 0 to maximum integer."),
+    HIVE_VECTORIZATION_MAPJOIN_NATIVE_FAST_HASHTABLE_ENABLED("hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled", false,
+         "This flag should be set to true to enable use of native fast vector map join hash tables in\n" +
+         "queries using MapJoin.\n" +
+         "The default value is false."),
+    HIVE_VECTORIZATION_GROUPBY_CHECKINTERVAL("hive.vectorized.groupby.checkinterval", 100000,
+        "Number of entries added to the group by aggregation hash before a recomputation of average entry size is performed."),
+    HIVE_VECTORIZATION_GROUPBY_MAXENTRIES("hive.vectorized.groupby.maxentries", 1000000,
+        "Max number of entries in the vector group by aggregation hashtables. \n" +
+        "Exceeding this will trigger a flush irrelevant of memory pressure condition."),
+    HIVE_VECTORIZATION_GROUPBY_FLUSH_PERCENT("hive.vectorized.groupby.flush.percent", (float) 0.1,
+        "Percent of entries in the group by aggregation hash flushed when the memory threshold is exceeded."),
+    HIVE_VECTORIZATION_REDUCESINK_NEW_ENABLED("hive.vectorized.execution.reducesink.new.enabled", true,
+        "This flag should be set to true to enable the new vectorization\n" +
+        "of queries using ReduceSink.\ni" +
+        "The default value is true."),
+    HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT("hive.vectorized.use.vectorized.input.format", true,
+        "This flag should be set to true to enable vectorizing with vectorized input file format capable SerDe.\n" +
+        "The default value is true."),
+    HIVE_VECTORIZATION_USE_VECTOR_DESERIALIZE("hive.vectorized.use.vector.serde.deserialize", true,
+        "This flag should be set to true to enable vectorizing rows using vector deserialize.\n" +
+        "The default value is true."),
+    HIVE_VECTORIZATION_USE_ROW_DESERIALIZE("hive.vectorized.use.row.serde.deserialize", false,
+        "This flag should be set to true to enable vectorizing using row deserialize.\n" +
+        "The default value is false."),
+    HIVE_VECTOR_ADAPTOR_USAGE_MODE("hive.vectorized.adaptor.usage.mode", "all", new StringSet("none", "chosen", "all"),
+        "Specifies the extent to which the VectorUDFAdaptor will be used for UDFs that do not have a cooresponding vectorized class.\n" +
+        "0. none   : disable any usage of VectorUDFAdaptor\n" +
+        "1. chosen : use VectorUDFAdaptor for a small set of UDFs that were choosen for good performance\n" +
+        "2. all    : use VectorUDFAdaptor for all UDFs"
+    ),
+
+    HIVE_TYPE_CHECK_ON_INSERT("hive.typecheck.on.insert", true, "This property has been extended to control "
+        + "whether to check, convert, and normalize partition value to conform to its column type in "
+        + "partition operations including but not limited to insert, such as alter, describe etc."),
+
+    HIVE_HADOOP_CLASSPATH("hive.hadoop.classpath", null,
+        "For Windows OS, we need to pass HIVE_HADOOP_CLASSPATH Java parameter while starting HiveServer2 \n" +
+        "using \"-hiveconf hive.hadoop.classpath=%HIVE_LIB%\"."),
+
+    HIVE_RPC_QUERY_PLAN("hive.rpc.query.plan", false,
+        "Whether to send the query plan via local resource or RPC"),
+    HIVE_AM_SPLIT_GENERATION("hive.compute.splits.in.am", true,
+        "Whether to generate the splits locally or in the AM (tez only)"),
+    HIVE_TEZ_GENERATE_CONSISTENT_SPLITS("hive.tez.input.generate.consistent.splits", true,
+        "Whether to generate consistent split locations when generating splits in the AM"),
+    HIVE_PREWARM_ENABLED("hive.prewarm.enabled", false, "Enables container prewarm for Tez/Spark (Hadoop 2 only)"),
+    HIVE_PREWARM_NUM_CONTAINERS("hive.prewarm.numcontainers", 10, "Controls the number of containers to prewarm for Tez/Spark (Hadoop 2 only)"),
+
+    HIVESTAGEIDREARRANGE("hive.stageid.rearrange", "none", new StringSet("none", "idonly", "traverse", "execution"), ""),
+    HIVEEXPLAINDEPENDENCYAPPENDTASKTYPES("hive.explain.dependency.append.tasktype", false, ""),
+
+    HIVECOUNTERGROUP("hive.counters.group.name", "HIVE",
+        "The name of counter group for internal Hive variables (CREATED_FILE, FATAL_ERROR, etc.)"),
+
+    HIVE_QUOTEDID_SUPPORT("hive.support.quoted.identifiers", "column",
+        new StringSet("none", "column"),
+        "Whether to use quoted identifier. 'none' or 'column' can be used. \n" +
+        "  none: default(past) behavior. Implies only alphaNumeric and underscore are valid characters in identifiers.\n" +
+        "  column: implies column names can contain any character."
+    ),
+    HIVE_SUPPORT_SPECICAL_CHARACTERS_IN_TABLE_NAMES("hive.support.special.characters.tablename", true,
+        "This flag should be set to true to enable support for special characters in table names.\n"
+        + "When it is set to false, only [a-zA-Z_0-9]+ are supported.\n"
+        + "The only supported special character right now is '/'. This flag applies only to quoted table names.\n"
+        + "The default value is true."),
+    // role names are case-insensitive
+    USERS_IN_ADMIN_ROLE("hive.users.in.admin.role", "", false,
+        "Comma separated list of users who are in admin role for bootstrapping.\n" +
+        "More users can be added in ADMIN role later."),
+
+    HIVE_COMPAT("hive.compat", HiveCompat.DEFAULT_COMPAT_LEVEL,
+        "Enable (configurable) deprecated behaviors by setting desired level of backward compatibility.\n" +
+        "Setting to 0.12:\n" +
+        "  Maintains division behavior: int / int = double"),
+    HIVE_CONVERT_JOIN_BUCKET_MAPJOIN_TEZ("hive.convert.join.bucket.mapjoin.tez", false,
+        "Whether joins can be automatically converted to bucket map joins in hive \n" +
+        "when tez is used as the execution engine."),
+
+    HIVE_CHECK_CROSS_PRODUCT("hive.exec.check.crossproducts", true,
+        "Check if a plan contains a Cross Product. If there is one, output a warning to the Session's console."),
+    HIVE_LOCALIZE_RESOURCE_WAIT_INTERVAL("hive.localize.resource.wait.interval", "5000ms",
+        new TimeValidator(TimeUnit.MILLISECONDS),
+        "Time to wait for another thread to localize the same resource for hive-tez."),
+    HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS("hive.localize.resource.num.wait.attempts", 5,
+        "The number of attempts waiting for localizing a resource in hive-tez."),
+    TEZ_AUTO_REDUCER_PARALLELISM("hive.tez.auto.reducer.parallelism", false,
+        "Turn on Tez' auto reducer parallelism feature. When enabled, Hive will still estimate data sizes\n" +
+        "and set parallelism estimates. Tez will sample source vertices' output sizes and adjust the estimates at runtime as\n" +
+        "necessary."),
+    TEZ_LLAP_MIN_REDUCER_PER_EXECUTOR("hive.tez.llap.min.reducer.per.executor", 0.95f,
+        "If above 0, the min number of reducers for auto-parallelism for LLAP scheduling will\n" +
+        "be set to this fraction of the number of executors."),
+    TEZ_MAX_PARTITION_FACTOR("hive.tez.max.partition.factor", 2f,
+        "When auto reducer parallelism is enabled this factor will be used to over-partition data in shuffle edges."),
+    TEZ_MIN_PARTITION_FACTOR("hive.tez.min.partition.factor", 0.25f,
+        "When auto reducer parallelism is enabled this factor will be used to put a lower limit to the number\n" +
+        "of reducers that tez specifies."),
+    TEZ_OPTIMIZE_BUCKET_PRUNING(
+        "hive.tez.bucket.pruning", false,
+         "When pruning is enabled, filters on bucket columns will be processed by \n" +
+         "filtering the splits against a bitset of included buckets. This needs predicates \n"+
+         "produced by hive.optimize.ppd and hive.optimize.index.filters."),
+    TEZ_OPTIMIZE_BUCKET_PRUNING_COMPAT(
+        "hive.tez.bucket.pruning.compat", true,
+        "When pruning is enabled, handle possibly broken inserts due to negative hashcodes.\n" +
+        "This occasionally doubles the data scan cost, but is default enabled for safety"),
+    TEZ_DYNAMIC_PARTITION_PRUNING(
+        "hive.tez.dynamic.partition.pruning", true,
+        "When dynamic pruning is enabled, joins on partition keys will be processed by sending\n" +
+        "events from the processing vertices to the Tez application master. These events will be\n" +
+        "used to prune unnecessary partitions."),
+    TEZ_DYNAMIC_PARTITION_PRUNING_MAX_EVENT_SIZE("hive.tez.dynamic.partition.pruning.max.event.size", 1*1024*1024L,
+        "Maximum size of events sent by processors in dynamic pruning. If this size is crossed no pruning will take place."),
+
+    TEZ_DYNAMIC_PARTITION_PRUNING_MAX_DATA_SIZE("hive.tez.dynamic.partition.pruning.max.data.size", 100*1024*1024L,
+        "Maximum total data size of events in dynamic pruning."),
+    TEZ_DYNAMIC_SEMIJOIN_REDUCTION("hive.tez.dynamic.semijoin.reduction", true,
+        "When dynamic semijoin is enabled, shuffle joins will perform a leaky semijoin before shuffle. This " +
+        "requires hive.tez.dynamic.partition.pruning to be enabled."),
+    TEZ_MIN_BLOOM_FILTER_ENTRIES("hive.tez.min.bloom.filter.entries", 1000000L,
+            "Bloom filter should be of at min certain size to be effective"),
+    TEZ_MAX_BLOOM_FILTER_ENTRIES("hive.tez.max.bloom.filter.entries", 100000000L,
+            "Bloom filter should be of at max certain size to be effective"),
+    TEZ_BLOOM_FILTER_FACTOR("hive.tez.bloom.filter.factor", (float) 2.0,
+            "Bloom filter should be a multiple of this factor with nDV"),
+    TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION("hive.tez.bigtable.minsize.semijoin.reduction", 100000000L,
+            "Big table for runtime filteting should be of atleast this size"),
+    TEZ_DYNAMIC_SEMIJOIN_REDUCTION_THRESHOLD("hive.tez.dynamic.semijoin.reduction.threshold", (float) 0.50,
+            "Only perform semijoin optimization if the estimated benefit at or above this fraction of the target table"),
+    TEZ_SMB_NUMBER_WAVES(
+        "hive.tez.smb.number.waves",
+        (float) 0.5,
+        "The number of waves in which to run the SMB join. Account for cluster being occupied. Ideally should be 1 wave."),
+    TEZ_EXEC_SUMMARY(
+        "hive.tez.exec.print.summary",
+        false,
+        "Display breakdown of execution steps, for every query executed by the shell."),
+    TEZ_EXEC_INPLACE_PROGRESS(
+        "hive.tez.exec.inplace.progress",
+        true,
+        "Updates tez job execution progress in-place in the terminal when hive-cli is used."),
+    HIVE_SERVER2_INPLACE_PROGRESS(
+        "hive.server2.in.place.progress",
+        true,
+        "Allows hive server 2 to send progress bar update information. This is currently available"
+            + " only if the execution engine is tez."),
+    SPARK_EXEC_INPLACE_PROGRESS("hive.spark.exec.inplace.progress", true,
+        "Updates spark job execution progress in-place in the terminal."),
+    TEZ_CONTAINER_MAX_JAVA_HEAP_FRACTION("hive.tez.container.max.java.heap.fraction", 0.8f,
+        "This is to override the tez setting with the same name"),
+    TEZ_TASK_SCALE_MEMORY_RESERVE_FRACTION_MIN("hive.tez.task.scale.memory.reserve-fraction.min",
+        0.3f, "This is to override the tez setting tez.task.scale.memory.reserve-fraction"),
+    TEZ_TASK_SCALE_MEMORY_RESERVE_FRACTION_MAX("hive.tez.task.scale.memory.reserve.fraction.max",
+        0.5f, "The maximum fraction of JVM memory which Tez will reserve for the processor"),
+    TEZ_TASK_SCALE_MEMORY_RESERVE_FRACTION("hive.tez.task.scale.memory.reserve.fraction",
+        -1f, "The customized fraction of JVM memory which Tez will reserve for the processor"),
+    // The default is different on the client and server, so it's null here.
+    LLAP_IO_ENABLED("hive.llap.io.enabled", null, "Whether the LLAP IO layer is enabled."),
+    LLAP_IO_NONVECTOR_WRAPPER_ENABLED("hive.llap.io.nonvector.wrapper.enabled", true,
+        "Whether the LLAP IO layer is enabled for non-vectorized queries that read inputs\n" +
+        "that can be vectorized"),
+    LLAP_IO_MEMORY_MODE("hive.llap.io.memory.mode", "cache",
+        new StringSet("cache", "none"),
+        "LLAP IO memory usage; 'cache' (the default) uses data and metadata cache with a\n" +
+        "custom off-heap allocator, 'none' doesn't use either (this mode may result in\n" +
+        "significant performance degradation)"),
+    LLAP_ALLOCATOR_MIN_ALLOC("hive.llap.io.allocator.alloc.min", "16Kb", new SizeValidator(),
+        "Minimum allocation possible from LLAP buddy allocator. Allocations below that are\n" +
+        "padded to minimum allocation. For ORC, should generally be the same as the expected\n" +
+        "compression buffer size, or next lowest power of 2. Must be a power of 2."),
+    LLAP_ALLOCATOR_MAX_ALLOC("hive.llap.io.allocator.alloc.max", "16Mb", new SizeValidator(),
+        "Maximum allocation possible from LLAP buddy allocator. For ORC, should be as large as\n" +
+        "the largest expected ORC compression buffer size. Must be a power of 2."),
+    @Deprecated
+    LLAP_IO_METADATA_FRACTION("hive.llap.io.metadata.fraction", 0.1f,
+        "Temporary setting for on-heap metadata cache fraction of xmx, set to avoid potential\n" +
+        "heap problems on very large datasets when on-heap metadata cache takes over\n" +
+        "everything. -1 managed metadata and data together (which is more flexible). This\n" +
+        "setting will be removed (in effect become -1) once ORC metadata cache is moved off-heap."),
+    LLAP_ALLOCATOR_ARENA_COUNT("hive.llap.io.allocator.arena.count", 8,
+        "Arena count for LLAP low-level cache; cache will be allocated in the steps of\n" +
+        "(size/arena_count) bytes. This size must be <= 1Gb and >= max allocation; if it is\n" +
+        "not the case, an adjusted size will be used. Using powers of 2 is recommended."),
+    LLAP_IO_MEMORY_MAX_SIZE("hive.llap.io.memory.size", "1Gb", new SizeValidator(),
+        "Maximum size for IO allocator or ORC low-level cache.", "hive.llap.io.cache.orc.size"),
+    LLAP_ALLOCATOR_DIRECT("hive.llap.io.allocator.direct", true,
+        "Whether ORC low-level cache should use direct allocation."),
+    LLAP_ALLOCATOR_MAPPED("hive.llap.io.allocator.mmap", false,
+        "Whether ORC low-level cache should use memory mapped allocation (direct I/O). \n" +
+        "This is recommended to be used along-side NVDIMM (DAX) or NVMe flash storage."),
+    LLAP_ALLOCATOR_MAPPED_PATH("hive.llap.io.allocator.mmap.path", "/tmp",
+        new WritableDirectoryValidator(),
+        "The directory location for mapping NVDIMM/NVMe flash storage into the ORC low-level cache."),
+    LLAP_ALLOCATOR_DISCARD_METHOD("hive.llap.io.allocator.discard.method", "both",
+        new StringSet("freelist", "brute", "both"),
+        "Which method to use to force-evict blocks to deal with fragmentation:\n" +
+        "freelist - use half-size free list (discards less, but also less reliable); brute -\n" +
+        "brute force, discard whatever we can; both - first try free list, then brute force."),
+    LLAP_ALLOCATOR_DEFRAG_HEADROOM("hive.llap.io.allocator.defrag.headroom", "1Mb",
+        "How much of a headroom to leave to allow allocator more flexibility to defragment.\n" +
+        "The allocator would further cap it to a fraction of total memory."),
+    LLAP_USE_LRFU("hive.llap.io.use.lrfu", true,
+        "Whether ORC low-level cache should use LRFU cache policy instead of default (FIFO)."),
+    LLAP_LRFU_LAMBDA("hive.llap.io.lrfu.lambda", 0.01f,
+        "Lambda for ORC low-level cache LRFU cache policy. Must be in [0, 1]. 0 makes LRFU\n" +
+        "behave like LFU, 1 makes it behave like LRU, values in between balance accordingly."),
+    LLAP_CACHE_ALLOW_SYNTHETIC_FILEID("hive.llap.cache.allow.synthetic.fileid", false,
+        "Whether LLAP cache should use synthetic file ID if real one is not available. Systems\n" +
+        "like HDFS, Isilon, etc. provide a unique file/inode ID. On other FSes (e.g. local\n" +
+        "FS), the cache would not work by default because LLAP is unable to uniquely track the\n" +
+        "files; enabling this setting allows LLAP to generate file ID from the path, size and\n" +
+        "modification time, which is almost certain to identify file uniquely. However, if you\n" +
+        "use a FS without file IDs and rewrite files a lot (or are paranoid), you might want\n" +
+        "to avoid this setting."),
+    LLAP_CACHE_ENABLE_ORC_GAP_CACHE("hive.llap.orc.gap.cache", true,
+        "Whether LLAP cache for ORC should remember gaps in ORC compression buffer read\n" +
+        "estimates, to avoid re-reading the data that was read once and discarded because it\n" +
+        "is unneeded. This is only necessary for ORC files written before HIVE-9660."),
+    LLAP_IO_USE_FILEID_PATH("hive.llap.io.use.fileid.path", true,
+        "Whether LLAP should use fileId (inode)-based path to ensure better consistency for the\n" +
+        "cases of file overwrites. This is supported on HDFS."),
+    // Restricted to text for now as this is a new feature; only text files can be sliced.
+    LLAP_IO_ENCODE_ENABLED("hive.llap.io.encode.enabled", true,
+        "Whether LLAP should try to re-encode and cache data for non-ORC formats. This is used\n" +
+        "on LLAP Server side to determine if the infrastructure for that is initialized."),
+    LLAP_IO_ENCODE_FORMATS("hive.llap.io.encode.formats",
+        "org.apache.hadoop.mapred.TextInputFormat,",
+        "The table input formats for which LLAP IO should re-encode and cache data.\n" +
+        "Comma-separated list."),
+    LLAP_IO_ENCODE_ALLOC_SIZE("hive.llap.io.encode.alloc.size", "256Kb", new SizeValidator(),
+        "Allocation size for the buffers used to cache encoded data from non-ORC files. Must\n" +
+        "be a power of two between " + LLAP_ALLOCATOR_MIN_ALLOC + " and\n" +
+        LLAP_ALLOCATOR_MAX_ALLOC + "."),
+    LLAP_IO_ENCODE_VECTOR_SERDE_ENABLED("hive.llap.io.encode.vector.serde.enabled", true,
+        "Whether LLAP should use vectorized SerDe reader to read text data when re-encoding."),
+    LLAP_IO_ENCODE_VECTOR_SERDE_ASYNC_ENABLED("hive.llap.io.encode.vector.serde.async.enabled",
+        true,
+        "Whether LLAP should use async mode in vectorized SerDe reader to read text data."),
+    LLAP_IO_ENCODE_SLICE_ROW_COUNT("hive.llap.io.encode.slice.row.count", 100000,
+        "Row count to use to separate cache slices when reading encoded data from row-based\n" +
+        "inputs into LLAP cache, if this feature is enabled."),
+    LLAP_IO_ENCODE_SLICE_LRR("hive.llap.io.encode.slice.lrr", true,
+        "Whether to separate cache slices when reading encoded data from text inputs via MR\n" +
+        "MR LineRecordRedader into LLAP cache, if this feature is enabled. Safety flag."),
+    LLAP_ORC_ENABLE_TIME_COUNTERS("hive.llap.io.orc.time.counters", true,
+        "Whether to enable time counters for LLAP IO layer (time spent in HDFS, etc.)"),
+    LLAP_AUTO_ALLOW_UBER("hive.llap.auto.allow.uber", false,
+        "Whether or not to allow the planner to run vertices in the AM."),
+    LLAP_AUTO_ENFORCE_TREE("hive.llap.auto.enforce.tree", true,
+        "Enforce that all parents are in llap, before considering vertex"),
+    LLAP_AUTO_ENFORCE_VECTORIZED("hive.llap.auto.enforce.vectorized", true,
+        "Enforce that inputs are vectorized, before considering vertex"),
+    LLAP_AUTO_ENFORCE_STATS("hive.llap.auto.enforce.stats", true,
+        "Enforce that col stats are available, before considering vertex"),
+    LLAP_AUTO_MAX_INPUT("hive.llap.auto.max.input.size", 10*1024*1024*1024L,
+        "Check input size, before considering vertex (-1 disables check)"),
+    LLAP_AUTO_MAX_OUTPUT("hive.llap.auto.max.output.size", 1*1024*1024*1024L,
+        "Check output size, before considering vertex (-1 disables check)"),
+    LLAP_SKIP_COMPILE_UDF_CHECK("hive.llap.skip.compile.udf.check", false,
+        "Whether to skip the compile-time check for non-built-in UDFs when deciding whether to\n" +
+        "execute tasks in LLAP. Skipping the check allows executing UDFs from pre-localized\n" +
+        "jars in LLAP; if the jars are not pre-localized, the UDFs will simply fail to load."),
+    LLAP_ALLOW_PERMANENT_FNS("hive.llap.allow.permanent.fns", true,
+        "Whether LLAP decider should allow permanent UDFs."),
+    LLAP_EXECUTION_MODE("hive.llap.execution.mode", "none",
+        new StringSet("auto", "none", "all", "map", "only"),
+        "Chooses whether query fragments will run in container or in llap"),
+    LLAP_OBJECT_CACHE_ENABLED("hive.llap.object.cache.enabled", true,
+        "Cache objects (plans, hashtables, etc) in llap"),
+    LLAP_IO_DECODING_METRICS_PERCENTILE_INTERVALS("hive.llap.io.decoding.metrics.percentiles.intervals", "30",
+        "Comma-delimited set of integers denoting the desired rollover intervals (in seconds)\n" +
+        "for percentile latency metrics on the LLAP daemon IO decoding time.\n" +
+        "hive.llap.queue.metrics.percentiles.intervals"),
+    LLAP_IO_THREADPOOL_SIZE("hive.llap.io.threadpool.size", 10,
+        "Specify the number of threads to use for low-level IO thread pool."),
+    LLAP_KERBEROS_PRINCIPAL(HIVE_LLAP_DAEMON_SERVICE_PRINCIPAL_NAME, "",
+        "The name of the LLAP daemon's service principal."),
+    LLAP_KERBEROS_KEYTAB_FILE("hive.llap.daemon.keytab.file", "",
+        "The path to the Kerberos Keytab file containing the LLAP daemon's service principal."),
+    LLAP_ZKSM_KERBEROS_PRINCIPAL("hive.llap.zk.sm.principal", "",
+        "The name of the principal to use to talk to ZooKeeper for ZooKeeper SecretManager."),
+    LLAP_ZKSM_KERBEROS_KEYTAB_FILE("hive.llap.zk.sm.keytab.file", "",
+        "The path to the Kerberos Keytab file containing the principal to use to talk to\n" +
+        "ZooKeeper for ZooKeeper SecretManager."),
+    LLAP_WEBUI_SPNEGO_KEYTAB_FILE("hive.llap.webui.spnego.keytab", "",
+        "The path to the Kerberos Keytab file containing the LLAP WebUI SPNEGO principal.\n" +
+        "Typical value would look like /etc/security/keytabs/spnego.service.keytab."),
+    LLAP_WEBUI_SPNEGO_PRINCIPAL("hive.llap.webui.spnego.principal", "",
+        "The LLAP WebUI SPNEGO service principal. Configured similarly to\n" +
+        "hive.server2.webui.spnego.principal"),
+    LLAP_FS_KERBEROS_PRINCIPAL("hive.llap.task.principal", "",
+        "The name of the principal to use to run tasks. By default, the clients are required\n" +
+        "to provide tokens to access HDFS/etc."),
+    LLAP_FS_KERBEROS_KEYTAB_FILE("hive.llap.task.keytab.file", "",
+        "The path to the Kerberos Keytab file containing the principal to use to run tasks.\n" +
+        "By default, the clients are required to provide tokens to access HDFS/etc."),
+    LLAP_ZKSM_ZK_CONNECTION_STRING("hive.llap.zk.sm.connectionString", "",
+        "ZooKeeper connection string for ZooKeeper SecretManager."),
+    LLAP_ZKSM_ZK_SESSION_TIMEOUT("hive.llap.zk.sm.session.timeout", "40s", new TimeValidator(
+        TimeUnit.MILLISECONDS), "ZooKeeper session timeout for ZK SecretManager."),
+    LLAP_ZK_REGISTRY_USER("hive.llap.zk.registry.user", "",
+        "In the LLAP ZooKeeper-based registry, specifies the username in the Zookeeper path.\n" +
+        "This should be the hive user or whichever user is running the LLAP daemon."),
+    LLAP_ZK_REGISTRY_NAMESPACE("hive.llap.zk.registry.namespace", null,
+        "In the LLAP ZooKeeper-based registry, overrides the ZK path namespace. Note that\n" +
+        "using this makes the path management (e.g. setting correct ACLs) your responsibility."),
+    // Note: do not rename to ..service.acl; Hadoop generates .hosts setting name from this,
+    // resulting in a collision with existing hive.llap.daemon.service.hosts and bizarre errors.
+    // These are read by Hadoop IPC, so you should check the usage and naming conventions (e.g.
+    // ".blocked" is a string hardcoded by Hadoop, and defaults are enforced elsewhere in Hive)
+    // before making changes or copy-pasting these.
+    LLAP_SECURITY_ACL("hive.llap.daemon.acl", "*", "The ACL for LLAP daemon."),
+    LLAP_SECURITY_ACL_DENY("hive.llap.daemon.acl.blocked", "", "The deny ACL for LLAP daemon."),
+    LLAP_MANAGEMENT_ACL("hive.llap.management.acl", "*", "The ACL for LLAP daemon management."),
+    LLAP_MANAGEMENT_ACL_DENY("hive.llap.management.acl.blocked", "",
+        "The deny ACL for LLAP daemon management."),
+    LLAP_REMOTE_TOKEN_REQUIRES_SIGNING("hive.llap.remote.token.requires.signing", "true",
+        new StringSet("false", "except_llap_owner", "true"),
+        "Whether the token returned from LLAP management API should require fragment signing.\n" +
+        "True by default; can be disabled to allow CLI to get tokens from LLAP in a secure\n" +
+        "cluster by setting it to true or 'except_llap_owner' (the latter returns such tokens\n" +
+        "to everyone except the user LLAP cluster is authenticating under)."),
+
+    // Hadoop DelegationTokenManager default is 1 week.
+    LLAP_DELEGATION_TOKEN_LIFETIME("hive.llap.daemon.delegation.token.lifetime", "14d",
+         new TimeValidator(TimeUnit.SECONDS),
+        "LLAP delegation token lifetime, in seconds if specified without a unit."),
+    LLAP_MANAGEMENT_RPC_PORT("hive.llap.management.rpc.port", 15004,
+        "RPC port for LLAP daemon management service."),
+    LLAP_WEB_AUTO_AUTH("hive.llap.auto.auth", false,
+        "Whether or not to set Hadoop configs to enable auth in LLAP web app."),
+
+    LLAP_DAEMON_RPC_NUM_HANDLERS("hive.llap.daemon.rpc.num.handlers", 5,
+      "Number of RPC handlers for LLAP daemon.", "llap.daemon.rpc.num.handlers"),
+    LLAP_DAEMON_WORK_DIRS("hive.llap.daemon.work.dirs", "",
+        "Working directories for the daemon. This should not be set if running as a YARN\n" +
+        "application via Slider. It must be set when not running via Slider on YARN. If the value\n" +
+        "is set when running as a Slider YARN application, the specified value will be used.",
+        "llap.daemon.work.dirs"),
+    LLAP_DAEMON_YARN_SHUFFLE_PORT("hive.llap.daemon.yarn.shuffle.port", 15551,
+      "YARN shuffle port for LLAP-daemon-hosted shuffle.", "llap.daemon.yarn.shuffle.port"),
+    LLAP_DAEMON_YARN_CONTAINER_MB("hive.llap.daemon.yarn.container.mb", -1,
+      "llap server yarn container size in MB. Used in LlapServiceDriver and package.py", "llap.daemon.yarn.container.mb"),
+    LLAP_DAEMON_QUEUE_NAME("hive.llap.daemon.queue.name", null,
+        "Queue name within which the llap slider application will run." +
+        " Used in LlapServiceDriver and package.py"),
+    // TODO Move the following 2 properties out of Configuration to a constant.
+    LLAP_DAEMON_CONTAINER_ID("hive.llap.daemon.container.id", null,
+        "ContainerId of a running LlapDaemon. Used to publish to the registry"),
+    LLAP_DAEMON_NM_ADDRESS("hive.llap.daemon.nm.address", null,
+        "NM Address host:rpcPort for the NodeManager on which the instance of the daemon is running.\n" +
+        "Published to the llap registry. Should never be set by users"),
+    LLAP_DAEMON_SHUFFLE_DIR_WATCHER_ENABLED("hive.llap.daemon.shuffle.dir.watcher.enabled", false,
+      "TODO doc", "llap.daemon.shuffle.dir-watcher.enabled"),
+    LLAP_DAEMON_AM_LIVENESS_HEARTBEAT_INTERVAL_MS(
+      "hive.llap.daemon.am.liveness.heartbeat.interval.ms", "10000ms",
+      new TimeValidator(TimeUnit.MILLISECONDS),
+      "Tez AM-LLAP heartbeat interval (milliseconds). This needs to be below the task timeout\n" +
+      "interval, but otherwise as high as possible to avoid unnecessary traffic.",
+      "llap.daemon.am.liveness.heartbeat.interval-ms"),
+    LLAP_DAEMON_AM_LIVENESS_CONNECTION_TIMEOUT_MS(
+      "hive.llap.am.liveness.connection.timeout.ms", "10000ms",
+      new TimeValidator(TimeUnit.MILLISECONDS),
+      "Amount of time to wait on connection failures to the AM from an LLAP daemon before\n" +
+      "considering the AM to be dead.", "llap.am.liveness.connection.timeout-millis"),
+    LLAP_DAEMON_AM_USE_FQDN("hive.llap.am.use.fqdn", false,
+        "Whether to use FQDN of the AM machine when submitting work to LLAP."),
+    // Not used yet - since the Writable RPC engine does not support this policy.
+    LLAP_DAEMON_AM_LIVENESS_CONNECTION_SLEEP_BETWEEN_RETRIES_MS(
+      "hive.llap.am.liveness.connection.sleep.between.retries.ms", "2000ms",
+      new TimeValidator(TimeUnit.MILLISECONDS),
+      "Sleep duration while waiting to retry connection failures to the AM from the daemon for\n" +
+      "the general keep-alive thread (milliseconds).",
+      "llap.am.liveness.connection.sleep-between-retries-millis"),
+    LLAP_DAEMON_TASK_SCHEDULER_TIMEOUT_SECONDS(
+        "hive.llap.task.scheduler.timeout.seconds", "60s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Amount of time to wait before failing the query when there are no llap daemons running\n" +
+            "(alive) in the cluster.", "llap.daemon.scheduler.timeout.seconds"),
+    LLAP_DAEMON_NUM_EXECUTORS("hive.llap.daemon.num.executors", 4,
+      "Number of executors to use in LLAP daemon; essentially, the number of tasks that can be\n" +
+      "executed in parallel.", "llap.daemon.num.executors"),
+    LLAP_MAPJOIN_MEMORY_OVERSUBSCRIBE_FACTOR("hive.llap.mapjoin.memory.oversubscribe.factor", 0.2f,
+      "Fraction of memory from hive.auto.convert.join.noconditionaltask.size that can be over subscribed\n" +
+        "by queries running in LLAP mode. This factor has to be from 0.0 to 1.0. Default is 20% over subscription.\n"),
+    LLAP_MEMORY_OVERSUBSCRIPTION_MAX_EXECUTORS_PER_QUERY("hive.llap.memory.oversubscription.max.executors.per.query", 3,
+      "Used along with hive.llap.mapjoin.memory.oversubscribe.factor to limit the number of executors from\n" +
+        "which memory for mapjoin can be borrowed. Default 3 (from 3 other executors\n" +
+        "hive.llap.mapjoin.memory.oversubscribe.factor amount of memory can be borrowed based on which mapjoin\n" +
+        "conversion decision will be made). This is only an upper bound. Lower bound is determined by number of\n" +
+        "executors and configured max concurrency."),
+    LLAP_MAPJOIN_MEMORY_MONITOR_CHECK_INTERVAL("hive.llap.mapjoin.memory.monitor.check.interval", 100000L,
+      "Check memory usage of mapjoin hash tables after every interval of this many rows. If map join hash table\n" +
+        "memory usage exceeds (hive.auto.convert.join.noconditionaltask.size * hive.hash.table.inflation.factor)\n" +
+        "when running in LLAP, tasks will get killed and not retried. Set the value to 0 to disable this feature."),
+    LLAP_DAEMON_AM_REPORTER_MAX_THREADS("hive.llap.daemon.am-reporter.max.threads", 4,
+        "Maximum number of threads to be used for AM reporter. If this is lower than number of\n" +
+        "executors in llap daemon, it would be set to number of executors at runtime.",
+        "llap.daemon.am-reporter.max.threads"),
+    LLAP_DAEMON_RPC_PORT("hive.llap.daemon.rpc.port", 0, "The LLAP daemon RPC port.",
+      "llap.daemon.rpc.port. A value of 0 indicates a dynamic port"),
+    LLAP_DAEMON_MEMORY_PER_INSTANCE_MB("hive.llap.daemon.memory.per.instance.mb", 4096,
+      "The total amount of memory to use for the executors inside LLAP (in megabytes).",
+      "llap.daemon.memory.per.instance.mb"),
+    LLAP_DAEMON_XMX_HEADROOM("hive.llap.daemon.xmx.headroom", "5%",
+      "The total amount of heap memory set aside by LLAP and not used by the executors. Can\n" +
+      "be specified as size (e.g. '512Mb'), or percentage (e.g. '5%'). Note that the latter is\n" +
+      "derived from the total daemon XMX, which can be different from the total executor\n" +
+      "memory if the cache is on-heap; although that's not the default configuration."),
+    LLAP_DAEMON_VCPUS_PER_INSTANCE("hive.llap.daemon.vcpus.per.instance", 4,
+      "The total number of vcpus to use for the executors inside LLAP.",
+      "llap.daemon.vcpus.per.instance"),
+    LLAP_DAEMON_NUM_FILE_CLEANER_THREADS("hive.llap.daemon.num.file.cleaner.threads", 1,
+      "Number of file cleaner threads in LLAP.", "llap.daemon.num.file.cleaner.threads"),
+    LLAP_FILE_CLEANUP_DELAY_SECONDS("hive.llap.file.cleanup.delay.seconds", "300s",
+       new TimeValidator(TimeUnit.SECONDS),
+      "How long to delay before cleaning up query files in LLAP (in seconds, for debugging).",
+      "llap.file.cleanup.delay-seconds"),
+    LLAP_DAEMON_SERVICE_HOSTS("hive.llap.daemon.service.hosts", null,
+      "Explicitly specified hosts to use for LLAP scheduling. Useful for testing. By default,\n" +
+      "YARN registry is used.", "llap.daemon.service.hosts"),
+    LLAP_DAEMON_SERVICE_REFRESH_INTERVAL("hive.llap.daemon.service.refresh.interval.sec", "60s",
+       new TimeValidator(TimeUnit.SECONDS),
+      "LLAP YARN registry service list refresh delay, in seconds.",
+      "llap.daemon.service.refresh.interval"),
+    LLAP_DAEMON_COMMUNICATOR_NUM_THREADS("hive.llap.daemon.communicator.num.threads", 10,
+      "Number of threads to use in LLAP task communicator in Tez AM.",
+      "llap.daemon.communicator.num.threads"),
+    LLAP_DAEMON_DOWNLOAD_PERMANENT_FNS("hive.llap.daemon.download.permanent.fns", false,
+        "Whether LLAP daemon should localize the resources for permanent UDFs."),
+    LLAP_TASK_SCHEDULER_NODE_REENABLE_MIN_TIMEOUT_MS(
+      "hive.llap.task.scheduler.node.reenable.min.timeout.ms", "200ms",
+      new TimeValidator(TimeUnit.MILLISECONDS),
+      "Minimum time after which a previously disabled node will be re-enabled for scheduling,\n" +
+      "in milliseconds. This may be modified by an exponential back-off if failures persist.",
+      "llap.task.scheduler.node.re-enable.min.timeout.ms"),
+    LLAP_TASK_SCHEDULER_NODE_REENABLE_MAX_TIMEOUT_MS(
+      "hive.llap.task.scheduler.node.reenable.max.timeout.ms", "10000ms",
+      new TimeValidator(TimeUnit.MILLISECONDS),
+      "Maximum time after which a previously disabled node will be re-enabled for scheduling,\n" +
+      "in milliseconds. This may be modified by an exponential back-off if failures persist.",
+      "llap.task.scheduler.node.re-enable.max.timeout.ms"),
+    LLAP_TASK_SCHEDULER_NODE_DISABLE_BACK_OFF_FACTOR(
+      "hive.llap.task.scheduler.node.disable.backoff.factor", 1.5f,
+      "Backoff factor on successive blacklists of a node due to some failures. Blacklist times\n" +
+      "start at the min timeout and go up to the max timeout based on this backoff factor.",
+      "llap.task.scheduler.node.disable.backoff.factor"),
+    LLAP_TASK_SCHEDULER_NUM_SCHEDULABLE_TASKS_PER_NODE(
+      "hive.llap.task.scheduler.num.schedulable.tasks.per.node", 0,
+      "The number of tasks the AM TaskScheduler will try allocating per node. 0 indicates that\n" +
+      "this should be picked up from the Registry. -1 indicates unlimited capacity; positive\n" +
+      "values indicate a specific bound.", "llap.task.scheduler.num.schedulable.tasks.per.node"),
+    LLAP_TASK_SCHEDULER_LOCALITY_DELAY(
+        "hive.llap.task.scheduler.locality.delay", "0ms",
+        new TimeValidator(TimeUnit.MILLISECONDS, -1l, true, Long.MAX_VALUE, true),
+        "Amount of time to wait before allocating a request which contains location information," +
+            " to a location other than the ones requested. Set to -1 for an infinite delay, 0" +
+            "for no delay."
+    ),
+    LLAP_DAEMON_TASK_PREEMPTION_METRICS_INTERVALS(
+        "hive.llap.daemon.task.preemption.metrics.intervals", "30,60,300",
+        "Comma-delimited set of integers denoting the desired rollover intervals (in seconds)\n" +
+        " for percentile latency metrics. Used by LLAP daemon task scheduler metrics for\n" +
+        " time taken to kill task (due to pre-emption) and useful time wasted by the task that\n" +
+        " is about to be preempted."
+    ),
+    LLAP_DAEMON_TASK_SCHEDULER_WAIT_QUEUE_SIZE("hive.llap.daemon.task.scheduler.wait.queue.size",
+      10, "LLAP scheduler maximum queue size.", "llap.daemon.task.scheduler.wait.queue.size"),
+    LLAP_DAEMON_WAIT_QUEUE_COMPARATOR_CLASS_NAME(
+      "hive.llap.daemon.wait.queue.comparator.class.name",
+      "org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator",
+      "The priority comparator to use for LLAP scheduler prioroty queue. The built-in options\n" +
+      "are org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator and\n" +
+      ".....FirstInFirstOutComparator", "llap.daemon.wait.queue.comparator.class.name"),
+    LLAP_DAEMON_TASK_SCHEDULER_ENABLE_PREEMPTION(
+      "hive.llap.daemon.task.scheduler.enable.preemption", true,
+      "Whether non-finishable running tasks (e.g. a reducer waiting for inputs) should be\n" +
+      "preempted by finishable tasks inside LLAP scheduler.",
+      "llap.daemon.task.scheduler.enable.preemption"),
+    LLAP_TASK_COMMUNICATOR_CONNECTION_TIMEOUT_MS(
+      "hive.llap.task.communicator.connection.timeout.ms", "16000ms",
+      new TimeValidator(TimeUnit.MILLISECONDS),
+      "Connection timeout (in milliseconds) before a failure to an LLAP daemon from Tez AM.",
+      "llap.task.communicator.connection.timeout-millis"),
+    LLAP_TASK_COMMUNICATOR_LISTENER_THREAD_COUNT(
+        "hive.llap.task.communicator.listener.thread-count", 30,
+        "The number of task communicator listener threads."),
+    LLAP_TASK_COMMUNICATOR_CONNECTION_SLEEP_BETWEEN_RETRIES_MS(
+      "hive.llap.task.communicator.connection.sleep.between.retries.ms", "2000ms",
+      new TimeValidator(TimeUnit.MILLISECONDS),
+      "Sleep duration (in milliseconds) to wait before retrying on error when obtaining a\n" +
+      "connection to LLAP daemon from Tez AM.",
+      "llap.task.communicator.connection.sleep-between-retries-millis"),
+    LLAP_DAEMON_WEB_PORT("hive.llap.daemon.web.port", 15002, "LLAP daemon web UI port.",
+      "llap.daemon.service.port"),
+    LLAP_DAEMON_WEB_SSL("hive.llap.daemon.web.ssl", false,
+      "Whether LLAP daemon web UI should use SSL.", "llap.daemon.service.ssl"),
+    LLAP_CLIENT_CONSISTENT_SPLITS("hive.llap.client.consistent.splits", false,
+        "Whether to setup split locations to match nodes on which llap daemons are running, " +
+        "instead of using the locations provided by the split itself. If there is no llap daemon " +
+        "running, fall back to locations provided by the split. This is effective only if " +
+        "hive.execution.mode is llap"),
+    LLAP_VALIDATE_ACLS("hive.llap.validate.acls", true,
+        "Whether LLAP should reject permissive ACLs in some cases (e.g. its own management\n" +
+        "protocol or ZK paths), similar to how ssh refuses a key with bad access permissions."),
+    LLAP_DAEMON_OUTPUT_SERVICE_PORT("hive.llap.daemon.output.service.port", 15003,
+        "LLAP daemon output service port"),
+    LLAP_DAEMON_OUTPUT_STREAM_TIMEOUT("hive.llap.daemon.output.stream.timeout", "120s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "The timeout for the client to connect to LLAP output service and start the fragment\n" +
+        "output after sending the fragment. The fragment will fail if its output is not claimed."),
+    LLAP_DAEMON_OUTPUT_SERVICE_SEND_BUFFER_SIZE("hive.llap.daemon.output.service.send.buffer.size",
+        128 * 1024, "Send buffer size to be used by LLAP daemon output service"),
+    LLAP_DAEMON_OUTPUT_SERVICE_MAX_PENDING_WRITES("hive.llap.daemon.output.service.max.pending.writes",
+        8, "Maximum number of queued writes allowed per connection when sending data\n" +
+        " via the LLAP output service to external clients."),
+    LLAP_ENABLE_GRACE_JOIN_IN_LLAP("hive.llap.enable.grace.join.in.llap", false,
+        "Override if grace join should be allowed to run in llap."),
+
+    LLAP_HS2_ENABLE_COORDINATOR("hive.llap.hs2.coordinator.enabled", true,
+        "Whether to create the LLAP coordinator; since execution engine and container vs llap\n" +
+        "settings are both coming from job configs, we don't know at start whether this should\n" +
+        "be created. Default true."),
+    LLAP_DAEMON_LOGGER("hive.llap.daemon.logger", Constants.LLAP_LOGGER_NAME_QUERY_ROUTING,
+        new StringSet(Constants.LLAP_LOGGER_NAME_QUERY_ROUTING,
+            Constants.LLAP_LOGGER_NAME_RFA,
+            Constants.LLAP_LOGGER_NAME_CONSOLE),
+        "logger used for llap-daemons."),
+
+    SPARK_USE_OP_STATS("hive.spark.use.op.stats", true,
+        "Whether to use operator stats to determine reducer parallelism for Hive on Spark.\n" +
+        "If this is false, Hive will use source table stats to determine reducer\n" +
+        "parallelism for all first level reduce tasks, and the maximum reducer parallelism\n" +
+        "from all parents for all the rest (second level and onward) reducer tasks."),
+    SPARK_USE_TS_STATS_FOR_MAPJOIN("hive.spark.use.ts.stats.for.mapjoin", false,
+        "If this is set to true, mapjoin optimization in Hive/Spark will use statistics from\n" +
+        "TableScan operators at the root of operator tree, instead of parent ReduceSink\n" +
+        "operators of the Join operator."),
+    SPARK_CLIENT_FUTURE_TIMEOUT("hive.spark.client.future.timeout",
+      "60s", new TimeValidator(TimeUnit.SECONDS),
+      "Timeout for requests from Hive client to remote Spark driver."),
+    SPARK_JOB_MONITOR_TIMEOUT("hive.spark.job.monitor.timeout",
+      "60s", new TimeValidator(TimeUnit.SECONDS),
+      "Timeout for job monitor to get Spark job state."),
+    SPARK_RPC_CLIENT_CONNECT_TIMEOUT("hive.spark.client.connect.timeout",
+      "1000ms", new TimeValidator(TimeUnit.MILLISECONDS),
+      "Timeout for remote Spark driver in connecting back to Hive client."),
+    SPARK_RPC_CLIENT_HANDSHAKE_TIMEOUT("hive.spark.client.server.connect.timeout",
+      "90000ms", new TimeValidator(TimeUnit.MILLISECONDS),
+      "Timeout for handshake between Hive client and remote Spark driver.  Checked by both processes."),
+    SPARK_RPC_SECRET_RANDOM_BITS("hive.spark.client.secret.bits", "256",
+      "Number of bits of randomness in the generated secret for communication between Hive client and remote Spark driver. " +
+      "Rounded down to the nearest multiple of 8."),
+    SPARK_RPC_MAX_THREADS("hive.spark.client.rpc.threads", 8,
+      "Maximum number of threads for remote Spark driver's RPC event loop."),
+    SPARK_RPC_MAX_MESSAGE_SIZE("hive.spark.client.rpc.max.size", 50 * 1024 * 1024,
+      "Maximum message size in bytes for communication between Hive client and remote Spark driver. Default is 50MB."),
+    SPARK_RPC_CHANNEL_LOG_LEVEL("hive.spark.client.channel.log.level", null,
+      "Channel logging level for remote Spark driver.  One of {DEBUG, ERROR, INFO, TRACE, WARN}."),
+    SPARK_RPC_SASL_MECHANISM("hive.spark.client.rpc.sasl.mechanisms", "DIGEST-MD5",
+      "Name of the SASL mechanism to use for authentication."),
+    SPARK_RPC_SERVER_ADDRESS("hive.spark.client.rpc.server.address", "",
+      "The server address of HiverServer2 host to be used for communication between Hive client and remote Spark driver. " +
+      "Default is empty, which means the address will be determined in the same way as for hive.server2.thrift.bind.host." +
+      "This is only necessary if the host has mutiple network addresses and if a different network address other than " +
+      "hive.server2.thrift.bind.host is to be used."),
+    SPARK_RPC_SERVER_PORT("hive.spark.client.rpc.server.port", "", "A list of port ranges which can be used by RPC server " +
+        "with the format of 49152-49222,49228 and a random one is selected from the list. Default is empty, which randomly " +
+        "selects one port from all available ones."),
+    SPARK_DYNAMIC_PARTITION_PRUNING(
+        "hive.spark.dynamic.partition.pruning", false,
+        "When dynamic pruning is enabled, joins on partition keys will be processed by writing\n" +
+            "to a temporary HDFS file, and read later for removing unnecessary partitions."),
+    SPARK_DYNAMIC_PARTITION_PRUNING_MAX_DATA_SIZE(
+        "hive.spark.dynamic.partition.pruning.max.data.size", 100*1024*1024L,
+        "Maximum total data size in dynamic pruning."),
+    SPARK_USE_GROUPBY_SHUFFLE(
+        "hive.spark.use.groupby.shuffle", true,
+        "Spark groupByKey transformation has better performance but uses unbounded memory." +
+            "Turn this off when there is a memory issue."),
+    SPARK_JOB_MAX_TASKS("hive.spark.job.max.tasks", -1, "The maximum number of tasks a Spark job may have.\n" +
+            "If a Spark job contains more tasks than the maximum, it will be cancelled. A value of -1 means no limit."),
+    SPARK_STAGE_MAX_TASKS("hive.spark.stage.max.tasks", -1, "The maximum number of tasks a stage in a Spark job may have.\n" +
+        "If a Spark job stage contains more tasks than the maximum, the job will be cancelled. A value of -1 means no limit."),
+    NWAYJOINREORDER("hive.reorder.nway.joins", true,
+      "Runs reordering of tables within single n-way join (i.e.: picks streamtable)"),
+    HIVE_MERGE_NWAY_JOINS("hive.merge.nway.joins", true,
+      "Merge adjacent joins into a single n-way join"),
+    HIVE_LOG_N_RECORDS("hive.log.every.n.records", 0L, new RangeValidator(0L, null),
+      "If value is greater than 0 logs in fixed intervals of size n rather than exponentially."),
+    HIVE_MSCK_PATH_VALIDATION("hive.msck.path.validation", "throw",
+        new StringSet("throw", "skip", "ignore"), "The approach msck should take with HDFS " +
+       "directories that are partition-like but contain unsupported characters. 'throw' (an " +
+       "exception) is the default; 'skip' will skip the invalid directories and still repair the" +
+       " others; 'ignore' will skip the validation (legacy behavior, causes bugs in many cases)"),
+    HIVE_MSCK_REPAIR_BATCH_SIZE(
+        "hive.msck.repair.batch.size", 0,
+        "Batch size for the msck repair command. If the value is greater than zero,\n "
+            + "it will execute batch wise with the configured batch size. In case of errors while\n"
+            + "adding unknown partitions the batch size is automatically reduced by half in the subsequent\n"
+            + "retry attempt. The default value is zero which means it will execute directly (not batch wise)"),
+    HIVE_MSCK_REPAIR_BATCH_MAX_RETRIES("hive.msck.repair.batch.max.retries", 0,
+        "Maximum number of retries for the msck repair command when adding unknown partitions.\n "
+        + "If the value is greater than zero it will retry adding unknown partitions until the maximum\n"
+        + "number of attempts is reached or batch size is reduced to 0, whichever is earlier.\n"
+        + "In each retry attempt it will reduce the batch size by a factor of 2 until it reaches zero.\n"
+        + "If the value is set to zero it will retry until the batch size becomes zero as described above."),
+    HIVE_SERVER2_LLAP_CONCURRENT_QUERIES("hive.server2.llap.concurrent.queries", -1,
+        "The number of queries allowed in parallel via llap. Negative number implies 'infinite'."),
+    HIVE_TEZ_ENABLE_MEMORY_MANAGER("hive.tez.enable.memory.manager", true,
+        "Enable memory manager for tez"),
+    HIVE_HASH_TABLE_INFLATION_FACTOR("hive.hash.table.inflation.factor", (float) 2.0,
+        "Expected inflation factor between disk/in memory representation of hash tables"),
+    HIVE_LOG_TRACE_ID("hive.log.trace.id", "",
+        "Log tracing id that can be used by upstream clients for tracking respective logs. " +
+        "Truncated to " + LOG_PREFIX_LENGTH + " characters. Defaults to use auto-generated session id."),
+
+
+    HIVE_CONF_RESTRICTED_LIST("hive.conf.restricted.list",
+        "hive.security.authenticator.manager,hive.security.authorization.manager," +
+        "hive.security.metastore.authorization.manager,hive.security.metastore.authenticator.manager," +
+        "hive.users.in.admin.role,hive.server2.xsrf.filter.enabled,hive.security.authorization.enabled," +
+            "hive.distcp.privileged.doAs," +
+            "hive.server2.authentication.ldap.baseDN," +
+            "hive.server2.authentication.ldap.url," +
+            "hive.server2.authentication.ldap.Domain," +
+            "hive.server2.authentication.ldap.groupDNPattern," +
+            "hive.server2.authentication.ldap.groupFilter," +
+            "hive.server2.authentication.ldap.userDNPattern," +
+            "hive.server2.authentication.ldap.userFilter," +
+            "hive.server2.authentication.ldap.groupMembershipKey," +
+            "hive.server2.authentication.ldap.userMembershipKey," +
+            "hive.server2.authentication.ldap.groupClassKey," +
+            "hive.server2.authentication.ldap.customLDAPQuery," +
+            "hive.spark.client.connect.timeout," +
+            "hive.spark.client.server.connect.timeout," +
+            "hive.spark.client.channel.log.level," +
+            "hive.spark.client.rpc.max.size," +
+            "hive.spark.client.rpc.threads," +
+            "hive.spark.client.secret.bits," +
+            "hive.spark.client.rpc.server.address," +
+            "hive.spark.client.rpc.server.port",
+        "Comma separated list of configuration options which are immutable at runtime"),
+    HIVE_CONF_HIDDEN_LIST("hive.conf.hidden.list",
+        METASTOREPWD.varname + "," + HIVE_SERVER2_SSL_KEYSTORE_PASSWORD.varname
+        // Adding the S3 credentials from Hadoop config to be hidden
+        + ",fs.s3.awsAccessKeyId"
+        + ",fs.s3.awsSecretAccessKey"
+        + ",fs.s3n.awsAccessKeyId"
+        + ",fs.s3n.awsSecretAccessKey"
+        + ",fs.s3a.access.key"
+        + ",fs.s3a.secret.key"
+        + ",fs.s3a.proxy.password",
+        "Comma separated list of configuration options which should not be read by normal user like passwords"),
+    HIVE_CONF_INTERNAL_VARIABLE_LIST("hive.conf.internal.variable.list",
+        "hive.added.files.path,hive.added.jars.path,hive.added.archives.path",
+        "Comma separated list of variables which are used internally and should not be configurable."),
+
+    HIVE_QUERY_TIMEOUT_SECONDS("hive.query.timeout.seconds", "0s",
+        new TimeValidator(TimeUnit.SECONDS),
+        "Timeout for Running Query in seconds. A nonpositive value means infinite. " +
+        "If the query timeout is also set by thrift API call, the smaller one will be taken."),
+
+
+    HIVE_EXEC_INPUT_LISTING_MAX_THREADS("hive.exec.input.listing.max.threads", 0, new  SizeValidator(0L, true, 1024L, true),
+        "Maximum number of threads that Hive uses to list file information from file systems (recommended > 1 for blobstore)."),
+
+    /* BLOBSTORE section */
+
+    HIVE_BLOBSTORE_SUPPORTED_SCHEMES("hive.blobstore.supported.schemes", "s3,s3a,s3n",
+            "Comma-separated list of supported blobstore schemes."),
+
+    HIVE_BLOBSTORE_USE_BLOBSTORE_AS_SCRATCHDIR("hive.blobstore.use.blobstore.as.scratchdir", false,
+            "Enable the use of scratch directories directly on blob storage systems (it may cause performance penalties)."),
+
+    HIVE_BLOBSTORE_OPTIMIZATIONS_ENABLED("hive.blobstore.optimizations.enabled", true,
+            "This parameter enables a number of optimizations when running on blobstores:\n" +
+            "(1) If hive.blobstore.use.blobstore.as.scratchdir is false, force the last Hive job to write to the blobstore.\n" +
+            "This is a performance optimization that forces the final FileSinkOperator to write to the blobstore.\n" +
+            "See HIVE-15121 for details.");
+
+    public final String varname;
+    public final String altName;
+    private final String defaultExpr;
+
+    public final String defaultStrVal;
+    public final int defaultIntVal;
+    public final long defaultLongVal;
+    public final float defaultFloatVal;
+    public final boolean defaultBoolVal;
+
+    private final Class<?> valClass;
+    private final VarType valType;
+
+    private final Validator validator;
+
+    private final String description;
+
+    private final boolean excluded;
+    private final boolean caseSensitive;
+
+    ConfVars(String varname, Object defaultVal, String description) {
+      this(varname, defaultVal, null, description, true, false, null);
+    }
+
+    ConfVars(String varname, Object defaultVal, String description, String altName) {
+      this(varname, defaultVal, null, description, true, false, altName);
+    }
+
+    ConfVars(String varname, Object defaultVal, Validator validator, String description,
+        String altName) {
+      this(varname, defaultVal, validator, description, true, false, altName);
+    }
+
+    ConfVars(String varname, Object defaultVal, String description, boolean excluded) {
+      this(varname, defaultVal, null, description, true, excluded, null);
+    }
+
+    ConfVars(String varname, String defaultVal, boolean caseSensitive, String description) {
+      this(varname, defaultVal, null, description, caseSensitive, false, null);
+    }
+
+    ConfVars(String varname, Object defaultVal, Validator validator, String description) {
+      this(varname, defaultVal, validator, description, true, false, null);
+    }
+
+    ConfVars(String varname, Object defaultVal, Validator validator, String description,
+        boolean caseSensitive, boolean excluded, String altName) {
+      this.varname = varname;
+      this.validator = validator;
+      this.description = description;
+      this.defaultExpr = defaultVal == null ? null : String.valueOf(defaultVal);
+      this.excluded = excluded;
+      this.caseSensitive = caseSensitive;
+      this.altName = altName;
+      if (defaultVal == null || defaultVal instanceof String) {
+        this.valClass = String.class;
+        this.valType = VarType.STRING;
+        this.defaultStrVal = SystemVariables.substitute((String)defaultVal);
+        this.defaultIntVal = -1;
+        this.defaultLongVal = -1;
+        this.defaultFloatVal = -1;
+        this.defaultBoolVal = false;
+      } else if (defaultVal instanceof Integer) {
+        this.valClass = Integer.class;
+        this.valType = VarType.INT;
+        this.defaultStrVal = null;
+        this.defaultIntVal = (Integer)defaultVal;
+        this.defaultLongVal = -1;
+        this.defaultFloatVal = -1;
+        this.defaultBoolVal = false;
+      } else if (defaultVal instanceof Long) {
+        this.valClass = Long.class;
+        this.valType = VarType.LONG;
+        this.defaultStrVal = null;
+        this.defaultIntVal = -1;
+        this.defaultLongVal = (Long)defaultVal;
+        this.defaultFloatVal = -1;
+        this.defaultBoolVal = false;
+      } else if (defaultVal instanceof Float) {
+        this.valClass = Float.class;
+        this.valType = VarType.FLOAT;
+        this.defaultStrVal = null;
+        this.defaultIntVal = -1;
+        this.defaultLongVal = -1;
+        this.defaultFloatVal = (Float)defaultVal;
+        this.defaultBoolVal = false;
+      } else if (defaultVal instanceof Boolean) {
+        this.valClass = Boolean.class;
+        this.valType = VarType.BOOLEAN;
+        this.defaultStrVal = null;
+        this.defaultIntVal = -1;
+        this.defaultLongVal = -1;
+        this.defaultFloatVal = -1;
+        this.defaultBoolVal = (Boolean)defaultVal;
+      } else {
+        throw new IllegalArgumentException("Not supported type value " + defaultVal.getClass() +
+            " for name " + varname);
+      }
+    }
+
+    public boolean isType(String value) {
+      return valType.isType(value);
+    }
+
+    public Validator getValidator() {
+      return validator;
+    }
+
+    public String validate(String value) {
+      return validator == null ? null : validator.validate(value);
+    }
+
+    public String validatorDescription() {
+      return validator == null ? null : validator.toDescription();
+    }
+
+    public String typeString() {
+      String type = valType.typeString();
+      if (valType == VarType.STRING && validator != null) {
+        if (validator instanceof TimeValidator) {
+          type += "(TIME)";
+        }
+      }
+      return type;
+    }
+
+    public String getRawDescription() {
+      return description;
+    }
+
+    public String getDescription() {
+      String validator = validatorDescription();
+      if (validator != null) {
+        return validator + ".\n" + description;
+      }
+      return description;
+    }
+
+    public boolean isExcluded() {
+      return excluded;
+    }
+
+    public boolean isCaseSensitive() {
+      return caseSensitive;
+    }
+
+    @Override
+    public String toString() {
+      return varname;
+    }
+
+    private static String findHadoopBinary() {
+      String val = findHadoopHome();
+      // if can't find hadoop home we can at least try /usr/bin/hadoop
+      val = (val == null ? File.separator + "usr" : val)
+          + File.separator + "bin" + File.separator + "hadoop";
+      // Launch hadoop command file on windows.
+      return val;
+    }
+
+    private static String findYarnBinary() {
+      String val = findHadoopHome();
+      val = (val == null ? "yarn" : val + File.separator + "bin" + File.separator + "yarn");
+      return val;
+    }
+
+    private static String findHadoopHome() {
+      String val = System.getenv("HADOOP_HOME");
+      // In Hadoop 1.X and Hadoop 2.X HADOOP_HOME is gone and replaced with HADOOP_PREFIX
+      if (val == null) {
+        val = System.getenv("HADOOP_PREFIX");
+      }
+      return val;
+    }
+
+    public String getDefaultValue() {
+      return valType.defaultValueString(this);
+    }
+
+    public String getDefaultExpr() {
+      return defaultExpr;
+    }
+
+    private Set<String> getValidStringValues() {
+      if (validator == null || !(validator instanceof StringSet)) {
+        throw new RuntimeException(varname + " does not specify a list of valid values");
+      }
+      return ((StringSet)validator).getExpected();
+    }
+
+    enum VarType {
+      STRING {
+        @Override
+        void checkType(String value) throws Exception { }
+        @Override
+        String defaultValueString(ConfVars confVar) { return confVar.defaultStrVal; }
+      },
+      INT {
+        @Override
+        void checkType(String value) throws Exception { Integer.valueOf(value); }
+      },
+      LONG {
+        @Override
+        void checkType(String value) throws Exception { Long.valueOf(value); }
+      },
+      FLOAT {
+        @Override
+        void checkType(String value) throws Exception { Float.valueOf(value); }
+      },
+      BOOLEAN {
+        @Override
+        void checkType(String value) throws Exception { Boolean.valueOf(value); }
+      };
+
+      boolean isType(String value) {
+        try { checkType(value); } catch (Exception e) { return false; }
+        return true;
+      }
+      String typeString() { return name().toUpperCase();}
+      String defaultValueString(ConfVars confVar) { return confVar.defaultExpr; }
+      abstract void checkType(String value) throws Exception;
+    }
+  }
+
+  /**
+   * Writes the default ConfVars out to a byte array and returns an input
+   * stream wrapping that byte array.
+   *
+   * We need this in order to initialize the ConfVar properties
+   * in the underling Configuration object using the addResource(InputStream)
+   * method.
+   *
+   * It is important to use a LoopingByteArrayInputStream because it turns out
+   * addResource(InputStream) is broken since Configuration tries to read the
+   * entire contents of the same InputStream repeatedly without resetting it.
+   * LoopingByteArrayInputStream has special logic to handle this.
+   */
+  private static synchronized InputStream getConfVarInputStream() {
+    if (confVarByteArray == null) {
+      try {
+        // Create a Hadoop configuration without inheriting default settings.
+        Configuration conf = new Configuration(false);
+
+        applyDefaultNonNullConfVars(conf);
+
+        ByteArrayOutputStream confVarBaos = new ByteArrayOutputStream();
+        conf.writeXml(confVarBaos);
+        confVarByteArray = confVarBaos.toByteArray();
+      } catch (Exception e) {
+        // We're pretty screwed if we can't load the default conf vars
+        throw new RuntimeException("Failed to initialize default Hive configuration variables!", e);
+      }
+    }
+    return new LoopingByteArrayInputStream(confVarByteArray);
+  }
+
+  public void verifyAndSet(String name, String value) throws IllegalArgumentException {
+    if (modWhiteListPattern != null) {
+      Matcher wlMatcher = modWhiteListPattern.matcher(name);
+      if (!wlMatcher.matches()) {
+        throw new IllegalArgumentException("Cannot modify " + name + " at runtime. "
+            + "It is not in list of params that are allowed to be modified at runtime");
+      }
+    }
+    if (restrictList.contains(name)) {
+      throw new IllegalArgumentException("Cannot modify " + name + " at runtime. It is in the list"
+          + " of parameters that can't be modified at runtime");
+    }
+    String oldValue = name != null ? get(name) : null;
+    if (name == null || value == null || !value.equals(oldValue)) {
+      // When either name or value is null, the set method below will fail,
+      // and throw IllegalArgumentException
+      set(name, value);
+      if (isSparkRelatedConfig(name)) {
+        isSparkConfigUpdated = true;
+      }
+    }
+  }
+
+  public boolean isHiddenConfig(String name) {
+    return hiddenSet.contains(name);
+  }
+
+  /**
+   * check whether spark related property is updated, which includes spark configurations,
+   * RSC configurations and yarn configuration in Spark on YARN mode.
+   * @param name
+   * @return
+   */
+  private boolean isSparkRelatedConfig(String name) {
+    boolean result = false;
+    if (name.startsWith("spark")) { // Spark property.
+      // for now we don't support changing spark app name on the fly
+      result = !name.equals("spark.app.name");
+    } else if (name.startsWith("yarn")) { // YARN property in Spark on YARN mode.
+      String sparkMaster = get("spark.master");
+      if (sparkMaster != null && sparkMaster.startsWith("yarn")) {
+        result = true;
+      }
+    } else if (name.startsWith("hive.spark")) { // Remote Spark Context property.
+      result = true;
+    } else if (name.equals("mapreduce.job.queuename")) {
+      // a special property starting with mapreduce that we would also like to effect if it changes
+      result = true;
+    }
+
+    return result;
+  }
+
+  public static int getIntVar(Configuration conf, ConfVars var) {
+    assert (var.valClass == Integer.class) : var.varname;
+    if (var.altName != null) {
+      return conf.getInt(var.varname, conf.getInt(var.altName, var.defaultIntVal));
+    }
+    return conf.getInt(var.varname, var.defaultIntVal);
+  }
+
+  public static void setIntVar(Configuration conf, ConfVars var, int val) {
+    assert (var.valClass == Integer.class) : var.varname;
+    conf.setInt(var.varname, val);
+  }
+
+  public int getIntVar(ConfVars var) {
+    return getIntVar(this, var);
+  }
+
+  public void setIntVar(ConfVars var, int val) {
+    setIntVar(this, var, val);
+  }
+
+  public static long getTimeVar(Configuration conf, ConfVars var, TimeUnit outUnit) {
+    return toTime(getVar(conf, var), getDefaultTimeUnit(var), outUnit);
+  }
+
+  public static void setTimeVar(Configuration conf, ConfVars var, long time, TimeUnit timeunit) {
+    assert (var.valClass == String.class) : var.varname;
+    conf.set(var.varname, time + stringFor(timeunit));
+  }
+
+  public long getTimeVar(ConfVars var, TimeUnit outUnit) {
+    return getTimeVar(this, var, outUnit);
+  }
+
+  public void setTimeVar(ConfVars var, long time, TimeUnit outUnit) {
+    setTimeVar(this, var, time, outUnit);
+  }
+
+  public static long getSizeVar(Configuration conf, ConfVars var) {
+    return toSizeBytes(getVar(conf, var));
+  }
+
+  public long getSizeVar(ConfVars var) {
+    return getSizeVar(this, var);
+  }
+
+  private static TimeUnit getDefaultTimeUnit(ConfVars var) {
+    TimeUnit inputUnit = null;
+    if (var.validator instanceof TimeValidator) {
+      inputUnit = ((TimeValidator)var.validator).getTimeUnit();
+    }
+    return inputUnit;
+  }
+
+  public static long toTime(String value, TimeUnit inputUnit, TimeUnit outUnit) {
+    String[] parsed = parseNumberFollowedByUnit(value.trim());
+    return outUnit.convert(Long.parseLong(parsed[0].trim()), unitFor(parsed[1].trim(), inputUnit));
+  }
+
+  public static long toSizeBytes(String value) {
+    String[] parsed = parseNumberFollowedByUnit(value.trim());
+    return Long.parseLong(parsed[0].trim()) * multiplierFor(parsed[1].trim());
+  }
+
+  private static String[] parseNumberFollowedByUnit(String value) {
+    char[] chars = value.toCharArray();
+    int i = 0;
+    for (; i < chars.length && (chars[i] == '-' || Character.isDigit(chars[i])); i++) {
+    }
+    return new String[] {value.substring(0, i), value.substring(i)};
+  }
+
+  public static TimeUnit unitFor(String unit, TimeUnit defaultUnit) {
+    unit = unit.trim().toLowerCase();
+    if (unit.isEmpty() || unit.equals("l")) {
+      if (defaultUnit == null) {
+        throw new IllegalArgumentException("Time unit is not specified");
+      }
+      return defaultUnit;
+    } else if (unit.equals("d") || unit.startsWith("day")) {
+      return TimeUnit.DAYS;
+    } else if (unit.equals("h") || unit.startsWith("hour")) {
+      return TimeUnit.HOURS;
+    } else if (unit.equals("m") || unit.startsWith("min")) {
+      return TimeUnit.MINUTES;
+    } else if (unit.equals("s") || unit.startsWith("sec")) {
+      return TimeUnit.SECONDS;
+    } else if (unit.equals("ms") || unit.startsWith("msec")) {
+      return TimeUnit.MILLISECONDS;
+    } else if (unit.equals("us") || unit.startsWith("usec")) {
+      return TimeUnit.MICROSECONDS;
+    } else if (unit.equals("ns") || unit.startsWith("nsec")) {
+      return TimeUnit.NANOSECONDS;
+    }
+    throw new IllegalArgumentException("Invalid time unit " + unit);
+  }
+
+
+  public static long multiplierFor(String unit) {
+    unit = unit.trim().toLowerCase();
+    if (unit.isEmpty() || unit.equals("b") || unit.equals("bytes")) {
+      return 1;
+    } else if (unit.equals("kb")) {
+      return 1024;
+    } else if (unit.equals("mb")) {
+      return 1024*1024;
+    } else if (unit.equals("gb")) {
+      return 1024*1024*1024;
+    } else if (unit.equals("tb")) {
+      return 1024*1024*1024*1024;
+    } else if (unit.equals("pb")) {
+      return 1024*1024*1024*1024*1024;
+    }
+    throw new IllegalArgumentException("Invalid size unit " + unit);
+  }
+
+  public static String stringFor(TimeUnit timeunit) {
+    switch (timeunit) {
+      case DAYS: return "day";
+      case HOURS: return "hour";
+      case MINUTES: return "min";
+      case SECONDS: return "sec";
+      case MILLISECONDS: return "msec";
+      case MICROSECONDS: return "usec";
+      case NANOSECONDS: return "nsec";
+    }
+    throw new IllegalArgumentException("Invalid timeunit " + timeunit);
+  }
+
+  public static long getLongVar(Configuration conf, ConfVars var) {
+    assert (var.valClass == Long.class) : var.varname;
+    if (var.altName != null) {
+      return conf.getLong(var.varname, conf.getLong(var.altName, var.defaultLongVal));
+    }
+    return conf.getLong(var.varname, var.defaultLongVal);
+  }
+
+  public static long getLongVar(Configuration conf, ConfVars var, long defaultVal) {
+    if (var.altName != null) {
+      return conf.getLong(var.varname, conf.getLong(var.altName, defaultVal));
+    }
+    return conf.getLong(var.varname, defaultVal);
+  }
+
+  public static void setLongVar(Configuration conf, ConfVars var, long val) {
+    assert (var.valClass == Long.class) : var.varname;
+    conf.setLong(var.varname, val);
+  }
+
+  public long getLongVar(ConfVars var) {
+    return getLongVar(this, var);
+  }
+
+  public void setLongVar(ConfVars var, long val) {
+    setLongVar(this, var, val);
+  }
+
+  public static float getFloatVar(Configuration conf, ConfVars var) {
+    assert (var.valClass == Float.class) : var.varname;
+    if (var.altName != null) {
+      return conf.getFloat(var.varname, conf.getFloat(var.altName, var.defaultFloatVal));
+    }
+    return conf.getFloat(var.varname, var.defaultFloatVal);
+  }
+
+  public static float getFloatVar(Configuration conf, ConfVars var, float defaultVal) {
+    if (var.altName != null) {
+      return conf.getFloat(var.varname, conf.getFloat(var.altName, defaultVal));
+    }
+    return conf.getFloat(var.varname, defaultVal);
+  }
+
+  public static void setFloatVar(Configuration conf, ConfVars var, float val) {
+    assert (var.valClass == Float.class) : var.varname;
+    conf.setFloat(var.varname, val);
+  }
+
+  public float getFloatVar(ConfVars var) {
+    return getFloatVar(this, var);
+  }
+
+  public void setFloatVar(ConfVars var, float val) {
+    setFloatVar(this, var, val);
+  }
+
+  public static boolean getBoolVar(Configuration conf, ConfVars var) {
+    assert (var.valClass == Boolean.class) : var.varname;
+    if (var.altName != null) {
+      return conf.getBoolean(var.varname, conf.getBoolean(var.altName, var.defaultBoolVal));
+    }
+    return conf.getBoolean(var.varname, var.defaultBoolVal);
+  }
+
+  public static boolean getBoolVar(Configuration conf, ConfVars var, boolean defaultVal) {
+    if (var.altName != null) {
+      return conf.getBoolean(var.varname, conf.getBoolean(var.altName, defaultVal));
+    }
+    return conf.getBoolean(var.varname, defaultVal);
+  }
+
+  public static void setBoolVar(Configuration conf, ConfVars var, boolean val) {
+    assert (var.valClass == Boolean.class) : var.varname;
+    conf.setBoolean(var.varname, val);
+  }
+
+  public boolean getBoolVar(ConfVars var) {
+    return getBoolVar(this, var);
+  }
+
+  public void setBoolVar(ConfVars var, boolean val) {
+    setBoolVar(this, var, val);
+  }
+
+  public static String getVar(Configuration conf, ConfVars var) {
+    assert (var.valClass == String.class) : var.varname;
+    return var.altName != null ? conf.get(var.varname, conf.get(var.altName, var.defaultStrVal))
+      : conf.get(var.varname, var.defaultStrVal);
+  }
+
+  public static String getVarWithoutType(Configuration conf, ConfVars var) {
+    return var.altName != null ? conf.get(var.varname, conf.get(var.altName, var.defaultExpr))
+      : conf.get(var.varname, var.defaultExpr);
+  }
+
+  public static String getTrimmedVar(Configuration conf, ConfVars var) {
+    assert (var.valClass == String.class) : var.varname;
+    if (var.altName != null) {
+      return conf.getTrimmed(var.varname, conf.getTrimmed(var.altName, var.defaultStrVal));
+    }
+    return conf.getTrimmed(var.varname, var.defaultStrVal);
+  }
+
+  public static String[] getTrimmedStringsVar(Configuration conf, ConfVars var) {
+    assert (var.valClass == String.class) : var.varname;
+    String[] result = conf.getTrimmedStrings(var.varname, (String[])null);
+    if (result != null) return result;
+    if (var.altName != null) {
+      result = conf.getTrimmedStrings(var.altName, (String[])null);
+      if (result != null) return result;
+    }
+    return org.apache.hadoop.util.StringUtils.getTrimmedStrings(var.defaultStrVal);
+  }
+
+  public static String getVar(Configuration conf, ConfVars var, String defaultVal) {
+    String ret = var.altName != null ? conf.get(var.varname, conf.get(var.altName, defaultVal))
+      : conf.get(var.varname, defaultVal);
+    return ret;
+  }
+
+  public static String getVar(Configuration conf, ConfVars var, EncoderDecoder<String, String> encoderDecoder) {
+    return encoderDecoder.decode(getVar(conf, var));
+  }
+
+  public String getLogIdVar(String defaultValue) {
+    String retval = getVar(ConfVars.HIVE_LOG_TRACE_ID);
+    if (retval.equals("")) {
+      l4j.info("Using the default value passed in for log id: " + defaultValue);
+      retval = defaultValue;
+    }
+    if (retval.length() > LOG_PREFIX_LENGTH) {
+      l4j.warn("The original log id prefix is " + retval + " has been truncated to "
+          + retval.substring(0, LOG_PREFIX_LENGTH - 1));
+      retval = retval.substring(0, LOG_PREFIX_LENGTH - 1);
+    }
+    return retval;
+  }
+
+  public static void setVar(Configuration conf, ConfVars var, String val) {
+    assert (var.valClass == String.class) : var.varname;
+    conf.set(var.varname, val);
+  }
+  public static void setVar(Configuration conf, ConfVars var, String val,
+    EncoderDecoder<String, String> encoderDecoder) {
+    setVar(conf, var, encoderDecoder.encode(val));
+  }
+
+  public static ConfVars getConfVars(String name) {
+    return vars.get(name);
+  }
+
+  public static ConfVars getMetaConf(String name) {
+    return metaConfs.get(name);
+  }
+
+  public String getVar(ConfVars var) {
+    return getVar(this, var);
+  }
+
+  public void setVar(ConfVars var, String val) {
+    setVar(this, var, val);
+  }
+
+  public String getQueryString() {
+    return getQueryString(this);
+  }
+
+  public static String getQueryString(Configuration conf) {
+    return getVar(conf, ConfVars.HIVEQUERYSTRING, EncoderDecoderFactory.URL_ENCODER_DECODER);
+  }
+
+  public void setQueryString(String query) {
+    setQueryString(this, query);
+  }
+
+  public static void setQueryString(Configuration conf, String query) {
+    setVar(conf, ConfVars.HIVEQUERYSTRING, query, EncoderDecoderFactory.URL_ENCODER_DECODER);
+  }
+  public void logVars(PrintStream ps) {
+    for (ConfVars one : ConfVars.values()) {
+      ps.println(one.varname + "=" + ((get(one.varname) != null) ? get(one.varname) : ""));
+    }
+  }
+
+  public HiveConf() {
+    super();
+    initialize(this.getClass());
+  }
+
+  public HiveConf(Class<?> cls) {
+    super();
+    initialize(cls);
+  }
+
+  public HiveConf(Configuration other, Class<?> cls) {
+    super(other);
+    initialize(cls);
+  }
+
+  /**
+   * Copy constructor
+   */
+  public HiveConf(HiveConf other) {
+    super(other);
+    hiveJar = other.hiveJar;
+    auxJars = other.auxJars;
+    isSparkConfigUpdated = other.isSparkConfigUpdated;
+    origProp = (Properties)other.origProp.clone();
+    restrictList.addAll(other.restrictList);
+    hiddenSet.addAll(other.hiddenSet);
+    modWhiteListPattern = other.modWhiteListPattern;
+  }
+
+  public Properties getAllProperties() {
+    return getProperties(this);
+  }
+
+  public static Properties getProperties(Configuration conf) {
+    Iterator<Map.Entry<String, String>> iter = conf.iterator();
+    Properties p = new Properties();
+    while (iter.hasNext()) {
+      Map.Entry<String, String> e = iter.next();
+      p.setProperty(e.getKey(), e.getValue());
+    }
+    return p;
+  }
+
+  private void initialize(Class<?> cls) {
+    hiveJar = (new JobConf(cls)).getJar();
+
+    // preserve the original configuration
+    origProp = getAllProperties();
+
+    // Overlay the ConfVars. Note that this ignores ConfVars with null values
+    addResource(getConfVarInputStream());
+
+    // Overlay hive-site.xml if it exists
+    if (hiveSiteURL != null) {
+      addResource(hiveSiteURL);
+    }
+
+    // if embedded metastore is to be used as per config so far
+    // then this is considered like the metastore server case
+    String msUri = this.getVar(HiveConf.ConfVars.METASTOREURIS);
+    if(HiveConfUtil.isEmbeddedMetaStore(msUri)){
+      setLoadMetastoreConfig(true);
+    }
+
+    // load hivemetastore-site.xml if this is metastore and file exists
+    if (isLoadMetastoreConfig() && hivemetastoreSiteUrl != null) {
+      addResource(hivemetastoreSiteUrl);
+    }
+
+    // load hiveserver2-site.xml if this is hiveserver2 and file exists
+    // metastore can be embedded within hiveserver2, in such cases
+    // the conf params in hiveserver2-site.xml will override whats defined
+    // in hivemetastore-site.xml
+    if (isLoadHiveServer2Config() && hiveServer2SiteUrl != null) {
+      addResource(hiveServer2SiteUrl);
+    }
+
+    // Overlay the values of any system properties whose names appear in the list of ConfVars
+    applySystemProperties();
+
+    if ((this.get("hive.metastore.ds.retry.attempts") != null) ||
+      this.get("hive.metastore.ds.retry.interval") != null) {
+        l4j.warn("DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  " +
+        "Use hive.hmshandler.retry.* instead");
+    }
+
+    // if the running class was loaded directly (through eclipse) rather than through a
+    // jar then this would be needed
+    if (hiveJar == null) {
+      hiveJar = this.get(ConfVars.HIVEJAR.varname);
+    }
+
+    if (auxJars == null) {
+      auxJars = StringUtils.join(FileUtils.getJarFilesByPath(this.get(ConfVars.HIVEAUXJARS.varname), this), ',');
+    }
+
+    if (getBoolVar(ConfVars.METASTORE_SCHEMA_VERIFICATION)) {
+      setBoolVar(ConfVars.METASTORE_AUTO_CREATE_ALL, false);
+    }
+
+    if (getBoolVar(HiveConf.ConfVars.HIVECONFVALIDATION)) {
+      List<String> trimmed = new ArrayList<String>();
+      for (Map.Entry<String,String> entry : this) {
+        String key = entry.getKey();
+        if (key == null || !key.startsWith("hive.")) {
+          continue;
+        }
+        ConfVars var = HiveConf.getConfVars(key);
+        if (var == null) {
+          var = HiveConf.getConfVars(key.trim());
+          if (var != null) {
+            trimmed.add(key);
+          }
+        }
+        if (var == null) {
+          l4j.warn("HiveConf of name " + key + " does not exist");
+        } else if (!var.isType(entry.getValue())) {
+          l4j.warn("HiveConf " + var.varname + " expects " + var.typeString() + " type value");
+        }
+      }
+      for (String key : trimmed) {
+        set(key.trim(), getRaw(key));
+        unset(key);
+      }
+    }
+
+    setupSQLStdAuthWhiteList();
+
+    // setup list of conf vars that are not allowed to change runtime
+    setupRestrictList();
+    hiddenSet.clear();
+    hiddenSet.addAll(HiveConfUtil.getHiddenSet(this));
+  }
+
+  /**
+   * If the config whitelist param for sql standard authorization is not set, set it up here.
+   */
+  private void setupSQLStdAuthWhiteList() {
+    String whiteListParamsStr = getVar(ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST);
+    if (whiteListParamsStr == null || whiteListParamsStr.trim().isEmpty()) {
+      // set the default configs in whitelist
+      whiteListParamsStr = getSQLStdAuthDefaultWhiteListPattern();
+    }
+    setVar(ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST, whiteListParamsStr);
+  }
+
+  private static String getSQLStdAuthDefaultWhiteListPattern() {
+    // create the default white list from list of safe config params
+    // and regex list
+    String confVarPatternStr = Joiner.on("|").join(convertVarsToRegex(sqlStdAuthSafeVarNames));
+    String regexPatternStr = Joiner.on("|").join(sqlStdAuthSafeVarNameRegexes);
+    return regexPatternStr + "|" + confVarPatternStr;
+  }
+
+  /**
+   * @param paramList  list of parameter strings
+   * @return list of parameter strings with "." replaced by "\."
+   */
+  private static String[] convertVarsToRegex(String[] paramList) {
+    String[] regexes = new String[paramList.length];
+    for(int i=0; i<paramList.length; i++) {
+      regexes[i] = paramList[i].replace(".", "\\." );
+    }
+    return regexes;
+  }
+
+  /**
+   * Default list of modifiable config parameters for sql standard authorization
+   * For internal use only.
+   */
+  private static final String [] sqlStdAuthSafeVarNames = new String [] {
+    ConfVars.AGGR_JOIN_TRANSPOSE.varname,
+    ConfVars.BYTESPERREDUCER.varname,
+    ConfVars.CLIENT_STATS_COUNTERS.varname,
+    ConfVars.DEFAULTPARTITIONNAME.varname,
+    ConfVars.DROPIGNORESNONEXISTENT.varname,
+    ConfVars.HIVECOUNTERGROUP.varname,
+    ConfVars.HIVEDEFAULTMANAGEDFILEFORMAT.varname,
+    ConfVars.HIVEENFORCEBUCKETMAPJOIN.varname,
+    ConfVars.HIVEENFORCESORTMERGEBUCKETMAPJOIN.varname,
+    ConfVars.HIVEEXPREVALUATIONCACHE.varname,
+    ConfVars.HIVEQUERYRESULTFILEFORMAT.varname,
+    ConfVars.HIVEHASHTABLELOADFACTOR.varname,
+    ConfVars.HIVEHASHTABLETHRESHOLD.varname,
+    ConfVars.HIVEIGNOREMAPJOINHINT.varname,
+    ConfVars.HIVELIMITMAXROWSIZE.varname,
+    ConfVars.HIVEMAPREDMODE.varname,
+    ConfVars.HIVEMAPSIDEAGGREGATE.varname,
+    ConfVars.HIVEOPTIMIZEMETADATAQUERIES.varname,
+    ConfVars.HIVEROWOFFSET.varname,
+    ConfVars.HIVEVARIABLESUBSTITUTE.varname,
+    ConfVars.HIVEVARIABLESUBSTITUTEDEPTH.varname,
+    ConfVars.HIVE_AUTOGEN_COLUMNALIAS_PREFIX_INCLUDEFUNCNAME.varname,
+    ConfVars.HIVE_AUTOGEN_COLUMNALIAS_PREFIX_LABEL.varname,
+    ConfVars.HIVE_CHECK_CROSS_PRODUCT.varname,
+    ConfVars.HIVE_CLI_TEZ_SESSION_ASYNC.varname,
+    ConfVars.HIVE_COMPAT.varname,
+    ConfVars.HIVE_CONCATENATE_CHECK_INDEX.varname,
+    ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY.varname,
+    ConfVars.HIVE_ERROR_ON_EMPTY_PARTITION.varname,
+    ConfVars.HIVE_EXECUTION_ENGINE.varname,
+    ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE.varname,
+    ConfVars.HIVE_EXIM_URI_SCHEME_WL.varname,
+    ConfVars.HIVE_FILE_MAX_FOOTER.varname,
+    ConfVars.HIVE_INSERT_INTO_MULTILEVEL_DIRS.varname,
+    ConfVars.HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS.varname,
+    ConfVars.HIVE_MULTI_INSERT_MOVE_TASKS_SHARE_DEPENDENCIES.varname,
+    ConfVars.HIVE_QUOTEDID_SUPPORT.varname,
+    ConfVars.HIVE_RESULTSET_USE_UNIQUE_COLUMN_NAMES.varname,
+    ConfVars.HIVE_STATS_COLLECT_PART_LEVEL_STATS.varname,
+    ConfVars.HIVE_SCHEMA_EVOLUTION.varname,
+    ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LEVEL.varname,
+    ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_SERIALIZE_IN_TASKS.varname,
+    ConfVars.HIVE_SUPPORT_SPECICAL_CHARACTERS_IN_TABLE_NAMES.varname,
+    ConfVars.JOB_DEBUG_CAPTURE_STACKTRACES.varname,
+    ConfVars.JOB_DEBUG_TIMEOUT.varname,
+    ConfVars.LLAP_IO_ENABLED.varname,
+    ConfVars.LLAP_IO_USE_FILEID_PATH.varname,
+    ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname,
+    ConfVars.LLAP_EXECUTION_MODE.varname,
+    ConfVars.LLAP_AUTO_ALLOW_UBER.varname,
+    ConfVars.LLAP_AUTO_ENFORCE_TREE.varname,
+    ConfVars.LLAP_AUTO_ENFORCE_VECTORIZED.varname,
+    ConfVars.LLAP_AUTO_ENFORCE_STATS.varname,
+    ConfVars.LLAP_AUTO_MAX_INPUT.varname,
+    ConfVars.LLAP_AUTO_MAX_OUTPUT.varname,
+    ConfVars.LLAP_SKIP_COMPILE_UDF_CHECK.varname,
+    ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS.varname,
+    ConfVars.LLAP_ENABLE_GRACE_JOIN_IN_LLAP.varname,
+    ConfVars.LLAP_ALLOW_PERMANENT_FNS.varname,
+    ConfVars.MAXCREATEDFILES.varname,
+    ConfVars.MAXREDUCERS.varname,
+    ConfVars.NWAYJOINREORDER.varname,
+    ConfVars.OUTPUT_FILE_EXTENSION.varname,
+    ConfVars.SHOW_JOB_FAIL_DEBUG_INFO.varname,
+    ConfVars.TASKLOG_DEBUG_TIMEOUT.varname,
+    ConfVars.HIVEQUERYID.varname,
+  };
+
+  /**
+   * Default list of regexes for config parameters that are modifiable with
+   * sql standard authorization enabled
+   */
+  static final String [] sqlStdAuthSafeVarNameRegexes = new String [] {
+    "hive\\.auto\\..*",
+    "hive\\.cbo\\..*",
+    "hive\\.convert\\..*",
+    "hive\\.exec\\.dynamic\\.partition.*",
+    "hive\\.exec\\.max\\.dynamic\\.partitions.*",
+    "hive\\.exec\\.compress\\..*",
+    "hive\\.exec\\.infer\\..*",
+    "hive\\.exec\\.mode.local\\..*",
+    "hive\\.exec\\.orc\\..*",
+    "hive\\.exec\\.parallel.*",
+    "hive\\.explain\\..*",
+    "hive\\.fetch.task\\..*",
+    "hive\\.groupby\\..*",
+    "hive\\.hbase\\..*",
+    "hive\\.index\\..*",
+    "hive\\.index\\..*",
+    "hive\\.intermediate\\..*",
+    "hive\\.join\\..*",
+    "hive\\.limit\\..*",
+    "hive\\.log\\..*",
+    "hive\\.mapjoin\\..*",
+    "hive\\.merge\\..*",
+    "hive\\.optimize\\..*",
+    "hive\\.orc\\..*",
+    "hive\\.outerjoin\\..*",
+    "hive\\.parquet\\..*",
+    "hive\\.ppd\\..*",
+    "hive\\.prewarm\\..*",
+    "hive\\.server2\\.thrift\\.resultset\\.default\\.fetch\\.size",
+    "hive\\.server2\\.proxy\\.user",
+    "hive\\.skewjoin\\..*",
+    "hive\\.smbjoin\\..*",
+    "hive\\.stats\\..*",
+    "hive\\.strict\\..*",
+    "hive\\.tez\\..*",
+    "hive\\.vectorized\\..*",
+    "mapred\\.map\\..*",
+    "mapred\\.reduce\\..*",
+    "mapred\\.output\\.compression\\.codec",
+    "mapred\\.job\\.queuename",
+    "mapred\\.output\\.compression\\.type",
+    "mapred\\.min\\.split\\.size",
+    "mapreduce\\.job\\.reduce\\.slowstart\\.completedmaps",
+    "mapreduce\\.job\\.queuename",
+    "mapreduce\\.job\\.tags",
+    "mapreduce\\.input\\.fileinputformat\\.split\\.minsize",
+    "mapreduce\\.map\\..*",
+    "mapreduce\\.reduce\\..*",
+    "mapreduce\\.output\\.fileoutputformat\\.compress\\.codec",
+    "mapreduce\\.output\\.fileoutputformat\\.compress\\.type",
+    "oozie\\..*",
+    "tez\\.am\\..*",
+    "tez\\.task\\..*",
+    "tez\\.runtime\\..*",
+    "tez\\.queue\\.name",
+
+  };
+
+
+
+  /**
+   * Apply system properties to this object if the property name is defined in ConfVars
+   * and the value is non-null and not an empty string.
+   */
+  private void applySystemProperties() {
+    Map<String, String> systemProperties = getConfSystemProperties();
+    for (Entry<String, String> systemProperty : systemProperties.entrySet()) {
+      this.set(systemProperty.getKey(), systemProperty.getValue());
+    }
+  }
+
+  /**
+   * This method returns a mapping from config variable name to its value for all config variables
+   * which have been set using System properties
+   */
+  public static Map<String, String> getConfSystemProperties() {
+    Map<String, String> systemProperties = new HashMap<String, String>();
+
+    for (ConfVars oneVar : ConfVars.values()) {
+      if (System.getProperty(oneVar.varname) != null) {
+        if (System.getProperty(oneVar.varname).length() > 0) {
+          systemProperties.put(oneVar.varname, System.getProperty(oneVar.varname));
+        }
+      }
+    }
+
+    return systemProperties;
+  }
+
+  /**
+   * Overlays ConfVar properties with non-null values
+   */
+  private static void applyDefaultNonNullConfVars(Configuration conf) {
+    for (ConfVars var : ConfVars.values()) {
+      String defaultValue = var.getDefaultValue();
+      if (defaultValue == null) {
+        // Don't override ConfVars with null values
+        continue;
+      }
+      conf.set(var.varname, defaultValue);
+    }
+  }
+
+  public Properties getChangedProperties() {
+    Properties ret = new Properties();
+    Properties newProp = getAllProperties();
+
+    for (Object one : newProp.keySet()) {
+      String oneProp = (String) one;
+      String oldValue = origProp.getProperty(oneProp);
+      if (!StringUtils.equals(oldValue, newProp.getProperty(oneProp))) {
+        ret.setProperty(oneProp, newProp.getProperty(oneProp));
+      }
+    }
+    return (ret);
+  }
+
+  public String getJar() {
+    return hiveJar;
+  }
+
+  /**
+   * @return the auxJars
+   */
+  public String getAuxJars() {
+    return auxJars;
+  }
+
+  /**
+   * Set the auxiliary jars. Used for unit tests only.
+   * @param auxJars the auxJars to set.
+   */
+  public void setAuxJars(String auxJars) {
+    this.auxJars = auxJars;
+    setVar(this, ConfVars.HIVEAUXJARS, auxJars);
+  }
+
+  public URL getHiveDefaultLocation() {
+    return hiveDefaultURL;
+  }
+
+  public static void setHiveSiteLocation(URL location) {
+    hiveSiteURL = location;
+  }
+
+  public static URL getHiveSiteLocation() {
+    return hiveSiteURL;
+  }
+
+  public static URL getMetastoreSiteLocation() {
+    return hivemetastoreSiteUrl;
+  }
+
+  public static URL getHiveServer2SiteLocation() {
+    return hiveServer2SiteUrl;
+  }
+
+  /**
+   * @return the user name set in hadoop.job.ugi param or the current user from System
+   * @throws IOException
+   */
+  public String getUser() throws IOException {
+    try {
+      UserGroupInformation ugi = Utils.getUGI();
+      return ugi.getUserName();
+    } catch (LoginException le) {
+      throw new IOException(le);
+    }
+  }
+
+  public static String getColumnInternalName(int pos) {
+    return "_col" + pos;
+  }
+
+  public static int getPositionFromInternalName(String internalName) {
+    Pattern internalPattern = Pattern.compile("_col([0-9]+)");
+    Matcher m = internalPattern.matcher(internalName);
+    if (!m.matches()){
+      return -1;
+    } else {
+      return Integer.parseInt(m.group(1));
+    }
+  }
+
+  /**
+   * Append comma separated list of config vars to the restrict List
+   * @param restrictListStr
+   */
+  public void addToRestrictList(String restrictListStr) {
+    if (restrictListStr == null) {
+      return;
+    }
+    String oldList = this.getVar(ConfVars.HIVE_CONF_RESTRICTED_LIST);
+    if (oldList == null || oldList.isEmpty()) {
+      this.setVar(ConfVars.HIVE_CONF_RESTRICTED_LIST, restrictListStr);
+    } else {
+      this.setVar(ConfVars.HIVE_CONF_RESTRICTED_LIST, oldList + "," + restrictListStr);
+    }
+    setupRestrictList();
+  }
+
+  /**
+   * Set white list of parameters that are allowed to be modified
+   *
+   * @param paramNameRegex
+   */
+  @LimitedPrivate(value = { "Currently only for use by HiveAuthorizer" })
+  public void setModifiableWhiteListRegex(String paramNameRegex) {
+    if (paramNameRegex == null) {
+      return;
+    }
+    modWhiteListPattern = Pattern.compile(paramNameRegex);
+  }
+
+  /**
+   * Add the HIVE_CONF_RESTRICTED_LIST values to restrictList,
+   * including HIVE_CONF_RESTRICTED_LIST itself
+   */
+  private void setupRestrictList() {
+    String restrictListStr = this.getVar(ConfVars.HIVE_CONF_RESTRICTED_LIST);
+    restrictList.clear();
+    if (restrictListStr != null) {
+      for (String entry : restrictListStr.split(",")) {
+        restrictList.add(entry.trim());
+      }
+    }
+
+    String internalVariableListStr = this.getVar(ConfVars.HIVE_CONF_INTERNAL_VARIABLE_LIST);
+    if (internalVariableListStr != null) {
+      for (String entry : internalVariableListStr.split(",")) {
+        restrictList.add(entry.trim());
+      }
+    }
+
+    restrictList.add(ConfVars.HIVE_IN_TEST.varname);
+    restrictList.add(ConfVars.HIVE_CONF_RESTRICTED_LIST.varname);
+    restrictList.add(ConfVars.HIVE_CONF_HIDDEN_LIST.varname);
+    restrictList.add(ConfVars.HIVE_CONF_INTERNAL_VARIABLE_LIST.varname);
+  }
+
+  /**
+   * Strips hidden config entries from configuration
+   */
+  public void stripHiddenConfigurations(Configuration conf) {
+    HiveConfUtil.stripConfigurations(conf, hiddenSet);
+  }
+
+  /**
+   * @return true if HS2 webui is enabled
+   */
+  public boolean isWebUiEnabled() {
+    return this.getIntVar(ConfVars.HIVE_SERVER2_WEBUI_PORT) != 0;
+  }
+
+  /**
+   * @return true if HS2 webui query-info cache is enabled
+   */
+  public boolean isWebUiQueryInfoCacheEnabled() {
+    return isWebUiEnabled() && this.getIntVar(ConfVars.HIVE_SERVER2_WEBUI_MAX_HISTORIC_QUERIES) > 0;
+  }
+
+
+  public static boolean isLoadMetastoreConfig() {
+    return loadMetastoreConfig;
+  }
+
+  public static void setLoadMetastoreConfig(boolean loadMetastoreConfig) {
+    HiveConf.loadMetastoreConfig = loadMetastoreConfig;
+  }
+
+  public static boolean isLoadHiveServer2Config() {
+    return loadHiveServer2Config;
+  }
+
+  public static void setLoadHiveServer2Config(boolean loadHiveServer2Config) {
+    HiveConf.loadHiveServer2Config = loadHiveServer2Config;
+  }
+
+  public static class StrictChecks {
+
+    private static final String NO_LIMIT_MSG = makeMessage(
+        "Order by-s without limit", ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY);
+    private static final String NO_PARTITIONLESS_MSG = makeMessage(
+        "Queries against partitioned tables without a partition filter",
+        ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY);
+    private static final String NO_COMPARES_MSG = makeMessage(
+        "Unsafe compares between different types", ConfVars.HIVE_STRICT_CHECKS_TYPE_SAFETY);
+    private static final String NO_CARTESIAN_MSG = makeMessage(
+        "Cartesian products", ConfVars.HIVE_STRICT_CHECKS_CARTESIAN);
+    private static final String NO_BUCKETING_MSG = makeMessage(
+        "Load into bucketed tables", ConfVars.HIVE_STRICT_CHECKS_BUCKETING);
+
+    private static String makeMessage(String what, ConfVars setting) {
+      return what + " are disabled for safety reasons. If you know what you are doing, please set"
+          + setting.varname + " to false and that " + ConfVars.HIVEMAPREDMODE.varname + " is not"
+          + " set to 'strict' to proceed. Note that if you may get errors or incorrect results if"
+          + " you make a mistake while using some of the unsafe features.";
+    }
+
+    public static String checkNoLimit(Configuration conf) {
+      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY) ? null : NO_LIMIT_MSG;
+    }
+
+    public static String checkNoPartitionFilter(Configuration conf) {
+      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY)
+          ? null : NO_PARTITIONLESS_MSG;
+    }
+
+    public static String checkTypeSafety(Configuration conf) {
+      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_TYPE_SAFETY) ? null : NO_COMPARES_MSG;
+    }
+
+    public static String checkCartesian(Configuration conf) {
+      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_CARTESIAN) ? null : NO_CARTESIAN_MSG;
+    }
+
+    public static String checkBucketing(Configuration conf) {
+      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_BUCKETING) ? null : NO_BUCKETING_MSG;
+    }
+
+    private static boolean isAllowed(Configuration conf, ConfVars setting) {
+      String mode = HiveConf.getVar(conf, ConfVars.HIVEMAPREDMODE, (String)null);
+      return (mode != null) ? !"strict".equals(mode) : !HiveConf.getBoolVar(conf, setting);
+    }
+  }
+
+  public static String getNonMrEngines() {
+    String result = "";
+    for (String s : ConfVars.HIVE_EXECUTION_ENGINE.getValidStringValues()) {
+      if ("mr".equals(s)) continue;
+      if (!result.isEmpty()) {
+        result += ", ";
+      }
+      result += s;
+    }
+    return result;
+  }
+
+  public static String generateMrDeprecationWarning() {
+    return "Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. "
+        + "Consider using a different execution engine (i.e. " + HiveConf.getNonMrEngines()
+        + ") or using Hive 1.X releases.";
+  }
+
+  private static final Object reverseMapLock = new Object();
+  private static HashMap<String, ConfVars> reverseMap = null;
+
+  public static HashMap<String, ConfVars> getOrCreateReverseMap() {
+    // This should be called rarely enough; for now it's ok to just lock every time.
+    synchronized (reverseMapLock) {
+      if (reverseMap != null) return reverseMap;
+    }
+    HashMap<String, ConfVars> vars = new HashMap<>();
+    for (ConfVars val : ConfVars.values()) {
+      vars.put(val.varname.toLowerCase(), val);
+      if (val.altName != null && !val.altName.isEmpty()) {
+        vars.put(val.altName.toLowerCase(), val);
+      }
+    }
+    synchronized (reverseMapLock) {
+      if (reverseMap != null) return reverseMap;
+      reverseMap = vars;
+      return reverseMap;
+    }
+  }
+}
diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index ac62dcc2b6..0aadee3e3d 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -623,6 +623,7 @@ minillaplocal.query.files=acid_globallimit.q,\
   union_remove_26.q,\
   union_top_level.q,\
   vector_auto_smb_mapjoin_14.q,\
+  vector_complex_all.q,\
   vector_decimal_2.q,\
   vector_decimal_udf.q,\
   vector_groupby_cube1.q,\
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvg.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvg.txt
index 46cbb5ba46..a463373b76 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvg.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvg.txt
@@ -30,6 +30,7 @@ import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -38,6 +39,8 @@ import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 
+import com.google.common.base.Preconditions;
+
 /**
  * Generated from template VectorUDAFAvg.txt.
  */
@@ -46,7 +49,7 @@ import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectIn
 public class <ClassName> extends VectorAggregateExpression {
 
     private static final long serialVersionUID = 1L;
-    
+
     /** class for storing the current aggregate value. */
     static class Aggregation implements AggregationBuffer {
 
@@ -59,10 +62,10 @@ public class <ClassName> extends VectorAggregateExpression {
       * Value is explicitly (re)initialized in reset()
       */
       transient private boolean isNull = true;
-      
-      public void sumValue(<ValueType> value) {
+
+      public void avgValue(<ValueType> value) {
         if (isNull) {
-          sum = value; 
+          sum = value;
           count = 1;
           isNull = false;
         } else {
@@ -75,7 +78,7 @@ public class <ClassName> extends VectorAggregateExpression {
       public int getVariableSize() {
         throw new UnsupportedOperationException();
       }
-      
+
       @Override
       public void reset () {
         isNull = true;
@@ -83,44 +86,65 @@ public class <ClassName> extends VectorAggregateExpression {
         count = 0L;
       }
     }
-    
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
 
+#IF PARTIAL1
     transient private Object[] partialResult;
     transient private LongWritable resultCount;
     transient private DoubleWritable resultSum;
+    transient private <CamelCaseValueType>Writable resultInput;
     transient private StructObjectInspector soi;
-        
-    public <ClassName>(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    transient private DoubleWritable fullResult;
+    transient private ObjectInspector oi;
+#ENDIF COMPLETE
+
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+#IF PARTIAL1
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL1);
+#ENDIF PARTIAL1
+#IF COMPLETE
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.COMPLETE);
+#ENDIF COMPLETE
     }
 
-    public <ClassName>() {
-      super();
-      partialResult = new Object[2];
+    private void init() {
+#IF PARTIAL1
+      partialResult = new Object[3];
       resultCount = new LongWritable();
       resultSum = new DoubleWritable();
+      resultInput = new <CamelCaseValueType>Writable();
       partialResult[0] = resultCount;
       partialResult[1] = resultSum;
+      partialResult[2] = resultInput;
       initPartialResultInspector();
+#ENDIF PARTIAL1
+#IF COMPLETE
+      fullResult = new DoubleWritable();
+      initFullResultInspector();
+#ENDIF COMPLETE
     }
 
+#IF PARTIAL1
     private void initPartialResultInspector() {
         List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
         foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
         foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
+        foi.add(PrimitiveObjectInspectorFactory.writable<CamelCaseValueType>ObjectInspector);
         List<String> fname = new ArrayList<String>();
         fname.add("count");
         fname.add("sum");
+        fname.add("input");
         soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
     }
-    
+#ENDIF PARTIAL1
+#IF COMPLETE
+    private void initFullResultInspector() {
+      oi = PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
+    }
+#ENDIF COMPLETE
+
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
         int bufferIndex,
@@ -129,21 +153,21 @@ public class <ClassName> extends VectorAggregateExpression {
       Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(bufferIndex);
       return myagg;
     }
-    
+
     @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int bufferIndex, 
+      int bufferIndex,
       VectorizedRowBatch batch) throws HiveException {
-      
+
       int batchSize = batch.size;
-      
+
       if (batchSize == 0) {
         return;
       }
-      
+
       inputExpression.evaluate(batch);
-      
+
        <InputColumnVectorType> inputVector = ( <InputColumnVectorType>)batch.
         cols[this.inputExpression.getOutputColumn()];
       <ValueType>[] vector = inputVector.vector;
@@ -197,12 +221,12 @@ public class <ClassName> extends VectorAggregateExpression {
 
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets, 
+          aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValue(value);
+        myagg.avgValue(value);
       }
-    } 
+    }
 
     private void iterateNoNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
@@ -210,13 +234,13 @@ public class <ClassName> extends VectorAggregateExpression {
       <ValueType>[] values,
       int[] selection,
       int batchSize) {
-      
+
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets, 
+          aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValue(values[selection[i]]);
+        myagg.avgValue(values[selection[i]]);
       }
     }
 
@@ -227,10 +251,10 @@ public class <ClassName> extends VectorAggregateExpression {
       int batchSize) {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets, 
+          aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValue(values[i]);
+        myagg.avgValue(values[i]);
       }
     }
 
@@ -245,15 +269,15 @@ public class <ClassName> extends VectorAggregateExpression {
       if (isNull[0]) {
         return;
       }
-      
+
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValue(value);
+        myagg.avgValue(value);
       }
-      
+
     }
 
     private void iterateHasNullsRepeatingWithAggregationSelection(
@@ -272,7 +296,7 @@ public class <ClassName> extends VectorAggregateExpression {
           aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValue(value);
+        myagg.avgValue(value);
       }
     }
 
@@ -288,10 +312,10 @@ public class <ClassName> extends VectorAggregateExpression {
         int i = selection[j];
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets, 
+            aggregationBufferSets,
             bufferIndex,
             j);
-          myagg.sumValue(values[i]);
+          myagg.avgValue(values[i]);
         }
       }
    }
@@ -306,68 +330,64 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets, 
+            aggregationBufferSets,
             bufferIndex,
             i);
-          myagg.sumValue(values[i]);
+          myagg.avgValue(values[i]);
         }
       }
    }
 
-    
     @Override
-    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch) 
+    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
         throws HiveException {
-        
-        inputExpression.evaluate(batch);
-        
-        <InputColumnVectorType> inputVector = 
-            (<InputColumnVectorType>)batch.cols[this.inputExpression.getOutputColumn()];
-        
-        int batchSize = batch.size;
-        
-        if (batchSize == 0) {
-          return;
-        }
-        
-        Aggregation myagg = (Aggregation)agg;
-  
-        <ValueType>[] vector = inputVector.vector;
-        
-        if (inputVector.isRepeating) {
-          if (inputVector.noNulls) {
-            if (myagg.isNull) {
-              myagg.isNull = false;
-              myagg.sum = 0;
-              myagg.count = 0;
-            }
-            myagg.sum += vector[0]*batchSize;
-            myagg.count += batchSize;
+
+      inputExpression.evaluate(batch);
+
+      <InputColumnVectorType> inputVector =
+          (<InputColumnVectorType>)batch.cols[this.inputExpression.getOutputColumn()];
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      Aggregation myagg = (Aggregation)agg;
+
+      <ValueType>[] vector = inputVector.vector;
+
+      if (inputVector.isRepeating) {
+        if (inputVector.noNulls) {
+          if (myagg.isNull) {
+            myagg.isNull = false;
+            myagg.sum = 0;
+            myagg.count = 0;
           }
-          return;
-        }
-        
-        if (!batch.selectedInUse && inputVector.noNulls) {
-          iterateNoSelectionNoNulls(myagg, vector, batchSize);
-        }
-        else if (!batch.selectedInUse) {
-          iterateNoSelectionHasNulls(myagg, vector, batchSize, inputVector.isNull);
-        }
-        else if (inputVector.noNulls){
-          iterateSelectionNoNulls(myagg, vector, batchSize, batch.selected);
-        }
-        else {
-          iterateSelectionHasNulls(myagg, vector, batchSize, inputVector.isNull, batch.selected);
+          myagg.sum += vector[0]*batchSize;
+          myagg.count += batchSize;
         }
+        return;
+      }
+
+      if (!batch.selectedInUse && inputVector.noNulls) {
+        iterateNoSelectionNoNulls(myagg, vector, batchSize);
+      } else if (!batch.selectedInUse) {
+        iterateNoSelectionHasNulls(myagg, vector, batchSize, inputVector.isNull);
+      } else if (inputVector.noNulls){
+        iterateSelectionNoNulls(myagg, vector, batchSize, batch.selected);
+      } else {
+        iterateSelectionHasNulls(myagg, vector, batchSize, inputVector.isNull, batch.selected);
+      }
     }
-  
+
     private void iterateSelectionHasNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
+        Aggregation myagg,
+        <ValueType>[] vector,
         int batchSize,
-        boolean[] isNull, 
+        boolean[] isNull,
         int[] selected) {
-      
+
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
@@ -384,17 +404,17 @@ public class <ClassName> extends VectorAggregateExpression {
     }
 
     private void iterateSelectionNoNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
-        int batchSize, 
+        Aggregation myagg,
+        <ValueType>[] vector,
+        int batchSize,
         int[] selected) {
-      
+
       if (myagg.isNull) {
         myagg.isNull = false;
         myagg.sum = 0;
         myagg.count = 0;
       }
-      
+
       for (int i=0; i< batchSize; ++i) {
         <ValueType> value = vector[selected[i]];
         myagg.sum += value;
@@ -403,15 +423,15 @@ public class <ClassName> extends VectorAggregateExpression {
     }
 
     private void iterateNoSelectionHasNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
+        Aggregation myagg,
+        <ValueType>[] vector,
         int batchSize,
         boolean[] isNull) {
-      
+
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
           <ValueType> value = vector[i];
-          if (myagg.isNull) { 
+          if (myagg.isNull) {
             myagg.isNull = false;
             myagg.sum = 0;
             myagg.count = 0;
@@ -423,15 +443,15 @@ public class <ClassName> extends VectorAggregateExpression {
     }
 
     private void iterateNoSelectionNoNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
+        Aggregation myagg,
+        <ValueType>[] vector,
         int batchSize) {
       if (myagg.isNull) {
         myagg.isNull = false;
         myagg.sum = 0;
         myagg.count = 0;
       }
-      
+
       for (int i=0;i<batchSize;++i) {
         <ValueType> value = vector[i];
         myagg.sum += value;
@@ -456,19 +476,29 @@ public class <ClassName> extends VectorAggregateExpression {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
         return null;
-      }
-      else {
-        assert(0 < myagg.count);
+      } else {
+        Preconditions.checkState(myagg.count > 0);
+#IF PARTIAL1
         resultCount.set (myagg.count);
         resultSum.set (myagg.sum);
         return partialResult;
+#ENDIF PARTIAL1
+#IF COMPLETE
+        fullResult.set (myagg.sum / myagg.count);
+        return fullResult;
+#ENDIF COMPLETE
       }
     }
-    
+
   @Override
-    public ObjectInspector getOutputObjectInspector() {
+  public ObjectInspector getOutputObjectInspector() {
+#IF PARTIAL1
     return soi;
-  }     
+#ENDIF PARTIAL1
+#IF COMPLETE
+    return oi;
+#ENDIF COMPLETE
+  }
 
   @Override
   public long getAggregationBufferFixedSize() {
@@ -481,15 +511,6 @@ public class <ClassName> extends VectorAggregateExpression {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
-    // No-op
+    init();
   }
-  
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
-  }
-}
-
+}
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimal.txt
similarity index 81%
rename from ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java
rename to ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimal.txt
index 4aac9d3442..fa7b7c7570 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimal.txt
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates;
+package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen;
 
 import java.util.ArrayList;
 import java.util.List;
@@ -33,6 +33,8 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
@@ -41,13 +43,16 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+
+import com.google.common.base.Preconditions;
 
 /**
  * Generated from template VectorUDAFAvg.txt.
  */
 @Description(name = "avg",
     value = "_FUNC_(AVG) - Returns the average value of expr (vectorized, type: decimal)")
-public class VectorUDAFAvgDecimal extends VectorAggregateExpression {
+public class <ClassName> extends VectorAggregateExpression {
 
     private static final long serialVersionUID = 1L;
 
@@ -60,7 +65,7 @@ static class Aggregation implements AggregationBuffer {
       transient private long count;
       transient private boolean isNull;
 
-      public void sumValueWithNullCheck(HiveDecimalWritable writable) {
+      public void avgValue(HiveDecimalWritable writable) {
         if (isNull) {
           // Make a copy since we intend to mutate sum.
           sum.set(writable);
@@ -74,7 +79,7 @@ public void sumValueWithNullCheck(HiveDecimalWritable writable) {
         }
       }
 
-      public void sumValueNoNullCheck(HiveDecimalWritable writable) {
+      public void avgValueNoNullCheck(HiveDecimalWritable writable) {
         sum.mutateAdd(writable);
         count++;
       }
@@ -92,69 +97,96 @@ public void reset() {
       }
     }
 
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
+#IF PARTIAL1
     transient private Object[] partialResult;
     transient private LongWritable resultCount;
     transient private HiveDecimalWritable resultSum;
+    transient private HiveDecimalWritable resultInput;
     transient private StructObjectInspector soi;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    transient private HiveDecimalWritable tempDecWritable;
+    transient private HiveDecimalWritable fullResult;
+    transient private ObjectInspector oi;
+#ENDIF COMPLETE
 
     /**
      * The scale of the SUM in the partial output
      */
-    private short sumScale;
+    private int sumScale;
 
     /**
      * The precision of the SUM in the partial output
      */
-    private short sumPrecision;
+    private int sumPrecision;
 
     /**
      * the scale of the input expression
      */
-    private short inputScale;
+    private int inputScale;
 
     /**
      * the precision of the input expression
      */
-    private short inputPrecision;
-
-    public VectorUDAFAvgDecimal(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+    private int inputPrecision;
+
+    public <ClassName>(VectorExpression inputExpression,
+        GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+#IF PARTIAL1
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL1);
+#ENDIF PARTIAL1
+#IF COMPLETE
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.COMPLETE);
+#ENDIF COMPLETE
     }
 
-    public VectorUDAFAvgDecimal() {
-      super();
-      partialResult = new Object[2];
+    private void init() {
+#IF PARTIAL1
+      partialResult = new Object[3];
       resultCount = new LongWritable();
       resultSum = new HiveDecimalWritable();
+      resultInput = new HiveDecimalWritable(0L);
       partialResult[0] = resultCount;
       partialResult[1] = resultSum;
-
+      partialResult[2] = resultInput;
+#ENDIF PARTIAL1
+#IF COMPLETE
+      tempDecWritable = new HiveDecimalWritable();
+      fullResult = new HiveDecimalWritable();
+#ENDIF COMPLETE
     }
 
+#IF PARTIAL1
     private void initPartialResultInspector() {
+#ENDIF PARTIAL1
+#IF COMPLETE
+    private void initFullResultInspector() {
+#ENDIF COMPLETE
       // the output type of the vectorized partial aggregate must match the
       // expected type for the row-mode aggregation
       // For decimal, the type is "same number of integer digits and 4 more decimal digits"
-      
-      DecimalTypeInfo dtiSum = GenericUDAFAverage.deriveSumFieldTypeInfo(inputPrecision, inputScale);
-      this.sumScale = (short) dtiSum.scale();
-      this.sumPrecision = (short) dtiSum.precision();
-      
+
+      DecimalTypeInfo decTypeInfo =
+          GenericUDAFAverageEvaluatorDecimal.deriveResultDecimalTypeInfo(
+              inputPrecision, inputScale, mode);
+      this.sumScale = decTypeInfo.scale();
+      this.sumPrecision = decTypeInfo.precision();
+
+#IF PARTIAL1
       List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
       foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
-      foi.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(dtiSum));
+      foi.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(decTypeInfo));
+      foi.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(decTypeInfo));
       List<String> fname = new ArrayList<String>();
       fname.add("count");
       fname.add("sum");
+      fname.add("input");
       soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
+#ENDIF PARTIAL1
+#IF COMPLETE
+      oi = PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(decTypeInfo);
+#ENDIF COMPLETE
     }
 
     private Aggregation getCurrentAggregationBuffer(
@@ -180,7 +212,7 @@ public void aggregateInputSelection(
 
       inputExpression.evaluate(batch);
 
-       DecimalColumnVector inputVector = ( DecimalColumnVector)batch.
+       DecimalColumnVector inputVector = (DecimalColumnVector) batch.
         cols[this.inputExpression.getOutputColumn()];
       HiveDecimalWritable[] vector = inputVector.vector;
 
@@ -236,7 +268,7 @@ private void iterateNoNullsRepeatingWithAggregationSelection(
           aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValueWithNullCheck(value);
+        myagg.avgValue(value);
       }
     }
 
@@ -252,7 +284,7 @@ private void iterateNoNullsSelectionWithAggregationSelection(
           aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValueWithNullCheck(values[selection[i]]);
+        myagg.avgValue(values[selection[i]]);
       }
     }
 
@@ -266,7 +298,7 @@ private void iterateNoNullsWithAggregationSelection(
           aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValueWithNullCheck(values[i]);
+        myagg.avgValue(values[i]);
       }
     }
 
@@ -287,7 +319,7 @@ private void iterateHasNullsRepeatingSelectionWithAggregationSelection(
           aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValueWithNullCheck(value);
+        myagg.avgValue(value);
       }
 
     }
@@ -308,7 +340,7 @@ private void iterateHasNullsRepeatingWithAggregationSelection(
           aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValueWithNullCheck(value);
+        myagg.avgValue(value);
       }
     }
 
@@ -327,7 +359,7 @@ private void iterateHasNullsSelectionWithAggregationSelection(
             aggregationBufferSets,
             bufferIndex,
             j);
-          myagg.sumValueWithNullCheck(values[i]);
+          myagg.avgValue(values[i]);
         }
       }
    }
@@ -345,7 +377,7 @@ private void iterateHasNullsWithAggregationSelection(
             aggregationBufferSets,
             bufferIndex,
             i);
-          myagg.sumValueWithNullCheck(values[i]);
+          myagg.avgValue(values[i]);
         }
       }
    }
@@ -409,7 +441,7 @@ private void iterateSelectionHasNulls(
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
-          myagg.sumValueWithNullCheck(vector[i]);
+          myagg.avgValue(vector[i]);
         }
       }
     }
@@ -427,7 +459,7 @@ private void iterateSelectionNoNulls(
       }
 
       for (int i=0; i< batchSize; ++i) {
-        myagg.sumValueNoNullCheck(vector[selected[i]]);
+        myagg.avgValueNoNullCheck(vector[selected[i]]);
       }
     }
 
@@ -439,7 +471,7 @@ private void iterateNoSelectionHasNulls(
 
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
-          myagg.sumValueWithNullCheck(vector[i]);
+          myagg.avgValue(vector[i]);
         }
       }
     }
@@ -455,7 +487,7 @@ private void iterateNoSelectionNoNulls(
       }
 
       for (int i=0;i<batchSize;++i) {
-        myagg.sumValueNoNullCheck(vector[i]);
+        myagg.avgValueNoNullCheck(vector[i]);
       }
     }
 
@@ -479,16 +511,30 @@ public Object evaluateOutput(
         return null;
       }
       else {
-        assert(0 < myagg.count);
+        Preconditions.checkState(myagg.count > 0);
+#IF PARTIAL1
         resultCount.set (myagg.count);
         resultSum.set(myagg.sum);
         return partialResult;
+#ENDIF PARTIAL1
+#IF COMPLETE
+        tempDecWritable.setFromLong (myagg.count);
+        fullResult.set(myagg.sum);
+        fullResult.mutateDivide(tempDecWritable);
+        fullResult.mutateEnforcePrecisionScale(sumPrecision, sumScale);
+        return fullResult;
+#ENDIF COMPLETE
       }
     }
 
   @Override
     public ObjectInspector getOutputObjectInspector() {
+#IF PARTIAL1
     return soi;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    return oi;
+#ENDIF COMPLETE
   }
 
   @Override
@@ -502,20 +548,19 @@ public long getAggregationBufferFixedSize() {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
+    init();
+
     ExprNodeDesc inputExpr = desc.getParameters().get(0);
     DecimalTypeInfo tiInput = (DecimalTypeInfo) inputExpr.getTypeInfo();
-    this.inputScale = (short) tiInput.scale();
-    this.inputPrecision = (short) tiInput.precision();
+    this.inputScale = tiInput.scale();
+    this.inputPrecision = tiInput.precision();
 
+#IF PARTIAL1
     initPartialResultInspector();
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    initFullResultInspector();
+#ENDIF COMPLETE
   }
 }
 
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimalMerge.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimalMerge.txt
new file mode 100644
index 0000000000..071efc915d
--- /dev/null
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimalMerge.txt
@@ -0,0 +1,597 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
+import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal;
+import org.apache.hadoop.hive.ql.util.JavaDataModel;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+
+import com.google.common.base.Preconditions;
+
+/**
+ * Generated from template VectorUDAFAvg.txt.
+ */
+@Description(name = "avg",
+    value = "_FUNC_(AVG) - Returns the average value of expr (vectorized, type: decimal)")
+public class <ClassName> extends VectorAggregateExpression {
+
+    private static final long serialVersionUID = 1L;
+
+    /** class for storing the current aggregate value. */
+    static class Aggregation implements AggregationBuffer {
+
+      private static final long serialVersionUID = 1L;
+
+      transient private final HiveDecimalWritable mergeSum = new HiveDecimalWritable();
+      transient private long mergeCount;
+      transient private boolean isNull;
+
+      public void merge(long count, HiveDecimalWritable sum) {
+        if (isNull) {
+          // Make a copy since we intend to mutate sum.
+          mergeCount = count;
+          mergeSum.set(sum);
+          isNull = false;
+        } else {
+          // Note that if sum is out of range, mutateAdd will ignore the call.
+          // At the end, sum.isSet() can be checked for null.
+          mergeCount += count;
+          mergeSum.mutateAdd(sum);
+        }
+      }
+
+      public void mergeNoNullCheck(long count, HiveDecimalWritable sum) {
+        mergeCount += count;
+        mergeSum.mutateAdd(sum);
+      }
+
+      @Override
+      public int getVariableSize() {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public void reset() {
+        isNull = true;
+        mergeCount = 0;
+        mergeSum.setFromLong(0L);
+      }
+    }
+
+#IF PARTIAL2
+    transient private Object[] partialResult;
+    transient private LongWritable resultCount;
+    transient private HiveDecimalWritable resultSum;
+    transient private HiveDecimalWritable resultInput;
+    transient private StructObjectInspector soi;
+#ENDIF PARTIAL2
+#IF FINAL
+    transient private HiveDecimalWritable tempDecWritable;
+    transient private HiveDecimalWritable fullResult;
+    transient private ObjectInspector oi;
+#ENDIF FINAL
+
+    private transient int countOffset;
+    private transient int sumOffset;
+    private transient int inputOffset;
+
+    /**
+     * The scale of the SUM in the partial output
+     */
+    private int sumScale;
+
+    /**
+     * The precision of the SUM in the partial output
+     */
+    private int sumPrecision;
+
+    /**
+     * the scale of the input expression
+     */
+    private int inputScale;
+
+    /**
+     * the precision of the input expression
+     */
+    private int inputPrecision;
+
+    public <ClassName>(VectorExpression inputExpression,
+        GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+#IF PARTIAL2
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL2);
+#ENDIF PARTIAL2
+#IF FINAL
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.FINAL);
+#ENDIF FINAL
+    }
+
+    private void init() {
+#IF PARTIAL2
+      partialResult = new Object[3];
+      resultCount = new LongWritable();
+      resultSum = new HiveDecimalWritable();
+      resultInput = new HiveDecimalWritable(0L);
+      partialResult[0] = resultCount;
+      partialResult[1] = resultSum;
+      partialResult[2] = resultInput;
+#ENDIF PARTIAL2
+#IF FINAL
+      tempDecWritable = new HiveDecimalWritable();
+      fullResult = new HiveDecimalWritable();
+#ENDIF FINAL
+    }
+
+#IF PARTIAL2
+    private void initPartialResultInspector() {
+#ENDIF PARTIAL2
+#IF FINAL
+    private void initFullResultInspector() {
+#ENDIF FINAL
+
+      // the output type of the vectorized partial aggregate must match the
+      // expected type for the row-mode aggregation
+      // For decimal, the type is "same number of integer digits and 4 more decimal digits"
+
+      DecimalTypeInfo decTypeInfo =
+          GenericUDAFAverageEvaluatorDecimal.deriveResultDecimalTypeInfo(
+               inputPrecision, inputScale, mode);
+      this.sumScale = decTypeInfo.scale();
+      this.sumPrecision = decTypeInfo.precision();
+
+#IF PARTIAL2
+      List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
+      foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
+      foi.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(decTypeInfo));
+      foi.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(decTypeInfo));
+      List<String> fname = new ArrayList<String>();
+      fname.add("count");
+      fname.add("sum");
+      fname.add("input");
+      soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
+#ENDIF PARTIAL2
+#IF FINAL
+      oi = PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(decTypeInfo);
+#ENDIF FINAL
+    }
+
+    private Aggregation getCurrentAggregationBuffer(
+        VectorAggregationBufferRow[] aggregationBufferSets,
+        int bufferIndex,
+        int row) {
+      VectorAggregationBufferRow mySet = aggregationBufferSets[row];
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(bufferIndex);
+      return myagg;
+    }
+
+    @Override
+    public void aggregateInputSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      VectorizedRowBatch batch) throws HiveException {
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      inputExpression.evaluate(batch);
+
+      StructColumnVector inputStructColVector =
+          (StructColumnVector) batch.cols[this.inputExpression.getOutputColumn()];
+      ColumnVector[] fields = inputStructColVector.fields;
+
+      long[] countVector = ((LongColumnVector) fields[countOffset]).vector;
+      HiveDecimalWritable[] sumVector = ((DecimalColumnVector) fields[sumOffset]).vector;
+
+      if (inputStructColVector.noNulls) {
+        if (inputStructColVector.isRepeating) {
+          iterateNoNullsRepeatingWithAggregationSelection(
+            aggregationBufferSets, bufferIndex,
+            countVector[0], sumVector[0], batchSize);
+        } else {
+          if (batch.selectedInUse) {
+            iterateNoNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, batch.selected, batchSize);
+          } else {
+            iterateNoNullsWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, batchSize);
+          }
+        }
+      } else {
+        if (inputStructColVector.isRepeating) {
+          if (batch.selectedInUse) {
+            iterateHasNullsRepeatingSelectionWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector[0], sumVector[0], batchSize, batch.selected, inputStructColVector.isNull);
+          } else {
+            iterateHasNullsRepeatingWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector[0], sumVector[0], batchSize, inputStructColVector.isNull);
+          }
+        } else {
+          if (batch.selectedInUse) {
+            iterateHasNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, batchSize, batch.selected, inputStructColVector.isNull);
+          } else {
+            iterateHasNullsWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, batchSize, inputStructColVector.isNull);
+          }
+        }
+      }
+    }
+
+    private void iterateNoNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long count,
+      HiveDecimalWritable sum,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(count, sum);
+      }
+    }
+
+    private void iterateNoNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      HiveDecimalWritable[] sumVector,
+      int[] selection,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        final int batchIndex = selection[i];
+        myagg.merge(countVector[batchIndex], sumVector[batchIndex]);
+      }
+    }
+
+    private void iterateNoNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      HiveDecimalWritable[] sumVector,
+      int batchSize) {
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(countVector[i], sumVector[i]);
+      }
+    }
+
+    private void iterateHasNullsRepeatingSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long count,
+      HiveDecimalWritable sum,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      if (isNull[0]) {
+        return;
+      }
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(count, sum);
+      }
+
+    }
+
+    private void iterateHasNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long count,
+      HiveDecimalWritable sum,
+      int batchSize,
+      boolean[] isNull) {
+
+      if (isNull[0]) {
+        return;
+      }
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(count, sum);
+      }
+    }
+
+    private void iterateHasNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      HiveDecimalWritable[] sumVector,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      for (int i = 0; i < batchSize; i++) {
+        final int batchIndex = selection[i];
+        if (!isNull[batchIndex]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            bufferIndex,
+            i);
+          myagg.merge(countVector[batchIndex], sumVector[batchIndex]);
+        }
+      }
+   }
+
+    private void iterateHasNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      HiveDecimalWritable[] sumVector,
+      int batchSize,
+      boolean[] isNull) {
+
+      for (int i=0; i < batchSize; ++i) {
+        if (!isNull[i]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            bufferIndex,
+            i);
+          myagg.merge(countVector[i], sumVector[i]);
+        }
+      }
+   }
+
+    @Override
+    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
+        throws HiveException {
+
+      inputExpression.evaluate(batch);
+
+      StructColumnVector inputStructColVector =
+          (StructColumnVector) batch.cols[this.inputExpression.getOutputColumn()];
+      ColumnVector[] fields = inputStructColVector.fields;
+
+      long[] countVector = ((LongColumnVector) fields[countOffset]).vector;
+      HiveDecimalWritable[] sumVector = ((DecimalColumnVector) fields[sumOffset]).vector;
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      Aggregation myagg = (Aggregation)agg;
+
+      if (inputStructColVector.isRepeating) {
+        if (inputStructColVector.noNulls) {
+          if (myagg.isNull) {
+            myagg.isNull = false;
+            myagg.mergeSum.setFromLong(0L);
+            myagg.mergeCount = 0;
+          }
+          myagg.mergeCount += countVector[0] * batchSize;
+          HiveDecimal sum = sumVector[0].getHiveDecimal();
+          HiveDecimal multiple = sum.multiply(HiveDecimal.create(batchSize));
+          myagg.mergeSum.mutateAdd(multiple);
+        }
+        return;
+      }
+
+      if (!batch.selectedInUse && inputStructColVector.noNulls) {
+        iterateNoSelectionNoNulls(myagg, countVector, sumVector, batchSize);
+      } else if (!batch.selectedInUse) {
+        iterateNoSelectionHasNulls(myagg, countVector, sumVector, batchSize, inputStructColVector.isNull);
+      } else if (inputStructColVector.noNulls){
+        iterateSelectionNoNulls(myagg, countVector, sumVector, batchSize, batch.selected);
+      } else {
+        iterateSelectionHasNulls(myagg, countVector, sumVector, batchSize, inputStructColVector.isNull, batch.selected);
+      }
+    }
+
+    private void iterateSelectionHasNulls(
+        Aggregation myagg,
+        long[] countVector,
+        HiveDecimalWritable[] sumVector,
+        int batchSize,
+        boolean[] isNull,
+        int[] selected) {
+
+      for (int i = 0; i < batchSize; i++) {
+        final int batchIndex = selected[i];
+        if (!isNull[batchIndex]) {
+          myagg.merge(countVector[batchIndex], sumVector[batchIndex]);
+        }
+      }
+    }
+
+    private void iterateSelectionNoNulls(
+        Aggregation myagg,
+        long[] countVector,
+        HiveDecimalWritable[] sumVector,
+        int batchSize,
+        int[] selected) {
+
+      if (myagg.isNull) {
+        myagg.isNull = false;
+        myagg.mergeSum.setFromLong(0L);
+        myagg.mergeCount = 0;
+      }
+
+      for (int i = 0; i< batchSize; i++) {
+        final int batchIndex = selected[i];
+        myagg.mergeNoNullCheck(countVector[batchIndex], sumVector[batchIndex]);
+      }
+    }
+
+    private void iterateNoSelectionHasNulls(
+        Aggregation myagg,
+        long[] countVector,
+        HiveDecimalWritable[] sumVector,
+        int batchSize,
+        boolean[] isNull) {
+
+      for(int i = 0; i < batchSize; i++) {
+        if (!isNull[i]) {
+          myagg.merge(countVector[i], sumVector[i]);
+        }
+      }
+    }
+
+    private void iterateNoSelectionNoNulls(
+        Aggregation myagg,
+        long[] countVector,
+        HiveDecimalWritable[] sumVector,
+        int batchSize) {
+      if (myagg.isNull) {
+        myagg.isNull = false;
+        myagg.mergeSum.setFromLong(0L);
+        myagg.mergeCount = 0;
+      }
+
+      for (int i = 0; i < batchSize; i++) {
+        myagg.mergeNoNullCheck(countVector[i], sumVector[i]);
+      }
+    }
+
+    @Override
+    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
+      return new Aggregation();
+    }
+
+    @Override
+    public void reset(AggregationBuffer agg) throws HiveException {
+      Aggregation myAgg = (Aggregation) agg;
+      myAgg.reset();
+    }
+
+    @Override
+    public Object evaluateOutput(
+        AggregationBuffer agg) throws HiveException {
+      Aggregation myagg = (Aggregation) agg;
+      // !isSet checks for overflow.
+      if (myagg.isNull || !myagg.mergeSum.isSet()) {
+        return null;
+      }
+      else {
+        Preconditions.checkState(myagg.mergeCount > 0);
+#IF PARTIAL2
+        resultCount.set (myagg.mergeCount);
+        resultSum.set(myagg.mergeSum);
+        return partialResult;
+#ENDIF PARTIAL2
+#IF FINAL
+        tempDecWritable.setFromLong (myagg.mergeCount);
+        fullResult.set(myagg.mergeSum);
+        fullResult.mutateDivide(tempDecWritable);
+        fullResult.mutateEnforcePrecisionScale(sumPrecision, sumScale);
+        return fullResult;
+#ENDIF FINAL
+      }
+    }
+
+  @Override
+    public ObjectInspector getOutputObjectInspector() {
+#IF PARTIAL2
+    return soi;
+#ENDIF PARTIAL2
+#IF FINAL
+    return oi;
+#ENDIF FINAL
+  }
+
+  @Override
+  public long getAggregationBufferFixedSize() {
+    JavaDataModel model = JavaDataModel.get();
+    return JavaDataModel.alignUp(
+      model.object() +
+      model.primitive2() * 2,
+      model.memoryAlign());
+  }
+
+  @Override
+  public void init(AggregationDesc desc) throws HiveException {
+    init();
+
+    ExprNodeDesc inputExpr = desc.getParameters().get(0);
+
+     StructTypeInfo partialStructTypeInfo = (StructTypeInfo) inputExpr.getTypeInfo();
+
+    ArrayList<String> fieldNames =  partialStructTypeInfo.getAllStructFieldNames();
+    countOffset = fieldNames.indexOf("count");
+    sumOffset = fieldNames.indexOf("sum");
+    inputOffset = fieldNames.indexOf("input");
+
+    DecimalTypeInfo tiInput = (DecimalTypeInfo) partialStructTypeInfo.getAllStructFieldTypeInfos().get(sumOffset);
+    this.inputScale = tiInput.scale();
+    this.inputPrecision = tiInput.precision();
+
+#IF PARTIAL2
+    initPartialResultInspector();
+#ENDIF PARTIAL2
+#IF FINAL
+    initFullResultInspector();
+#ENDIF FINAL
+  }
+}
+
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgMerge.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgMerge.txt
new file mode 100644
index 0000000000..996d0dcb46
--- /dev/null
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgMerge.txt
@@ -0,0 +1,547 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
+import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
+import org.apache.hadoop.hive.ql.util.JavaDataModel;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+
+import com.google.common.base.Preconditions;
+
+/**
+ * Generated from template VectorUDAFAvg.txt.
+ */
+@Description(name = "avg",
+    value = "_FUNC_(expr) - Returns the average value of expr (vectorized, type: <ValueType>)")
+public class <ClassName> extends VectorAggregateExpression {
+
+    private static final long serialVersionUID = 1L;
+
+    /** class for storing the current aggregate value. */
+    static class Aggregation implements AggregationBuffer {
+
+      private static final long serialVersionUID = 1L;
+
+      transient private long mergeCount;
+      transient private double mergeSum;
+
+      /**
+      * Value is explicitly (re)initialized in reset()
+      */
+      transient private boolean isNull = true;
+
+      public void merge(long count, double sum) {
+        if (isNull) {
+          mergeCount = count;
+          mergeSum = sum;
+          isNull = false;
+        } else {
+          mergeCount += count;
+          mergeSum += sum;
+        }
+      }
+
+      @Override
+      public int getVariableSize() {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public void reset () {
+        isNull = true;
+        mergeCount = 0L;
+        mergeSum = 0;
+      }
+    }
+
+#IF PARTIAL2
+    transient private Object[] partialResult;
+    transient private LongWritable resultCount;
+    transient private DoubleWritable resultSum;
+    transient private DoubleWritable resultInput;
+    transient private StructObjectInspector soi;
+#ENDIF PARTIAL2
+#IF FINAL
+    transient private DoubleWritable fullResult;
+
+    transient private ObjectInspector oi;
+#ENDIF FINAL
+
+    private transient int countOffset;
+    private transient int sumOffset;
+    private transient int inputOffset;
+
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+#IF PARTIAL2
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL2);
+#ENDIF PARTIAL2
+#IF FINAL
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.FINAL);
+#ENDIF FINAL
+    }
+
+    private void init() {
+#IF PARTIAL2
+      partialResult = new Object[3];
+      resultCount = new LongWritable();
+      resultSum = new DoubleWritable();
+      partialResult[0] = resultCount;
+      partialResult[1] = resultSum;
+      partialResult[2] = resultInput;
+      initPartialResultInspector();
+#ENDIF PARTIAL2
+#IF FINAL
+      fullResult = new DoubleWritable();
+      initFullResultInspector();
+#ENDIF FINAL
+    }
+
+#IF PARTIAL2
+    private void initPartialResultInspector() {
+        List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
+        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
+        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
+        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
+        List<String> fname = new ArrayList<String>();
+        fname.add("count");
+        fname.add("sum");
+        fname.add("input");
+        soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
+    }
+#ENDIF PARTIAL2
+#IF FINAL
+    private void initFullResultInspector() {
+      oi = PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
+    }
+#ENDIF FINAL
+
+    private Aggregation getCurrentAggregationBuffer(
+        VectorAggregationBufferRow[] aggregationBufferSets,
+        int bufferIndex,
+        int row) {
+      VectorAggregationBufferRow mySet = aggregationBufferSets[row];
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(bufferIndex);
+      return myagg;
+    }
+
+    @Override
+    public void aggregateInputSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      VectorizedRowBatch batch) throws HiveException {
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      inputExpression.evaluate(batch);
+
+      StructColumnVector inputStructColVector =
+          (StructColumnVector) batch.cols[this.inputExpression.getOutputColumn()];
+      ColumnVector[] fields = inputStructColVector.fields;
+
+      long[] countVector = ((LongColumnVector) fields[countOffset]).vector;
+      double[] sumVector = ((DoubleColumnVector) fields[sumOffset]).vector;
+
+      if (inputStructColVector.noNulls) {
+        if (inputStructColVector.isRepeating) {
+          iterateNoNullsRepeatingWithAggregationSelection(
+            aggregationBufferSets, bufferIndex,
+            countVector[0], sumVector[0], batchSize);
+        } else {
+          if (batch.selectedInUse) {
+            iterateNoNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, batch.selected, batchSize);
+          } else {
+            iterateNoNullsWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, batchSize);
+          }
+        }
+      } else {
+        if (inputStructColVector.isRepeating) {
+          if (batch.selectedInUse) {
+            iterateHasNullsRepeatingSelectionWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector[0], sumVector[0], batchSize, batch.selected, inputStructColVector.isNull);
+          } else {
+            iterateHasNullsRepeatingWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector[0], sumVector[0], batchSize, inputStructColVector.isNull);
+          }
+        } else {
+          if (batch.selectedInUse) {
+            iterateHasNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, batchSize, batch.selected, inputStructColVector.isNull);
+          } else {
+            iterateHasNullsWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, batchSize, inputStructColVector.isNull);
+          }
+        }
+      }
+    }
+
+    private void iterateNoNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long count,
+      double sum,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(count, sum);
+      }
+    }
+
+    private void iterateNoNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      double[] sumVector,
+      int[] selection,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        final int batchIndex = selection[i];
+        myagg.merge(countVector[batchIndex], sumVector[batchIndex]);
+      }
+    }
+
+    private void iterateNoNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      double[] sumVector,
+      int batchSize) {
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(countVector[i], sumVector[i]);
+      }
+    }
+
+    private void iterateHasNullsRepeatingSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long count,
+      double sum,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      if (isNull[0]) {
+        return;
+      }
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(count, sum);
+      }
+
+    }
+
+    private void iterateHasNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long count,
+      double sum,
+      int batchSize,
+      boolean[] isNull) {
+
+      if (isNull[0]) {
+        return;
+      }
+
+      for (int i = 0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(count, sum);
+      }
+    }
+
+    private void iterateHasNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      double[] sumVector,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      for (int i = 0; i < batchSize; i++) {
+        final int batchIndex = selection[i];
+        if (!isNull[batchIndex]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            bufferIndex,
+            i);
+          myagg.merge(countVector[batchIndex], sumVector[batchIndex]);
+        }
+      }
+   }
+
+    private void iterateHasNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      double[] sumVector,
+      int batchSize,
+      boolean[] isNull) {
+
+      for (int i=0; i < batchSize; ++i) {
+        if (!isNull[i]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            bufferIndex,
+            i);
+          myagg.merge(countVector[i], sumVector[i]);
+        }
+      }
+   }
+
+    @Override
+    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
+        throws HiveException {
+
+      inputExpression.evaluate(batch);
+
+      StructColumnVector inputStructColVector =
+          (StructColumnVector) batch.cols[this.inputExpression.getOutputColumn()];
+      ColumnVector[] fields = inputStructColVector.fields;
+
+      long[] countVector = ((LongColumnVector) fields[countOffset]).vector;
+      double[] sumVector = ((DoubleColumnVector) fields[sumOffset]).vector;
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      Aggregation myagg = (Aggregation)agg;
+
+      if (inputStructColVector.isRepeating) {
+        if (inputStructColVector.noNulls) {
+          if (myagg.isNull) {
+            myagg.isNull = false;
+            myagg.mergeCount = 0;
+            myagg.mergeSum = 0;
+          }
+          myagg.mergeCount += countVector[0] * batchSize;
+          myagg.mergeSum += sumVector[0] * batchSize;
+        }
+        return;
+      }
+
+      if (!batch.selectedInUse && inputStructColVector.noNulls) {
+        iterateNoSelectionNoNulls(myagg, countVector, sumVector, batchSize);
+      } else if (!batch.selectedInUse) {
+        iterateNoSelectionHasNulls(myagg, countVector, sumVector, batchSize, inputStructColVector.isNull);
+      } else if (inputStructColVector.noNulls){
+        iterateSelectionNoNulls(myagg, countVector, sumVector, batchSize, batch.selected);
+      } else {
+        iterateSelectionHasNulls(myagg, countVector, sumVector, batchSize, inputStructColVector.isNull, batch.selected);
+      }
+    }
+
+    private void iterateSelectionHasNulls(
+        Aggregation myagg,
+        long[] countVector,
+        double[] sumVector,
+        int batchSize,
+        boolean[] isNull,
+        int[] selected) {
+
+      for (int i=0; i < batchSize; i++) {
+        int batchIndex = selected[i];
+        if (!isNull[batchIndex]) {
+          if (myagg.isNull) {
+            myagg.isNull = false;
+            myagg.mergeCount = 0;
+            myagg.mergeSum = 0;
+          }
+          myagg.mergeCount += countVector[batchIndex];
+          myagg.mergeSum += sumVector[batchIndex];
+        }
+      }
+    }
+
+    private void iterateSelectionNoNulls(
+        Aggregation myagg,
+        long[] countVector,
+        double[] sumVector,
+        int batchSize,
+        int[] selected) {
+
+      if (myagg.isNull) {
+        myagg.isNull = false;
+        myagg.mergeCount = 0;
+        myagg.mergeSum = 0;
+      }
+
+      for (int i = 0; i< batchSize; ++i) {
+        final int batchIndex = selected[i];
+        myagg.mergeCount += countVector[batchIndex];
+        myagg.mergeSum += sumVector[batchIndex];
+      }
+    }
+
+    private void iterateNoSelectionHasNulls(
+        Aggregation myagg,
+        long[] countVector,
+        double[] sumVector,
+        int batchSize,
+        boolean[] isNull) {
+
+      for(int i = 0; i < batchSize; i++) {
+        if (!isNull[i]) {
+          if (myagg.isNull) {
+            myagg.isNull = false;
+            myagg.mergeCount = 0;
+            myagg.mergeSum = 0;
+          }
+          myagg.mergeCount += countVector[i];
+          myagg.mergeSum += sumVector[i];
+        }
+      }
+    }
+
+    private void iterateNoSelectionNoNulls(
+        Aggregation myagg,
+        long[] countVector,
+        double[] sumVector,
+        int batchSize) {
+      if (myagg.isNull) {
+        myagg.isNull = false;
+        myagg.mergeCount = 0;
+        myagg.mergeSum = 0;
+      }
+
+      for (int i=0;i<batchSize;++i) {
+        myagg.mergeCount += countVector[i];
+        myagg.mergeSum += sumVector[i];
+      }
+    }
+
+    @Override
+    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
+      return new Aggregation();
+    }
+
+    @Override
+    public void reset(AggregationBuffer agg) throws HiveException {
+      Aggregation myAgg = (Aggregation) agg;
+      myAgg.reset();
+    }
+
+    @Override
+    public Object evaluateOutput(
+        AggregationBuffer agg) throws HiveException {
+      Aggregation myagg = (Aggregation) agg;
+      if (myagg.isNull) {
+        return null;
+      }
+      else {
+        assert(0 < myagg.mergeCount);
+#IF PARTIAL2
+        resultCount.set (myagg.mergeCount);
+        resultSum.set (myagg.mergeSum);
+        return partialResult;
+#ENDIF PARTIAL2
+#IF FINAL
+        fullResult.set (myagg.mergeSum / myagg.mergeCount);
+        return fullResult;
+#ENDIF FINAL
+      }
+    }
+
+  @Override
+  public ObjectInspector getOutputObjectInspector() {
+#IF PARTIAL2
+    return soi;
+#ENDIF PARTIAL2
+#IF FINAL
+    return oi;
+#ENDIF FINAL
+  }
+
+  @Override
+  public long getAggregationBufferFixedSize() {
+    JavaDataModel model = JavaDataModel.get();
+    return JavaDataModel.alignUp(
+      model.object() +
+      model.primitive2() * 2,
+      model.memoryAlign());
+  }
+
+  @Override
+  public void init(AggregationDesc desc) throws HiveException {
+    init();
+
+    ExprNodeDesc inputExpr = desc.getParameters().get(0);
+    StructTypeInfo partialStructTypeInfo = (StructTypeInfo) inputExpr.getTypeInfo();
+
+    ArrayList<String> fieldNames =  partialStructTypeInfo.getAllStructFieldNames();
+    countOffset = fieldNames.indexOf("count");
+    sumOffset = fieldNames.indexOf("sum");
+    inputOffset = fieldNames.indexOf("input");
+  }
+}
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgTimestamp.java b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgTimestamp.txt
similarity index 87%
rename from ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgTimestamp.java
rename to ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgTimestamp.txt
index 365dcf69ad..b816a357f7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgTimestamp.java
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgTimestamp.txt
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates;
+package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen;
 
 import java.util.ArrayList;
 import java.util.List;
@@ -29,20 +29,25 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.ql.util.TimestampUtils;
+
+import com.google.common.base.Preconditions;
 
 /**
  * Generated from template VectorUDAFAvg.txt.
  */
 @Description(name = "avg",
     value = "_FUNC_(expr) - Returns the average value of expr (vectorized, type: timestamp)")
-public class VectorUDAFAvgTimestamp extends VectorAggregateExpression {
+public class <ClassName> extends VectorAggregateExpression {
 
     private static final long serialVersionUID = 1L;
 
@@ -76,49 +81,70 @@ public int getVariableSize() {
       }
 
       @Override
-      public void reset () {
+      public void reset() {
         isNull = true;
         sum = 0;
         count = 0L;
       }
     }
-    
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
 
+#IF PARTIAL1
     transient private Object[] partialResult;
     transient private LongWritable resultCount;
     transient private DoubleWritable resultSum;
+    transient private TimestampWritable resultInput;
     transient private StructObjectInspector soi;
-
-    public VectorUDAFAvgTimestamp(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    transient private DoubleWritable fullResult;
+    transient private ObjectInspector oi;
+#ENDIF COMPLETE
+
+    public <ClassName>(VectorExpression inputExpression,
+        GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+#IF PARTIAL1
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL1);
+#ENDIF PARTIAL1
+#IF COMPLETE
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.COMPLETE);
+#ENDIF COMPLETE
     }
 
-    public VectorUDAFAvgTimestamp() {
-      super();
-      partialResult = new Object[2];
+    private void init() {
+#IF PARTIAL1
+      partialResult = new Object[3];
       resultCount = new LongWritable();
       resultSum = new DoubleWritable();
+      resultInput = new TimestampWritable();
       partialResult[0] = resultCount;
       partialResult[1] = resultSum;
+      partialResult[2] = resultInput;
       initPartialResultInspector();
+#ENDIF PARTIAL1
+#IF COMPLETE
+      fullResult = new DoubleWritable();
+#ENDIF COMPLETE
     }
 
+#IF PARTIAL1
     private void initPartialResultInspector() {
         List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
         foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
         foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
+        foi.add(PrimitiveObjectInspectorFactory.writableTimestampObjectInspector);
         List<String> fname = new ArrayList<String>();
         fname.add("count");
         fname.add("sum");
+        fname.add("input");
         soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
     }
+#ENDIF PARTIAL1
+#IF COMPLETE
+    private void initFullResultInspector() {
+        oi = PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
+    }
+#ENDIF COMPLETE
 
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
@@ -264,7 +290,7 @@ private void iterateHasNullsRepeatingWithAggregationSelection(
       for (int i=0; i < batchSize; ++i) {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets, 
+            aggregationBufferSets,
             bufferIndex,
             i);
           myagg.sumValue(value);
@@ -316,7 +342,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
 
         inputExpression.evaluate(batch);
 
-        TimestampColumnVector inputColVector = 
+        TimestampColumnVector inputColVector =
             (TimestampColumnVector)batch.cols[this.inputExpression.getOutputColumn()];
 
         int batchSize = batch.size;
@@ -326,7 +352,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
         }
 
         Aggregation myagg = (Aggregation)agg;
-        
+
         if (inputColVector.isRepeating) {
           if (inputColVector.noNulls) {
             if (myagg.isNull) {
@@ -355,7 +381,7 @@ else if (inputColVector.noNulls){
     }
 
     private void iterateSelectionHasNulls(
-        Aggregation myagg, 
+        Aggregation myagg,
         TimestampColumnVector inputColVector,
         int batchSize,
         boolean[] isNull,
@@ -417,7 +443,7 @@ private void iterateNoSelectionHasNulls(
 
     private void iterateNoSelectionNoNulls(
         Aggregation myagg,
-        TimestampColumnVector inputColVector, 
+        TimestampColumnVector inputColVector,
         int batchSize) {
       if (myagg.isNull) {
         myagg.isNull = false;
@@ -452,15 +478,26 @@ public Object evaluateOutput(
       }
       else {
         assert(0 < myagg.count);
-        resultCount.set (myagg.count);
-        resultSum.set (myagg.sum);
+#IF PARTIAL1
+        resultCount.set(myagg.count);
+        resultSum.set(myagg.sum);
         return partialResult;
+#ENDIF PARTIAL1
+#IF COMPLETE
+        fullResult.set(myagg.sum / myagg.count);
+        return fullResult;
+#ENDIF COMPLETE
       }
     }
 
   @Override
-    public ObjectInspector getOutputObjectInspector() {
+  public ObjectInspector getOutputObjectInspector() {
+#IF PARTIAL1
     return soi;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    return oi;
+#ENDIF COMPLETE
   }
 
   @Override
@@ -474,15 +511,7 @@ public long getAggregationBufferFixedSize() {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
-    // No-op
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
+    init();
   }
 }
 
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMax.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMax.txt
index 2261e1b0fb..81bd64f4be 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMax.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMax.txt
@@ -22,26 +22,27 @@ import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory;    
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 
 /**
-* <ClassName>. Vectorized implementation for MIN/MAX aggregates. 
+* <ClassName>. Vectorized implementation for MIN/MAX aggregates.
 */
-@Description(name = "<DescriptionName>", 
+@Description(name = "<DescriptionName>",
     value = "<DescriptionValue>")
 public class <ClassName> extends VectorAggregateExpression {
-   
+
     private static final long serialVersionUID = 1L;
-    
-    /** 
+
+    /**
      * class for storing the current aggregate value.
      */
     static private final class Aggregation implements AggregationBuffer {
@@ -55,7 +56,7 @@ public class <ClassName> extends VectorAggregateExpression {
       */
       transient private boolean isNull = true;
 
-      public void checkValue(<ValueType> value) {
+      public void minmaxValue(<ValueType> value) {
         if (isNull) {
           isNull = false;
           this.value = value;
@@ -64,6 +65,13 @@ public class <ClassName> extends VectorAggregateExpression {
         }
       }
 
+      // The isNull check and work has already been performed.
+      public void minmaxValueNoCheck(<ValueType> value) {
+        if (value <OperatorSymbol> this.value) {
+          this.value = value;
+        }
+      }
+
       @Override
       public int getVariableSize() {
         throw new UnsupportedOperationException();
@@ -75,31 +83,24 @@ public class <ClassName> extends VectorAggregateExpression {
         value = 0;
       }
     }
-    
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
 
     private transient VectorExpressionWriter resultWriter;
-    
-    public <ClassName>(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    public <ClassName>() {
-      super();
+    private void init() {
     }
-    
+
     @Override
     public void init(AggregationDesc desc) throws HiveException {
+      init();
+
       resultWriter = VectorExpressionWriterFactory.genVectorExpressionWritable(
           desc.getParameters().get(0));
     }
-    
+
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
         int aggregrateIndex,
@@ -108,21 +109,21 @@ public class <ClassName> extends VectorAggregateExpression {
       Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregrateIndex);
       return myagg;
     }
-    
+
     @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex, 
+      int aggregrateIndex,
       VectorizedRowBatch batch) throws HiveException {
-      
+
       int batchSize = batch.size;
-      
+
       if (batchSize == 0) {
         return;
       }
-      
+
       inputExpression.evaluate(batch);
-      
+
       <InputColumnVectorType> inputVector = (<InputColumnVectorType>)batch.
         cols[this.inputExpression.getOutputColumn()];
       <ValueType>[] vector = inputVector.vector;
@@ -176,10 +177,10 @@ public class <ClassName> extends VectorAggregateExpression {
 
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets, 
+          aggregationBufferSets,
           aggregrateIndex,
           i);
-        myagg.checkValue(value);
+        myagg.minmaxValue(value);
       }
     } 
 
@@ -189,13 +190,13 @@ public class <ClassName> extends VectorAggregateExpression {
       <ValueType>[] values,
       int[] selection,
       int batchSize) {
-      
+
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets, 
+          aggregationBufferSets,
           aggregrateIndex,
           i);
-        myagg.checkValue(values[selection[i]]);
+        myagg.minmaxValue(values[selection[i]]);
       }
     }
 
@@ -206,10 +207,10 @@ public class <ClassName> extends VectorAggregateExpression {
       int batchSize) {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets, 
+          aggregationBufferSets,
           aggregrateIndex,
           i);
-        myagg.checkValue(values[i]);
+        myagg.minmaxValue(values[i]);
       }
     }
 
@@ -224,15 +225,15 @@ public class <ClassName> extends VectorAggregateExpression {
       if (isNull[0]) {
         return;
       }
-      
+
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
           aggregrateIndex,
           i);
-        myagg.checkValue(value);
+        myagg.minmaxValue(value);
       }
-      
+
     }
 
     private void iterateHasNullsRepeatingWithAggregationSelection(
@@ -251,7 +252,7 @@ public class <ClassName> extends VectorAggregateExpression {
           aggregationBufferSets,
           aggregrateIndex,
           i);
-        myagg.checkValue(value);
+        myagg.minmaxValue(value);
       }
     }
 
@@ -267,10 +268,10 @@ public class <ClassName> extends VectorAggregateExpression {
         int i = selection[j];
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets, 
+            aggregationBufferSets,
             aggregrateIndex,
             j);
-          myagg.checkValue(values[i]);
+          myagg.minmaxValue(values[i]);
         }
       }
    }
@@ -285,42 +286,40 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets, 
+            aggregationBufferSets,
             aggregrateIndex,
             i);
-          myagg.checkValue(values[i]);
+          myagg.minmaxValue(values[i]);
         }
       }
-   }
-    
+    }
+
     @Override
-    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch) 
-      throws HiveException {
-        
+    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
+        throws HiveException {
+
         inputExpression.evaluate(batch);
-        
+
         <InputColumnVectorType> inputVector = (<InputColumnVectorType>)batch.
             cols[this.inputExpression.getOutputColumn()];
-        
+
         int batchSize = batch.size;
-        
+
         if (batchSize == 0) {
           return;
         }
-        
+
         Aggregation myagg = (Aggregation)agg;
-  
+
         <ValueType>[] vector = inputVector.vector;
-        
+
         if (inputVector.isRepeating) {
-          if (inputVector.noNulls &&
-            (myagg.isNull || (vector[0] <OperatorSymbol> myagg.value))) {
-            myagg.isNull = false;
-            myagg.value = vector[0];
+          if (inputVector.noNulls) {
+            myagg.minmaxValue(vector[0]);
           }
           return;
         }
-        
+
         if (!batch.selectedInUse && inputVector.noNulls) {
           iterateNoSelectionNoNulls(myagg, vector, batchSize);
         }
@@ -334,82 +333,66 @@ public class <ClassName> extends VectorAggregateExpression {
           iterateSelectionHasNulls(myagg, vector, batchSize, inputVector.isNull, batch.selected);
         }
     }
-  
+
     private void iterateSelectionHasNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
+        Aggregation myagg,
+        <ValueType>[] vector,
         int batchSize,
-        boolean[] isNull, 
+        boolean[] isNull,
         int[] selected) {
-      
+
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
           <ValueType> value = vector[i];
-          if (myagg.isNull) {
-            myagg.isNull = false;
-            myagg.value = value;
-          }
-          else if (value <OperatorSymbol> myagg.value) {
-            myagg.value = value;
-          }
+          myagg.minmaxValue(value);
         }
       }
     }
 
     private void iterateSelectionNoNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
-        int batchSize, 
+        Aggregation myagg,
+        <ValueType>[] vector,
+        int batchSize,
         int[] selected) {
-      
+
       if (myagg.isNull) {
         myagg.value = vector[selected[0]];
         myagg.isNull = false;
       }
-      
+
       for (int i=0; i< batchSize; ++i) {
         <ValueType> value = vector[selected[i]];
-        if (value <OperatorSymbol> myagg.value) {
-          myagg.value = value;
-        }
+        myagg.minmaxValueNoCheck(value);
       }
     }
 
     private void iterateNoSelectionHasNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
+        Aggregation myagg,
+        <ValueType>[] vector,
         int batchSize,
         boolean[] isNull) {
-      
+
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
           <ValueType> value = vector[i];
-          if (myagg.isNull) { 
-            myagg.value = value;
-            myagg.isNull = false;
-          }
-          else if (value <OperatorSymbol> myagg.value) {
-            myagg.value = value;
-          }
+          myagg.minmaxValue(value);
         }
       }
     }
 
     private void iterateNoSelectionNoNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
+        Aggregation myagg,
+        <ValueType>[] vector,
         int batchSize) {
       if (myagg.isNull) {
         myagg.value = vector[0];
         myagg.isNull = false;
       }
-      
+
       for (int i=0;i<batchSize;++i) {
         <ValueType> value = vector[i];
-        if (value <OperatorSymbol> myagg.value) {
-          myagg.value = value;
-        }
+        myagg.minmaxValueNoCheck(value);
       }
     }
 
@@ -435,7 +418,7 @@ public class <ClassName> extends VectorAggregateExpression {
         return resultWriter.writeValue(myagg.value);
       }
     }
-    
+
     @Override
     public ObjectInspector getOutputObjectInspector() {
       return resultWriter.getObjectInspector();
@@ -449,13 +432,4 @@ public class <ClassName> extends VectorAggregateExpression {
       model.primitive2(),
       model.memoryAlign());
   }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
-  }
 }
-
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxDecimal.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxDecimal.txt
index 58d2d22bab..6c024f7be8 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxDecimal.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxDecimal.txt
@@ -29,6 +29,7 @@ import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -81,26 +82,19 @@ public class <ClassName> extends VectorAggregateExpression {
       }
     }
 
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
     private transient VectorExpressionWriter resultWriter;
 
-    public <ClassName>(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    public <ClassName>() {
-      super();
+    private void init() {
     }
 
     @Override
     public void init(AggregationDesc desc) throws HiveException {
+      init();
+
       resultWriter = VectorExpressionWriterFactory.genVectorExpressionWritable(
           desc.getParameters().get(0));
     }
@@ -335,14 +329,14 @@ public class <ClassName> extends VectorAggregateExpression {
           iterateNoSelectionNoNulls(myagg, vector, inputVector.scale, batchSize);
         }
         else if (!batch.selectedInUse) {
-          iterateNoSelectionHasNulls(myagg, vector, inputVector.scale, 
+          iterateNoSelectionHasNulls(myagg, vector, inputVector.scale,
             batchSize, inputVector.isNull);
         }
         else if (inputVector.noNulls){
           iterateSelectionNoNulls(myagg, vector, inputVector.scale, batchSize, batch.selected);
         }
         else {
-          iterateSelectionHasNulls(myagg, vector, inputVector.scale, 
+          iterateSelectionHasNulls(myagg, vector, inputVector.scale,
             batchSize, inputVector.isNull, batch.selected);
         }
     }
@@ -465,12 +459,4 @@ public class <ClassName> extends VectorAggregateExpression {
       model.primitive2(),
       model.memoryAlign());
   }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
-  }
 }
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt
index 515692e5d3..d12f231218 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt
@@ -28,6 +28,7 @@ import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
@@ -80,26 +81,19 @@ public class <ClassName> extends VectorAggregateExpression {
       }
     }
 
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
     private transient VectorExpressionWriter resultWriter;
 
-    public <ClassName>(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    public <ClassName>() {
-      super();
+    private void init() {
     }
 
     @Override
     public void init(AggregationDesc desc) throws HiveException {
+      init();
+
       resultWriter = VectorExpressionWriterFactory.genVectorExpressionWritable(
           desc.getParameters().get(0));
     }
@@ -448,13 +442,5 @@ public class <ClassName> extends VectorAggregateExpression {
       model.primitive2(),
       model.memoryAlign());
   }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
-  }
 }
 
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxString.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxString.txt
index c210e4c81f..d5eb712125 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxString.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxString.txt
@@ -28,6 +28,7 @@ import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -92,22 +93,13 @@ public class <ClassName> extends VectorAggregateExpression {
 
     }
 
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
     transient private Text result;
 
-    public <ClassName>(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    public <ClassName>() {
-      super();
+    private void init() {
       result = new Text();
     }
 
@@ -120,7 +112,7 @@ public class <ClassName> extends VectorAggregateExpression {
       return myagg;
     }
 
-@Override
+    @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
       int aggregrateIndex,
@@ -404,15 +396,6 @@ public class <ClassName> extends VectorAggregateExpression {
 
     @Override
     public void init(AggregationDesc desc) throws HiveException {
-      // No-op
-    }
-
-    public VectorExpression getInputExpression() {
-      return inputExpression;
-    }
-
-    public void setInputExpression(VectorExpression inputExpression) {
-      this.inputExpression = inputExpression;
+      init();
     }
 }
-
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt
index 074aefd0e1..f78de56b97 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt
@@ -30,6 +30,7 @@ import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -82,26 +83,19 @@ public class <ClassName> extends VectorAggregateExpression {
       }
     }
 
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
     private transient VectorExpressionWriter resultWriter;
 
-    public <ClassName>(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    public <ClassName>() {
-      super();
+    private void init() {
     }
 
     @Override
     public void init(AggregationDesc desc) throws HiveException {
+      init();
+
       resultWriter = VectorExpressionWriterFactory.genVectorExpressionWritable(
           desc.getParameters().get(0));
     }
@@ -450,13 +444,5 @@ public class <ClassName> extends VectorAggregateExpression {
       model.primitive2(),
       model.memoryAlign());
   }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
-  }
 }
 
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFSum.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFSum.txt
index a89ae0aceb..475d5787c9 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFSum.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFSum.txt
@@ -27,6 +27,7 @@ import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -34,15 +35,15 @@ import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 
 /**
-* <ClassName>. Vectorized implementation for SUM aggregates. 
+* <ClassName>. Vectorized implementation for SUM aggregates.
 */
 @Description(name = "sum", 
     value = "_FUNC_(expr) - Returns the sum value of expr (vectorized, type: <ValueType>)")
 public class <ClassName> extends VectorAggregateExpression {
-   
+
     private static final long serialVersionUID = 1L;
-    
-    /** 
+
+    /**
      * class for storing the current aggregate value.
      */
     private static final class Aggregation implements AggregationBuffer {
@@ -55,7 +56,7 @@ public class <ClassName> extends VectorAggregateExpression {
       * Value is explicitly (re)initialized in reset()
       */
       transient private boolean isNull = true;
-      
+
       public void sumValue(<ValueType> value) {
         if (isNull) {
           sum = value;
@@ -65,6 +66,11 @@ public class <ClassName> extends VectorAggregateExpression {
         }
       }
 
+      // The isNull check and work has already been performed.
+      public void sumValueNoCheck(<ValueType> value) {
+        sum += value;
+      }
+
       @Override
       public int getVariableSize() {
         throw new UnsupportedOperationException();
@@ -76,23 +82,14 @@ public class <ClassName> extends VectorAggregateExpression {
         sum = 0;;
       }
     }
-    
-    private VectorExpression inputExpression;
 
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
+    transient private <OutputType> result;
 
-    transient private final <OutputType> result;
-    
-    public <ClassName>(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    public <ClassName>() {
-      super();
+    private void init() {
       result = new <OutputType>();
     }
 
@@ -104,21 +101,21 @@ public class <ClassName> extends VectorAggregateExpression {
       Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
       return myagg;
     }
-    
+
     @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregateIndex, 
+      int aggregateIndex,
       VectorizedRowBatch batch) throws HiveException {
-      
+
       int batchSize = batch.size;
-      
+
       if (batchSize == 0) {
         return;
       }
-      
+
       inputExpression.evaluate(batch);
-      
+
       <InputColumnVectorType> inputVector = (<InputColumnVectorType>)batch.
         cols[this.inputExpression.getOutputColumn()];
       <ValueType>[] vector = inputVector.vector;
@@ -172,12 +169,12 @@ public class <ClassName> extends VectorAggregateExpression {
 
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets, 
+          aggregationBufferSets,
           aggregateIndex,
           i);
         myagg.sumValue(value);
       }
-    } 
+    }
 
     private void iterateNoNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
@@ -185,10 +182,10 @@ public class <ClassName> extends VectorAggregateExpression {
       <ValueType>[] values,
       int[] selection,
       int batchSize) {
-      
+
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets, 
+          aggregationBufferSets,
           aggregateIndex,
           i);
         myagg.sumValue(values[selection[i]]);
@@ -202,7 +199,7 @@ public class <ClassName> extends VectorAggregateExpression {
       int batchSize) {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets, 
+          aggregationBufferSets,
           aggregateIndex,
           i);
         myagg.sumValue(values[i]);
@@ -228,7 +225,7 @@ public class <ClassName> extends VectorAggregateExpression {
           i);
         myagg.sumValue(value);
       }
-      
+
     }
 
     private void iterateHasNullsRepeatingWithAggregationSelection(
@@ -263,7 +260,7 @@ public class <ClassName> extends VectorAggregateExpression {
         int i = selection[j];
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets, 
+            aggregationBufferSets,
             aggregateIndex,
             j);
           myagg.sumValue(values[i]);
@@ -281,45 +278,44 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets, 
+            aggregationBufferSets,
             aggregateIndex,
             i);
           myagg.sumValue(values[i]);
         }
       }
    }
-    
-    
+
     @Override
-    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch) 
-    throws HiveException {
-      
+    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
+        throws HiveException {
+
       inputExpression.evaluate(batch);
-      
+
       <InputColumnVectorType> inputVector = (<InputColumnVectorType>)batch.
           cols[this.inputExpression.getOutputColumn()];
-      
+
       int batchSize = batch.size;
-      
+
       if (batchSize == 0) {
         return;
       }
-      
+
       Aggregation myagg = (Aggregation)agg;
 
       <ValueType>[] vector = inputVector.vector;
-      
+
       if (inputVector.isRepeating) {
         if (inputVector.noNulls) {
         if (myagg.isNull) {
           myagg.isNull = false;
           myagg.sum = 0;
         }
-        myagg.sum += vector[0]*batchSize;
+        myagg.sumValueNoCheck(vector[0]*batchSize);
       }
         return;
       }
-      
+
       if (!batch.selectedInUse && inputVector.noNulls) {
         iterateNoSelectionNoNulls(myagg, vector, batchSize);
       }
@@ -333,14 +329,14 @@ public class <ClassName> extends VectorAggregateExpression {
         iterateSelectionHasNulls(myagg, vector, batchSize, inputVector.isNull, batch.selected);
       }
     }
-  
+
     private void iterateSelectionHasNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
+        Aggregation myagg,
+        <ValueType>[] vector,
         int batchSize,
-        boolean[] isNull, 
+        boolean[] isNull,
         int[] selected) {
-      
+
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
@@ -349,58 +345,58 @@ public class <ClassName> extends VectorAggregateExpression {
             myagg.isNull = false;
             myagg.sum = 0;
           }
-          myagg.sum += value;
+          myagg.sumValueNoCheck(value);
         }
       }
     }
 
     private void iterateSelectionNoNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
-        int batchSize, 
+        Aggregation myagg,
+        <ValueType>[] vector,
+        int batchSize,
         int[] selected) {
-      
+
       if (myagg.isNull) {
         myagg.sum = 0;
         myagg.isNull = false;
       }
-      
+
       for (int i=0; i< batchSize; ++i) {
         <ValueType> value = vector[selected[i]];
-        myagg.sum += value;
+        myagg.sumValueNoCheck(value);
       }
     }
 
     private void iterateNoSelectionHasNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
+        Aggregation myagg,
+        <ValueType>[] vector,
         int batchSize,
         boolean[] isNull) {
-      
+
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
           <ValueType> value = vector[i];
-          if (myagg.isNull) { 
+          if (myagg.isNull) {
             myagg.sum = 0;
             myagg.isNull = false;
           }
-          myagg.sum += value;
+          myagg.sumValueNoCheck(value);
         }
       }
     }
 
     private void iterateNoSelectionNoNulls(
-        Aggregation myagg, 
-        <ValueType>[] vector, 
+        Aggregation myagg,
+        <ValueType>[] vector,
         int batchSize) {
       if (myagg.isNull) {
         myagg.sum = 0;
         myagg.isNull = false;
       }
-      
+
       for (int i=0;i<batchSize;++i) {
         <ValueType> value = vector[i];
-        myagg.sum += value;
+        myagg.sumValueNoCheck(value);
       }
     }
 
@@ -426,7 +422,7 @@ public class <ClassName> extends VectorAggregateExpression {
         return result;
       }
     }
-    
+
     @Override
     public ObjectInspector getOutputObjectInspector() {
       return <OutputTypeInspector>;
@@ -442,15 +438,6 @@ public class <ClassName> extends VectorAggregateExpression {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
-    // No-op
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
+    init();
   }
 }
-
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVar.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVar.txt
index 1e3516b5d3..390bd02f55 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVar.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVar.txt
@@ -30,13 +30,17 @@ import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 
+import com.google.common.base.Preconditions;
+
 /**
 * <ClassName>. Vectorized implementation for VARIANCE aggregates.
 */
@@ -69,6 +73,22 @@ public class <ClassName> extends VectorAggregateExpression {
         variance = 0;
       }
 
+     public void varianceValue(double value) {
+        if (isNull) {
+          sum = value;
+          count = 1;
+          variance = 0;
+          isNull = false;
+        } else {
+          sum += value;
+          count++;
+          if (count > 1) {
+            double t = count * value - sum;
+            variance += (t * t) / ((double) count * (count - 1));
+          }
+        }
+      }
+
       @Override
       public int getVariableSize() {
         throw new UnsupportedOperationException();
@@ -83,28 +103,32 @@ public class <ClassName> extends VectorAggregateExpression {
       }
     }
 
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
+#IF PARTIAL1
     transient private LongWritable resultCount;
     transient private DoubleWritable resultSum;
     transient private DoubleWritable resultVariance;
     transient private Object[] partialResult;
 
     transient private ObjectInspector soi;
-
-
-    public <ClassName>(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    transient private DoubleWritable fullResult;
+
+    transient private ObjectInspector oi;
+#ENDIF COMPLETE
+
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+#IF PARTIAL1
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL1);
+#ENDIF PARTIAL1
+#IF COMPLETE
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.COMPLETE);
+#ENDIF COMPLETE
     }
 
-    public <ClassName>() {
-      super();
+    private void init() {
+#IF PARTIAL1
       partialResult = new Object[3];
       resultCount = new LongWritable();
       resultSum = new DoubleWritable();
@@ -113,8 +137,14 @@ public class <ClassName> extends VectorAggregateExpression {
       partialResult[1] = resultSum;
       partialResult[2] = resultVariance;
       initPartialResultInspector();
+#ENDIF PARTIAL1
+#IF COMPLETE
+      fullResult = new DoubleWritable();
+      initFullResultInspector();
+#ENDIF COMPLETE
     }
 
+#IF PARTIAL1
   private void initPartialResultInspector() {
         List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
         foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
@@ -128,6 +158,12 @@ public class <ClassName> extends VectorAggregateExpression {
 
         soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
     }
+#ENDIF PARTIAL1
+#IF COMPLETE
+    private void initFullResultInspector() {
+      oi = PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
+    }
+#ENDIF COMPLETE
 
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
@@ -196,14 +232,9 @@ public class <ClassName> extends VectorAggregateExpression {
           aggregateIndex,
           i);
         if (myagg.isNull) {
-          myagg.init ();
-        }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
+          myagg.init();
         }
+        myagg.varianceValue(value);
       }
     }
 
@@ -226,12 +257,7 @@ public class <ClassName> extends VectorAggregateExpression {
           if (myagg.isNull) {
             myagg.init ();
           }
-          myagg.sum += value;
-          myagg.count += 1;
-          if(myagg.count > 1) {
-            double t = myagg.count*value - myagg.sum;
-            myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-          }
+          myagg.varianceValue(value);
         }
       }
     }
@@ -252,12 +278,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (myagg.isNull) {
           myagg.init ();
         }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+        myagg.varianceValue(value);
       }
     }
 
@@ -278,12 +299,7 @@ public class <ClassName> extends VectorAggregateExpression {
           if (myagg.isNull) {
             myagg.init ();
           }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+          myagg.varianceValue(value);
         }
       }
     }
@@ -302,19 +318,13 @@ public class <ClassName> extends VectorAggregateExpression {
         if (myagg.isNull) {
           myagg.init ();
         }
-        double value = vector[i];
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+        myagg.varianceValue(vector[i]);
       }
     }
 
     @Override
     public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
-    throws HiveException {
+        throws HiveException {
 
       inputExpression.evaluate(batch);
 
@@ -359,15 +369,7 @@ public class <ClassName> extends VectorAggregateExpression {
         myagg.init ();
       }
 
-      // TODO: conjure a formula w/o iterating
-      //
-
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
+      myagg.varianceValue(value);
 
       // We pulled out i=0 so we can remove the count > 1 check in the loop
       for (int i=1; i<batchSize; ++i) {
@@ -392,12 +394,7 @@ public class <ClassName> extends VectorAggregateExpression {
           if (myagg.isNull) {
             myagg.init ();
           }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+          myagg.varianceValue(value);
         }
       }
     }
@@ -413,12 +410,7 @@ public class <ClassName> extends VectorAggregateExpression {
       }
 
       double value = vector[selected[0]];
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
+      myagg.varianceValue(value);
 
       // i=0 was pulled out to remove the count > 1 check in the loop
       //
@@ -443,12 +435,7 @@ public class <ClassName> extends VectorAggregateExpression {
           if (myagg.isNull) {
             myagg.init ();
           }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+          myagg.varianceValue(value);
         }
       }
     }
@@ -463,13 +450,7 @@ public class <ClassName> extends VectorAggregateExpression {
       }
 
       double value = vector[0];
-      myagg.sum += value;
-      myagg.count += 1;
-
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
+      myagg.varianceValue(value);
 
       // i=0 was pulled out to remove count > 1 check
       for (int i=1; i<batchSize; ++i) {
@@ -498,19 +479,48 @@ public class <ClassName> extends VectorAggregateExpression {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
         return null;
-      }
-      else {
-        assert(0 < myagg.count);
+       } else {
+#IF PARTIAL1
         resultCount.set (myagg.count);
         resultSum.set (myagg.sum);
         resultVariance.set (myagg.variance);
         return partialResult;
+#ENDIF PARTIAL1
+#IF COMPLETE
+        if (myagg.count == 0) {
+          return null;   // SQL standard - return null for zero elements
+        } else if (myagg.count > 1) {
+#IF VARIANCE
+          fullResult.set(myagg.variance / (myagg.count));
+#ENDIF VARIANCE
+#IF VARIANCE_SAMPLE
+          fullResult.set(myagg.variance / (myagg.count - 1));
+#ENDIF VARIANCE_SAMPLE
+#IF STD
+          fullResult.set(Math.sqrt(myagg.variance / (myagg.count)));
+#ENDIF STD
+#IF STD_SAMPLE
+          fullResult.set(Math.sqrt(myagg.variance / (myagg.count - 1)));
+#ENDIF STD_SAMPLE
+        } else {
+
+          // For one element the variance is always 0.
+          fullResult.set(0);
+        }
+        return fullResult;
+#ENDIF COMPLETE
       }
     }
+
   @Override
-    public ObjectInspector getOutputObjectInspector() {
-      return soi;
-    }
+  public ObjectInspector getOutputObjectInspector() {
+#IF PARTIAL1
+    return soi;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    return oi;
+#ENDIF COMPLETE
+  }
 
   @Override
   public long getAggregationBufferFixedSize() {
@@ -524,15 +534,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
-    // No-op
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
+    init();
   }
 }
 
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarDecimal.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarDecimal.txt
index b3ec7e97a8..ba246e2077 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarDecimal.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarDecimal.txt
@@ -31,13 +31,17 @@ import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 
+import com.google.common.base.Preconditions;
+
 /**
 * <ClassName>. Vectorized implementation for VARIANCE aggregates.
 */
@@ -68,9 +72,29 @@ public class <ClassName> extends VectorAggregateExpression {
 
       public void init() {
         isNull = false;
-        sum = 0f;
+        sum = 0.0;
         count = 0;
-        variance = 0f;
+        variance = 0.0;
+      }
+
+     public void varianceValue(double value) {
+        if (isNull) {
+          sum = value;
+          count = 1;
+          variance = 0;
+          isNull = false;
+        } else {
+          sum += value;
+          count++;
+          if (count > 1) {
+            double t = count * value - sum;
+            variance += (t * t) / ((double) count * (count - 1));
+          }
+        }
+      }
+
+      public void varianceValue(HiveDecimalWritable value) {
+        varianceValue(value.doubleValue());
       }
 
       @Override
@@ -81,56 +105,39 @@ public class <ClassName> extends VectorAggregateExpression {
       @Override
       public void reset () {
         isNull = true;
-        sum = 0f;
+        sum = 0.0;
         count = 0;
-        variance = 0f;
-      }
-
-      public void updateValueWithCheckAndInit(HiveDecimalWritable value, short scale) {
-        if (this.isNull) {
-          this.init();
-        }
-
-        double dval = value.getHiveDecimal().doubleValue();
-        this.sum += dval;
-        this.count += 1;
-        if(this.count > 1) {
-           double t = this.count*dval - this.sum;
-           this.variance += (t*t) / ((double)this.count*(this.count-1));
-        }
-      }
-
-      public void updateValueNoCheck(HiveDecimalWritable value, short scale) {
-        double dval = value.getHiveDecimal().doubleValue();
-        this.sum += dval;
-        this.count += 1;
-        double t = this.count*dval - this.sum;
-        this.variance += (t*t) / ((double)this.count*(this.count-1));
+        variance = 0.0;
       }
 
     }
 
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
+#IF PARTIAL1
     transient private LongWritable resultCount;
     transient private DoubleWritable resultSum;
     transient private DoubleWritable resultVariance;
     transient private Object[] partialResult;
 
     transient private ObjectInspector soi;
-
-    public <ClassName>(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+#ENDIF PARTIAL1
+#IF COMPLETE
+      transient private DoubleWritable fullResult;
+
+      transient private ObjectInspector oi;
+#ENDIF COMPLETE
+
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+#IF PARTIAL1
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL1);
+#ENDIF PARTIAL1
+#IF COMPLETE
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.COMPLETE);
+#ENDIF COMPLETE
     }
 
-    public <ClassName>() {
-      super();
+    private void init() {
+#IF PARTIAL1
       partialResult = new Object[3];
       resultCount = new LongWritable();
       resultSum = new DoubleWritable();
@@ -139,8 +146,14 @@ public class <ClassName> extends VectorAggregateExpression {
       partialResult[1] = resultSum;
       partialResult[2] = resultVariance;
       initPartialResultInspector();
+#ENDIF PARTIAL1
+#IF COMPLETE
+      fullResult = new DoubleWritable();
+      initFullResultInspector();
+#ENDIF COMPLETE
     }
 
+#IF PARTIAL1
   private void initPartialResultInspector() {
         List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
         foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
@@ -154,6 +167,12 @@ public class <ClassName> extends VectorAggregateExpression {
 
         soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
     }
+#ENDIF PARTIAL1
+#IF COMPLETE
+    private void initFullResultInspector() {
+      oi = PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
+    }
+#ENDIF COMPLETE
 
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
@@ -196,12 +215,12 @@ public class <ClassName> extends VectorAggregateExpression {
       }
       else if (!batch.selectedInUse) {
         iterateNoSelectionHasNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, vector, inputVector.scale, 
+            aggregationBufferSets, aggregateIndex, vector, inputVector.scale,
             batchSize, inputVector.isNull);
       }
       else if (inputVector.noNulls){
         iterateSelectionNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, vector, inputVector.scale, 
+            aggregationBufferSets, aggregateIndex, vector, inputVector.scale,
             batchSize, batch.selected);
       }
       else {
@@ -224,7 +243,7 @@ public class <ClassName> extends VectorAggregateExpression {
           aggregationBufferSets,
           aggregateIndex,
           i);
-        myagg.updateValueWithCheckAndInit(value, scale);
+        myagg.varianceValue(value);
       }
     }
 
@@ -244,8 +263,7 @@ public class <ClassName> extends VectorAggregateExpression {
           j);
         int i = selected[j];
         if (!isNull[i]) {
-          HiveDecimalWritable value = vector[i];
-          myagg.updateValueWithCheckAndInit(value, scale);
+          myagg.varianceValue(vector[i]);
         }
       }
     }
@@ -263,8 +281,7 @@ public class <ClassName> extends VectorAggregateExpression {
           aggregationBufferSets,
           aggregateIndex,
           i);
-        HiveDecimalWritable value = vector[selected[i]];
-        myagg.updateValueWithCheckAndInit(value, scale);
+        myagg.varianceValue(vector[selected[i]]);
       }
     }
 
@@ -282,8 +299,7 @@ public class <ClassName> extends VectorAggregateExpression {
             aggregationBufferSets,
             aggregateIndex,
           i);
-          HiveDecimalWritable value = vector[i];
-          myagg.updateValueWithCheckAndInit(value, scale);
+          myagg.varianceValue(vector[i]);
         }
       }
     }
@@ -300,8 +316,7 @@ public class <ClassName> extends VectorAggregateExpression {
           aggregationBufferSets,
           aggregateIndex,
           i);
-        HiveDecimalWritable value = vector[i];
-        myagg.updateValueWithCheckAndInit(value, scale);
+         myagg.varianceValue(vector[i]);
       }
     }
 
@@ -339,7 +354,7 @@ public class <ClassName> extends VectorAggregateExpression {
         iterateSelectionNoNulls(myagg, vector, inputVector.scale, batchSize, batch.selected);
       }
       else {
-        iterateSelectionHasNulls(myagg, vector, inputVector.scale, 
+        iterateSelectionHasNulls(myagg, vector, inputVector.scale,
           batchSize, inputVector.isNull, batch.selected);
       }
     }
@@ -350,14 +365,9 @@ public class <ClassName> extends VectorAggregateExpression {
         short scale,
         int batchSize) {
 
-      // TODO: conjure a formula w/o iterating
-      //
-
-      myagg.updateValueWithCheckAndInit(value, scale);
-
-      // We pulled out i=0 so we can remove the count > 1 check in the loop
-      for (int i=1; i<batchSize; ++i) {
-        myagg.updateValueNoCheck(value, scale);
+      double doubleValue = value.doubleValue();
+      for (int i=0; i<batchSize; ++i) {
+        myagg.varianceValue(doubleValue);
       }
     }
 
@@ -372,8 +382,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
-          HiveDecimalWritable value = vector[i];
-          myagg.updateValueWithCheckAndInit(value, scale);
+          myagg.varianceValue(vector[i]);
         }
       }
     }
@@ -385,18 +394,8 @@ public class <ClassName> extends VectorAggregateExpression {
         int batchSize,
         int[] selected) {
 
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      HiveDecimalWritable value = vector[selected[0]];
-      myagg.updateValueWithCheckAndInit(value, scale);
-
-      // i=0 was pulled out to remove the count > 1 check in the loop
-      //
-      for (int i=1; i< batchSize; ++i) {
-        value = vector[selected[i]];
-        myagg.updateValueNoCheck(value, scale);
+      for (int i=0; i< batchSize; ++i) {
+        myagg.varianceValue(vector[selected[i]]);
       }
     }
 
@@ -409,8 +408,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
-          HiveDecimalWritable value = vector[i];
-          myagg.updateValueWithCheckAndInit(value, scale);
+          myagg.varianceValue(vector[i]);
         }
       }
     }
@@ -421,17 +419,8 @@ public class <ClassName> extends VectorAggregateExpression {
         short scale,
         int batchSize) {
 
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      HiveDecimalWritable value = vector[0];
-      myagg.updateValueWithCheckAndInit(value, scale);
-
-      // i=0 was pulled out to remove count > 1 check
-      for (int i=1; i<batchSize; ++i) {
-        value = vector[i];
-        myagg.updateValueNoCheck(value, scale);
+      for (int i=0; i<batchSize; ++i) {
+        myagg.varianceValue(vector[i]);
       }
     }
 
@@ -452,19 +441,49 @@ public class <ClassName> extends VectorAggregateExpression {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
         return null;
-      }
-      else {
-        assert(0 < myagg.count);
-        resultCount.set(myagg.count);
-        resultSum.set(myagg.sum);
-        resultVariance.set(myagg.variance);
+      } else {
+#IF PARTIAL1
+        resultCount.set (myagg.count);
+        resultSum.set (myagg.sum);
+        resultVariance.set (myagg.variance);
         return partialResult;
+#ENDIF PARTIAL1
+#IF COMPLETE
+        if (myagg.count == 0) {
+          return null;   // SQL standard - return null for zero elements
+        } else if (myagg.count > 1) {
+#IF VARIANCE
+          fullResult.set(myagg.variance / (myagg.count));
+#ENDIF VARIANCE
+#IF VARIANCE_SAMPLE
+          fullResult.set(myagg.variance / (myagg.count - 1));
+#ENDIF VARIANCE_SAMPLE
+#IF STD
+          fullResult.set(Math.sqrt(myagg.variance / (myagg.count)));
+#ENDIF STD
+#IF STD_SAMPLE
+          fullResult.set(Math.sqrt(myagg.variance / (myagg.count - 1)));
+#ENDIF STD_SAMPLE
+        } else {
+
+          // For one element the variance is always 0.
+          fullResult.set(0);
+        }
+
+        return fullResult;
+#ENDIF COMPLETE
       }
     }
+
   @Override
-    public ObjectInspector getOutputObjectInspector() {
-      return soi;
-    }
+  public ObjectInspector getOutputObjectInspector() {
+#IF PARTIAL1
+    return soi;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    return oi;
+#ENDIF COMPLETE
+  }
 
   @Override
   public long getAggregationBufferFixedSize() {
@@ -478,14 +497,6 @@ public class <ClassName> extends VectorAggregateExpression {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
-    // No-op
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
+    init();
   }
 }
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarMerge.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarMerge.txt
new file mode 100644
index 0000000000..447685bbd6
--- /dev/null
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarMerge.txt
@@ -0,0 +1,573 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
+import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
+import org.apache.hadoop.hive.ql.util.JavaDataModel;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+
+import com.google.common.base.Preconditions;
+
+/**
+ * Generated from template VectorUDAFVarMerge.txt.
+ */
+@Description(name = "variance",
+    value = "_FUNC_(expr) - Returns the average value of expr (vectorized, type: <ValueType>)")
+public class <ClassName> extends VectorAggregateExpression {
+
+    private static final long serialVersionUID = 1L;
+
+    /** class for storing the current aggregate value. */
+    static class Aggregation implements AggregationBuffer {
+
+      private static final long serialVersionUID = 1L;
+
+      transient private long mergeCount;
+      transient private double mergeSum;
+      transient private double mergeVariance;
+
+      /**
+      * Value is explicitly (re)initialized in reset()
+      */
+      transient private boolean isNull = true;
+
+      public void merge(long partialCount, double partialSum, double partialVariance) {
+        final long origMergeCount;
+        if (isNull || mergeCount == 0) {
+          // Just copy the information since there is nothing so far.
+          origMergeCount = 0;
+          mergeCount = partialCount;
+          mergeSum = partialSum;
+          mergeVariance = partialVariance;
+          isNull = false;
+        } else {
+          origMergeCount = mergeCount;
+        }
+
+        if (partialCount > 0 && origMergeCount > 0) {
+
+          // Merge the two partials
+
+          mergeCount += partialCount;
+          final double origMergeSum = mergeSum;
+          mergeSum += partialSum;
+
+          final double doublePartialCount = (double) partialCount;
+          final double doubleOrigMergeCount = (double) origMergeCount;
+          double t = (doublePartialCount / doubleOrigMergeCount) * origMergeSum - partialSum;
+          mergeVariance +=
+              partialVariance + ((doubleOrigMergeCount / doublePartialCount) /
+                  (doubleOrigMergeCount + doublePartialCount)) * t * t;
+        }
+      }
+
+      @Override
+      public int getVariableSize() {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public void reset () {
+        isNull = true;
+        mergeCount = 0L;
+        mergeSum = 0;
+        mergeVariance = 0;
+      }
+    }
+
+#IF PARTIAL2
+    transient private Object[] partialResult;
+    transient private LongWritable resultCount;
+    transient private DoubleWritable resultSum;
+    transient private DoubleWritable resultVariance;
+    transient private StructObjectInspector soi;
+#ENDIF PARTIAL2
+#IF FINAL
+    transient private DoubleWritable fullResult;
+    transient private ObjectInspector oi;
+#ENDIF FINAL
+
+    private transient int countOffset;
+    private transient int sumOffset;
+    private transient int varianceOffset;
+
+    public <ClassName>(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+#IF PARTIAL2
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL2);
+#ENDIF PARTIAL2
+#IF FINAL
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.FINAL);
+#ENDIF FINAL
+    }
+
+    private void init() {
+#IF PARTIAL2
+      partialResult = new Object[3];
+      resultCount = new LongWritable();
+      resultSum = new DoubleWritable();
+      resultVariance = new DoubleWritable();
+      partialResult[0] = resultCount;
+      partialResult[1] = resultSum;
+      partialResult[2] = resultVariance;
+      initPartialResultInspector();
+#ENDIF PARTIAL2
+#IF FINAL
+      fullResult = new DoubleWritable();
+      initFullResultInspector();
+#ENDIF FINAL
+    }
+
+#IF PARTIAL2
+    private void initPartialResultInspector() {
+        List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
+        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
+        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
+        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
+        List<String> fname = new ArrayList<String>();
+        fname.add("count");
+        fname.add("sum");
+        fname.add("variance");
+        soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
+    }
+#ENDIF PARTIAL2
+#IF FINAL
+    private void initFullResultInspector() {
+      oi = PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
+    }
+#ENDIF FINAL
+
+    private Aggregation getCurrentAggregationBuffer(
+        VectorAggregationBufferRow[] aggregationBufferSets,
+        int bufferIndex,
+        int row) {
+      VectorAggregationBufferRow mySet = aggregationBufferSets[row];
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(bufferIndex);
+      return myagg;
+    }
+
+    @Override
+    public void aggregateInputSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      VectorizedRowBatch batch) throws HiveException {
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      inputExpression.evaluate(batch);
+
+      StructColumnVector inputStructColVector =
+          (StructColumnVector) batch.cols[this.inputExpression.getOutputColumn()];
+      ColumnVector[] fields = inputStructColVector.fields;
+
+      long[] countVector = ((LongColumnVector) fields[countOffset]).vector;
+      double[] sumVector = ((DoubleColumnVector) fields[sumOffset]).vector;
+      double[] varianceVector = ((DoubleColumnVector) fields[varianceOffset]).vector;
+
+      if (inputStructColVector.noNulls) {
+        if (inputStructColVector.isRepeating) {
+          iterateNoNullsRepeatingWithAggregationSelection(
+            aggregationBufferSets, bufferIndex,
+            countVector[0], sumVector[0], varianceVector[0], batchSize);
+        } else {
+          if (batch.selectedInUse) {
+            iterateNoNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, varianceVector, batch.selected, batchSize);
+          } else {
+            iterateNoNullsWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, varianceVector, batchSize);
+          }
+        }
+      } else {
+        if (inputStructColVector.isRepeating) {
+          if (batch.selectedInUse) {
+            iterateHasNullsRepeatingSelectionWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector[0], sumVector[0], varianceVector[0], batchSize, batch.selected, inputStructColVector.isNull);
+          } else {
+            iterateHasNullsRepeatingWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector[0], sumVector[0], varianceVector[0], batchSize, inputStructColVector.isNull);
+          }
+        } else {
+          if (batch.selectedInUse) {
+            iterateHasNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, varianceVector, batchSize, batch.selected, inputStructColVector.isNull);
+          } else {
+            iterateHasNullsWithAggregationSelection(
+              aggregationBufferSets, bufferIndex,
+              countVector, sumVector, varianceVector, batchSize, inputStructColVector.isNull);
+          }
+        }
+      }
+    }
+
+    private void iterateNoNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long count,
+      double sum,
+      double variance,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(count, sum, variance);
+      }
+    }
+
+    private void iterateNoNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      double[] sumVector,
+      double[] varianceVector,
+      int[] selection,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        final int batchIndex = selection[i];
+        myagg.merge(countVector[batchIndex], sumVector[batchIndex], varianceVector[batchIndex]);
+      }
+    }
+
+    private void iterateNoNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      double[] sumVector,
+      double[] varianceVector,
+      int batchSize) {
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(countVector[i], sumVector[i], varianceVector[i]);
+      }
+    }
+
+    private void iterateHasNullsRepeatingSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long count,
+      double sum,
+      double variance,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      if (isNull[0]) {
+        return;
+      }
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(count, sum, variance);
+      }
+
+    }
+
+    private void iterateHasNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long count,
+      double sum,
+      double variance,
+      int batchSize,
+      boolean[] isNull) {
+
+      if (isNull[0]) {
+        return;
+      }
+
+      for (int i = 0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          bufferIndex,
+          i);
+        myagg.merge(count, sum, variance);
+      }
+    }
+
+    private void iterateHasNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      double[] sumVector,
+      double[] varianceVector,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      for (int i = 0; i < batchSize; i++) {
+        final int batchIndex = selection[i];
+        if (!isNull[batchIndex]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            bufferIndex,
+            i);
+          myagg.merge(countVector[batchIndex], sumVector[batchIndex], varianceVector[batchIndex]);
+        }
+      }
+   }
+
+    private void iterateHasNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int bufferIndex,
+      long[] countVector,
+      double[] sumVector,
+      double[] varianceVector,
+      int batchSize,
+      boolean[] isNull) {
+
+      for (int i=0; i < batchSize; ++i) {
+        if (!isNull[i]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            bufferIndex,
+            i);
+          myagg.merge(countVector[i], sumVector[i], varianceVector[i]);
+        }
+      }
+   }
+
+    @Override
+    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
+        throws HiveException {
+
+      inputExpression.evaluate(batch);
+
+      StructColumnVector inputStructColVector =
+          (StructColumnVector) batch.cols[this.inputExpression.getOutputColumn()];
+      ColumnVector[] fields = inputStructColVector.fields;
+
+      long[] countVector = ((LongColumnVector) fields[countOffset]).vector;
+      double[] sumVector = ((DoubleColumnVector) fields[sumOffset]).vector;
+      double[] varianceVector = ((DoubleColumnVector) fields[varianceOffset]).vector;
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      Aggregation myagg = (Aggregation)agg;
+
+      if (inputStructColVector.isRepeating) {
+        if (inputStructColVector.noNulls) {
+          final long count = countVector[0];
+          final double sum = sumVector[0];
+          final double variance = varianceVector[0];
+          for (int i = 0; i < batchSize; i++) {
+            myagg.merge(count, sum, variance);
+          }
+        }
+        return;
+      }
+
+      if (!batch.selectedInUse && inputStructColVector.noNulls) {
+        iterateNoSelectionNoNulls(myagg, countVector, sumVector, varianceVector, batchSize);
+      } else if (!batch.selectedInUse) {
+        iterateNoSelectionHasNulls(myagg, countVector, sumVector, varianceVector, batchSize, inputStructColVector.isNull);
+      } else if (inputStructColVector.noNulls){
+        iterateSelectionNoNulls(myagg, countVector, sumVector, varianceVector, batchSize, batch.selected);
+      } else {
+        iterateSelectionHasNulls(myagg, countVector, sumVector, varianceVector, batchSize, inputStructColVector.isNull, batch.selected);
+      }
+    }
+
+    private void iterateSelectionHasNulls(
+        Aggregation myagg,
+        long[] countVector,
+        double[] sumVector,
+        double[] varianceVector,
+        int batchSize,
+        boolean[] isNull,
+        int[] selected) {
+
+      for (int i=0; i < batchSize; i++) {
+        int batchIndex = selected[i];
+        if (!isNull[batchIndex]) {
+          myagg.merge(countVector[batchIndex], sumVector[batchIndex], varianceVector[batchIndex]);
+        }
+      }
+    }
+
+    private void iterateSelectionNoNulls(
+        Aggregation myagg,
+        long[] countVector,
+        double[] sumVector,
+        double[] varianceVector,
+        int batchSize,
+        int[] selected) {
+
+      for (int i = 0; i< batchSize; ++i) {
+        final int batchIndex = selected[i];
+        myagg.merge(countVector[batchIndex], sumVector[batchIndex], varianceVector[batchIndex]);
+      }
+    }
+
+    private void iterateNoSelectionHasNulls(
+        Aggregation myagg,
+        long[] countVector,
+        double[] sumVector,
+        double[] varianceVector,
+        int batchSize,
+        boolean[] isNull) {
+
+      for(int i = 0; i < batchSize; i++) {
+        if (!isNull[i]) {
+          myagg.merge(countVector[i], sumVector[i], varianceVector[i]);
+        }
+      }
+    }
+
+    private void iterateNoSelectionNoNulls(
+        Aggregation myagg,
+        long[] countVector,
+        double[] sumVector,
+        double[] varianceVector,
+        int batchSize) {
+      for (int i=0;i<batchSize;++i) {
+        myagg.merge(countVector[i], sumVector[i], varianceVector[i]);
+      }
+    }
+
+    @Override
+    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
+      return new Aggregation();
+    }
+
+    @Override
+    public void reset(AggregationBuffer agg) throws HiveException {
+      Aggregation myAgg = (Aggregation) agg;
+      myAgg.reset();
+    }
+
+    @Override
+    public Object evaluateOutput(
+        AggregationBuffer agg) throws HiveException {
+      Aggregation myagg = (Aggregation) agg;
+      if (myagg.isNull) {
+        return null;
+      } else {
+#IF PARTIAL2
+        resultCount.set (myagg.mergeCount);
+        resultSum.set (myagg.mergeSum);
+        resultVariance.set (myagg.mergeVariance);
+        return partialResult;
+#ENDIF PARTIAL2
+#IF FINAL
+       if (myagg.mergeCount == 0) {
+          return null;   // SQL standard - return null for zero elements
+        } else if (myagg.mergeCount > 1) {
+#IF VARIANCE
+          fullResult.set(myagg.mergeVariance / (myagg.mergeCount));
+#ENDIF VARIANCE
+#IF VARIANCE_SAMPLE
+          fullResult.set(myagg.mergeVariance / (myagg.mergeCount - 1));
+#ENDIF VARIANCE_SAMPLE
+#IF STD
+          fullResult.set(Math.sqrt(myagg.mergeVariance / (myagg.mergeCount)));
+#ENDIF STD
+#IF STD_SAMPLE
+          fullResult.set(Math.sqrt(myagg.mergeVariance / (myagg.mergeCount - 1)));
+#ENDIF STD_SAMPLE
+        } else {
+
+          // For one element the variance is always 0.
+          fullResult.set(0);
+        }
+
+        return fullResult;
+#ENDIF FINAL
+      }
+    }
+
+  @Override
+  public ObjectInspector getOutputObjectInspector() {
+#IF PARTIAL2
+    return soi;
+#ENDIF PARTIAL2
+#IF FINAL
+    return oi;
+#ENDIF FINAL
+  }
+
+  @Override
+  public long getAggregationBufferFixedSize() {
+    JavaDataModel model = JavaDataModel.get();
+    return JavaDataModel.alignUp(
+      model.object() +
+      model.primitive2() * 2,
+      model.memoryAlign());
+  }
+
+  @Override
+  public void init(AggregationDesc desc) throws HiveException {
+    init();
+
+    ExprNodeDesc inputExpr = desc.getParameters().get(0);
+    StructTypeInfo partialStructTypeInfo = (StructTypeInfo) inputExpr.getTypeInfo();
+
+    ArrayList<String> fieldNames =  partialStructTypeInfo.getAllStructFieldNames();
+    countOffset = fieldNames.indexOf("count");
+    sumOffset = fieldNames.indexOf("sum");
+    varianceOffset = fieldNames.indexOf("variance");
+  }
+}
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarPopTimestamp.java b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarTimestamp.txt
similarity index 69%
rename from ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarPopTimestamp.java
rename to ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarTimestamp.txt
index 61cdeaa12d..8ef1a9f39a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarPopTimestamp.java
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFVarTimestamp.txt
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates;
+package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen;
 
 import java.util.ArrayList;
 import java.util.List;
@@ -29,19 +29,23 @@
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 
+import com.google.common.base.Preconditions;
+
 /**
 * VectorUDAFVarPopTimestamp. Vectorized implementation for VARIANCE aggregates.
 */
 @Description(name = "variance, var_pop",
     value = "_FUNC_(x) - Returns the variance of a set of numbers (vectorized, timestamp)")
-public class VectorUDAFVarPopTimestamp extends VectorAggregateExpression {
+public class <ClassName> extends VectorAggregateExpression {
 
     private static final long serialVersionUID = 1L;
 
@@ -68,6 +72,22 @@ public void init() {
         variance = 0;
       }
 
+      public void varianceValue(double value) {
+        if (isNull) {
+          sum = value;
+          count = 1;
+          variance = 0;
+          isNull = false;
+        } else {
+          sum += value;
+          count++;
+          if (count > 1) {
+            double t = count * value - sum;
+            variance += (t * t) / ((double) count * (count - 1));
+          }
+        }
+      }
+
       @Override
       public int getVariableSize() {
         throw new UnsupportedOperationException();
@@ -82,28 +102,34 @@ public void reset () {
       }
     }
 
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
+#IF PARTIAL1
     transient private LongWritable resultCount;
     transient private DoubleWritable resultSum;
     transient private DoubleWritable resultVariance;
     transient private Object[] partialResult;
 
     transient private ObjectInspector soi;
-
-
-    public VectorUDAFVarPopTimestamp(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    transient private DoubleWritable fullResult;
+
+    transient private ObjectInspector oi;
+#ENDIF COMPLETE
+
+
+    public <ClassName>(VectorExpression inputExpression,
+        GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+#IF PARTIAL1
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL1);
+#ENDIF PARTIAL1
+#IF COMPLETE
+      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.COMPLETE);
+#ENDIF COMPLETE
     }
 
-    public VectorUDAFVarPopTimestamp() {
-      super();
+    private void init() {
+#IF PARTIAL1
       partialResult = new Object[3];
       resultCount = new LongWritable();
       resultSum = new DoubleWritable();
@@ -112,8 +138,14 @@ public VectorUDAFVarPopTimestamp() {
       partialResult[1] = resultSum;
       partialResult[2] = resultVariance;
       initPartialResultInspector();
+#ENDIF PARTIAL1
+#IF COMPLETE
+      fullResult = new DoubleWritable();
+      initFullResultInspector();
+#ENDIF COMPLETE
     }
 
+#IF PARTIAL1
   private void initPartialResultInspector() {
         List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
         foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
@@ -127,6 +159,12 @@ private void initPartialResultInspector() {
 
         soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
     }
+#ENDIF PARTIAL1
+#IF COMPLETE
+    private void initFullResultInspector() {
+      oi = PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
+    }
+#ENDIF COMPLETE
 
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
@@ -192,15 +230,7 @@ private void  iterateRepeatingNoNullsWithAggregationSelection(
           aggregationBufferSets,
           aggregateIndex,
           i);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+        myagg.varianceValue(value);
       }
     }
 
@@ -219,16 +249,7 @@ private void iterateSelectionHasNullsWithAggregationSelection(
           j);
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-          if(myagg.count > 1) {
-            double t = myagg.count*value - myagg.sum;
-            myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-          }
+          myagg.varianceValue(inputColVector.getDouble(i));
         }
       }
     }
@@ -245,16 +266,7 @@ private void iterateSelectionNoNullsWithAggregationSelection(
           aggregationBufferSets,
           aggregateIndex,
           i);
-        double value = inputColVector.getDouble(selected[i]);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+        myagg.varianceValue(inputColVector.getDouble(selected[i]));
       }
     }
 
@@ -271,16 +283,7 @@ private void iterateNoSelectionHasNullsWithAggregationSelection(
             aggregationBufferSets,
             aggregateIndex,
           i);
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+          myagg.varianceValue(inputColVector.getDouble(i));
         }
       }
     }
@@ -296,16 +299,7 @@ private void iterateNoSelectionNoNullsWithAggregationSelection(
           aggregationBufferSets,
           aggregateIndex,
           i);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        double value = inputColVector.getDouble(i);
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+        myagg.varianceValue(inputColVector.getDouble(i));
       }
     }
 
@@ -350,26 +344,8 @@ private void  iterateRepeatingNoNulls(
         double value,
         int batchSize) {
 
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      // TODO: conjure a formula w/o iterating
-      //
-
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // We pulled out i=0 so we can remove the count > 1 check in the loop
-      for (int i=1; i<batchSize; ++i) {
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
+      for (int i=0; i<batchSize; ++i) {
+        myagg.varianceValue(value);
       }
     }
 
@@ -383,16 +359,7 @@ private void iterateSelectionHasNulls(
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+          myagg.varianceValue(inputColVector.getDouble(i));
         }
       }
     }
@@ -403,26 +370,8 @@ private void iterateSelectionNoNulls(
         int batchSize,
         int[] selected) {
 
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      double value = inputColVector.getDouble(selected[0]);
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // i=0 was pulled out to remove the count > 1 check in the loop
-      //
-      for (int i=1; i< batchSize; ++i) {
-        value = inputColVector.getDouble(selected[i]);
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
+      for (int i=0; i< batchSize; ++i) {
+        myagg.varianceValue(inputColVector.getDouble(selected[i]));
       }
     }
 
@@ -434,16 +383,7 @@ private void iterateNoSelectionHasNulls(
 
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
+          myagg.varianceValue(inputColVector.getDouble(i));
         }
       }
     }
@@ -453,26 +393,8 @@ private void iterateNoSelectionNoNulls(
         TimestampColumnVector inputColVector,
         int batchSize) {
 
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      double value = inputColVector.getDouble(0);
-      myagg.sum += value;
-      myagg.count += 1;
-
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // i=0 was pulled out to remove count > 1 check
-      for (int i=1; i<batchSize; ++i) {
-        value = inputColVector.getDouble(i);
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
+      for (int i=0; i<batchSize; ++i) {
+        myagg.varianceValue(inputColVector.getDouble(i));
       }
     }
 
@@ -493,19 +415,49 @@ public Object evaluateOutput(
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
         return null;
-      }
-      else {
-        assert(0 < myagg.count);
+      } else {
+#IF PARTIAL1
         resultCount.set (myagg.count);
         resultSum.set (myagg.sum);
         resultVariance.set (myagg.variance);
         return partialResult;
+#ENDIF PARTIAL1
+#IF COMPLETE
+       if (myagg.count == 0) {
+          return null;   // SQL standard - return null for zero elements
+        } else if (myagg.count > 1) {
+#IF VARIANCE
+          fullResult.set(myagg.variance / (myagg.count));
+#ENDIF VARIANCE
+#IF VARIANCE_SAMPLE
+          fullResult.set(myagg.variance / (myagg.count - 1));
+#ENDIF VARIANCE_SAMPLE
+#IF STD
+          fullResult.set(Math.sqrt(myagg.variance / (myagg.count)));
+#ENDIF STD
+#IF STD_SAMPLE
+          fullResult.set(Math.sqrt(myagg.variance / (myagg.count - 1)));
+#ENDIF STD_SAMPLE
+        } else {
+
+          // For one element the variance is always 0.
+          fullResult.set(0);
+        }
+
+        return fullResult;
+#ENDIF COMPLETE
       }
     }
+
   @Override
-    public ObjectInspector getOutputObjectInspector() {
-      return soi;
-    }
+  public ObjectInspector getOutputObjectInspector() {
+#IF PARTIAL1
+    return soi;
+#ENDIF PARTIAL1
+#IF COMPLETE
+    return oi;
+#ENDIF COMPLETE
+  }
 
   @Override
   public long getAggregationBufferFixedSize() {
@@ -519,15 +471,7 @@ public long getAggregationBufferFixedSize() {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
-    // No-op
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
+    init();
   }
 }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index c70e1e04e8..a1cf76b78c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -3784,59 +3784,6 @@ public static StandardStructObjectInspector constructVectorizedReduceRowOI(
     return rowObjectInspector;
   }
 
-  /**
-   * Check if LLAP IO supports the column type that is being read
-   * @param conf - configuration
-   * @return false for types not supported by vectorization, true otherwise
-   */
-  public static boolean checkVectorizerSupportedTypes(final Configuration conf) {
-    final String[] readColumnNames = ColumnProjectionUtils.getReadColumnNames(conf);
-    final String columnNames = conf.get(serdeConstants.LIST_COLUMNS);
-    final String columnTypes = conf.get(serdeConstants.LIST_COLUMN_TYPES);
-    if (columnNames == null || columnTypes == null || columnNames.isEmpty() ||
-        columnTypes.isEmpty()) {
-      LOG.warn("Column names ({}) or types ({}) is null. Skipping type checking for LLAP IO.",
-          columnNames, columnTypes);
-      return true;
-    }
-    final List<String> allColumnNames = Lists.newArrayList(columnNames.split(","));
-    final List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString(columnTypes);
-    final List<String> allColumnTypes = TypeInfoUtils.getTypeStringsFromTypeInfo(typeInfos);
-    return checkVectorizerSupportedTypes(Lists.newArrayList(readColumnNames), allColumnNames,
-        allColumnTypes);
-  }
-
-  /**
-   * Check if LLAP IO supports the column type that is being read
-   * @param readColumnNames - columns that will be read from the table/partition
-   * @param allColumnNames - all columns
-   * @param allColumnTypes - all column types
-   * @return false for types not supported by vectorization, true otherwise
-   */
-  public static boolean checkVectorizerSupportedTypes(final List<String> readColumnNames,
-      final List<String> allColumnNames, final List<String> allColumnTypes) {
-    final String[] readColumnTypes = getReadColumnTypes(readColumnNames, allColumnNames,
-        allColumnTypes);
-
-    if (readColumnTypes != null) {
-      for (String readColumnType : readColumnTypes) {
-        if (readColumnType != null) {
-          if (!Vectorizer.validateDataType(readColumnType,
-              VectorExpressionDescriptor.Mode.PROJECTION)) {
-            LOG.warn("Unsupported column type encountered ({}). Disabling LLAP IO.",
-                readColumnType);
-            return false;
-          }
-        }
-      }
-    } else {
-      LOG.warn("readColumnTypes is null. Skipping type checking for LLAP IO. " +
-          "readColumnNames: {} allColumnNames: {} allColumnTypes: {} readColumnTypes: {}",
-          readColumnNames, allColumnNames, allColumnTypes, readColumnTypes);
-    }
-    return true;
-  }
-
   private static String[] getReadColumnTypes(final List<String> readColumnNames,
       final List<String> allColumnNames, final List<String> allColumnTypes) {
     if (readColumnNames == null || allColumnNames == null || allColumnTypes == null ||
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
index e47358070f..36158a1795 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
@@ -32,6 +32,9 @@
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.exec.mr.ExecMapper.ReportStats;
 import org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext;
+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
@@ -45,6 +48,9 @@
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
+import org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe;
+import org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead;
+import org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
@@ -56,6 +62,8 @@
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 
+import com.google.common.base.Preconditions;
+
 /**
  * Clone from ExecReducer, it is the bridge between the spark framework and
  * the Hive operator pipeline at execution time. It's main responsibilities are:
@@ -85,20 +93,29 @@ public class SparkReduceRecordHandler extends SparkRecordHandler {
   private ObjectInspector[] rowObjectInspector;
   private boolean vectorized = false;
 
-  // runtime objects
-  private transient Object keyObject;
-  private transient BytesWritable groupKey;
+  private VectorDeserializeRow<BinarySortableDeserializeRead> keyBinarySortableDeserializeToRow;
+
+  private VectorDeserializeRow<LazyBinaryDeserializeRead> valueLazyBinaryDeserializeToRow;
+
+  private VectorizedRowBatch batch;
+  private long batchBytes = 0;
+  private boolean handleGroupKey = true;  // For now.
 
   private DataOutputBuffer buffer;
-  private VectorizedRowBatch[] batches;
+
   // number of columns pertaining to keys in a vectorized row batch
-  private int keysColumnOffset;
+  private int firstValueColumnOffset;
+
   private static final int BATCH_SIZE = VectorizedRowBatch.DEFAULT_SIZE;
   private static final int BATCH_BYTES = VectorizedRowBatch.DEFAULT_BYTES;
+
+  // runtime objects
+  private transient Object keyObject;
+  private transient BytesWritable groupKey;
+
   private StructObjectInspector keyStructInspector;
-  private StructObjectInspector[] valueStructInspectors;
+  private StructObjectInspector valueStructInspector;
   /* this is only used in the error code path */
-  private List<VectorExpressionWriter>[] valueStringWriters;
   private MapredLocalWork localWork = null;
 
   @Override
@@ -128,11 +145,14 @@ public void init(JobConf job, OutputCollector output, Reporter reporter) throws
 
       if (vectorized) {
         final int maxTags = gWork.getTagToValueDesc().size();
+
+        // CONSIDER: Cleaning up this code and eliminating the arrays.  Vectorization only handles
+        // one operator tree.
+        Preconditions.checkState(maxTags == 1);
+
         keyStructInspector = (StructObjectInspector) keyObjectInspector;
-        batches = new VectorizedRowBatch[maxTags];
-        valueStructInspectors = new StructObjectInspector[maxTags];
-        valueStringWriters = new List[maxTags];
-        keysColumnOffset = keyStructInspector.getAllStructFieldRefs().size();
+        firstValueColumnOffset = keyStructInspector.getAllStructFieldRefs().size();
+
         buffer = new DataOutputBuffer();
       }
 
@@ -149,20 +169,48 @@ public void init(JobConf job, OutputCollector output, Reporter reporter) throws
 
         if (vectorized) {
           /* vectorization only works with struct object inspectors */
-          valueStructInspectors[tag] = (StructObjectInspector) valueObjectInspector[tag];
+          valueStructInspector = (StructObjectInspector) valueObjectInspector[tag];
 
-          final int totalColumns = keysColumnOffset
-              + valueStructInspectors[tag].getAllStructFieldRefs().size();
-          valueStringWriters[tag] = new ArrayList<VectorExpressionWriter>(totalColumns);
-          valueStringWriters[tag].addAll(Arrays.asList(VectorExpressionWriterFactory
-              .genVectorStructExpressionWritables(keyStructInspector)));
-          valueStringWriters[tag].addAll(Arrays.asList(VectorExpressionWriterFactory
-              .genVectorStructExpressionWritables(valueStructInspectors[tag])));
+          final int totalColumns = firstValueColumnOffset
+              + valueStructInspector.getAllStructFieldRefs().size();
 
           rowObjectInspector[tag] = Utilities.constructVectorizedReduceRowOI(keyStructInspector,
-              valueStructInspectors[tag]);
-          batches[tag] = gWork.getVectorizedRowBatchCtx().createVectorizedRowBatch();
-
+              valueStructInspector);
+          batch = gWork.getVectorizedRowBatchCtx().createVectorizedRowBatch();
+
+          // Setup vectorized deserialization for the key and value.
+          BinarySortableSerDe binarySortableSerDe = (BinarySortableSerDe) inputKeyDeserializer;
+
+          keyBinarySortableDeserializeToRow =
+                    new VectorDeserializeRow<BinarySortableDeserializeRead>(
+                          new BinarySortableDeserializeRead(
+                                    VectorizedBatchUtil.typeInfosFromStructObjectInspector(
+                                        keyStructInspector),
+                                    /* useExternalBuffer */ true,
+                                    binarySortableSerDe.getSortOrders(),
+                                    binarySortableSerDe.getNullMarkers(),
+                                    binarySortableSerDe.getNotNullMarkers()));
+          keyBinarySortableDeserializeToRow.init(0);
+
+          final int valuesSize = valueStructInspector.getAllStructFieldRefs().size();
+          if (valuesSize > 0) {
+            valueLazyBinaryDeserializeToRow =
+                    new VectorDeserializeRow<LazyBinaryDeserializeRead>(
+                          new LazyBinaryDeserializeRead(
+                              VectorizedBatchUtil.typeInfosFromStructObjectInspector(
+                                         valueStructInspector),
+                              /* useExternalBuffer */ true));
+            valueLazyBinaryDeserializeToRow.init(firstValueColumnOffset);
+
+            // Create data buffers for value bytes column vectors.
+            for (int i = firstValueColumnOffset; i < batch.numCols; i++) {
+              ColumnVector colVector = batch.cols[i];
+              if (colVector instanceof BytesColumnVector) {
+                BytesColumnVector bytesColumnVector = (BytesColumnVector) colVector;
+                bytesColumnVector.initBuffer();
+              }
+            }
+          }
 
         } else {
           ois.add(keyObjectInspector);
@@ -243,16 +291,27 @@ public void remove() {
   private DummyIterator dummyIterator = new DummyIterator();
 
   /**
-   * Process one row using a dummy iterator.
+   * Process one row using a dummy iterator.  Or, add row to vector batch.
    */
   @Override
   public void processRow(Object key, final Object value) throws IOException {
-    dummyIterator.setValue(value);
-    processRow(key, dummyIterator);
+    if (vectorized) {
+      processVectorRow(key, value);
+    } else {
+      dummyIterator.setValue(value);
+      processRow(key, dummyIterator);
+    }
   }
 
+
+
   @Override
   public <E> void processRow(Object key, Iterator<E> values) throws IOException {
+    if (vectorized) {
+      processVectorRows(key, values);
+      return;
+    }
+
     if (reducer.getDone()) {
       return;
     }
@@ -295,12 +354,7 @@ public <E> void processRow(Object key, Iterator<E> values) throws IOException {
         reducer.setGroupKeyObject(keyObject);
         reducer.startGroup();
       }
-      /* this.keyObject passed via reference */
-      if (vectorized) {
-        processVectors(values, tag);
-      } else {
-        processKeyValues(values, tag);
-      }
+      processKeyValues(values, tag);
 
     } catch (Throwable e) {
       abort = true;
@@ -357,62 +411,152 @@ private <E> boolean processKeyValues(Iterator<E> values, byte tag) throws HiveEx
     return true; // give me more
   }
 
-  /**
-   * @param values
-   * @return true if it is not done and can take more inputs
-   */
-  private <E> boolean processVectors(Iterator<E> values, byte tag) throws HiveException {
-    VectorizedRowBatch batch = batches[tag];
-    batch.reset();
-    buffer.reset();
-
-    /* deserialize key into columns */
-    VectorizedBatchUtil.addRowToBatchFrom(keyObject, keyStructInspector, 0, 0, batch, buffer);
-    for (int i = 0; i < keysColumnOffset; i++) {
-      VectorizedBatchUtil.setRepeatingColumn(batch, i);
+  private <E> void processVectorRows(Object key, Iterator<E> values) throws IOException {
+    if (reducer.getDone()) {
+      return;
     }
+    while (values.hasNext()) {
+      processVectorRow(key, values.next());
+    }
+  }
+
+  private void processVectorRow(Object key, final Object value) throws IOException {
+    BytesWritable keyWritable = (BytesWritable) key;
+    BytesWritable valueWritable = (BytesWritable) value;
 
-    int rowIdx = 0;
-    int batchBytes = 0;
     try {
-      while (values.hasNext()) {
-        /* deserialize value into columns */
-        BytesWritable valueWritable = (BytesWritable) values.next();
-        Object valueObj = deserializeValue(valueWritable, tag);
-
-        VectorizedBatchUtil.addRowToBatchFrom(valueObj, valueStructInspectors[tag], rowIdx,
-            keysColumnOffset, batch, buffer);
-        batchBytes += valueWritable.getLength();
-        rowIdx++;
-        if (rowIdx >= BATCH_SIZE || batchBytes > BATCH_BYTES) {
-          VectorizedBatchUtil.setBatchSize(batch, rowIdx);
-          reducer.process(batch, tag);
-          rowIdx = 0;
-          batchBytes = 0;
-          if (LOG.isInfoEnabled()) {
-            logMemoryInfo();
+
+      if (handleGroupKey) {
+        final boolean isKeyChange;
+        if (groupKey == null) {
+
+          // The first group.
+          isKeyChange = true;
+          groupKey = new BytesWritable();
+        } else {
+          isKeyChange = !keyWritable.equals(groupKey);
+        }
+
+        if (isKeyChange) {
+
+          // Flush current group batch as last batch of group.
+          if (batch.size > 0) {
+
+            // Forward; reset key and value columns.
+            forwardBatch(/* resetValueColumnsOnly */ false);
+            reducer.endGroup();
+          }
+
+          reducer.startGroup();
+
+          // Deserialize group key into vector row columns.
+          byte[] keyBytes = keyWritable.getBytes();
+          int keyLength = keyWritable.getLength();
+
+          groupKey.set(keyBytes, 0, keyLength);
+
+          keyBinarySortableDeserializeToRow.setBytes(keyBytes, 0, keyLength);
+          try {
+            keyBinarySortableDeserializeToRow.deserialize(batch, 0);
+          } catch (Exception e) {
+            throw new HiveException(
+                "\nDeserializeRead details: " +
+                    keyBinarySortableDeserializeToRow.getDetailedReadPositionString(),
+                e);
+          }
+
+          // And, mark group keys as repeating.
+          for(int i = 0; i < firstValueColumnOffset; i++) {
+            VectorizedBatchUtil.setRepeatingColumn(batch, i);
           }
         }
+
+        // Can we add to current batch?
+        if (batch.size >= batch.getMaxSize() ||
+            batch.size > 0 && batchBytes >= BATCH_BYTES) {
+
+          // Batch is full or using too much space.
+          forwardBatch(/* resetValueColumnsOnly */ true);
+        }
+
+        if (valueLazyBinaryDeserializeToRow != null) {
+          // Deserialize value into vector row columns.
+          byte[] valueBytes = valueWritable.getBytes();
+          int valueLength = valueWritable.getLength();
+          batchBytes += valueLength;
+
+          valueLazyBinaryDeserializeToRow.setBytes(valueBytes, 0, valueLength);
+          valueLazyBinaryDeserializeToRow.deserialize(batch, batch.size);
+        }
+        batch.size++;
+      } else {
+
+        // No group key.
+
+        // Can we add to current batch?
+        if (batch.size >= batch.getMaxSize() ||
+            batch.size > 0 && batchBytes >= BATCH_BYTES) {
+
+          // Batch is full or using too much space.
+          forwardBatch(/* resetValueColumnsOnly */ false);
+        }
+
+        // Deserialize key into vector row columns.
+        byte[] keyBytes = keyWritable.getBytes();
+        int keyLength = keyWritable.getLength();
+
+        keyBinarySortableDeserializeToRow.setBytes(keyBytes, 0, keyLength);
+        try {
+          keyBinarySortableDeserializeToRow.deserialize(batch, 0);
+        } catch (Exception e) {
+          throw new HiveException(
+              "\nDeserializeRead details: " +
+                  keyBinarySortableDeserializeToRow.getDetailedReadPositionString(),
+              e);
+        }
+
+        if (valueLazyBinaryDeserializeToRow != null) {
+          // Deserialize value into vector row columns.
+          byte[] valueBytes = valueWritable.getBytes();
+          int valueLength = valueWritable.getLength();
+
+          batchBytes += valueLength;
+
+          valueLazyBinaryDeserializeToRow.setBytes(valueBytes, 0, valueLength);
+          valueLazyBinaryDeserializeToRow.deserialize(batch, batch.size);
+        }
+        batch.size++;
       }
-      if (rowIdx > 0) {
-        VectorizedBatchUtil.setBatchSize(batch, rowIdx);
-        reducer.process(batch, tag);
-      }
-      if (LOG.isInfoEnabled()) {
-        logMemoryInfo();
+    } catch (Throwable e) {
+      abort = true;
+      if (e instanceof OutOfMemoryError) {
+        // Don't create a new object if we are already out of memory
+        throw (OutOfMemoryError) e;
+      } else {
+        throw new RuntimeException(e);
       }
-    } catch (Exception e) {
-      String rowString = null;
-      try {
-        rowString = batch.toString();
-      } catch (Exception e2) {
-        rowString = "[Error getting row data with exception " + StringUtils.stringifyException(e2)
-          + " ]";
+    }
+  }
+
+  private void forwardBatch(boolean resetValueColumnsOnly) throws HiveException {
+    reducer.process(batch, 0);
+
+    if (resetValueColumnsOnly) {
+      // Reset just the value columns and value buffer.
+      for (int i = firstValueColumnOffset; i < batch.numCols; i++) {
+        // Note that reset also resets the data buffer for bytes column vectors.
+        batch.cols[i].reset();
       }
-      throw new HiveException("Error while processing vector batch (tag=" + tag + ") "
-        + rowString, e);
+      batch.size = 0;
+    } else {
+      // Reset key and value columns; and batch.size
+      batch.reset();
+    }
+
+    batchBytes = 0;
+    if (LOG.isInfoEnabled()) {
+      logMemoryInfo();
     }
-    return true; // give me more
   }
 
   private Object deserializeValue(BytesWritable valueWritable, byte tag) throws HiveException {
@@ -435,10 +579,19 @@ public void close() {
     }
 
     try {
-      if (groupKey != null) {
-        // If a operator wants to do some work at the end of a group
-        LOG.trace("End Group");
-        reducer.endGroup();
+      if (vectorized) {
+        if (batch.size > 0) {
+          forwardBatch(/* resetValueColumnsOnly */ false);
+          if (handleGroupKey) {
+            reducer.endGroup();
+          }
+        }
+      } else {
+        if (groupKey != null) {
+          // If a operator wants to do some work at the end of a group
+          LOG.trace("End Group");
+          reducer.endGroup();
+        }
       }
       if (LOG.isInfoEnabled()) {
         logCloseInfo();
@@ -472,4 +625,12 @@ public void close() {
   public boolean getDone() {
     return reducer.getDone();
   }
+
+  public static String displayBytes(byte[] bytes, int start, int length) {
+    StringBuilder sb = new StringBuilder();
+    for (int i = start; i < start + length; i++) {
+      sb.append(String.format("\\%03d", (int) (bytes[i] & 0xff)));
+    }
+    return sb.toString();
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
index 60660aca3c..43f9db35b1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
@@ -108,9 +108,6 @@ public class ReduceRecordSource implements RecordSource {
   private StructObjectInspector keyStructInspector;
   private StructObjectInspector valueStructInspectors;
 
-  /* this is only used in the error code path */
-  private List<VectorExpressionWriter> valueStringWriters;
-
   private KeyValuesAdapter reader;
 
   private boolean handleGroupKey;
@@ -171,13 +168,6 @@ void init(JobConf jconf, Operator<?> reducer, boolean vectorized, TableDesc keyT
 
         final int totalColumns = firstValueColumnOffset +
             valueStructInspectors.getAllStructFieldRefs().size();
-        valueStringWriters = new ArrayList<VectorExpressionWriter>(totalColumns);
-        valueStringWriters.addAll(Arrays
-            .asList(VectorExpressionWriterFactory
-                .genVectorStructExpressionWritables(keyStructInspector)));
-        valueStringWriters.addAll(Arrays
-            .asList(VectorExpressionWriterFactory
-                .genVectorStructExpressionWritables(valueStructInspectors)));
 
         rowObjectInspector = Utilities.constructVectorizedReduceRowOI(keyStructInspector,
             valueStructInspectors);
@@ -449,9 +439,6 @@ private void processVectorGroup(BytesWritable keyWritable,
           int valueLength = valueWritable.getLength();
           batchBytes += valueLength;
 
-          // l4j.info("ReduceRecordSource processVectorGroup valueBytes " + valueLength + " " +
-          //     VectorizedBatchUtil.displayBytes(valueBytes, 0, valueLength));
-
           valueLazyBinaryDeserializeToRow.setBytes(valueBytes, 0, valueLength);
           valueLazyBinaryDeserializeToRow.deserialize(batch, rowIdx);
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java
index b0d1c75589..c7aa93ec00 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java
@@ -248,6 +248,15 @@ public void init(List<String> typeNames) throws HiveException {
     }
   }
 
+  /*
+   * Initialize using one target data type info.
+   */
+  public void init(TypeInfo typeInfo, int outputColumnNum) throws HiveException {
+
+    allocateArrays(1);
+    initTargetEntry(0, outputColumnNum, typeInfo);
+  }
+
   /**
    * Initialize for conversion from a provided (source) data types to the target data types
    * desired in the VectorizedRowBatch.
@@ -365,53 +374,111 @@ private void assignRowColumn(
           VectorizedBatchUtil.setNullColIsNullValue(columnVector, batchIndex);
           return;
         case BOOLEAN:
-          ((LongColumnVector) columnVector).vector[batchIndex] =
-              (((BooleanWritable) object).get() ? 1 : 0);
+          if (object instanceof Boolean) {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                (((Boolean) object) ? 1 : 0);
+          } else {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                (((BooleanWritable) object).get() ? 1 : 0);
+          }
           break;
         case BYTE:
-          ((LongColumnVector) columnVector).vector[batchIndex] =
-             ((ByteWritable) object).get();
+          if (object instanceof Byte) {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                ((Byte) object);
+          } else {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+               ((ByteWritable) object).get();
+          }
           break;
         case SHORT:
-          ((LongColumnVector) columnVector).vector[batchIndex] =
-              ((ShortWritable) object).get();
+          if (object instanceof Short) {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                ((Short) object);
+          } else {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                ((ShortWritable) object).get();
+          }
           break;
         case INT:
-          ((LongColumnVector) columnVector).vector[batchIndex] =
-              ((IntWritable) object).get();
+          if (object instanceof Integer) {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                ((Integer) object);
+          } else {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                ((IntWritable) object).get();
+          }
           break;
         case LONG:
-          ((LongColumnVector) columnVector).vector[batchIndex] =
-              ((LongWritable) object).get();
+          if (object instanceof Long) {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                ((Long) object);
+          } else {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                ((LongWritable) object).get();
+          }
           break;
         case TIMESTAMP:
-          ((TimestampColumnVector) columnVector).set(
-              batchIndex, ((TimestampWritable) object).getTimestamp());
+          if (object instanceof Timestamp) {
+            ((TimestampColumnVector) columnVector).set(
+                batchIndex, ((Timestamp) object));
+          } else {
+            ((TimestampColumnVector) columnVector).set(
+                batchIndex, ((TimestampWritable) object).getTimestamp());
+          }
           break;
         case DATE:
-          ((LongColumnVector) columnVector).vector[batchIndex] =
-             ((DateWritable) object).getDays();
+          if (object instanceof Date) {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                DateWritable.dateToDays((Date) object);
+          } else {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+               ((DateWritable) object).getDays();
+          }
           break;
         case FLOAT:
-          ((DoubleColumnVector) columnVector).vector[batchIndex] =
-              ((FloatWritable) object).get();
+          if (object instanceof Float) {
+            ((DoubleColumnVector) columnVector).vector[batchIndex] =
+                ((Float) object);
+          } else {
+            ((DoubleColumnVector) columnVector).vector[batchIndex] =
+                ((FloatWritable) object).get();
+          }
           break;
         case DOUBLE:
-          ((DoubleColumnVector) columnVector).vector[batchIndex] =
-              ((DoubleWritable) object).get();
+          if (object instanceof Double) {
+            ((DoubleColumnVector) columnVector).vector[batchIndex] =
+                ((Double) object);
+          } else {
+            ((DoubleColumnVector) columnVector).vector[batchIndex] =
+                ((DoubleWritable) object).get();
+          }
           break;
         case BINARY:
           {
-            BytesWritable bw = (BytesWritable) object;
-            ((BytesColumnVector) columnVector).setVal(
-                batchIndex, bw.getBytes(), 0, bw.getLength());
+            if (object instanceof byte[]) {
+              byte[] bytes = (byte[]) object;
+              ((BytesColumnVector) columnVector).setVal(
+                  batchIndex, bytes, 0, bytes.length);
+            } else {
+              BytesWritable bw = (BytesWritable) object;
+              ((BytesColumnVector) columnVector).setVal(
+                  batchIndex, bw.getBytes(), 0, bw.getLength());
+            }
           }
           break;
         case STRING:
           {
-            Text tw = (Text) object;
-            ((BytesColumnVector) columnVector).setVal(
-                batchIndex, tw.getBytes(), 0, tw.getLength());
+            if (object instanceof String) {
+              String string = (String) object;
+              byte[] bytes = string.getBytes();
+              ((BytesColumnVector) columnVector).setVal(
+                  batchIndex, bytes, 0, bytes.length);
+            } else {
+              Text tw = (Text) object;
+              ((BytesColumnVector) columnVector).setVal(
+                  batchIndex, tw.getBytes(), 0, tw.getLength());
+            }
           }
           break;
         case VARCHAR:
@@ -463,12 +530,22 @@ private void assignRowColumn(
           }
           break;
         case INTERVAL_YEAR_MONTH:
-          ((LongColumnVector) columnVector).vector[batchIndex] =
-              ((HiveIntervalYearMonthWritable) object).getHiveIntervalYearMonth().getTotalMonths();
+          if (object instanceof HiveIntervalYearMonth) {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                ((HiveIntervalYearMonth) object).getTotalMonths();
+          } else {
+            ((LongColumnVector) columnVector).vector[batchIndex] =
+                ((HiveIntervalYearMonthWritable) object).getHiveIntervalYearMonth().getTotalMonths();
+          }
           break;
         case INTERVAL_DAY_TIME:
-          ((IntervalDayTimeColumnVector) columnVector).set(
-              batchIndex, ((HiveIntervalDayTimeWritable) object).getHiveIntervalDayTime());
+          if (object instanceof HiveIntervalDayTime) {
+            ((IntervalDayTimeColumnVector) columnVector).set(
+                batchIndex, (HiveIntervalDayTime) object);
+          } else {
+            ((IntervalDayTimeColumnVector) columnVector).set(
+                batchIndex, ((HiveIntervalDayTimeWritable) object).getHiveIntervalDayTime());
+          }
           break;
         default:
           throw new RuntimeException("Primitive category " + targetPrimitiveCategory.name() +
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
index 30916a03bb..642dd46798 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
@@ -947,7 +947,9 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
       for (int i = 0; i < aggregators.length; ++i) {
         aggregators[i].init(conf.getAggregators().get(i));
-        objectInspectors.add(aggregators[i].getOutputObjectInspector());
+        ObjectInspector objInsp = aggregators[i].getOutputObjectInspector();
+        Preconditions.checkState(objInsp != null);
+        objectInspectors.add(objInsp);
       }
 
       keyWrappersBatch = VectorHashKeyWrapperBatch.compileKeyWrapperBatch(keyExpressions);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
index 7dc4c812c7..503bd0c483 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
@@ -52,20 +52,25 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.InputExpressionType;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.*;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFBloomFilter;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFBloomFilterMerge;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountMerge;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFStdPopTimestamp;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFStdSampTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFVarPopTimestamp;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFVarSampTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgDecimal;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgDecimalComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgDecimalFinal;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgDecimalPartial2;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgDouble;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgDoubleComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgFinal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgLong;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgLongComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgPartial2;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgTimestampComplete;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxDecimal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxDouble;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxLong;
@@ -77,19 +82,44 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinString;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopDecimal;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopDecimalComplete;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopDouble;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopDoubleComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopFinal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopLong;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopLongComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopTimestampComplete;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampDecimal;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampDecimalComplete;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampDouble;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampDoubleComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampFinal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampLong;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampLongComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampTimestampComplete;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFSumDouble;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFSumLong;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPartial2;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopDecimal;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopDecimalComplete;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopDouble;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopDoubleComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopFinal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopLong;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopLongComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopTimestampComplete;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampDecimal;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampDecimalComplete;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampDouble;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampDoubleComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampFinal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampLong;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampLongComplete;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampTimestampComplete;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.*;
 import org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor;
 import org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc;
@@ -2643,12 +2673,19 @@ private Constructor<?> getConstructor(Class<?> cl) throws HiveException {
   }
 
   static String getScratchName(TypeInfo typeInfo) throws HiveException {
+
     // For now, leave DECIMAL precision/scale in the name so DecimalColumnVector scratch columns
     // don't need their precision/scale adjusted...
     if (typeInfo.getCategory() == Category.PRIMITIVE &&
         ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory() == PrimitiveCategory.DECIMAL) {
       return typeInfo.getTypeName();
     }
+
+    // And, for Complex Types, also leave the children types in place...
+    if (typeInfo.getCategory() != Category.PRIMITIVE) {
+      return typeInfo.getTypeName();
+    }
+
     Type columnVectorType = VectorizationContext.getColumnVectorTypeFromTypeInfo(typeInfo);
     return columnVectorType.name().toLowerCase();
   }
@@ -2819,6 +2856,13 @@ public static ColumnVector.Type getColumnVectorTypeFromTypeInfo(TypeInfo typeInf
     add(new AggregateDefinition("count",       ArgumentType.INT_FAMILY,                      Mode.PARTIAL2,     VectorUDAFCountMerge.class));
     add(new AggregateDefinition("count",       ArgumentType.INT_FAMILY,                      Mode.FINAL,        VectorUDAFCountMerge.class));
 
+    // TIMESTAMP SUM takes a TimestampColumnVector as input for PARTIAL1 and COMPLETE.
+    // But the output is a double.
+    add(new AggregateDefinition("sum",         ArgumentType.TIMESTAMP,                       Mode.PARTIAL1,     VectorUDAFSumTimestamp.class));
+    add(new AggregateDefinition("sum",         ArgumentType.TIMESTAMP,                       Mode.COMPLETE,     VectorUDAFSumTimestamp.class));
+    add(new AggregateDefinition("sum",         ArgumentType.TIMESTAMP,                       Mode.PARTIAL2,     VectorUDAFSumDouble.class));
+    add(new AggregateDefinition("sum",         ArgumentType.TIMESTAMP,                       Mode.FINAL,        VectorUDAFSumDouble.class));
+
     // Since the partial aggregation produced by AVG is a STRUCT with count and sum and the
     // STRUCT data type isn't vectorized yet, we currently only support PARTIAL1.  When we do
     // support STRUCTs for average partial aggregation, we'll need 4 variations:
@@ -2828,12 +2872,29 @@ public static ColumnVector.Type getColumnVectorTypeFromTypeInfo(TypeInfo typeInf
     //   FINAL         STRUCT Average Partial Aggregation   --> Full Aggregation
     //   COMPLETE      Original data                        --> Full Aggregation
     //
+    // NOTE: Since we do average of timestamps internally as double, we do not need a VectorUDAFAvgTimestampPartial2.
+    //
     add(new AggregateDefinition("avg",         ArgumentType.INT_FAMILY,                      Mode.PARTIAL1,     VectorUDAFAvgLong.class));
     add(new AggregateDefinition("avg",         ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL1,     VectorUDAFAvgDouble.class));
     add(new AggregateDefinition("avg",         ArgumentType.DECIMAL,                         Mode.PARTIAL1,     VectorUDAFAvgDecimal.class));
     add(new AggregateDefinition("avg",         ArgumentType.TIMESTAMP,                       Mode.PARTIAL1,     VectorUDAFAvgTimestamp.class));
 
-    // We haven't had a chance to examine the VAR* and STD* area and expand it beyond PARTIAL1.
+    // (PARTIAL2 FLOAT_FAMILY covers INT_FAMILY and TIMESTAMP because it is:
+    //    STRUCT Average Partial Aggregation   --> STRUCT Average Partial Aggregation
+    add(new AggregateDefinition("avg",         ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL2,     VectorUDAFAvgPartial2.class));
+    add(new AggregateDefinition("avg",         ArgumentType.DECIMAL,                         Mode.PARTIAL2,     VectorUDAFAvgDecimalPartial2.class));
+
+    // (FINAL FLOAT_FAMILY covers INT_FAMILY and TIMESTAMP)
+    add(new AggregateDefinition("avg",         ArgumentType.FLOAT_FAMILY,                    Mode.FINAL,        VectorUDAFAvgFinal.class));
+    add(new AggregateDefinition("avg",         ArgumentType.DECIMAL,                         Mode.FINAL,        VectorUDAFAvgDecimalFinal.class));
+    add(new AggregateDefinition("avg",         ArgumentType.TIMESTAMP,                       Mode.FINAL,        VectorUDAFAvgFinal.class));
+
+    add(new AggregateDefinition("avg",         ArgumentType.INT_FAMILY,                      Mode.COMPLETE,     VectorUDAFAvgLongComplete.class));
+    add(new AggregateDefinition("avg",         ArgumentType.FLOAT_FAMILY,                    Mode.COMPLETE,     VectorUDAFAvgDoubleComplete.class));
+    add(new AggregateDefinition("avg",         ArgumentType.DECIMAL,                         Mode.COMPLETE,     VectorUDAFAvgDecimalComplete.class));
+    add(new AggregateDefinition("avg",         ArgumentType.TIMESTAMP,                       Mode.COMPLETE,     VectorUDAFAvgTimestampComplete.class));
+
+    // We haven't had a chance to examine the VAR* and STD* area and expand it beyond PARTIAL1 and COMPLETE.
     add(new AggregateDefinition("variance",    ArgumentType.INT_FAMILY,                      Mode.PARTIAL1,     VectorUDAFVarPopLong.class));
     add(new AggregateDefinition("var_pop",     ArgumentType.INT_FAMILY,                      Mode.PARTIAL1,     VectorUDAFVarPopLong.class));
     add(new AggregateDefinition("variance",    ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL1,     VectorUDAFVarPopDouble.class));
@@ -2863,6 +2924,52 @@ public static ColumnVector.Type getColumnVectorTypeFromTypeInfo(TypeInfo typeInf
     add(new AggregateDefinition("stddev_samp", ArgumentType.DECIMAL,                         Mode.PARTIAL1,     VectorUDAFStdSampDecimal.class));
     add(new AggregateDefinition("stddev_samp", ArgumentType.TIMESTAMP,                       Mode.PARTIAL1,     VectorUDAFStdSampTimestamp.class));
 
+    add(new AggregateDefinition("variance",    ArgumentType.INT_FAMILY,                      Mode.COMPLETE,     VectorUDAFVarPopLongComplete.class));
+    add(new AggregateDefinition("var_pop",     ArgumentType.INT_FAMILY,                      Mode.COMPLETE,     VectorUDAFVarPopLongComplete.class));
+    add(new AggregateDefinition("variance",    ArgumentType.FLOAT_FAMILY,                    Mode.COMPLETE,     VectorUDAFVarPopDoubleComplete.class));
+    add(new AggregateDefinition("var_pop",     ArgumentType.FLOAT_FAMILY,                    Mode.COMPLETE,     VectorUDAFVarPopDoubleComplete.class));
+    add(new AggregateDefinition("variance",    ArgumentType.DECIMAL,                         Mode.COMPLETE,     VectorUDAFVarPopDecimalComplete.class));
+    add(new AggregateDefinition("var_pop",     ArgumentType.DECIMAL,                         Mode.COMPLETE,     VectorUDAFVarPopDecimalComplete.class));
+    add(new AggregateDefinition("variance",    ArgumentType.TIMESTAMP,                       Mode.COMPLETE,     VectorUDAFVarPopTimestampComplete.class));
+    add(new AggregateDefinition("var_pop",     ArgumentType.TIMESTAMP,                       Mode.COMPLETE,     VectorUDAFVarPopTimestampComplete.class));
+    add(new AggregateDefinition("var_samp",    ArgumentType.INT_FAMILY,                      Mode.COMPLETE,     VectorUDAFVarSampLongComplete.class));
+    add(new AggregateDefinition("var_samp" ,   ArgumentType.FLOAT_FAMILY,                    Mode.COMPLETE,     VectorUDAFVarSampDoubleComplete.class));
+    add(new AggregateDefinition("var_samp" ,   ArgumentType.DECIMAL,                         Mode.COMPLETE,     VectorUDAFVarSampDecimalComplete.class));
+    add(new AggregateDefinition("var_samp" ,   ArgumentType.TIMESTAMP,                       Mode.COMPLETE,     VectorUDAFVarSampTimestampComplete.class));
+    add(new AggregateDefinition("std",         ArgumentType.INT_FAMILY,                      Mode.COMPLETE,     VectorUDAFStdPopLongComplete.class));
+    add(new AggregateDefinition("stddev",      ArgumentType.INT_FAMILY,                      Mode.COMPLETE,     VectorUDAFStdPopLongComplete.class));
+    add(new AggregateDefinition("stddev_pop",  ArgumentType.INT_FAMILY,                      Mode.COMPLETE,     VectorUDAFStdPopLongComplete.class));
+    add(new AggregateDefinition("std",         ArgumentType.FLOAT_FAMILY,                    Mode.COMPLETE,     VectorUDAFStdPopDoubleComplete.class));
+    add(new AggregateDefinition("stddev",      ArgumentType.FLOAT_FAMILY,                    Mode.COMPLETE,     VectorUDAFStdPopDoubleComplete.class));
+    add(new AggregateDefinition("stddev_pop",  ArgumentType.FLOAT_FAMILY,                    Mode.COMPLETE,     VectorUDAFStdPopDoubleComplete.class));
+    add(new AggregateDefinition("std",         ArgumentType.DECIMAL,                         Mode.COMPLETE,     VectorUDAFStdPopDecimalComplete.class));
+    add(new AggregateDefinition("stddev",      ArgumentType.DECIMAL,                         Mode.COMPLETE,     VectorUDAFStdPopDecimalComplete.class));
+    add(new AggregateDefinition("stddev_pop",  ArgumentType.DECIMAL,                         Mode.COMPLETE,     VectorUDAFStdPopDecimalComplete.class));
+    add(new AggregateDefinition("std",         ArgumentType.TIMESTAMP,                       Mode.COMPLETE,     VectorUDAFStdPopTimestampComplete.class));
+    add(new AggregateDefinition("stddev",      ArgumentType.TIMESTAMP,                       Mode.COMPLETE,     VectorUDAFStdPopTimestampComplete.class));
+    add(new AggregateDefinition("stddev_pop",  ArgumentType.TIMESTAMP,                       Mode.COMPLETE,     VectorUDAFStdPopTimestampComplete.class));
+    add(new AggregateDefinition("stddev_samp", ArgumentType.INT_FAMILY,                      Mode.COMPLETE,     VectorUDAFStdSampLongComplete.class));
+    add(new AggregateDefinition("stddev_samp", ArgumentType.FLOAT_FAMILY,                    Mode.COMPLETE,     VectorUDAFStdSampDoubleComplete.class));
+    add(new AggregateDefinition("stddev_samp", ArgumentType.DECIMAL,                         Mode.COMPLETE,     VectorUDAFStdSampDecimalComplete.class));
+    add(new AggregateDefinition("stddev_samp", ArgumentType.TIMESTAMP,                       Mode.COMPLETE,     VectorUDAFStdSampTimestampComplete.class));
+
+    // (PARTIAL2L FLOAT_FAMILY covers INT_FAMILY, DECIMAL, and TIMESTAMP)
+    add(new AggregateDefinition("variance",    ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL2,     VectorUDAFVarPartial2.class));
+    add(new AggregateDefinition("var_pop",     ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL2,     VectorUDAFVarPartial2.class));
+    add(new AggregateDefinition("var_samp",    ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL2,     VectorUDAFVarPartial2.class));
+    add(new AggregateDefinition("std",         ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL2,     VectorUDAFVarPartial2.class));
+    add(new AggregateDefinition("stddev",      ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL2,     VectorUDAFVarPartial2.class));
+    add(new AggregateDefinition("stddev_pop",  ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL2,     VectorUDAFVarPartial2.class));
+    add(new AggregateDefinition("stddev_samp", ArgumentType.FLOAT_FAMILY,                    Mode.PARTIAL2,     VectorUDAFVarPartial2.class));
+
+    add(new AggregateDefinition("variance",    ArgumentType.FLOAT_FAMILY,                    Mode.FINAL,        VectorUDAFVarPopFinal.class));
+    add(new AggregateDefinition("var_pop",     ArgumentType.FLOAT_FAMILY,                    Mode.FINAL,        VectorUDAFVarPopFinal.class));
+    add(new AggregateDefinition("var_samp",    ArgumentType.FLOAT_FAMILY,                    Mode.FINAL,        VectorUDAFVarSampFinal.class));
+    add(new AggregateDefinition("std",         ArgumentType.FLOAT_FAMILY,                    Mode.FINAL,        VectorUDAFStdPopFinal.class));
+    add(new AggregateDefinition("stddev",      ArgumentType.FLOAT_FAMILY,                    Mode.FINAL,        VectorUDAFStdPopFinal.class));
+    add(new AggregateDefinition("stddev_pop",  ArgumentType.FLOAT_FAMILY,                    Mode.FINAL,        VectorUDAFStdPopFinal.class));
+    add(new AggregateDefinition("stddev_samp", ArgumentType.FLOAT_FAMILY,                    Mode.FINAL,        VectorUDAFStdSampFinal.class));
+
     // UDAFBloomFilter. Original data is one type, partial/final is another,
     // so this requires 2 aggregation classes (partial1/complete), (partial2/final)
     add(new AggregateDefinition("bloom_filter", ArgumentType.ALL_FAMILY,                     Mode.PARTIAL1,     VectorUDAFBloomFilter.class));
@@ -2885,16 +2992,42 @@ public VectorAggregateExpression getAggregatorExpression(AggregationDesc desc)
 
     String aggregateName = desc.getGenericUDAFName();
     VectorExpressionDescriptor.ArgumentType inputType = VectorExpressionDescriptor.ArgumentType.NONE;
+    GenericUDAFEvaluator.Mode udafEvaluatorMode = desc.getMode();
 
     if (paramDescList.size() > 0) {
       ExprNodeDesc inputExpr = paramDescList.get(0);
-      inputType = VectorExpressionDescriptor.ArgumentType.fromHiveTypeName(inputExpr.getTypeString());
-      if (inputType == VectorExpressionDescriptor.ArgumentType.NONE) {
-        throw new HiveException("No vector argument type for Hive type name " + inputExpr.getTypeString());
+      TypeInfo inputTypeInfo = inputExpr.getTypeInfo();
+      if (inputTypeInfo.getCategory() == Category.STRUCT) {
+
+        // Must be AVG or one of the variance aggregations doing PARTIAL2 or FINAL.
+        // E.g. AVG PARTIAL2 and FINAL accept struct<count:bigint,sum:double,input:double>
+        if (udafEvaluatorMode != GenericUDAFEvaluator.Mode.PARTIAL2 &&
+            udafEvaluatorMode != GenericUDAFEvaluator.Mode.FINAL) {
+          throw new HiveException("Input expression Hive type name " + inputExpr.getTypeString() + " and group by mode is " + udafEvaluatorMode.name() +
+              " -- expected PARTIAL2 or FINAL");
+        }
+        GenericUDAFEvaluator evaluator = desc.getGenericUDAFEvaluator();
+
+        // UNDONE: What about AVG FINAL TIMESTAMP?
+        if (evaluator instanceof GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble ||
+            evaluator instanceof GenericUDAFVariance.GenericUDAFVarianceEvaluator) {
+          inputType = VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY;
+        } else if (evaluator instanceof GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal) {
+          inputType = VectorExpressionDescriptor.ArgumentType.DECIMAL;
+        } else {
+          // Nothing else supported yet...
+          throw new HiveException("Evaluator " + evaluator.getClass().getName() + " not supported");
+        }
+      } else {
+        String inputExprTypeString = inputTypeInfo.getTypeName();
+
+        inputType = VectorExpressionDescriptor.ArgumentType.fromHiveTypeName(inputExpr.getTypeString());
+        if (inputType == VectorExpressionDescriptor.ArgumentType.NONE) {
+          throw new HiveException("No vector argument type for Hive type name " + inputExpr.getTypeString());
+        }
       }
     }
 
-    GenericUDAFEvaluator.Mode udafEvaluatorMode = desc.getMode();
     for (AggregateDefinition aggDef : aggregatesDefinition) {
       if (aggregateName.equalsIgnoreCase(aggDef.getName()) &&
           ((aggDef.getType() == VectorExpressionDescriptor.ArgumentType.NONE &&
@@ -2911,14 +3044,14 @@ public VectorAggregateExpression getAggregatorExpression(AggregationDesc desc)
         try
         {
           Constructor<? extends VectorAggregateExpression> ctor =
-              aggClass.getConstructor(VectorExpression.class);
+              aggClass.getConstructor(VectorExpression.class, GenericUDAFEvaluator.Mode.class);
           VectorAggregateExpression aggExpr = ctor.newInstance(
-              vectorParams.length > 0 ? vectorParams[0] : null);
+              vectorParams.length > 0 ? vectorParams[0] : null, udafEvaluatorMode);
           aggExpr.init(desc);
           return aggExpr;
         } catch (Exception e) {
           throw new HiveException("Internal exception for vector aggregate : \"" +
-               aggregateName + "\" for type: \"" + inputType + "", e);
+               aggregateName + "\" for type: \"" + inputType + "\": " + getStackTraceAsSingleLine(e));
         }
       }
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java
index c20bc685e4..1fb70f87e2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java
@@ -38,7 +38,10 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.SettableListObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.SettableStructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.SettableUnionObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBinaryObjectInspector;
@@ -564,6 +567,50 @@ public static VectorExpressionWriter genVectorExpressionWritable(ExprNodeDesc no
       return genVectorExpressionWritable(objectInspector);
     }
 
+    /**
+     * Specialized writer for ListColumnVector. Will throw cast exception
+     * if the wrong vector column is used.
+     */
+    private static abstract class VectorExpressionWriterList extends VectorExpressionWriterBase {
+
+      // For now, we just use this to hold the object inspector.  There are no writeValue,
+      // setValue, etc methods yet...
+
+    }
+
+    /**
+     * Specialized writer for MapColumnVector. Will throw cast exception
+     * if the wrong vector column is used.
+     */
+    private static abstract class VectorExpressionWriterMap extends VectorExpressionWriterBase {
+
+      // For now, we just use this to hold the object inspector.  There are no writeValue,
+      // setValue, etc methods yet...
+
+    }
+
+    /**
+     * Specialized writer for StructColumnVector. Will throw cast exception
+     * if the wrong vector column is used.
+     */
+    private static abstract class VectorExpressionWriterStruct extends VectorExpressionWriterBase {
+
+      // For now, we just use this to hold the object inspector.  There are no writeValue,
+      // setValue, etc methods yet...
+
+    }
+
+    /**
+     * Specialized writer for UnionColumnVector. Will throw cast exception
+     * if the wrong vector column is used.
+     */
+    private static abstract class VectorExpressionWriterUnion extends VectorExpressionWriterBase {
+
+      // For now, we just use this to hold the object inspector.  There are no writeValue,
+      // setValue, etc methods yet...
+
+    }
+
     /**
      * Compiles the appropriate vector expression writer based on an expression info (ExprNodeDesc)
      */
@@ -629,11 +676,22 @@ public static VectorExpressionWriter genVectorExpressionWritable(
               ((PrimitiveObjectInspector) fieldObjInspector).getPrimitiveCategory());
         }
 
+      case LIST:
+        return genVectorExpressionWritableList(
+            (SettableListObjectInspector) fieldObjInspector);
+
+      case MAP:
+        return genVectorExpressionWritableMap(
+            (SettableMapObjectInspector) fieldObjInspector);
+
       case STRUCT:
+        return genVectorExpressionWritableStruct(
+            (SettableStructObjectInspector) fieldObjInspector);
+
       case UNION:
-      case MAP:
-      case LIST:
-        return genVectorExpressionWritableEmpty();
+        return genVectorExpressionWritableUnion(
+            (SettableUnionObjectInspector) fieldObjInspector);
+
       default:
         throw new IllegalArgumentException("Unknown type " +
             fieldObjInspector.getCategory());
@@ -1339,6 +1397,130 @@ public Object initValue(Object ignored) {
     }.init(fieldObjInspector);
   }
 
+  private static VectorExpressionWriter genVectorExpressionWritableList(
+      SettableListObjectInspector fieldObjInspector) throws HiveException {
+
+    return new VectorExpressionWriterList() {
+      private Object obj;
+
+      public VectorExpressionWriter init(SettableListObjectInspector objInspector) throws HiveException {
+        super.init(objInspector);
+        obj = initValue(null);
+        return this;
+      }
+
+      @Override
+      public Object initValue(Object ignored) {
+        return ((SettableListObjectInspector) this.objectInspector).create(0);
+      }
+
+      @Override
+      public Object writeValue(ColumnVector column, int row)
+          throws HiveException {
+        throw new HiveException("Not implemented yet");
+      }
+
+      @Override
+      public Object setValue(Object row, ColumnVector column, int columnRow)
+          throws HiveException {
+        throw new HiveException("Not implemented yet");
+      }
+    }.init(fieldObjInspector);
+  }
+
+  private static VectorExpressionWriter genVectorExpressionWritableMap(
+      SettableMapObjectInspector fieldObjInspector) throws HiveException {
+
+    return new VectorExpressionWriterMap() {
+      private Object obj;
+
+      public VectorExpressionWriter init(SettableMapObjectInspector objInspector) throws HiveException {
+        super.init(objInspector);
+        obj = initValue(null);
+        return this;
+      }
+
+      @Override
+      public Object initValue(Object ignored) {
+        return ((SettableMapObjectInspector) this.objectInspector).create();
+      }
+
+      @Override
+      public Object writeValue(ColumnVector column, int row)
+          throws HiveException {
+        throw new HiveException("Not implemented yet");
+      }
+
+      @Override
+      public Object setValue(Object row, ColumnVector column, int columnRow)
+          throws HiveException {
+        throw new HiveException("Not implemented yet");
+      }
+    }.init(fieldObjInspector);
+  }
+
+  private static VectorExpressionWriter genVectorExpressionWritableStruct(
+      SettableStructObjectInspector fieldObjInspector) throws HiveException {
+
+    return new VectorExpressionWriterMap() {
+      private Object obj;
+
+      public VectorExpressionWriter init(SettableStructObjectInspector objInspector) throws HiveException {
+        super.init(objInspector);
+        obj = initValue(null);
+        return this;
+      }
+
+      @Override
+      public Object initValue(Object ignored) {
+        return ((SettableStructObjectInspector) this.objectInspector).create();
+      }
+
+      @Override
+      public Object writeValue(ColumnVector column, int row)
+          throws HiveException {
+        throw new HiveException("Not implemented yet");
+      }
+
+      @Override
+      public Object setValue(Object row, ColumnVector column, int columnRow)
+          throws HiveException {
+        throw new HiveException("Not implemented yet");
+      }
+    }.init(fieldObjInspector);
+  }
+
+  private static VectorExpressionWriter genVectorExpressionWritableUnion(
+      SettableUnionObjectInspector fieldObjInspector) throws HiveException {
+
+    return new VectorExpressionWriterMap() {
+      private Object obj;
+
+      public VectorExpressionWriter init(SettableUnionObjectInspector objInspector) throws HiveException {
+        super.init(objInspector);
+        obj = initValue(null);
+        return this;
+      }
+
+      @Override
+      public Object initValue(Object ignored) {
+        return ((SettableUnionObjectInspector) this.objectInspector).create();
+      }
+
+      @Override
+      public Object writeValue(ColumnVector column, int row)
+          throws HiveException {
+        throw new HiveException("Not implemented yet");
+      }
+
+      @Override
+      public Object setValue(Object row, ColumnVector column, int columnRow)
+          throws HiveException {
+        throw new HiveException("Not implemented yet");
+      }
+    }.init(fieldObjInspector);
+  }
+
   // For complex types like STRUCT, MAP, etc we do not support, we need a writer that
   // does nothing.  We assume the Vectorizer class has not validated the query to actually
   // try and use the complex types.  They do show up in inputObjInspector[0] and need to be
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorAggregateExpression.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorAggregateExpression.java
index 7ab4473764..702c3d5a42 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorAggregateExpression.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorAggregateExpression.java
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 
 /**
@@ -34,6 +35,19 @@ public abstract class VectorAggregateExpression  implements Serializable {
 
   private static final long serialVersionUID = 1L;
 
+  protected final VectorExpression inputExpression;
+  protected final GenericUDAFEvaluator.Mode mode;
+
+  public VectorAggregateExpression(VectorExpression inputExpression,
+      GenericUDAFEvaluator.Mode mode) {
+    this.inputExpression = inputExpression;
+    this.mode = mode;
+  }
+
+  public VectorExpression getInputExpression() {
+    return inputExpression;
+  }
+
   /**
    * Buffer interface to store aggregates.
    */
@@ -56,7 +70,6 @@ public abstract void aggregateInputSelection(VectorAggregationBufferRow[] aggreg
   public boolean hasVariableSize() {
     return false;
   }
-  public abstract VectorExpression inputExpression();
 
   public abstract void init(AggregationDesc desc) throws HiveException;
 
@@ -64,7 +77,7 @@ public boolean hasVariableSize() {
   public String toString() {
     StringBuilder sb = new StringBuilder();
     sb.append(this.getClass().getSimpleName());
-    VectorExpression inputExpression = inputExpression();
+    VectorExpression inputExpression = getInputExpression();
     if (inputExpression != null) {
       sb.append("(");
       sb.append(inputExpression.toString());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilter.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilter.java
index 52b05ca1a6..4b3eca093c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilter.java
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -44,7 +45,6 @@
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hive.common.util.BloomFilter;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -54,18 +54,11 @@ public class VectorUDAFBloomFilter extends VectorAggregateExpression {
 
   private static final long serialVersionUID = 1L;
 
-  private VectorExpression inputExpression;
-
-  @Override
-  public VectorExpression inputExpression() {
-    return inputExpression;
-  }
-
   private long expectedEntries = -1;
   private ValueProcessor valueProcessor;
-  transient private int bitSetSize = -1;
-  transient private BytesWritable bw = new BytesWritable();
-  transient private ByteArrayOutputStream byteStream = new ByteArrayOutputStream();
+  transient private int bitSetSize;
+  transient private BytesWritable bw;
+  transient private ByteArrayOutputStream byteStream;
 
   /**
    * class for storing the current aggregate value.
@@ -90,9 +83,15 @@ public void reset() {
     }
   }
 
-  public VectorUDAFBloomFilter(VectorExpression inputExpression) {
-    this();
-    this.inputExpression = inputExpression;
+  public VectorUDAFBloomFilter(VectorExpression inputExpression,
+      GenericUDAFEvaluator.Mode mode) {
+    super(inputExpression, mode);
+  }
+
+  private void init() {
+    bitSetSize = -1;
+    bw = new BytesWritable();
+    byteStream = new ByteArrayOutputStream();
 
     // Instantiate the ValueProcessor based on the input type
     VectorExpressionDescriptor.ArgumentType inputType =
@@ -123,10 +122,6 @@ public VectorUDAFBloomFilter(VectorExpression inputExpression) {
     }
   }
 
-  public VectorUDAFBloomFilter() {
-    super();
-  }
-
   @Override
   public AggregationBuffer getNewAggregationBuffer() throws HiveException {
     if (expectedEntries < 0) {
@@ -405,19 +400,13 @@ public long getAggregationBufferFixedSize() {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
+    init();
+
     GenericUDAFBloomFilterEvaluator udafBloomFilter =
         (GenericUDAFBloomFilterEvaluator) desc.getGenericUDAFEvaluator();
     expectedEntries = udafBloomFilter.getExpectedEntries();
   }
 
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
-  }
-
   public long getExpectedEntries() {
     return expectedEntries;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilterMerge.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilterMerge.java
index b986eb4125..67a7c508a3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilterMerge.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilterMerge.java
@@ -29,6 +29,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -39,16 +40,9 @@ public class VectorUDAFBloomFilterMerge extends VectorAggregateExpression {
 
   private static final long serialVersionUID = 1L;
 
-  private VectorExpression inputExpression;
-
-  @Override
-  public VectorExpression inputExpression() {
-    return inputExpression;
-  }
-
   private long expectedEntries = -1;
-  transient private int aggBufferSize = -1;
-  transient private BytesWritable bw = new BytesWritable();
+  transient private int aggBufferSize;
+  transient private BytesWritable bw;
 
   /**
    * class for storing the current aggregate value.
@@ -81,13 +75,14 @@ public void reset() {
     }
   }
 
-  public VectorUDAFBloomFilterMerge(VectorExpression inputExpression) {
-    this();
-    this.inputExpression = inputExpression;
+  public VectorUDAFBloomFilterMerge(VectorExpression inputExpression,
+      GenericUDAFEvaluator.Mode mode) {
+    super(inputExpression, mode);
   }
 
-  public VectorUDAFBloomFilterMerge() {
-    super();
+  private void init() {
+    aggBufferSize = -1;
+    bw = new BytesWritable();
   }
 
   @Override
@@ -355,6 +350,8 @@ public long getAggregationBufferFixedSize() {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
+    init();
+
     GenericUDAFBloomFilterEvaluator udafBloomFilter =
         (GenericUDAFBloomFilterEvaluator) desc.getGenericUDAFEvaluator();
     expectedEntries = udafBloomFilter.getExpectedEntries();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCount.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCount.java
index cadb6dd9f8..d9490c39cc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCount.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCount.java
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -59,22 +60,13 @@ public void reset() {
       }
     }
 
-    private VectorExpression inputExpression = null;
+    transient private LongWritable result;
 
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
-    transient private final LongWritable result;
-
-    public VectorUDAFCount(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+    public VectorUDAFCount(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    public VectorUDAFCount() {
-      super();
+    private void init() {
       result = new LongWritable(0);
     }
 
@@ -270,15 +262,7 @@ public long getAggregationBufferFixedSize() {
 
     @Override
     public void init(AggregationDesc desc) throws HiveException {
-      // No-op
-    }
-
-    public VectorExpression getInputExpression() {
-      return inputExpression;
-    }
-
-    public void setInputExpression(VectorExpression inputExpression) {
-      this.inputExpression = inputExpression;
+      init();
     }
 }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCountMerge.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCountMerge.java
index c489f8f5a6..10a8660a68 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCountMerge.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCountMerge.java
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -60,22 +61,13 @@ public void reset() {
       }
     }
 
-    private VectorExpression inputExpression = null;
+    transient private LongWritable result;
 
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
-    transient private final LongWritable result;
-
-    public VectorUDAFCountMerge(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
+    public VectorUDAFCountMerge(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    public VectorUDAFCountMerge() {
-      super();
+    private void init() {
       result = new LongWritable(0);
     }
 
@@ -396,15 +388,7 @@ public long getAggregationBufferFixedSize() {
 
     @Override
     public void init(AggregationDesc desc) throws HiveException {
-      // No-op
-    }
-
-    public VectorExpression getInputExpression() {
-      return inputExpression;
-    }
-
-    public void setInputExpression(VectorExpression inputExpression) {
-      this.inputExpression = inputExpression;
+      init();
     }
 }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCountStar.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCountStar.java
index 3b6603092a..3bc6a71de0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCountStar.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCountStar.java
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -57,21 +58,13 @@ public void reset() {
       }
     }
 
+    transient private LongWritable result;
 
-    @Override
-    public VectorExpression inputExpression() {
-      // None.
-      return null;
+    public VectorUDAFCountStar(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    transient private final LongWritable result;
-
-    public VectorUDAFCountStar(VectorExpression inputExpression) {
-      this();
-    }
-
-    public VectorUDAFCountStar() {
-      super();
+    private void init() {
       result = new LongWritable(0);
     }
 
@@ -153,7 +146,6 @@ public long getAggregationBufferFixedSize() {
 
     @Override
     public void init(AggregationDesc desc) throws HiveException {
-      // No-op
+      init();
     }
 }
-
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdPopTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdPopTimestamp.java
deleted file mode 100644
index 5388d37638..0000000000
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdPopTimestamp.java
+++ /dev/null
@@ -1,533 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.plan.AggregationDesc;
-import org.apache.hadoop.hive.ql.util.JavaDataModel;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-
-/**
-* VectorUDAFStdPopTimestamp. Vectorized implementation for VARIANCE aggregates.
-*/
-@Description(name = "std,stddev,stddev_pop",
-    value = "_FUNC_(x) - Returns the standard deviation of a set of numbers (vectorized, timestamp)")
-public class VectorUDAFStdPopTimestamp extends VectorAggregateExpression {
-
-    private static final long serialVersionUID = 1L;
-
-    /**
-    /* class for storing the current aggregate value.
-    */
-    private static final class Aggregation implements AggregationBuffer {
-
-      private static final long serialVersionUID = 1L;
-
-      transient private double sum;
-      transient private long count;
-      transient private double variance;
-
-      /**
-      * Value is explicitly (re)initialized in reset() (despite the init() bellow...)
-      */
-      transient private boolean isNull = true;
-
-      public void init() {
-        isNull = false;
-        sum = 0;
-        count = 0;
-        variance = 0;
-      }
-
-      @Override
-      public int getVariableSize() {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public void reset () {
-        isNull = true;
-        sum = 0;
-        count = 0;
-        variance = 0;
-      }
-    }
-
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
-    transient private LongWritable resultCount;
-    transient private DoubleWritable resultSum;
-    transient private DoubleWritable resultVariance;
-    transient private Object[] partialResult;
-
-    transient private ObjectInspector soi;
-
-
-    public VectorUDAFStdPopTimestamp(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
-    }
-
-    public VectorUDAFStdPopTimestamp() {
-      super();
-      partialResult = new Object[3];
-      resultCount = new LongWritable();
-      resultSum = new DoubleWritable();
-      resultVariance = new DoubleWritable();
-      partialResult[0] = resultCount;
-      partialResult[1] = resultSum;
-      partialResult[2] = resultVariance;
-      initPartialResultInspector();
-    }
-
-  private void initPartialResultInspector() {
-        List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
-        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
-        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
-        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
-
-        List<String> fname = new ArrayList<String>();
-        fname.add("count");
-        fname.add("sum");
-        fname.add("variance");
-
-        soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
-    }
-
-    private Aggregation getCurrentAggregationBuffer(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        int row) {
-      VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
-      return myagg;
-    }
-
-
-    @Override
-    public void aggregateInputSelection(
-      VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregateIndex,
-      VectorizedRowBatch batch) throws HiveException {
-
-      inputExpression.evaluate(batch);
-
-      TimestampColumnVector inputColVector = (TimestampColumnVector)batch.
-        cols[this.inputExpression.getOutputColumn()];
-
-      int batchSize = batch.size;
-
-      if (batchSize == 0) {
-        return;
-      }
-
-      if (inputColVector.isRepeating) {
-        if (inputColVector.noNulls || !inputColVector.isNull[0]) {
-          iterateRepeatingNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector.getDouble(0), batchSize);
-        }
-      }
-      else if (!batch.selectedInUse && inputColVector.noNulls) {
-        iterateNoSelectionNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize);
-      }
-      else if (!batch.selectedInUse) {
-        iterateNoSelectionHasNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize, inputColVector.isNull);
-      }
-      else if (inputColVector.noNulls){
-        iterateSelectionNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize, batch.selected);
-      }
-      else {
-        iterateSelectionHasNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize,
-            inputColVector.isNull, batch.selected);
-      }
-
-    }
-
-    private void  iterateRepeatingNoNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        double value,
-        int batchSize) {
-
-      for (int i=0; i<batchSize; ++i) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          i);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-      }
-    }
-
-    private void iterateSelectionHasNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull,
-        int[] selected) {
-
-      for (int j=0; j< batchSize; ++j) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          j);
-        int i = selected[j];
-        if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-          if(myagg.count > 1) {
-            double t = myagg.count*value - myagg.sum;
-            myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-          }
-        }
-      }
-    }
-
-    private void iterateSelectionNoNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        int[] selected) {
-
-      for (int i=0; i< batchSize; ++i) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          i);
-        double value = inputColVector.getDouble(selected[i]);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-      }
-    }
-
-    private void iterateNoSelectionHasNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull) {
-
-      for(int i=0;i<batchSize;++i) {
-        if (!isNull[i]) {
-          Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets,
-            aggregateIndex,
-          i);
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-        }
-      }
-    }
-
-    private void iterateNoSelectionNoNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize) {
-
-      for (int i=0; i<batchSize; ++i) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          i);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        double value = inputColVector.getDouble(i);
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-      }
-    }
-
-    @Override
-    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
-    throws HiveException {
-
-      inputExpression.evaluate(batch);
-
-      TimestampColumnVector inputColVector = (TimestampColumnVector)batch.
-        cols[this.inputExpression.getOutputColumn()];
-
-      int batchSize = batch.size;
-
-      if (batchSize == 0) {
-        return;
-      }
-
-      Aggregation myagg = (Aggregation)agg;
-
-      if (inputColVector.isRepeating) {
-        if (inputColVector.noNulls) {
-          iterateRepeatingNoNulls(myagg, inputColVector.getDouble(0), batchSize);
-        }
-      }
-      else if (!batch.selectedInUse && inputColVector.noNulls) {
-        iterateNoSelectionNoNulls(myagg, inputColVector, batchSize);
-      }
-      else if (!batch.selectedInUse) {
-        iterateNoSelectionHasNulls(myagg, inputColVector, batchSize, inputColVector.isNull);
-      }
-      else if (inputColVector.noNulls){
-        iterateSelectionNoNulls(myagg, inputColVector, batchSize, batch.selected);
-      }
-      else {
-        iterateSelectionHasNulls(myagg, inputColVector, batchSize, inputColVector.isNull, batch.selected);
-      }
-    }
-
-    private void  iterateRepeatingNoNulls(
-        Aggregation myagg,
-        double value,
-        int batchSize) {
-
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      // TODO: conjure a formula w/o iterating
-      //
-
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // We pulled out i=0 so we can remove the count > 1 check in the loop
-      for (int i=1; i<batchSize; ++i) {
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-    }
-
-    private void iterateSelectionHasNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull,
-        int[] selected) {
-
-      for (int j=0; j< batchSize; ++j) {
-        int i = selected[j];
-        if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-        }
-      }
-    }
-
-    private void iterateSelectionNoNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        int[] selected) {
-
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      double value = inputColVector.getDouble(selected[0]);
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // i=0 was pulled out to remove the count > 1 check in the loop
-      //
-      for (int i=1; i< batchSize; ++i) {
-        value = inputColVector.getDouble(selected[i]);
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-    }
-
-    private void iterateNoSelectionHasNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull) {
-
-      for(int i=0;i<batchSize;++i) {
-        if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-        }
-      }
-    }
-
-    private void iterateNoSelectionNoNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize) {
-
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      double value = inputColVector.getDouble(0);
-      myagg.sum += value;
-      myagg.count += 1;
-
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // i=0 was pulled out to remove count > 1 check
-      for (int i=1; i<batchSize; ++i) {
-        value = inputColVector.getDouble(i);
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-    }
-
-    @Override
-    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
-      return new Aggregation();
-    }
-
-    @Override
-    public void reset(AggregationBuffer agg) throws HiveException {
-      Aggregation myAgg = (Aggregation) agg;
-      myAgg.reset();
-    }
-
-    @Override
-    public Object evaluateOutput(
-        AggregationBuffer agg) throws HiveException {
-      Aggregation myagg = (Aggregation) agg;
-      if (myagg.isNull) {
-        return null;
-      }
-      else {
-        assert(0 < myagg.count);
-        resultCount.set (myagg.count);
-        resultSum.set (myagg.sum);
-        resultVariance.set (myagg.variance);
-        return partialResult;
-      }
-    }
-  @Override
-    public ObjectInspector getOutputObjectInspector() {
-      return soi;
-    }
-
-  @Override
-  public long getAggregationBufferFixedSize() {
-      JavaDataModel model = JavaDataModel.get();
-      return JavaDataModel.alignUp(
-        model.object() +
-        model.primitive2()*3+
-        model.primitive1(),
-        model.memoryAlign());
-  }
-
-  @Override
-  public void init(AggregationDesc desc) throws HiveException {
-    // No-op
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
-  }
-}
-
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdSampTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdSampTimestamp.java
deleted file mode 100644
index 1769dc032b..0000000000
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdSampTimestamp.java
+++ /dev/null
@@ -1,533 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.plan.AggregationDesc;
-import org.apache.hadoop.hive.ql.util.JavaDataModel;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-
-/**
-* VectorUDAFStdSampTimestamp. Vectorized implementation for VARIANCE aggregates.
-*/
-@Description(name = "stddev_samp",
-    value = "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, double)")
-public class VectorUDAFStdSampTimestamp extends VectorAggregateExpression {
-
-    private static final long serialVersionUID = 1L;
-
-    /**
-    /* class for storing the current aggregate value.
-    */
-    private static final class Aggregation implements AggregationBuffer {
-
-      private static final long serialVersionUID = 1L;
-
-      transient private double sum;
-      transient private long count;
-      transient private double variance;
-
-      /**
-      * Value is explicitly (re)initialized in reset() (despite the init() bellow...)
-      */
-      transient private boolean isNull = true;
-
-      public void init() {
-        isNull = false;
-        sum = 0;
-        count = 0;
-        variance = 0;
-      }
-
-      @Override
-      public int getVariableSize() {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public void reset () {
-        isNull = true;
-        sum = 0;
-        count = 0;
-        variance = 0;
-      }
-    }
-
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
-    transient private LongWritable resultCount;
-    transient private DoubleWritable resultSum;
-    transient private DoubleWritable resultVariance;
-    transient private Object[] partialResult;
-
-    transient private ObjectInspector soi;
-
-
-    public VectorUDAFStdSampTimestamp(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
-    }
-
-    public VectorUDAFStdSampTimestamp() {
-      super();
-      partialResult = new Object[3];
-      resultCount = new LongWritable();
-      resultSum = new DoubleWritable();
-      resultVariance = new DoubleWritable();
-      partialResult[0] = resultCount;
-      partialResult[1] = resultSum;
-      partialResult[2] = resultVariance;
-      initPartialResultInspector();
-    }
-
-  private void initPartialResultInspector() {
-        List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
-        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
-        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
-        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
-
-        List<String> fname = new ArrayList<String>();
-        fname.add("count");
-        fname.add("sum");
-        fname.add("variance");
-
-        soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
-    }
-
-    private Aggregation getCurrentAggregationBuffer(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        int row) {
-      VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
-      return myagg;
-    }
-
-
-    @Override
-    public void aggregateInputSelection(
-      VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregateIndex,
-      VectorizedRowBatch batch) throws HiveException {
-
-      inputExpression.evaluate(batch);
-
-      TimestampColumnVector inputColVector = (TimestampColumnVector)batch.
-        cols[this.inputExpression.getOutputColumn()];
-
-      int batchSize = batch.size;
-
-      if (batchSize == 0) {
-        return;
-      }
-
-      if (inputColVector.isRepeating) {
-        if (inputColVector.noNulls || !inputColVector.isNull[0]) {
-          iterateRepeatingNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector.getDouble(0), batchSize);
-        }
-      }
-      else if (!batch.selectedInUse && inputColVector.noNulls) {
-        iterateNoSelectionNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize);
-      }
-      else if (!batch.selectedInUse) {
-        iterateNoSelectionHasNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize, inputColVector.isNull);
-      }
-      else if (inputColVector.noNulls){
-        iterateSelectionNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize, batch.selected);
-      }
-      else {
-        iterateSelectionHasNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize,
-            inputColVector.isNull, batch.selected);
-      }
-
-    }
-
-    private void  iterateRepeatingNoNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        double value,
-        int batchSize) {
-
-      for (int i=0; i<batchSize; ++i) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          i);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-      }
-    }
-
-    private void iterateSelectionHasNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull,
-        int[] selected) {
-
-      for (int j=0; j< batchSize; ++j) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          j);
-        int i = selected[j];
-        if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-          if(myagg.count > 1) {
-            double t = myagg.count*value - myagg.sum;
-            myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-          }
-        }
-      }
-    }
-
-    private void iterateSelectionNoNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        int[] selected) {
-
-      for (int i=0; i< batchSize; ++i) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          i);
-        double value = inputColVector.getDouble(selected[i]);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-      }
-    }
-
-    private void iterateNoSelectionHasNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull) {
-
-      for(int i=0;i<batchSize;++i) {
-        if (!isNull[i]) {
-          Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets,
-            aggregateIndex,
-          i);
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-        }
-      }
-    }
-
-    private void iterateNoSelectionNoNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize) {
-
-      for (int i=0; i<batchSize; ++i) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          i);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        double value = inputColVector.getDouble(i);
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-      }
-    }
-
-    @Override
-    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
-    throws HiveException {
-
-      inputExpression.evaluate(batch);
-
-      TimestampColumnVector inputColVector = (TimestampColumnVector)batch.
-        cols[this.inputExpression.getOutputColumn()];
-
-      int batchSize = batch.size;
-
-      if (batchSize == 0) {
-        return;
-      }
-
-      Aggregation myagg = (Aggregation)agg;
-
-      if (inputColVector.isRepeating) {
-        if (inputColVector.noNulls) {
-          iterateRepeatingNoNulls(myagg, inputColVector.getDouble(0), batchSize);
-        }
-      }
-      else if (!batch.selectedInUse && inputColVector.noNulls) {
-        iterateNoSelectionNoNulls(myagg, inputColVector, batchSize);
-      }
-      else if (!batch.selectedInUse) {
-        iterateNoSelectionHasNulls(myagg, inputColVector, batchSize, inputColVector.isNull);
-      }
-      else if (inputColVector.noNulls){
-        iterateSelectionNoNulls(myagg, inputColVector, batchSize, batch.selected);
-      }
-      else {
-        iterateSelectionHasNulls(myagg, inputColVector, batchSize, inputColVector.isNull, batch.selected);
-      }
-    }
-
-    private void  iterateRepeatingNoNulls(
-        Aggregation myagg,
-        double value,
-        int batchSize) {
-
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      // TODO: conjure a formula w/o iterating
-      //
-
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // We pulled out i=0 so we can remove the count > 1 check in the loop
-      for (int i=1; i<batchSize; ++i) {
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-    }
-
-    private void iterateSelectionHasNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull,
-        int[] selected) {
-
-      for (int j=0; j< batchSize; ++j) {
-        int i = selected[j];
-        if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-        }
-      }
-    }
-
-    private void iterateSelectionNoNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        int[] selected) {
-
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      double value = inputColVector.getDouble(selected[0]);
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // i=0 was pulled out to remove the count > 1 check in the loop
-      //
-      for (int i=1; i< batchSize; ++i) {
-        value = inputColVector.getDouble(selected[i]);
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-    }
-
-    private void iterateNoSelectionHasNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull) {
-
-      for(int i=0;i<batchSize;++i) {
-        if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-        }
-      }
-    }
-
-    private void iterateNoSelectionNoNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize) {
-
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      double value = inputColVector.getDouble(0);
-      myagg.sum += value;
-      myagg.count += 1;
-
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // i=0 was pulled out to remove count > 1 check
-      for (int i=1; i<batchSize; ++i) {
-        value = inputColVector.getDouble(i);
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-    }
-
-    @Override
-    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
-      return new Aggregation();
-    }
-
-    @Override
-    public void reset(AggregationBuffer agg) throws HiveException {
-      Aggregation myAgg = (Aggregation) agg;
-      myAgg.reset();
-    }
-
-    @Override
-    public Object evaluateOutput(
-        AggregationBuffer agg) throws HiveException {
-      Aggregation myagg = (Aggregation) agg;
-      if (myagg.isNull) {
-        return null;
-      }
-      else {
-        assert(0 < myagg.count);
-        resultCount.set (myagg.count);
-        resultSum.set (myagg.sum);
-        resultVariance.set (myagg.variance);
-        return partialResult;
-      }
-    }
-  @Override
-    public ObjectInspector getOutputObjectInspector() {
-      return soi;
-    }
-
-  @Override
-  public long getAggregationBufferFixedSize() {
-      JavaDataModel model = JavaDataModel.get();
-      return JavaDataModel.alignUp(
-        model.object() +
-        model.primitive2()*3+
-        model.primitive1(),
-        model.memoryAlign());
-  }
-
-  @Override
-  public void init(AggregationDesc desc) throws HiveException {
-    // No-op
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
-  }
-}
-
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java
index a37e3f6fad..e3e8574e29 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java
@@ -27,10 +27,15 @@
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 
 /**
 * VectorUDAFSumDecimal. Vectorized implementation for SUM aggregates.
@@ -73,20 +78,22 @@ public void reset() {
       }
     }
 
-    private VectorExpression inputExpression;
+    private DecimalTypeInfo outputDecimalTypeInfo;
 
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
+    public VectorUDAFSumDecimal(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
     }
 
-    public VectorUDAFSumDecimal(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
-    }
+    private void init() {
 
-    public VectorUDAFSumDecimal() {
-      super();
+      String outputType = inputExpression.getOutputType();
+      DecimalTypeInfo inputDecimalTypeInfo =
+          (DecimalTypeInfo) TypeInfoUtils.getTypeInfoFromTypeString(outputType);
+
+      outputDecimalTypeInfo =
+          GenericUDAFSum.GenericUDAFSumHiveDecimal.getOutputDecimalTypeInfoForSum(
+              inputDecimalTypeInfo.getPrecision(), inputDecimalTypeInfo.getScale(),
+              this.mode);
     }
 
     private Aggregation getCurrentAggregationBuffer(
@@ -421,6 +428,8 @@ public Object evaluateOutput(AggregationBuffer agg) throws HiveException {
         return null;
       }
       else {
+        myagg.sum.mutateEnforcePrecisionScale(
+            outputDecimalTypeInfo.getPrecision(), outputDecimalTypeInfo.getScale());
         return myagg.sum.getHiveDecimal();
       }
     }
@@ -440,15 +449,7 @@ public long getAggregationBufferFixedSize() {
 
   @Override
   public void init(AggregationDesc desc) throws HiveException {
-    // No-op
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
+    init();
   }
 }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumTimestamp.java
new file mode 100644
index 0000000000..9651ad318f
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumTimestamp.java
@@ -0,0 +1,434 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates;
+
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
+import org.apache.hadoop.hive.ql.util.JavaDataModel;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
+
+/**
+* VectorUDAFSumTimestamp. Vectorized implementation for SUM aggregates.
+*/
+@Description(name = "sum",
+    value = "_FUNC_(expr) - Returns the sum value of expr (vectorized, type: <ValueType>)")
+public class VectorUDAFSumTimestamp extends VectorAggregateExpression {
+
+    private static final long serialVersionUID = 1L;
+
+    /**
+     * class for storing the current aggregate value.
+     */
+    private static final class Aggregation implements AggregationBuffer {
+
+      private static final long serialVersionUID = 1L;
+
+      transient private double sum;
+
+      /**
+      * Value is explicitly (re)initialized in reset()
+      */
+      transient private boolean isNull = true;
+
+      public void sumValue(double value) {
+        if (isNull) {
+          sum = value;
+          isNull = false;
+        } else {
+          sum += value;
+        }
+      }
+
+      @Override
+      public int getVariableSize() {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public void reset () {
+        isNull = true;
+        sum = 0;;
+      }
+    }
+
+    transient private DoubleWritable result;
+
+    public VectorUDAFSumTimestamp(VectorExpression inputExpression, GenericUDAFEvaluator.Mode mode) {
+      super(inputExpression, mode);
+    }
+
+    private void init() {
+      result = new DoubleWritable();
+    }
+
+    private Aggregation getCurrentAggregationBuffer(
+        VectorAggregationBufferRow[] aggregationBufferSets,
+        int aggregateIndex,
+        int row) {
+      VectorAggregationBufferRow mySet = aggregationBufferSets[row];
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
+      return myagg;
+    }
+
+    @Override
+    public void aggregateInputSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregateIndex,
+      VectorizedRowBatch batch) throws HiveException {
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      inputExpression.evaluate(batch);
+
+      TimestampColumnVector inputVector =
+          (TimestampColumnVector)batch.cols[this.inputExpression.getOutputColumn()];
+
+      if (inputVector.noNulls) {
+        if (inputVector.isRepeating) {
+          iterateNoNullsRepeatingWithAggregationSelection(
+            aggregationBufferSets, aggregateIndex,
+            inputVector.getDouble(0), batchSize);
+        } else {
+          if (batch.selectedInUse) {
+            iterateNoNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, aggregateIndex,
+              inputVector, batch.selected, batchSize);
+          } else {
+            iterateNoNullsWithAggregationSelection(
+              aggregationBufferSets, aggregateIndex,
+              inputVector, batchSize);
+          }
+        }
+      } else {
+        if (inputVector.isRepeating) {
+          if (batch.selectedInUse) {
+            iterateHasNullsRepeatingSelectionWithAggregationSelection(
+              aggregationBufferSets, aggregateIndex,
+              inputVector.getDouble(0), batchSize, batch.selected, inputVector.isNull);
+          } else {
+            iterateHasNullsRepeatingWithAggregationSelection(
+              aggregationBufferSets, aggregateIndex,
+              inputVector.getDouble(0), batchSize, inputVector.isNull);
+          }
+        } else {
+          if (batch.selectedInUse) {
+            iterateHasNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, aggregateIndex,
+              inputVector, batchSize, batch.selected, inputVector.isNull);
+          } else {
+            iterateHasNullsWithAggregationSelection(
+              aggregationBufferSets, aggregateIndex,
+              inputVector, batchSize, inputVector.isNull);
+          }
+        }
+      }
+    }
+
+    private void iterateNoNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregateIndex,
+      double value,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          aggregateIndex,
+          i);
+        myagg.sumValue(value);
+      }
+    }
+
+    private void iterateNoNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregateIndex,
+      TimestampColumnVector inputVector,
+      int[] selection,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          aggregateIndex,
+          i);
+        myagg.sumValue(inputVector.getDouble(selection[i]));
+      }
+    }
+
+    private void iterateNoNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregateIndex,
+      TimestampColumnVector inputVector,
+      int batchSize) {
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          aggregateIndex,
+          i);
+        myagg.sumValue(inputVector.getDouble(i));
+      }
+    }
+
+    private void iterateHasNullsRepeatingSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregateIndex,
+      double value,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      if (isNull[0]) {
+        return;
+      }
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          aggregateIndex,
+          i);
+        myagg.sumValue(value);
+      }
+
+    }
+
+    private void iterateHasNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregateIndex,
+      double value,
+      int batchSize,
+      boolean[] isNull) {
+
+      if (isNull[0]) {
+        return;
+      }
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          aggregateIndex,
+          i);
+        myagg.sumValue(value);
+      }
+    }
+
+    private void iterateHasNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregateIndex,
+      TimestampColumnVector inputVector,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      for (int j=0; j < batchSize; ++j) {
+        int i = selection[j];
+        if (!isNull[i]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            aggregateIndex,
+            j);
+          myagg.sumValue(inputVector.getDouble(i));
+        }
+      }
+   }
+
+    private void iterateHasNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregateIndex,
+      TimestampColumnVector inputVector,
+      int batchSize,
+      boolean[] isNull) {
+
+      for (int i=0; i < batchSize; ++i) {
+        if (!isNull[i]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            aggregateIndex,
+            i);
+          myagg.sumValue(inputVector.getDouble(i));
+        }
+      }
+    }
+
+    @Override
+    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
+    throws HiveException {
+
+      inputExpression.evaluate(batch);
+
+      TimestampColumnVector inputVector =
+          (TimestampColumnVector)batch.cols[this.inputExpression.getOutputColumn()];
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      Aggregation myagg = (Aggregation)agg;
+
+      if (inputVector.isRepeating) {
+        if (inputVector.noNulls) {
+        if (myagg.isNull) {
+          myagg.isNull = false;
+          myagg.sum = 0;
+        }
+        myagg.sum += inputVector.getDouble(0) * batchSize;
+      }
+        return;
+      }
+
+      if (!batch.selectedInUse && inputVector.noNulls) {
+        iterateNoSelectionNoNulls(myagg, inputVector, batchSize);
+      }
+      else if (!batch.selectedInUse) {
+        iterateNoSelectionHasNulls(myagg, inputVector, batchSize, inputVector.isNull);
+      }
+      else if (inputVector.noNulls){
+        iterateSelectionNoNulls(myagg, inputVector, batchSize, batch.selected);
+      }
+      else {
+        iterateSelectionHasNulls(myagg, inputVector, batchSize, inputVector.isNull, batch.selected);
+      }
+    }
+
+    private void iterateSelectionHasNulls(
+        Aggregation myagg,
+        TimestampColumnVector inputVector,
+        int batchSize,
+        boolean[] isNull,
+        int[] selected) {
+
+      for (int j=0; j< batchSize; ++j) {
+        int i = selected[j];
+        if (!isNull[i]) {
+          if (myagg.isNull) {
+            myagg.isNull = false;
+            myagg.sum = 0;
+          }
+          myagg.sum += inputVector.getDouble(i);
+        }
+      }
+    }
+
+    private void iterateSelectionNoNulls(
+        Aggregation myagg,
+        TimestampColumnVector inputVector,
+        int batchSize,
+        int[] selected) {
+
+      if (myagg.isNull) {
+        myagg.sum = 0;
+        myagg.isNull = false;
+      }
+
+      for (int i=0; i< batchSize; ++i) {
+        myagg.sum += inputVector.getDouble(selected[i]);
+      }
+    }
+
+    private void iterateNoSelectionHasNulls(
+        Aggregation myagg,
+        TimestampColumnVector inputVector,
+        int batchSize,
+        boolean[] isNull) {
+
+      for(int i=0;i<batchSize;++i) {
+        if (!isNull[i]) {
+          if (myagg.isNull) {
+            myagg.sum = 0;
+            myagg.isNull = false;
+          }
+          myagg.sum += inputVector.getDouble(i);
+        }
+      }
+    }
+
+    private void iterateNoSelectionNoNulls(
+        Aggregation myagg,
+        TimestampColumnVector inputVector,
+        int batchSize) {
+      if (myagg.isNull) {
+        myagg.sum = 0;
+        myagg.isNull = false;
+      }
+
+      for (int i=0;i<batchSize;++i) {
+        myagg.sum += inputVector.getDouble(i);
+      }
+    }
+
+    @Override
+    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
+      return new Aggregation();
+    }
+
+    @Override
+    public void reset(AggregationBuffer agg) throws HiveException {
+      Aggregation myAgg = (Aggregation) agg;
+      myAgg.reset();
+    }
+
+    @Override
+    public Object evaluateOutput(AggregationBuffer agg) throws HiveException {
+      Aggregation myagg = (Aggregation) agg;
+      if (myagg.isNull) {
+        return null;
+      }
+      else {
+        result.set(myagg.sum);
+        return result;
+      }
+    }
+
+    @Override
+    public ObjectInspector getOutputObjectInspector() {
+      return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;
+    }
+
+  @Override
+  public long getAggregationBufferFixedSize() {
+      JavaDataModel model = JavaDataModel.get();
+      return JavaDataModel.alignUp(
+        model.object(),
+        model.memoryAlign());
+  }
+
+  @Override
+  public void init(AggregationDesc desc) throws HiveException {
+    init();
+  }
+}
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarSampTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarSampTimestamp.java
deleted file mode 100644
index c375461014..0000000000
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarSampTimestamp.java
+++ /dev/null
@@ -1,533 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.plan.AggregationDesc;
-import org.apache.hadoop.hive.ql.util.JavaDataModel;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-
-/**
-* VectorUDAFVarSampTimestamp. Vectorized implementation for VARIANCE aggregates.
-*/
-@Description(name = "var_samp",
-    value = "_FUNC_(x) - Returns the sample variance of a set of numbers (vectorized, double)")
-public class VectorUDAFVarSampTimestamp extends VectorAggregateExpression {
-
-    private static final long serialVersionUID = 1L;
-
-    /**
-    /* class for storing the current aggregate value.
-    */
-    private static final class Aggregation implements AggregationBuffer {
-
-      private static final long serialVersionUID = 1L;
-
-      transient private double sum;
-      transient private long count;
-      transient private double variance;
-
-      /**
-      * Value is explicitly (re)initialized in reset() (despite the init() bellow...)
-      */
-      transient private boolean isNull = true;
-
-      public void init() {
-        isNull = false;
-        sum = 0;
-        count = 0;
-        variance = 0;
-      }
-
-      @Override
-      public int getVariableSize() {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public void reset () {
-        isNull = true;
-        sum = 0;
-        count = 0;
-        variance = 0;
-      }
-    }
-
-    private VectorExpression inputExpression;
-
-    @Override
-    public VectorExpression inputExpression() {
-      return inputExpression;
-    }
-
-    transient private LongWritable resultCount;
-    transient private DoubleWritable resultSum;
-    transient private DoubleWritable resultVariance;
-    transient private Object[] partialResult;
-
-    transient private ObjectInspector soi;
-
-
-    public VectorUDAFVarSampTimestamp(VectorExpression inputExpression) {
-      this();
-      this.inputExpression = inputExpression;
-    }
-
-    public VectorUDAFVarSampTimestamp() {
-      super();
-      partialResult = new Object[3];
-      resultCount = new LongWritable();
-      resultSum = new DoubleWritable();
-      resultVariance = new DoubleWritable();
-      partialResult[0] = resultCount;
-      partialResult[1] = resultSum;
-      partialResult[2] = resultVariance;
-      initPartialResultInspector();
-    }
-
-  private void initPartialResultInspector() {
-        List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
-        foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
-        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
-        foi.add(PrimitiveObjectInspectorFactory.writableDoubleObjectInspector);
-
-        List<String> fname = new ArrayList<String>();
-        fname.add("count");
-        fname.add("sum");
-        fname.add("variance");
-
-        soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
-    }
-
-    private Aggregation getCurrentAggregationBuffer(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        int row) {
-      VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
-      return myagg;
-    }
-
-
-    @Override
-    public void aggregateInputSelection(
-      VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregateIndex,
-      VectorizedRowBatch batch) throws HiveException {
-
-      inputExpression.evaluate(batch);
-
-      TimestampColumnVector inputColVector = (TimestampColumnVector)batch.
-        cols[this.inputExpression.getOutputColumn()];
-
-      int batchSize = batch.size;
-
-      if (batchSize == 0) {
-        return;
-      }
-
-      if (inputColVector.isRepeating) {
-        if (inputColVector.noNulls || !inputColVector.isNull[0]) {
-          iterateRepeatingNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector.getDouble(0), batchSize);
-        }
-      }
-      else if (!batch.selectedInUse && inputColVector.noNulls) {
-        iterateNoSelectionNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize);
-      }
-      else if (!batch.selectedInUse) {
-        iterateNoSelectionHasNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize, inputColVector.isNull);
-      }
-      else if (inputColVector.noNulls){
-        iterateSelectionNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize, batch.selected);
-      }
-      else {
-        iterateSelectionHasNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector, batchSize,
-            inputColVector.isNull, batch.selected);
-      }
-
-    }
-
-    private void  iterateRepeatingNoNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        double value,
-        int batchSize) {
-
-      for (int i=0; i<batchSize; ++i) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          i);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-      }
-    }
-
-    private void iterateSelectionHasNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull,
-        int[] selected) {
-
-      for (int j=0; j< batchSize; ++j) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          j);
-        int i = selected[j];
-        if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-          if(myagg.count > 1) {
-            double t = myagg.count*value - myagg.sum;
-            myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-          }
-        }
-      }
-    }
-
-    private void iterateSelectionNoNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        int[] selected) {
-
-      for (int i=0; i< batchSize; ++i) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          i);
-        double value = inputColVector.getDouble(selected[i]);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-      }
-    }
-
-    private void iterateNoSelectionHasNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull) {
-
-      for(int i=0;i<batchSize;++i) {
-        if (!isNull[i]) {
-          Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets,
-            aggregateIndex,
-          i);
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-        }
-      }
-    }
-
-    private void iterateNoSelectionNoNullsWithAggregationSelection(
-        VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregateIndex,
-        TimestampColumnVector inputColVector,
-        int batchSize) {
-
-      for (int i=0; i<batchSize; ++i) {
-        Aggregation myagg = getCurrentAggregationBuffer(
-          aggregationBufferSets,
-          aggregateIndex,
-          i);
-        if (myagg.isNull) {
-          myagg.init ();
-        }
-        double value = inputColVector.getDouble(i);
-        myagg.sum += value;
-        myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-      }
-    }
-
-    @Override
-    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
-    throws HiveException {
-
-      inputExpression.evaluate(batch);
-
-      TimestampColumnVector inputColVector = (TimestampColumnVector)batch.
-        cols[this.inputExpression.getOutputColumn()];
-
-      int batchSize = batch.size;
-
-      if (batchSize == 0) {
-        return;
-      }
-
-      Aggregation myagg = (Aggregation)agg;
-
-      if (inputColVector.isRepeating) {
-        if (inputColVector.noNulls) {
-          iterateRepeatingNoNulls(myagg, inputColVector.getDouble(0), batchSize);
-        }
-      }
-      else if (!batch.selectedInUse && inputColVector.noNulls) {
-        iterateNoSelectionNoNulls(myagg, inputColVector, batchSize);
-      }
-      else if (!batch.selectedInUse) {
-        iterateNoSelectionHasNulls(myagg, inputColVector, batchSize, inputColVector.isNull);
-      }
-      else if (inputColVector.noNulls){
-        iterateSelectionNoNulls(myagg, inputColVector, batchSize, batch.selected);
-      }
-      else {
-        iterateSelectionHasNulls(myagg, inputColVector, batchSize, inputColVector.isNull, batch.selected);
-      }
-    }
-
-    private void  iterateRepeatingNoNulls(
-        Aggregation myagg,
-        double value,
-        int batchSize) {
-
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      // TODO: conjure a formula w/o iterating
-      //
-
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // We pulled out i=0 so we can remove the count > 1 check in the loop
-      for (int i=1; i<batchSize; ++i) {
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-    }
-
-    private void iterateSelectionHasNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull,
-        int[] selected) {
-
-      for (int j=0; j< batchSize; ++j) {
-        int i = selected[j];
-        if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-        }
-      }
-    }
-
-    private void iterateSelectionNoNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        int[] selected) {
-
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      double value = inputColVector.getDouble(selected[0]);
-      myagg.sum += value;
-      myagg.count += 1;
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // i=0 was pulled out to remove the count > 1 check in the loop
-      //
-      for (int i=1; i< batchSize; ++i) {
-        value = inputColVector.getDouble(selected[i]);
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-    }
-
-    private void iterateNoSelectionHasNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize,
-        boolean[] isNull) {
-
-      for(int i=0;i<batchSize;++i) {
-        if (!isNull[i]) {
-          double value = inputColVector.getDouble(i);
-          if (myagg.isNull) {
-            myagg.init ();
-          }
-          myagg.sum += value;
-          myagg.count += 1;
-        if(myagg.count > 1) {
-          double t = myagg.count*value - myagg.sum;
-          myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-        }
-        }
-      }
-    }
-
-    private void iterateNoSelectionNoNulls(
-        Aggregation myagg,
-        TimestampColumnVector inputColVector,
-        int batchSize) {
-
-      if (myagg.isNull) {
-        myagg.init ();
-      }
-
-      double value = inputColVector.getDouble(0);
-      myagg.sum += value;
-      myagg.count += 1;
-
-      if(myagg.count > 1) {
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-
-      // i=0 was pulled out to remove count > 1 check
-      for (int i=1; i<batchSize; ++i) {
-        value = inputColVector.getDouble(i);
-        myagg.sum += value;
-        myagg.count += 1;
-        double t = myagg.count*value - myagg.sum;
-        myagg.variance += (t*t) / ((double)myagg.count*(myagg.count-1));
-      }
-    }
-
-    @Override
-    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
-      return new Aggregation();
-    }
-
-    @Override
-    public void reset(AggregationBuffer agg) throws HiveException {
-      Aggregation myAgg = (Aggregation) agg;
-      myAgg.reset();
-    }
-
-    @Override
-    public Object evaluateOutput(
-        AggregationBuffer agg) throws HiveException {
-      Aggregation myagg = (Aggregation) agg;
-      if (myagg.isNull) {
-        return null;
-      }
-      else {
-        assert(0 < myagg.count);
-        resultCount.set (myagg.count);
-        resultSum.set (myagg.sum);
-        resultVariance.set (myagg.variance);
-        return partialResult;
-      }
-    }
-  @Override
-    public ObjectInspector getOutputObjectInspector() {
-      return soi;
-    }
-
-  @Override
-  public long getAggregationBufferFixedSize() {
-      JavaDataModel model = JavaDataModel.get();
-      return JavaDataModel.alignUp(
-        model.object() +
-        model.primitive2()*3+
-        model.primitive1(),
-        model.memoryAlign());
-  }
-
-  @Override
-  public void init(AggregationDesc desc) throws HiveException {
-    // No-op
-  }
-
-  public VectorExpression getInputExpression() {
-    return inputExpression;
-  }
-
-  public void setInputExpression(VectorExpression inputExpression) {
-    this.inputExpression = inputExpression;
-  }
-}
-
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFAdaptor.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFAdaptor.java
index 3b3624d32f..14ba64635f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFAdaptor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFAdaptor.java
@@ -19,6 +19,7 @@
 
 import java.sql.Date;
 import java.sql.Timestamp;
+import java.util.Map;
 
 import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
@@ -37,8 +38,10 @@
 import org.apache.hadoop.hive.serde2.io.HiveCharWritable;
 import org.apache.hadoop.hive.serde2.io.HiveVarcharWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.*;
 import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.io.HiveIntervalDayTimeWritable;
@@ -61,7 +64,8 @@ public class VectorUDFAdaptor extends VectorExpression {
 
   private transient GenericUDF genericUDF;
   private transient GenericUDF.DeferredObject[] deferredChildren;
-  private transient ObjectInspector outputOI;
+  private transient TypeInfo outputTypeInfo;
+  private transient VectorAssignRow outputVectorAssignRow;
   private transient ObjectInspector[] childrenOIs;
   private transient VectorExpressionWriter[] writers;
 
@@ -95,8 +99,9 @@ public void init() throws HiveException, UDFArgumentException {
     if (context != null) {
       context.setup(genericUDF);
     }
-    outputOI = VectorExpressionWriterFactory.genVectorExpressionWritable(expr)
-        .getObjectInspector();
+    outputTypeInfo = expr.getTypeInfo();
+    outputVectorAssignRow = new VectorAssignRow();
+    outputVectorAssignRow.init(outputTypeInfo, outputColumn);
 
     genericUDF.initialize(childrenOIs);
 
@@ -207,163 +212,9 @@ private void setResult(int i, VectorizedRowBatch b) {
       result = null;
     }
 
-    // set output column vector entry
-    if (result == null) {
-      b.cols[outputColumn].noNulls = false;
-      b.cols[outputColumn].isNull[i] = true;
-    } else {
-      b.cols[outputColumn].isNull[i] = false;
-      setOutputCol(b.cols[outputColumn], i, result);
-    }
-  }
-
-  private void setOutputCol(ColumnVector colVec, int i, Object value) {
-
-    /* Depending on the output type, get the value, cast the result to the
-     * correct type if needed, and assign the result into the output vector.
-     */
-    if (outputOI instanceof WritableStringObjectInspector) {
-      BytesColumnVector bv = (BytesColumnVector) colVec;
-      Text t;
-      if (value instanceof String) {
-        t = new Text((String) value);
-      } else {
-        t = ((WritableStringObjectInspector) outputOI).getPrimitiveWritableObject(value);
-      }
-      bv.setVal(i, t.getBytes(), 0, t.getLength());
-    } else if (outputOI instanceof WritableHiveCharObjectInspector) {
-      WritableHiveCharObjectInspector writableHiveCharObjectOI = (WritableHiveCharObjectInspector) outputOI;
-      int maxLength = ((CharTypeInfo) writableHiveCharObjectOI.getTypeInfo()).getLength();
-      BytesColumnVector bv = (BytesColumnVector) colVec;
-
-      HiveCharWritable hiveCharWritable;
-      if (value instanceof HiveCharWritable) {
-        hiveCharWritable = ((HiveCharWritable) value);
-      } else {
-        hiveCharWritable = writableHiveCharObjectOI.getPrimitiveWritableObject(value);
-      }
-      Text t = hiveCharWritable.getTextValue();
-
-      // In vector mode, we stored CHAR as unpadded.
-      StringExpr.rightTrimAndTruncate(bv, i, t.getBytes(), 0, t.getLength(), maxLength);
-    } else if (outputOI instanceof WritableHiveVarcharObjectInspector) {
-      WritableHiveVarcharObjectInspector writableHiveVarcharObjectOI = (WritableHiveVarcharObjectInspector) outputOI;
-      int maxLength = ((VarcharTypeInfo) writableHiveVarcharObjectOI.getTypeInfo()).getLength();
-      BytesColumnVector bv = (BytesColumnVector) colVec;
-
-      HiveVarcharWritable hiveVarcharWritable;
-      if (value instanceof HiveVarcharWritable) {
-        hiveVarcharWritable = ((HiveVarcharWritable) value);
-      } else {
-        hiveVarcharWritable = writableHiveVarcharObjectOI.getPrimitiveWritableObject(value);
-      }
-      Text t = hiveVarcharWritable.getTextValue();
-
-      StringExpr.truncate(bv, i, t.getBytes(), 0, t.getLength(), maxLength);
-    } else if (outputOI instanceof WritableIntObjectInspector) {
-      LongColumnVector lv = (LongColumnVector) colVec;
-      if (value instanceof Integer) {
-        lv.vector[i] = (Integer) value;
-      } else {
-        lv.vector[i] = ((WritableIntObjectInspector) outputOI).get(value);
-      }
-    } else if (outputOI instanceof WritableLongObjectInspector) {
-      LongColumnVector lv = (LongColumnVector) colVec;
-      if (value instanceof Long) {
-        lv.vector[i] = (Long) value;
-      } else {
-        lv.vector[i] = ((WritableLongObjectInspector) outputOI).get(value);
-      }
-    } else if (outputOI instanceof WritableDoubleObjectInspector) {
-      DoubleColumnVector dv = (DoubleColumnVector) colVec;
-      if (value instanceof Double) {
-        dv.vector[i] = (Double) value;
-      } else {
-        dv.vector[i] = ((WritableDoubleObjectInspector) outputOI).get(value);
-      }
-    } else if (outputOI instanceof WritableFloatObjectInspector) {
-      DoubleColumnVector dv = (DoubleColumnVector) colVec;
-      if (value instanceof Float) {
-        dv.vector[i] = (Float) value;
-      } else {
-        dv.vector[i] = ((WritableFloatObjectInspector) outputOI).get(value);
-      }
-    } else if (outputOI instanceof WritableShortObjectInspector) {
-      LongColumnVector lv = (LongColumnVector) colVec;
-      if (value instanceof Short) {
-        lv.vector[i] = (Short) value;
-      } else {
-        lv.vector[i] = ((WritableShortObjectInspector) outputOI).get(value);
-      }
-    } else if (outputOI instanceof WritableByteObjectInspector) {
-      LongColumnVector lv = (LongColumnVector) colVec;
-      if (value instanceof Byte) {
-        lv.vector[i] = (Byte) value;
-      } else {
-        lv.vector[i] = ((WritableByteObjectInspector) outputOI).get(value);
-      }
-    } else if (outputOI instanceof WritableTimestampObjectInspector) {
-      TimestampColumnVector tv = (TimestampColumnVector) colVec;
-      Timestamp ts;
-      if (value instanceof Timestamp) {
-        ts = (Timestamp) value;
-      } else {
-        ts = ((WritableTimestampObjectInspector) outputOI).getPrimitiveJavaObject(value);
-      }
-      tv.set(i, ts);
-    } else if (outputOI instanceof WritableDateObjectInspector) {
-      LongColumnVector lv = (LongColumnVector) colVec;
-      Date ts;
-      if (value instanceof Date) {
-        ts = (Date) value;
-      } else {
-        ts = ((WritableDateObjectInspector) outputOI).getPrimitiveJavaObject(value);
-      }
-      long l = DateWritable.dateToDays(ts);
-      lv.vector[i] = l;
-    } else if (outputOI instanceof WritableBooleanObjectInspector) {
-      LongColumnVector lv = (LongColumnVector) colVec;
-      if (value instanceof Boolean) {
-        lv.vector[i] = (Boolean) value ? 1 : 0;
-      } else {
-        lv.vector[i] = ((WritableBooleanObjectInspector) outputOI).get(value) ? 1 : 0;
-      }
-    } else if (outputOI instanceof WritableHiveDecimalObjectInspector) {
-      DecimalColumnVector dcv = (DecimalColumnVector) colVec;
-      if (value instanceof HiveDecimal) {
-        dcv.set(i, (HiveDecimal) value);
-      } else {
-        HiveDecimal hd = ((WritableHiveDecimalObjectInspector) outputOI).getPrimitiveJavaObject(value);
-        dcv.set(i, hd);
-      }
-    } else if (outputOI instanceof WritableBinaryObjectInspector) {
-      BytesWritable bw = (BytesWritable) value;
-      BytesColumnVector bv = (BytesColumnVector) colVec;
-      bv.setVal(i, bw.getBytes(), 0, bw.getLength());
-    } else if (outputOI instanceof WritableHiveIntervalYearMonthObjectInspector) {
-      LongColumnVector lv = (LongColumnVector) colVec;
-      HiveIntervalYearMonth iym;
-      if (value instanceof HiveIntervalYearMonth) {
-        iym = (HiveIntervalYearMonth) value;
-      } else {
-        iym = ((WritableHiveIntervalYearMonthObjectInspector) outputOI).getPrimitiveJavaObject(value);
-      }
-      long l = iym.getTotalMonths();
-      lv.vector[i] = l;
-    } else if (outputOI instanceof WritableHiveIntervalDayTimeObjectInspector) {
-      IntervalDayTimeColumnVector idtv = (IntervalDayTimeColumnVector) colVec;
-      HiveIntervalDayTime idt;
-      if (value instanceof HiveIntervalDayTime) {
-        idt = (HiveIntervalDayTime) value;
-      } else {
-        idt = ((WritableHiveIntervalDayTimeObjectInspector) outputOI).getPrimitiveJavaObject(value);
-      }
-      idtv.set(i, idt);
-    } else {
-      throw new RuntimeException("Unhandled object type " + outputOI.getTypeName() +
-          " inspector class " + outputOI.getClass().getName() +
-          " value class " + value.getClass().getName());
-    }
+    // Set output column vector entry.  Since we have one output column, the logical index = 0.
+    outputVectorAssignRow.assignRowColumn(
+        b, /* batchIndex */ i, /* logicalColumnIndex */ 0, result);
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
index 72bdc71dbd..933e47d416 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
@@ -263,6 +263,8 @@ public class Vectorizer implements PhysicalPlanResolver {
   private boolean useVectorDeserialize;
   private boolean useRowDeserialize;
   private boolean isReduceVectorizationEnabled;
+  private boolean isVectorizationComplexTypesEnabled;
+  private boolean isVectorizationGroupByComplexTypesEnabled;
 
   private boolean isSchemaEvolution;
 
@@ -1379,6 +1381,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
         try {
           ret = validateMapWorkOperator(op, mapWork, isTezOrSpark);
         } catch (Exception e) {
+          String oneLineStackTrace = VectorizationContext.getStackTraceAsSingleLine(e);
+          LOG.info(oneLineStackTrace);
           throw new SemanticException(e);
         }
         if (!ret) {
@@ -1699,6 +1703,13 @@ public PhysicalContext resolve(PhysicalContext physicalContext) throws SemanticE
         HiveConf.getBoolVar(hiveConf,
             HiveConf.ConfVars.HIVE_VECTORIZATION_REDUCE_ENABLED);
 
+    isVectorizationComplexTypesEnabled =
+        HiveConf.getBoolVar(hiveConf,
+            HiveConf.ConfVars.HIVE_VECTORIZATION_COMPLEX_TYPES_ENABLED);
+    isVectorizationGroupByComplexTypesEnabled =
+        HiveConf.getBoolVar(hiveConf,
+            HiveConf.ConfVars.HIVE_VECTORIZATION_GROUPBY_COMPLEX_TYPES_ENABLED);
+
     isSchemaEvolution =
         HiveConf.getBoolVar(hiveConf,
             HiveConf.ConfVars.HIVE_SCHEMA_EVOLUTION);
@@ -1872,7 +1883,8 @@ private boolean validateMapJoinOperator(MapJoinOperator op) {
   private boolean validateMapJoinDesc(MapJoinDesc desc) {
     byte posBigTable = (byte) desc.getPosBigTable();
     List<ExprNodeDesc> filterExprs = desc.getFilters().get(posBigTable);
-    if (!validateExprNodeDesc(filterExprs, "Filter", VectorExpressionDescriptor.Mode.FILTER)) {
+    if (!validateExprNodeDesc(
+        filterExprs, "Filter", VectorExpressionDescriptor.Mode.FILTER, /* allowComplex */ true)) {
       return false;
     }
     List<ExprNodeDesc> keyExprs = desc.getKeys().get(posBigTable);
@@ -1903,7 +1915,8 @@ private boolean validateSparkHashTableSinkOperator(SparkHashTableSinkOperator op
     List<ExprNodeDesc> filterExprs = desc.getFilters().get(tag);
     List<ExprNodeDesc> keyExprs = desc.getKeys().get(tag);
     List<ExprNodeDesc> valueExprs = desc.getExprs().get(tag);
-    return validateExprNodeDesc(filterExprs, "Filter", VectorExpressionDescriptor.Mode.FILTER) &&
+    return validateExprNodeDesc(
+        filterExprs, "Filter", VectorExpressionDescriptor.Mode.FILTER, /* allowComplex */ true) &&
         validateExprNodeDesc(keyExprs, "Key") && validateExprNodeDesc(valueExprs, "Value");
   }
 
@@ -1928,7 +1941,8 @@ private boolean validateSelectOperator(SelectOperator op) {
 
   private boolean validateFilterOperator(FilterOperator op) {
     ExprNodeDesc desc = op.getConf().getPredicate();
-    return validateExprNodeDesc(desc, "Predicate", VectorExpressionDescriptor.Mode.FILTER);
+    return validateExprNodeDesc(
+        desc, "Predicate", VectorExpressionDescriptor.Mode.FILTER, /* allowComplex */ true);
   }
 
   private boolean validateGroupByOperator(GroupByOperator op, boolean isReduce, boolean isTezOrSpark) {
@@ -1938,7 +1952,7 @@ private boolean validateGroupByOperator(GroupByOperator op, boolean isReduce, bo
       setOperatorIssue("DISTINCT not supported");
       return false;
     }
-    boolean ret = validateExprNodeDesc(desc.getKeys(), "Key");
+    boolean ret = validateExprNodeDescNoComplex(desc.getKeys(), "Key");
     if (!ret) {
       return false;
     }
@@ -2045,12 +2059,12 @@ private boolean validateGroupByOperator(GroupByOperator op, boolean isReduce, bo
         VectorGroupByDesc.groupByDescModeToVectorProcessingMode(desc.getMode(), hasKeys);
     if (desc.isGroupingSetsPresent() &&
         (processingMode != ProcessingMode.HASH && processingMode != ProcessingMode.STREAMING)) {
-      LOG.info("Vectorized GROUPING SETS only expected for HASH and STREAMING processing modes");
+      setOperatorIssue("Vectorized GROUPING SETS only expected for HASH and STREAMING processing modes");
       return false;
     }
 
     Pair<Boolean,Boolean> retPair =
-        validateAggregationDescs(desc.getAggregators(), processingMode, hasKeys);
+        validateAggregationDescs(desc.getAggregators(), desc.getMode(), hasKeys);
     if (!retPair.left) {
       return false;
     }
@@ -2064,6 +2078,9 @@ private boolean validateGroupByOperator(GroupByOperator op, boolean isReduce, bo
 
     vectorDesc.setProcessingMode(processingMode);
 
+    vectorDesc.setIsVectorizationComplexTypesEnabled(isVectorizationComplexTypesEnabled);
+    vectorDesc.setIsVectorizationGroupByComplexTypesEnabled(isVectorizationGroupByComplexTypesEnabled);
+
     LOG.info("Vector GROUP BY operator will use processing mode " + processingMode.name() +
         ", isVectorOutput " + vectorDesc.isVectorOutput());
 
@@ -2075,14 +2092,21 @@ private boolean validateFileSinkOperator(FileSinkOperator op) {
   }
 
   private boolean validateExprNodeDesc(List<ExprNodeDesc> descs, String expressionTitle) {
-    return validateExprNodeDesc(descs, expressionTitle, VectorExpressionDescriptor.Mode.PROJECTION);
+    return validateExprNodeDesc(
+        descs, expressionTitle, VectorExpressionDescriptor.Mode.PROJECTION, /* allowComplex */ true);
+  }
+
+  private boolean validateExprNodeDescNoComplex(List<ExprNodeDesc> descs, String expressionTitle) {
+    return validateExprNodeDesc(
+        descs, expressionTitle, VectorExpressionDescriptor.Mode.PROJECTION, /* allowComplex */ false);
   }
 
   private boolean validateExprNodeDesc(List<ExprNodeDesc> descs,
           String expressionTitle,
-          VectorExpressionDescriptor.Mode mode) {
+          VectorExpressionDescriptor.Mode mode,
+          boolean allowComplex) {
     for (ExprNodeDesc d : descs) {
-      boolean ret = validateExprNodeDesc(d, expressionTitle, mode);
+      boolean ret = validateExprNodeDesc(d, expressionTitle, mode, allowComplex);
       if (!ret) {
         return false;
       }
@@ -2091,10 +2115,10 @@ private boolean validateExprNodeDesc(List<ExprNodeDesc> descs,
   }
 
   private Pair<Boolean,Boolean> validateAggregationDescs(List<AggregationDesc> descs,
-      ProcessingMode processingMode, boolean hasKeys) {
+      GroupByDesc.Mode groupByMode, boolean hasKeys) {
     boolean outputIsPrimitive = true;
     for (AggregationDesc d : descs) {
-      Pair<Boolean,Boolean>  retPair = validateAggregationDesc(d, processingMode, hasKeys);
+      Pair<Boolean,Boolean>  retPair = validateAggregationDesc(d, groupByMode, hasKeys);
       if (!retPair.left) {
         return retPair;
       }
@@ -2106,7 +2130,7 @@ private Pair<Boolean,Boolean> validateAggregationDescs(List<AggregationDesc> des
   }
 
   private boolean validateExprNodeDescRecursive(ExprNodeDesc desc, String expressionTitle,
-      VectorExpressionDescriptor.Mode mode) {
+      VectorExpressionDescriptor.Mode mode, boolean allowComplex) {
     if (desc instanceof ExprNodeColumnDesc) {
       ExprNodeColumnDesc c = (ExprNodeColumnDesc) desc;
       // Currently, we do not support vectorized virtual columns (see HIVE-5570).
@@ -2116,9 +2140,11 @@ private boolean validateExprNodeDescRecursive(ExprNodeDesc desc, String expressi
       }
     }
     String typeName = desc.getTypeInfo().getTypeName();
-    boolean ret = validateDataType(typeName, mode);
+    boolean ret = validateDataType(typeName, mode, allowComplex && isVectorizationComplexTypesEnabled);
     if (!ret) {
-      setExpressionIssue(expressionTitle, "Data type " + typeName + " of " + desc.toString() + " not supported");
+      setExpressionIssue(expressionTitle,
+          getValidateDataTypeErrorMsg(
+              typeName, mode, allowComplex, isVectorizationComplexTypesEnabled));
       return false;
     }
     boolean isInExpression = false;
@@ -2144,7 +2170,8 @@ private boolean validateExprNodeDescRecursive(ExprNodeDesc desc, String expressi
         for (ExprNodeDesc d : desc.getChildren()) {
           // Don't restrict child expressions for projection.
           // Always use loose FILTER mode.
-          if (!validateExprNodeDescRecursive(d, expressionTitle, VectorExpressionDescriptor.Mode.FILTER)) {
+          if (!validateExprNodeDescRecursive(
+              d, expressionTitle, VectorExpressionDescriptor.Mode.FILTER, /* allowComplex */ true)) {
             return false;
           }
         }
@@ -2195,12 +2222,13 @@ private boolean validateStructInExpression(ExprNodeDesc desc,
   }
 
   private boolean validateExprNodeDesc(ExprNodeDesc desc, String expressionTitle) {
-    return validateExprNodeDesc(desc, expressionTitle, VectorExpressionDescriptor.Mode.PROJECTION);
+    return validateExprNodeDesc(
+        desc, expressionTitle, VectorExpressionDescriptor.Mode.PROJECTION, /* allowComplex */ true);
   }
 
   boolean validateExprNodeDesc(ExprNodeDesc desc, String expressionTitle,
-      VectorExpressionDescriptor.Mode mode) {
-    if (!validateExprNodeDescRecursive(desc, expressionTitle, mode)) {
+      VectorExpressionDescriptor.Mode mode, boolean allowComplex) {
+    if (!validateExprNodeDescRecursive(desc, expressionTitle, mode, allowComplex)) {
       return false;
     }
     try {
@@ -2239,12 +2267,12 @@ private boolean validateGenericUdf(ExprNodeGenericFuncDesc genericUDFExpr) {
     return true;
   }
 
-  public static ObjectInspector.Category aggregationOutputCategory(VectorAggregateExpression vectorAggrExpr) {
+  public static Category aggregationOutputCategory(VectorAggregateExpression vectorAggrExpr) {
     ObjectInspector outputObjInspector = vectorAggrExpr.getOutputObjectInspector();
     return outputObjInspector.getCategory();
   }
 
-  private Pair<Boolean,Boolean> validateAggregationDesc(AggregationDesc aggDesc, ProcessingMode processingMode,
+  private Pair<Boolean,Boolean> validateAggregationDesc(AggregationDesc aggDesc, GroupByDesc.Mode groupByMode,
       boolean hasKeys) {
 
     String udfName = aggDesc.getGenericUDAFName().toLowerCase();
@@ -2253,12 +2281,16 @@ private Pair<Boolean,Boolean> validateAggregationDesc(AggregationDesc aggDesc, P
       return new Pair<Boolean,Boolean>(false, false);
     }
     /*
+    // The planner seems to pull this one out.
     if (aggDesc.getDistinct()) {
       setExpressionIssue("Aggregation Function", "DISTINCT not supported");
       return new Pair<Boolean,Boolean>(false, false);
     }
     */
-    if (aggDesc.getParameters() != null && !validateExprNodeDesc(aggDesc.getParameters(), "Aggregation Function UDF " + udfName + " parameter")) {
+
+    ArrayList<ExprNodeDesc> parameters = aggDesc.getParameters();
+
+    if (parameters != null && !validateExprNodeDesc(parameters, "Aggregation Function UDF " + udfName + " parameter")) {
       return new Pair<Boolean,Boolean>(false, false);
     }
 
@@ -2280,27 +2312,90 @@ private Pair<Boolean,Boolean> validateAggregationDesc(AggregationDesc aggDesc, P
           " vector expression " + vectorAggrExpr.toString());
     }
 
-    ObjectInspector.Category outputCategory = aggregationOutputCategory(vectorAggrExpr);
-    boolean outputIsPrimitive = (outputCategory == ObjectInspector.Category.PRIMITIVE);
-    if (processingMode == ProcessingMode.MERGE_PARTIAL &&
-        hasKeys &&
-        !outputIsPrimitive) {
-      setOperatorIssue("Vectorized Reduce MergePartial GROUP BY keys can only handle aggregate outputs that are primitive types");
-      return new Pair<Boolean,Boolean>(false, false);
+    boolean canVectorizeComplexType =
+        (isVectorizationComplexTypesEnabled && isVectorizationGroupByComplexTypesEnabled);
+
+    boolean isVectorOutput;
+    if (canVectorizeComplexType) {
+      isVectorOutput = true;
+    } else {
+
+      // Do complex input type checking...
+      boolean inputIsPrimitive;
+      if (parameters == null || parameters.size() == 0) {
+        inputIsPrimitive = true;   // Pretend for COUNT(*)
+      } else {
+
+        // Multi-input should have been eliminated earlier.
+        // Preconditions.checkState(parameters.size() == 1);
+
+        final Category inputCategory = parameters.get(0).getTypeInfo().getCategory();
+        inputIsPrimitive = (inputCategory == Category.PRIMITIVE);
+      }
+
+      if (!inputIsPrimitive) {
+        setOperatorIssue("Cannot vectorize GROUP BY with aggregation complex type inputs in " +
+            aggDesc.getExprString() + " since " +
+            GroupByDesc.getComplexTypeWithGroupByEnabledCondition(
+                isVectorizationComplexTypesEnabled, isVectorizationGroupByComplexTypesEnabled));
+        return new Pair<Boolean,Boolean>(false, false);
+      }
+
+      // Now, look a the output.  If the output is complex, we switch to row-mode for all child
+      // operators...
+      isVectorOutput = (aggregationOutputCategory(vectorAggrExpr) == Category.PRIMITIVE);
     }
 
-    return new Pair<Boolean,Boolean>(true, outputIsPrimitive);
+    return new Pair<Boolean,Boolean>(true, isVectorOutput);
   }
 
-  public static boolean validateDataType(String type, VectorExpressionDescriptor.Mode mode) {
+  public static boolean validateDataType(String type, VectorExpressionDescriptor.Mode mode,
+      boolean allowComplex) {
+
     type = type.toLowerCase();
     boolean result = supportedDataTypesPattern.matcher(type).matches();
     if (result && mode == VectorExpressionDescriptor.Mode.PROJECTION && type.equals("void")) {
       return false;
     }
+
+    if (!result) {
+      TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(type);
+      if (typeInfo.getCategory() != Category.PRIMITIVE) {
+        if (allowComplex) {
+          return true;
+        }
+      }
+    }
     return result;
   }
 
+  public static String getValidateDataTypeErrorMsg(String type, VectorExpressionDescriptor.Mode mode,
+      boolean allowComplex, boolean isVectorizationComplexTypesEnabled) {
+
+    type = type.toLowerCase();
+    boolean result = supportedDataTypesPattern.matcher(type).matches();
+    if (result && mode == VectorExpressionDescriptor.Mode.PROJECTION && type.equals("void")) {
+      return "Vectorizing data type void not supported when mode = PROJECTION";
+    }
+
+    if (!result) {
+      TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(type);
+      if (typeInfo.getCategory() != Category.PRIMITIVE) {
+        if (allowComplex && isVectorizationComplexTypesEnabled) {
+          return null;
+        } else if (!allowComplex) {
+          return "Vectorizing complex type " + typeInfo.getCategory() + " not supported";
+        } else {
+          return "Vectorizing complex type " + typeInfo.getCategory() + " not enabled (" +
+              type +  ") since " +
+              GroupByDesc.getComplexTypeEnabledCondition(
+                  isVectorizationComplexTypesEnabled);
+        }
+      }
+    }
+    return (result ? null : "Vectorizing data type " + type + " not supported");
+  }
+
   private VectorizationContext getVectorizationContext(String contextName,
       VectorTaskColumnInfo vectorTaskColumnInfo) {
 
@@ -3482,7 +3577,7 @@ public Operator<? extends OperatorDesc> vectorizeOperator(Operator<? extends Ope
             }
             VectorAggregateExpression[] vecAggregators = vectorGroupByDesc.getAggregators();
             for (VectorAggregateExpression vecAggr : vecAggregators) {
-              if (usesVectorUDFAdaptor(vecAggr.inputExpression())) {
+              if (usesVectorUDFAdaptor(vecAggr.getInputExpression())) {
                 vectorTaskColumnInfo.setUsesVectorUDFAdaptor(true);
               }
             }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
index fe91ee7025..45d100d9eb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
@@ -22,6 +22,7 @@
 import java.util.Arrays;
 import java.util.List;
 
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
 import org.apache.hadoop.hive.ql.udf.UDFType;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
@@ -305,6 +306,21 @@ public void setDistinct(boolean isDistinct) {
     this.isDistinct = isDistinct;
   }
 
+  @Override
+  public Object clone() {
+    ArrayList<java.lang.String> outputColumnNames = new ArrayList<>();
+    outputColumnNames.addAll(this.outputColumnNames);
+    ArrayList<ExprNodeDesc> keys = new ArrayList<>();
+    keys.addAll(this.keys);
+    ArrayList<org.apache.hadoop.hive.ql.plan.AggregationDesc> aggregators = new ArrayList<>();
+    aggregators.addAll(this.aggregators);
+    List<Integer> listGroupingSets = new ArrayList<>();
+    listGroupingSets.addAll(this.listGroupingSets);
+    return new GroupByDesc(this.mode, outputColumnNames, keys, aggregators,
+        this.groupByMemoryUsage, this.memoryThreshold, listGroupingSets, this.groupingSetsPresent,
+        this.groupingSetPosition, this.isDistinct);
+  }
+
   public class GroupByOperatorExplainVectorization extends OperatorExplainVectorization {
 
     private final GroupByDesc groupByDesc;
@@ -337,20 +353,42 @@ public boolean getGroupByRowOutputCascade() {
       return vectorGroupByDesc.isVectorOutput();
     }
 
+    @Explain(vectorization = Vectorization.OPERATOR, displayName = "vectorProcessingMode", explainLevels = { Level.DEFAULT, Level.EXTENDED })
+    public String getProcessingMode() {
+      return vectorGroupByDesc.getProcessingMode().name();
+    }
+
+    @Explain(vectorization = Vectorization.OPERATOR, displayName = "groupByMode", explainLevels = { Level.DEFAULT, Level.EXTENDED })
+    public String getGroupByMode() {
+      return groupByDesc.getMode().name();
+    }
+
     @Explain(vectorization = Vectorization.OPERATOR, displayName = "vectorOutputConditionsNotMet", explainLevels = { Level.DEFAULT, Level.EXTENDED })
     public List<String> getVectorOutputConditionsNotMet() {
       List<String> results = new ArrayList<String>();
+
+      boolean isVectorizationComplexTypesEnabled = vectorGroupByDesc.getIsVectorizationComplexTypesEnabled();
+      boolean isVectorizationGroupByComplexTypesEnabled = vectorGroupByDesc.getIsVectorizationGroupByComplexTypesEnabled();
+
+      if (isVectorizationComplexTypesEnabled && isVectorizationGroupByComplexTypesEnabled) {
+        return null;
+      }
+
       VectorAggregateExpression[] vecAggregators = vectorGroupByDesc.getAggregators();
       for (VectorAggregateExpression vecAggr : vecAggregators) {
         Category category = Vectorizer.aggregationOutputCategory(vecAggr);
         if (category != ObjectInspector.Category.PRIMITIVE) {
           results.add(
-              "Vector output of " + vecAggr.toString() + " output type " + category + " requires PRIMITIVE IS false");
+              "Vector output of " + vecAggr.toString() + " output type " + category + " requires PRIMITIVE type IS false");
         }
       }
       if (results.size() == 0) {
         return null;
       }
+
+      results.add(
+          getComplexTypeWithGroupByEnabledCondition(
+              isVectorizationComplexTypesEnabled, isVectorizationGroupByComplexTypesEnabled));
       return results;
     }
 
@@ -368,18 +406,21 @@ public GroupByOperatorExplainVectorization getGroupByVectorization() {
     return new GroupByOperatorExplainVectorization(this, vectorDesc);
   }
 
-  @Override
-  public Object clone() {
-    ArrayList<java.lang.String> outputColumnNames = new ArrayList<>();
-    outputColumnNames.addAll(this.outputColumnNames);
-    ArrayList<ExprNodeDesc> keys = new ArrayList<>();
-    keys.addAll(this.keys);
-    ArrayList<org.apache.hadoop.hive.ql.plan.AggregationDesc> aggregators = new ArrayList<>();
-    aggregators.addAll(this.aggregators);
-    List<Integer> listGroupingSets = new ArrayList<>();
-    listGroupingSets.addAll(this.listGroupingSets);
-    return new GroupByDesc(this.mode, outputColumnNames, keys, aggregators,
-        this.groupByMemoryUsage, this.memoryThreshold, listGroupingSets, this.groupingSetsPresent,
-        this.groupingSetPosition, this.isDistinct);
+  public static String getComplexTypeEnabledCondition(
+      boolean isVectorizationComplexTypesEnabled) {
+    return
+        HiveConf.ConfVars.HIVE_VECTORIZATION_COMPLEX_TYPES_ENABLED.varname +
+        " IS " + isVectorizationComplexTypesEnabled;
+  }
+
+  public static String getComplexTypeWithGroupByEnabledCondition(
+      boolean isVectorizationComplexTypesEnabled,
+      boolean isVectorizationGroupByComplexTypesEnabled) {
+    final boolean enabled = (isVectorizationComplexTypesEnabled && isVectorizationGroupByComplexTypesEnabled);
+    return "(" +
+        HiveConf.ConfVars.HIVE_VECTORIZATION_COMPLEX_TYPES_ENABLED.varname + " " + isVectorizationComplexTypesEnabled +
+        " AND " +
+        HiveConf.ConfVars.HIVE_VECTORIZATION_GROUPBY_COMPLEX_TYPES_ENABLED.varname + " " + isVectorizationGroupByComplexTypesEnabled +
+        ") IS " + enabled;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
index 2120400d8b..8b99ae07ac 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
@@ -299,27 +299,6 @@ public void deriveLlap(Configuration conf, boolean isExecDriver) {
         isLlapOn, canWrapAny, hasPathToPartInfo, hasLlap, hasNonLlap, hasAcid);
   }
 
-  private boolean checkVectorizerSupportedTypes(boolean hasLlap) {
-    for (Map.Entry<String, Operator<? extends OperatorDesc>> entry : aliasToWork.entrySet()) {
-      final String alias = entry.getKey();
-      Operator<? extends OperatorDesc> op = entry.getValue();
-      PartitionDesc partitionDesc = aliasToPartnInfo.get(alias);
-      if (op instanceof TableScanOperator && partitionDesc != null &&
-          partitionDesc.getTableDesc() != null) {
-        final TableScanOperator tsOp = (TableScanOperator) op;
-        final List<String> readColumnNames = tsOp.getNeededColumns();
-        final Properties props = partitionDesc.getTableDesc().getProperties();
-        final List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString(
-            props.getProperty(serdeConstants.LIST_COLUMN_TYPES));
-        final List<String> allColumnTypes = TypeInfoUtils.getTypeStringsFromTypeInfo(typeInfos);
-        final List<String> allColumnNames = Utilities.getColumnNames(props);
-        hasLlap = Utilities.checkVectorizerSupportedTypes(readColumnNames, allColumnNames,
-            allColumnTypes);
-      }
-    }
-    return hasLlap;
-  }
-
   private static String deriveLlapIoDescString(boolean isLlapOn, boolean canWrapAny,
       boolean hasPathToPartInfo, boolean hasLlap, boolean hasNonLlap, boolean hasAcid) {
     if (!isLlapOn) return null; // LLAP IO is off, don't output.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java
index f9a87257a4..89d868d103 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java
@@ -65,6 +65,8 @@ public static enum ProcessingMode {
   private VectorExpression[] keyExpressions;
   private VectorAggregateExpression[] aggregators;
   private int[] projectedOutputColumns;
+  private boolean isVectorizationComplexTypesEnabled;
+  private boolean isVectorizationGroupByComplexTypesEnabled;
 
   public VectorGroupByDesc() {
     this.processingMode = ProcessingMode.NONE;
@@ -110,6 +112,22 @@ public int[] getProjectedOutputColumns() {
     return projectedOutputColumns;
   }
 
+  public void setIsVectorizationComplexTypesEnabled(boolean isVectorizationComplexTypesEnabled) {
+    this.isVectorizationComplexTypesEnabled = isVectorizationComplexTypesEnabled;
+  }
+
+  public boolean getIsVectorizationComplexTypesEnabled() {
+    return isVectorizationComplexTypesEnabled;
+  }
+
+  public void setIsVectorizationGroupByComplexTypesEnabled(boolean isVectorizationGroupByComplexTypesEnabled) {
+    this.isVectorizationGroupByComplexTypesEnabled = isVectorizationGroupByComplexTypesEnabled;
+  }
+
+  public boolean getIsVectorizationGroupByComplexTypesEnabled() {
+    return isVectorizationGroupByComplexTypesEnabled;
+  }
+
   /**
    * Which ProcessingMode for VectorGroupByOperator?
    *
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
index a28f7e8d56..2ea426c964 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
@@ -259,16 +259,18 @@ protected ObjectInspector getSumFieldWritableObjectInspector() {
     }
 
     private DecimalTypeInfo deriveResultDecimalTypeInfo() {
-      int prec = inputOI.precision();
-      int scale = inputOI.scale();
+      return deriveResultDecimalTypeInfo(inputOI.precision(), inputOI.scale(), mode);
+    }
+
+    public static DecimalTypeInfo deriveResultDecimalTypeInfo(int precision, int scale, Mode mode) {
       if (mode == Mode.FINAL || mode == Mode.COMPLETE) {
-        int intPart = prec - scale;
+        int intPart = precision - scale;
         // The avg() result type has the same number of integer digits and 4 more decimal digits.
         scale = Math.min(scale + 4, HiveDecimal.MAX_SCALE - intPart);
         return TypeInfoFactory.getDecimalTypeInfo(intPart + scale, scale);
       } else {
         // For intermediate sum field
-        return GenericUDAFAverage.deriveSumFieldTypeInfo(prec, scale);
+        return GenericUDAFAverage.deriveSumFieldTypeInfo(precision, scale);
       }
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
index 6d3b92bcd3..a041ffc93d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
@@ -211,15 +211,15 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
       super.init(m, parameters);
       result = new HiveDecimalWritable(0);
       inputOI = (PrimitiveObjectInspector) parameters[0];
-      // The output precision is 10 greater than the input which should cover at least
-      // 10b rows. The scale is the same as the input.
-      DecimalTypeInfo outputTypeInfo = null;
+
+      final DecimalTypeInfo outputTypeInfo;
       if (mode == Mode.PARTIAL1 || mode == Mode.COMPLETE) {
-        int precision = Math.min(HiveDecimal.MAX_PRECISION, inputOI.precision() + 10);
-        outputTypeInfo = TypeInfoFactory.getDecimalTypeInfo(precision, inputOI.scale());
+        outputTypeInfo = getOutputDecimalTypeInfoForSum(inputOI.precision(), inputOI.scale(), mode);
       } else {
+        // No change.
         outputTypeInfo = (DecimalTypeInfo) inputOI.getTypeInfo();
       }
+
       ObjectInspector oi = PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(outputTypeInfo);
       outputOI = (PrimitiveObjectInspector) ObjectInspectorUtils.getStandardObjectInspector(
           oi, ObjectInspectorCopyOption.JAVA);
@@ -227,6 +227,21 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
       return oi;
     }
 
+    public static DecimalTypeInfo getOutputDecimalTypeInfoForSum(final int inputPrecision,
+        int inputScale, Mode mode) {
+
+      // The output precision is 10 greater than the input which should cover at least
+      // 10b rows. The scale is the same as the input.
+      DecimalTypeInfo outputTypeInfo = null;
+      if (mode == Mode.PARTIAL1 || mode == Mode.COMPLETE) {
+        int precision = Math.min(HiveDecimal.MAX_PRECISION, inputPrecision + 10);
+        outputTypeInfo = TypeInfoFactory.getDecimalTypeInfo(precision, inputScale);
+      } else {
+        outputTypeInfo = TypeInfoFactory.getDecimalTypeInfo(inputPrecision, inputScale);
+      }
+      return outputTypeInfo;
+    }
+
     /** class for storing decimal sum value. */
     @AggregationType(estimable = false) // hard to know exactly for decimals
     static class SumHiveDecimalWritableAgg extends SumAgg<HiveDecimalWritable> {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorGroupByOperator.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorGroupByOperator.java
index 0bc690f1a8..1432bfb3e0 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorGroupByOperator.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorGroupByOperator.java
@@ -2157,7 +2157,7 @@ public void validate(String key, Object expected, Object result) {
       } else {
         assertEquals (true, arr[0] instanceof Object[]);
         Object[] vals = (Object[]) arr[0];
-        assertEquals (2, vals.length);
+        assertEquals (3, vals.length);
 
         assertEquals (true, vals[0] instanceof LongWritable);
         LongWritable lw = (LongWritable) vals[0];
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/physical/TestVectorizer.java b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/physical/TestVectorizer.java
index ede60b8651..a3a8aa57e7 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/physical/TestVectorizer.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/physical/TestVectorizer.java
@@ -156,8 +156,8 @@ public void testValidateNestedExpressions() {
 
     Vectorizer v = new Vectorizer();
     v.testSetCurrentBaseWork(new MapWork());
-    Assert.assertFalse(v.validateExprNodeDesc(andExprDesc, "test", VectorExpressionDescriptor.Mode.FILTER));
-    Assert.assertFalse(v.validateExprNodeDesc(andExprDesc, "test", VectorExpressionDescriptor.Mode.PROJECTION));
+    Assert.assertFalse(v.validateExprNodeDesc(andExprDesc, "test", VectorExpressionDescriptor.Mode.FILTER, false));
+    Assert.assertFalse(v.validateExprNodeDesc(andExprDesc, "test", VectorExpressionDescriptor.Mode.PROJECTION, false));
   }
 
   /**
@@ -230,8 +230,8 @@ public void testValidateSMBJoinOperator() {
   public void testExprNodeDynamicValue() {
     ExprNodeDesc exprNode = new ExprNodeDynamicValueDesc(new DynamicValue("id1", TypeInfoFactory.stringTypeInfo));
     Vectorizer v = new Vectorizer();
-    Assert.assertTrue(v.validateExprNodeDesc(exprNode, "Test", Mode.FILTER));
-    Assert.assertTrue(v.validateExprNodeDesc(exprNode, "Test", Mode.PROJECTION));
+    Assert.assertTrue(v.validateExprNodeDesc(exprNode, "Test", Mode.FILTER, false));
+    Assert.assertTrue(v.validateExprNodeDesc(exprNode, "Test", Mode.PROJECTION, false));
   }
 
   @Test
@@ -254,7 +254,7 @@ public void testExprNodeBetweenWithDynamicValue() {
 
     Vectorizer v = new Vectorizer();
     v.testSetCurrentBaseWork(new MapWork());
-    boolean valid = v.validateExprNodeDesc(betweenExpr, "Test", Mode.FILTER);
+    boolean valid = v.validateExprNodeDesc(betweenExpr, "Test", Mode.FILTER, false);
     Assert.assertTrue(valid);
   }
 }
diff --git a/ql/src/test/queries/clientpositive/schema_evol_text_vec_part_all_complex.q b/ql/src/test/queries/clientpositive/schema_evol_text_vec_part_all_complex.q
index 131a1af262..02f7c751e7 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_text_vec_part_all_complex.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_text_vec_part_all_complex.q
@@ -12,6 +12,9 @@ set hive.metastore.disallow.incompatible.col.type.changes=false;
 set hive.default.fileformat=textfile;
 set hive.llap.io.enabled=false;
 
+-- TEMPORARY UNTIL Vectorized Text Schema Evolution works.
+set hive.vectorized.complex.types.enabled=false;
+
 -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: TEXTFILE, Non-Vectorized, MapWork, Partitioned --> all complex conversions
diff --git a/ql/src/test/queries/clientpositive/schema_evol_text_vecrow_part_all_complex.q b/ql/src/test/queries/clientpositive/schema_evol_text_vecrow_part_all_complex.q
index b4a9d664e2..d780074bc7 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_text_vecrow_part_all_complex.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_text_vecrow_part_all_complex.q
@@ -12,6 +12,9 @@ set hive.metastore.disallow.incompatible.col.type.changes=false;
 set hive.default.fileformat=textfile;
 set hive.llap.io.enabled=false;
 
+-- TEMPORARY UNTIL Vectorized Text Schema Evolution works.
+set hive.vectorized.complex.types.enabled=false;
+
 -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: TEXTFILE, Non-Vectorized, MapWork, Partitioned --> all complex conversions
diff --git a/ql/src/test/queries/clientpositive/vector_aggregate_9.q b/ql/src/test/queries/clientpositive/vector_aggregate_9.q
index 04fdeec7e8..d7322ec60d 100644
--- a/ql/src/test/queries/clientpositive/vector_aggregate_9.q
+++ b/ql/src/test/queries/clientpositive/vector_aggregate_9.q
@@ -39,9 +39,19 @@ STORED AS ORC;
 
 INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;
 
-explain vectorization expression
+-- SORT_QUERY_RESULTS
+
+explain vectorization detail
 select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc;
 
--- SORT_QUERY_RESULTS
+select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc;
+
+explain vectorization detail
+select min(d), max(d), sum(d), avg(d) from vectortab2korc;
+
+select min(d), max(d), sum(d), avg(d) from vectortab2korc;
+
+explain vectorization detail
+select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc;
 
-select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc;
\ No newline at end of file
+select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/vector_complex_all.q b/ql/src/test/queries/clientpositive/vector_complex_all.q
index b71ac62f74..920210f751 100644
--- a/ql/src/test/queries/clientpositive/vector_complex_all.q
+++ b/ql/src/test/queries/clientpositive/vector_complex_all.q
@@ -6,6 +6,7 @@ set hive.fetch.task.conversion=none;
 SET hive.vectorized.execution.enabled=true;
 set hive.llap.io.enabled=false;
 set hive.mapred.mode=nonstrict;
+set hive.auto.convert.join=true;
 
 CREATE TABLE orc_create_staging (
   str STRING,
@@ -32,35 +33,75 @@ SELECT orc_create_staging.*, '0' FROM orc_create_staging;
 
 set hive.llap.io.enabled=true;
 
+EXPLAIN VECTORIZATION DETAIL
 SELECT * FROM orc_create_complex;
 
+SELECT * FROM orc_create_complex;
+
+EXPLAIN VECTORIZATION DETAIL
+SELECT str FROM orc_create_complex;
+
 SELECT str FROM orc_create_complex;
 
+EXPLAIN VECTORIZATION DETAIL
 SELECT strct, mp, lst FROM orc_create_complex;
 
+SELECT strct, mp, lst FROM orc_create_complex;
+
+EXPLAIN VECTORIZATION DETAIL
+SELECT lst, str FROM orc_create_complex;
+
 SELECT lst, str FROM orc_create_complex;
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT mp, str FROM orc_create_complex;
+
 SELECT mp, str FROM orc_create_complex;
 
+EXPLAIN VECTORIZATION DETAIL
 SELECT strct, str FROM orc_create_complex;
 
+SELECT strct, str FROM orc_create_complex;
+
+EXPLAIN VECTORIZATION DETAIL
+SELECT strct.B, str FROM orc_create_complex;
+
 SELECT strct.B, str FROM orc_create_complex;
 
 set hive.llap.io.enabled=false;
 
+EXPLAIN VECTORIZATION DETAIL
 INSERT INTO TABLE orc_create_complex
 SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2;
 
+INSERT INTO TABLE orc_create_complex
+SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2;
+
+EXPLAIN VECTORIZATION DETAIL
+select count(*) from orc_create_complex;
+
 select count(*) from orc_create_complex;
 
 set hive.llap.io.enabled=true;
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT distinct lst, strct FROM orc_create_complex;
+
 SELECT distinct lst, strct FROM orc_create_complex;
 
+EXPLAIN VECTORIZATION DETAIL
 SELECT str, count(val)  FROM orc_create_complex GROUP BY str;
 
+SELECT str, count(val)  FROM orc_create_complex GROUP BY str;
+
+EXPLAIN VECTORIZATION DETAIL
+SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B;
+
 SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B;
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str;
+
 SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str;
 
 
diff --git a/ql/src/test/queries/clientpositive/vector_groupby_reduce.q b/ql/src/test/queries/clientpositive/vector_groupby_reduce.q
index f23b26feb2..a030c62751 100644
--- a/ql/src/test/queries/clientpositive/vector_groupby_reduce.q
+++ b/ql/src/test/queries/clientpositive/vector_groupby_reduce.q
@@ -16,20 +16,20 @@ create table store_sales_txt
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float                  
+    ss_wholesale_cost         double,
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
-row format delimited fields terminated by '|' 
+row format delimited fields terminated by '|'
 stored as textfile;
 
 LOAD DATA LOCAL INPATH '../../data/files/store_sales.txt' OVERWRITE INTO TABLE store_sales_txt;
@@ -47,18 +47,19 @@ create table store_sales
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float
+    ss_wholesale_cost         double,
+    ss_wholesale_cost_decimal     decimal(38,18),
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
 stored as orc
 tblproperties ("orc.stripe.size"="33554432", "orc.compress.size"="16384");
@@ -79,6 +80,7 @@ ss_sold_date_sk           ,
     ss_ticket_number      ,
     ss_quantity           ,
     ss_wholesale_cost     ,
+    cast(ss_wholesale_cost as decimal(38,18)),
     ss_list_price         ,
     ss_sales_price        ,
     ss_ext_discount_amt   ,
@@ -138,23 +140,25 @@ order by m;
 
 explain vectorization expression
 select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number;
 
 select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number;
@@ -162,10 +166,10 @@ order by ss_ticket_number;
 
 explain vectorization expression
 select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
@@ -173,13 +177,12 @@ group by ss_ticket_number, ss_item_sk
 order by ss_ticket_number, ss_item_sk;
 
 select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(wc), avg(wc), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_wholesale_cost) wc, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number, ss_item_sk
-order by ss_ticket_number, ss_item_sk;
-
+order by ss_ticket_number, ss_item_sk;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/vector_tablesample_rows.q b/ql/src/test/queries/clientpositive/vector_tablesample_rows.q
index 94b2f5be43..bb3c5a43f6 100644
--- a/ql/src/test/queries/clientpositive/vector_tablesample_rows.q
+++ b/ql/src/test/queries/clientpositive/vector_tablesample_rows.q
@@ -4,7 +4,7 @@ SET hive.vectorized.execution.enabled=true;
 set hive.fetch.task.conversion=none;
 set hive.mapred.mode=nonstrict;
 
-explain vectorization expression
+explain vectorization detail
 select 'key1', 'value1' from alltypesorc tablesample (1 rows);
 
 select 'key1', 'value1' from alltypesorc tablesample (1 rows);
@@ -12,7 +12,7 @@ select 'key1', 'value1' from alltypesorc tablesample (1 rows);
 
 create table decimal_2 (t decimal(18,9)) stored as orc;
 
-explain vectorization expression
+explain vectorization detail
 insert overwrite table decimal_2
   select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows);
 
@@ -25,12 +25,12 @@ drop table decimal_2;
 
 
 -- Dummy tables HIVE-13190
-explain vectorization expression
+explain vectorization detail
 select count(1) from (select * from (Select 1 a) x order by x.a) y;
 
 select count(1) from (select * from (Select 1 a) x order by x.a) y;
 
-explain vectorization expression
+explain vectorization detail
 create temporary table dual as select 1;
 
 create temporary table dual as select 1;
diff --git a/ql/src/test/queries/clientpositive/vector_udf1.q b/ql/src/test/queries/clientpositive/vector_udf1.q
index 6ebe58feb3..48d3e1ee4d 100644
--- a/ql/src/test/queries/clientpositive/vector_udf1.q
+++ b/ql/src/test/queries/clientpositive/vector_udf1.q
@@ -9,20 +9,20 @@ insert overwrite table varchar_udf_1
   select key, value, key, value, '2015-01-14', '2015-01-14', '2017-01-11', '2017-01-11' from src where key = '238' limit 1;
 
 -- UDFs with varchar support
-explain
-select 
+explain vectorization detail
+select
   concat(c1, c2),
   concat(c3, c4),
   concat(c1, c2) = concat(c3, c4)
 from varchar_udf_1 limit 1;
 
-select 
+select
   concat(c1, c2),
   concat(c3, c4),
   concat(c1, c2) = concat(c3, c4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   upper(c2),
   upper(c4),
@@ -35,7 +35,7 @@ select
   upper(c2) = upper(c4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   lower(c2),
   lower(c4),
@@ -49,7 +49,7 @@ select
 from varchar_udf_1 limit 1;
 
 -- Scalar UDFs
-explain
+explain vectorization detail
 select
   ascii(c2),
   ascii(c4),
@@ -62,20 +62,20 @@ select
   ascii(c2) = ascii(c4)
 from varchar_udf_1 limit 1;
 
-explain
-select 
+explain vectorization detail
+select
   concat_ws('|', c1, c2),
   concat_ws('|', c3, c4),
   concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
 from varchar_udf_1 limit 1;
 
-select 
+select
   concat_ws('|', c1, c2),
   concat_ws('|', c3, c4),
   concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   decode(encode(c2, 'US-ASCII'), 'US-ASCII'),
   decode(encode(c4, 'US-ASCII'), 'US-ASCII'),
@@ -88,7 +88,7 @@ select
   decode(encode(c2, 'US-ASCII'), 'US-ASCII') = decode(encode(c4, 'US-ASCII'), 'US-ASCII')
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   instr(c2, '_'),
   instr(c4, '_'),
@@ -101,7 +101,7 @@ select
   instr(c2, '_') = instr(c4, '_')
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   replace(c1, '_', c2),
   replace(c3, '_', c4),
@@ -114,7 +114,7 @@ select
   replace(c1, '_', c2) = replace(c3, '_', c4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   reverse(c2),
   reverse(c4),
@@ -127,7 +127,7 @@ select
   reverse(c2) = reverse(c4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   next_day(d1, 'TU'),
   next_day(d4, 'WE'),
@@ -140,7 +140,7 @@ select
   next_day(d1, 'TU') = next_day(d4, 'WE')
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   months_between(d1, d3),
   months_between(d2, d4),
@@ -153,7 +153,7 @@ select
   months_between(d1, d3) = months_between(d2, d4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   length(c2),
   length(c4),
@@ -166,7 +166,7 @@ select
   length(c2) = length(c4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   locate('a', 'abcdabcd', 3),
   locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3),
@@ -179,7 +179,7 @@ select
   locate('a', 'abcdabcd', 3) = locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   lpad(c2, 15, ' '),
   lpad(c4, 15, ' '),
@@ -192,7 +192,7 @@ select
   lpad(c2, 15, ' ') = lpad(c4, 15, ' ')
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   ltrim(c2),
   ltrim(c4),
@@ -205,7 +205,7 @@ select
   ltrim(c2) = ltrim(c4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   c2 regexp 'val',
   c4 regexp 'val',
@@ -218,7 +218,7 @@ select
   (c2 regexp 'val') = (c4 regexp 'val')
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   regexp_extract(c2, 'val_([0-9]+)', 1),
   regexp_extract(c4, 'val_([0-9]+)', 1),
@@ -231,7 +231,7 @@ select
   regexp_extract(c2, 'val_([0-9]+)', 1) = regexp_extract(c4, 'val_([0-9]+)', 1)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   regexp_replace(c2, 'val', 'replaced'),
   regexp_replace(c4, 'val', 'replaced'),
@@ -244,7 +244,7 @@ select
   regexp_replace(c2, 'val', 'replaced') = regexp_replace(c4, 'val', 'replaced')
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   reverse(c2),
   reverse(c4),
@@ -257,7 +257,7 @@ select
   reverse(c2) = reverse(c4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   rpad(c2, 15, ' '),
   rpad(c4, 15, ' '),
@@ -270,7 +270,7 @@ select
   rpad(c2, 15, ' ') = rpad(c4, 15, ' ')
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   rtrim(c2),
   rtrim(c4),
@@ -283,7 +283,7 @@ select
   rtrim(c2) = rtrim(c4)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   sentences('See spot run.  See jane run.'),
   sentences(cast('See spot run.  See jane run.' as varchar(50)))
@@ -294,7 +294,7 @@ select
   sentences(cast('See spot run.  See jane run.' as varchar(50)))
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   split(c2, '_'),
   split(c4, '_')
@@ -305,18 +305,18 @@ select
   split(c4, '_')
 from varchar_udf_1 limit 1;
 
-explain
-select 
+explain vectorization detail
+select
   str_to_map('a:1,b:2,c:3',',',':'),
   str_to_map(cast('a:1,b:2,c:3' as varchar(20)),',',':')
 from varchar_udf_1 limit 1;
 
-select 
+select
   str_to_map('a:1,b:2,c:3',',',':'),
   str_to_map(cast('a:1,b:2,c:3' as varchar(20)),',',':')
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   substr(c2, 1, 3),
   substr(c4, 1, 3),
@@ -329,7 +329,7 @@ select
   substr(c2, 1, 3) = substr(c4, 1, 3)
 from varchar_udf_1 limit 1;
 
-explain
+explain vectorization detail
 select
   trim(c2),
   trim(c4),
@@ -344,7 +344,7 @@ from varchar_udf_1 limit 1;
 
 
 -- Aggregate Functions
-explain
+explain vectorization detail
 select
   compute_stats(c2, 16),
   compute_stats(c4, 16)
@@ -355,7 +355,7 @@ select
   compute_stats(c4, 16)
 from varchar_udf_1;
 
-explain
+explain vectorization detail
 select
   min(c2),
   min(c4)
@@ -366,7 +366,7 @@ select
   min(c4)
 from varchar_udf_1;
 
-explain
+explain vectorization detail
 select
   max(c2),
   max(c4)
diff --git a/ql/src/test/queries/clientpositive/vectorization_0.q b/ql/src/test/queries/clientpositive/vectorization_0.q
index c97cd9fc75..3af9422717 100644
--- a/ql/src/test/queries/clientpositive/vectorization_0.q
+++ b/ql/src/test/queries/clientpositive/vectorization_0.q
@@ -1,12 +1,13 @@
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 SET hive.vectorized.execution.enabled=true;
+set hive.vectorized.execution.reduce.enabled=true;
 set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
 -- Use ORDER BY clauses to generate 2 stages.
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ctinyint) as c1,
        MAX(ctinyint),
        COUNT(ctinyint),
@@ -21,7 +22,7 @@ SELECT MIN(ctinyint) as c1,
 FROM   alltypesorc
 ORDER BY c1;
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(ctinyint) as c1
 FROM   alltypesorc
 ORDER BY c1;
@@ -55,7 +56,7 @@ SELECT
 FROM alltypesorc
 ORDER BY c1;
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cbigint) as c1,
        MAX(cbigint),
        COUNT(cbigint),
@@ -70,7 +71,7 @@ SELECT MIN(cbigint) as c1,
 FROM   alltypesorc
 ORDER BY c1;
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cbigint) as c1
 FROM   alltypesorc
 ORDER BY c1;
@@ -104,7 +105,7 @@ SELECT
 FROM alltypesorc
 ORDER BY c1;
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cfloat) as c1,
        MAX(cfloat),
        COUNT(cfloat),
@@ -119,7 +120,7 @@ SELECT MIN(cfloat) as c1,
 FROM   alltypesorc
 ORDER BY c1;
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cfloat) as c1
 FROM   alltypesorc
 ORDER BY c1;
@@ -153,7 +154,7 @@ SELECT
 FROM alltypesorc
 ORDER BY c1;
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT AVG(cbigint),
        (-(AVG(cbigint))),
        (-6432 + AVG(cbigint)),
diff --git a/ql/src/test/queries/clientpositive/vectorization_1.q b/ql/src/test/queries/clientpositive/vectorization_1.q
index f71218f2a6..b03ae71a11 100644
--- a/ql/src/test/queries/clientpositive/vectorization_1.q
+++ b/ql/src/test/queries/clientpositive/vectorization_1.q
@@ -1,8 +1,30 @@
 SET hive.vectorized.execution.enabled=true;
 set hive.fetch.task.conversion=none;
+set hive.vectorized.execution.reduce.enabled=true;
 
 -- SORT_QUERY_RESULTS
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT VAR_POP(ctinyint),
+       (VAR_POP(ctinyint) / -26.28),
+       SUM(cfloat),
+       (-1.389 + SUM(cfloat)),
+       (SUM(cfloat) * (-1.389 + SUM(cfloat))),
+       MAX(ctinyint),
+       (-((SUM(cfloat) * (-1.389 + SUM(cfloat))))),
+       MAX(cint),
+       (MAX(cint) * 79.553),
+       VAR_SAMP(cdouble),
+       (10.175 % (-((SUM(cfloat) * (-1.389 + SUM(cfloat)))))),
+       COUNT(cint),
+       (-563 % MAX(cint))
+FROM   alltypesorc
+WHERE  (((cdouble > ctinyint)
+         AND (cboolean2 > 0))
+        OR ((cbigint < ctinyint)
+            OR ((cint > cbigint)
+                OR (cboolean1 < 0))));
+
 SELECT VAR_POP(ctinyint),
        (VAR_POP(ctinyint) / -26.28),
        SUM(cfloat),
diff --git a/ql/src/test/queries/clientpositive/vectorization_10.q b/ql/src/test/queries/clientpositive/vectorization_10.q
index c5f4d439d2..8b620686e7 100644
--- a/ql/src/test/queries/clientpositive/vectorization_10.q
+++ b/ql/src/test/queries/clientpositive/vectorization_10.q
@@ -3,6 +3,30 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT cdouble,
+       ctimestamp1,
+       ctinyint,
+       cboolean1,
+       cstring1,
+       (-(cdouble)),
+       (cdouble + csmallint),
+       ((cdouble + csmallint) % 33),
+       (-(cdouble)),
+       (ctinyint % cdouble),
+       (ctinyint % csmallint),
+       (-(cdouble)),
+       (cbigint * (ctinyint % csmallint)),
+       (9763215.5639 - (cdouble + csmallint)),
+       (-((-(cdouble))))
+FROM   alltypesorc
+WHERE  (((cstring2 <= '10')
+         OR ((ctinyint > cdouble)
+             AND (-5638.15 >= ctinyint)))
+        OR ((cdouble > 6981)
+            AND ((csmallint = 9763215.5639)
+                 OR (cstring1 LIKE '%a'))));
+
 SELECT cdouble,
        ctimestamp1,
        ctinyint,
diff --git a/ql/src/test/queries/clientpositive/vectorization_11.q b/ql/src/test/queries/clientpositive/vectorization_11.q
index 3830ea9902..aa05fe5e51 100644
--- a/ql/src/test/queries/clientpositive/vectorization_11.q
+++ b/ql/src/test/queries/clientpositive/vectorization_11.q
@@ -3,6 +3,21 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT cstring1,
+       cboolean1,
+       cdouble,
+       ctimestamp1,
+       (-3728 * csmallint),
+       (cdouble - 9763215.5639),
+       (-(cdouble)),
+       ((-(cdouble)) + 6981),
+       (cdouble * -5638.15)
+FROM   alltypesorc
+WHERE  ((cstring2 = cstring1)
+        OR ((ctimestamp1 IS NULL)
+            AND (cstring1 LIKE '%a')));
+
 SELECT cstring1,
        cboolean1,
        cdouble,
diff --git a/ql/src/test/queries/clientpositive/vectorization_12.q b/ql/src/test/queries/clientpositive/vectorization_12.q
index 0728ba944e..18d9184ae1 100644
--- a/ql/src/test/queries/clientpositive/vectorization_12.q
+++ b/ql/src/test/queries/clientpositive/vectorization_12.q
@@ -4,6 +4,38 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT   cbigint,
+         cboolean1,
+         cstring1,
+         ctimestamp1,
+         cdouble,
+         (-6432 * cdouble),
+         (-(cbigint)),
+         COUNT(cbigint),
+         (cbigint * COUNT(cbigint)),
+         STDDEV_SAMP(cbigint),
+         ((-6432 * cdouble) / -6432),
+         (-(((-6432 * cdouble) / -6432))),
+         AVG(cdouble),
+         (-((-6432 * cdouble))),
+         (-5638.15 + cbigint),
+         SUM(cbigint),
+         (AVG(cdouble) / (-6432 * cdouble)),
+         AVG(cdouble),
+         (-((-(((-6432 * cdouble) / -6432))))),
+         (((-6432 * cdouble) / -6432) + (-((-6432 * cdouble)))),
+         STDDEV_POP(cdouble)
+FROM     alltypesorc
+WHERE    (((ctimestamp1 IS NULL)
+           AND ((cboolean1 >= cboolean2)
+                OR (ctinyint != csmallint)))
+          AND ((cstring1 LIKE '%a')
+              OR ((cboolean2 <= 1)
+                  AND (cbigint >= csmallint))))
+GROUP BY cbigint, cboolean1, cstring1, ctimestamp1, cdouble
+ORDER BY ctimestamp1, cdouble, cbigint, cstring1;
+
 SELECT   cbigint,
          cboolean1,
          cstring1,
diff --git a/ql/src/test/queries/clientpositive/vectorization_13.q b/ql/src/test/queries/clientpositive/vectorization_13.q
index 84ae9942b7..7250a0c068 100644
--- a/ql/src/test/queries/clientpositive/vectorization_13.q
+++ b/ql/src/test/queries/clientpositive/vectorization_13.q
@@ -5,7 +5,7 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT   cboolean1,
          ctinyint,
          ctimestamp1,
diff --git a/ql/src/test/queries/clientpositive/vectorization_14.q b/ql/src/test/queries/clientpositive/vectorization_14.q
index 825fd6375e..2547500d91 100644
--- a/ql/src/test/queries/clientpositive/vectorization_14.q
+++ b/ql/src/test/queries/clientpositive/vectorization_14.q
@@ -5,7 +5,7 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
-EXPLAIN VECTORIZATION 
+EXPLAIN VECTORIZATION DETAIL
 SELECT   ctimestamp1,
          cfloat,
          cstring1,
diff --git a/ql/src/test/queries/clientpositive/vectorization_15.q b/ql/src/test/queries/clientpositive/vectorization_15.q
index 5c48c58d3e..bb33ffd802 100644
--- a/ql/src/test/queries/clientpositive/vectorization_15.q
+++ b/ql/src/test/queries/clientpositive/vectorization_15.q
@@ -3,9 +3,12 @@ set hive.explain.user=false;
 SET hive.vectorized.execution.enabled=true;
 set hive.fetch.task.conversion=none;
 
+-- Until HIVE-16756: 'Vectorization: LongColModuloLongColumn throws "java.lang.ArithmeticException: / by zero"' is resolved, do not vectorize Reducers
+set hive.vectorized.execution.reduce.enabled=false;
+
 -- SORT_QUERY_RESULTS
 
-EXPLAIN VECTORIZATION 
+EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cboolean1,
          cdouble,
diff --git a/ql/src/test/queries/clientpositive/vectorization_16.q b/ql/src/test/queries/clientpositive/vectorization_16.q
index 822c824b66..e9cb5c3c7f 100644
--- a/ql/src/test/queries/clientpositive/vectorization_16.q
+++ b/ql/src/test/queries/clientpositive/vectorization_16.q
@@ -5,7 +5,7 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
-EXPLAIN VECTORIZATION 
+EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
diff --git a/ql/src/test/queries/clientpositive/vectorization_17.q b/ql/src/test/queries/clientpositive/vectorization_17.q
index 57cdc41ecc..48062325c3 100644
--- a/ql/src/test/queries/clientpositive/vectorization_17.q
+++ b/ql/src/test/queries/clientpositive/vectorization_17.q
@@ -5,7 +5,7 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
-EXPLAIN VECTORIZATION 
+EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cstring1,
          cint,
diff --git a/ql/src/test/queries/clientpositive/vectorization_2.q b/ql/src/test/queries/clientpositive/vectorization_2.q
index 4941d1e4ef..5a1269c556 100644
--- a/ql/src/test/queries/clientpositive/vectorization_2.q
+++ b/ql/src/test/queries/clientpositive/vectorization_2.q
@@ -3,6 +3,29 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(csmallint),
+       (AVG(csmallint) % -563),
+       (AVG(csmallint) + 762),
+       SUM(cfloat),
+       VAR_POP(cbigint),
+       (-(VAR_POP(cbigint))),
+       (SUM(cfloat) - AVG(csmallint)),
+       COUNT(*),
+       (-((SUM(cfloat) - AVG(csmallint)))),
+       (VAR_POP(cbigint) - 762),
+       MIN(ctinyint),
+       ((-(VAR_POP(cbigint))) + MIN(ctinyint)),
+       AVG(cdouble),
+       (((-(VAR_POP(cbigint))) + MIN(ctinyint)) - SUM(cfloat))
+FROM   alltypesorc
+WHERE  (((ctimestamp1 < ctimestamp2)
+         AND ((cstring2 LIKE 'b%')
+              AND (cfloat <= -5638.15)))
+        OR ((cdouble < ctinyint)
+            AND ((-10669 != ctimestamp2)
+                 OR (359 > cint))));
+
 SELECT AVG(csmallint),
        (AVG(csmallint) % -563),
        (AVG(csmallint) + 762),
diff --git a/ql/src/test/queries/clientpositive/vectorization_3.q b/ql/src/test/queries/clientpositive/vectorization_3.q
index 2e0350ade7..dea7936ae1 100644
--- a/ql/src/test/queries/clientpositive/vectorization_3.q
+++ b/ql/src/test/queries/clientpositive/vectorization_3.q
@@ -4,6 +4,31 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT STDDEV_SAMP(csmallint),
+       (STDDEV_SAMP(csmallint) - 10.175),
+       STDDEV_POP(ctinyint),
+       (STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)),
+       (-(STDDEV_POP(ctinyint))),
+       (STDDEV_SAMP(csmallint) % 79.553),
+       (-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))),
+       STDDEV_SAMP(cfloat),
+       (-(STDDEV_SAMP(csmallint))),
+       SUM(cfloat),
+       ((-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))) / (STDDEV_SAMP(csmallint) - 10.175)),
+       (-((STDDEV_SAMP(csmallint) - 10.175))),
+       AVG(cint),
+       (-3728 - STDDEV_SAMP(csmallint)),
+       STDDEV_POP(cint),
+       (AVG(cint) / STDDEV_SAMP(cfloat))
+FROM   alltypesorc
+WHERE  (((cint <= cfloat)
+         AND ((79.553 != cbigint)
+              AND (ctimestamp2 = -29071)))
+        OR ((cbigint > cdouble)
+            AND ((79.553 <= csmallint)
+                 AND (ctimestamp1 > ctimestamp2))));
+
 SELECT STDDEV_SAMP(csmallint),
        (STDDEV_SAMP(csmallint) - 10.175),
        STDDEV_POP(ctinyint),
diff --git a/ql/src/test/queries/clientpositive/vectorization_4.q b/ql/src/test/queries/clientpositive/vectorization_4.q
index ba603c8add..e7c88e6fa3 100644
--- a/ql/src/test/queries/clientpositive/vectorization_4.q
+++ b/ql/src/test/queries/clientpositive/vectorization_4.q
@@ -3,6 +3,29 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT SUM(cint),
+       (SUM(cint) * -563),
+       (-3728 + SUM(cint)),
+       STDDEV_POP(cdouble),
+       (-(STDDEV_POP(cdouble))),
+       AVG(cdouble),
+       ((SUM(cint) * -563) % SUM(cint)),
+       (((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)),
+       VAR_POP(cdouble),
+       (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))),
+       ((-3728 + SUM(cint)) - (SUM(cint) * -563)),
+       MIN(ctinyint),
+       MIN(ctinyint),
+       (MIN(ctinyint) * (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))))
+FROM   alltypesorc
+WHERE  (((csmallint >= cint)
+         OR ((-89010 >= ctinyint)
+             AND (cdouble > 79.553)))
+        OR ((-563 != cbigint)
+            AND ((ctinyint != cbigint)
+                 OR (-3728 >= cdouble))));
+
 SELECT SUM(cint),
        (SUM(cint) * -563),
        (-3728 + SUM(cint)),
diff --git a/ql/src/test/queries/clientpositive/vectorization_5.q b/ql/src/test/queries/clientpositive/vectorization_5.q
index e2d4d0a65c..c3b335a26b 100644
--- a/ql/src/test/queries/clientpositive/vectorization_5.q
+++ b/ql/src/test/queries/clientpositive/vectorization_5.q
@@ -3,6 +3,26 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT MAX(csmallint),
+       (MAX(csmallint) * -75),
+       COUNT(*),
+       ((MAX(csmallint) * -75) / COUNT(*)),
+       (6981 * MAX(csmallint)),
+       MIN(csmallint),
+       (-(MIN(csmallint))),
+       (197 % ((MAX(csmallint) * -75) / COUNT(*))),
+       SUM(cint),
+       MAX(ctinyint),
+       (-(MAX(ctinyint))),
+       ((-(MAX(ctinyint))) + MAX(ctinyint))
+FROM   alltypesorc
+WHERE  (((cboolean2 IS NOT NULL)
+         AND (cstring1 LIKE '%b%'))
+        OR ((ctinyint = cdouble)
+            AND ((ctimestamp2 IS NOT NULL)
+                 AND (cstring2 LIKE 'a'))));
+
 SELECT MAX(csmallint),
        (MAX(csmallint) * -75),
        COUNT(*),
diff --git a/ql/src/test/queries/clientpositive/vectorization_6.q b/ql/src/test/queries/clientpositive/vectorization_6.q
index f55a2fbd45..f38b5136c9 100644
--- a/ql/src/test/queries/clientpositive/vectorization_6.q
+++ b/ql/src/test/queries/clientpositive/vectorization_6.q
@@ -3,6 +3,27 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
+EXPLAIN VECTORIZATION DETAIL
+SELECT cboolean1,
+       cfloat,
+       cstring1,
+       (988888 * csmallint),
+       (-(csmallint)),
+       (-(cfloat)),
+       (-26.28 / cfloat),
+       (cfloat * 359),
+       (cint % ctinyint),
+       (-(cdouble)),
+       (ctinyint - -75),
+       (762 * (cint % ctinyint))
+FROM   alltypesorc
+WHERE  ((ctinyint != 0)
+        AND ((((cboolean1 <= 0)
+          AND (cboolean2 >= cboolean1))
+          OR ((cbigint IS NOT NULL)
+              AND ((cstring2 LIKE '%a')
+                   OR (cfloat <= -257))))));
+
 SELECT cboolean1,
        cfloat,
        cstring1,
diff --git a/ql/src/test/queries/clientpositive/vectorization_7.q b/ql/src/test/queries/clientpositive/vectorization_7.q
index bf3a1c246c..a85585b5d0 100644
--- a/ql/src/test/queries/clientpositive/vectorization_7.q
+++ b/ql/src/test/queries/clientpositive/vectorization_7.q
@@ -5,7 +5,7 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT cboolean1,
        cbigint,
        csmallint,
diff --git a/ql/src/test/queries/clientpositive/vectorization_8.q b/ql/src/test/queries/clientpositive/vectorization_8.q
index d43db26441..3aa0eb5492 100644
--- a/ql/src/test/queries/clientpositive/vectorization_8.q
+++ b/ql/src/test/queries/clientpositive/vectorization_8.q
@@ -5,7 +5,7 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT ctimestamp1,
        cdouble,
        cboolean1,
diff --git a/ql/src/test/queries/clientpositive/vectorization_9.q b/ql/src/test/queries/clientpositive/vectorization_9.q
index 822c824b66..e9cb5c3c7f 100644
--- a/ql/src/test/queries/clientpositive/vectorization_9.q
+++ b/ql/src/test/queries/clientpositive/vectorization_9.q
@@ -5,7 +5,7 @@ set hive.fetch.task.conversion=none;
 
 -- SORT_QUERY_RESULTS
 
-EXPLAIN VECTORIZATION 
+EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
diff --git a/ql/src/test/queries/clientpositive/vectorized_distinct_gby.q b/ql/src/test/queries/clientpositive/vectorized_distinct_gby.q
index 4339a5f0d9..44a06f57fc 100644
--- a/ql/src/test/queries/clientpositive/vectorized_distinct_gby.q
+++ b/ql/src/test/queries/clientpositive/vectorized_distinct_gby.q
@@ -1,6 +1,7 @@
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 SET hive.vectorized.execution.enabled=true;
+SET hive.vectorized.execution.reduce.enabled=true;
 set hive.fetch.task.conversion=none;
 
 SET hive.map.groupby.sorted=true;
@@ -8,8 +9,10 @@ SET hive.map.groupby.sorted=true;
 create table dtest(a int, b int) clustered by (a) sorted by (a) into 1 buckets stored as orc;
 insert into table dtest select c,b from (select array(300,300,300,300,300) as a, 1 as b from src order by a limit 1) y lateral view  explode(a) t1 as c;
 
-explain vectorization select sum(distinct a), count(distinct a) from dtest;
+explain vectorization detail
+select sum(distinct a), count(distinct a) from dtest;
 select sum(distinct a), count(distinct a) from dtest;
 
-explain vectorization select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc;
+explain vectorization detail
+select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc;
 select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc;
diff --git a/ql/src/test/queries/clientpositive/vectorized_timestamp.q b/ql/src/test/queries/clientpositive/vectorized_timestamp.q
index c110597289..8de4e868a2 100644
--- a/ql/src/test/queries/clientpositive/vectorized_timestamp.q
+++ b/ql/src/test/queries/clientpositive/vectorized_timestamp.q
@@ -1,38 +1,41 @@
 set hive.fetch.task.conversion=none;
-set hive.explain.user=true;
+set hive.explain.user=false;
+set hive.vectorized.execution.reduce.enabled=true;
 
 DROP TABLE IF EXISTS test;
 CREATE TABLE test(ts TIMESTAMP) STORED AS ORC;
 INSERT INTO TABLE test VALUES ('0001-01-01 00:00:00.000000000'), ('9999-12-31 23:59:59.999999999');
 
 SET hive.vectorized.execution.enabled = false;
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test;
 
 SELECT ts FROM test;
 
-EXPLAIN VECTORIZATION EXPRESSION
 SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test;
 
-SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test;
-
-EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000');
-
 SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000');
 
 SET hive.vectorized.execution.enabled = true;
-EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test;
 
 SELECT ts FROM test;
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test;
 
 SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test;
 
-EXPLAIN VECTORIZATION EXPRESSION
+EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000');
 
 SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000');
+
+EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test;
+
+SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test;
+
+EXPLAIN VECTORIZATION DETAIL
+SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test;
+
+SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test;
diff --git a/ql/src/test/results/clientpositive/llap/llap_vector_nohybridgrace.q.out b/ql/src/test/results/clientpositive/llap/llap_vector_nohybridgrace.q.out
index d26400d280..0e9e1207cb 100644
--- a/ql/src/test/results/clientpositive/llap/llap_vector_nohybridgrace.q.out
+++ b/ql/src/test/results/clientpositive/llap/llap_vector_nohybridgrace.q.out
@@ -72,8 +72,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFCountStar(*) -> bigint
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0]
                           mode: hash
                           outputColumnNames: _col0
@@ -153,8 +155,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -250,8 +254,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFCountStar(*) -> bigint
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0]
                           mode: hash
                           outputColumnNames: _col0
@@ -331,8 +337,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/llap/schema_evol_orc_vec_part_all_complex.q.out b/ql/src/test/results/clientpositive/llap/schema_evol_orc_vec_part_all_complex.q.out
index d0cafaa2db..f57481dcc1 100644
--- a/ql/src/test/results/clientpositive/llap/schema_evol_orc_vec_part_all_complex.q.out
+++ b/ql/src/test/results/clientpositive/llap/schema_evol_orc_vec_part_all_complex.q.out
@@ -156,24 +156,42 @@ STAGE PLANS:
                 TableScan
                   alias: part_change_various_various_struct1
                   Statistics: Num rows: 6 Data size: 4734 Basic stats: COMPLETE Column stats: PARTIAL
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3]
                   Select Operator
                     expressions: insert_num (type: int), part (type: int), s1 (type: struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string>), b (type: string)
                     outputColumnNames: _col0, _col1, _col2, _col3
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 3, 1, 2]
                     Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: PARTIAL
                     File Output Operator
                       compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
                       Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: PARTIAL
                       table:
                           input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                           output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string> of Column[s1] not supported
-                vectorized: false
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 3
+                    includeColumns: [0, 1, 2]
+                    dataColumns: insert_num:int, s1:struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string>, b:string
+                    partitionColumnCount: 1
+                    partitionColumns: part:int
 
   Stage: Stage-0
     Fetch Operator
@@ -438,24 +456,42 @@ STAGE PLANS:
                 TableScan
                   alias: part_add_various_various_struct2
                   Statistics: Num rows: 8 Data size: 4912 Basic stats: COMPLETE Column stats: PARTIAL
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3]
                   Select Operator
                     expressions: insert_num (type: int), part (type: int), b (type: string), s2 (type: struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string>)
                     outputColumnNames: _col0, _col1, _col2, _col3
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 3, 1, 2]
                     Statistics: Num rows: 8 Data size: 32 Basic stats: COMPLETE Column stats: PARTIAL
                     File Output Operator
                       compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
                       Statistics: Num rows: 8 Data size: 32 Basic stats: COMPLETE Column stats: PARTIAL
                       table:
                           input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                           output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string> of Column[s2] not supported
-                vectorized: false
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 3
+                    includeColumns: [0, 1, 2]
+                    dataColumns: insert_num:int, b:string, s2:struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string>
+                    partitionColumnCount: 1
+                    partitionColumns: part:int
 
   Stage: Stage-0
     Fetch Operator
@@ -648,24 +684,42 @@ STAGE PLANS:
                 TableScan
                   alias: part_add_to_various_various_struct4
                   Statistics: Num rows: 4 Data size: 1172 Basic stats: COMPLETE Column stats: PARTIAL
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3]
                   Select Operator
                     expressions: insert_num (type: int), part (type: int), b (type: string), s3 (type: struct<c1:boolean,c2:tinyint,c3:smallint,c4:int,c5:bigint,c6:float,c7:double,c8:decimal(38,18),c9:char(25),c10:varchar(25),c11:timestamp,c12:date,c13:binary>)
                     outputColumnNames: _col0, _col1, _col2, _col3
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 3, 1, 2]
                     Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: PARTIAL
                     File Output Operator
                       compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
                       Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: PARTIAL
                       table:
                           input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                           output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type struct<c1:boolean,c2:tinyint,c3:smallint,c4:int,c5:bigint,c6:float,c7:double,c8:decimal(38,18),c9:char(25),c10:varchar(25),c11:timestamp,c12:date,c13:binary> of Column[s3] not supported
-                vectorized: false
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 3
+                    includeColumns: [0, 1, 2]
+                    dataColumns: insert_num:int, b:string, s3:struct<c1:boolean,c2:tinyint,c3:smallint,c4:int,c5:bigint,c6:float,c7:double,c8:decimal(38,18),c9:char(25),c10:varchar(25),c11:timestamp,c12:date,c13:binary>
+                    partitionColumnCount: 1
+                    partitionColumns: part:int
 
   Stage: Stage-0
     Fetch Operator
diff --git a/ql/src/test/results/clientpositive/llap/schema_evol_text_vec_part_all_complex.q.out b/ql/src/test/results/clientpositive/llap/schema_evol_text_vec_part_all_complex.q.out
index 97270fcbee..0ea29727ce 100644
--- a/ql/src/test/results/clientpositive/llap/schema_evol_text_vec_part_all_complex.q.out
+++ b/ql/src/test/results/clientpositive/llap/schema_evol_text_vec_part_all_complex.q.out
@@ -172,7 +172,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                 inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string> of Column[s1] not supported
+                notVectorizedReason: Select expression for SELECT operator: Vectorizing complex type STRUCT not enabled (struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string>) since hive.vectorized.complex.types.enabled IS false
                 vectorized: false
 
   Stage: Stage-0
@@ -454,7 +454,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                 inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string> of Column[s2] not supported
+                notVectorizedReason: Select expression for SELECT operator: Vectorizing complex type STRUCT not enabled (struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string>) since hive.vectorized.complex.types.enabled IS false
                 vectorized: false
 
   Stage: Stage-0
@@ -664,7 +664,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                 inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type struct<c1:boolean,c2:tinyint,c3:smallint,c4:int,c5:bigint,c6:float,c7:double,c8:decimal(38,18),c9:char(25),c10:varchar(25),c11:timestamp,c12:date,c13:binary> of Column[s3] not supported
+                notVectorizedReason: Select expression for SELECT operator: Vectorizing complex type STRUCT not enabled (struct<c1:boolean,c2:tinyint,c3:smallint,c4:int,c5:bigint,c6:float,c7:double,c8:decimal(38,18),c9:char(25),c10:varchar(25),c11:timestamp,c12:date,c13:binary>) since hive.vectorized.complex.types.enabled IS false
                 vectorized: false
 
   Stage: Stage-0
diff --git a/ql/src/test/results/clientpositive/llap/schema_evol_text_vecrow_part_all_complex.q.out b/ql/src/test/results/clientpositive/llap/schema_evol_text_vecrow_part_all_complex.q.out
index b35bcccdde..70be462886 100644
--- a/ql/src/test/results/clientpositive/llap/schema_evol_text_vecrow_part_all_complex.q.out
+++ b/ql/src/test/results/clientpositive/llap/schema_evol_text_vecrow_part_all_complex.q.out
@@ -172,7 +172,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.row.serde.deserialize IS true
                 inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string> of Column[s1] not supported
+                notVectorizedReason: Select expression for SELECT operator: Vectorizing complex type STRUCT not enabled (struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string>) since hive.vectorized.complex.types.enabled IS false
                 vectorized: false
 
   Stage: Stage-0
@@ -454,7 +454,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.row.serde.deserialize IS true
                 inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string> of Column[s2] not supported
+                notVectorizedReason: Select expression for SELECT operator: Vectorizing complex type STRUCT not enabled (struct<c1:string,c2:string,c3:string,c4:string,c5:string,c6:string,c7:string,c8:string,c9:string,c10:string,c11:string,c12:string,c13:string>) since hive.vectorized.complex.types.enabled IS false
                 vectorized: false
 
   Stage: Stage-0
@@ -664,7 +664,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.row.serde.deserialize IS true
                 inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type struct<c1:boolean,c2:tinyint,c3:smallint,c4:int,c5:bigint,c6:float,c7:double,c8:decimal(38,18),c9:char(25),c10:varchar(25),c11:timestamp,c12:date,c13:binary> of Column[s3] not supported
+                notVectorizedReason: Select expression for SELECT operator: Vectorizing complex type STRUCT not enabled (struct<c1:boolean,c2:tinyint,c3:smallint,c4:int,c5:bigint,c6:float,c7:double,c8:decimal(38,18),c9:char(25),c10:varchar(25),c11:timestamp,c12:date,c13:binary>) since hive.vectorized.complex.types.enabled IS false
                 vectorized: false
 
   Stage: Stage-0
diff --git a/ql/src/test/results/clientpositive/llap/vector_adaptor_usage_mode.q.out b/ql/src/test/results/clientpositive/llap/vector_adaptor_usage_mode.q.out
index 6a0f490cd4..e63cbf8063 100644
--- a/ql/src/test/results/clientpositive/llap/vector_adaptor_usage_mode.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_adaptor_usage_mode.q.out
@@ -982,9 +982,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 5) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: string)
                       mode: hash
@@ -1025,9 +1027,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: string)
                 mode: mergepartial
@@ -1107,9 +1111,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 5) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: string)
                       mode: hash
@@ -1150,9 +1156,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_aggregate_9.q.out b/ql/src/test/results/clientpositive/llap/vector_aggregate_9.q.out
index 8a23d6a319..cf3dc23281 100644
--- a/ql/src/test/results/clientpositive/llap/vector_aggregate_9.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_aggregate_9.q.out
@@ -101,10 +101,10 @@ POSTHOOK: Lineage: vectortab2korc.si SIMPLE [(vectortab2k)vectortab2k.FieldSchem
 POSTHOOK: Lineage: vectortab2korc.t SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:t, type:tinyint, comment:null), ]
 POSTHOOK: Lineage: vectortab2korc.ts SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:ts, type:timestamp, comment:null), ]
 POSTHOOK: Lineage: vectortab2korc.ts2 SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:ts2, type:timestamp, comment:null), ]
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
@@ -142,17 +142,24 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: min(dc), max(dc), sum(dc), avg(dc)
                       Group By Vectorization:
-                          aggregators: VectorUDAFMinDecimal(col 6) -> decimal(38,18), VectorUDAFMaxDecimal(col 6) -> decimal(38,18), VectorUDAFSumDecimal(col 6) -> decimal(38,18), VectorUDAFAvgDecimal(col 6) -> struct<count:bigint,sum:decimal(38,18)>
+                          aggregators: VectorUDAFMinDecimal(col 6) -> decimal(38,18), VectorUDAFMaxDecimal(col 6) -> decimal(38,18), VectorUDAFSumDecimal(col 6) -> decimal(38,18), VectorUDAFAvgDecimal(col 6) -> struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDecimal(col 6) -> struct<count:bigint,sum:decimal(38,18)> output type STRUCT requires PRIMITIVE IS false
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
                       Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3]
                         Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: decimal(38,18)), _col1 (type: decimal(38,18)), _col2 (type: decimal(38,18)), _col3 (type: struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>)
             Execution mode: vectorized, llap
@@ -160,26 +167,50 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 13
+                    includeColumns: [6]
+                    dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
+                    partitionColumnCount: 0
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)> of Column[VALUE._col3] not supported
-                vectorized: false
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:decimal(38,18), VALUE._col1:decimal(38,18), VALUE._col2:decimal(38,18), VALUE._col3:struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), avg(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMinDecimal(col 0) -> decimal(38,18), VectorUDAFMaxDecimal(col 1) -> decimal(38,18), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFAvgDecimalFinal(col 3) -> decimal(38,18)
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -201,3 +232,265 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@vectortab2korc
 #### A masked pattern was here ####
 -4997414117561.546875000000000000	4994550248722.298828000000000000	-10252745435816.024410000000000000	-5399023399.587163986308583465
+PREHOOK: query: explain vectorization detail
+select min(d), max(d), sum(d), avg(d) from vectortab2korc
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization detail
+select min(d), max(d), sum(d), avg(d) from vectortab2korc
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: vectortab2korc
+                  Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
+                  Select Operator
+                    expressions: d (type: double)
+                    outputColumnNames: d
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [5]
+                    Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: min(d), max(d), sum(d), avg(d)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFMinDouble(col 5) -> double, VectorUDAFMaxDouble(col 5) -> double, VectorUDAFSumDouble(col 5) -> double, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0, 1, 2, 3]
+                      mode: hash
+                      outputColumnNames: _col0, _col1, _col2, _col3
+                      Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3]
+                        Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: double), _col1 (type: double), _col2 (type: double), _col3 (type: struct<count:bigint,sum:double,input:double>)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 13
+                    includeColumns: [5]
+                    dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:double, VALUE._col1:double, VALUE._col2:double, VALUE._col3:struct<count:bigint,sum:double,input:double>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), avg(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMinDouble(col 0) -> double, VectorUDAFMaxDouble(col 1) -> double, VectorUDAFSumDouble(col 2) -> double, VectorUDAFAvgFinal(col 3) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select min(d), max(d), sum(d), avg(d) from vectortab2korc
+PREHOOK: type: QUERY
+PREHOOK: Input: default@vectortab2korc
+#### A masked pattern was here ####
+POSTHOOK: query: select min(d), max(d), sum(d), avg(d) from vectortab2korc
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@vectortab2korc
+#### A masked pattern was here ####
+-4999829.07	4997627.14	-1.7516847286999977E8	-92193.93308947356
+PREHOOK: query: explain vectorization detail
+select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization detail
+select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: vectortab2korc
+                  Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
+                  Select Operator
+                    expressions: ts (type: timestamp)
+                    outputColumnNames: ts
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [10]
+                    Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: min(ts), max(ts), sum(ts), avg(ts)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFMinTimestamp(col 10) -> timestamp, VectorUDAFMaxTimestamp(col 10) -> timestamp, VectorUDAFSumTimestamp(col 10) -> double, VectorUDAFAvgTimestamp(col 10) -> struct<count:bigint,sum:double,input:timestamp>
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0, 1, 2, 3]
+                      mode: hash
+                      outputColumnNames: _col0, _col1, _col2, _col3
+                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3]
+                        Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: timestamp), _col1 (type: timestamp), _col2 (type: double), _col3 (type: struct<count:bigint,sum:double,input:timestamp>)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 13
+                    includeColumns: [10]
+                    dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:timestamp, VALUE._col1:timestamp, VALUE._col2:double, VALUE._col3:struct<count:bigint,sum:double,input:timestamp>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), avg(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 1) -> timestamp, VectorUDAFSumDouble(col 2) -> double, VectorUDAFAvgFinal(col 3) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
+PREHOOK: type: QUERY
+PREHOOK: Input: default@vectortab2korc
+#### A masked pattern was here ####
+POSTHOOK: query: select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@vectortab2korc
+#### A masked pattern was here ####
+2013-02-18 21:06:48	2081-02-22 01:21:53	4.591384881081E12	2.4254542425150557E9
diff --git a/ql/src/test/results/clientpositive/llap/vector_auto_smb_mapjoin_14.q.out b/ql/src/test/results/clientpositive/llap/vector_auto_smb_mapjoin_14.q.out
index a98c34f823..def04a8847 100644
--- a/ql/src/test/results/clientpositive/llap/vector_auto_smb_mapjoin_14.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_auto_smb_mapjoin_14.q.out
@@ -97,8 +97,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           mode: hash
                           outputColumnNames: _col0
@@ -123,8 +125,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -231,8 +235,10 @@ STAGE PLANS:
                         Statistics: Num rows: 11 Data size: 1023 Basic stats: COMPLETE Column stats: NONE
                         Group By Operator
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           keys: _col0 (type: int)
                           mode: hash
@@ -257,9 +263,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -276,8 +284,10 @@ STAGE PLANS:
                     Group By Vectorization:
                         aggregators: VectorUDAFCountStar(*) -> bigint
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: [0]
                     mode: hash
                     outputColumnNames: _col0
@@ -305,8 +315,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -439,8 +451,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           keys: _col0 (type: int)
                           mode: hash
@@ -487,8 +501,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           keys: _col0 (type: int)
                           mode: hash
@@ -516,9 +532,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -571,9 +589,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -703,8 +723,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           mode: hash
                           outputColumnNames: _col0
@@ -729,8 +751,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -844,8 +868,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           mode: hash
                           outputColumnNames: _col0
@@ -870,8 +896,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1009,8 +1037,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           mode: hash
                           outputColumnNames: _col0
@@ -1035,8 +1065,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1162,8 +1194,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           mode: hash
                           outputColumnNames: _col0
@@ -1188,8 +1222,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1360,8 +1396,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: count()
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -1385,8 +1423,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1486,8 +1526,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           mode: hash
                           outputColumnNames: _col0
@@ -1512,8 +1554,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1632,8 +1676,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           mode: hash
                           outputColumnNames: _col0
@@ -1658,8 +1704,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1781,8 +1829,10 @@ STAGE PLANS:
                         Group By Operator
                           aggregations: count()
                           Group By Vectorization:
+                              groupByMode: HASH
                               vectorOutput: false
                               native: false
+                              vectorProcessingMode: NONE
                               projectedOutputColumns: null
                           mode: hash
                           outputColumnNames: _col0
@@ -1807,8 +1857,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -2175,8 +2227,10 @@ STAGE PLANS:
                           Group By Operator
                             aggregations: count()
                             Group By Vectorization:
+                                groupByMode: HASH
                                 vectorOutput: false
                                 native: false
+                                vectorProcessingMode: NONE
                                 projectedOutputColumns: null
                             keys: _col0 (type: int)
                             mode: hash
@@ -2204,9 +2258,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_between_in.q.out b/ql/src/test/results/clientpositive/llap/vector_between_in.q.out
index 18dd1c6ef7..80c3060e32 100644
--- a/ql/src/test/results/clientpositive/llap/vector_between_in.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_between_in.q.out
@@ -154,8 +154,10 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFCountStar(*) -> bigint
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0]
                         mode: hash
                         outputColumnNames: _col0
@@ -193,8 +195,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -358,8 +362,10 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFCountStar(*) -> bigint
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0]
                         mode: hash
                         outputColumnNames: _col0
@@ -397,8 +403,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -754,8 +762,10 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFCountStar(*) -> bigint
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0]
                         mode: hash
                         outputColumnNames: _col0
@@ -793,8 +803,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1104,9 +1116,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 5:long) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 4
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: boolean)
                       mode: hash
@@ -1147,9 +1161,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
@@ -1242,9 +1258,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 5:long) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 4
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: boolean)
                       mode: hash
@@ -1285,9 +1303,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
@@ -1380,9 +1400,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 5:long) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 4
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: boolean)
                       mode: hash
@@ -1423,9 +1445,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
@@ -1518,9 +1542,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 5:long) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 4
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: boolean)
                       mode: hash
@@ -1561,9 +1587,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_binary_join_groupby.q.out b/ql/src/test/results/clientpositive/llap/vector_binary_join_groupby.q.out
index 160a43b33f..9f059b9312 100644
--- a/ql/src/test/results/clientpositive/llap/vector_binary_join_groupby.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_binary_join_groupby.q.out
@@ -174,8 +174,10 @@ STAGE PLANS:
                             Group By Vectorization:
                                 aggregators: VectorUDAFSumLong(col 21) -> bigint
                                 className: VectorGroupByOperator
+                                groupByMode: HASH
                                 vectorOutput: true
                                 native: false
+                                vectorProcessingMode: HASH
                                 projectedOutputColumns: [0]
                             mode: hash
                             outputColumnNames: _col0
@@ -256,8 +258,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -368,9 +372,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 10
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: bin (type: binary)
                       mode: hash
@@ -411,9 +417,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: binary)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_cast_constant.q.out b/ql/src/test/results/clientpositive/llap/vector_cast_constant.q.out
index f06d49a32b..2b8aaaa0df 100644
--- a/ql/src/test/results/clientpositive/llap/vector_cast_constant.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_cast_constant.q.out
@@ -145,13 +145,14 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: avg(50), avg(50.0), avg(50)
                       Group By Vectorization:
-                          aggregators: VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 11:long) -> struct<count:bigint,sum:double>, VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 12:double) -> struct<count:bigint,sum:double>, VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 13:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0)>
+                          aggregators: VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 11:long) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 12:double) -> struct<count:bigint,sum:double,input:double>, VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 13:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0),input:decimal(20,0)>
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           keyExpressions: col 2
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 11:long) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 12:double) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 13:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0)> output type STRUCT requires PRIMITIVE IS false
                       keys: _col0 (type: int)
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -160,6 +161,10 @@ STAGE PLANS:
                         key expressions: _col0 (type: int)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: int)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkLongOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                         TopN Hash Memory Usage: 0.1
                         value expressions: _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: struct<count:bigint,sum:double,input:double>), _col3 (type: struct<count:bigint,sum:decimal(12,0),input:decimal(10,0)>)
@@ -168,21 +173,32 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:int> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), avg(VALUE._col1), avg(VALUE._col2)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 1) -> double, VectorUDAFAvgFinal(col 2) -> double, VectorUDAFAvgDecimalFinal(col 3) -> decimal(16,4)
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2]
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -190,6 +206,10 @@ STAGE PLANS:
                 Reduce Output Operator
                   key expressions: _col0 (type: int)
                   sort order: +
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkObjectHashOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                   Statistics: Num rows: 524 Data size: 155436 Basic stats: COMPLETE Column stats: NONE
                   TopN Hash Memory Usage: 0.1
                   value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: decimal(14,4))
diff --git a/ql/src/test/results/clientpositive/llap/vector_char_2.q.out b/ql/src/test/results/clientpositive/llap/vector_char_2.q.out
index 65fafb0ad4..48c62812f0 100644
--- a/ql/src/test/results/clientpositive/llap/vector_char_2.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_char_2.q.out
@@ -98,9 +98,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 2) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 1
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1]
                       keys: _col0 (type: char(20))
                       mode: hash
@@ -142,9 +144,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 1) -> bigint, VectorUDAFCountMerge(col 2) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0, 1]
                 keys: KEY._col0 (type: char(20))
                 mode: mergepartial
@@ -294,9 +298,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 2) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 1
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1]
                       keys: _col0 (type: char(20))
                       mode: hash
@@ -338,9 +344,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 1) -> bigint, VectorUDAFCountMerge(col 2) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0, 1]
                 keys: KEY._col0 (type: char(20))
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_coalesce_2.q.out b/ql/src/test/results/clientpositive/llap/vector_coalesce_2.q.out
index d57d39f335..55a1d4208a 100644
--- a/ql/src/test/results/clientpositive/llap/vector_coalesce_2.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_coalesce_2.q.out
@@ -54,8 +54,10 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: sum(_col1)
                       Group By Vectorization:
+                          groupByMode: HASH
                           vectorOutput: false
                           native: false
+                          vectorProcessingMode: NONE
                           projectedOutputColumns: null
                       keys: _col0 (type: string)
                       mode: hash
@@ -75,8 +77,10 @@ STAGE PLANS:
               Group By Operator
                 aggregations: sum(VALUE._col0)
                 Group By Vectorization:
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: false
                     native: false
+                    vectorProcessingMode: NONE
                     projectedOutputColumns: null
                 keys: KEY._col0 (type: string)
                 mode: mergepartial
@@ -226,9 +230,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 4) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 1
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: string)
                       mode: hash
@@ -269,9 +275,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_complex_all.q.out b/ql/src/test/results/clientpositive/llap/vector_complex_all.q.out
index 1107f82bff..4503cc4696 100644
--- a/ql/src/test/results/clientpositive/llap/vector_complex_all.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_complex_all.q.out
@@ -66,6 +66,74 @@ POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create
 POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
 POSTHOOK: Lineage: orc_create_complex.val SIMPLE []
 orc_create_staging.str	orc_create_staging.mp	orc_create_staging.lst	orc_create_staging.strct	_c1
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT * FROM orc_create_complex
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT * FROM orc_create_complex
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  Select Operator
+                    expressions: str (type: string), mp (type: map<string,string>), lst (type: array<string>), strct (type: struct<a:string,b:string>), val (type: string)
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1, 2, 3, 4]
+                    Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
+                      Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                      table:
+                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    includeColumns: [0, 1, 2, 3, 4]
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>, val:string
+                    partitionColumnCount: 0
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT * FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -75,9 +143,77 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
 orc_create_complex.str	orc_create_complex.mp	orc_create_complex.lst	orc_create_complex.strct	orc_create_complex.val
-line1	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]	{"a":"one","b":"two"}	0
+line1	{"key11":"value11","key12":"value12","key13":"value13"}	["a","b","c"]	{"a":"one","b":"two"}	0
 line2	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]	{"a":"three","b":"four"}	0
 line3	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]	{"a":"five","b":"six"}	0
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT str FROM orc_create_complex
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT str FROM orc_create_complex
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  Select Operator
+                    expressions: str (type: string)
+                    outputColumnNames: _col0
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
+                      Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                      table:
+                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    includeColumns: [0]
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>, val:string
+                    partitionColumnCount: 0
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT str FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -90,6 +226,74 @@ str
 line1
 line2
 line3
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct, mp, lst FROM orc_create_complex
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct, mp, lst FROM orc_create_complex
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  Select Operator
+                    expressions: strct (type: struct<a:string,b:string>), mp (type: map<string,string>), lst (type: array<string>)
+                    outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [3, 1, 2]
+                    Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
+                      Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                      table:
+                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    includeColumns: [1, 2, 3]
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>, val:string
+                    partitionColumnCount: 0
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT strct, mp, lst FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -99,9 +303,77 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
 strct	mp	lst
-{"a":"one","b":"two"}	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]
+{"a":"one","b":"two"}	{"key11":"value11","key12":"value12","key13":"value13"}	["a","b","c"]
 {"a":"three","b":"four"}	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]
 {"a":"five","b":"six"}	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT lst, str FROM orc_create_complex
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT lst, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  Select Operator
+                    expressions: lst (type: array<string>), str (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [2, 0]
+                    Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
+                      Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                      table:
+                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    includeColumns: [0, 2]
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>, val:string
+                    partitionColumnCount: 0
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT lst, str FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -114,6 +386,74 @@ lst	str
 ["a","b","c"]	line1
 ["d","e","f"]	line2
 ["g","h","i"]	line3
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT mp, str FROM orc_create_complex
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT mp, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  Select Operator
+                    expressions: mp (type: map<string,string>), str (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [1, 0]
+                    Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
+                      Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                      table:
+                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    includeColumns: [0, 1]
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>, val:string
+                    partitionColumnCount: 0
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT mp, str FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -123,9 +463,77 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
 mp	str
-{"key13":"value13","key11":"value11","key12":"value12"}	line1
+{"key11":"value11","key12":"value12","key13":"value13"}	line1
 {"key21":"value21","key22":"value22","key23":"value23"}	line2
 {"key31":"value31","key32":"value32","key33":"value33"}	line3
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct, str FROM orc_create_complex
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  Select Operator
+                    expressions: strct (type: struct<a:string,b:string>), str (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [3, 0]
+                    Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
+                      Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                      table:
+                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    includeColumns: [0, 3]
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>, val:string
+                    partitionColumnCount: 0
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT strct, str FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -138,6 +546,58 @@ strct	str
 {"a":"one","b":"two"}	line1
 {"a":"three","b":"four"}	line2
 {"a":"five","b":"six"}	line3
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct.B, str FROM orc_create_complex
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct.B, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Pruned Column Paths: strct.b
+                  Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: strct.b (type: string), str (type: string)
+                    outputColumnNames: _col0, _col1
+                    Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      Statistics: Num rows: 3 Data size: 3432 Basic stats: COMPLETE Column stats: NONE
+                      table:
+                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                notVectorizedReason: Select expression for SELECT operator: Could not vectorize expression (mode = PROJECTION): Column[strct].b
+                vectorized: false
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT strct.B, str FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -150,7 +610,237 @@ b	str
 two	line1
 four	line2
 six	line3
-Warning: Shuffle Join MERGEJOIN[15][tables = [$hdt$_1, $hdt$_2, $hdt$_3, $hdt$_0]] in Stage 'Reducer 2' is a cross product
+Warning: Map Join MAPJOIN[15][bigTable=?] in task 'Map 4' is a cross product
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+INSERT INTO TABLE orc_create_complex
+SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+INSERT INTO TABLE orc_create_complex
+SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Map 4 <- Map 1 (BROADCAST_EDGE), Map 2 (BROADCAST_EDGE), Map 3 (BROADCAST_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_staging
+                  Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3]
+                  Select Operator
+                    expressions: str (type: string), mp (type: map<string,string>), lst (type: array<string>), strct (type: struct<a:string,b:string>)
+                    outputColumnNames: _col0, _col1, _col2, _col3
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1, 2, 3]
+                    Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      sort order: 
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkEmptyKeyOperator
+                          keyColumns: []
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                          valueColumns: [0, 1, 2, 3]
+                      Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string), _col1 (type: map<string,string>), _col2 (type: array<string>), _col3 (type: struct<a:string,b:string>)
+            Execution mode: vectorized, llap
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    includeColumns: [0, 1, 2, 3]
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>
+                    partitionColumnCount: 0
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: spam2
+                  Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3]
+                  Select Operator
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: []
+                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      sort order: 
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkEmptyKeyOperator
+                          keyColumns: []
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                          valueColumns: []
+                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+            Execution mode: vectorized, llap
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    includeColumns: []
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>
+                    partitionColumnCount: 0
+        Map 3 
+            Map Operator Tree:
+                TableScan
+                  alias: spam1
+                  Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3]
+                  Select Operator
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: []
+                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      sort order: 
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkEmptyKeyOperator
+                          keyColumns: []
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                          valueColumns: []
+                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+            Execution mode: vectorized, llap
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    includeColumns: []
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>
+                    partitionColumnCount: 0
+        Map 4 
+            Map Operator Tree:
+                TableScan
+                  alias: src1
+                  Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string)
+                    outputColumnNames: _col0
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
+                    Map Join Operator
+                      condition map:
+                           Inner Join 0 to 1
+                           Inner Join 0 to 2
+                           Inner Join 0 to 3
+                      keys:
+                        0 
+                        1 
+                        2 
+                        3 
+                      Map Join Vectorization:
+                          className: VectorMapJoinOperator
+                          native: false
+                          nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No nullsafe IS true, Small table vectorizes IS true, Optimized Table and Supports Key Types IS true
+                          nativeConditionsNotMet: One MapJoin Condition IS false
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col6
+                      input vertices:
+                        0 Map 1
+                        1 Map 2
+                        2 Map 3
+                      Statistics: Num rows: 500 Data size: 143000 Basic stats: COMPLETE Column stats: NONE
+                      Select Operator
+                        expressions: _col0 (type: string), _col1 (type: map<string,string>), _col2 (type: array<string>), _col3 (type: struct<a:string,b:string>), _col6 (type: string)
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                        Select Vectorization:
+                            className: VectorSelectOperator
+                            native: true
+                            projectedOutputColumns: [0, 1, 2, 3, 4]
+                        Statistics: Num rows: 500 Data size: 143000 Basic stats: COMPLETE Column stats: NONE
+                        File Output Operator
+                          compressed: false
+                          File Sink Vectorization:
+                              className: VectorFileSinkOperator
+                              native: false
+                          Statistics: Num rows: 500 Data size: 143000 Basic stats: COMPLETE Column stats: NONE
+                          table:
+                              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                              name: default.orc_create_complex
+            Execution mode: vectorized, llap
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    includeColumns: [0]
+                    dataColumns: key:string, value:string
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, map<string,string>, array<string>, struct<a:string,b:string>
+
+  Stage: Stage-2
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.orc_create_complex
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+Warning: Map Join MAPJOIN[15][bigTable=?] in task 'Map 4' is a cross product
 PREHOOK: query: INSERT INTO TABLE orc_create_complex
 SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2
 PREHOOK: type: QUERY
@@ -169,6 +859,126 @@ POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create
 POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
 POSTHOOK: Lineage: orc_create_complex.val SIMPLE [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 orc_create_staging.str	orc_create_staging.mp	orc_create_staging.lst	orc_create_staging.strct	src1.key
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+select count(*) from orc_create_complex
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+select count(*) from orc_create_complex
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  Select Operator
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: []
+                    Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: COMPLETE
+                    Group By Operator
+                      aggregations: count()
+                      Group By Vectorization:
+                          aggregators: VectorUDAFCountStar(*) -> bigint
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0]
+                      mode: hash
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0]
+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col0 (type: bigint)
+            Execution mode: vectorized, llap
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    includeColumns: []
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>, val:string
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:bigint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: select count(*) from orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -179,6 +989,83 @@ POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
 _c0
 13503
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT distinct lst, strct FROM orc_create_complex
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT distinct lst, strct FROM orc_create_complex
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: lst (type: array<string>), strct (type: struct<a:string,b:string>)
+                    outputColumnNames: lst, strct
+                    Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      keys: lst (type: array<string>), strct (type: struct<a:string,b:string>)
+                      mode: hash
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: array<string>), _col1 (type: struct<a:string,b:string>)
+                        sort order: ++
+                        Map-reduce partition columns: _col0 (type: array<string>), _col1 (type: struct<a:string,b:string>)
+                        Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                notVectorizedReason: Key expression for GROUPBY operator: Vectorizing complex type LIST not supported
+                vectorized: false
+        Reducer 2 
+            Execution mode: llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                notVectorizedReason: Key expression for GROUPBY operator: Vectorizing complex type LIST not supported
+                vectorized: false
+            Reduce Operator Tree:
+              Group By Operator
+                keys: KEY._col0 (type: array<string>), KEY._col1 (type: struct<a:string,b:string>)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 6751 Data size: 7729893 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 6751 Data size: 7729893 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT distinct lst, strct FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -191,6 +1078,135 @@ lst	strct
 ["a","b","c"]	{"a":"one","b":"two"}
 ["d","e","f"]	{"a":"three","b":"four"}
 ["g","h","i"]	{"a":"five","b":"six"}
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT str, count(val)  FROM orc_create_complex GROUP BY str
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT str, count(val)  FROM orc_create_complex GROUP BY str
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  Select Operator
+                    expressions: str (type: string), val (type: string)
+                    outputColumnNames: str, val
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 4]
+                    Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: count(val)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFCount(col 4) -> bigint
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          keyExpressions: col 0
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0]
+                      keys: str (type: string)
+                      mode: hash
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: string)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkStringOperator
+                            keyColumns: [0]
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [1]
+                        Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col1 (type: bigint)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    includeColumns: [0, 4]
+                    dataColumns: str:string, mp:map<string,string>, lst:array<string>, strct:struct<a:string,b:string>, val:string
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    dataColumns: KEY._col0:string, VALUE._col0:bigint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 1) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0]
+                keys: KEY._col0 (type: string)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 6751 Data size: 7729893 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 6751 Data size: 7729893 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT str, count(val)  FROM orc_create_complex GROUP BY str
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -203,6 +1219,107 @@ str	_c1
 line3	4501
 line1	4501
 line2	4501
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Pruned Column Paths: strct.b
+                  Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: strct.b (type: string), val (type: string)
+                    outputColumnNames: _col0, _col1
+                    Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: count(_col1)
+                      keys: _col0 (type: string)
+                      mode: hash
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: string)
+                        Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col1 (type: bigint)
+            Execution mode: llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                notVectorizedReason: Select expression for SELECT operator: Could not vectorize expression (mode = PROJECTION): Column[strct].b
+                vectorized: false
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    dataColumns: KEY._col0:string, VALUE._col0:bigint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 1) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0]
+                keys: KEY._col0 (type: string)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 6751 Data size: 7729893 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 6751 Data size: 7729893 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -215,6 +1332,90 @@ strct.b	_c1
 six	4501
 two	4501
 four	4501
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orc_create_complex
+                  Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: str (type: string), mp (type: map<string,string>), lst (type: array<string>), strct (type: struct<a:string,b:string>), val (type: string)
+                    outputColumnNames: str, mp, lst, strct, val
+                    Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: count(val)
+                      keys: str (type: string), mp (type: map<string,string>), lst (type: array<string>), strct (type: struct<a:string,b:string>)
+                      mode: hash
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                      Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string), _col1 (type: map<string,string>), _col2 (type: array<string>), _col3 (type: struct<a:string,b:string>)
+                        sort order: ++++
+                        Map-reduce partition columns: _col0 (type: string), _col1 (type: map<string,string>), _col2 (type: array<string>), _col3 (type: struct<a:string,b:string>)
+                        Statistics: Num rows: 13503 Data size: 15460932 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col4 (type: bigint)
+            Execution mode: llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                notVectorizedReason: Key expression for GROUPBY operator: Vectorizing complex type MAP not supported
+                vectorized: false
+        Reducer 2 
+            Execution mode: llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                notVectorizedReason: Key expression for GROUPBY operator: Vectorizing complex type MAP not supported
+                vectorized: false
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                keys: KEY._col0 (type: string), KEY._col1 (type: map<string,string>), KEY._col2 (type: array<string>), KEY._col3 (type: struct<a:string,b:string>)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Statistics: Num rows: 6751 Data size: 7729893 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col3 (type: struct<a:string,b:string>), _col1 (type: map<string,string>), _col2 (type: array<string>), _col0 (type: string), _col4 (type: bigint)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                  Statistics: Num rows: 6751 Data size: 7729893 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 6751 Data size: 7729893 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
diff --git a/ql/src/test/results/clientpositive/llap/vector_complex_join.q.out b/ql/src/test/results/clientpositive/llap/vector_complex_join.q.out
index 5ea4b0f639..e389cd36c2 100644
--- a/ql/src/test/results/clientpositive/llap/vector_complex_join.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_complex_join.q.out
@@ -45,12 +45,23 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 3093170 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: SelectColumnIsNotNull(col 2) -> boolean
                     predicate: cint is not null (type: boolean)
                     Statistics: Num rows: 9173 Data size: 2309110 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
                       expressions: ctinyint (type: tinyint), csmallint (type: smallint), cint (type: int), cbigint (type: bigint), cfloat (type: float), cdouble (type: double), cstring1 (type: string), cstring2 (type: string), ctimestamp1 (type: timestamp), ctimestamp2 (type: timestamp), cboolean1 (type: boolean), cboolean2 (type: boolean)
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                       Statistics: Num rows: 9173 Data size: 2309110 Basic stats: COMPLETE Column stats: COMPLETE
                       Map Join Operator
                         condition map:
@@ -58,51 +69,77 @@ STAGE PLANS:
                         keys:
                           0 _col2 (type: int)
                           1 _col0 (type: int)
+                        Map Join Vectorization:
+                            className: VectorMapJoinInnerLongOperator
+                            native: true
+                            nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true, Optimized Table and Supports Key Types IS true
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
                         input vertices:
                           1 Map 2
                         Statistics: Num rows: 10090 Data size: 2540021 Basic stats: COMPLETE Column stats: NONE
                         File Output Operator
                           compressed: false
+                          File Sink Vectorization:
+                              className: VectorFileSinkOperator
+                              native: false
                           Statistics: Num rows: 10090 Data size: 2540021 Basic stats: COMPLETE Column stats: NONE
                           table:
                               input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                               output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            Execution mode: llap
+            Execution mode: vectorized, llap
             LLAP IO: all inputs
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Small Table expression for MAPJOIN operator: Data type map<int,string> of Column[_col1] not supported
-                vectorized: false
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
         Map 2 
             Map Operator Tree:
                 TableScan
                   alias: test
                   Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: SelectColumnIsNotNull(col 0) -> boolean
                     predicate: a is not null (type: boolean)
                     Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: a (type: int), b (type: map<int,string>)
                       outputColumnNames: _col0, _col1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1]
                       Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         key expressions: _col0 (type: int)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: int)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkLongOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col1 (type: map<int,string>)
-            Execution mode: llap
+            Execution mode: vectorized, llap
             LLAP IO: all inputs
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type map<int,string> of Column[b] not supported
-                vectorized: false
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
 
   Stage: Stage-0
     Fetch Operator
@@ -184,7 +221,14 @@ STAGE PLANS:
                 TableScan
                   alias: test2b
                   Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: SelectColumnIsNotNull(col 0) -> boolean
                     predicate: a is not null (type: boolean)
                     Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                     Map Join Operator
@@ -193,6 +237,10 @@ STAGE PLANS:
                       keys:
                         0 a (type: int)
                         1 a[1] (type: int)
+                      Map Join Vectorization:
+                          className: VectorMapJoinInnerLongOperator
+                          native: true
+                          nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true, Optimized Table and Supports Key Types IS true
                       outputColumnNames: _col0, _col4
                       input vertices:
                         1 Map 2
@@ -200,22 +248,31 @@ STAGE PLANS:
                       Select Operator
                         expressions: _col0 (type: int), _col4 (type: array<int>)
                         outputColumnNames: _col0, _col1
+                        Select Vectorization:
+                            className: VectorSelectOperator
+                            native: true
+                            projectedOutputColumns: [0, 1]
                         Statistics: Num rows: 3 Data size: 13 Basic stats: COMPLETE Column stats: NONE
                         File Output Operator
                           compressed: false
+                          File Sink Vectorization:
+                              className: VectorFileSinkOperator
+                              native: false
                           Statistics: Num rows: 3 Data size: 13 Basic stats: COMPLETE Column stats: NONE
                           table:
                               input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                               output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            Execution mode: llap
+            Execution mode: vectorized, llap
             LLAP IO: all inputs
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Small Table expression for MAPJOIN operator: Data type array<int> of Column[a] not supported
-                vectorized: false
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
         Map 2 
             Map Operator Tree:
                 TableScan
@@ -236,7 +293,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Predicate expression for FILTER operator: Data type array<int> of Column[a] not supported
+                notVectorizedReason: Predicate expression for FILTER operator: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected hive type name array<int>
                 vectorized: false
 
   Stage: Stage-0
diff --git a/ql/src/test/results/clientpositive/llap/vector_count.q.out b/ql/src/test/results/clientpositive/llap/vector_count.q.out
index 5fa5a8221e..21d92cd06b 100644
--- a/ql/src/test/results/clientpositive/llap/vector_count.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_count.q.out
@@ -84,9 +84,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 2) -> bigint, VectorUDAFSumLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 1, col 2
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2]
                       keys: a (type: int), b (type: int), c (type: int)
                       mode: hash
@@ -194,9 +196,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 4:long) -> bigint, VectorUDAFCountStar(*) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 2) -> bigint, VectorUDAFCount(col 3) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 2) -> bigint, VectorUDAFCount(col 3) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 2) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 1, col 2, col 3
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
                       keys: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                       mode: hash
diff --git a/ql/src/test/results/clientpositive/llap/vector_count_distinct.q.out b/ql/src/test/results/clientpositive/llap/vector_count_distinct.q.out
index b9d0f06c3f..d45a15ffb8 100644
--- a/ql/src/test/results/clientpositive/llap/vector_count_distinct.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_count_distinct.q.out
@@ -1267,9 +1267,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 16
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: ws_order_number (type: int)
                       mode: hash
@@ -1307,9 +1309,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -1320,8 +1324,10 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFCount(col 0) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: [0]
                   mode: hash
                   outputColumnNames: _col0
@@ -1349,8 +1355,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/llap/vector_decimal_aggregate.q.out b/ql/src/test/results/clientpositive/llap/vector_decimal_aggregate.q.out
index ab38382342..8ea03f7446 100644
--- a/ql/src/test/results/clientpositive/llap/vector_decimal_aggregate.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_decimal_aggregate.q.out
@@ -71,9 +71,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFMaxDecimal(col 1) -> decimal(20,10), VectorUDAFMinDecimal(col 1) -> decimal(20,10), VectorUDAFSumDecimal(col 1) -> decimal(38,18), VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(23,14), VectorUDAFMinDecimal(col 2) -> decimal(23,14), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 3
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
                       keys: cint (type: int)
                       mode: hash
@@ -114,9 +116,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(20,10), VectorUDAFMinDecimal(col 3) -> decimal(20,10), VectorUDAFSumDecimal(col 4) -> decimal(38,18), VectorUDAFCountMerge(col 5) -> bigint, VectorUDAFMaxDecimal(col 6) -> decimal(23,14), VectorUDAFMinDecimal(col 7) -> decimal(23,14), VectorUDAFSumDecimal(col 8) -> decimal(38,18), VectorUDAFCountMerge(col 9) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -229,13 +233,14 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: count(cdecimal1), max(cdecimal1), min(cdecimal1), sum(cdecimal1), avg(cdecimal1), stddev_pop(cdecimal1), stddev_samp(cdecimal1), count(cdecimal2), max(cdecimal2), min(cdecimal2), sum(cdecimal2), avg(cdecimal2), stddev_pop(cdecimal2), stddev_samp(cdecimal2), count()
                       Group By Vectorization:
-                          aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFMaxDecimal(col 1) -> decimal(20,10), VectorUDAFMinDecimal(col 1) -> decimal(20,10), VectorUDAFSumDecimal(col 1) -> decimal(38,18), VectorUDAFAvgDecimal(col 1) -> struct<count:bigint,sum:decimal(30,10)>, VectorUDAFStdPopDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(23,14), VectorUDAFMinDecimal(col 2) -> decimal(23,14), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFAvgDecimal(col 2) -> struct<count:bigint,sum:decimal(33,14)>, VectorUDAFStdPopDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint
+                          aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFMaxDecimal(col 1) -> decimal(20,10), VectorUDAFMinDecimal(col 1) -> decimal(20,10), VectorUDAFSumDecimal(col 1) -> decimal(38,18), VectorUDAFAvgDecimal(col 1) -> struct<count:bigint,sum:decimal(30,10),input:decimal(30,10)>, VectorUDAFStdPopDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(23,14), VectorUDAFMinDecimal(col 2) -> decimal(23,14), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFAvgDecimal(col 2) -> struct<count:bigint,sum:decimal(33,14),input:decimal(33,14)>, VectorUDAFStdPopDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           keyExpressions: col 3
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDecimal(col 1) -> struct<count:bigint,sum:decimal(30,10)> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDecimal(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampDecimal(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDecimal(col 2) -> struct<count:bigint,sum:decimal(33,14)> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDecimal(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampDecimal(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                       keys: cint (type: int)
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
@@ -244,6 +249,10 @@ STAGE PLANS:
                         key expressions: _col0 (type: int)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: int)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkLongOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 12288 Data size: 2165060 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col1 (type: bigint), _col2 (type: decimal(20,10)), _col3 (type: decimal(20,10)), _col4 (type: decimal(30,10)), _col5 (type: struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)>), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: bigint), _col9 (type: decimal(23,14)), _col10 (type: decimal(23,14)), _col11 (type: decimal(33,14)), _col12 (type: struct<count:bigint,sum:decimal(33,14),input:decimal(23,14)>), _col13 (type: struct<count:bigint,sum:double,variance:double>), _col14 (type: struct<count:bigint,sum:double,variance:double>), _col15 (type: bigint)
             Execution mode: vectorized, llap
@@ -251,34 +260,56 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)> of Column[VALUE._col4] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), sum(VALUE._col3), avg(VALUE._col4), stddev_pop(VALUE._col5), stddev_samp(VALUE._col6), count(VALUE._col7), max(VALUE._col8), min(VALUE._col9), sum(VALUE._col10), avg(VALUE._col11), stddev_pop(VALUE._col12), stddev_samp(VALUE._col13), count(VALUE._col14)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 1) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(20,10), VectorUDAFMinDecimal(col 3) -> decimal(20,10), VectorUDAFSumDecimal(col 4) -> decimal(38,18), VectorUDAFAvgDecimalFinal(col 5) -> decimal(34,14), VectorUDAFStdPopFinal(col 6) -> double, VectorUDAFStdSampFinal(col 7) -> double, VectorUDAFCountMerge(col 8) -> bigint, VectorUDAFMaxDecimal(col 9) -> decimal(23,14), VectorUDAFMinDecimal(col 10) -> decimal(23,14), VectorUDAFSumDecimal(col 11) -> decimal(38,18), VectorUDAFAvgDecimalFinal(col 12) -> decimal(37,18), VectorUDAFStdPopFinal(col 13) -> double, VectorUDAFStdSampFinal(col 14) -> double, VectorUDAFCountMerge(col 15) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
                 Statistics: Num rows: 6144 Data size: 1082530 Basic stats: COMPLETE Column stats: NONE
                 Filter Operator
+                  Filter Vectorization:
+                      className: VectorFilterOperator
+                      native: true
+                      predicateExpression: FilterLongColGreaterLongScalar(col 15, val 1) -> boolean
                   predicate: (_col15 > 1) (type: boolean)
                   Statistics: Num rows: 2048 Data size: 360843 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
                     expressions: _col0 (type: int), _col1 (type: bigint), _col2 (type: decimal(20,10)), _col3 (type: decimal(20,10)), _col4 (type: decimal(30,10)), _col5 (type: decimal(24,14)), _col6 (type: double), _col7 (type: double), _col8 (type: bigint), _col9 (type: decimal(23,14)), _col10 (type: decimal(23,14)), _col11 (type: decimal(33,14)), _col12 (type: decimal(27,18)), _col13 (type: double), _col14 (type: double)
                     outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
                     Statistics: Num rows: 2048 Data size: 360843 Basic stats: COMPLETE Column stats: NONE
                     File Output Operator
                       compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
                       Statistics: Num rows: 2048 Data size: 360843 Basic stats: COMPLETE Column stats: NONE
                       table:
                           input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vector_decimal_precision.q.out b/ql/src/test/results/clientpositive/llap/vector_decimal_precision.q.out
index 5d6208608f..3f32eb2822 100644
--- a/ql/src/test/results/clientpositive/llap/vector_decimal_precision.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_decimal_precision.q.out
@@ -584,17 +584,22 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: avg(dec), sum(dec)
                       Group By Vectorization:
-                          aggregators: VectorUDAFAvgDecimal(col 0) -> struct<count:bigint,sum:decimal(30,10)>, VectorUDAFSumDecimal(col 0) -> decimal(38,18)
+                          aggregators: VectorUDAFAvgDecimal(col 0) -> struct<count:bigint,sum:decimal(30,10),input:decimal(30,10)>, VectorUDAFSumDecimal(col 0) -> decimal(38,18)
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDecimal(col 0) -> struct<count:bigint,sum:decimal(30,10)> output type STRUCT requires PRIMITIVE IS false
                       mode: hash
                       outputColumnNames: _col0, _col1
                       Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)>), _col1 (type: decimal(30,10))
             Execution mode: vectorized, llap
@@ -602,26 +607,39 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), sum(VALUE._col1)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgDecimalFinal(col 0) -> decimal(34,14), VectorUDAFSumDecimal(col 1) -> decimal(38,18)
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vector_decimal_udf.q.out b/ql/src/test/results/clientpositive/llap/vector_decimal_udf.q.out
index c271b8230c..56127a51f4 100644
--- a/ql/src/test/results/clientpositive/llap/vector_decimal_udf.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_decimal_udf.q.out
@@ -1666,7 +1666,7 @@ STAGE PLANS:
             Execution mode: vectorized, llap
             LLAP IO: all inputs
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Operator Tree:
               Group By Operator
                 aggregations: sum(VALUE._col0), count(VALUE._col1), avg(VALUE._col2)
@@ -2338,7 +2338,7 @@ STAGE PLANS:
             Execution mode: vectorized, llap
             LLAP IO: all inputs
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev(VALUE._col0), variance(VALUE._col1)
@@ -2425,7 +2425,7 @@ STAGE PLANS:
             Execution mode: vectorized, llap
             LLAP IO: all inputs
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev_samp(VALUE._col0), var_samp(VALUE._col1)
diff --git a/ql/src/test/results/clientpositive/llap/vector_distinct_2.q.out b/ql/src/test/results/clientpositive/llap/vector_distinct_2.q.out
index f17583f09a..b7f6a80686 100644
--- a/ql/src/test/results/clientpositive/llap/vector_distinct_2.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_distinct_2.q.out
@@ -142,9 +142,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 8
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: t (type: tinyint), s (type: string)
                       mode: hash
@@ -182,9 +184,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: tinyint), KEY._col1 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_empty_where.q.out b/ql/src/test/results/clientpositive/llap/vector_empty_where.q.out
index f2bc0a5940..b250332974 100644
--- a/ql/src/test/results/clientpositive/llap/vector_empty_where.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_empty_where.q.out
@@ -47,9 +47,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 2
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: cint (type: int)
                         mode: hash
@@ -87,9 +89,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -100,8 +104,10 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFCount(col 0) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: [0]
                   mode: hash
                   outputColumnNames: _col0
@@ -129,8 +135,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -202,9 +210,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 2
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: cint (type: int)
                       mode: hash
@@ -242,9 +252,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -255,8 +267,10 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFCount(col 0) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: [0]
                   mode: hash
                   outputColumnNames: _col0
@@ -284,8 +298,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -365,9 +381,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 2
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: cint (type: int)
                         mode: hash
@@ -405,9 +423,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -418,8 +438,10 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFCount(col 0) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: [0]
                   mode: hash
                   outputColumnNames: _col0
@@ -447,8 +469,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -528,9 +552,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 2
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: cint (type: int)
                         mode: hash
@@ -568,9 +594,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -581,8 +609,10 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFCount(col 0) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: [0]
                   mode: hash
                   outputColumnNames: _col0
@@ -610,8 +640,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/llap/vector_groupby4.q.out b/ql/src/test/results/clientpositive/llap/vector_groupby4.q.out
index ffeab2c132..43995549f6 100644
--- a/ql/src/test/results/clientpositive/llap/vector_groupby4.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_groupby4.q.out
@@ -94,9 +94,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: PARTIAL1
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string)
                 mode: partial1
@@ -124,9 +126,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string)
                 mode: final
diff --git a/ql/src/test/results/clientpositive/llap/vector_groupby6.q.out b/ql/src/test/results/clientpositive/llap/vector_groupby6.q.out
index 5bfa9b5e8a..a91b715780 100644
--- a/ql/src/test/results/clientpositive/llap/vector_groupby6.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_groupby6.q.out
@@ -94,9 +94,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: PARTIAL1
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string)
                 mode: partial1
@@ -124,9 +126,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string)
                 mode: final
diff --git a/ql/src/test/results/clientpositive/llap/vector_groupby_3.q.out b/ql/src/test/results/clientpositive/llap/vector_groupby_3.q.out
index 0242cbdf17..cd9ff27350 100644
--- a/ql/src/test/results/clientpositive/llap/vector_groupby_3.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_groupby_3.q.out
@@ -144,9 +144,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMaxLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 8
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: t (type: tinyint), s (type: string)
                       mode: hash
@@ -187,9 +189,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMaxLong(col 2) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: tinyint), KEY._col1 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_id3.q.out b/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_id3.q.out
index a4ef2e7780..22a71e4eeb 100644
--- a/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_id3.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_id3.q.out
@@ -79,9 +79,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 1, ConstantVectorExpression(val 0) -> 2:long
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: key (type: int), value (type: int), 0 (type: int)
                       mode: hash
@@ -145,9 +147,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1, ConstantVectorExpression(val 1) -> 4:long
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: int), KEY._col1 (type: int), 1 (type: int)
                 mode: mergepartial
@@ -253,9 +257,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 1, ConstantVectorExpression(val 0) -> 2:long
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: int), _col1 (type: int), 0 (type: int)
                       mode: hash
@@ -310,9 +316,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1, col 2
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: int), KEY._col1 (type: int), KEY._col2 (type: int)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_sets3.q.out b/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_sets3.q.out
index b30aabd017..95964155ad 100644
--- a/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_sets3.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_sets3.q.out
@@ -79,7 +79,7 @@ STAGE PLANS:
             Execution mode: llap
             LLAP IO: all inputs
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), count(VALUE._col1)
@@ -149,7 +149,7 @@ STAGE PLANS:
             Execution mode: llap
             LLAP IO: all inputs
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), count(VALUE._col1)
@@ -245,7 +245,7 @@ STAGE PLANS:
             Execution mode: llap
             LLAP IO: all inputs
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), count(VALUE._col1)
@@ -260,7 +260,7 @@ STAGE PLANS:
                   Statistics: Num rows: 48 Data size: 12240 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col3 (type: struct<count:bigint,sum:double,input:string>), _col4 (type: bigint)
         Reducer 3 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), count(VALUE._col1)
diff --git a/ql/src/test/results/clientpositive/llap/vector_groupby_mapjoin.q.out b/ql/src/test/results/clientpositive/llap/vector_groupby_mapjoin.q.out
index bfa87bb06a..16b716c4e5 100644
--- a/ql/src/test/results/clientpositive/llap/vector_groupby_mapjoin.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_groupby_mapjoin.q.out
@@ -131,8 +131,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint, VectorUDAFCount(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1]
                       mode: hash
                       outputColumnNames: _col0, _col1
@@ -148,9 +150,11 @@ STAGE PLANS:
                   Group By Operator
                     Group By Vectorization:
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         keyExpressions: col 0
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: []
                     keys: key (type: string)
                     mode: hash
@@ -218,8 +222,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint, VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
@@ -245,9 +251,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_groupby_reduce.q.out b/ql/src/test/results/clientpositive/llap/vector_groupby_reduce.q.out
index 39e81f3789..56afa70575 100644
--- a/ql/src/test/results/clientpositive/llap/vector_groupby_reduce.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_groupby_reduce.q.out
@@ -11,20 +11,20 @@ PREHOOK: query: create table store_sales_txt
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float                  
+    ss_wholesale_cost         double,
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
-row format delimited fields terminated by '|' 
+row format delimited fields terminated by '|'
 stored as textfile
 PREHOOK: type: CREATETABLE
 PREHOOK: Output: database:default
@@ -42,20 +42,20 @@ POSTHOOK: query: create table store_sales_txt
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float                  
+    ss_wholesale_cost         double,
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
-row format delimited fields terminated by '|' 
+row format delimited fields terminated by '|'
 stored as textfile
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: database:default
@@ -81,18 +81,19 @@ PREHOOK: query: create table store_sales
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float
+    ss_wholesale_cost         double,
+    ss_wholesale_cost_decimal     decimal(38,18),
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
 stored as orc
 tblproperties ("orc.stripe.size"="33554432", "orc.compress.size"="16384")
@@ -112,18 +113,19 @@ POSTHOOK: query: create table store_sales
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float
+    ss_wholesale_cost         double,
+    ss_wholesale_cost_decimal     decimal(38,18),
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
 stored as orc
 tblproperties ("orc.stripe.size"="33554432", "orc.compress.size"="16384")
@@ -144,6 +146,7 @@ ss_sold_date_sk           ,
     ss_ticket_number      ,
     ss_quantity           ,
     ss_wholesale_cost     ,
+    cast(ss_wholesale_cost as decimal(38,18)),
     ss_list_price         ,
     ss_sales_price        ,
     ss_ext_discount_amt   ,
@@ -173,6 +176,7 @@ ss_sold_date_sk           ,
     ss_ticket_number      ,
     ss_quantity           ,
     ss_wholesale_cost     ,
+    cast(ss_wholesale_cost as decimal(38,18)),
     ss_list_price         ,
     ss_sales_price        ,
     ss_ext_discount_amt   ,
@@ -190,27 +194,28 @@ POSTHOOK: Input: default@store_sales_txt
 POSTHOOK: Output: default@store_sales
 POSTHOOK: Lineage: store_sales.ss_addr_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_addr_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_cdemo_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_cdemo_sk, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_coupon_amt SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_coupon_amt, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_coupon_amt SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_coupon_amt, type:double, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_customer_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_customer_sk, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_discount_amt SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_discount_amt, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_list_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_list_price, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_sales_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_sales_price, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_tax SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_tax, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_wholesale_cost SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_wholesale_cost, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_discount_amt SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_discount_amt, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_list_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_list_price, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_sales_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_sales_price, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_tax SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_tax, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_wholesale_cost SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_wholesale_cost, type:double, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_hdemo_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_hdemo_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_item_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_item_sk, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_list_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_list_price, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_net_paid SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_paid, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_net_paid_inc_tax SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_net_profit SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_list_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_list_price, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_net_paid SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_paid, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_net_paid_inc_tax SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_paid_inc_tax, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_net_profit SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_profit, type:double, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_promo_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_promo_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_quantity SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_quantity, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_sales_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_sales_price, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_sales_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_sales_price, type:double, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_sold_date_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_sold_date_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_sold_time_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_sold_time_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_store_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_store_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_ticket_number SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ticket_number, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_wholesale_cost SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_wholesale_cost, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_wholesale_cost SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_wholesale_cost, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_wholesale_cost_decimal EXPRESSION [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_wholesale_cost, type:double, comment:null), ]
 PREHOOK: query: explain vectorization expression
 select 
   ss_ticket_number
@@ -250,10 +255,10 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: store_sales
-                  Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                   TableScan Vectorization:
                       native: true
-                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
                   Select Operator
                     expressions: ss_ticket_number (type: int)
                     outputColumnNames: ss_ticket_number
@@ -261,18 +266,20 @@ STAGE PLANS:
                         className: VectorSelectOperator
                         native: true
                         projectedOutputColumns: [9]
-                    Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 9
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: ss_ticket_number (type: int)
                       mode: hash
                       outputColumnNames: _col0
-                      Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         key expressions: _col0 (type: int)
                         sort order: +
@@ -281,7 +288,7 @@ STAGE PLANS:
                             className: VectorReduceSinkLongOperator
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
-                        Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                        Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                         TopN Hash Memory Usage: 0.1
             Execution mode: vectorized, llap
             LLAP IO: all inputs
@@ -306,14 +313,16 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
                 outputColumnNames: _col0
-                Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   key expressions: _col0 (type: int)
                   sort order: +
@@ -321,7 +330,7 @@ STAGE PLANS:
                       className: VectorReduceSinkObjectHashOperator
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
-                  Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
                   TopN Hash Memory Usage: 0.1
         Reducer 3 
             Execution mode: vectorized, llap
@@ -340,19 +349,19 @@ STAGE PLANS:
                     className: VectorSelectOperator
                     native: true
                     projectedOutputColumns: [0]
-                Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
                 Limit
                   Number of rows: 20
                   Limit Vectorization:
                       className: VectorLimitOperator
                       native: true
-                  Statistics: Num rows: 20 Data size: 1760 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 20 Data size: 4820 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
                     File Sink Vectorization:
                         className: VectorFileSinkOperator
                         native: false
-                    Statistics: Num rows: 20 Data size: 1760 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 20 Data size: 4820 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -449,10 +458,10 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: store_sales
-                  Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                   TableScan Vectorization:
                       native: true
-                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
                   Select Operator
                     expressions: ss_ticket_number (type: int)
                     outputColumnNames: ss_ticket_number
@@ -460,18 +469,20 @@ STAGE PLANS:
                         className: VectorSelectOperator
                         native: true
                         projectedOutputColumns: [9]
-                    Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 9
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: ss_ticket_number (type: int)
                       mode: hash
                       outputColumnNames: _col0
-                      Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         key expressions: _col0 (type: int)
                         sort order: +
@@ -480,7 +491,7 @@ STAGE PLANS:
                             className: VectorReduceSinkLongOperator
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
-                        Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                        Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
             Execution mode: vectorized, llap
             LLAP IO: all inputs
             Map Vectorization:
@@ -504,27 +515,31 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
                 outputColumnNames: _col0
-                Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
                 Group By Operator
                   aggregations: min(_col0)
                   Group By Vectorization:
                       aggregators: VectorUDAFMinLong(col 0) -> int
                       className: VectorGroupByOperator
+                      groupByMode: COMPLETE
                       vectorOutput: true
                       keyExpressions: col 0
                       native: false
+                      vectorProcessingMode: STREAMING
                       projectedOutputColumns: [0]
                   keys: _col0 (type: int)
                   mode: complete
                   outputColumnNames: _col0, _col1
-                  Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
                     expressions: _col1 (type: int)
                     outputColumnNames: _col0
@@ -532,7 +547,7 @@ STAGE PLANS:
                         className: VectorSelectOperator
                         native: true
                         projectedOutputColumns: [1]
-                    Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: _col0 (type: int)
                       sort order: +
@@ -540,7 +555,7 @@ STAGE PLANS:
                           className: VectorReduceSinkObjectHashOperator
                           native: true
                           nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
-                      Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
         Reducer 3 
             Execution mode: vectorized, llap
             Reduce Vectorization:
@@ -558,13 +573,13 @@ STAGE PLANS:
                     className: VectorSelectOperator
                     native: true
                     projectedOutputColumns: [0]
-                Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
                   File Sink Vectorization:
                       className: VectorFileSinkOperator
                       native: false
-                  Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -686,24 +701,26 @@ POSTHOOK: Input: default@store_sales
 82
 PREHOOK: query: explain vectorization expression
 select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number
 PREHOOK: type: QUERY
 POSTHOOK: query: explain vectorization expression
 select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number
@@ -729,41 +746,50 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: store_sales
-                  Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                   TableScan Vectorization:
                       native: true
-                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
-                  Select Operator
-                    expressions: ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int)
-                    outputColumnNames: ss_item_sk, ss_ticket_number, ss_quantity
-                    Select Vectorization:
-                        className: VectorSelectOperator
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
                         native: true
-                        projectedOutputColumns: [2, 9, 10]
-                    Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
-                    Group By Operator
-                      aggregations: min(ss_quantity)
-                      Group By Vectorization:
-                          aggregators: VectorUDAFMinLong(col 10) -> int
-                          className: VectorGroupByOperator
-                          vectorOutput: true
-                          keyExpressions: col 9, col 2
-                          native: false
-                          projectedOutputColumns: [0]
-                      keys: ss_ticket_number (type: int), ss_item_sk (type: int)
-                      mode: hash
-                      outputColumnNames: _col0, _col1, _col2
-                      Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
-                      Reduce Output Operator
-                        key expressions: _col0 (type: int), _col1 (type: int)
-                        sort order: ++
-                        Map-reduce partition columns: _col0 (type: int)
-                        Reduce Sink Vectorization:
-                            className: VectorReduceSinkObjectHashOperator
-                            native: true
-                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
-                        Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col2 (type: int)
+                        predicateExpression: FilterLongColEqualLongScalar(col 9, val 1) -> boolean
+                    predicate: (ss_ticket_number = 1) (type: boolean)
+                    Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ss_item_sk (type: int), ss_quantity (type: int), ss_wholesale_cost_decimal (type: decimal(38,18)), ss_net_profit (type: double)
+                      outputColumnNames: ss_item_sk, ss_quantity, ss_wholesale_cost_decimal, ss_net_profit
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [2, 10, 12, 23]
+                      Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: min(ss_quantity), max(ss_net_profit), max(ss_wholesale_cost_decimal)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFMinLong(col 10) -> int, VectorUDAFMaxDouble(col 23) -> double, VectorUDAFMaxDecimal(col 12) -> decimal(38,18)
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 2
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2]
+                        keys: ss_item_sk (type: int)
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3
+                        Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          key expressions: _col0 (type: int)
+                          sort order: +
+                          Map-reduce partition columns: _col0 (type: int)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkLongOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                          Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col1 (type: int), _col2 (type: double), _col3 (type: decimal(38,18))
             Execution mode: vectorized, llap
             LLAP IO: all inputs
             Map Vectorization:
@@ -785,48 +811,53 @@ STAGE PLANS:
                 vectorized: true
             Reduce Operator Tree:
               Group By Operator
-                aggregations: min(VALUE._col0)
+                aggregations: min(VALUE._col0), max(VALUE._col1), max(VALUE._col2)
                 Group By Vectorization:
-                    aggregators: VectorUDAFMinLong(col 2) -> int
+                    aggregators: VectorUDAFMinLong(col 1) -> int, VectorUDAFMaxDouble(col 2) -> double, VectorUDAFMaxDecimal(col 3) -> decimal(38,18)
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
-                    keyExpressions: col 0, col 1
+                    keyExpressions: col 0
                     native: false
-                    projectedOutputColumns: [0]
-                keys: KEY._col0 (type: int), KEY._col1 (type: int)
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2]
+                keys: KEY._col0 (type: int)
                 mode: mergepartial
-                outputColumnNames: _col0, _col1, _col2
-                Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
-                  expressions: _col1 (type: int), _col0 (type: int), _col2 (type: int)
-                  outputColumnNames: _col0, _col1, _col2
+                  expressions: _col0 (type: int), _col1 (type: int), _col2 (type: double), _col3 (type: decimal(38,18))
+                  outputColumnNames: _col1, _col2, _col3, _col4
                   Select Vectorization:
                       className: VectorSelectOperator
                       native: true
-                      projectedOutputColumns: [1, 0, 2]
-                  Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+                      projectedOutputColumns: [0, 1, 2, 3]
+                  Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
                   Group By Operator
-                    aggregations: sum(_col0), sum(_col2)
+                    aggregations: sum(_col1), sum(_col2), avg(_col2), sum(_col3), avg(_col3), sum(_col4), avg(_col4)
                     Group By Vectorization:
-                        aggregators: VectorUDAFSumLong(col 1) -> bigint, VectorUDAFSumLong(col 2) -> bigint
+                        aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFSumLong(col 1) -> bigint, VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFSumDouble(col 2) -> double, VectorUDAFAvgDouble(col 2) -> struct<count:bigint,sum:double,input:double>, VectorUDAFSumDecimal(col 3) -> decimal(38,18), VectorUDAFAvgDecimal(col 3) -> struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
-                        keyExpressions: col 0
+                        keyExpressions: ConstantVectorExpression(val 1) -> 4:long
                         native: false
-                        projectedOutputColumns: [0, 1]
-                    keys: _col1 (type: int)
-                    mode: complete
-                    outputColumnNames: _col0, _col1, _col2
-                    Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+                        vectorProcessingMode: HASH
+                        projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6]
+                    keys: 1 (type: int)
+                    mode: hash
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+                    Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: _col0 (type: int)
                       sort order: +
+                      Map-reduce partition columns: _col0 (type: int)
                       Reduce Sink Vectorization:
-                          className: VectorReduceSinkObjectHashOperator
+                          className: VectorReduceSinkLongOperator
                           native: true
                           nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
-                      Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-                      value expressions: _col1 (type: bigint), _col2 (type: bigint)
+                      Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: struct<count:bigint,sum:double,input:int>), _col4 (type: double), _col5 (type: struct<count:bigint,sum:double,input:double>), _col6 (type: decimal(38,18)), _col7 (type: struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>)
         Reducer 3 
             Execution mode: vectorized, llap
             Reduce Vectorization:
@@ -837,24 +868,40 @@ STAGE PLANS:
                 usesVectorUDFAdaptor: false
                 vectorized: true
             Reduce Operator Tree:
-              Select Operator
-                expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: bigint), VALUE._col1 (type: bigint)
-                outputColumnNames: _col0, _col1, _col2
-                Select Vectorization:
-                    className: VectorSelectOperator
-                    native: true
-                    projectedOutputColumns: [0, 1, 2]
-                Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  File Sink Vectorization:
-                      className: VectorFileSinkOperator
-                      native: false
-                  Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              Group By Operator
+                aggregations: sum(VALUE._col0), sum(VALUE._col1), avg(VALUE._col2), sum(VALUE._col3), avg(VALUE._col4), sum(VALUE._col5), avg(VALUE._col6)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 1) -> bigint, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFAvgFinal(col 3) -> double, VectorUDAFSumDouble(col 4) -> double, VectorUDAFAvgFinal(col 5) -> double, VectorUDAFSumDecimal(col 6) -> decimal(38,18), VectorUDAFAvgDecimalFinal(col 7) -> decimal(38,18)
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6]
+                keys: KEY._col0 (type: int)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+                Statistics: Num rows: 125 Data size: 30150 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: 1 (type: int), _col1 (type: bigint), _col2 (type: bigint), _col3 (type: double), _col4 (type: double), _col5 (type: double), _col6 (type: decimal(38,18)), _col7 (type: decimal(38,18))
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [8, 1, 2, 3, 4, 5, 6, 7]
+                      selectExpressions: ConstantVectorExpression(val 1) -> 8:long
+                  Statistics: Num rows: 125 Data size: 30150 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 125 Data size: 30150 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
 
   Stage: Stage-0
     Fetch Operator
@@ -863,12 +910,13 @@ STAGE PLANS:
         ListSink
 
 PREHOOK: query: select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number
@@ -876,106 +924,26 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@store_sales
 #### A masked pattern was here ####
 POSTHOOK: query: select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@store_sales
 #### A masked pattern was here ####
-1	85411	816
-2	157365	812
-3	147948	710
-4	69545	411
-5	163232	840
-6	86307	627
-7	114874	563
-8	117953	662
-9	173250	690
-10	60338	602
-11	138545	657
-12	97181	586
-13	109484	555
-14	137333	442
-15	176829	652
-16	115004	654
-17	105008	460
-18	165135	738
-19	128252	831
-20	104789	374
-21	72771	469
-22	128153	449
-23	110253	603
-24	100662	1029
-25	118714	760
-26	81596	502
-27	164068	871
-28	58632	409
-29	133777	417
-30	130451	772
-31	114967	586
-32	142021	592
-33	151818	691
-34	112559	662
-35	137027	780
-36	118285	538
-37	94528	401
-38	81368	521
-39	101064	937
-40	84435	480
-41	112444	688
-42	95731	840
-43	57298	410
-44	159880	839
-45	68919	474
-46	111212	374
-47	78210	416
-48	94459	445
-49	90879	589
-50	37821	407
-51	124927	612
-52	98099	489
-53	138706	609
-54	87478	354
-55	90290	406
-56	78812	372
-57	101175	597
-58	88044	202
-59	104582	753
-60	99218	900
-61	66514	392
-62	126713	527
-63	98778	648
-64	131659	380
-65	86990	494
-66	108808	492
-67	75250	711
-68	91671	548
-69	92821	405
-70	75021	319
-71	124484	748
-72	161470	744
-73	104358	621
-74	88609	688
-75	92940	649
-76	75853	580
-77	124755	873
-78	98285	573
-79	160595	581
-80	151471	704
-81	105109	429
-82	55611	254
+1	85411	816	58.285714285714285	-5080.17	-362.8692857142857	621.350000000000000000	44.382142857142857143
 PREHOOK: query: explain vectorization expression
 select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
@@ -984,10 +952,10 @@ order by ss_ticket_number, ss_item_sk
 PREHOOK: type: QUERY
 POSTHOOK: query: explain vectorization expression
 select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
@@ -1015,31 +983,33 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: store_sales
-                  Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                   TableScan Vectorization:
                       native: true
-                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
                   Select Operator
-                    expressions: ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int)
-                    outputColumnNames: ss_item_sk, ss_ticket_number, ss_quantity
+                    expressions: ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int), ss_wholesale_cost_decimal (type: decimal(38,18)), ss_net_profit (type: double)
+                    outputColumnNames: ss_item_sk, ss_ticket_number, ss_quantity, ss_wholesale_cost_decimal, ss_net_profit
                     Select Vectorization:
                         className: VectorSelectOperator
                         native: true
-                        projectedOutputColumns: [2, 9, 10]
-                    Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                        projectedOutputColumns: [2, 9, 10, 12, 23]
+                    Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
-                      aggregations: min(ss_quantity)
+                      aggregations: min(ss_quantity), max(ss_net_profit), max(ss_wholesale_cost_decimal)
                       Group By Vectorization:
-                          aggregators: VectorUDAFMinLong(col 10) -> int
+                          aggregators: VectorUDAFMinLong(col 10) -> int, VectorUDAFMaxDouble(col 23) -> double, VectorUDAFMaxDecimal(col 12) -> decimal(38,18)
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 9, col 2
                           native: false
-                          projectedOutputColumns: [0]
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0, 1, 2]
                       keys: ss_ticket_number (type: int), ss_item_sk (type: int)
                       mode: hash
-                      outputColumnNames: _col0, _col1, _col2
-                      Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                      Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         key expressions: _col0 (type: int), _col1 (type: int)
                         sort order: ++
@@ -1048,8 +1018,8 @@ STAGE PLANS:
                             className: VectorReduceSinkMultiKeyOperator
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
-                        Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col2 (type: int)
+                        Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col2 (type: int), _col3 (type: double), _col4 (type: decimal(38,18))
             Execution mode: vectorized, llap
             LLAP IO: all inputs
             Map Vectorization:
@@ -1071,48 +1041,60 @@ STAGE PLANS:
                 vectorized: true
             Reduce Operator Tree:
               Group By Operator
-                aggregations: min(VALUE._col0)
+                aggregations: min(VALUE._col0), max(VALUE._col1), max(VALUE._col2)
                 Group By Vectorization:
-                    aggregators: VectorUDAFMinLong(col 2) -> int
+                    aggregators: VectorUDAFMinLong(col 2) -> int, VectorUDAFMaxDouble(col 3) -> double, VectorUDAFMaxDecimal(col 4) -> decimal(38,18)
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1
                     native: false
-                    projectedOutputColumns: [0]
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2]
                 keys: KEY._col0 (type: int), KEY._col1 (type: int)
                 mode: mergepartial
-                outputColumnNames: _col0, _col1, _col2
-                Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
-                  expressions: _col1 (type: int), _col0 (type: int), _col2 (type: int)
-                  outputColumnNames: _col0, _col1, _col2
+                  expressions: _col1 (type: int), _col0 (type: int), _col2 (type: int), _col3 (type: double), _col4 (type: decimal(38,18))
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
                   Select Vectorization:
                       className: VectorSelectOperator
                       native: true
-                      projectedOutputColumns: [1, 0, 2]
-                  Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+                      projectedOutputColumns: [1, 0, 2, 3, 4]
+                  Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
                   Group By Operator
-                    aggregations: sum(_col2)
+                    aggregations: sum(_col2), avg(_col2), sum(_col3), avg(_col3), sum(_col4), avg(_col4)
                     Group By Vectorization:
-                        aggregators: VectorUDAFSumLong(col 2) -> bigint
+                        aggregators: VectorUDAFSumLong(col 2) -> bigint, VectorUDAFAvgLongComplete(col 2) -> double, VectorUDAFSumDouble(col 3) -> double, VectorUDAFAvgDoubleComplete(col 3) -> double, VectorUDAFSumDecimal(col 4) -> decimal(38,18), VectorUDAFAvgDecimalComplete(col 4) -> decimal(38,18)
                         className: VectorGroupByOperator
+                        groupByMode: COMPLETE
                         vectorOutput: true
                         keyExpressions: col 0, col 1
                         native: false
-                        projectedOutputColumns: [0]
+                        vectorProcessingMode: STREAMING
+                        projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                     keys: _col1 (type: int), _col0 (type: int)
                     mode: complete
-                    outputColumnNames: _col0, _col1, _col2
-                    Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-                    Reduce Output Operator
-                      key expressions: _col0 (type: int), _col1 (type: int)
-                      sort order: ++
-                      Reduce Sink Vectorization:
-                          className: VectorReduceSinkObjectHashOperator
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+                    Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: double), _col4 (type: double), _col5 (type: double), _col6 (type: decimal(38,18)), _col7 (type: decimal(38,18))
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+                      Select Vectorization:
+                          className: VectorSelectOperator
                           native: true
-                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
-                      Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-                      value expressions: _col2 (type: bigint)
+                          projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
+                      Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int), _col1 (type: int)
+                        sort order: ++
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col2 (type: bigint), _col3 (type: double), _col4 (type: double), _col5 (type: double), _col6 (type: decimal(38,18)), _col7 (type: decimal(38,18))
         Reducer 3 
             Execution mode: vectorized, llap
             Reduce Vectorization:
@@ -1124,19 +1106,19 @@ STAGE PLANS:
                 vectorized: true
             Reduce Operator Tree:
               Select Operator
-                expressions: KEY.reducesinkkey0 (type: int), KEY.reducesinkkey1 (type: int), VALUE._col0 (type: bigint)
-                outputColumnNames: _col0, _col1, _col2
+                expressions: KEY.reducesinkkey0 (type: int), KEY.reducesinkkey1 (type: int), VALUE._col0 (type: bigint), VALUE._col1 (type: double), VALUE._col2 (type: double), VALUE._col3 (type: double), VALUE._col4 (type: decimal(38,18)), VALUE._col5 (type: decimal(38,18))
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                 Select Vectorization:
                     className: VectorSelectOperator
                     native: true
-                    projectedOutputColumns: [0, 1, 2]
-                Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
+                Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
                   File Sink Vectorization:
                       className: VectorFileSinkOperator
                       native: false
-                  Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -1149,10 +1131,10 @@ STAGE PLANS:
         ListSink
 
 PREHOOK: query: select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(wc), avg(wc), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_wholesale_cost) wc, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
@@ -1162,10 +1144,10 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@store_sales
 #### A masked pattern was here ####
 POSTHOOK: query: select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(wc), avg(wc), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_wholesale_cost) wc, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
@@ -1174,1003 +1156,1003 @@ order by ss_ticket_number, ss_item_sk
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@store_sales
 #### A masked pattern was here ####
-1	49	5
-1	173	65
-1	1553	50
-1	3248	58
-1	3617	79
-1	4553	100
-1	4583	72
-1	4682	44
-1	5527	88
-1	5981	14
-1	10993	91
-1	13283	37
-1	13538	14
-1	13631	99
-2	1363	4
-2	2930	36
-2	3740	49
-2	6928	65
-2	7654	25
-2	9436	79
-2	10768	30
-2	12068	74
-2	12223	78
-2	13340	71
-2	13927	93
-2	14701	58
-2	15085	88
-2	15782	62
-2	17420	NULL
-3	246	96
-3	1531	NULL
-3	3525	42
-3	4698	98
-3	5355	53
-3	10693	27
-3	12447	82
-3	13021	64
-3	14100	79
-3	14443	4
-3	15786	56
-3	16869	4
-3	17263	17
-3	17971	88
-4	163	17
-4	1576	74
-4	5350	86
-4	5515	23
-4	6988	23
-4	7990	56
-4	8452	27
-4	9685	21
-4	11036	41
-4	12790	43
-5	1808	NULL
-5	1940	60
-5	5842	50
-5	6068	76
-5	6466	36
-5	11324	52
-5	11590	15
-5	12650	66
-5	13562	64
-5	13958	60
-5	14599	83
-5	14686	91
-5	15752	66
-5	16195	50
-5	16792	71
-6	2549	62
-6	2647	100
-6	3049	31
-6	3291	100
-6	6437	72
-6	8621	NULL
-6	10355	94
-6	10895	1
-6	11705	61
-6	13245	64
-6	13513	42
-7	4627	9
-7	4795	73
-7	4833	88
-7	5183	51
-7	5905	69
-7	8955	54
-7	9751	4
-7	10487	52
-7	12571	82
-7	15179	12
-7	15333	NULL
-7	17255	69
-8	665	31
-8	4183	90
-8	5929	83
-8	7115	54
-8	11365	7
-8	11893	95
-8	12041	95
-8	13427	87
-8	16671	20
-8	17119	51
-8	17545	49
-9	69	11
-9	889	6
-9	1185	62
-9	4623	34
-9	7945	83
-9	8334	71
-9	12027	27
-9	12969	59
-9	13483	NULL
-9	13717	53
-9	15133	15
-9	16083	32
-9	16363	54
-9	16461	66
-9	16659	84
-9	17310	33
-10	755	74
-10	1425	92
-10	1511	76
-10	3433	83
-10	3933	52
-10	4357	17
-10	5863	47
-10	9811	28
-10	13803	66
-10	15447	67
-11	157	84
-11	1315	70
-11	7519	68
-11	7608	66
-11	9901	57
-11	10699	33
-11	11490	NULL
-11	11991	38
-11	12438	16
-11	15157	96
-11	15649	33
-11	17226	11
-11	17395	85
-12	373	57
-12	1591	82
-12	4888	56
-12	6148	36
-12	6248	36
-12	9616	66
-12	9788	73
-12	13399	46
-12	14746	26
-12	14944	9
-12	15440	99
-13	868	NULL
-13	1760	12
-13	1898	NULL
-13	2108	9
-13	2191	NULL
-13	4430	73
-13	5971	80
-13	6085	58
-13	6140	15
-13	6682	80
-13	7640	48
-13	7723	27
-13	10096	12
-13	11758	34
-13	16894	87
-13	17240	20
-14	177	41
-14	769	20
-14	4507	4
-14	10175	19
-14	11549	6
-14	11653	60
-14	11817	81
-14	12587	NULL
-14	13069	77
-14	13515	57
-14	13845	17
-14	16741	46
-14	16929	14
-15	4241	21
-15	4505	59
-15	4777	28
-15	7391	98
-15	8336	15
-15	8353	NULL
-15	8690	32
-15	8707	21
-15	10361	39
-15	11659	80
-15	13172	25
-15	16619	81
-15	17267	7
-15	17330	82
-15	17564	26
-15	17857	38
-16	457	60
-16	1888	4
-16	4144	94
-16	6008	59
-16	7504	51
-16	8887	35
-16	9769	42
-16	9790	17
-16	9997	94
-16	11168	86
-16	11920	29
-16	16226	13
-16	17246	70
-17	2092	37
-17	4678	34
-17	6811	70
-17	9214	57
-17	10543	54
-17	11203	21
-17	13177	45
-17	13826	32
-17	15781	76
-17	17683	34
-18	2440	40
-18	5251	41
-18	7378	94
-18	8779	9
-18	8884	18
-18	9886	62
-18	11584	76
-18	11890	7
-18	12602	81
-18	12826	93
-18	12860	18
-18	14011	95
-18	14372	76
-18	14377	15
-18	17995	13
-19	1094	48
-19	3133	96
-19	3376	84
-19	4882	84
-19	6772	97
-19	7087	1
-19	7814	29
-19	8662	97
-19	9094	49
-19	9346	39
-19	10558	82
-19	10651	46
-19	11914	59
-19	16330	NULL
-19	17539	20
-20	1451	89
-20	2618	4
-20	5312	9
-20	5425	15
-20	5483	8
-20	6026	21
-20	7207	90
-20	8714	NULL
-20	9086	4
-20	9800	32
-20	13601	17
-20	14935	NULL
-20	15131	85
-21	230	48
-21	1810	59
-21	2870	50
-21	5170	45
-21	5998	51
-21	6476	49
-21	9187	14
-21	12266	47
-21	14368	18
-21	14396	88
-22	9985	70
-22	10474	31
-22	11599	66
-22	12415	10
-22	15310	15
-22	16396	85
-22	16922	88
-22	17392	14
-22	17660	70
-23	319	86
-23	7242	37
-23	8181	13
-23	8413	1
-23	9093	38
-23	9097	81
-23	11220	91
-23	11257	64
-23	12397	80
-23	15403	96
-23	17631	16
-24	407	53
-24	1389	72
-24	1795	21
-24	2497	85
-24	3103	73
-24	4425	57
-24	4749	28
-24	4873	41
-24	5653	92
-24	6043	1
-24	6751	82
-24	7375	97
-24	10265	93
-24	11551	48
-24	13303	97
-24	16483	89
-25	1333	55
-25	2150	100
-25	2608	76
-25	3454	100
-25	4880	29
-25	5954	34
-25	6955	40
-25	7874	65
-25	9472	48
-25	10159	24
-25	14488	26
-25	14635	68
-25	17000	40
-25	17752	55
-26	1989	26
-26	5053	4
-26	5385	97
-26	5721	81
-26	6647	64
-26	7337	45
-26	9679	18
-26	11895	77
-26	12851	56
-26	15039	34
-27	1305	44
-27	2137	96
-27	2671	92
-27	5831	61
-27	7139	59
-27	8167	28
-27	10757	15
-27	11441	15
-27	11509	65
-27	12237	89
-27	12749	31
-27	13885	66
-27	15025	26
-27	16029	59
-27	16419	65
-27	16767	60
-28	1807	98
-28	2817	8
-28	2967	29
-28	4483	78
-28	5437	15
-28	6411	3
-28	7965	93
-28	8043	58
-28	8407	14
-28	10295	13
-29	20	18
-29	1363	75
-29	2930	23
-29	3740	5
-29	7654	20
-29	9458	33
-29	10795	33
-29	12068	37
-29	12223	59
-29	13340	21
-29	13693	NULL
-29	15085	40
-29	15626	NULL
-29	15782	53
-30	217	91
-30	1951	59
-30	3238	16
-30	3506	15
-30	3928	87
-30	5431	77
-30	6752	69
-30	7870	7
-30	8666	21
-30	12572	33
-30	12670	20
-30	13579	75
-30	14848	62
-30	17348	62
-30	17875	78
-31	913	54
-31	4963	67
-31	6617	11
-31	6917	4
-31	7513	82
-31	11739	95
-31	14575	97
-31	14727	41
-31	15341	31
-31	15411	53
-31	16251	51
-32	1115	61
-32	2095	34
-32	2887	8
-32	4339	6
-32	4537	22
-32	4808	NULL
-32	5798	87
-32	7547	24
-32	9683	26
-32	11005	46
-32	11348	41
-32	12134	21
-32	15001	57
-32	15644	34
-32	16421	74
-32	17659	51
-33	4798	27
-33	7300	3
-33	9649	36
-33	10376	21
-33	11119	92
-33	11756	26
-33	12643	89
-33	12760	54
-33	12964	80
-33	14125	66
-33	14158	82
-33	14692	93
-33	15478	22
-34	1526	91
-34	1717	53
-34	2312	6
-34	4118	88
-34	5197	63
-34	5449	9
-34	6193	61
-34	9325	3
-34	9766	83
-34	12016	42
-34	12290	53
-34	12512	60
-34	13814	20
-34	16324	30
-35	411	51
-35	2377	52
-35	3667	97
-35	4325	56
-35	5179	83
-35	11635	87
-35	11661	81
-35	14239	55
-35	15619	45
-35	15757	9
-35	17341	92
-35	17365	65
-35	17451	7
-36	1115	80
-36	2095	43
-36	2887	31
-36	7547	46
-36	11005	49
-36	11349	80
-36	15001	54
-36	15645	23
-36	16421	25
-36	17561	16
-36	17659	91
-37	2997	94
-37	7283	87
-37	10715	52
-37	10929	88
-37	13171	6
-37	15337	62
-37	16971	12
-37	17125	NULL
-38	757	2
-38	2164	17
-38	3439	84
-38	4154	35
-38	5113	73
-38	6220	98
-38	7018	15
-38	7784	56
-38	8870	15
-38	9710	7
-38	10441	62
-38	15698	57
-39	386	89
-39	1598	64
-39	3476	73
-39	3943	64
-39	4190	86
-39	4957	24
-39	5393	98
-39	7097	78
-39	7118	67
-39	7604	49
-39	7697	24
-39	8078	54
-39	8411	96
-39	15491	54
-39	15625	17
-40	2854	71
-40	3490	65
-40	3985	63
-40	5098	35
-40	5318	87
-40	10094	80
-40	10912	23
-40	12050	NULL
-40	13658	53
-40	16976	3
-41	10	50
-41	64	29
-41	3380	88
-41	5566	11
-41	6310	90
-41	7402	69
-41	7603	94
-41	9322	8
-41	10915	81
-41	14788	15
-41	15242	87
-41	15328	46
-41	16514	20
-42	619	69
-42	976	100
-42	1436	94
-42	2314	74
-42	2392	14
-42	2602	30
-42	3346	74
-42	3613	30
-42	6058	30
-42	6134	92
-42	8462	23
-42	9740	52
-42	10016	57
-42	10471	19
-42	12550	41
-42	15002	41
-43	2923	16
-43	3344	22
-43	3911	26
-43	4364	77
-43	4691	41
-43	5773	85
-43	5852	16
-43	11771	30
-43	14669	97
-44	2351	56
-44	2623	18
-44	7303	14
-44	7527	67
-44	9059	68
-44	11707	83
-44	12341	20
-44	13331	98
-44	13449	45
-44	14149	80
-44	15803	81
-44	16491	56
-44	16837	92
-44	16909	61
-45	811	62
-45	1479	49
-45	3265	98
-45	5309	18
-45	7363	87
-45	10115	68
-45	11095	40
-45	13133	46
-45	16349	6
-46	1960	12
-46	3010	67
-46	7040	33
-46	8065	NULL
-46	11426	72
-46	13042	58
-46	15595	32
-46	16540	30
-46	17150	57
-46	17384	13
-47	254	NULL
-47	481	30
-47	1132	66
-47	1916	71
-47	3085	51
-47	3202	7
-47	3878	NULL
-47	4774	11
-47	5008	82
-47	5305	NULL
-47	5468	7
-47	7214	1
-47	9770	33
-47	13246	47
-47	13477	10
-48	1761	22
-48	2820	4
-48	2829	65
-48	4431	39
-48	5971	29
-48	6085	1
-48	6684	44
-48	9199	88
-48	11259	NULL
-48	12468	62
-48	13153	74
-48	17799	17
-49	749	60
-49	2135	4
-49	5342	69
-49	5852	47
-49	6805	40
-49	7141	94
-49	9049	68
-49	9553	71
-49	12737	48
-49	15155	84
-49	16361	4
-50	1280	69
-50	1312	30
-50	1909	53
-50	1984	40
-50	3097	64
-50	5023	NULL
-50	7135	69
-50	16081	82
-51	422	21
-51	3091	28
-51	4687	6
-51	5029	12
-51	5059	51
-51	6565	33
-51	8384	79
-51	9311	90
-51	10133	54
-51	11234	NULL
-51	12625	53
-51	13199	97
-51	17483	22
-51	17705	66
-52	2420	90
-52	3334	73
-52	6098	NULL
-52	7606	45
-52	11488	76
-52	15649	29
-52	16646	48
-52	17402	91
-52	17456	37
-53	1114	40
-53	2095	62
-53	2786	70
-53	2887	39
-53	7546	58
-53	11348	38
-53	13220	76
-53	13795	38
-53	15991	37
-53	16420	14
-53	16648	79
-53	17296	43
-53	17560	15
-54	702	40
-54	825	50
-54	1165	62
-54	3861	NULL
-54	6517	40
-54	9159	75
-54	14737	38
-54	16059	15
-54	16974	NULL
-54	17479	34
-55	1339	16
-55	3001	7
-55	5137	33
-55	9703	44
-55	12170	92
-55	12205	90
-55	14135	36
-55	14923	71
-55	17677	17
-56	4242	2
-56	4506	57
-56	8353	35
-56	8691	59
-56	8707	68
-56	10362	54
-56	16620	23
-56	17331	74
-57	3253	71
-57	4028	88
-57	4933	22
-57	12596	91
-57	12721	62
-57	12740	52
-57	15182	86
-57	17729	26
-57	17993	99
-58	1829	52
-58	3848	6
-58	5117	2
-58	7649	19
-58	9743	62
-58	10802	14
-58	15635	6
-58	16472	6
-58	16949	35
-59	3133	92
-59	3546	22
-59	5772	70
-59	7087	80
-59	8010	46
-59	8335	36
-59	9348	62
-59	9397	92
-59	10651	100
-59	11916	19
-59	12858	90
-59	14529	44
-60	97	50
-60	555	62
-60	633	71
-60	999	43
-60	1117	78
-60	1573	90
-60	4041	25
-60	4235	28
-60	4513	72
-60	4937	22
-60	7231	95
-60	10277	62
-60	10393	75
-60	13975	14
-60	16887	25
-60	17755	88
-61	1106	4
-61	2264	36
-61	3362	48
-61	4567	26
-61	5528	78
-61	6380	77
-61	7591	78
-61	8924	11
-61	10330	8
-61	16462	26
-62	4093	94
-62	6403	NULL
-62	8457	37
-62	10149	75
-62	12163	29
-62	12199	5
-62	12407	NULL
-62	13559	80
-62	15399	74
-62	15733	40
-62	16151	93
-63	4488	73
-63	5079	79
-63	5217	66
-63	5658	99
-63	9319	80
-63	11370	38
-63	11946	85
-63	13339	19
-63	15793	40
-63	16569	69
-64	1213	NULL
-64	3090	87
-64	3963	NULL
-64	11835	82
-64	13224	NULL
-64	14407	8
-64	15867	59
-64	15936	30
-64	16921	19
-64	17586	78
-64	17617	17
-65	2287	100
-65	4227	42
-65	9625	51
-65	9847	54
-65	13897	40
-65	14905	85
-65	15177	55
-65	17025	67
-66	6507	76
-66	7033	65
-66	7227	66
-66	8197	41
-66	9237	29
-66	10019	10
-66	11419	66
-66	15629	20
-66	16745	91
-66	16795	28
-67	757	77
-67	2133	74
-67	3439	73
-67	4155	87
-67	5113	NULL
-67	7020	79
-67	7507	77
-67	8469	59
-67	8871	71
-67	12087	70
-67	15699	44
-68	1387	74
-68	1603	57
-68	1820	54
-68	2035	22
-68	2296	52
-68	2564	83
-68	5162	23
-68	6763	77
-68	7765	NULL
-68	12526	3
-68	12724	88
-68	17426	2
-68	17600	13
-69	322	45
-69	337	34
-69	4208	9
-69	4267	10
-69	6136	7
-69	7264	67
-69	7822	30
-69	8599	53
-69	11137	68
-69	13489	66
-69	13792	NULL
-69	15448	16
-70	1592	53
-70	2462	NULL
-70	3296	48
-70	3947	NULL
-70	6185	82
-70	6425	NULL
-70	8893	17
-70	9857	20
-70	14549	4
-70	17815	95
-71	457	75
-71	1888	4
-71	2098	51
-71	4144	49
-71	5858	NULL
-71	6008	54
-71	7504	3
-71	8887	10
-71	9274	36
-71	9769	79
-71	9790	96
-71	9997	26
-71	10108	66
-71	10288	30
-71	11168	79
-71	17246	90
-72	1535	9
-72	5917	85
-72	6113	45
-72	6671	13
-72	9860	26
-72	10427	66
-72	10753	16
-72	11741	62
-72	12788	29
-72	12901	57
-72	13085	94
-72	13423	62
-72	13904	37
-72	15587	87
-72	16765	56
-73	247	53
-73	1063	37
-73	3205	82
-73	4946	54
-73	6862	58
-73	10051	49
-73	12502	75
-73	15109	38
-73	16519	97
-73	16585	38
-73	17269	40
-74	326	29
-74	3104	78
-74	3175	23
-74	3278	NULL
-74	3542	96
-74	3754	26
-74	5492	54
-74	7694	17
-74	8653	12
-74	9620	95
-74	10069	99
-74	13208	87
-74	16694	72
-75	607	20
-75	2948	25
-75	4625	73
-75	6938	89
-75	6953	71
-75	8726	6
-75	9905	54
-75	10217	85
-75	11039	70
-75	14186	63
-75	16796	93
-76	257	5
-76	465	2
-76	1107	16
-76	1503	97
-76	2265	98
-76	2869	32
-76	3363	25
-76	4237	48
-76	4567	40
-76	5529	78
-76	6381	50
-76	7591	27
-76	8925	6
-76	10331	3
-76	16463	53
-77	992	62
-77	1399	34
-77	2713	85
-77	3868	89
-77	6289	30
-77	7339	88
-77	7448	95
-77	7486	49
-77	8686	38
-77	9220	90
-77	11918	36
-77	12439	95
-77	13456	48
-77	14815	18
-77	16687	16
-78	901	3
-78	3304	50
-78	3856	27
-78	5965	78
-78	6044	59
-78	6110	43
-78	6500	76
-78	7576	87
-78	8611	79
-78	10507	6
-78	11209	7
-78	12706	19
-78	14996	39
-79	247	NULL
-79	1063	85
-79	3205	48
-79	4947	35
-79	6864	1
-79	10051	10
-79	10524	36
-79	12504	81
-79	14322	41
-79	15109	NULL
-79	15498	3
-79	15888	58
-79	16519	9
-79	16585	93
-79	17269	81
-80	998	93
-80	1519	25
-80	1573	40
-80	4040	66
-80	4513	NULL
-80	4622	1
-80	7231	49
-80	7610	37
-80	10393	5
-80	12968	NULL
-80	13717	91
-80	13975	13
-80	16363	84
-80	16886	77
-80	17308	29
-80	17755	94
-81	4486	31
-81	5078	75
-81	5216	64
-81	5656	24
-81	7166	7
-81	7663	79
-81	8918	37
-81	9319	36
-81	11107	36
-81	11368	26
-81	13339	6
-81	15793	8
-82	2572	53
-82	7862	75
-82	13138	59
-82	14998	49
-82	17041	18
+1	49	5	5.0	10.68	10.68	10.680000000000000000	10.680000000000000000
+1	173	65	65.0	27.16	27.16	27.160000000000000000	27.160000000000000000
+1	1553	50	50.0	67.71	67.71	67.710000000000000000	67.710000000000000000
+1	3248	58	58.0	4.57	4.57	4.570000000000000000	4.570000000000000000
+1	3617	79	79.0	11.41	11.41	11.410000000000000000	11.410000000000000000
+1	4553	100	100.0	25.08	25.08	25.080000000000000000	25.080000000000000000
+1	4583	72	72.0	84.72	84.72	84.720000000000000000	84.720000000000000000
+1	4682	44	44.0	31.07	31.07	31.070000000000000000	31.070000000000000000
+1	5527	88	88.0	52.41	52.41	52.410000000000000000	52.410000000000000000
+1	5981	14	14.0	57.37	57.37	57.370000000000000000	57.370000000000000000
+1	10993	91	91.0	93.48	93.48	93.480000000000000000	93.480000000000000000
+1	13283	37	37.0	63.63	63.63	63.630000000000000000	63.630000000000000000
+1	13538	14	14.0	11.54	11.54	11.540000000000000000	11.540000000000000000
+1	13631	99	99.0	80.52	80.52	80.520000000000000000	80.520000000000000000
+2	1363	4	4.0	13.46	13.46	13.460000000000000000	13.460000000000000000
+2	2930	36	36.0	61.23	61.23	61.230000000000000000	61.230000000000000000
+2	3740	49	49.0	6.55	6.55	6.550000000000000000	6.550000000000000000
+2	6928	65	65.0	93.86	93.86	93.860000000000000000	93.860000000000000000
+2	7654	25	25.0	74.26	74.26	74.260000000000000000	74.260000000000000000
+2	9436	79	79.0	88.02	88.02	88.020000000000000000	88.020000000000000000
+2	10768	30	30.0	2.27	2.27	2.270000000000000000	2.270000000000000000
+2	12068	74	74.0	16.55	16.55	16.550000000000000000	16.550000000000000000
+2	12223	78	78.0	65.71	65.71	65.710000000000000000	65.710000000000000000
+2	13340	71	71.0	36.01	36.01	36.010000000000000000	36.010000000000000000
+2	13927	93	93.0	35.87	35.87	35.870000000000000000	35.870000000000000000
+2	14701	58	58.0	53.09	53.09	53.090000000000000000	53.090000000000000000
+2	15085	88	88.0	64.43	64.43	64.430000000000000000	64.430000000000000000
+2	15782	62	62.0	77.97	77.97	77.970000000000000000	77.970000000000000000
+2	17420	NULL	NULL	17.12	17.12	17.120000000000000000	17.120000000000000000
+3	246	96	96.0	98.02	98.02	98.020000000000000000	98.020000000000000000
+3	1531	NULL	NULL	NULL	NULL	NULL	NULL
+3	3525	42	42.0	97.03	97.03	97.030000000000000000	97.030000000000000000
+3	4698	98	98.0	85.0	85.0	85.000000000000000000	85.000000000000000000
+3	5355	53	53.0	23.04	23.04	23.040000000000000000	23.040000000000000000
+3	10693	27	27.0	37.04	37.04	37.040000000000000000	37.040000000000000000
+3	12447	82	82.0	56.14	56.14	56.140000000000000000	56.140000000000000000
+3	13021	64	64.0	74.69	74.69	74.690000000000000000	74.690000000000000000
+3	14100	79	79.0	44.66	44.66	44.660000000000000000	44.660000000000000000
+3	14443	4	4.0	95.75	95.75	95.750000000000000000	95.750000000000000000
+3	15786	56	56.0	4.31	4.31	4.310000000000000000	4.310000000000000000
+3	16869	4	4.0	75.67	75.67	75.670000000000000000	75.670000000000000000
+3	17263	17	17.0	72.38	72.38	72.380000000000000000	72.380000000000000000
+3	17971	88	88.0	27.95	27.95	27.950000000000000000	27.950000000000000000
+4	163	17	17.0	54.26	54.26	54.260000000000000000	54.260000000000000000
+4	1576	74	74.0	81.81	81.81	81.810000000000000000	81.810000000000000000
+4	5350	86	86.0	64.67	64.67	64.670000000000000000	64.670000000000000000
+4	5515	23	23.0	2.91	2.91	2.910000000000000000	2.910000000000000000
+4	6988	23	23.0	53.28	53.28	53.280000000000000000	53.280000000000000000
+4	7990	56	56.0	64.68	64.68	64.680000000000000000	64.680000000000000000
+4	8452	27	27.0	26.21	26.21	26.210000000000000000	26.210000000000000000
+4	9685	21	21.0	40.39	40.39	40.390000000000000000	40.390000000000000000
+4	11036	41	41.0	67.18	67.18	67.180000000000000000	67.180000000000000000
+4	12790	43	43.0	54.34	54.34	54.340000000000000000	54.340000000000000000
+5	1808	NULL	NULL	NULL	NULL	NULL	NULL
+5	1940	60	60.0	69.54	69.54	69.540000000000000000	69.540000000000000000
+5	5842	50	50.0	30.69	30.69	30.690000000000000000	30.690000000000000000
+5	6068	76	76.0	89.78	89.78	89.780000000000000000	89.780000000000000000
+5	6466	36	36.0	7.93	7.93	7.930000000000000000	7.930000000000000000
+5	11324	52	52.0	16.33	16.33	16.330000000000000000	16.330000000000000000
+5	11590	15	15.0	21.21	21.21	21.210000000000000000	21.210000000000000000
+5	12650	66	66.0	21.01	21.01	21.010000000000000000	21.010000000000000000
+5	13562	64	64.0	87.9	87.9	87.900000000000000000	87.900000000000000000
+5	13958	60	60.0	41.72	41.72	41.720000000000000000	41.720000000000000000
+5	14599	83	83.0	74.15	74.15	74.150000000000000000	74.150000000000000000
+5	14686	91	91.0	27.68	27.68	27.680000000000000000	27.680000000000000000
+5	15752	66	66.0	71.06	71.06	71.060000000000000000	71.060000000000000000
+5	16195	50	50.0	30.96	30.96	30.960000000000000000	30.960000000000000000
+5	16792	71	71.0	22.1	22.1	22.100000000000000000	22.100000000000000000
+6	2549	62	62.0	85.07	85.07	85.070000000000000000	85.070000000000000000
+6	2647	100	100.0	4.45	4.45	4.450000000000000000	4.450000000000000000
+6	3049	31	31.0	49.78	49.78	49.780000000000000000	49.780000000000000000
+6	3291	100	100.0	41.08	41.08	41.080000000000000000	41.080000000000000000
+6	6437	72	72.0	55.49	55.49	55.490000000000000000	55.490000000000000000
+6	8621	NULL	NULL	NULL	NULL	NULL	NULL
+6	10355	94	94.0	62.67	62.67	62.670000000000000000	62.670000000000000000
+6	10895	1	1.0	71.1	71.1	71.100000000000000000	71.100000000000000000
+6	11705	61	61.0	48.18	48.18	48.180000000000000000	48.180000000000000000
+6	13245	64	64.0	86.35	86.35	86.350000000000000000	86.350000000000000000
+6	13513	42	42.0	64.46	64.46	64.460000000000000000	64.460000000000000000
+7	4627	9	9.0	56.13	56.13	56.130000000000000000	56.130000000000000000
+7	4795	73	73.0	12.17	12.17	12.170000000000000000	12.170000000000000000
+7	4833	88	88.0	38.23	38.23	38.230000000000000000	38.230000000000000000
+7	5183	51	51.0	84.65	84.65	84.650000000000000000	84.650000000000000000
+7	5905	69	69.0	99.85	99.85	99.850000000000000000	99.850000000000000000
+7	8955	54	54.0	42.82	42.82	42.820000000000000000	42.820000000000000000
+7	9751	4	4.0	NULL	NULL	NULL	NULL
+7	10487	52	52.0	63.8	63.8	63.800000000000000000	63.800000000000000000
+7	12571	82	82.0	69.53	69.53	69.530000000000000000	69.530000000000000000
+7	15179	12	12.0	47.6	47.6	47.600000000000000000	47.600000000000000000
+7	15333	NULL	NULL	NULL	NULL	NULL	NULL
+7	17255	69	69.0	34.19	34.19	34.190000000000000000	34.190000000000000000
+8	665	31	31.0	15.64	15.64	15.640000000000000000	15.640000000000000000
+8	4183	90	90.0	81.63	81.63	81.630000000000000000	81.630000000000000000
+8	5929	83	83.0	14.11	14.11	14.110000000000000000	14.110000000000000000
+8	7115	54	54.0	36.99	36.99	36.990000000000000000	36.990000000000000000
+8	11365	7	7.0	18.65	18.65	18.650000000000000000	18.650000000000000000
+8	11893	95	95.0	21.29	21.29	21.290000000000000000	21.290000000000000000
+8	12041	95	95.0	91.8	91.8	91.800000000000000000	91.800000000000000000
+8	13427	87	87.0	31.78	31.78	31.780000000000000000	31.780000000000000000
+8	16671	20	20.0	18.95	18.95	18.950000000000000000	18.950000000000000000
+8	17119	51	51.0	8.04	8.04	8.040000000000000000	8.040000000000000000
+8	17545	49	49.0	72.15	72.15	72.150000000000000000	72.150000000000000000
+9	69	11	11.0	31.7	31.7	31.700000000000000000	31.700000000000000000
+9	889	6	6.0	27.17	27.17	27.170000000000000000	27.170000000000000000
+9	1185	62	62.0	55.68	55.68	55.680000000000000000	55.680000000000000000
+9	4623	34	34.0	2.97	2.97	2.970000000000000000	2.970000000000000000
+9	7945	83	83.0	8.1	8.1	8.100000000000000000	8.100000000000000000
+9	8334	71	71.0	34.79	34.79	34.790000000000000000	34.790000000000000000
+9	12027	27	27.0	98.68	98.68	98.680000000000000000	98.680000000000000000
+9	12969	59	59.0	88.31	88.31	88.310000000000000000	88.310000000000000000
+9	13483	NULL	NULL	59.14	59.14	59.140000000000000000	59.140000000000000000
+9	13717	53	53.0	75.37	75.37	75.370000000000000000	75.370000000000000000
+9	15133	15	15.0	35.89	35.89	35.890000000000000000	35.890000000000000000
+9	16083	32	32.0	99.1	99.1	99.100000000000000000	99.100000000000000000
+9	16363	54	54.0	NULL	NULL	NULL	NULL
+9	16461	66	66.0	15.21	15.21	15.210000000000000000	15.210000000000000000
+9	16659	84	84.0	76.71	76.71	76.710000000000000000	76.710000000000000000
+9	17310	33	33.0	27.13	27.13	27.130000000000000000	27.130000000000000000
+10	755	74	74.0	82.24	82.24	82.240000000000000000	82.240000000000000000
+10	1425	92	92.0	NULL	NULL	NULL	NULL
+10	1511	76	76.0	31.47	31.47	31.470000000000000000	31.470000000000000000
+10	3433	83	83.0	10.26	10.26	10.260000000000000000	10.260000000000000000
+10	3933	52	52.0	52.19	52.19	52.190000000000000000	52.190000000000000000
+10	4357	17	17.0	88.36	88.36	88.360000000000000000	88.360000000000000000
+10	5863	47	47.0	11.71	11.71	11.710000000000000000	11.710000000000000000
+10	9811	28	28.0	47.85	47.85	47.850000000000000000	47.850000000000000000
+10	13803	66	66.0	82.35	82.35	82.350000000000000000	82.350000000000000000
+10	15447	67	67.0	33.28	33.28	33.280000000000000000	33.280000000000000000
+11	157	84	84.0	64.63	64.63	64.630000000000000000	64.630000000000000000
+11	1315	70	70.0	45.84	45.84	45.840000000000000000	45.840000000000000000
+11	7519	68	68.0	7.16	7.16	7.160000000000000000	7.160000000000000000
+11	7608	66	66.0	8.34	8.34	8.340000000000000000	8.340000000000000000
+11	9901	57	57.0	46.93	46.93	46.930000000000000000	46.930000000000000000
+11	10699	33	33.0	73.77	73.77	73.770000000000000000	73.770000000000000000
+11	11490	NULL	NULL	NULL	NULL	NULL	NULL
+11	11991	38	38.0	3.27	3.27	3.270000000000000000	3.270000000000000000
+11	12438	16	16.0	92.94	92.94	92.940000000000000000	92.940000000000000000
+11	15157	96	96.0	15.52	15.52	15.520000000000000000	15.520000000000000000
+11	15649	33	33.0	66.11	66.11	66.110000000000000000	66.110000000000000000
+11	17226	11	11.0	34.03	34.03	34.030000000000000000	34.030000000000000000
+11	17395	85	85.0	38.04	38.04	38.040000000000000000	38.040000000000000000
+12	373	57	57.0	13.95	13.95	13.950000000000000000	13.950000000000000000
+12	1591	82	82.0	45.84	45.84	45.840000000000000000	45.840000000000000000
+12	4888	56	56.0	75.74	75.74	75.740000000000000000	75.740000000000000000
+12	6148	36	36.0	97.62	97.62	97.620000000000000000	97.620000000000000000
+12	6248	36	36.0	75.17	75.17	75.170000000000000000	75.170000000000000000
+12	9616	66	66.0	99.06	99.06	99.060000000000000000	99.060000000000000000
+12	9788	73	73.0	79.42	79.42	79.420000000000000000	79.420000000000000000
+12	13399	46	46.0	45.27	45.27	45.270000000000000000	45.270000000000000000
+12	14746	26	26.0	58.74	58.74	58.740000000000000000	58.740000000000000000
+12	14944	9	9.0	7.33	7.33	7.330000000000000000	7.330000000000000000
+12	15440	99	99.0	27.09	27.09	27.090000000000000000	27.090000000000000000
+13	868	NULL	NULL	62.85	62.85	62.850000000000000000	62.850000000000000000
+13	1760	12	12.0	80.96	80.96	80.960000000000000000	80.960000000000000000
+13	1898	NULL	NULL	96.46	96.46	96.460000000000000000	96.460000000000000000
+13	2108	9	9.0	NULL	NULL	NULL	NULL
+13	2191	NULL	NULL	NULL	NULL	NULL	NULL
+13	4430	73	73.0	5.86	5.86	5.860000000000000000	5.860000000000000000
+13	5971	80	80.0	72.61	72.61	72.610000000000000000	72.610000000000000000
+13	6085	58	58.0	21.45	21.45	21.450000000000000000	21.450000000000000000
+13	6140	15	15.0	89.9	89.9	89.900000000000000000	89.900000000000000000
+13	6682	80	80.0	32.05	32.05	32.050000000000000000	32.050000000000000000
+13	7640	48	48.0	17.06	17.06	17.060000000000000000	17.060000000000000000
+13	7723	27	27.0	59.09	59.09	59.090000000000000000	59.090000000000000000
+13	10096	12	12.0	17.14	17.14	17.140000000000000000	17.140000000000000000
+13	11758	34	34.0	72.24	72.24	72.240000000000000000	72.240000000000000000
+13	16894	87	87.0	20.99	20.99	20.990000000000000000	20.990000000000000000
+13	17240	20	20.0	93.85	93.85	93.850000000000000000	93.850000000000000000
+14	177	41	41.0	13.05	13.05	13.050000000000000000	13.050000000000000000
+14	769	20	20.0	26.29	26.29	26.290000000000000000	26.290000000000000000
+14	4507	4	4.0	45.45	45.45	45.450000000000000000	45.450000000000000000
+14	10175	19	19.0	39.97	39.97	39.970000000000000000	39.970000000000000000
+14	11549	6	6.0	19.33	19.33	19.330000000000000000	19.330000000000000000
+14	11653	60	60.0	86.94	86.94	86.940000000000000000	86.940000000000000000
+14	11817	81	81.0	60.77	60.77	60.770000000000000000	60.770000000000000000
+14	12587	NULL	NULL	NULL	NULL	NULL	NULL
+14	13069	77	77.0	93.6	93.6	93.600000000000000000	93.600000000000000000
+14	13515	57	57.0	87.32	87.32	87.320000000000000000	87.320000000000000000
+14	13845	17	17.0	52.3	52.3	52.300000000000000000	52.300000000000000000
+14	16741	46	46.0	76.43	76.43	76.430000000000000000	76.430000000000000000
+14	16929	14	14.0	54.76	54.76	54.760000000000000000	54.760000000000000000
+15	4241	21	21.0	89.07	89.07	89.070000000000000000	89.070000000000000000
+15	4505	59	59.0	77.35	77.35	77.350000000000000000	77.350000000000000000
+15	4777	28	28.0	36.86	36.86	36.860000000000000000	36.860000000000000000
+15	7391	98	98.0	53.76	53.76	53.760000000000000000	53.760000000000000000
+15	8336	15	15.0	44.09	44.09	44.090000000000000000	44.090000000000000000
+15	8353	NULL	NULL	NULL	NULL	NULL	NULL
+15	8690	32	32.0	67.37	67.37	67.370000000000000000	67.370000000000000000
+15	8707	21	21.0	48.54	48.54	48.540000000000000000	48.540000000000000000
+15	10361	39	39.0	74.88	74.88	74.880000000000000000	74.880000000000000000
+15	11659	80	80.0	86.23	86.23	86.230000000000000000	86.230000000000000000
+15	13172	25	25.0	47.11	47.11	47.110000000000000000	47.110000000000000000
+15	16619	81	81.0	80.21	80.21	80.210000000000000000	80.210000000000000000
+15	17267	7	7.0	30.61	30.61	30.610000000000000000	30.610000000000000000
+15	17330	82	82.0	67.45	67.45	67.450000000000000000	67.450000000000000000
+15	17564	26	26.0	63.52	63.52	63.520000000000000000	63.520000000000000000
+15	17857	38	38.0	96.35	96.35	96.350000000000000000	96.350000000000000000
+16	457	60	60.0	91.53	91.53	91.530000000000000000	91.530000000000000000
+16	1888	4	4.0	47.64	47.64	47.640000000000000000	47.640000000000000000
+16	4144	94	94.0	19.91	19.91	19.910000000000000000	19.910000000000000000
+16	6008	59	59.0	59.62	59.62	59.620000000000000000	59.620000000000000000
+16	7504	51	51.0	31.35	31.35	31.350000000000000000	31.350000000000000000
+16	8887	35	35.0	59.82	59.82	59.820000000000000000	59.820000000000000000
+16	9769	42	42.0	29.53	29.53	29.530000000000000000	29.530000000000000000
+16	9790	17	17.0	36.95	36.95	36.950000000000000000	36.950000000000000000
+16	9997	94	94.0	64.76	64.76	64.760000000000000000	64.760000000000000000
+16	11168	86	86.0	62.85	62.85	62.850000000000000000	62.850000000000000000
+16	11920	29	29.0	94.31	94.31	94.310000000000000000	94.310000000000000000
+16	16226	13	13.0	31.3	31.3	31.300000000000000000	31.300000000000000000
+16	17246	70	70.0	80.85	80.85	80.850000000000000000	80.850000000000000000
+17	2092	37	37.0	31.71	31.71	31.710000000000000000	31.710000000000000000
+17	4678	34	34.0	32.47	32.47	32.470000000000000000	32.470000000000000000
+17	6811	70	70.0	62.96	62.96	62.960000000000000000	62.960000000000000000
+17	9214	57	57.0	14.2	14.2	14.200000000000000000	14.200000000000000000
+17	10543	54	54.0	57.11	57.11	57.110000000000000000	57.110000000000000000
+17	11203	21	21.0	93.44	93.44	93.440000000000000000	93.440000000000000000
+17	13177	45	45.0	44.18	44.18	44.180000000000000000	44.180000000000000000
+17	13826	32	32.0	58.61	58.61	58.610000000000000000	58.610000000000000000
+17	15781	76	76.0	24.79	24.79	24.790000000000000000	24.790000000000000000
+17	17683	34	34.0	81.48	81.48	81.480000000000000000	81.480000000000000000
+18	2440	40	40.0	15.39	15.39	15.390000000000000000	15.390000000000000000
+18	5251	41	41.0	45.83	45.83	45.830000000000000000	45.830000000000000000
+18	7378	94	94.0	61.01	61.01	61.010000000000000000	61.010000000000000000
+18	8779	9	9.0	75.19	75.19	75.190000000000000000	75.190000000000000000
+18	8884	18	18.0	43.49	43.49	43.490000000000000000	43.490000000000000000
+18	9886	62	62.0	9.59	9.59	9.590000000000000000	9.590000000000000000
+18	11584	76	76.0	4.26	4.26	4.260000000000000000	4.260000000000000000
+18	11890	7	7.0	82.36	82.36	82.360000000000000000	82.360000000000000000
+18	12602	81	81.0	11.32	11.32	11.320000000000000000	11.320000000000000000
+18	12826	93	93.0	82.82	82.82	82.820000000000000000	82.820000000000000000
+18	12860	18	18.0	19.89	19.89	19.890000000000000000	19.890000000000000000
+18	14011	95	95.0	55.01	55.01	55.010000000000000000	55.010000000000000000
+18	14372	76	76.0	89.58	89.58	89.580000000000000000	89.580000000000000000
+18	14377	15	15.0	15.47	15.47	15.470000000000000000	15.470000000000000000
+18	17995	13	13.0	46.79	46.79	46.790000000000000000	46.790000000000000000
+19	1094	48	48.0	19.55	19.55	19.550000000000000000	19.550000000000000000
+19	3133	96	96.0	68.89	68.89	68.890000000000000000	68.890000000000000000
+19	3376	84	84.0	63.07	63.07	63.070000000000000000	63.070000000000000000
+19	4882	84	84.0	41.48	41.48	41.480000000000000000	41.480000000000000000
+19	6772	97	97.0	36.04	36.04	36.040000000000000000	36.040000000000000000
+19	7087	1	1.0	48.67	48.67	48.670000000000000000	48.670000000000000000
+19	7814	29	29.0	61.78	61.78	61.780000000000000000	61.780000000000000000
+19	8662	97	97.0	72.78	72.78	72.780000000000000000	72.780000000000000000
+19	9094	49	49.0	61.82	61.82	61.820000000000000000	61.820000000000000000
+19	9346	39	39.0	84.06	84.06	84.060000000000000000	84.060000000000000000
+19	10558	82	82.0	12.34	12.34	12.340000000000000000	12.340000000000000000
+19	10651	46	46.0	57.69	57.69	57.690000000000000000	57.690000000000000000
+19	11914	59	59.0	88.03	88.03	88.030000000000000000	88.030000000000000000
+19	16330	NULL	NULL	79.15	79.15	79.150000000000000000	79.150000000000000000
+19	17539	20	20.0	69.2	69.2	69.200000000000000000	69.200000000000000000
+20	1451	89	89.0	84.34	84.34	84.340000000000000000	84.340000000000000000
+20	2618	4	4.0	69.47	69.47	69.470000000000000000	69.470000000000000000
+20	5312	9	9.0	29.45	29.45	29.450000000000000000	29.450000000000000000
+20	5425	15	15.0	28.19	28.19	28.190000000000000000	28.190000000000000000
+20	5483	8	8.0	30.74	30.74	30.740000000000000000	30.740000000000000000
+20	6026	21	21.0	80.56	80.56	80.560000000000000000	80.560000000000000000
+20	7207	90	90.0	83.12	83.12	83.120000000000000000	83.120000000000000000
+20	8714	NULL	NULL	8.15	8.15	8.150000000000000000	8.150000000000000000
+20	9086	4	4.0	98.99	98.99	98.990000000000000000	98.990000000000000000
+20	9800	32	32.0	18.09	18.09	18.090000000000000000	18.090000000000000000
+20	13601	17	17.0	1.4	1.4	1.400000000000000000	1.400000000000000000
+20	14935	NULL	NULL	NULL	NULL	NULL	NULL
+20	15131	85	85.0	42.56	42.56	42.560000000000000000	42.560000000000000000
+21	230	48	48.0	13.37	13.37	13.370000000000000000	13.370000000000000000
+21	1810	59	59.0	66.37	66.37	66.370000000000000000	66.370000000000000000
+21	2870	50	50.0	91.94	91.94	91.940000000000000000	91.940000000000000000
+21	5170	45	45.0	90.0	90.0	90.000000000000000000	90.000000000000000000
+21	5998	51	51.0	9.41	9.41	9.410000000000000000	9.410000000000000000
+21	6476	49	49.0	20.29	20.29	20.290000000000000000	20.290000000000000000
+21	9187	14	14.0	35.49	35.49	35.490000000000000000	35.490000000000000000
+21	12266	47	47.0	11.55	11.55	11.550000000000000000	11.550000000000000000
+21	14368	18	18.0	51.29	51.29	51.290000000000000000	51.290000000000000000
+21	14396	88	88.0	45.26	45.26	45.260000000000000000	45.260000000000000000
+22	9985	70	70.0	21.46	21.46	21.460000000000000000	21.460000000000000000
+22	10474	31	31.0	45.65	45.65	45.650000000000000000	45.650000000000000000
+22	11599	66	66.0	5.01	5.01	5.010000000000000000	5.010000000000000000
+22	12415	10	10.0	38.97	38.97	38.970000000000000000	38.970000000000000000
+22	15310	15	15.0	82.24	82.24	82.240000000000000000	82.240000000000000000
+22	16396	85	85.0	86.46	86.46	86.460000000000000000	86.460000000000000000
+22	16922	88	88.0	28.0	28.0	28.000000000000000000	28.000000000000000000
+22	17392	14	14.0	51.86	51.86	51.860000000000000000	51.860000000000000000
+22	17660	70	70.0	95.56	95.56	95.560000000000000000	95.560000000000000000
+23	319	86	86.0	66.36	66.36	66.360000000000000000	66.360000000000000000
+23	7242	37	37.0	54.82	54.82	54.820000000000000000	54.820000000000000000
+23	8181	13	13.0	4.63	4.63	4.630000000000000000	4.630000000000000000
+23	8413	1	1.0	14.2	14.2	14.200000000000000000	14.200000000000000000
+23	9093	38	38.0	80.2	80.2	80.200000000000000000	80.200000000000000000
+23	9097	81	81.0	72.51	72.51	72.510000000000000000	72.510000000000000000
+23	11220	91	91.0	71.3	71.3	71.300000000000000000	71.300000000000000000
+23	11257	64	64.0	29.95	29.95	29.950000000000000000	29.950000000000000000
+23	12397	80	80.0	78.73	78.73	78.730000000000000000	78.730000000000000000
+23	15403	96	96.0	51.96	51.96	51.960000000000000000	51.960000000000000000
+23	17631	16	16.0	22.06	22.06	22.060000000000000000	22.060000000000000000
+24	407	53	53.0	98.05	98.05	98.050000000000000000	98.050000000000000000
+24	1389	72	72.0	60.01	60.01	60.010000000000000000	60.010000000000000000
+24	1795	21	21.0	76.67	76.67	76.670000000000000000	76.670000000000000000
+24	2497	85	85.0	57.93	57.93	57.930000000000000000	57.930000000000000000
+24	3103	73	73.0	44.96	44.96	44.960000000000000000	44.960000000000000000
+24	4425	57	57.0	29.31	29.31	29.310000000000000000	29.310000000000000000
+24	4749	28	28.0	18.17	18.17	18.170000000000000000	18.170000000000000000
+24	4873	41	41.0	40.34	40.34	40.340000000000000000	40.340000000000000000
+24	5653	92	92.0	64.99	64.99	64.990000000000000000	64.990000000000000000
+24	6043	1	1.0	33.41	33.41	33.410000000000000000	33.410000000000000000
+24	6751	82	82.0	7.48	7.48	7.480000000000000000	7.480000000000000000
+24	7375	97	97.0	78.55	78.55	78.550000000000000000	78.550000000000000000
+24	10265	93	93.0	12.03	12.03	12.030000000000000000	12.030000000000000000
+24	11551	48	48.0	30.8	30.8	30.800000000000000000	30.800000000000000000
+24	13303	97	97.0	94.48	94.48	94.480000000000000000	94.480000000000000000
+24	16483	89	89.0	13.84	13.84	13.840000000000000000	13.840000000000000000
+25	1333	55	55.0	30.82	30.82	30.820000000000000000	30.820000000000000000
+25	2150	100	100.0	67.24	67.24	67.240000000000000000	67.240000000000000000
+25	2608	76	76.0	87.75	87.75	87.750000000000000000	87.750000000000000000
+25	3454	100	100.0	1.61	1.61	1.610000000000000000	1.610000000000000000
+25	4880	29	29.0	15.35	15.35	15.350000000000000000	15.350000000000000000
+25	5954	34	34.0	76.57	76.57	76.570000000000000000	76.570000000000000000
+25	6955	40	40.0	87.12	87.12	87.120000000000000000	87.120000000000000000
+25	7874	65	65.0	2.75	2.75	2.750000000000000000	2.750000000000000000
+25	9472	48	48.0	4.97	4.97	4.970000000000000000	4.970000000000000000
+25	10159	24	24.0	76.64	76.64	76.640000000000000000	76.640000000000000000
+25	14488	26	26.0	68.17	68.17	68.170000000000000000	68.170000000000000000
+25	14635	68	68.0	45.79	45.79	45.790000000000000000	45.790000000000000000
+25	17000	40	40.0	89.34	89.34	89.340000000000000000	89.340000000000000000
+25	17752	55	55.0	11.49	11.49	11.490000000000000000	11.490000000000000000
+26	1989	26	26.0	83.31	83.31	83.310000000000000000	83.310000000000000000
+26	5053	4	4.0	19.63	19.63	19.630000000000000000	19.630000000000000000
+26	5385	97	97.0	51.89	51.89	51.890000000000000000	51.890000000000000000
+26	5721	81	81.0	74.96	74.96	74.960000000000000000	74.960000000000000000
+26	6647	64	64.0	57.04	57.04	57.040000000000000000	57.040000000000000000
+26	7337	45	45.0	37.59	37.59	37.590000000000000000	37.590000000000000000
+26	9679	18	18.0	77.54	77.54	77.540000000000000000	77.540000000000000000
+26	11895	77	77.0	36.85	36.85	36.850000000000000000	36.850000000000000000
+26	12851	56	56.0	14.02	14.02	14.020000000000000000	14.020000000000000000
+26	15039	34	34.0	22.65	22.65	22.650000000000000000	22.650000000000000000
+27	1305	44	44.0	8.35	8.35	8.350000000000000000	8.350000000000000000
+27	2137	96	96.0	3.07	3.07	3.070000000000000000	3.070000000000000000
+27	2671	92	92.0	4.35	4.35	4.350000000000000000	4.350000000000000000
+27	5831	61	61.0	8.79	8.79	8.790000000000000000	8.790000000000000000
+27	7139	59	59.0	6.17	6.17	6.170000000000000000	6.170000000000000000
+27	8167	28	28.0	38.83	38.83	38.830000000000000000	38.830000000000000000
+27	10757	15	15.0	8.7	8.7	8.700000000000000000	8.700000000000000000
+27	11441	15	15.0	14.45	14.45	14.450000000000000000	14.450000000000000000
+27	11509	65	65.0	80.34	80.34	80.340000000000000000	80.340000000000000000
+27	12237	89	89.0	73.9	73.9	73.900000000000000000	73.900000000000000000
+27	12749	31	31.0	80.27	80.27	80.270000000000000000	80.270000000000000000
+27	13885	66	66.0	40.62	40.62	40.620000000000000000	40.620000000000000000
+27	15025	26	26.0	35.56	35.56	35.560000000000000000	35.560000000000000000
+27	16029	59	59.0	2.11	2.11	2.110000000000000000	2.110000000000000000
+27	16419	65	65.0	80.1	80.1	80.100000000000000000	80.100000000000000000
+27	16767	60	60.0	68.33	68.33	68.330000000000000000	68.330000000000000000
+28	1807	98	98.0	78.91	78.91	78.910000000000000000	78.910000000000000000
+28	2817	8	8.0	98.75	98.75	98.750000000000000000	98.750000000000000000
+28	2967	29	29.0	47.87	47.87	47.870000000000000000	47.870000000000000000
+28	4483	78	78.0	73.9	73.9	73.900000000000000000	73.900000000000000000
+28	5437	15	15.0	7.49	7.49	7.490000000000000000	7.490000000000000000
+28	6411	3	3.0	67.26	67.26	67.260000000000000000	67.260000000000000000
+28	7965	93	93.0	77.74	77.74	77.740000000000000000	77.740000000000000000
+28	8043	58	58.0	60.26	60.26	60.260000000000000000	60.260000000000000000
+28	8407	14	14.0	95.01	95.01	95.010000000000000000	95.010000000000000000
+28	10295	13	13.0	31.83	31.83	31.830000000000000000	31.830000000000000000
+29	20	18	18.0	66.26	66.26	66.260000000000000000	66.260000000000000000
+29	1363	75	75.0	NULL	NULL	NULL	NULL
+29	2930	23	23.0	64.78	64.78	64.780000000000000000	64.780000000000000000
+29	3740	5	5.0	90.13	90.13	90.130000000000000000	90.130000000000000000
+29	7654	20	20.0	98.14	98.14	98.140000000000000000	98.140000000000000000
+29	9458	33	33.0	52.33	52.33	52.330000000000000000	52.330000000000000000
+29	10795	33	33.0	68.24	68.24	68.240000000000000000	68.240000000000000000
+29	12068	37	37.0	80.75	80.75	80.750000000000000000	80.750000000000000000
+29	12223	59	59.0	12.89	12.89	12.890000000000000000	12.890000000000000000
+29	13340	21	21.0	40.5	40.5	40.500000000000000000	40.500000000000000000
+29	13693	NULL	NULL	95.63	95.63	95.630000000000000000	95.630000000000000000
+29	15085	40	40.0	NULL	NULL	NULL	NULL
+29	15626	NULL	NULL	17.61	17.61	17.610000000000000000	17.610000000000000000
+29	15782	53	53.0	57.11	57.11	57.110000000000000000	57.110000000000000000
+30	217	91	91.0	52.03	52.03	52.030000000000000000	52.030000000000000000
+30	1951	59	59.0	17.14	17.14	17.140000000000000000	17.140000000000000000
+30	3238	16	16.0	9.84	9.84	9.840000000000000000	9.840000000000000000
+30	3506	15	15.0	16.31	16.31	16.310000000000000000	16.310000000000000000
+30	3928	87	87.0	27.01	27.01	27.010000000000000000	27.010000000000000000
+30	5431	77	77.0	52.37	52.37	52.370000000000000000	52.370000000000000000
+30	6752	69	69.0	40.8	40.8	40.800000000000000000	40.800000000000000000
+30	7870	7	7.0	4.51	4.51	4.510000000000000000	4.510000000000000000
+30	8666	21	21.0	64.0	64.0	64.000000000000000000	64.000000000000000000
+30	12572	33	33.0	61.96	61.96	61.960000000000000000	61.960000000000000000
+30	12670	20	20.0	6.44	6.44	6.440000000000000000	6.440000000000000000
+30	13579	75	75.0	62.71	62.71	62.710000000000000000	62.710000000000000000
+30	14848	62	62.0	64.03	64.03	64.030000000000000000	64.030000000000000000
+30	17348	62	62.0	88.74	88.74	88.740000000000000000	88.740000000000000000
+30	17875	78	78.0	2.91	2.91	2.910000000000000000	2.910000000000000000
+31	913	54	54.0	79.11	79.11	79.110000000000000000	79.110000000000000000
+31	4963	67	67.0	56.37	56.37	56.370000000000000000	56.370000000000000000
+31	6617	11	11.0	86.78	86.78	86.780000000000000000	86.780000000000000000
+31	6917	4	4.0	49.76	49.76	49.760000000000000000	49.760000000000000000
+31	7513	82	82.0	44.95	44.95	44.950000000000000000	44.950000000000000000
+31	11739	95	95.0	6.99	6.99	6.990000000000000000	6.990000000000000000
+31	14575	97	97.0	59.9	59.9	59.900000000000000000	59.900000000000000000
+31	14727	41	41.0	48.1	48.1	48.100000000000000000	48.100000000000000000
+31	15341	31	31.0	16.15	16.15	16.150000000000000000	16.150000000000000000
+31	15411	53	53.0	47.64	47.64	47.640000000000000000	47.640000000000000000
+31	16251	51	51.0	91.49	91.49	91.490000000000000000	91.490000000000000000
+32	1115	61	61.0	97.03	97.03	97.030000000000000000	97.030000000000000000
+32	2095	34	34.0	89.33	89.33	89.330000000000000000	89.330000000000000000
+32	2887	8	8.0	48.71	48.71	48.710000000000000000	48.710000000000000000
+32	4339	6	6.0	88.27	88.27	88.270000000000000000	88.270000000000000000
+32	4537	22	22.0	65.72	65.72	65.720000000000000000	65.720000000000000000
+32	4808	NULL	NULL	57.01	57.01	57.010000000000000000	57.010000000000000000
+32	5798	87	87.0	46.23	46.23	46.230000000000000000	46.230000000000000000
+32	7547	24	24.0	43.33	43.33	43.330000000000000000	43.330000000000000000
+32	9683	26	26.0	NULL	NULL	NULL	NULL
+32	11005	46	46.0	51.48	51.48	51.480000000000000000	51.480000000000000000
+32	11348	41	41.0	55.14	55.14	55.140000000000000000	55.140000000000000000
+32	12134	21	21.0	51.01	51.01	51.010000000000000000	51.010000000000000000
+32	15001	57	57.0	30.07	30.07	30.070000000000000000	30.070000000000000000
+32	15644	34	34.0	80.54	80.54	80.540000000000000000	80.540000000000000000
+32	16421	74	74.0	89.89	89.89	89.890000000000000000	89.890000000000000000
+32	17659	51	51.0	23.88	23.88	23.880000000000000000	23.880000000000000000
+33	4798	27	27.0	28.56	28.56	28.560000000000000000	28.560000000000000000
+33	7300	3	3.0	3.13	3.13	3.130000000000000000	3.130000000000000000
+33	9649	36	36.0	18.91	18.91	18.910000000000000000	18.910000000000000000
+33	10376	21	21.0	55.09	55.09	55.090000000000000000	55.090000000000000000
+33	11119	92	92.0	3.49	3.49	3.490000000000000000	3.490000000000000000
+33	11756	26	26.0	58.87	58.87	58.870000000000000000	58.870000000000000000
+33	12643	89	89.0	35.74	35.74	35.740000000000000000	35.740000000000000000
+33	12760	54	54.0	48.97	48.97	48.970000000000000000	48.970000000000000000
+33	12964	80	80.0	83.86	83.86	83.860000000000000000	83.860000000000000000
+33	14125	66	66.0	44.03	44.03	44.030000000000000000	44.030000000000000000
+33	14158	82	82.0	48.07	48.07	48.070000000000000000	48.070000000000000000
+33	14692	93	93.0	56.78	56.78	56.780000000000000000	56.780000000000000000
+33	15478	22	22.0	95.96	95.96	95.960000000000000000	95.960000000000000000
+34	1526	91	91.0	78.12	78.12	78.120000000000000000	78.120000000000000000
+34	1717	53	53.0	99.68	99.68	99.680000000000000000	99.680000000000000000
+34	2312	6	6.0	51.4	51.4	51.400000000000000000	51.400000000000000000
+34	4118	88	88.0	38.38	38.38	38.380000000000000000	38.380000000000000000
+34	5197	63	63.0	13.5	13.5	13.500000000000000000	13.500000000000000000
+34	5449	9	9.0	21.24	21.24	21.240000000000000000	21.240000000000000000
+34	6193	61	61.0	54.55	54.55	54.550000000000000000	54.550000000000000000
+34	9325	3	3.0	92.35	92.35	92.350000000000000000	92.350000000000000000
+34	9766	83	83.0	68.57	68.57	68.570000000000000000	68.570000000000000000
+34	12016	42	42.0	42.44	42.44	42.440000000000000000	42.440000000000000000
+34	12290	53	53.0	88.61	88.61	88.610000000000000000	88.610000000000000000
+34	12512	60	60.0	40.48	40.48	40.480000000000000000	40.480000000000000000
+34	13814	20	20.0	22.82	22.82	22.820000000000000000	22.820000000000000000
+34	16324	30	30.0	37.27	37.27	37.270000000000000000	37.270000000000000000
+35	411	51	51.0	NULL	NULL	NULL	NULL
+35	2377	52	52.0	98.03	98.03	98.030000000000000000	98.030000000000000000
+35	3667	97	97.0	59.31	59.31	59.310000000000000000	59.310000000000000000
+35	4325	56	56.0	67.43	67.43	67.430000000000000000	67.430000000000000000
+35	5179	83	83.0	90.54	90.54	90.540000000000000000	90.540000000000000000
+35	11635	87	87.0	92.02	92.02	92.020000000000000000	92.020000000000000000
+35	11661	81	81.0	NULL	NULL	NULL	NULL
+35	14239	55	55.0	8.27	8.27	8.270000000000000000	8.270000000000000000
+35	15619	45	45.0	90.28	90.28	90.280000000000000000	90.280000000000000000
+35	15757	9	9.0	14.83	14.83	14.830000000000000000	14.830000000000000000
+35	17341	92	92.0	59.48	59.48	59.480000000000000000	59.480000000000000000
+35	17365	65	65.0	76.2	76.2	76.200000000000000000	76.200000000000000000
+35	17451	7	7.0	45.66	45.66	45.660000000000000000	45.660000000000000000
+36	1115	80	80.0	11.13	11.13	11.130000000000000000	11.130000000000000000
+36	2095	43	43.0	91.17	91.17	91.170000000000000000	91.170000000000000000
+36	2887	31	31.0	24.53	24.53	24.530000000000000000	24.530000000000000000
+36	7547	46	46.0	8.04	8.04	8.040000000000000000	8.040000000000000000
+36	11005	49	49.0	70.6	70.6	70.600000000000000000	70.600000000000000000
+36	11349	80	80.0	58.17	58.17	58.170000000000000000	58.170000000000000000
+36	15001	54	54.0	16.24	16.24	16.240000000000000000	16.240000000000000000
+36	15645	23	23.0	32.35	32.35	32.350000000000000000	32.350000000000000000
+36	16421	25	25.0	69.67	69.67	69.670000000000000000	69.670000000000000000
+36	17561	16	16.0	82.46	82.46	82.460000000000000000	82.460000000000000000
+36	17659	91	91.0	44.83	44.83	44.830000000000000000	44.830000000000000000
+37	2997	94	94.0	85.67	85.67	85.670000000000000000	85.670000000000000000
+37	7283	87	87.0	54.25	54.25	54.250000000000000000	54.250000000000000000
+37	10715	52	52.0	89.22	89.22	89.220000000000000000	89.220000000000000000
+37	10929	88	88.0	65.45	65.45	65.450000000000000000	65.450000000000000000
+37	13171	6	6.0	84.14	84.14	84.140000000000000000	84.140000000000000000
+37	15337	62	62.0	16.64	16.64	16.640000000000000000	16.640000000000000000
+37	16971	12	12.0	53.97	53.97	53.970000000000000000	53.970000000000000000
+37	17125	NULL	NULL	NULL	NULL	NULL	NULL
+38	757	2	2.0	NULL	NULL	NULL	NULL
+38	2164	17	17.0	72.04	72.04	72.040000000000000000	72.040000000000000000
+38	3439	84	84.0	11.71	11.71	11.710000000000000000	11.710000000000000000
+38	4154	35	35.0	10.28	10.28	10.280000000000000000	10.280000000000000000
+38	5113	73	73.0	50.59	50.59	50.590000000000000000	50.590000000000000000
+38	6220	98	98.0	14.54	14.54	14.540000000000000000	14.540000000000000000
+38	7018	15	15.0	69.78	69.78	69.780000000000000000	69.780000000000000000
+38	7784	56	56.0	31.89	31.89	31.890000000000000000	31.890000000000000000
+38	8870	15	15.0	46.69	46.69	46.690000000000000000	46.690000000000000000
+38	9710	7	7.0	82.77	82.77	82.770000000000000000	82.770000000000000000
+38	10441	62	62.0	80.37	80.37	80.370000000000000000	80.370000000000000000
+38	15698	57	57.0	11.4	11.4	11.400000000000000000	11.400000000000000000
+39	386	89	89.0	28.08	28.08	28.080000000000000000	28.080000000000000000
+39	1598	64	64.0	44.63	44.63	44.630000000000000000	44.630000000000000000
+39	3476	73	73.0	80.57	80.57	80.570000000000000000	80.570000000000000000
+39	3943	64	64.0	59.68	59.68	59.680000000000000000	59.680000000000000000
+39	4190	86	86.0	35.56	35.56	35.560000000000000000	35.560000000000000000
+39	4957	24	24.0	16.1	16.1	16.100000000000000000	16.100000000000000000
+39	5393	98	98.0	58.75	58.75	58.750000000000000000	58.750000000000000000
+39	7097	78	78.0	33.1	33.1	33.100000000000000000	33.100000000000000000
+39	7118	67	67.0	68.99	68.99	68.990000000000000000	68.990000000000000000
+39	7604	49	49.0	46.49	46.49	46.490000000000000000	46.490000000000000000
+39	7697	24	24.0	44.89	44.89	44.890000000000000000	44.890000000000000000
+39	8078	54	54.0	73.6	73.6	73.600000000000000000	73.600000000000000000
+39	8411	96	96.0	35.69	35.69	35.690000000000000000	35.690000000000000000
+39	15491	54	54.0	3.2	3.2	3.200000000000000000	3.200000000000000000
+39	15625	17	17.0	96.62	96.62	96.620000000000000000	96.620000000000000000
+40	2854	71	71.0	10.62	10.62	10.620000000000000000	10.620000000000000000
+40	3490	65	65.0	41.24	41.24	41.240000000000000000	41.240000000000000000
+40	3985	63	63.0	22.94	22.94	22.940000000000000000	22.940000000000000000
+40	5098	35	35.0	33.91	33.91	33.910000000000000000	33.910000000000000000
+40	5318	87	87.0	32.66	32.66	32.660000000000000000	32.660000000000000000
+40	10094	80	80.0	8.63	8.63	8.630000000000000000	8.630000000000000000
+40	10912	23	23.0	2.46	2.46	2.460000000000000000	2.460000000000000000
+40	12050	NULL	NULL	38.12	38.12	38.120000000000000000	38.120000000000000000
+40	13658	53	53.0	56.42	56.42	56.420000000000000000	56.420000000000000000
+40	16976	3	3.0	20.7	20.7	20.700000000000000000	20.700000000000000000
+41	10	50	50.0	54.36	54.36	54.360000000000000000	54.360000000000000000
+41	64	29	29.0	27.18	27.18	27.180000000000000000	27.180000000000000000
+41	3380	88	88.0	14.11	14.11	14.110000000000000000	14.110000000000000000
+41	5566	11	11.0	50.45	50.45	50.450000000000000000	50.450000000000000000
+41	6310	90	90.0	60.1	60.1	60.100000000000000000	60.100000000000000000
+41	7402	69	69.0	57.23	57.23	57.230000000000000000	57.230000000000000000
+41	7603	94	94.0	6.12	6.12	6.120000000000000000	6.120000000000000000
+41	9322	8	8.0	59.4	59.4	59.400000000000000000	59.400000000000000000
+41	10915	81	81.0	91.63	91.63	91.630000000000000000	91.630000000000000000
+41	14788	15	15.0	90.04	90.04	90.040000000000000000	90.040000000000000000
+41	15242	87	87.0	48.25	48.25	48.250000000000000000	48.250000000000000000
+41	15328	46	46.0	84.03	84.03	84.030000000000000000	84.030000000000000000
+41	16514	20	20.0	5.05	5.05	5.050000000000000000	5.050000000000000000
+42	619	69	69.0	56.85	56.85	56.850000000000000000	56.850000000000000000
+42	976	100	100.0	12.59	12.59	12.590000000000000000	12.590000000000000000
+42	1436	94	94.0	54.21	54.21	54.210000000000000000	54.210000000000000000
+42	2314	74	74.0	24.46	24.46	24.460000000000000000	24.460000000000000000
+42	2392	14	14.0	49.48	49.48	49.480000000000000000	49.480000000000000000
+42	2602	30	30.0	55.77	55.77	55.770000000000000000	55.770000000000000000
+42	3346	74	74.0	29.72	29.72	29.720000000000000000	29.720000000000000000
+42	3613	30	30.0	56.33	56.33	56.330000000000000000	56.330000000000000000
+42	6058	30	30.0	81.1	81.1	81.100000000000000000	81.100000000000000000
+42	6134	92	92.0	18.91	18.91	18.910000000000000000	18.910000000000000000
+42	8462	23	23.0	27.88	27.88	27.880000000000000000	27.880000000000000000
+42	9740	52	52.0	52.46	52.46	52.460000000000000000	52.460000000000000000
+42	10016	57	57.0	12.47	12.47	12.470000000000000000	12.470000000000000000
+42	10471	19	19.0	42.67	42.67	42.670000000000000000	42.670000000000000000
+42	12550	41	41.0	17.09	17.09	17.090000000000000000	17.090000000000000000
+42	15002	41	41.0	58.33	58.33	58.330000000000000000	58.330000000000000000
+43	2923	16	16.0	82.12	82.12	82.120000000000000000	82.120000000000000000
+43	3344	22	22.0	88.77	88.77	88.770000000000000000	88.770000000000000000
+43	3911	26	26.0	21.75	21.75	21.750000000000000000	21.750000000000000000
+43	4364	77	77.0	82.92	82.92	82.920000000000000000	82.920000000000000000
+43	4691	41	41.0	2.24	2.24	2.240000000000000000	2.240000000000000000
+43	5773	85	85.0	66.42	66.42	66.420000000000000000	66.420000000000000000
+43	5852	16	16.0	81.99	81.99	81.990000000000000000	81.990000000000000000
+43	11771	30	30.0	41.13	41.13	41.130000000000000000	41.130000000000000000
+43	14669	97	97.0	52.94	52.94	52.940000000000000000	52.940000000000000000
+44	2351	56	56.0	55.53	55.53	55.530000000000000000	55.530000000000000000
+44	2623	18	18.0	39.17	39.17	39.170000000000000000	39.170000000000000000
+44	7303	14	14.0	36.13	36.13	36.130000000000000000	36.130000000000000000
+44	7527	67	67.0	90.05	90.05	90.050000000000000000	90.050000000000000000
+44	9059	68	68.0	30.11	30.11	30.110000000000000000	30.110000000000000000
+44	11707	83	83.0	85.49	85.49	85.490000000000000000	85.490000000000000000
+44	12341	20	20.0	82.28	82.28	82.280000000000000000	82.280000000000000000
+44	13331	98	98.0	3.53	3.53	3.530000000000000000	3.530000000000000000
+44	13449	45	45.0	50.83	50.83	50.830000000000000000	50.830000000000000000
+44	14149	80	80.0	18.83	18.83	18.830000000000000000	18.830000000000000000
+44	15803	81	81.0	43.81	43.81	43.810000000000000000	43.810000000000000000
+44	16491	56	56.0	32.28	32.28	32.280000000000000000	32.280000000000000000
+44	16837	92	92.0	30.11	30.11	30.110000000000000000	30.110000000000000000
+44	16909	61	61.0	92.15	92.15	92.150000000000000000	92.150000000000000000
+45	811	62	62.0	23.41	23.41	23.410000000000000000	23.410000000000000000
+45	1479	49	49.0	5.01	5.01	5.010000000000000000	5.010000000000000000
+45	3265	98	98.0	27.12	27.12	27.120000000000000000	27.120000000000000000
+45	5309	18	18.0	51.16	51.16	51.160000000000000000	51.160000000000000000
+45	7363	87	87.0	85.95	85.95	85.950000000000000000	85.950000000000000000
+45	10115	68	68.0	38.09	38.09	38.090000000000000000	38.090000000000000000
+45	11095	40	40.0	52.97	52.97	52.970000000000000000	52.970000000000000000
+45	13133	46	46.0	85.87	85.87	85.870000000000000000	85.870000000000000000
+45	16349	6	6.0	94.59	94.59	94.590000000000000000	94.590000000000000000
+46	1960	12	12.0	53.47	53.47	53.470000000000000000	53.470000000000000000
+46	3010	67	67.0	66.87	66.87	66.870000000000000000	66.870000000000000000
+46	7040	33	33.0	90.87	90.87	90.870000000000000000	90.870000000000000000
+46	8065	NULL	NULL	43.04	43.04	43.040000000000000000	43.040000000000000000
+46	11426	72	72.0	53.81	53.81	53.810000000000000000	53.810000000000000000
+46	13042	58	58.0	41.38	41.38	41.380000000000000000	41.380000000000000000
+46	15595	32	32.0	29.12	29.12	29.120000000000000000	29.120000000000000000
+46	16540	30	30.0	54.36	54.36	54.360000000000000000	54.360000000000000000
+46	17150	57	57.0	71.68	71.68	71.680000000000000000	71.680000000000000000
+46	17384	13	13.0	93.68	93.68	93.680000000000000000	93.680000000000000000
+47	254	NULL	NULL	NULL	NULL	NULL	NULL
+47	481	30	30.0	36.51	36.51	36.510000000000000000	36.510000000000000000
+47	1132	66	66.0	53.46	53.46	53.460000000000000000	53.460000000000000000
+47	1916	71	71.0	47.62	47.62	47.620000000000000000	47.620000000000000000
+47	3085	51	51.0	63.55	63.55	63.550000000000000000	63.550000000000000000
+47	3202	7	7.0	26.06	26.06	26.060000000000000000	26.060000000000000000
+47	3878	NULL	NULL	NULL	NULL	NULL	NULL
+47	4774	11	11.0	63.71	63.71	63.710000000000000000	63.710000000000000000
+47	5008	82	82.0	1.76	1.76	1.760000000000000000	1.760000000000000000
+47	5305	NULL	NULL	84.7	84.7	84.700000000000000000	84.700000000000000000
+47	5468	7	7.0	5.03	5.03	5.030000000000000000	5.030000000000000000
+47	7214	1	1.0	12.8	12.8	12.800000000000000000	12.800000000000000000
+47	9770	33	33.0	69.12	69.12	69.120000000000000000	69.120000000000000000
+47	13246	47	47.0	11.71	11.71	11.710000000000000000	11.710000000000000000
+47	13477	10	10.0	78.83	78.83	78.830000000000000000	78.830000000000000000
+48	1761	22	22.0	55.73	55.73	55.730000000000000000	55.730000000000000000
+48	2820	4	4.0	6.46	6.46	6.460000000000000000	6.460000000000000000
+48	2829	65	65.0	22.1	22.1	22.100000000000000000	22.100000000000000000
+48	4431	39	39.0	97.07	97.07	97.070000000000000000	97.070000000000000000
+48	5971	29	29.0	40.46	40.46	40.460000000000000000	40.460000000000000000
+48	6085	1	1.0	58.13	58.13	58.130000000000000000	58.130000000000000000
+48	6684	44	44.0	20.22	20.22	20.220000000000000000	20.220000000000000000
+48	9199	88	88.0	37.89	37.89	37.890000000000000000	37.890000000000000000
+48	11259	NULL	NULL	NULL	NULL	NULL	NULL
+48	12468	62	62.0	43.72	43.72	43.720000000000000000	43.720000000000000000
+48	13153	74	74.0	34.26	34.26	34.260000000000000000	34.260000000000000000
+48	17799	17	17.0	80.36	80.36	80.360000000000000000	80.360000000000000000
+49	749	60	60.0	42.11	42.11	42.110000000000000000	42.110000000000000000
+49	2135	4	4.0	15.8	15.8	15.800000000000000000	15.800000000000000000
+49	5342	69	69.0	46.41	46.41	46.410000000000000000	46.410000000000000000
+49	5852	47	47.0	74.9	74.9	74.900000000000000000	74.900000000000000000
+49	6805	40	40.0	12.9	12.9	12.900000000000000000	12.900000000000000000
+49	7141	94	94.0	50.5	50.5	50.500000000000000000	50.500000000000000000
+49	9049	68	68.0	75.38	75.38	75.380000000000000000	75.380000000000000000
+49	9553	71	71.0	29.28	29.28	29.280000000000000000	29.280000000000000000
+49	12737	48	48.0	2.17	2.17	2.170000000000000000	2.170000000000000000
+49	15155	84	84.0	4.4	4.4	4.400000000000000000	4.400000000000000000
+49	16361	4	4.0	79.85	79.85	79.850000000000000000	79.850000000000000000
+50	1280	69	69.0	8.66	8.66	8.660000000000000000	8.660000000000000000
+50	1312	30	30.0	25.84	25.84	25.840000000000000000	25.840000000000000000
+50	1909	53	53.0	56.01	56.01	56.010000000000000000	56.010000000000000000
+50	1984	40	40.0	8.81	8.81	8.810000000000000000	8.810000000000000000
+50	3097	64	64.0	33.17	33.17	33.170000000000000000	33.170000000000000000
+50	5023	NULL	NULL	16.24	16.24	16.240000000000000000	16.240000000000000000
+50	7135	69	69.0	12.68	12.68	12.680000000000000000	12.680000000000000000
+50	16081	82	82.0	99.55	99.55	99.550000000000000000	99.550000000000000000
+51	422	21	21.0	69.89	69.89	69.890000000000000000	69.890000000000000000
+51	3091	28	28.0	92.87	92.87	92.870000000000000000	92.870000000000000000
+51	4687	6	6.0	93.02	93.02	93.020000000000000000	93.020000000000000000
+51	5029	12	12.0	34.53	34.53	34.530000000000000000	34.530000000000000000
+51	5059	51	51.0	48.54	48.54	48.540000000000000000	48.540000000000000000
+51	6565	33	33.0	32.44	32.44	32.440000000000000000	32.440000000000000000
+51	8384	79	79.0	15.35	15.35	15.350000000000000000	15.350000000000000000
+51	9311	90	90.0	39.48	39.48	39.480000000000000000	39.480000000000000000
+51	10133	54	54.0	46.71	46.71	46.710000000000000000	46.710000000000000000
+51	11234	NULL	NULL	NULL	NULL	NULL	NULL
+51	12625	53	53.0	97.27	97.27	97.270000000000000000	97.270000000000000000
+51	13199	97	97.0	99.32	99.32	99.320000000000000000	99.320000000000000000
+51	17483	22	22.0	31.99	31.99	31.990000000000000000	31.990000000000000000
+51	17705	66	66.0	46.11	46.11	46.110000000000000000	46.110000000000000000
+52	2420	90	90.0	22.31	22.31	22.310000000000000000	22.310000000000000000
+52	3334	73	73.0	29.2	29.2	29.200000000000000000	29.200000000000000000
+52	6098	NULL	NULL	4.83	4.83	4.830000000000000000	4.830000000000000000
+52	7606	45	45.0	42.51	42.51	42.510000000000000000	42.510000000000000000
+52	11488	76	76.0	78.68	78.68	78.680000000000000000	78.680000000000000000
+52	15649	29	29.0	22.86	22.86	22.860000000000000000	22.860000000000000000
+52	16646	48	48.0	95.82	95.82	95.820000000000000000	95.820000000000000000
+52	17402	91	91.0	81.94	81.94	81.940000000000000000	81.940000000000000000
+52	17456	37	37.0	7.93	7.93	7.930000000000000000	7.930000000000000000
+53	1114	40	40.0	28.34	28.34	28.340000000000000000	28.340000000000000000
+53	2095	62	62.0	23.98	23.98	23.980000000000000000	23.980000000000000000
+53	2786	70	70.0	76.55	76.55	76.550000000000000000	76.550000000000000000
+53	2887	39	39.0	66.68	66.68	66.680000000000000000	66.680000000000000000
+53	7546	58	58.0	73.79	73.79	73.790000000000000000	73.790000000000000000
+53	11348	38	38.0	5.54	5.54	5.540000000000000000	5.540000000000000000
+53	13220	76	76.0	27.93	27.93	27.930000000000000000	27.930000000000000000
+53	13795	38	38.0	93.96	93.96	93.960000000000000000	93.960000000000000000
+53	15991	37	37.0	77.75	77.75	77.750000000000000000	77.750000000000000000
+53	16420	14	14.0	36.72	36.72	36.720000000000000000	36.720000000000000000
+53	16648	79	79.0	55.29	55.29	55.290000000000000000	55.290000000000000000
+53	17296	43	43.0	21.4	21.4	21.400000000000000000	21.400000000000000000
+53	17560	15	15.0	46.39	46.39	46.390000000000000000	46.390000000000000000
+54	702	40	40.0	16.76	16.76	16.760000000000000000	16.760000000000000000
+54	825	50	50.0	99.64	99.64	99.640000000000000000	99.640000000000000000
+54	1165	62	62.0	69.84	69.84	69.840000000000000000	69.840000000000000000
+54	3861	NULL	NULL	NULL	NULL	NULL	NULL
+54	6517	40	40.0	23.38	23.38	23.380000000000000000	23.380000000000000000
+54	9159	75	75.0	55.47	55.47	55.470000000000000000	55.470000000000000000
+54	14737	38	38.0	29.2	29.2	29.200000000000000000	29.200000000000000000
+54	16059	15	15.0	7.9	7.9	7.900000000000000000	7.900000000000000000
+54	16974	NULL	NULL	NULL	NULL	NULL	NULL
+54	17479	34	34.0	94.14	94.14	94.140000000000000000	94.140000000000000000
+55	1339	16	16.0	71.32	71.32	71.320000000000000000	71.320000000000000000
+55	3001	7	7.0	57.58	57.58	57.580000000000000000	57.580000000000000000
+55	5137	33	33.0	57.28	57.28	57.280000000000000000	57.280000000000000000
+55	9703	44	44.0	57.21	57.21	57.210000000000000000	57.210000000000000000
+55	12170	92	92.0	69.53	69.53	69.530000000000000000	69.530000000000000000
+55	12205	90	90.0	56.92	56.92	56.920000000000000000	56.920000000000000000
+55	14135	36	36.0	26.4	26.4	26.400000000000000000	26.400000000000000000
+55	14923	71	71.0	30.04	30.04	30.040000000000000000	30.040000000000000000
+55	17677	17	17.0	26.59	26.59	26.590000000000000000	26.590000000000000000
+56	4242	2	2.0	88.74	88.74	88.740000000000000000	88.740000000000000000
+56	4506	57	57.0	69.45	69.45	69.450000000000000000	69.450000000000000000
+56	8353	35	35.0	80.42	80.42	80.420000000000000000	80.420000000000000000
+56	8691	59	59.0	98.91	98.91	98.910000000000000000	98.910000000000000000
+56	8707	68	68.0	79.7	79.7	79.700000000000000000	79.700000000000000000
+56	10362	54	54.0	82.62	82.62	82.620000000000000000	82.620000000000000000
+56	16620	23	23.0	9.94	9.94	9.940000000000000000	9.940000000000000000
+56	17331	74	74.0	32.12	32.12	32.120000000000000000	32.120000000000000000
+57	3253	71	71.0	91.02	91.02	91.020000000000000000	91.020000000000000000
+57	4028	88	88.0	82.23	82.23	82.230000000000000000	82.230000000000000000
+57	4933	22	22.0	93.86	93.86	93.860000000000000000	93.860000000000000000
+57	12596	91	91.0	36.67	36.67	36.670000000000000000	36.670000000000000000
+57	12721	62	62.0	76.4	76.4	76.400000000000000000	76.400000000000000000
+57	12740	52	52.0	55.58	55.58	55.580000000000000000	55.580000000000000000
+57	15182	86	86.0	84.85	84.85	84.850000000000000000	84.850000000000000000
+57	17729	26	26.0	97.2	97.2	97.200000000000000000	97.200000000000000000
+57	17993	99	99.0	NULL	NULL	NULL	NULL
+58	1829	52	52.0	19.97	19.97	19.970000000000000000	19.970000000000000000
+58	3848	6	6.0	45.41	45.41	45.410000000000000000	45.410000000000000000
+58	5117	2	2.0	56.01	56.01	56.010000000000000000	56.010000000000000000
+58	7649	19	19.0	44.04	44.04	44.040000000000000000	44.040000000000000000
+58	9743	62	62.0	73.14	73.14	73.140000000000000000	73.140000000000000000
+58	10802	14	14.0	79.64	79.64	79.640000000000000000	79.640000000000000000
+58	15635	6	6.0	82.45	82.45	82.450000000000000000	82.450000000000000000
+58	16472	6	6.0	7.58	7.58	7.580000000000000000	7.580000000000000000
+58	16949	35	35.0	25.76	25.76	25.760000000000000000	25.760000000000000000
+59	3133	92	92.0	14.57	14.57	14.570000000000000000	14.570000000000000000
+59	3546	22	22.0	64.21	64.21	64.210000000000000000	64.210000000000000000
+59	5772	70	70.0	56.19	56.19	56.190000000000000000	56.190000000000000000
+59	7087	80	80.0	58.71	58.71	58.710000000000000000	58.710000000000000000
+59	8010	46	46.0	20.15	20.15	20.150000000000000000	20.150000000000000000
+59	8335	36	36.0	32.82	32.82	32.820000000000000000	32.820000000000000000
+59	9348	62	62.0	83.62	83.62	83.620000000000000000	83.620000000000000000
+59	9397	92	92.0	70.69	70.69	70.690000000000000000	70.690000000000000000
+59	10651	100	100.0	35.78	35.78	35.780000000000000000	35.780000000000000000
+59	11916	19	19.0	34.31	34.31	34.310000000000000000	34.310000000000000000
+59	12858	90	90.0	61.18	61.18	61.180000000000000000	61.180000000000000000
+59	14529	44	44.0	42.76	42.76	42.760000000000000000	42.760000000000000000
+60	97	50	50.0	37.49	37.49	37.490000000000000000	37.490000000000000000
+60	555	62	62.0	49.17	49.17	49.170000000000000000	49.170000000000000000
+60	633	71	71.0	96.74	96.74	96.740000000000000000	96.740000000000000000
+60	999	43	43.0	22.13	22.13	22.130000000000000000	22.130000000000000000
+60	1117	78	78.0	46.63	46.63	46.630000000000000000	46.630000000000000000
+60	1573	90	90.0	19.02	19.02	19.020000000000000000	19.020000000000000000
+60	4041	25	25.0	36.26	36.26	36.260000000000000000	36.260000000000000000
+60	4235	28	28.0	29.67	29.67	29.670000000000000000	29.670000000000000000
+60	4513	72	72.0	79.56	79.56	79.560000000000000000	79.560000000000000000
+60	4937	22	22.0	27.75	27.75	27.750000000000000000	27.750000000000000000
+60	7231	95	95.0	45.42	45.42	45.420000000000000000	45.420000000000000000
+60	10277	62	62.0	28.05	28.05	28.050000000000000000	28.050000000000000000
+60	10393	75	75.0	98.86	98.86	98.860000000000000000	98.860000000000000000
+60	13975	14	14.0	76.01	76.01	76.010000000000000000	76.010000000000000000
+60	16887	25	25.0	17.92	17.92	17.920000000000000000	17.920000000000000000
+60	17755	88	88.0	52.17	52.17	52.170000000000000000	52.170000000000000000
+61	1106	4	4.0	78.21	78.21	78.210000000000000000	78.210000000000000000
+61	2264	36	36.0	60.94	60.94	60.940000000000000000	60.940000000000000000
+61	3362	48	48.0	67.92	67.92	67.920000000000000000	67.920000000000000000
+61	4567	26	26.0	29.6	29.6	29.600000000000000000	29.600000000000000000
+61	5528	78	78.0	13.85	13.85	13.850000000000000000	13.850000000000000000
+61	6380	77	77.0	69.52	69.52	69.520000000000000000	69.520000000000000000
+61	7591	78	78.0	91.99	91.99	91.990000000000000000	91.990000000000000000
+61	8924	11	11.0	86.51	86.51	86.510000000000000000	86.510000000000000000
+61	10330	8	8.0	46.45	46.45	46.450000000000000000	46.450000000000000000
+61	16462	26	26.0	24.34	24.34	24.340000000000000000	24.340000000000000000
+62	4093	94	94.0	5.53	5.53	5.530000000000000000	5.530000000000000000
+62	6403	NULL	NULL	92.02	92.02	92.020000000000000000	92.020000000000000000
+62	8457	37	37.0	99.97	99.97	99.970000000000000000	99.970000000000000000
+62	10149	75	75.0	48.36	48.36	48.360000000000000000	48.360000000000000000
+62	12163	29	29.0	16.7	16.7	16.700000000000000000	16.700000000000000000
+62	12199	5	5.0	85.54	85.54	85.540000000000000000	85.540000000000000000
+62	12407	NULL	NULL	NULL	NULL	NULL	NULL
+62	13559	80	80.0	52.56	52.56	52.560000000000000000	52.560000000000000000
+62	15399	74	74.0	71.7	71.7	71.700000000000000000	71.700000000000000000
+62	15733	40	40.0	28.03	28.03	28.030000000000000000	28.030000000000000000
+62	16151	93	93.0	84.72	84.72	84.720000000000000000	84.720000000000000000
+63	4488	73	73.0	22.85	22.85	22.850000000000000000	22.850000000000000000
+63	5079	79	79.0	36.05	36.05	36.050000000000000000	36.050000000000000000
+63	5217	66	66.0	15.71	15.71	15.710000000000000000	15.710000000000000000
+63	5658	99	99.0	88.78	88.78	88.780000000000000000	88.780000000000000000
+63	9319	80	80.0	9.27	9.27	9.270000000000000000	9.270000000000000000
+63	11370	38	38.0	56.43	56.43	56.430000000000000000	56.430000000000000000
+63	11946	85	85.0	94.28	94.28	94.280000000000000000	94.280000000000000000
+63	13339	19	19.0	19.44	19.44	19.440000000000000000	19.440000000000000000
+63	15793	40	40.0	75.62	75.62	75.620000000000000000	75.620000000000000000
+63	16569	69	69.0	NULL	NULL	NULL	NULL
+64	1213	NULL	NULL	38.46	38.46	38.460000000000000000	38.460000000000000000
+64	3090	87	87.0	78.06	78.06	78.060000000000000000	78.060000000000000000
+64	3963	NULL	NULL	NULL	NULL	NULL	NULL
+64	11835	82	82.0	30.65	30.65	30.650000000000000000	30.650000000000000000
+64	13224	NULL	NULL	NULL	NULL	NULL	NULL
+64	14407	8	8.0	44.36	44.36	44.360000000000000000	44.360000000000000000
+64	15867	59	59.0	43.77	43.77	43.770000000000000000	43.770000000000000000
+64	15936	30	30.0	56.24	56.24	56.240000000000000000	56.240000000000000000
+64	16921	19	19.0	98.61	98.61	98.610000000000000000	98.610000000000000000
+64	17586	78	78.0	77.26	77.26	77.260000000000000000	77.260000000000000000
+64	17617	17	17.0	91.67	91.67	91.670000000000000000	91.670000000000000000
+65	2287	100	100.0	91.8	91.8	91.800000000000000000	91.800000000000000000
+65	4227	42	42.0	45.38	45.38	45.380000000000000000	45.380000000000000000
+65	9625	51	51.0	40.95	40.95	40.950000000000000000	40.950000000000000000
+65	9847	54	54.0	64.26	64.26	64.260000000000000000	64.260000000000000000
+65	13897	40	40.0	52.84	52.84	52.840000000000000000	52.840000000000000000
+65	14905	85	85.0	81.24	81.24	81.240000000000000000	81.240000000000000000
+65	15177	55	55.0	89.19	89.19	89.190000000000000000	89.190000000000000000
+65	17025	67	67.0	25.52	25.52	25.520000000000000000	25.520000000000000000
+66	6507	76	76.0	43.81	43.81	43.810000000000000000	43.810000000000000000
+66	7033	65	65.0	4.08	4.08	4.080000000000000000	4.080000000000000000
+66	7227	66	66.0	92.15	92.15	92.150000000000000000	92.150000000000000000
+66	8197	41	41.0	84.22	84.22	84.220000000000000000	84.220000000000000000
+66	9237	29	29.0	76.94	76.94	76.940000000000000000	76.940000000000000000
+66	10019	10	10.0	48.77	48.77	48.770000000000000000	48.770000000000000000
+66	11419	66	66.0	10.12	10.12	10.120000000000000000	10.120000000000000000
+66	15629	20	20.0	22.04	22.04	22.040000000000000000	22.040000000000000000
+66	16745	91	91.0	9.53	9.53	9.530000000000000000	9.530000000000000000
+66	16795	28	28.0	42.0	42.0	42.000000000000000000	42.000000000000000000
+67	757	77	77.0	94.12	94.12	94.120000000000000000	94.120000000000000000
+67	2133	74	74.0	71.99	71.99	71.990000000000000000	71.990000000000000000
+67	3439	73	73.0	23.52	23.52	23.520000000000000000	23.520000000000000000
+67	4155	87	87.0	87.74	87.74	87.740000000000000000	87.740000000000000000
+67	5113	NULL	NULL	49.59	49.59	49.590000000000000000	49.590000000000000000
+67	7020	79	79.0	97.01	97.01	97.010000000000000000	97.010000000000000000
+67	7507	77	77.0	26.78	26.78	26.780000000000000000	26.780000000000000000
+67	8469	59	59.0	NULL	NULL	NULL	NULL
+67	8871	71	71.0	78.59	78.59	78.590000000000000000	78.590000000000000000
+67	12087	70	70.0	80.71	80.71	80.710000000000000000	80.710000000000000000
+67	15699	44	44.0	34.59	34.59	34.590000000000000000	34.590000000000000000
+68	1387	74	74.0	90.2	90.2	90.200000000000000000	90.200000000000000000
+68	1603	57	57.0	21.03	21.03	21.030000000000000000	21.030000000000000000
+68	1820	54	54.0	55.82	55.82	55.820000000000000000	55.820000000000000000
+68	2035	22	22.0	54.35	54.35	54.350000000000000000	54.350000000000000000
+68	2296	52	52.0	98.9	98.9	98.900000000000000000	98.900000000000000000
+68	2564	83	83.0	77.32	77.32	77.320000000000000000	77.320000000000000000
+68	5162	23	23.0	83.48	83.48	83.480000000000000000	83.480000000000000000
+68	6763	77	77.0	96.29	96.29	96.290000000000000000	96.290000000000000000
+68	7765	NULL	NULL	69.58	69.58	69.580000000000000000	69.580000000000000000
+68	12526	3	3.0	13.06	13.06	13.060000000000000000	13.060000000000000000
+68	12724	88	88.0	9.63	9.63	9.630000000000000000	9.630000000000000000
+68	17426	2	2.0	48.36	48.36	48.360000000000000000	48.360000000000000000
+68	17600	13	13.0	52.66	52.66	52.660000000000000000	52.660000000000000000
+69	322	45	45.0	NULL	NULL	NULL	NULL
+69	337	34	34.0	20.99	20.99	20.990000000000000000	20.990000000000000000
+69	4208	9	9.0	99.77	99.77	99.770000000000000000	99.770000000000000000
+69	4267	10	10.0	72.37	72.37	72.370000000000000000	72.370000000000000000
+69	6136	7	7.0	49.79	49.79	49.790000000000000000	49.790000000000000000
+69	7264	67	67.0	78.29	78.29	78.290000000000000000	78.290000000000000000
+69	7822	30	30.0	78.1	78.1	78.100000000000000000	78.100000000000000000
+69	8599	53	53.0	56.42	56.42	56.420000000000000000	56.420000000000000000
+69	11137	68	68.0	22.04	22.04	22.040000000000000000	22.040000000000000000
+69	13489	66	66.0	2.68	2.68	2.680000000000000000	2.680000000000000000
+69	13792	NULL	NULL	85.64	85.64	85.640000000000000000	85.640000000000000000
+69	15448	16	16.0	94.38	94.38	94.380000000000000000	94.380000000000000000
+70	1592	53	53.0	99.59	99.59	99.590000000000000000	99.590000000000000000
+70	2462	NULL	NULL	92.7	92.7	92.700000000000000000	92.700000000000000000
+70	3296	48	48.0	10.23	10.23	10.230000000000000000	10.230000000000000000
+70	3947	NULL	NULL	63.8	63.8	63.800000000000000000	63.800000000000000000
+70	6185	82	82.0	84.6	84.6	84.600000000000000000	84.600000000000000000
+70	6425	NULL	NULL	NULL	NULL	NULL	NULL
+70	8893	17	17.0	63.51	63.51	63.510000000000000000	63.510000000000000000
+70	9857	20	20.0	54.96	54.96	54.960000000000000000	54.960000000000000000
+70	14549	4	4.0	35.39	35.39	35.390000000000000000	35.390000000000000000
+70	17815	95	95.0	36.89	36.89	36.890000000000000000	36.890000000000000000
+71	457	75	75.0	27.02	27.02	27.020000000000000000	27.020000000000000000
+71	1888	4	4.0	40.47	40.47	40.470000000000000000	40.470000000000000000
+71	2098	51	51.0	57.87	57.87	57.870000000000000000	57.870000000000000000
+71	4144	49	49.0	26.75	26.75	26.750000000000000000	26.750000000000000000
+71	5858	NULL	NULL	NULL	NULL	NULL	NULL
+71	6008	54	54.0	38.98	38.98	38.980000000000000000	38.980000000000000000
+71	7504	3	3.0	78.44	78.44	78.440000000000000000	78.440000000000000000
+71	8887	10	10.0	61.4	61.4	61.400000000000000000	61.400000000000000000
+71	9274	36	36.0	12.39	12.39	12.390000000000000000	12.390000000000000000
+71	9769	79	79.0	52.15	52.15	52.150000000000000000	52.150000000000000000
+71	9790	96	96.0	37.78	37.78	37.780000000000000000	37.780000000000000000
+71	9997	26	26.0	53.28	53.28	53.280000000000000000	53.280000000000000000
+71	10108	66	66.0	9.49	9.49	9.490000000000000000	9.490000000000000000
+71	10288	30	30.0	29.57	29.57	29.570000000000000000	29.570000000000000000
+71	11168	79	79.0	24.66	24.66	24.660000000000000000	24.660000000000000000
+71	17246	90	90.0	50.57	50.57	50.570000000000000000	50.570000000000000000
+72	1535	9	9.0	69.06	69.06	69.060000000000000000	69.060000000000000000
+72	5917	85	85.0	NULL	NULL	NULL	NULL
+72	6113	45	45.0	59.65	59.65	59.650000000000000000	59.650000000000000000
+72	6671	13	13.0	42.82	42.82	42.820000000000000000	42.820000000000000000
+72	9860	26	26.0	69.92	69.92	69.920000000000000000	69.920000000000000000
+72	10427	66	66.0	55.31	55.31	55.310000000000000000	55.310000000000000000
+72	10753	16	16.0	32.01	32.01	32.010000000000000000	32.010000000000000000
+72	11741	62	62.0	79.25	79.25	79.250000000000000000	79.250000000000000000
+72	12788	29	29.0	34.57	34.57	34.570000000000000000	34.570000000000000000
+72	12901	57	57.0	1.64	1.64	1.640000000000000000	1.640000000000000000
+72	13085	94	94.0	85.13	85.13	85.130000000000000000	85.130000000000000000
+72	13423	62	62.0	34.39	34.39	34.390000000000000000	34.390000000000000000
+72	13904	37	37.0	40.39	40.39	40.390000000000000000	40.390000000000000000
+72	15587	87	87.0	19.04	19.04	19.040000000000000000	19.040000000000000000
+72	16765	56	56.0	89.44	89.44	89.440000000000000000	89.440000000000000000
+73	247	53	53.0	5.22	5.22	5.220000000000000000	5.220000000000000000
+73	1063	37	37.0	34.93	34.93	34.930000000000000000	34.930000000000000000
+73	3205	82	82.0	44.64	44.64	44.640000000000000000	44.640000000000000000
+73	4946	54	54.0	71.08	71.08	71.080000000000000000	71.080000000000000000
+73	6862	58	58.0	86.48	86.48	86.480000000000000000	86.480000000000000000
+73	10051	49	49.0	97.28	97.28	97.280000000000000000	97.280000000000000000
+73	12502	75	75.0	21.63	21.63	21.630000000000000000	21.630000000000000000
+73	15109	38	38.0	53.9	53.9	53.900000000000000000	53.900000000000000000
+73	16519	97	97.0	82.11	82.11	82.110000000000000000	82.110000000000000000
+73	16585	38	38.0	69.27	69.27	69.270000000000000000	69.270000000000000000
+73	17269	40	40.0	NULL	NULL	NULL	NULL
+74	326	29	29.0	76.73	76.73	76.730000000000000000	76.730000000000000000
+74	3104	78	78.0	52.23	52.23	52.230000000000000000	52.230000000000000000
+74	3175	23	23.0	50.69	50.69	50.690000000000000000	50.690000000000000000
+74	3278	NULL	NULL	NULL	NULL	NULL	NULL
+74	3542	96	96.0	93.18	93.18	93.180000000000000000	93.180000000000000000
+74	3754	26	26.0	89.35	89.35	89.350000000000000000	89.350000000000000000
+74	5492	54	54.0	31.24	31.24	31.240000000000000000	31.240000000000000000
+74	7694	17	17.0	36.61	36.61	36.610000000000000000	36.610000000000000000
+74	8653	12	12.0	4.33	4.33	4.330000000000000000	4.330000000000000000
+74	9620	95	95.0	56.35	56.35	56.350000000000000000	56.350000000000000000
+74	10069	99	99.0	99.98	99.98	99.980000000000000000	99.980000000000000000
+74	13208	87	87.0	82.61	82.61	82.610000000000000000	82.610000000000000000
+74	16694	72	72.0	36.04	36.04	36.040000000000000000	36.040000000000000000
+75	607	20	20.0	88.61	88.61	88.610000000000000000	88.610000000000000000
+75	2948	25	25.0	7.48	7.48	7.480000000000000000	7.480000000000000000
+75	4625	73	73.0	76.04	76.04	76.040000000000000000	76.040000000000000000
+75	6938	89	89.0	20.73	20.73	20.730000000000000000	20.730000000000000000
+75	6953	71	71.0	33.34	33.34	33.340000000000000000	33.340000000000000000
+75	8726	6	6.0	25.87	25.87	25.870000000000000000	25.870000000000000000
+75	9905	54	54.0	63.01	63.01	63.010000000000000000	63.010000000000000000
+75	10217	85	85.0	83.25	83.25	83.250000000000000000	83.250000000000000000
+75	11039	70	70.0	87.84	87.84	87.840000000000000000	87.840000000000000000
+75	14186	63	63.0	82.77	82.77	82.770000000000000000	82.770000000000000000
+75	16796	93	93.0	5.19	5.19	5.190000000000000000	5.190000000000000000
+76	257	5	5.0	8.47	8.47	8.470000000000000000	8.470000000000000000
+76	465	2	2.0	95.45	95.45	95.450000000000000000	95.450000000000000000
+76	1107	16	16.0	NULL	NULL	NULL	NULL
+76	1503	97	97.0	30.22	30.22	30.220000000000000000	30.220000000000000000
+76	2265	98	98.0	89.7	89.7	89.700000000000000000	89.700000000000000000
+76	2869	32	32.0	NULL	NULL	NULL	NULL
+76	3363	25	25.0	89.9	89.9	89.900000000000000000	89.900000000000000000
+76	4237	48	48.0	60.58	60.58	60.580000000000000000	60.580000000000000000
+76	4567	40	40.0	2.19	2.19	2.190000000000000000	2.190000000000000000
+76	5529	78	78.0	49.64	49.64	49.640000000000000000	49.640000000000000000
+76	6381	50	50.0	34.93	34.93	34.930000000000000000	34.930000000000000000
+76	7591	27	27.0	61.86	61.86	61.860000000000000000	61.860000000000000000
+76	8925	6	6.0	80.04	80.04	80.040000000000000000	80.040000000000000000
+76	10331	3	3.0	29.09	29.09	29.090000000000000000	29.090000000000000000
+76	16463	53	53.0	86.06	86.06	86.060000000000000000	86.060000000000000000
+77	992	62	62.0	21.65	21.65	21.650000000000000000	21.650000000000000000
+77	1399	34	34.0	96.21	96.21	96.210000000000000000	96.210000000000000000
+77	2713	85	85.0	85.72	85.72	85.720000000000000000	85.720000000000000000
+77	3868	89	89.0	3.72	3.72	3.720000000000000000	3.720000000000000000
+77	6289	30	30.0	26.16	26.16	26.160000000000000000	26.160000000000000000
+77	7339	88	88.0	31.13	31.13	31.130000000000000000	31.130000000000000000
+77	7448	95	95.0	29.07	29.07	29.070000000000000000	29.070000000000000000
+77	7486	49	49.0	NULL	NULL	NULL	NULL
+77	8686	38	38.0	45.3	45.3	45.300000000000000000	45.300000000000000000
+77	9220	90	90.0	87.41	87.41	87.410000000000000000	87.410000000000000000
+77	11918	36	36.0	25.95	25.95	25.950000000000000000	25.950000000000000000
+77	12439	95	95.0	74.32	74.32	74.320000000000000000	74.320000000000000000
+77	13456	48	48.0	85.61	85.61	85.610000000000000000	85.610000000000000000
+77	14815	18	18.0	69.28	69.28	69.280000000000000000	69.280000000000000000
+77	16687	16	16.0	67.63	67.63	67.630000000000000000	67.630000000000000000
+78	901	3	3.0	54.5	54.5	54.500000000000000000	54.500000000000000000
+78	3304	50	50.0	61.49	61.49	61.490000000000000000	61.490000000000000000
+78	3856	27	27.0	60.1	60.1	60.100000000000000000	60.100000000000000000
+78	5965	78	78.0	7.47	7.47	7.470000000000000000	7.470000000000000000
+78	6044	59	59.0	15.94	15.94	15.940000000000000000	15.940000000000000000
+78	6110	43	43.0	28.45	28.45	28.450000000000000000	28.450000000000000000
+78	6500	76	76.0	38.42	38.42	38.420000000000000000	38.420000000000000000
+78	7576	87	87.0	60.62	60.62	60.620000000000000000	60.620000000000000000
+78	8611	79	79.0	95.42	95.42	95.420000000000000000	95.420000000000000000
+78	10507	6	6.0	50.83	50.83	50.830000000000000000	50.830000000000000000
+78	11209	7	7.0	88.17	88.17	88.170000000000000000	88.170000000000000000
+78	12706	19	19.0	81.1	81.1	81.100000000000000000	81.100000000000000000
+78	14996	39	39.0	36.47	36.47	36.470000000000000000	36.470000000000000000
+79	247	NULL	NULL	NULL	NULL	NULL	NULL
+79	1063	85	85.0	77.51	77.51	77.510000000000000000	77.510000000000000000
+79	3205	48	48.0	21.34	21.34	21.340000000000000000	21.340000000000000000
+79	4947	35	35.0	99.77	99.77	99.770000000000000000	99.770000000000000000
+79	6864	1	1.0	86.12	86.12	86.120000000000000000	86.120000000000000000
+79	10051	10	10.0	94.53	94.53	94.530000000000000000	94.530000000000000000
+79	10524	36	36.0	21.73	21.73	21.730000000000000000	21.730000000000000000
+79	12504	81	81.0	14.87	14.87	14.870000000000000000	14.870000000000000000
+79	14322	41	41.0	58.88	58.88	58.880000000000000000	58.880000000000000000
+79	15109	NULL	NULL	45.07	45.07	45.070000000000000000	45.070000000000000000
+79	15498	3	3.0	94.64	94.64	94.640000000000000000	94.640000000000000000
+79	15888	58	58.0	99.75	99.75	99.750000000000000000	99.750000000000000000
+79	16519	9	9.0	69.84	69.84	69.840000000000000000	69.840000000000000000
+79	16585	93	93.0	70.7	70.7	70.700000000000000000	70.700000000000000000
+79	17269	81	81.0	5.88	5.88	5.880000000000000000	5.880000000000000000
+80	998	93	93.0	69.32	69.32	69.320000000000000000	69.320000000000000000
+80	1519	25	25.0	66.36	66.36	66.360000000000000000	66.360000000000000000
+80	1573	40	40.0	43.33	43.33	43.330000000000000000	43.330000000000000000
+80	4040	66	66.0	15.01	15.01	15.010000000000000000	15.010000000000000000
+80	4513	NULL	NULL	76.02	76.02	76.020000000000000000	76.020000000000000000
+80	4622	1	1.0	60.1	60.1	60.100000000000000000	60.100000000000000000
+80	7231	49	49.0	76.07	76.07	76.070000000000000000	76.070000000000000000
+80	7610	37	37.0	24.62	24.62	24.620000000000000000	24.620000000000000000
+80	10393	5	5.0	71.37	71.37	71.370000000000000000	71.370000000000000000
+80	12968	NULL	NULL	NULL	NULL	NULL	NULL
+80	13717	91	91.0	60.42	60.42	60.420000000000000000	60.420000000000000000
+80	13975	13	13.0	83.81	83.81	83.810000000000000000	83.810000000000000000
+80	16363	84	84.0	84.8	84.8	84.800000000000000000	84.800000000000000000
+80	16886	77	77.0	89.22	89.22	89.220000000000000000	89.220000000000000000
+80	17308	29	29.0	94.38	94.38	94.380000000000000000	94.380000000000000000
+80	17755	94	94.0	56.04	56.04	56.040000000000000000	56.040000000000000000
+81	4486	31	31.0	63.84	63.84	63.840000000000000000	63.840000000000000000
+81	5078	75	75.0	33.72	33.72	33.720000000000000000	33.720000000000000000
+81	5216	64	64.0	4.59	4.59	4.590000000000000000	4.590000000000000000
+81	5656	24	24.0	40.61	40.61	40.610000000000000000	40.610000000000000000
+81	7166	7	7.0	22.87	22.87	22.870000000000000000	22.870000000000000000
+81	7663	79	79.0	52.07	52.07	52.070000000000000000	52.070000000000000000
+81	8918	37	37.0	86.54	86.54	86.540000000000000000	86.540000000000000000
+81	9319	36	36.0	91.74	91.74	91.740000000000000000	91.740000000000000000
+81	11107	36	36.0	47.86	47.86	47.860000000000000000	47.860000000000000000
+81	11368	26	26.0	NULL	NULL	NULL	NULL
+81	13339	6	6.0	4.63	4.63	4.630000000000000000	4.630000000000000000
+81	15793	8	8.0	5.61	5.61	5.610000000000000000	5.610000000000000000
+82	2572	53	53.0	55.41	55.41	55.410000000000000000	55.410000000000000000
+82	7862	75	75.0	21.65	21.65	21.650000000000000000	21.650000000000000000
+82	13138	59	59.0	31.81	31.81	31.810000000000000000	31.810000000000000000
+82	14998	49	49.0	52.59	52.59	52.590000000000000000	52.590000000000000000
+82	17041	18	18.0	4.71	4.71	4.710000000000000000	4.710000000000000000
diff --git a/ql/src/test/results/clientpositive/llap/vector_grouping_sets.q.out b/ql/src/test/results/clientpositive/llap/vector_grouping_sets.q.out
index 92a6a6cd26..a84da2ccd8 100644
--- a/ql/src/test/results/clientpositive/llap/vector_grouping_sets.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_grouping_sets.q.out
@@ -168,9 +168,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 1, ConstantVectorExpression(val 0) -> 29:long
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: s_store_id (type: string), 0 (type: int)
                       mode: hash
@@ -208,9 +210,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string), KEY._col1 (type: int)
                 mode: mergepartial
@@ -298,9 +302,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 1, ConstantVectorExpression(val 0) -> 29:long
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: string), 0 (type: int)
                       mode: hash
@@ -338,9 +344,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string), KEY._col1 (type: int)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_include_no_sel.q.out b/ql/src/test/results/clientpositive/llap/vector_include_no_sel.q.out
index efd49cdba9..f5cbe53836 100644
--- a/ql/src/test/results/clientpositive/llap/vector_include_no_sel.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_include_no_sel.q.out
@@ -257,8 +257,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 25:long) -> bigint
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0]
                           mode: hash
                           outputColumnNames: _col0
@@ -297,8 +299,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/llap/vector_inner_join.q.out b/ql/src/test/results/clientpositive/llap/vector_inner_join.q.out
index 73468110b0..3e2ed6f1b5 100644
--- a/ql/src/test/results/clientpositive/llap/vector_inner_join.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_inner_join.q.out
@@ -304,9 +304,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
diff --git a/ql/src/test/results/clientpositive/llap/vector_join30.q.out b/ql/src/test/results/clientpositive/llap/vector_join30.q.out
index 6b544f3ec4..381f13e978 100644
--- a/ql/src/test/results/clientpositive/llap/vector_join30.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_join30.q.out
@@ -164,8 +164,10 @@ STAGE PLANS:
                     Group By Vectorization:
                         aggregators: VectorUDAFSumLong(VectorUDFAdaptor(hash(_col2,_col3)) -> 2:int) -> bigint
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: [0]
                     mode: hash
                     outputColumnNames: _col0
@@ -193,8 +195,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -403,8 +407,10 @@ STAGE PLANS:
                     Group By Vectorization:
                         aggregators: VectorUDAFSumLong(VectorUDFAdaptor(hash(_col2,_col3)) -> 3:int) -> bigint
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: [0]
                     mode: hash
                     outputColumnNames: _col0
@@ -432,8 +438,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -669,8 +677,10 @@ STAGE PLANS:
                     Group By Vectorization:
                         aggregators: VectorUDAFSumLong(VectorUDFAdaptor(hash(_col2,_col3)) -> 2:int) -> bigint
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: [0]
                     mode: hash
                     outputColumnNames: _col0
@@ -698,8 +708,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -914,8 +926,10 @@ STAGE PLANS:
                     Group By Vectorization:
                         aggregators: VectorUDAFSumLong(VectorUDFAdaptor(hash(_col2,_col3)) -> 2:int) -> bigint
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: [0]
                     mode: hash
                     outputColumnNames: _col0
@@ -943,8 +957,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1209,8 +1225,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: sum(hash(_col2,_col3))
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -1234,8 +1252,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1500,8 +1520,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: sum(hash(_col2,_col3))
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -1525,8 +1547,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1799,8 +1823,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: sum(hash(_col2,_col3))
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -1824,8 +1850,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -2090,8 +2118,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: sum(hash(_col2,_col3))
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -2115,8 +2145,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/llap/vector_join30.q.out.orig b/ql/src/test/results/clientpositive/llap/vector_join30.q.out.orig
new file mode 100644
index 0000000000..6b544f3ec4
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/vector_join30.q.out.orig
@@ -0,0 +1,2220 @@
+PREHOOK: query: CREATE TABLE orcsrc STORED AS ORC AS SELECT * FROM src
+PREHOOK: type: CREATETABLE_AS_SELECT
+PREHOOK: Input: default@src
+PREHOOK: Output: database:default
+PREHOOK: Output: default@orcsrc
+POSTHOOK: query: CREATE TABLE orcsrc STORED AS ORC AS SELECT * FROM src
+POSTHOOK: type: CREATETABLE_AS_SELECT
+POSTHOOK: Input: default@src
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@orcsrc
+POSTHOOK: Lineage: orcsrc.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: orcsrc.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 5 (BROADCAST_EDGE)
+        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 5 <- Map 4 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: SelectColumnIsNotNull(col 0) -> boolean
+                    predicate: key is not null (type: boolean)
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: string)
+                      outputColumnNames: _col0
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0]
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string)
+                        sort order: +
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Map 4 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: SelectColumnIsNotNull(col 0) -> boolean
+                    predicate: key is not null (type: boolean)
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: string), value (type: string)
+                      outputColumnNames: _col0, _col1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1]
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col1 (type: string)
+                        sort order: +
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: string)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Map Join Operator
+                  condition map:
+                       Inner Join 0 to 1
+                  keys:
+                    0 _col0 (type: string)
+                    1 _col0 (type: string)
+                  Map Join Vectorization:
+                      className: VectorMapJoinInnerStringOperator
+                      native: true
+                      nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true, Optimized Table and Supports Key Types IS true
+                  outputColumnNames: _col2, _col3
+                  input vertices:
+                    1 Reducer 5
+                  Statistics: Num rows: 550 Data size: 96800 Basic stats: COMPLETE Column stats: NONE
+                  Group By Operator
+                    aggregations: sum(hash(_col2,_col3))
+                    Group By Vectorization:
+                        aggregators: VectorUDAFSumLong(VectorUDFAdaptor(hash(_col2,_col3)) -> 2:int) -> bigint
+                        className: VectorGroupByOperator
+                        vectorOutput: true
+                        native: false
+                        projectedOutputColumns: [0]
+                    mode: hash
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      sort order: 
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkEmptyKeyOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: bigint)
+        Reducer 3 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    vectorOutput: true
+                    native: false
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 5 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0, _col1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col1 (type: string)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN 
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+POSTHOOK: query: FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN 
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+103231310608
+PREHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 5 (BROADCAST_EDGE)
+        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 5 <- Map 4 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string)
+                    outputColumnNames: _col0
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Map 4 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Map Join Operator
+                  condition map:
+                       Left Outer Join 0 to 1
+                  keys:
+                    0 _col0 (type: string)
+                    1 _col0 (type: string)
+                  Map Join Vectorization:
+                      className: VectorMapJoinOuterStringOperator
+                      native: true
+                      nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true, Optimized Table and Supports Key Types IS true
+                  outputColumnNames: _col2, _col3
+                  input vertices:
+                    1 Reducer 5
+                  Statistics: Num rows: 550 Data size: 96800 Basic stats: COMPLETE Column stats: NONE
+                  Group By Operator
+                    aggregations: sum(hash(_col2,_col3))
+                    Group By Vectorization:
+                        aggregators: VectorUDAFSumLong(VectorUDFAdaptor(hash(_col2,_col3)) -> 3:int) -> bigint
+                        className: VectorGroupByOperator
+                        vectorOutput: true
+                        native: false
+                        projectedOutputColumns: [0]
+                    mode: hash
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      sort order: 
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkEmptyKeyOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: bigint)
+        Reducer 3 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    vectorOutput: true
+                    native: false
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 5 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0, _col1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col1 (type: string)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN 
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+POSTHOOK: query: FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN 
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+103231310608
+PREHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+        Reducer 4 <- Map 3 (SIMPLE_EDGE), Reducer 2 (BROADCAST_EDGE)
+        Reducer 5 <- Reducer 4 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string)
+                    outputColumnNames: _col0
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Map 3 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+        Reducer 4 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0, _col1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Map Join Operator
+                  condition map:
+                       Right Outer Join 0 to 1
+                  keys:
+                    0 _col0 (type: string)
+                    1 _col0 (type: string)
+                  Map Join Vectorization:
+                      className: VectorMapJoinOuterStringOperator
+                      native: true
+                      nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true, Optimized Table and Supports Key Types IS true
+                  outputColumnNames: _col2, _col3
+                  input vertices:
+                    0 Reducer 2
+                  Statistics: Num rows: 550 Data size: 96800 Basic stats: COMPLETE Column stats: NONE
+                  Group By Operator
+                    aggregations: sum(hash(_col2,_col3))
+                    Group By Vectorization:
+                        aggregators: VectorUDAFSumLong(VectorUDFAdaptor(hash(_col2,_col3)) -> 2:int) -> bigint
+                        className: VectorGroupByOperator
+                        vectorOutput: true
+                        native: false
+                        projectedOutputColumns: [0]
+                    mode: hash
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      sort order: 
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkEmptyKeyOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: bigint)
+        Reducer 5 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    vectorOutput: true
+                    native: false
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+RIGHT OUTER JOIN 
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+POSTHOOK: query: FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+RIGHT OUTER JOIN 
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+103231310608
+PREHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 5 (BROADCAST_EDGE), Reducer 6 (BROADCAST_EDGE)
+        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 5 <- Map 4 (SIMPLE_EDGE)
+        Reducer 6 <- Map 4 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: SelectColumnIsNotNull(col 0) -> boolean
+                    predicate: key is not null (type: boolean)
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: string)
+                      outputColumnNames: _col0
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0]
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string)
+                        sort order: +
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Map 4 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: SelectColumnIsNotNull(col 0) -> boolean
+                    predicate: key is not null (type: boolean)
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: string), value (type: string)
+                      outputColumnNames: _col0, _col1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1]
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col1 (type: string)
+                        sort order: +
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: string)
+                      Reduce Output Operator
+                        key expressions: _col1 (type: string)
+                        sort order: +
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: string)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Map Join Operator
+                  condition map:
+                       Inner Join 0 to 1
+                       Inner Join 0 to 2
+                  keys:
+                    0 _col0 (type: string)
+                    1 _col0 (type: string)
+                    2 _col0 (type: string)
+                  Map Join Vectorization:
+                      className: VectorMapJoinOperator
+                      native: false
+                      nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No nullsafe IS true, Small table vectorizes IS true, Optimized Table and Supports Key Types IS true
+                      nativeConditionsNotMet: One MapJoin Condition IS false
+                  outputColumnNames: _col2, _col3
+                  input vertices:
+                    1 Reducer 5
+                    2 Reducer 6
+                  Statistics: Num rows: 1100 Data size: 193600 Basic stats: COMPLETE Column stats: NONE
+                  Group By Operator
+                    aggregations: sum(hash(_col2,_col3))
+                    Group By Vectorization:
+                        aggregators: VectorUDAFSumLong(VectorUDFAdaptor(hash(_col2,_col3)) -> 2:int) -> bigint
+                        className: VectorGroupByOperator
+                        vectorOutput: true
+                        native: false
+                        projectedOutputColumns: [0]
+                    mode: hash
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      sort order: 
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkEmptyKeyOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: bigint)
+        Reducer 3 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    vectorOutput: true
+                    native: false
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 5 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0, _col1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col1 (type: string)
+        Reducer 6 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+POSTHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+348019368476
+PREHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE), Reducer 7 (SIMPLE_EDGE)
+        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
+        Reducer 6 <- Map 5 (SIMPLE_EDGE)
+        Reducer 7 <- Map 5 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string)
+                    outputColumnNames: _col0
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Map 5 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+        Reducer 3 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                     Left Outer Join 0 to 2
+                keys:
+                  0 _col0 (type: string)
+                  1 _col0 (type: string)
+                  2 _col0 (type: string)
+                outputColumnNames: _col2, _col3
+                Statistics: Num rows: 1100 Data size: 193600 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: sum(hash(_col2,_col3))
+                  Group By Vectorization:
+                      vectorOutput: false
+                      native: false
+                      projectedOutputColumns: null
+                  mode: hash
+                  outputColumnNames: _col0
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    sort order: 
+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: bigint)
+        Reducer 4 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    vectorOutput: true
+                    native: false
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 6 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0, _col1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col1 (type: string)
+        Reducer 7 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+POSTHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+348019368476
+PREHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE), Reducer 7 (SIMPLE_EDGE)
+        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
+        Reducer 6 <- Map 5 (SIMPLE_EDGE)
+        Reducer 7 <- Map 5 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string)
+                    outputColumnNames: _col0
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Map 5 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+        Reducer 3 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Left Outer Join 0 to 1
+                     Left Outer Join 0 to 2
+                keys:
+                  0 _col0 (type: string)
+                  1 _col0 (type: string)
+                  2 _col0 (type: string)
+                outputColumnNames: _col2, _col3
+                Statistics: Num rows: 1100 Data size: 193600 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: sum(hash(_col2,_col3))
+                  Group By Vectorization:
+                      vectorOutput: false
+                      native: false
+                      projectedOutputColumns: null
+                  mode: hash
+                  outputColumnNames: _col0
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    sort order: 
+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: bigint)
+        Reducer 4 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    vectorOutput: true
+                    native: false
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 6 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0, _col1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col1 (type: string)
+        Reducer 7 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+POSTHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+348019368476
+PREHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE), Reducer 7 (SIMPLE_EDGE)
+        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
+        Reducer 6 <- Map 5 (SIMPLE_EDGE)
+        Reducer 7 <- Map 5 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string)
+                    outputColumnNames: _col0
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Map 5 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+        Reducer 3 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Left Outer Join 0 to 1
+                     Right Outer Join 0 to 2
+                keys:
+                  0 _col0 (type: string)
+                  1 _col0 (type: string)
+                  2 _col0 (type: string)
+                outputColumnNames: _col2, _col3
+                Statistics: Num rows: 1100 Data size: 193600 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: sum(hash(_col2,_col3))
+                  Group By Vectorization:
+                      vectorOutput: false
+                      native: false
+                      projectedOutputColumns: null
+                  mode: hash
+                  outputColumnNames: _col0
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    sort order: 
+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: bigint)
+        Reducer 4 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    vectorOutput: true
+                    native: false
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 6 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0, _col1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col1 (type: string)
+        Reducer 7 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+POSTHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+LEFT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+348019368476
+PREHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization expression
+FROM 
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE), Reducer 7 (SIMPLE_EDGE)
+        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
+        Reducer 6 <- Map 5 (SIMPLE_EDGE)
+        Reducer 7 <- Map 5 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string)
+                    outputColumnNames: _col0
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Map 5 
+            Map Operator Tree:
+                TableScan
+                  alias: orcsrc
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1]
+                    Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+                    Reduce Output Operator
+                      key expressions: _col1 (type: string)
+                      sort order: +
+                      Reduce Sink Vectorization:
+                          className: VectorReduceSinkObjectHashOperator
+                          native: true
+                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col0 (type: string)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+        Reducer 3 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Right Outer Join 0 to 1
+                     Right Outer Join 0 to 2
+                keys:
+                  0 _col0 (type: string)
+                  1 _col0 (type: string)
+                  2 _col0 (type: string)
+                outputColumnNames: _col2, _col3
+                Statistics: Num rows: 1100 Data size: 193600 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: sum(hash(_col2,_col3))
+                  Group By Vectorization:
+                      vectorOutput: false
+                      native: false
+                      projectedOutputColumns: null
+                  mode: hash
+                  outputColumnNames: _col0
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    sort order: 
+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: bigint)
+        Reducer 4 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    vectorOutput: true
+                    native: false
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 6 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: string)
+                outputColumnNames: _col0, _col1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 0]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col1 (type: string)
+        Reducer 7 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                groupByVectorOutput: true
+                allNative: true
+                usesVectorUDFAdaptor: false
+                vectorized: true
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: string)
+                outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1]
+                Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: string)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkStringOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  Statistics: Num rows: 500 Data size: 88000 Basic stats: COMPLETE Column stats: NONE
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+POSTHOOK: query: FROM
+(SELECT orcsrc.* FROM orcsrc sort by key) x
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Y
+ON (x.key = Y.key)
+RIGHT OUTER JOIN
+(SELECT orcsrc.* FROM orcsrc sort by value) Z
+ON (x.key = Z.key)
+select sum(hash(Y.key,Y.value))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orcsrc
+#### A masked pattern was here ####
+348019368476
diff --git a/ql/src/test/results/clientpositive/llap/vector_leftsemi_mapjoin.q.out b/ql/src/test/results/clientpositive/llap/vector_leftsemi_mapjoin.q.out
index f77f0cef3f..c02f879d02 100644
--- a/ql/src/test/results/clientpositive/llap/vector_leftsemi_mapjoin.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_leftsemi_mapjoin.q.out
@@ -3389,8 +3389,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -3500,8 +3502,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -3613,8 +3617,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -3721,8 +3727,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -3837,8 +3845,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -3919,8 +3929,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -4030,8 +4042,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -4138,8 +4152,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -4274,8 +4290,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -4398,8 +4416,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -4531,8 +4551,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -4652,8 +4674,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkMultiKeyOperator
                               native: true
@@ -4771,8 +4795,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -4799,8 +4825,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -4932,8 +4960,10 @@ STAGE PLANS:
                         native: true
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkLongOperator
                             native: true
@@ -5072,8 +5102,10 @@ STAGE PLANS:
                         native: true
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkLongOperator
                             native: true
@@ -5198,8 +5230,10 @@ STAGE PLANS:
                         native: true
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkLongOperator
                             native: true
@@ -5342,8 +5376,10 @@ STAGE PLANS:
                         native: true
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkLongOperator
                             native: true
@@ -5488,8 +5524,10 @@ STAGE PLANS:
                         native: true
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkLongOperator
                             native: true
@@ -5661,8 +5699,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkLongOperator
                               native: true
@@ -5822,8 +5862,10 @@ STAGE PLANS:
                           native: true
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                           Reduce Sink Vectorization:
                               className: VectorReduceSinkStringOperator
                               native: true
@@ -5956,9 +5998,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -6151,9 +6195,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -6348,9 +6394,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -6545,9 +6593,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col1 (type: int), _col1 (type: int)
                         mode: hash
@@ -6745,9 +6795,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: string)
                         mode: hash
@@ -6882,9 +6934,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -7082,9 +7136,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: string)
                         mode: hash
@@ -7279,9 +7335,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -7528,9 +7586,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -7736,9 +7796,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -7986,9 +8048,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -8191,9 +8255,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: string)
                         mode: hash
@@ -8397,9 +8463,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -8457,9 +8525,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -8685,9 +8755,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -8908,9 +8980,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -9117,9 +9191,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -9360,9 +9436,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -9605,9 +9683,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -9904,9 +9984,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -10171,9 +10253,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: string)
                         mode: hash
@@ -10329,9 +10413,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -10527,9 +10613,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -10727,9 +10815,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -10927,9 +11017,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col1 (type: int), _col1 (type: int)
                         mode: hash
@@ -11130,9 +11222,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: string)
                         mode: hash
@@ -11267,9 +11361,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -11470,9 +11566,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: string)
                         mode: hash
@@ -11670,9 +11768,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -11925,9 +12025,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -12136,9 +12238,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -12386,9 +12490,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -12594,9 +12700,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: string)
                         mode: hash
@@ -12800,9 +12908,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -12860,9 +12970,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -13088,9 +13200,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -13311,9 +13425,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -13520,9 +13636,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -13763,9 +13881,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -14008,9 +14128,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -14313,9 +14435,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -14583,9 +14707,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: string)
                         mode: hash
@@ -14741,9 +14867,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -14939,9 +15067,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -15139,9 +15269,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -15339,9 +15471,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col1 (type: int), _col1 (type: int)
                         mode: hash
@@ -15542,9 +15676,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: string)
                         mode: hash
@@ -15679,9 +15815,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -15882,9 +16020,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: string)
                         mode: hash
@@ -16082,9 +16222,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -16337,9 +16479,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -16548,9 +16692,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -16798,9 +16944,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -17006,9 +17154,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: string)
                         mode: hash
@@ -17212,9 +17362,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -17272,9 +17424,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -17500,9 +17654,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -17723,9 +17879,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -17932,9 +18090,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -18175,9 +18335,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -18420,9 +18582,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: int)
                       mode: hash
@@ -18725,9 +18889,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -18995,9 +19161,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: string)
                         mode: hash
diff --git a/ql/src/test/results/clientpositive/llap/vector_mapjoin_reduce.q.out b/ql/src/test/results/clientpositive/llap/vector_mapjoin_reduce.q.out
index d3586e0db2..8aabb62b9e 100644
--- a/ql/src/test/results/clientpositive/llap/vector_mapjoin_reduce.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_mapjoin_reduce.q.out
@@ -44,9 +44,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 1
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: l_partkey (type: int)
                       mode: hash
@@ -140,9 +142,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -180,9 +184,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -311,9 +317,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 1
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: l_partkey (type: int)
                       mode: hash
@@ -408,9 +416,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 3
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: int)
                         mode: hash
@@ -448,9 +458,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_null_projection.q.out b/ql/src/test/results/clientpositive/llap/vector_null_projection.q.out
index 84266a2026..8c60363b19 100644
--- a/ql/src/test/results/clientpositive/llap/vector_null_projection.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_null_projection.q.out
@@ -69,7 +69,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type void of Const void null not supported
+                notVectorizedReason: Select expression for SELECT operator: Vectorizing data type void not supported when mode = PROJECTION
                 vectorized: false
 
   Stage: Stage-0
@@ -136,7 +136,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Key expression for GROUPBY operator: Data type void of Const void null not supported
+                notVectorizedReason: Key expression for GROUPBY operator: Vectorizing data type void not supported when mode = PROJECTION
                 vectorized: false
         Map 4 
             Map Operator Tree:
@@ -163,14 +163,14 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Key expression for GROUPBY operator: Data type void of Const void null not supported
+                notVectorizedReason: Key expression for GROUPBY operator: Vectorizing data type void not supported when mode = PROJECTION
                 vectorized: false
         Reducer 3 
             Execution mode: llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Key expression for GROUPBY operator: Data type void of Column[KEY._col0] not supported
+                notVectorizedReason: Key expression for GROUPBY operator: Vectorizing data type void not supported when mode = PROJECTION
                 vectorized: false
             Reduce Operator Tree:
               Group By Operator
diff --git a/ql/src/test/results/clientpositive/llap/vector_number_compare_projection.q.out b/ql/src/test/results/clientpositive/llap/vector_number_compare_projection.q.out
index aa1b9d805d..33cc940b51 100644
--- a/ql/src/test/results/clientpositive/llap/vector_number_compare_projection.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_number_compare_projection.q.out
@@ -181,8 +181,10 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFSumLong(col 6) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: COMPLETE
                       vectorOutput: true
                       native: false
+                      vectorProcessingMode: GLOBAL
                       projectedOutputColumns: [0]
                   mode: complete
                   outputColumnNames: _col0
@@ -305,8 +307,10 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFSumLong(col 8) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: COMPLETE
                       vectorOutput: true
                       native: false
+                      vectorProcessingMode: GLOBAL
                       projectedOutputColumns: [0]
                   mode: complete
                   outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/llap/vector_nvl.q.out b/ql/src/test/results/clientpositive/llap/vector_nvl.q.out
index aa8ed4abd9..3dc952c6bf 100644
--- a/ql/src/test/results/clientpositive/llap/vector_nvl.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_nvl.q.out
@@ -334,7 +334,7 @@ STAGE PLANS:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Select expression for SELECT operator: Data type void of Const void null not supported
+                notVectorizedReason: Select expression for SELECT operator: Vectorizing data type void not supported when mode = PROJECTION
                 vectorized: false
 
   Stage: Stage-0
diff --git a/ql/src/test/results/clientpositive/llap/vector_orderby_5.q.out b/ql/src/test/results/clientpositive/llap/vector_orderby_5.q.out
index 7faf8922fa..a99cb2b412 100644
--- a/ql/src/test/results/clientpositive/llap/vector_orderby_5.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_orderby_5.q.out
@@ -145,9 +145,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMaxLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 7
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: bo (type: boolean)
                       mode: hash
@@ -188,9 +190,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMaxLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_outer_join1.q.out b/ql/src/test/results/clientpositive/llap/vector_outer_join1.q.out
index 2857c6c999..f64e7393d9 100644
--- a/ql/src/test/results/clientpositive/llap/vector_outer_join1.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_outer_join1.q.out
@@ -736,8 +736,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFCountStar(*) -> bigint, VectorUDAFSumLong(col 0) -> bigint
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1]
                           mode: hash
                           outputColumnNames: _col0, _col1
@@ -872,8 +874,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint, VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/llap/vector_outer_join2.q.out b/ql/src/test/results/clientpositive/llap/vector_outer_join2.q.out
index ca38df592f..c24a2d0e6e 100644
--- a/ql/src/test/results/clientpositive/llap/vector_outer_join2.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_outer_join2.q.out
@@ -318,8 +318,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFCountStar(*) -> bigint, VectorUDAFSumLong(col 3) -> bigint
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1]
                           mode: hash
                           outputColumnNames: _col0, _col1
@@ -454,8 +456,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint, VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/llap/vector_partition_diff_num_cols.q.out b/ql/src/test/results/clientpositive/llap/vector_partition_diff_num_cols.q.out
index f963a62449..4683b4bb62 100644
--- a/ql/src/test/results/clientpositive/llap/vector_partition_diff_num_cols.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_partition_diff_num_cols.q.out
@@ -115,8 +115,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -154,8 +156,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -284,8 +288,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -323,8 +329,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -453,8 +461,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -492,8 +502,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -609,8 +621,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -648,8 +662,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -765,8 +781,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -804,8 +822,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/llap/vector_partitioned_date_time.q.out b/ql/src/test/results/clientpositive/llap/vector_partitioned_date_time.q.out
index e8444fcbf4..c5f7128d9d 100644
--- a/ql/src/test/results/clientpositive/llap/vector_partitioned_date_time.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_partitioned_date_time.q.out
@@ -454,9 +454,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 2
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: fl_date (type: date)
                       mode: hash
@@ -497,9 +499,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: date)
                 mode: mergepartial
@@ -1386,9 +1390,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 5
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: fl_date (type: date)
                       mode: hash
@@ -1429,9 +1435,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: date)
                 mode: mergepartial
@@ -2342,9 +2350,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 5
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: fl_time (type: timestamp)
                       mode: hash
@@ -2385,9 +2395,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: timestamp)
                 mode: mergepartial
@@ -2882,9 +2894,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 2
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: fl_date (type: date)
                       mode: hash
@@ -2925,9 +2939,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: date)
                 mode: mergepartial
@@ -3814,9 +3830,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 5
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: fl_date (type: date)
                       mode: hash
@@ -3857,9 +3875,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: date)
                 mode: mergepartial
@@ -4770,9 +4790,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 5
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: fl_time (type: timestamp)
                       mode: hash
@@ -4813,9 +4835,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: timestamp)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_reduce_groupby_decimal.q.out b/ql/src/test/results/clientpositive/llap/vector_reduce_groupby_decimal.q.out
index 7348af88f2..579bb61d38 100644
--- a/ql/src/test/results/clientpositive/llap/vector_reduce_groupby_decimal.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_reduce_groupby_decimal.q.out
@@ -65,9 +65,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinDecimal(col 2) -> decimal(20,10)
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 1, col 2, col 3
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: cint (type: int), cdouble (type: double), cdecimal1 (type: decimal(20,10)), cdecimal2 (type: decimal(23,14))
                       mode: hash
@@ -109,9 +111,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinDecimal(col 4) -> decimal(20,10)
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1, col 2, col 3
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: int), KEY._col1 (type: double), KEY._col2 (type: decimal(20,10)), KEY._col3 (type: decimal(23,14))
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_string_concat.q.out b/ql/src/test/results/clientpositive/llap/vector_string_concat.q.out
index fb9e1211ce..d5331ec830 100644
--- a/ql/src/test/results/clientpositive/llap/vector_string_concat.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_string_concat.q.out
@@ -356,9 +356,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 19
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: string)
                       mode: hash
@@ -397,9 +399,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vector_tablesample_rows.q.out b/ql/src/test/results/clientpositive/llap/vector_tablesample_rows.q.out
index b0780686e8..bb89dd7cc6 100644
--- a/ql/src/test/results/clientpositive/llap/vector_tablesample_rows.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_tablesample_rows.q.out
@@ -1,7 +1,7 @@
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 select 'key1', 'value1' from alltypesorc tablesample (1 rows)
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 select 'key1', 'value1' from alltypesorc tablesample (1 rows)
 POSTHOOK: type: QUERY
 Explain
@@ -56,6 +56,12 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: []
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string
 
   Stage: Stage-0
     Fetch Operator
@@ -81,11 +87,11 @@ POSTHOOK: query: create table decimal_2 (t decimal(18,9)) stored as orc
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: database:default
 POSTHOOK: Output: default@decimal_2
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 insert overwrite table decimal_2
   select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows)
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 insert overwrite table decimal_2
   select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows)
 POSTHOOK: type: QUERY
@@ -144,6 +150,12 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: []
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: decimal(18,9)
 
   Stage: Stage-2
     Dependency Collection
@@ -191,10 +203,10 @@ POSTHOOK: query: drop table decimal_2
 POSTHOOK: type: DROPTABLE
 POSTHOOK: Input: default@decimal_2
 POSTHOOK: Output: default@decimal_2
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 select count(1) from (select * from (Select 1 a) x order by x.a) y
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 select count(1) from (select * from (Select 1 a) x order by x.a) y
 POSTHOOK: type: QUERY
 Explain
@@ -237,10 +249,17 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: KEY.reducesinkkey0:int
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint
             Reduce Operator Tree:
               Select Operator
                 Select Vectorization:
@@ -253,8 +272,10 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 1:long) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: [0]
                   mode: hash
                   outputColumnNames: _col0
@@ -263,8 +284,10 @@ STAGE PLANS:
                     sort order: 
                     Reduce Sink Vectorization:
                         className: VectorReduceSinkEmptyKeyOperator
+                        keyColumns: []
                         native: true
                         nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        valueColumns: [0]
                     Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                     value expressions: _col0 (type: bigint)
         Reducer 3 
@@ -272,18 +295,26 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0)
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -315,10 +346,10 @@ POSTHOOK: Input: _dummy_database@_dummy_table
 #### A masked pattern was here ####
 _c0
 1
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 create temporary table dual as select 1
 PREHOOK: type: CREATETABLE_AS_SELECT
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 create temporary table dual as select 1
 POSTHOOK: type: CREATETABLE_AS_SELECT
 Explain
diff --git a/ql/src/test/results/clientpositive/llap/vector_udf1.q.out b/ql/src/test/results/clientpositive/llap/vector_udf1.q.out
index 0dd278e24c..16edaacf94 100644
--- a/ql/src/test/results/clientpositive/llap/vector_udf1.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_udf1.q.out
@@ -30,20 +30,24 @@ POSTHOOK: Lineage: varchar_udf_1.d1 SIMPLE []
 POSTHOOK: Lineage: varchar_udf_1.d2 SIMPLE []
 POSTHOOK: Lineage: varchar_udf_1.d3 EXPRESSION []
 POSTHOOK: Lineage: varchar_udf_1.d4 EXPRESSION []
-PREHOOK: query: explain
-select 
+PREHOOK: query: explain vectorization detail
+select
   concat(c1, c2),
   concat(c3, c4),
   concat(c1, c2) = concat(c3, c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
-select 
+POSTHOOK: query: explain vectorization detail
+select
   concat(c1, c2),
   concat(c3, c4),
   concat(c1, c2) = concat(c3, c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -58,15 +62,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: concat(c1, c2) (type: string), concat(c3, c4) (type: varchar(30)), (concat(c1, c2) = UDFToString(concat(c3, c4))) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 13]
+                        selectExpressions: StringGroupConcatColCol(col 0, col 1) -> 8:String_Family, StringGroupConcatColCol(col 2, col 3) -> 9:String_Family, StringGroupColEqualStringGroupColumn(col 10, col 12)(children: StringGroupConcatColCol(col 0, col 1) -> 10:String_Family, CastStringGroupToString(col 11)(children: StringGroupConcatColCol(col 2, col 3) -> 11:String_Family) -> 12:String) -> 13:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -74,6 +92,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [0, 1, 2, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -81,7 +113,7 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-PREHOOK: query: select 
+PREHOOK: query: select
   concat(c1, c2),
   concat(c3, c4),
   concat(c1, c2) = concat(c3, c4)
@@ -89,7 +121,7 @@ from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
-POSTHOOK: query: select 
+POSTHOOK: query: select
   concat(c1, c2),
   concat(c3, c4),
   concat(c1, c2) = concat(c3, c4)
@@ -98,20 +130,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 238val_238	238val_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   upper(c2),
   upper(c4),
   upper(c2) = upper(c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   upper(c2),
   upper(c4),
   upper(c2) = upper(c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -126,15 +162,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: upper(c2) (type: string), upper(c4) (type: varchar(20)), (upper(c2) = UDFToString(upper(c4))) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 13]
+                        selectExpressions: StringUpper(col 1) -> 8:String, StringUpper(col 3) -> 9:String, StringGroupColEqualStringGroupColumn(col 10, col 12)(children: StringUpper(col 1) -> 10:String, CastStringGroupToString(col 11)(children: StringUpper(col 3) -> 11:String) -> 12:String) -> 13:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -142,6 +192,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -166,20 +230,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 VAL_238	VAL_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   lower(c2),
   lower(c4),
   lower(c2) = lower(c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   lower(c2),
   lower(c4),
   lower(c2) = lower(c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -194,15 +262,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: lower(c2) (type: string), lower(c4) (type: varchar(20)), (lower(c2) = UDFToString(lower(c4))) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 13]
+                        selectExpressions: StringLower(col 1) -> 8:String, StringLower(col 3) -> 9:String, StringGroupColEqualStringGroupColumn(col 10, col 12)(children: StringLower(col 1) -> 10:String, CastStringGroupToString(col 11)(children: StringLower(col 3) -> 11:String) -> 12:String) -> 13:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -210,6 +292,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -234,20 +330,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 val_238	val_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   ascii(c2),
   ascii(c4),
   ascii(c2) = ascii(c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   ascii(c2),
   ascii(c4),
   ascii(c2) = ascii(c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -262,15 +362,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: ascii(c2) (type: int), ascii(c4) (type: int), (ascii(c2) = ascii(c4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(ascii(c2)) -> 8:int, VectorUDFAdaptor(ascii(c4)) -> 9:int, LongColEqualLongColumn(col 10, col 11)(children: VectorUDFAdaptor(ascii(c2)) -> 10:int, VectorUDFAdaptor(ascii(c4)) -> 11:int) -> 12:long
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -278,6 +392,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint, bigint, bigint, bigint, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -302,20 +430,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 118	118	true
-PREHOOK: query: explain
-select 
+PREHOOK: query: explain vectorization detail
+select
   concat_ws('|', c1, c2),
   concat_ws('|', c3, c4),
   concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
-select 
+POSTHOOK: query: explain vectorization detail
+select
   concat_ws('|', c1, c2),
   concat_ws('|', c3, c4),
   concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -330,15 +462,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: concat_ws('|', c1, c2) (type: string), concat_ws('|', c3, c4) (type: string), (concat_ws('|', c1, c2) = concat_ws('|', c3, c4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(concat_ws('|', c1, c2)) -> 8:string, VectorUDFAdaptor(concat_ws('|', c3, c4)) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: VectorUDFAdaptor(concat_ws('|', c1, c2)) -> 10:string, VectorUDFAdaptor(concat_ws('|', c3, c4)) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -346,6 +492,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [0, 1, 2, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -353,7 +513,7 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-PREHOOK: query: select 
+PREHOOK: query: select
   concat_ws('|', c1, c2),
   concat_ws('|', c3, c4),
   concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
@@ -361,7 +521,7 @@ from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
-POSTHOOK: query: select 
+POSTHOOK: query: select
   concat_ws('|', c1, c2),
   concat_ws('|', c3, c4),
   concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
@@ -370,20 +530,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 238|val_238	238|val_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   decode(encode(c2, 'US-ASCII'), 'US-ASCII'),
   decode(encode(c4, 'US-ASCII'), 'US-ASCII'),
   decode(encode(c2, 'US-ASCII'), 'US-ASCII') = decode(encode(c4, 'US-ASCII'), 'US-ASCII')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   decode(encode(c2, 'US-ASCII'), 'US-ASCII'),
   decode(encode(c4, 'US-ASCII'), 'US-ASCII'),
   decode(encode(c2, 'US-ASCII'), 'US-ASCII') = decode(encode(c4, 'US-ASCII'), 'US-ASCII')
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -398,15 +562,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: decode(encode(c2,'US-ASCII'),'US-ASCII') (type: string), decode(encode(c4,'US-ASCII'),'US-ASCII') (type: string), (decode(encode(c2,'US-ASCII'),'US-ASCII') = decode(encode(c4,'US-ASCII'),'US-ASCII')) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [9, 10, 13]
+                        selectExpressions: VectorUDFAdaptor(decode(encode(c2,'US-ASCII'),'US-ASCII'))(children: VectorUDFAdaptor(encode(c2,'US-ASCII')) -> 8:binary) -> 9:string, VectorUDFAdaptor(decode(encode(c4,'US-ASCII'),'US-ASCII'))(children: VectorUDFAdaptor(encode(c4,'US-ASCII')) -> 8:binary) -> 10:string, StringGroupColEqualStringGroupColumn(col 11, col 12)(children: VectorUDFAdaptor(decode(encode(c2,'US-ASCII'),'US-ASCII'))(children: VectorUDFAdaptor(encode(c2,'US-ASCII')) -> 8:binary) -> 11:string, VectorUDFAdaptor(decode(encode(c4,'US-ASCII'),'US-ASCII'))(children: VectorUDFAdaptor(encode(c4,'US-ASCII')) -> 8:binary) -> 12:string) -> 13:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -414,6 +592,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -438,20 +630,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 val_238	val_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   instr(c2, '_'),
   instr(c4, '_'),
   instr(c2, '_') = instr(c4, '_')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   instr(c2, '_'),
   instr(c4, '_'),
   instr(c2, '_') = instr(c4, '_')
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -466,15 +662,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: instr(c2, '_') (type: int), instr(c4, '_') (type: int), (instr(c2, '_') = instr(c4, '_')) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(instr(c2, '_')) -> 8:int, VectorUDFAdaptor(instr(c4, '_')) -> 9:int, LongColEqualLongColumn(col 10, col 11)(children: VectorUDFAdaptor(instr(c2, '_')) -> 10:int, VectorUDFAdaptor(instr(c4, '_')) -> 11:int) -> 12:long
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -482,6 +692,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint, bigint, bigint, bigint, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -506,20 +730,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 4	4	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   replace(c1, '_', c2),
   replace(c3, '_', c4),
   replace(c1, '_', c2) = replace(c3, '_', c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   replace(c1, '_', c2),
   replace(c3, '_', c4),
   replace(c1, '_', c2) = replace(c3, '_', c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -534,15 +762,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: replace(c1, '_', c2) (type: string), replace(c3, '_', c4) (type: string), (replace(c1, '_', c2) = replace(c3, '_', c4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(replace(c1, '_', c2)) -> 8:string, VectorUDFAdaptor(replace(c3, '_', c4)) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: VectorUDFAdaptor(replace(c1, '_', c2)) -> 10:string, VectorUDFAdaptor(replace(c3, '_', c4)) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -550,6 +792,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [0, 1, 2, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -574,20 +830,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 238	238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   reverse(c2),
   reverse(c4),
   reverse(c2) = reverse(c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   reverse(c2),
   reverse(c4),
   reverse(c2) = reverse(c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -602,15 +862,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: reverse(c2) (type: string), reverse(c4) (type: string), (reverse(c2) = reverse(c4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(reverse(c2)) -> 8:string, VectorUDFAdaptor(reverse(c4)) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: VectorUDFAdaptor(reverse(c2)) -> 10:string, VectorUDFAdaptor(reverse(c4)) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -618,6 +892,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -642,20 +930,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 832_lav	832_lav	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   next_day(d1, 'TU'),
   next_day(d4, 'WE'),
   next_day(d1, 'TU') = next_day(d4, 'WE')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   next_day(d1, 'TU'),
   next_day(d4, 'WE'),
   next_day(d1, 'TU') = next_day(d4, 'WE')
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -670,15 +962,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: next_day(d1, 'TU') (type: string), next_day(d4, 'WE') (type: string), (next_day(d1, 'TU') = next_day(d4, 'WE')) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(next_day(d1, 'TU')) -> 8:string, VectorUDFAdaptor(next_day(d4, 'WE')) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: VectorUDFAdaptor(next_day(d1, 'TU')) -> 10:string, VectorUDFAdaptor(next_day(d4, 'WE')) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -686,6 +992,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [4, 7]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -710,20 +1030,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 2015-01-20	2017-01-18	false
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   months_between(d1, d3),
   months_between(d2, d4),
   months_between(d1, d3) = months_between(d2, d4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   months_between(d1, d3),
   months_between(d2, d4),
   months_between(d1, d3) = months_between(d2, d4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -738,15 +1062,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: months_between(d1, d3) (type: double), months_between(d2, d4) (type: double), (months_between(d1, d3) = months_between(d2, d4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(months_between(d1, d3)) -> 8:double, VectorUDFAdaptor(months_between(d2, d4)) -> 9:double, DoubleColEqualDoubleColumn(col 10, col 11)(children: VectorUDFAdaptor(months_between(d1, d3)) -> 10:double, VectorUDFAdaptor(months_between(d2, d4)) -> 11:double) -> 12:long
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -754,6 +1092,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [4, 5, 6, 7]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, double, double, double, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -778,20 +1130,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 -23.90322581	-23.90322581	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   length(c2),
   length(c4),
   length(c2) = length(c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   length(c2),
   length(c4),
   length(c2) = length(c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -806,15 +1162,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: length(c2) (type: int), length(c4) (type: int), (length(c2) = length(c4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: StringLength(col 1) -> 8:Long, StringLength(col 3) -> 9:Long, LongColEqualLongColumn(col 10, col 11)(children: StringLength(col 1) -> 10:Long, StringLength(col 3) -> 11:Long) -> 12:long
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -822,6 +1192,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint, bigint, bigint, bigint, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -846,20 +1230,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 7	7	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   locate('a', 'abcdabcd', 3),
   locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3),
   locate('a', 'abcdabcd', 3) = locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   locate('a', 'abcdabcd', 3),
   locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3),
   locate('a', 'abcdabcd', 3) = locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -874,15 +1262,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: 5 (type: int), 5 (type: int), true (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 10]
+                        selectExpressions: ConstantVectorExpression(val 5) -> 8:long, ConstantVectorExpression(val 5) -> 9:long, ConstantVectorExpression(val 1) -> 10:long
                     Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: COMPLETE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: COMPLETE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: COMPLETE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -890,6 +1292,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: []
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint, bigint, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -914,20 +1330,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 5	5	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   lpad(c2, 15, ' '),
   lpad(c4, 15, ' '),
   lpad(c2, 15, ' ') = lpad(c4, 15, ' ')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   lpad(c2, 15, ' '),
   lpad(c4, 15, ' '),
   lpad(c2, 15, ' ') = lpad(c4, 15, ' ')
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -942,15 +1362,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: lpad(c2, 15, ' ') (type: string), lpad(c4, 15, ' ') (type: string), (lpad(c2, 15, ' ') = lpad(c4, 15, ' ')) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(lpad(c2, 15, ' ')) -> 8:string, VectorUDFAdaptor(lpad(c4, 15, ' ')) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: VectorUDFAdaptor(lpad(c2, 15, ' ')) -> 10:string, VectorUDFAdaptor(lpad(c4, 15, ' ')) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -958,6 +1392,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -982,20 +1430,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
         val_238	        val_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   ltrim(c2),
   ltrim(c4),
   ltrim(c2) = ltrim(c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   ltrim(c2),
   ltrim(c4),
   ltrim(c2) = ltrim(c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1010,15 +1462,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: ltrim(c2) (type: string), ltrim(c4) (type: string), (ltrim(c2) = ltrim(c4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: StringLTrim(col 1) -> 8:String, StringLTrim(col 3) -> 9:String, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: StringLTrim(col 1) -> 10:String, StringLTrim(col 3) -> 11:String) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1026,6 +1492,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -1050,20 +1530,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 val_238	val_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   c2 regexp 'val',
   c4 regexp 'val',
   (c2 regexp 'val') = (c4 regexp 'val')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   c2 regexp 'val',
   c4 regexp 'val',
   (c2 regexp 'val') = (c4 regexp 'val')
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1078,15 +1562,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: c2 regexp 'val' (type: boolean), c4 regexp 'val' (type: boolean), (c2 regexp 'val' = c4 regexp 'val') (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(c2 regexp 'val') -> 8:boolean, VectorUDFAdaptor(c4 regexp 'val') -> 9:boolean, LongColEqualLongColumn(col 10, col 11)(children: VectorUDFAdaptor(c2 regexp 'val') -> 10:boolean, VectorUDFAdaptor(c4 regexp 'val') -> 11:boolean) -> 12:long
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1094,6 +1592,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint, bigint, bigint, bigint, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -1118,20 +1630,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 true	true	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   regexp_extract(c2, 'val_([0-9]+)', 1),
   regexp_extract(c4, 'val_([0-9]+)', 1),
   regexp_extract(c2, 'val_([0-9]+)', 1) = regexp_extract(c4, 'val_([0-9]+)', 1)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   regexp_extract(c2, 'val_([0-9]+)', 1),
   regexp_extract(c4, 'val_([0-9]+)', 1),
   regexp_extract(c2, 'val_([0-9]+)', 1) = regexp_extract(c4, 'val_([0-9]+)', 1)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1146,15 +1662,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: regexp_extract(c2, 'val_([0-9]+)', 1) (type: string), regexp_extract(c4, 'val_([0-9]+)', 1) (type: string), (regexp_extract(c2, 'val_([0-9]+)', 1) = regexp_extract(c4, 'val_([0-9]+)', 1)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(regexp_extract(c2, 'val_([0-9]+)', 1)) -> 8:string, VectorUDFAdaptor(regexp_extract(c4, 'val_([0-9]+)', 1)) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: VectorUDFAdaptor(regexp_extract(c2, 'val_([0-9]+)', 1)) -> 10:string, VectorUDFAdaptor(regexp_extract(c4, 'val_([0-9]+)', 1)) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1162,6 +1692,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -1186,20 +1730,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 238	238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   regexp_replace(c2, 'val', 'replaced'),
   regexp_replace(c4, 'val', 'replaced'),
   regexp_replace(c2, 'val', 'replaced') = regexp_replace(c4, 'val', 'replaced')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   regexp_replace(c2, 'val', 'replaced'),
   regexp_replace(c4, 'val', 'replaced'),
   regexp_replace(c2, 'val', 'replaced') = regexp_replace(c4, 'val', 'replaced')
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1214,15 +1762,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: regexp_replace(c2, 'val', 'replaced') (type: string), regexp_replace(c4, 'val', 'replaced') (type: string), (regexp_replace(c2, 'val', 'replaced') = regexp_replace(c4, 'val', 'replaced')) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(regexp_replace(c2, 'val', 'replaced')) -> 8:string, VectorUDFAdaptor(regexp_replace(c4, 'val', 'replaced')) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: VectorUDFAdaptor(regexp_replace(c2, 'val', 'replaced')) -> 10:string, VectorUDFAdaptor(regexp_replace(c4, 'val', 'replaced')) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1230,6 +1792,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -1254,20 +1830,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 replaced_238	replaced_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   reverse(c2),
   reverse(c4),
   reverse(c2) = reverse(c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   reverse(c2),
   reverse(c4),
   reverse(c2) = reverse(c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1282,15 +1862,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: reverse(c2) (type: string), reverse(c4) (type: string), (reverse(c2) = reverse(c4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(reverse(c2)) -> 8:string, VectorUDFAdaptor(reverse(c4)) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: VectorUDFAdaptor(reverse(c2)) -> 10:string, VectorUDFAdaptor(reverse(c4)) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1298,6 +1892,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -1322,20 +1930,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 832_lav	832_lav	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   rpad(c2, 15, ' '),
   rpad(c4, 15, ' '),
   rpad(c2, 15, ' ') = rpad(c4, 15, ' ')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   rpad(c2, 15, ' '),
   rpad(c4, 15, ' '),
   rpad(c2, 15, ' ') = rpad(c4, 15, ' ')
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1350,15 +1962,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: rpad(c2, 15, ' ') (type: string), rpad(c4, 15, ' ') (type: string), (rpad(c2, 15, ' ') = rpad(c4, 15, ' ')) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: VectorUDFAdaptor(rpad(c2, 15, ' ')) -> 8:string, VectorUDFAdaptor(rpad(c4, 15, ' ')) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: VectorUDFAdaptor(rpad(c2, 15, ' ')) -> 10:string, VectorUDFAdaptor(rpad(c4, 15, ' ')) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1366,6 +1992,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -1390,20 +2030,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 val_238        	val_238        	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   rtrim(c2),
   rtrim(c4),
   rtrim(c2) = rtrim(c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   rtrim(c2),
   rtrim(c4),
   rtrim(c2) = rtrim(c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1418,15 +2062,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: rtrim(c2) (type: string), rtrim(c4) (type: string), (rtrim(c2) = rtrim(c4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: StringRTrim(col 1) -> 8:String, StringRTrim(col 3) -> 9:String, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: StringRTrim(col 1) -> 10:String, StringRTrim(col 3) -> 11:String) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1434,6 +2092,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -1458,18 +2130,22 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 val_238	val_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   sentences('See spot run.  See jane run.'),
   sentences(cast('See spot run.  See jane run.' as varchar(50)))
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   sentences('See spot run.  See jane run.'),
   sentences(cast('See spot run.  See jane run.' as varchar(50)))
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1484,22 +2160,50 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: sentences('See spot run.  See jane run.') (type: array<array<string>>), sentences('See spot run.  See jane run.') (type: array<array<string>>)
                     outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9]
+                        selectExpressions: VectorUDFAdaptor(sentences('See spot run.  See jane run.')) -> 8:array<array<string>>, VectorUDFAdaptor(sentences('See spot run.  See jane run.')) -> 9:array<array<string>>
                     Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                             output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            Execution mode: llap
+            Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: []
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: array<array<string>>, array<array<string>>
 
   Stage: Stage-0
     Fetch Operator
@@ -1522,18 +2226,22 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 [["See","spot","run"],["See","jane","run"]]	[["See","spot","run"],["See","jane","run"]]
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   split(c2, '_'),
   split(c4, '_')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   split(c2, '_'),
   split(c4, '_')
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1548,22 +2256,50 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: split(c2, '_') (type: array<string>), split(c4, '_') (type: array<string>)
                     outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9]
+                        selectExpressions: VectorUDFAdaptor(split(c2, '_')) -> 8:array<string>, VectorUDFAdaptor(split(c4, '_')) -> 9:array<string>
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                             output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            Execution mode: llap
+            Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: array<string>, array<string>
 
   Stage: Stage-0
     Fetch Operator
@@ -1586,18 +2322,22 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 ["val","238"]	["val","238"]
-PREHOOK: query: explain
-select 
+PREHOOK: query: explain vectorization detail
+select
   str_to_map('a:1,b:2,c:3',',',':'),
   str_to_map(cast('a:1,b:2,c:3' as varchar(20)),',',':')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
-select 
+POSTHOOK: query: explain vectorization detail
+select
   str_to_map('a:1,b:2,c:3',',',':'),
   str_to_map(cast('a:1,b:2,c:3' as varchar(20)),',',':')
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1612,22 +2352,50 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: str_to_map('a:1,b:2,c:3',',',':') (type: map<string,string>), str_to_map('a:1,b:2,c:3',',',':') (type: map<string,string>)
                     outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9]
+                        selectExpressions: VectorUDFAdaptor(str_to_map('a:1,b:2,c:3',',',':')) -> 8:map<string,string>, VectorUDFAdaptor(str_to_map('a:1,b:2,c:3',',',':')) -> 9:map<string,string>
                     Statistics: Num rows: 1 Data size: 1508 Basic stats: COMPLETE Column stats: COMPLETE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 1508 Basic stats: COMPLETE Column stats: COMPLETE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 1508 Basic stats: COMPLETE Column stats: COMPLETE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                             output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            Execution mode: llap
+            Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: []
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: map<string,string>, map<string,string>
 
   Stage: Stage-0
     Fetch Operator
@@ -1635,14 +2403,14 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-PREHOOK: query: select 
+PREHOOK: query: select
   str_to_map('a:1,b:2,c:3',',',':'),
   str_to_map(cast('a:1,b:2,c:3' as varchar(20)),',',':')
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
-POSTHOOK: query: select 
+POSTHOOK: query: select
   str_to_map('a:1,b:2,c:3',',',':'),
   str_to_map(cast('a:1,b:2,c:3' as varchar(20)),',',':')
 from varchar_udf_1 limit 1
@@ -1650,20 +2418,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 {"a":"1","b":"2","c":"3"}	{"a":"1","b":"2","c":"3"}
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   substr(c2, 1, 3),
   substr(c4, 1, 3),
   substr(c2, 1, 3) = substr(c4, 1, 3)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   substr(c2, 1, 3),
   substr(c4, 1, 3),
   substr(c2, 1, 3) = substr(c4, 1, 3)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1678,15 +2450,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: substr(c2, 1, 3) (type: string), substr(c4, 1, 3) (type: string), (substr(c2, 1, 3) = substr(c4, 1, 3)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: StringSubstrColStartLen(col 1, start 0, length 3) -> 8:string, StringSubstrColStartLen(col 3, start 0, length 3) -> 9:string, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: StringSubstrColStartLen(col 1, start 0, length 3) -> 10:string, StringSubstrColStartLen(col 3, start 0, length 3) -> 11:string) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1694,6 +2480,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -1718,20 +2518,24 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 val	val	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   trim(c2),
   trim(c4),
   trim(c2) = trim(c4)
 from varchar_udf_1 limit 1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   trim(c2),
   trim(c4),
   trim(c2) = trim(c4)
 from varchar_udf_1 limit 1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1746,15 +2550,29 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: trim(c2) (type: string), trim(c4) (type: string), (trim(c2) = trim(c4)) (type: boolean)
                     outputColumnNames: _col0, _col1, _col2
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [8, 9, 12]
+                        selectExpressions: StringTrim(col 1) -> 8:String, StringTrim(col 3) -> 9:String, StringGroupColEqualStringGroupColumn(col 10, col 11)(children: StringTrim(col 1) -> 10:String, StringTrim(col 3) -> 11:String) -> 12:boolean
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Limit
                       Number of rows: 1
+                      Limit Vectorization:
+                          className: VectorLimitOperator
+                          native: true
                       Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                       File Output Operator
                         compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
                         Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                         table:
                             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1762,6 +2580,20 @@ STAGE PLANS:
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string, string, string, bigint
 
   Stage: Stage-0
     Fetch Operator
@@ -1786,18 +2618,22 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 val_238	val_238	true
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   compute_stats(c2, 16),
   compute_stats(c4, 16)
 from varchar_udf_1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   compute_stats(c2, 16),
   compute_stats(c4, 16)
 from varchar_udf_1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1830,8 +2666,19 @@ STAGE PLANS:
                         value expressions: _col0 (type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>), _col1 (type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>)
             Execution mode: llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF compute_stats not supported
+                vectorized: false
         Reducer 2 
             Execution mode: llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF compute_stats not supported
+                vectorized: false
             Reduce Operator Tree:
               Group By Operator
                 aggregations: compute_stats(VALUE._col0), compute_stats(VALUE._col1)
@@ -1867,18 +2714,22 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 {"columntype":"String","maxlength":7,"avglength":7.0,"countnulls":0,"numdistinctvalues":1,"ndvbitvector":"{0}{3}{2}{3}{1}{0}{2}{0}{1}{0}{0}{1}{3}{2}{0}{3}"}	{"columntype":"String","maxlength":7,"avglength":7.0,"countnulls":0,"numdistinctvalues":1,"ndvbitvector":"{0}{3}{2}{3}{1}{0}{2}{0}{1}{0}{0}{1}{3}{2}{0}{3}"}
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   min(c2),
   min(c4)
 from varchar_udf_1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   min(c2),
   min(c4)
 from varchar_udf_1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1896,31 +2747,89 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: c2 (type: string), c4 (type: varchar(20))
                     outputColumnNames: c2, c4
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [1, 3]
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
                       aggregations: min(c2), min(c4)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFMinString(col 1) -> string, VectorUDAFMinString(col 3) -> string
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0, 1]
                       mode: hash
                       outputColumnNames: _col0, _col1
                       Statistics: Num rows: 1 Data size: 288 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1]
                         Statistics: Num rows: 1 Data size: 288 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: string), _col1 (type: varchar(20))
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    dataColumns: VALUE._col0:string, VALUE._col1:varchar(20)
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: min(VALUE._col0), min(VALUE._col1)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMinString(col 0) -> string, VectorUDAFMinString(col 1) -> string
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 1 Data size: 288 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 1 Data size: 288 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -1948,18 +2857,22 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@varchar_udf_1
 #### A masked pattern was here ####
 val_238	val_238
-PREHOOK: query: explain
+PREHOOK: query: explain vectorization detail
 select
   max(c2),
   max(c4)
 from varchar_udf_1
 PREHOOK: type: QUERY
-POSTHOOK: query: explain
+POSTHOOK: query: explain vectorization detail
 select
   max(c2),
   max(c4)
 from varchar_udf_1
 POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-0 depends on stages: Stage-1
@@ -1977,31 +2890,89 @@ STAGE PLANS:
                 TableScan
                   alias: varchar_udf_1
                   Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                   Select Operator
                     expressions: c2 (type: string), c4 (type: varchar(20))
                     outputColumnNames: c2, c4
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [1, 3]
                     Statistics: Num rows: 1 Data size: 732 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
                       aggregations: max(c2), max(c4)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFMaxString(col 1) -> string, VectorUDAFMaxString(col 3) -> string
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0, 1]
                       mode: hash
                       outputColumnNames: _col0, _col1
                       Statistics: Num rows: 1 Data size: 288 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1]
                         Statistics: Num rows: 1 Data size: 288 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: string), _col1 (type: varchar(20))
             Execution mode: vectorized, llap
             LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 8
+                    includeColumns: [1, 3]
+                    dataColumns: c1:string, c2:string, c3:varchar(10), c4:varchar(20), d1:string, d2:string, d3:varchar(10), d4:varchar(10)
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    dataColumns: VALUE._col0:string, VALUE._col1:varchar(20)
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), max(VALUE._col1)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxString(col 0) -> string, VectorUDAFMaxString(col 1) -> string
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 1 Data size: 288 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 1 Data size: 288 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vector_when_case_null.q.out b/ql/src/test/results/clientpositive/llap/vector_when_case_null.q.out
index 28edb6f5b3..f137c63aa0 100644
--- a/ql/src/test/results/clientpositive/llap/vector_when_case_null.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_when_case_null.q.out
@@ -58,9 +58,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 5) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: string)
                       mode: hash
@@ -101,9 +103,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_0.q.out b/ql/src/test/results/clientpositive/llap/vectorization_0.q.out
index b44e749bfa..fba9c07350 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_0.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_0.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ctinyint) as c1,
        MAX(ctinyint),
        COUNT(ctinyint),
@@ -6,7 +6,7 @@ SELECT MIN(ctinyint) as c1,
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ctinyint) as c1,
        MAX(ctinyint),
        COUNT(ctinyint),
@@ -52,8 +52,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -62,8 +64,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3]
                         Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                         value expressions: _col0 (type: tinyint), _col1 (type: tinyint), _col2 (type: bigint), _col3 (type: bigint)
             Execution mode: vectorized, llap
@@ -76,23 +80,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:tinyint, VALUE._col1:tinyint, VALUE._col2:bigint, VALUE._col3:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3)
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFMaxLong(col 1) -> tinyint, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -102,8 +119,10 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: [1, 2, 3]
                   Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                   value expressions: _col1 (type: tinyint), _col2 (type: bigint), _col3 (type: bigint)
         Reducer 3 
@@ -111,10 +130,16 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: KEY.reducesinkkey0:tinyint, VALUE._col0:tinyint, VALUE._col1:bigint, VALUE._col2:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: tinyint), VALUE._col0 (type: tinyint), VALUE._col1 (type: bigint), VALUE._col2 (type: bigint)
@@ -160,12 +185,12 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -64	62	9173	12288
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(ctinyint) as c1
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(ctinyint) as c1
 FROM   alltypesorc
 ORDER BY c1
@@ -208,8 +233,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -218,8 +245,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0]
                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                         value expressions: _col0 (type: bigint)
             Execution mode: vectorized, llap
@@ -232,23 +261,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: sum(VALUE._col0)
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -258,18 +300,26 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: []
                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
         Reducer 3 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: KEY.reducesinkkey0:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: bigint)
@@ -375,18 +425,20 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:tinyint> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), variance(VALUE._col1), var_pop(VALUE._col2), var_samp(VALUE._col3), std(VALUE._col4), stddev(VALUE._col5), stddev_pop(VALUE._col6), stddev_samp(VALUE._col7)
@@ -455,7 +507,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -4.344925324321378	1158.3003004768184	1158.3003004768184	1158.4265870337827	34.033811136527426	34.033811136527426	34.033811136527426	34.03566639620536
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cbigint) as c1,
        MAX(cbigint),
        COUNT(cbigint),
@@ -463,7 +515,7 @@ SELECT MIN(cbigint) as c1,
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cbigint) as c1,
        MAX(cbigint),
        COUNT(cbigint),
@@ -509,8 +561,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinLong(col 3) -> bigint, VectorUDAFMaxLong(col 3) -> bigint, VectorUDAFCount(col 3) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -519,8 +573,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3]
                         Statistics: Num rows: 1 Data size: 32 Basic stats: COMPLETE Column stats: COMPLETE
                         value expressions: _col0 (type: bigint), _col1 (type: bigint), _col2 (type: bigint), _col3 (type: bigint)
             Execution mode: vectorized, llap
@@ -533,23 +589,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [3]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:bigint, VALUE._col1:bigint, VALUE._col2:bigint, VALUE._col3:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3)
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> bigint, VectorUDAFMaxLong(col 1) -> bigint, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -559,8 +628,10 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: [1, 2, 3]
                   Statistics: Num rows: 1 Data size: 32 Basic stats: COMPLETE Column stats: COMPLETE
                   value expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: bigint)
         Reducer 3 
@@ -568,10 +639,16 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: KEY.reducesinkkey0:bigint, VALUE._col0:bigint, VALUE._col1:bigint, VALUE._col2:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: bigint), VALUE._col0 (type: bigint), VALUE._col1 (type: bigint), VALUE._col2 (type: bigint)
@@ -617,12 +694,12 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -2147311592	2145498388	9173	12288
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cbigint) as c1
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cbigint) as c1
 FROM   alltypesorc
 ORDER BY c1
@@ -665,8 +742,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -675,8 +754,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0]
                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                         value expressions: _col0 (type: bigint)
             Execution mode: vectorized, llap
@@ -689,23 +770,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [3]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: sum(VALUE._col0)
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -715,18 +809,26 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: []
                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
         Reducer 3 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: KEY.reducesinkkey0:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: bigint)
@@ -832,18 +934,20 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:bigint> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), variance(VALUE._col1), var_pop(VALUE._col2), var_samp(VALUE._col3), std(VALUE._col4), stddev(VALUE._col5), stddev_pop(VALUE._col6), stddev_samp(VALUE._col7)
@@ -912,7 +1016,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -1.8515862077935246E8	2.07689300543081907E18	2.07689300543081907E18	2.07711944383088768E18	1.441142951074188E9	1.441142951074188E9	1.441142951074188E9	1.4412215110214279E9
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cfloat) as c1,
        MAX(cfloat),
        COUNT(cfloat),
@@ -920,7 +1024,7 @@ SELECT MIN(cfloat) as c1,
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cfloat) as c1,
        MAX(cfloat),
        COUNT(cfloat),
@@ -966,8 +1070,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinDouble(col 4) -> float, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFCount(col 4) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -976,8 +1082,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3]
                         Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                         value expressions: _col0 (type: float), _col1 (type: float), _col2 (type: bigint), _col3 (type: bigint)
             Execution mode: vectorized, llap
@@ -990,23 +1098,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [4]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:float, VALUE._col1:float, VALUE._col2:bigint, VALUE._col3:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3)
                 Group By Vectorization:
                     aggregators: VectorUDAFMinDouble(col 0) -> float, VectorUDAFMaxDouble(col 1) -> float, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -1016,8 +1137,10 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: [1, 2, 3]
                   Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                   value expressions: _col1 (type: float), _col2 (type: bigint), _col3 (type: bigint)
         Reducer 3 
@@ -1025,10 +1148,16 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: KEY.reducesinkkey0:float, VALUE._col0:float, VALUE._col1:bigint, VALUE._col2:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: float), VALUE._col0 (type: float), VALUE._col1 (type: bigint), VALUE._col2 (type: bigint)
@@ -1074,12 +1203,12 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -64.0	79.553	9173	12288
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cfloat) as c1
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cfloat) as c1
 FROM   alltypesorc
 ORDER BY c1
@@ -1122,8 +1251,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumDouble(col 4) -> double
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -1132,8 +1263,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0]
                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                         value expressions: _col0 (type: double)
             Execution mode: vectorized, llap
@@ -1146,23 +1279,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [4]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: sum(VALUE._col0)
                 Group By Vectorization:
                     aggregators: VectorUDAFSumDouble(col 0) -> double
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1172,18 +1318,26 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: []
                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
         Reducer 3 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: KEY.reducesinkkey0:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: double)
@@ -1289,18 +1443,20 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:float> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), variance(VALUE._col1), var_pop(VALUE._col2), var_samp(VALUE._col3), std(VALUE._col4), stddev(VALUE._col5), stddev_pop(VALUE._col6), stddev_samp(VALUE._col7)
@@ -1370,7 +1526,7 @@ POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -4.303895780321011	1163.8972588604984	1163.8972588604984	1164.0241556397025	34.115938487171924	34.115938487171924	34.115938487171924	34.11779822379666
 WARNING: Comparing a bigint and a double may result in a loss of precision.
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT AVG(cbigint),
        (-(AVG(cbigint))),
        (-6432 + AVG(cbigint)),
@@ -1397,7 +1553,7 @@ WHERE  (((cstring2 LIKE '%b%')
             AND ((cboolean2 = 1)
                  AND (3569 = ctinyint))))
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT AVG(cbigint),
        (-(AVG(cbigint))),
        (-6432 + AVG(cbigint)),
@@ -1466,17 +1622,24 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: avg(cbigint), stddev_pop(cbigint), var_samp(cbigint), count(), sum(cfloat), min(ctinyint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFAvgLong(col 3) -> struct<count:bigint,sum:double>, VectorUDAFStdPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFMinLong(col 0) -> tinyint
+                            aggregators: VectorUDAFAvgLong(col 3) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFStdPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFMinLong(col 0) -> tinyint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(col 3) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                         Statistics: Num rows: 1 Data size: 260 Basic stats: COMPLETE Column stats: COMPLETE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4, 5]
                           Statistics: Num rows: 1 Data size: 260 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col0 (type: struct<count:bigint,sum:double,input:bigint>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: bigint), _col4 (type: double), _col5 (type: tinyint)
             Execution mode: vectorized, llap
@@ -1484,30 +1647,60 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 7, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: decimal(13,3), double
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:bigint> of Column[VALUE._col0] not supported
-                vectorized: false
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,input:bigint>, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:bigint, VALUE._col4:double, VALUE._col5:tinyint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), stddev_pop(VALUE._col1), var_samp(VALUE._col2), count(VALUE._col3), sum(VALUE._col4), min(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFStdPopFinal(col 1) -> double, VectorUDAFVarSampFinal(col 2) -> double, VectorUDAFCountMerge(col 3) -> bigint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFMinLong(col 5) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                 Statistics: Num rows: 1 Data size: 44 Basic stats: COMPLETE Column stats: COMPLETE
                 Select Operator
                   expressions: _col0 (type: double), (- _col0) (type: double), (-6432.0 + _col0) (type: double), _col1 (type: double), (- (-6432.0 + _col0)) (type: double), ((- (-6432.0 + _col0)) + (-6432.0 + _col0)) (type: double), _col2 (type: double), (- (-6432.0 + _col0)) (type: double), (-6432.0 + (- (-6432.0 + _col0))) (type: double), (- (-6432.0 + _col0)) (type: double), ((- (-6432.0 + _col0)) / (- (-6432.0 + _col0))) (type: double), _col3 (type: bigint), _col4 (type: double), (_col2 % _col1) (type: double), (- _col2) (type: double), ((- (-6432.0 + _col0)) * (- _col0)) (type: double), _col5 (type: tinyint), (- _col5) (type: tinyint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 7, 1, 9, 11, 2, 10, 8, 13, 12, 3, 4, 14, 15, 18, 5, 19]
+                      selectExpressions: DoubleColUnaryMinus(col 0) -> 6:double, DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 7:double, DoubleColUnaryMinus(col 8)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 9:double, DoubleColAddDoubleColumn(col 10, col 8)(children: DoubleColUnaryMinus(col 8)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 10:double, DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 11:double, DoubleColUnaryMinus(col 8)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 10:double, DoubleScalarAddDoubleColumn(val -6432.0, col 12)(children: DoubleColUnaryMinus(col 8)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 12:double) -> 8:double, DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 12:double) -> 13:double, DoubleColDivideDoubleColumn(col 14, col 15)(children: DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 12:double) -> 14:double, DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 12:double) -> 15:double) -> 12:double, DoubleColModuloDoubleColumn(col 2, col 1) -> 14:double, DoubleColUnaryMinus(col 2) -> 15:double, DoubleColMultiplyDoubleColumn(col 17, col 16)(children: DoubleColUnaryMinus(col 16)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 16:double) -> 17:double, DoubleColUnaryMinus(col 0) -> 16:double) -> 18:double, LongColUnaryMinus(col 5) -> 19:long
                   Statistics: Num rows: 1 Data size: 136 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 136 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_1.q.out b/ql/src/test/results/clientpositive/llap/vectorization_1.q.out
index e0a434480b..4699c2e71a 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_1.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_1.q.out
@@ -1,3 +1,178 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT VAR_POP(ctinyint),
+       (VAR_POP(ctinyint) / -26.28),
+       SUM(cfloat),
+       (-1.389 + SUM(cfloat)),
+       (SUM(cfloat) * (-1.389 + SUM(cfloat))),
+       MAX(ctinyint),
+       (-((SUM(cfloat) * (-1.389 + SUM(cfloat))))),
+       MAX(cint),
+       (MAX(cint) * 79.553),
+       VAR_SAMP(cdouble),
+       (10.175 % (-((SUM(cfloat) * (-1.389 + SUM(cfloat)))))),
+       COUNT(cint),
+       (-563 % MAX(cint))
+FROM   alltypesorc
+WHERE  (((cdouble > ctinyint)
+         AND (cboolean2 > 0))
+        OR ((cbigint < ctinyint)
+            OR ((cint > cbigint)
+                OR (cboolean1 < 0))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT VAR_POP(ctinyint),
+       (VAR_POP(ctinyint) / -26.28),
+       SUM(cfloat),
+       (-1.389 + SUM(cfloat)),
+       (SUM(cfloat) * (-1.389 + SUM(cfloat))),
+       MAX(ctinyint),
+       (-((SUM(cfloat) * (-1.389 + SUM(cfloat))))),
+       MAX(cint),
+       (MAX(cint) * 79.553),
+       VAR_SAMP(cdouble),
+       (10.175 % (-((SUM(cfloat) * (-1.389 + SUM(cfloat)))))),
+       COUNT(cint),
+       (-563 % MAX(cint))
+FROM   alltypesorc
+WHERE  (((cdouble > ctinyint)
+         AND (cboolean2 > 0))
+        OR ((cbigint < ctinyint)
+            OR ((cint > cbigint)
+                OR (cboolean1 < 0))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 330276 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: FilterDoubleColGreaterDoubleColumn(col 5, col 12)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterLongColGreaterLongScalar(col 11, val 0) -> boolean) -> boolean, FilterLongColLessLongColumn(col 3, col 0)(children: col 0) -> boolean, FilterLongColGreaterLongColumn(col 2, col 3)(children: col 2) -> boolean, FilterLongColLessLongScalar(col 10, val 0) -> boolean) -> boolean
+                    predicate: (((cdouble > UDFToDouble(ctinyint)) and (cboolean2 > 0)) or (cbigint < UDFToLong(ctinyint)) or (UDFToLong(cint) > cbigint) or (cboolean1 < 0)) (type: boolean)
+                    Statistics: Num rows: 12288 Data size: 330276 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), cint (type: int), cfloat (type: float), cdouble (type: double)
+                      outputColumnNames: ctinyint, cint, cfloat, cdouble
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 2, 4, 5]
+                      Statistics: Num rows: 12288 Data size: 330276 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: var_pop(ctinyint), sum(cfloat), max(ctinyint), max(cint), var_samp(cdouble), count(cint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumDouble(col 4) -> double, VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFMaxLong(col 2) -> int, VectorUDAFVarSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 2) -> bigint
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                        Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4, 5]
+                          Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: double), _col2 (type: tinyint), _col3 (type: int), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: bigint)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 2, 3, 4, 5, 10, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:double, VALUE._col2:tinyint, VALUE._col3:int, VALUE._col4:struct<count:bigint,sum:double,variance:double>, VALUE._col5:bigint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: var_pop(VALUE._col0), sum(VALUE._col1), max(VALUE._col2), max(VALUE._col3), var_samp(VALUE._col4), count(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFVarPopFinal(col 0) -> double, VectorUDAFSumDouble(col 1) -> double, VectorUDAFMaxLong(col 2) -> tinyint, VectorUDAFMaxLong(col 3) -> int, VectorUDAFVarSampFinal(col 4) -> double, VectorUDAFCountMerge(col 5) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col0 (type: double), (_col0 / -26.28) (type: double), _col1 (type: double), (-1.389 + _col1) (type: double), (_col1 * (-1.389 + _col1)) (type: double), _col2 (type: tinyint), (- (_col1 * (-1.389 + _col1))) (type: double), _col3 (type: int), (CAST( _col3 AS decimal(10,0)) * 79.553) (type: decimal(16,3)), _col4 (type: double), (10.175 % (- (_col1 * (-1.389 + _col1)))) (type: double), _col5 (type: bigint), (-563 % _col3) (type: int)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 1, 7, 9, 2, 8, 3, 12, 4, 13, 5, 14]
+                      selectExpressions: DoubleColDivideDoubleScalar(col 0, val -26.28) -> 6:double, DoubleScalarAddDoubleColumn(val -1.389, col 1) -> 7:double, DoubleColMultiplyDoubleColumn(col 1, col 8)(children: DoubleScalarAddDoubleColumn(val -1.389, col 1) -> 8:double) -> 9:double, DoubleColUnaryMinus(col 10)(children: DoubleColMultiplyDoubleColumn(col 1, col 8)(children: DoubleScalarAddDoubleColumn(val -1.389, col 1) -> 8:double) -> 10:double) -> 8:double, DecimalColMultiplyDecimalScalar(col 11, val 79.553)(children: CastLongToDecimal(col 3) -> 11:decimal(10,0)) -> 12:decimal(16,3), DoubleScalarModuloDoubleColumn(val 10.175, col 10)(children: DoubleColUnaryMinus(col 13)(children: DoubleColMultiplyDoubleColumn(col 1, col 10)(children: DoubleScalarAddDoubleColumn(val -1.389, col 1) -> 10:double) -> 13:double) -> 10:double) -> 13:double, LongScalarModuloLongColumn(val -563, col 3) -> 14:long
+                  Statistics: Num rows: 1 Data size: 196 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 196 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT VAR_POP(ctinyint),
        (VAR_POP(ctinyint) / -26.28),
        SUM(cfloat),
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_10.q.out b/ql/src/test/results/clientpositive/llap/vectorization_10.q.out
index 9dad4c440d..f06c2dbcc6 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_10.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_10.q.out
@@ -1,3 +1,121 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cdouble,
+       ctimestamp1,
+       ctinyint,
+       cboolean1,
+       cstring1,
+       (-(cdouble)),
+       (cdouble + csmallint),
+       ((cdouble + csmallint) % 33),
+       (-(cdouble)),
+       (ctinyint % cdouble),
+       (ctinyint % csmallint),
+       (-(cdouble)),
+       (cbigint * (ctinyint % csmallint)),
+       (9763215.5639 - (cdouble + csmallint)),
+       (-((-(cdouble))))
+FROM   alltypesorc
+WHERE  (((cstring2 <= '10')
+         OR ((ctinyint > cdouble)
+             AND (-5638.15 >= ctinyint)))
+        OR ((cdouble > 6981)
+            AND ((csmallint = 9763215.5639)
+                 OR (cstring1 LIKE '%a'))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cdouble,
+       ctimestamp1,
+       ctinyint,
+       cboolean1,
+       cstring1,
+       (-(cdouble)),
+       (cdouble + csmallint),
+       ((cdouble + csmallint) % 33),
+       (-(cdouble)),
+       (ctinyint % cdouble),
+       (ctinyint % csmallint),
+       (-(cdouble)),
+       (cbigint * (ctinyint % csmallint)),
+       (9763215.5639 - (cdouble + csmallint)),
+       (-((-(cdouble))))
+FROM   alltypesorc
+WHERE  (((cstring2 <= '10')
+         OR ((ctinyint > cdouble)
+             AND (-5638.15 >= ctinyint)))
+        OR ((cdouble > 6981)
+            AND ((csmallint = 9763215.5639)
+                 OR (cstring1 LIKE '%a'))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 2491562 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterStringGroupColLessEqualStringScalar(col 7, val 10) -> boolean, FilterExprAndExpr(children: FilterDoubleColGreaterDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterDecimalScalarGreaterEqualDecimalColumn(val -5638.15, col 13)(children: CastLongToDecimal(col 0) -> 13:decimal(6,2)) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColGreaterDoubleScalar(col 5, val 6981.0) -> boolean, FilterExprOrExpr(children: FilterDecimalColEqualDecimalScalar(col 14, val 9763215.5639)(children: CastLongToDecimal(col 1) -> 14:decimal(11,4)) -> boolean, FilterStringColLikeStringScalar(col 6, pattern %a) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: ((cstring2 <= '10') or ((UDFToDouble(ctinyint) > cdouble) and (-5638.15 >= CAST( ctinyint AS decimal(6,2)))) or ((cdouble > 6981.0) and ((CAST( csmallint AS decimal(11,4)) = 9763215.5639) or (cstring1 like '%a')))) (type: boolean)
+                    Statistics: Num rows: 5461 Data size: 1107444 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: cdouble (type: double), ctimestamp1 (type: timestamp), ctinyint (type: tinyint), cboolean1 (type: boolean), cstring1 (type: string), (- cdouble) (type: double), (cdouble + UDFToDouble(csmallint)) (type: double), ((cdouble + UDFToDouble(csmallint)) % 33.0) (type: double), (- cdouble) (type: double), (UDFToDouble(ctinyint) % cdouble) (type: double), (UDFToShort(ctinyint) % csmallint) (type: smallint), (- cdouble) (type: double), (cbigint * UDFToLong((UDFToShort(ctinyint) % csmallint))) (type: bigint), (9763215.5639 - (cdouble + UDFToDouble(csmallint))) (type: double), (- (- cdouble)) (type: double)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [5, 8, 0, 10, 6, 12, 16, 15, 17, 19, 20, 18, 22, 23, 25]
+                          selectExpressions: DoubleColUnaryMinus(col 5) -> 12:double, DoubleColAddDoubleColumn(col 5, col 15)(children: CastLongToDouble(col 1) -> 15:double) -> 16:double, DoubleColModuloDoubleScalar(col 17, val 33.0)(children: DoubleColAddDoubleColumn(col 5, col 15)(children: CastLongToDouble(col 1) -> 15:double) -> 17:double) -> 15:double, DoubleColUnaryMinus(col 5) -> 17:double, DoubleColModuloDoubleColumn(col 18, col 5)(children: CastLongToDouble(col 0) -> 18:double) -> 19:double, LongColModuloLongColumn(col 0, col 1)(children: col 0) -> 20:long, DoubleColUnaryMinus(col 5) -> 18:double, LongColMultiplyLongColumn(col 3, col 21)(children: col 21) -> 22:long, DoubleScalarSubtractDoubleColumn(val 9763215.5639, col 24)(children: DoubleColAddDoubleColumn(col 5, col 23)(children: CastLongToDouble(col 1) -> 23:double) -> 24:double) -> 23:double, DoubleColUnaryMinus(col 24)(children: DoubleColUnaryMinus(col 5) -> 24:double) -> 25:double
+                      Statistics: Num rows: 5461 Data size: 1082056 Basic stats: COMPLETE Column stats: COMPLETE
+                      File Output Operator
+                        compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
+                        Statistics: Num rows: 5461 Data size: 1082056 Basic stats: COMPLETE Column stats: COMPLETE
+                        table:
+                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 3, 5, 6, 7, 8, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, decimal(6,2), decimal(11,4), double, double, double, double, double, bigint, bigint, bigint, double, double, double
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT cdouble,
        ctimestamp1,
        ctinyint,
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_11.q.out b/ql/src/test/results/clientpositive/llap/vectorization_11.q.out
index dff58dab42..2b8c39109f 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_11.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_11.q.out
@@ -1,3 +1,103 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cstring1,
+       cboolean1,
+       cdouble,
+       ctimestamp1,
+       (-3728 * csmallint),
+       (cdouble - 9763215.5639),
+       (-(cdouble)),
+       ((-(cdouble)) + 6981),
+       (cdouble * -5638.15)
+FROM   alltypesorc
+WHERE  ((cstring2 = cstring1)
+        OR ((ctimestamp1 IS NULL)
+            AND (cstring1 LIKE '%a')))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cstring1,
+       cboolean1,
+       cdouble,
+       ctimestamp1,
+       (-3728 * csmallint),
+       (cdouble - 9763215.5639),
+       (-(cdouble)),
+       ((-(cdouble)) + 6981),
+       (cdouble * -5638.15)
+FROM   alltypesorc
+WHERE  ((cstring2 = cstring1)
+        OR ((ctimestamp1 IS NULL)
+            AND (cstring1 LIKE '%a')))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 2381474 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterStringGroupColEqualStringGroupColumn(col 7, col 6) -> boolean, FilterExprAndExpr(children: SelectColumnIsNull(col 8) -> boolean, FilterStringColLikeStringScalar(col 6, pattern %a) -> boolean) -> boolean) -> boolean
+                    predicate: ((cstring2 = cstring1) or (ctimestamp1 is null and (cstring1 like '%a'))) (type: boolean)
+                    Statistics: Num rows: 6144 Data size: 1190792 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: cstring1 (type: string), cboolean1 (type: boolean), cdouble (type: double), ctimestamp1 (type: timestamp), (-3728 * UDFToInteger(csmallint)) (type: int), (cdouble - 9763215.5639) (type: double), (- cdouble) (type: double), ((- cdouble) + 6981.0) (type: double), (cdouble * -5638.15) (type: double)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [6, 10, 5, 8, 12, 13, 14, 16, 15]
+                          selectExpressions: LongScalarMultiplyLongColumn(val -3728, col 1)(children: col 1) -> 12:long, DoubleColSubtractDoubleScalar(col 5, val 9763215.5639) -> 13:double, DoubleColUnaryMinus(col 5) -> 14:double, DoubleColAddDoubleScalar(col 15, val 6981.0)(children: DoubleColUnaryMinus(col 5) -> 15:double) -> 16:double, DoubleColMultiplyDoubleScalar(col 5, val -5638.15) -> 15:double
+                      Statistics: Num rows: 6144 Data size: 953272 Basic stats: COMPLETE Column stats: COMPLETE
+                      File Output Operator
+                        compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
+                        Statistics: Num rows: 6144 Data size: 953272 Basic stats: COMPLETE Column stats: COMPLETE
+                        table:
+                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [1, 5, 6, 7, 8, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint, double, double, double, double
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT cstring1,
        cboolean1,
        cdouble,
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_12.q.out b/ql/src/test/results/clientpositive/llap/vectorization_12.q.out
index 6a7f69c698..863a26f928 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_12.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_12.q.out
@@ -1,3 +1,243 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT   cbigint,
+         cboolean1,
+         cstring1,
+         ctimestamp1,
+         cdouble,
+         (-6432 * cdouble),
+         (-(cbigint)),
+         COUNT(cbigint),
+         (cbigint * COUNT(cbigint)),
+         STDDEV_SAMP(cbigint),
+         ((-6432 * cdouble) / -6432),
+         (-(((-6432 * cdouble) / -6432))),
+         AVG(cdouble),
+         (-((-6432 * cdouble))),
+         (-5638.15 + cbigint),
+         SUM(cbigint),
+         (AVG(cdouble) / (-6432 * cdouble)),
+         AVG(cdouble),
+         (-((-(((-6432 * cdouble) / -6432))))),
+         (((-6432 * cdouble) / -6432) + (-((-6432 * cdouble)))),
+         STDDEV_POP(cdouble)
+FROM     alltypesorc
+WHERE    (((ctimestamp1 IS NULL)
+           AND ((cboolean1 >= cboolean2)
+                OR (ctinyint != csmallint)))
+          AND ((cstring1 LIKE '%a')
+              OR ((cboolean2 <= 1)
+                  AND (cbigint >= csmallint))))
+GROUP BY cbigint, cboolean1, cstring1, ctimestamp1, cdouble
+ORDER BY ctimestamp1, cdouble, cbigint, cstring1
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT   cbigint,
+         cboolean1,
+         cstring1,
+         ctimestamp1,
+         cdouble,
+         (-6432 * cdouble),
+         (-(cbigint)),
+         COUNT(cbigint),
+         (cbigint * COUNT(cbigint)),
+         STDDEV_SAMP(cbigint),
+         ((-6432 * cdouble) / -6432),
+         (-(((-6432 * cdouble) / -6432))),
+         AVG(cdouble),
+         (-((-6432 * cdouble))),
+         (-5638.15 + cbigint),
+         SUM(cbigint),
+         (AVG(cdouble) / (-6432 * cdouble)),
+         AVG(cdouble),
+         (-((-(((-6432 * cdouble) / -6432))))),
+         (((-6432 * cdouble) / -6432) + (-((-6432 * cdouble)))),
+         STDDEV_POP(cdouble)
+FROM     alltypesorc
+WHERE    (((ctimestamp1 IS NULL)
+           AND ((cboolean1 >= cboolean2)
+                OR (ctinyint != csmallint)))
+          AND ((cstring1 LIKE '%a')
+              OR ((cboolean2 <= 1)
+                  AND (cbigint >= csmallint))))
+GROUP BY cbigint, cboolean1, cstring1, ctimestamp1, cdouble
+ORDER BY ctimestamp1, cdouble, cbigint, cstring1
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 1647554 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: SelectColumnIsNull(col 8) -> boolean, FilterExprOrExpr(children: FilterLongColGreaterEqualLongColumn(col 10, col 11) -> boolean, FilterLongColNotEqualLongColumn(col 0, col 1)(children: col 0) -> boolean) -> boolean, FilterExprOrExpr(children: FilterStringColLikeStringScalar(col 6, pattern %a) -> boolean, FilterExprAndExpr(children: FilterLongColLessEqualLongScalar(col 11, val 1) -> boolean, FilterLongColGreaterEqualLongColumn(col 3, col 1)(children: col 1) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: (ctimestamp1 is null and ((cboolean1 >= cboolean2) or (UDFToShort(ctinyint) <> csmallint)) and ((cstring1 like '%a') or ((cboolean2 <= 1) and (cbigint >= UDFToLong(csmallint))))) (type: boolean)
+                    Statistics: Num rows: 1 Data size: 166 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: cbigint (type: bigint), cdouble (type: double), cstring1 (type: string), cboolean1 (type: boolean)
+                      outputColumnNames: cbigint, cdouble, cstring1, cboolean1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [3, 5, 6, 10]
+                      Statistics: Num rows: 1 Data size: 166 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: count(cbigint), stddev_samp(cbigint), avg(cdouble), sum(cbigint), stddev_pop(cdouble)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFCount(col 3) -> bigint, VectorUDAFStdSampLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 5, col 3, col 6, col 10
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4]
+                        keys: cdouble (type: double), cbigint (type: bigint), cstring1 (type: string), cboolean1 (type: boolean)
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+                        Statistics: Num rows: 1 Data size: 370 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          key expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: string), _col3 (type: boolean)
+                          sort order: ++++
+                          Map-reduce partition columns: _col0 (type: double), _col1 (type: bigint), _col2 (type: string), _col3 (type: boolean)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkMultiKeyOperator
+                              keyColumns: [0, 1, 2, 3]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [4, 5, 6, 7, 8]
+                          Statistics: Num rows: 1 Data size: 370 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col4 (type: bigint), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: struct<count:bigint,sum:double,input:double>), _col7 (type: bigint), _col8 (type: struct<count:bigint,sum:double,variance:double>)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 3, 5, 6, 8, 10, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaa
+                reduceColumnSortOrder: ++++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 9
+                    dataColumns: KEY._col0:double, KEY._col1:bigint, KEY._col2:string, KEY._col3:boolean, VALUE._col0:bigint, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:struct<count:bigint,sum:double,input:double>, VALUE._col3:bigint, VALUE._col4:struct<count:bigint,sum:double,variance:double>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0), stddev_samp(VALUE._col1), avg(VALUE._col2), sum(VALUE._col3), stddev_pop(VALUE._col4)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 4) -> bigint, VectorUDAFStdSampFinal(col 5) -> double, VectorUDAFAvgFinal(col 6) -> double, VectorUDAFSumLong(col 7) -> bigint, VectorUDAFStdPopFinal(col 8) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2, col 3
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4]
+                keys: KEY._col0 (type: double), KEY._col1 (type: bigint), KEY._col2 (type: string), KEY._col3 (type: boolean)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+                Statistics: Num rows: 1 Data size: 154 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col1 (type: bigint), _col3 (type: boolean), _col2 (type: string), _col0 (type: double), (-6432.0 * _col0) (type: double), (- _col1) (type: bigint), _col4 (type: bigint), (_col1 * _col4) (type: bigint), _col5 (type: double), ((-6432.0 * _col0) / -6432.0) (type: double), (- ((-6432.0 * _col0) / -6432.0)) (type: double), _col6 (type: double), (- (-6432.0 * _col0)) (type: double), (-5638.15 + CAST( _col1 AS decimal(19,0))) (type: decimal(22,2)), _col7 (type: bigint), (_col6 / (-6432.0 * _col0)) (type: double), (- (- ((-6432.0 * _col0) / -6432.0))) (type: double), (((-6432.0 * _col0) / -6432.0) + (- (-6432.0 * _col0))) (type: double), _col8 (type: double)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col17, _col18, _col19
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [1, 3, 2, 0, 9, 10, 4, 11, 5, 13, 12, 6, 15, 17, 7, 18, 19, 14, 8]
+                      selectExpressions: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 9:double, LongColUnaryMinus(col 1) -> 10:long, LongColMultiplyLongColumn(col 1, col 4) -> 11:long, DoubleColDivideDoubleScalar(col 12, val -6432.0)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 12:double) -> 13:double, DoubleColUnaryMinus(col 14)(children: DoubleColDivideDoubleScalar(col 12, val -6432.0)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 12:double) -> 14:double) -> 12:double, DoubleColUnaryMinus(col 14)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 15:double, DecimalScalarAddDecimalColumn(val -5638.15, col 16)(children: CastLongToDecimal(col 1) -> 16:decimal(19,0)) -> 17:decimal(22,2), DoubleColDivideDoubleColumn(col 6, col 14)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 18:double, DoubleColUnaryMinus(col 14)(children: DoubleColUnaryMinus(col 19)(children: DoubleColDivideDoubleScalar(col 14, val -6432.0)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 19:double) -> 14:double) -> 19:double, DoubleColAddDoubleColumn(col 20, col 21)(children: DoubleColDivideDoubleScalar(col 14, val -6432.0)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 20:double, DoubleColUnaryMinus(col 14)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 21:double) -> 14:double
+                  Statistics: Num rows: 1 Data size: 338 Basic stats: COMPLETE Column stats: COMPLETE
+                  Reduce Output Operator
+                    key expressions: _col3 (type: double), _col0 (type: bigint), _col2 (type: string)
+                    sort order: +++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        keyColumns: [0, 1, 2]
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        valueColumns: [3, 9, 10, 4, 11, 5, 13, 12, 6, 15, 17, 7, 18, 19, 14, 8]
+                    Statistics: Num rows: 1 Data size: 338 Basic stats: COMPLETE Column stats: COMPLETE
+                    value expressions: _col1 (type: boolean), _col4 (type: double), _col5 (type: bigint), _col6 (type: bigint), _col7 (type: bigint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: double), _col12 (type: double), _col13 (type: decimal(22,2)), _col14 (type: bigint), _col15 (type: double), _col17 (type: double), _col18 (type: double), _col19 (type: double)
+        Reducer 3 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: aaa
+                reduceColumnSortOrder: +++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 19
+                    dataColumns: KEY.reducesinkkey0:double, KEY.reducesinkkey1:bigint, KEY.reducesinkkey2:string, VALUE._col0:boolean, VALUE._col1:double, VALUE._col2:bigint, VALUE._col3:bigint, VALUE._col4:bigint, VALUE._col5:double, VALUE._col6:double, VALUE._col7:double, VALUE._col8:double, VALUE._col9:double, VALUE._col10:decimal(22,2), VALUE._col11:bigint, VALUE._col12:double, VALUE._col13:double, VALUE._col14:double, VALUE._col15:double
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: timestamp
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey1 (type: bigint), VALUE._col0 (type: boolean), KEY.reducesinkkey2 (type: string), null (type: timestamp), KEY.reducesinkkey0 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: bigint), VALUE._col3 (type: bigint), VALUE._col4 (type: bigint), VALUE._col5 (type: double), VALUE._col6 (type: double), VALUE._col7 (type: double), VALUE._col8 (type: double), VALUE._col9 (type: double), VALUE._col10 (type: decimal(22,2)), VALUE._col11 (type: bigint), VALUE._col12 (type: double), VALUE._col8 (type: double), VALUE._col13 (type: double), VALUE._col14 (type: double), VALUE._col15 (type: double)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 3, 2, 19, 0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 11, 16, 17, 18]
+                    selectExpressions: ConstantVectorExpression(val null) -> 19:timestamp
+                Statistics: Num rows: 1 Data size: 386 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 386 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT   cbigint,
          cboolean1,
          cstring1,
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_13.q.out b/ql/src/test/results/clientpositive/llap/vectorization_13.q.out
index 3ae67b6981..d3e43098c8 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_13.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_13.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cboolean1,
          ctinyint,
          ctimestamp1,
@@ -31,7 +31,7 @@ GROUP BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1
 ORDER BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16
 LIMIT 40
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cboolean1,
          ctinyint,
          ctimestamp1,
@@ -109,11 +109,12 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFMinLong(col 0) -> tinyint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 10, col 0, col 8, col 4, col 6
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: cboolean1 (type: boolean), ctinyint (type: tinyint), ctimestamp1 (type: timestamp), cfloat (type: float), cstring1 (type: string)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -122,6 +123,12 @@ STAGE PLANS:
                           key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
                           sort order: +++++
                           Map-reduce partition columns: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkMultiKeyOperator
+                              keyColumns: [0, 1, 2, 3, 4]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [5, 6, 7, 8, 9, 10]
                           Statistics: Num rows: 2730 Data size: 816734 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col5 (type: tinyint), _col6 (type: double), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,variance:double>), _col9 (type: float), _col10 (type: tinyint)
             Execution mode: vectorized, llap
@@ -129,21 +136,44 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 4, 5, 6, 8, 9, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, decimal(11,4)
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col2] not supported
-                vectorized: false
+                reduceColumnNullOrder: aaaaa
+                reduceColumnSortOrder: +++++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 11
+                    dataColumns: KEY._col0:boolean, KEY._col1:tinyint, KEY._col2:timestamp, KEY._col3:float, KEY._col4:string, VALUE._col0:tinyint, VALUE._col1:double, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:struct<count:bigint,sum:double,variance:double>, VALUE._col4:float, VALUE._col5:tinyint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), sum(VALUE._col1), stddev_pop(VALUE._col2), stddev_pop(VALUE._col3), max(VALUE._col4), min(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxLong(col 5) -> tinyint, VectorUDAFSumDouble(col 6) -> double, VectorUDAFStdPopFinal(col 7) -> double, VectorUDAFStdPopFinal(col 8) -> double, VectorUDAFMaxDouble(col 9) -> float, VectorUDAFMinLong(col 10) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2, col 3, col 4
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 keys: KEY._col0 (type: boolean), KEY._col1 (type: tinyint), KEY._col2 (type: timestamp), KEY._col3 (type: float), KEY._col4 (type: string)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -151,10 +181,21 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string), (- _col1) (type: tinyint), _col5 (type: tinyint), ((- _col1) + _col5) (type: tinyint), _col6 (type: double), (_col6 * UDFToDouble(((- _col1) + _col5))) (type: double), (- _col6) (type: double), (79.553 * _col3) (type: float), _col7 (type: double), (- _col6) (type: double), _col8 (type: double), (CAST( ((- _col1) + _col5) AS decimal(3,0)) - 10.175) (type: decimal(7,3)), (- (- _col6)) (type: double), (-26.28 / (- (- _col6))) (type: double), _col9 (type: float), ((_col6 * UDFToDouble(((- _col1) + _col5))) / UDFToDouble(_col1)) (type: double), _col10 (type: tinyint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 11, 5, 13, 6, 16, 15, 17, 7, 18, 8, 20, 22, 21, 9, 25, 10]
+                      selectExpressions: LongColUnaryMinus(col 1) -> 11:long, LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 13:long, DoubleColMultiplyDoubleColumn(col 6, col 15)(children: CastLongToDouble(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 15:double) -> 16:double, DoubleColUnaryMinus(col 6) -> 15:double, DoubleScalarMultiplyDoubleColumn(val 79.5530014038086, col 3) -> 17:double, DoubleColUnaryMinus(col 6) -> 18:double, DecimalColSubtractDecimalScalar(col 19, val 10.175)(children: CastLongToDecimal(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 19:decimal(3,0)) -> 20:decimal(7,3), DoubleColUnaryMinus(col 21)(children: DoubleColUnaryMinus(col 6) -> 21:double) -> 22:double, DoubleScalarDivideDoubleColumn(val -26.28, col 23)(children: DoubleColUnaryMinus(col 21)(children: DoubleColUnaryMinus(col 6) -> 21:double) -> 23:double) -> 21:double, DoubleColDivideDoubleColumn(col 24, col 23)(children: DoubleColMultiplyDoubleColumn(col 6, col 23)(children: CastLongToDouble(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 23:double) -> 24:double, CastLongToDouble(col 1) -> 23:double) -> 25:double
                   Statistics: Num rows: 1365 Data size: 446640 Basic stats: COMPLETE Column stats: COMPLETE
                   Reduce Output Operator
                     key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string), _col5 (type: tinyint), _col6 (type: tinyint), _col7 (type: tinyint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: double), _col15 (type: decimal(7,3)), _col16 (type: double), _col17 (type: double), _col18 (type: float), _col19 (type: double), _col20 (type: tinyint)
                     sort order: +++++++++++++++++++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        keyColumns: [0, 1, 2, 3, 4, 11, 5, 13, 6, 16, 15, 17, 7, 18, 8, 20, 22, 21, 9, 25, 10]
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        valueColumns: []
                     Statistics: Num rows: 1365 Data size: 446640 Basic stats: COMPLETE Column stats: COMPLETE
                     TopN Hash Memory Usage: 0.1
         Reducer 3 
@@ -162,10 +203,16 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaaaaaaaaaaaaaaaaaaa
+                reduceColumnSortOrder: +++++++++++++++++++++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 21
+                    dataColumns: KEY.reducesinkkey0:boolean, KEY.reducesinkkey1:tinyint, KEY.reducesinkkey2:timestamp, KEY.reducesinkkey3:float, KEY.reducesinkkey4:string, KEY.reducesinkkey5:tinyint, KEY.reducesinkkey6:tinyint, KEY.reducesinkkey7:tinyint, KEY.reducesinkkey8:double, KEY.reducesinkkey9:double, KEY.reducesinkkey10:double, KEY.reducesinkkey11:float, KEY.reducesinkkey12:double, KEY.reducesinkkey13:double, KEY.reducesinkkey14:double, KEY.reducesinkkey15:decimal(7,3), KEY.reducesinkkey16:double, KEY.reducesinkkey17:double, KEY.reducesinkkey18:float, KEY.reducesinkkey19:double, KEY.reducesinkkey20:tinyint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: boolean), KEY.reducesinkkey1 (type: tinyint), KEY.reducesinkkey2 (type: timestamp), KEY.reducesinkkey3 (type: float), KEY.reducesinkkey4 (type: string), KEY.reducesinkkey5 (type: tinyint), KEY.reducesinkkey6 (type: tinyint), KEY.reducesinkkey7 (type: tinyint), KEY.reducesinkkey8 (type: double), KEY.reducesinkkey9 (type: double), KEY.reducesinkkey10 (type: double), KEY.reducesinkkey11 (type: float), KEY.reducesinkkey12 (type: double), KEY.reducesinkkey10 (type: double), KEY.reducesinkkey14 (type: double), KEY.reducesinkkey15 (type: decimal(7,3)), KEY.reducesinkkey16 (type: double), KEY.reducesinkkey17 (type: double), KEY.reducesinkkey18 (type: float), KEY.reducesinkkey19 (type: double), KEY.reducesinkkey20 (type: tinyint)
@@ -417,11 +464,12 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFMinLong(col 0) -> tinyint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 10, col 0, col 8, col 4, col 6
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: cboolean1 (type: boolean), ctinyint (type: tinyint), ctimestamp1 (type: timestamp), cfloat (type: float), cstring1 (type: string)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -430,6 +478,10 @@ STAGE PLANS:
                           key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
                           sort order: +++++
                           Map-reduce partition columns: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkMultiKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 2730 Data size: 816734 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col5 (type: tinyint), _col6 (type: double), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,variance:double>), _col9 (type: float), _col10 (type: tinyint)
             Execution mode: vectorized, llap
@@ -437,21 +489,32 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col2] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), sum(VALUE._col1), stddev_pop(VALUE._col2), stddev_pop(VALUE._col3), max(VALUE._col4), min(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxLong(col 5) -> tinyint, VectorUDAFSumDouble(col 6) -> double, VectorUDAFStdPopFinal(col 7) -> double, VectorUDAFStdPopFinal(col 8) -> double, VectorUDAFMaxDouble(col 9) -> float, VectorUDAFMinLong(col 10) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2, col 3, col 4
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 keys: KEY._col0 (type: boolean), KEY._col1 (type: tinyint), KEY._col2 (type: timestamp), KEY._col3 (type: float), KEY._col4 (type: string)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -459,10 +522,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string), (- _col1) (type: tinyint), _col5 (type: tinyint), ((- _col1) + _col5) (type: tinyint), _col6 (type: double), (_col6 * UDFToDouble(((- _col1) + _col5))) (type: double), (- _col6) (type: double), (79.553 * _col3) (type: float), _col7 (type: double), (- _col6) (type: double), _col8 (type: double), (CAST( ((- _col1) + _col5) AS decimal(3,0)) - 10.175) (type: decimal(7,3)), (- (- _col6)) (type: double), (-26.28 / (- (- _col6))) (type: double), _col9 (type: float), ((_col6 * UDFToDouble(((- _col1) + _col5))) / UDFToDouble(_col1)) (type: double), _col10 (type: tinyint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 11, 5, 13, 6, 16, 15, 17, 7, 18, 8, 20, 22, 21, 9, 25, 10]
+                      selectExpressions: LongColUnaryMinus(col 1) -> 11:long, LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 13:long, DoubleColMultiplyDoubleColumn(col 6, col 15)(children: CastLongToDouble(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 15:double) -> 16:double, DoubleColUnaryMinus(col 6) -> 15:double, DoubleScalarMultiplyDoubleColumn(val 79.5530014038086, col 3) -> 17:double, DoubleColUnaryMinus(col 6) -> 18:double, DecimalColSubtractDecimalScalar(col 19, val 10.175)(children: CastLongToDecimal(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 19:decimal(3,0)) -> 20:decimal(7,3), DoubleColUnaryMinus(col 21)(children: DoubleColUnaryMinus(col 6) -> 21:double) -> 22:double, DoubleScalarDivideDoubleColumn(val -26.28, col 23)(children: DoubleColUnaryMinus(col 21)(children: DoubleColUnaryMinus(col 6) -> 21:double) -> 23:double) -> 21:double, DoubleColDivideDoubleColumn(col 24, col 23)(children: DoubleColMultiplyDoubleColumn(col 6, col 23)(children: CastLongToDouble(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 23:double) -> 24:double, CastLongToDouble(col 1) -> 23:double) -> 25:double
                   Statistics: Num rows: 1365 Data size: 446640 Basic stats: COMPLETE Column stats: COMPLETE
                   Reduce Output Operator
                     key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string), _col5 (type: tinyint), _col6 (type: tinyint), _col7 (type: tinyint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: double), _col15 (type: decimal(7,3)), _col16 (type: double), _col17 (type: double), _col18 (type: float), _col19 (type: double), _col20 (type: tinyint)
                     sort order: +++++++++++++++++++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 1365 Data size: 446640 Basic stats: COMPLETE Column stats: COMPLETE
                     TopN Hash Memory Usage: 0.1
         Reducer 3 
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_14.q.out b/ql/src/test/results/clientpositive/llap/vectorization_14.q.out
index 541d13fcc3..418e4ea79f 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_14.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_14.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   ctimestamp1,
          cfloat,
          cstring1,
@@ -31,7 +31,7 @@ WHERE    (((ctinyint <= cbigint)
 GROUP BY ctimestamp1, cfloat, cstring1, cboolean1, cdouble
 ORDER BY cstring1, cfloat, cdouble, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   ctimestamp1,
          cfloat,
          cstring1,
@@ -86,15 +86,36 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 2139070 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterLongColLessEqualLongColumn(col 0, col 3)(children: col 0) -> boolean, FilterExprOrExpr(children: FilterDoubleColLessEqualDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 2) -> 12:double) -> boolean, FilterTimestampColLessTimestampColumn(col 9, col 8) -> boolean) -> boolean, FilterDoubleColLessDoubleColumn(col 5, col 12)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterExprOrExpr(children: FilterLongColGreaterLongScalar(col 3, val -257) -> boolean, FilterDoubleColLessDoubleColumn(col 4, col 12)(children: CastLongToFloatViaLongToDouble(col 2) -> 12:double) -> boolean) -> boolean) -> boolean
                     predicate: ((UDFToLong(ctinyint) <= cbigint) and ((UDFToDouble(cint) <= cdouble) or (ctimestamp2 < ctimestamp1)) and (cdouble < UDFToDouble(ctinyint)) and ((cbigint > -257) or (cfloat < UDFToFloat(cint)))) (type: boolean)
                     Statistics: Num rows: 606 Data size: 105558 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
                       expressions: ctimestamp1 (type: timestamp), cfloat (type: float), cstring1 (type: string), cboolean1 (type: boolean), cdouble (type: double), (- (-26.28 + cdouble)) (type: double)
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [8, 4, 6, 10, 5, 13]
+                          selectExpressions: DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -26.28, col 5) -> 12:double) -> 13:double
                       Statistics: Num rows: 606 Data size: 105558 Basic stats: COMPLETE Column stats: COMPLETE
                       Group By Operator
                         aggregations: stddev_samp(_col5), max(_col1), stddev_pop(_col1), count(_col1), var_pop(_col1), var_samp(_col1)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFStdSampDouble(col 13) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 4) -> bigint, VectorUDAFVarPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 6, col 4, col 5, col 8, col 10
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                         keys: _col2 (type: string), _col1 (type: float), _col4 (type: double), _col0 (type: timestamp), _col3 (type: boolean)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -103,6 +124,12 @@ STAGE PLANS:
                           key expressions: _col0 (type: string), _col1 (type: float), _col2 (type: double), _col3 (type: timestamp), _col4 (type: boolean)
                           sort order: +++++
                           Map-reduce partition columns: _col0 (type: string), _col1 (type: float), _col2 (type: double), _col3 (type: timestamp), _col4 (type: boolean)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkMultiKeyOperator
+                              keyColumns: [0, 1, 2, 3, 4]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [5, 6, 7, 8, 9, 10]
                           Statistics: Num rows: 303 Data size: 137686 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: float), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: bigint), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,variance:double>)
             Execution mode: vectorized, llap
@@ -110,21 +137,44 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 2, 3, 4, 5, 6, 8, 9, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, double
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                reduceColumnNullOrder: aaaaa
+                reduceColumnSortOrder: +++++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 11
+                    dataColumns: KEY._col0:string, KEY._col1:float, KEY._col2:double, KEY._col3:timestamp, KEY._col4:boolean, VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:float, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:bigint, VALUE._col4:struct<count:bigint,sum:double,variance:double>, VALUE._col5:struct<count:bigint,sum:double,variance:double>
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev_samp(VALUE._col0), max(VALUE._col1), stddev_pop(VALUE._col2), count(VALUE._col3), var_pop(VALUE._col4), var_samp(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFStdSampFinal(col 5) -> double, VectorUDAFMaxDouble(col 6) -> float, VectorUDAFStdPopFinal(col 7) -> double, VectorUDAFCountMerge(col 8) -> bigint, VectorUDAFVarPopFinal(col 9) -> double, VectorUDAFVarSampFinal(col 10) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2, col 3, col 4
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 keys: KEY._col0 (type: string), KEY._col1 (type: float), KEY._col2 (type: double), KEY._col3 (type: timestamp), KEY._col4 (type: boolean)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -132,10 +182,21 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col3 (type: timestamp), _col1 (type: float), _col0 (type: string), _col4 (type: boolean), _col2 (type: double), (-26.28 + _col2) (type: double), (- (-26.28 + _col2)) (type: double), _col5 (type: double), (_col1 * -26.28) (type: float), _col6 (type: float), (- _col1) (type: float), (- _col6) (type: float), ((- (-26.28 + _col2)) / 10.175) (type: double), _col7 (type: double), _col8 (type: bigint), (- ((- (-26.28 + _col2)) / 10.175)) (type: double), (-1.389 % _col5) (type: double), (UDFToDouble(_col1) - _col2) (type: double), _col9 (type: double), (_col9 % 10.175) (type: double), _col10 (type: double), (- (UDFToDouble(_col1) - _col2)) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [3, 1, 0, 4, 2, 11, 13, 5, 12, 6, 14, 15, 16, 7, 8, 18, 17, 19, 9, 20, 10, 22]
+                      selectExpressions: DoubleScalarAddDoubleColumn(val -26.28, col 2) -> 11:double, DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -26.28, col 2) -> 12:double) -> 13:double, DoubleColMultiplyDoubleScalar(col 1, val -26.280000686645508) -> 12:double, DoubleColUnaryMinus(col 1) -> 14:double, DoubleColUnaryMinus(col 6) -> 15:double, DoubleColDivideDoubleScalar(col 17, val 10.175)(children: DoubleColUnaryMinus(col 16)(children: DoubleScalarAddDoubleColumn(val -26.28, col 2) -> 16:double) -> 17:double) -> 16:double, DoubleColUnaryMinus(col 17)(children: DoubleColDivideDoubleScalar(col 18, val 10.175)(children: DoubleColUnaryMinus(col 17)(children: DoubleScalarAddDoubleColumn(val -26.28, col 2) -> 17:double) -> 18:double) -> 17:double) -> 18:double, DoubleScalarModuloDoubleColumn(val -1.389, col 5) -> 17:double, DoubleColSubtractDoubleColumn(col 1, col 2)(children: col 1) -> 19:double, DoubleColModuloDoubleScalar(col 9, val 10.175) -> 20:double, DoubleColUnaryMinus(col 21)(children: DoubleColSubtractDoubleColumn(col 1, col 2)(children: col 1) -> 21:double) -> 22:double
                   Statistics: Num rows: 151 Data size: 36700 Basic stats: COMPLETE Column stats: COMPLETE
                   Reduce Output Operator
                     key expressions: _col2 (type: string), _col1 (type: float), _col4 (type: double), _col0 (type: timestamp)
                     sort order: ++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        keyColumns: [0, 1, 2, 3]
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        valueColumns: [4, 11, 13, 5, 12, 6, 14, 15, 16, 7, 8, 18, 17, 19, 9, 20, 10, 22]
                     Statistics: Num rows: 151 Data size: 36700 Basic stats: COMPLETE Column stats: COMPLETE
                     value expressions: _col3 (type: boolean), _col5 (type: double), _col6 (type: double), _col7 (type: double), _col8 (type: float), _col9 (type: float), _col10 (type: float), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: bigint), _col15 (type: double), _col16 (type: double), _col17 (type: double), _col18 (type: double), _col19 (type: double), _col20 (type: double), _col21 (type: double)
         Reducer 3 
@@ -143,17 +204,30 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaa
+                reduceColumnSortOrder: ++++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 22
+                    dataColumns: KEY.reducesinkkey0:string, KEY.reducesinkkey1:float, KEY.reducesinkkey2:double, KEY.reducesinkkey3:timestamp, VALUE._col0:boolean, VALUE._col1:double, VALUE._col2:double, VALUE._col3:double, VALUE._col4:float, VALUE._col5:float, VALUE._col6:float, VALUE._col7:float, VALUE._col8:double, VALUE._col9:double, VALUE._col10:bigint, VALUE._col11:double, VALUE._col12:double, VALUE._col13:double, VALUE._col14:double, VALUE._col15:double, VALUE._col16:double, VALUE._col17:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey3 (type: timestamp), KEY.reducesinkkey1 (type: float), KEY.reducesinkkey0 (type: string), VALUE._col0 (type: boolean), KEY.reducesinkkey2 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: double), VALUE._col3 (type: double), VALUE._col4 (type: float), VALUE._col5 (type: float), VALUE._col6 (type: float), VALUE._col7 (type: float), VALUE._col8 (type: double), VALUE._col9 (type: double), VALUE._col10 (type: bigint), VALUE._col11 (type: double), VALUE._col12 (type: double), VALUE._col13 (type: double), VALUE._col14 (type: double), VALUE._col15 (type: double), VALUE._col16 (type: double), VALUE._col17 (type: double)
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [3, 1, 0, 4, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
                 Statistics: Num rows: 151 Data size: 36700 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 151 Data size: 36700 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_15.q.out b/ql/src/test/results/clientpositive/llap/vectorization_15.q.out
index 766904ee4e..79c70849b6 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_15.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_15.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cboolean1,
          cdouble,
@@ -29,7 +29,7 @@ WHERE    (((cstring2 LIKE '%ss%')
 GROUP BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1
 ORDER BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cboolean1,
          cdouble,
@@ -82,15 +82,35 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 2491562 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterStringColLikeStringScalar(col 7, pattern %ss%) -> boolean, FilterStringColLikeStringScalar(col 6, pattern 10%) -> boolean, FilterExprAndExpr(children: FilterLongColGreaterEqualLongScalar(col 2, val -75) -> boolean, FilterLongColEqualLongColumn(col 0, col 1)(children: col 0) -> boolean, FilterDoubleColGreaterEqualDoubleScalar(col 5, val -3728.0) -> boolean) -> boolean) -> boolean
                     predicate: ((cstring2 like '%ss%') or (cstring1 like '10%') or ((cint >= -75) and (UDFToShort(ctinyint) = csmallint) and (cdouble >= -3728.0))) (type: boolean)
                     Statistics: Num rows: 12288 Data size: 2491562 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
                       expressions: ctinyint (type: tinyint), cint (type: int), cfloat (type: float), cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp), cboolean1 (type: boolean)
                       outputColumnNames: ctinyint, cint, cfloat, cdouble, cstring1, ctimestamp1, cboolean1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 2, 4, 5, 6, 8, 10]
                       Statistics: Num rows: 12288 Data size: 2491562 Basic stats: COMPLETE Column stats: COMPLETE
                       Group By Operator
                         aggregations: stddev_samp(cfloat), min(cdouble), stddev_samp(ctinyint), var_pop(ctinyint), var_samp(cint), stddev_pop(cint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFStdSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinDouble(col 5) -> double, VectorUDAFStdSampLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 4, col 10, col 5, col 6, col 0, col 2, col 8
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                         keys: cfloat (type: float), cboolean1 (type: boolean), cdouble (type: double), cstring1 (type: string), ctinyint (type: tinyint), cint (type: int), ctimestamp1 (type: timestamp)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
@@ -99,6 +119,12 @@ STAGE PLANS:
                           key expressions: _col0 (type: float), _col1 (type: boolean), _col2 (type: double), _col3 (type: string), _col4 (type: tinyint), _col5 (type: int), _col6 (type: timestamp)
                           sort order: +++++++
                           Map-reduce partition columns: _col0 (type: float), _col1 (type: boolean), _col2 (type: double), _col3 (type: string), _col4 (type: tinyint), _col5 (type: int), _col6 (type: timestamp)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkMultiKeyOperator
+                              keyColumns: [0, 1, 2, 3, 4, 5, 6]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [7, 8, 9, 10, 11, 12]
                           Statistics: Num rows: 6144 Data size: 3293884 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: double), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,variance:double>), _col11 (type: struct<count:bigint,sum:double,variance:double>), _col12 (type: struct<count:bigint,sum:double,variance:double>)
             Execution mode: vectorized, llap
@@ -106,21 +132,31 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 4, 5, 6, 7, 8, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: llap
             Reduce Vectorization:
-                enabled: true
-                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                enabled: false
+                enableConditionsMet: hive.execution.engine tez IN [tez, spark] IS true
+                enableConditionsNotMet: hive.vectorized.execution.reduce.enabled IS false
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev_samp(VALUE._col0), min(VALUE._col1), stddev_samp(VALUE._col2), var_pop(VALUE._col3), var_samp(VALUE._col4), stddev_pop(VALUE._col5)
+                Group By Vectorization:
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: false
+                    native: false
+                    vectorProcessingMode: NONE
+                    projectedOutputColumns: null
                 keys: KEY._col0 (type: float), KEY._col1 (type: boolean), KEY._col2 (type: double), KEY._col3 (type: string), KEY._col4 (type: tinyint), KEY._col5 (type: int), KEY._col6 (type: timestamp)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
@@ -135,14 +171,11 @@ STAGE PLANS:
                     Statistics: Num rows: 3072 Data size: 1327460 Basic stats: COMPLETE Column stats: COMPLETE
                     value expressions: _col7 (type: double), _col8 (type: decimal(13,2)), _col9 (type: double), _col10 (type: double), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: double), _col15 (type: tinyint), _col16 (type: double), _col17 (type: float), _col18 (type: int), _col19 (type: decimal(13,2)), _col20 (type: double)
         Reducer 3 
-            Execution mode: vectorized, llap
+            Execution mode: llap
             Reduce Vectorization:
-                enabled: true
-                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                groupByVectorOutput: true
-                allNative: false
-                usesVectorUDFAdaptor: false
-                vectorized: true
+                enabled: false
+                enableConditionsMet: hive.execution.engine tez IN [tez, spark] IS true
+                enableConditionsNotMet: hive.vectorized.execution.reduce.enabled IS false
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: float), KEY.reducesinkkey1 (type: boolean), KEY.reducesinkkey2 (type: double), KEY.reducesinkkey3 (type: string), KEY.reducesinkkey4 (type: tinyint), KEY.reducesinkkey5 (type: int), KEY.reducesinkkey6 (type: timestamp), VALUE._col0 (type: double), VALUE._col1 (type: decimal(13,2)), VALUE._col2 (type: double), VALUE._col3 (type: double), VALUE._col4 (type: float), VALUE._col5 (type: double), VALUE._col6 (type: double), VALUE._col7 (type: double), VALUE._col8 (type: tinyint), VALUE._col9 (type: double), VALUE._col10 (type: float), VALUE._col11 (type: int), VALUE._col12 (type: decimal(13,2)), VALUE._col13 (type: double)
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_16.q.out b/ql/src/test/results/clientpositive/llap/vectorization_16.q.out
index 686b16cbef..d961af2513 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_16.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_16.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -18,7 +18,7 @@ WHERE    ((cstring2 LIKE '%b%')
               OR (cstring1 < 'a')))
 GROUP BY cstring1, cdouble, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -59,15 +59,35 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 2308074 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterStringColLikeStringScalar(col 7, pattern %b%) -> boolean, FilterExprOrExpr(children: FilterDoubleColGreaterEqualDoubleScalar(col 5, val -1.389) -> boolean, FilterStringGroupColLessStringScalar(col 6, val a) -> boolean) -> boolean) -> boolean
                     predicate: ((cstring2 like '%b%') and ((cdouble >= -1.389) or (cstring1 < 'a'))) (type: boolean)
                     Statistics: Num rows: 4096 Data size: 769522 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
                       expressions: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                       outputColumnNames: cdouble, cstring1, ctimestamp1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [5, 6, 8]
                       Statistics: Num rows: 4096 Data size: 769522 Basic stats: COMPLETE Column stats: COMPLETE
                       Group By Operator
                         aggregations: count(cdouble), stddev_samp(cdouble), min(cdouble)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFCount(col 5) -> bigint, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinDouble(col 5) -> double
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 5, col 6, col 8
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2]
                         keys: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -76,6 +96,12 @@ STAGE PLANS:
                           key expressions: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
                           sort order: +++
                           Map-reduce partition columns: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkMultiKeyOperator
+                              keyColumns: [0, 1, 2]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [3, 4, 5]
                           Statistics: Num rows: 2048 Data size: 434588 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: double)
             Execution mode: vectorized, llap
@@ -83,21 +109,43 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [5, 6, 7, 8]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col1] not supported
-                vectorized: false
+                reduceColumnNullOrder: aaa
+                reduceColumnSortOrder: +++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: KEY._col0:double, KEY._col1:string, KEY._col2:timestamp, VALUE._col0:bigint, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), stddev_samp(VALUE._col1), min(VALUE._col2)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 3) -> bigint, VectorUDAFStdSampFinal(col 4) -> double, VectorUDAFMinDouble(col 5) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2]
                 keys: KEY._col0 (type: double), KEY._col1 (type: string), KEY._col2 (type: timestamp)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -105,9 +153,17 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col1 (type: string), _col0 (type: double), _col2 (type: timestamp), (_col0 - 9763215.5639) (type: double), (- (_col0 - 9763215.5639)) (type: double), _col3 (type: bigint), _col4 (type: double), (- _col4) (type: double), (_col4 * UDFToDouble(_col3)) (type: double), _col5 (type: double), (9763215.5639 / _col0) (type: double), (CAST( _col3 AS decimal(19,0)) / -1.389) (type: decimal(28,6)), _col4 (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [1, 0, 2, 6, 8, 3, 4, 7, 10, 5, 9, 12, 4]
+                      selectExpressions: DoubleColSubtractDoubleScalar(col 0, val 9763215.5639) -> 6:double, DoubleColUnaryMinus(col 7)(children: DoubleColSubtractDoubleScalar(col 0, val 9763215.5639) -> 7:double) -> 8:double, DoubleColUnaryMinus(col 4) -> 7:double, DoubleColMultiplyDoubleColumn(col 4, col 9)(children: CastLongToDouble(col 3) -> 9:double) -> 10:double, DoubleScalarDivideDoubleColumn(val 9763215.5639, col 0) -> 9:double, DecimalColDivideDecimalScalar(col 11, val -1.389)(children: CastLongToDecimal(col 3) -> 11:decimal(19,0)) -> 12:decimal(28,6)
                   Statistics: Num rows: 1024 Data size: 307406 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1024 Data size: 307406 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_17.q.out b/ql/src/test/results/clientpositive/llap/vectorization_17.q.out
index 735c015e64..1c07962d83 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_17.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_17.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cstring1,
          cint,
@@ -22,7 +22,7 @@ WHERE    (((cbigint > -23)
                   OR (cfloat = cdouble))))
 ORDER BY cbigint, cfloat
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cstring1,
          cint,
@@ -67,16 +67,34 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 1647550 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterLongColGreaterLongScalar(col 3, val -23) -> boolean, FilterExprOrExpr(children: FilterDoubleColNotEqualDoubleScalar(col 5, val 988888.0) -> boolean, FilterDecimalColGreaterDecimalScalar(col 12, val -863.257)(children: CastLongToDecimal(col 2) -> 12:decimal(13,3)) -> boolean) -> boolean, FilterExprOrExpr(children: FilterLongColGreaterEqualLongScalar(col 0, val 33) -> boolean, FilterLongColGreaterEqualLongColumn(col 1, col 3)(children: col 1) -> boolean, FilterDoubleColEqualDoubleColumn(col 4, col 5)(children: col 4) -> boolean) -> boolean) -> boolean
                     predicate: ((cbigint > -23) and ((cdouble <> 988888.0) or (CAST( cint AS decimal(13,3)) > -863.257)) and ((ctinyint >= 33) or (UDFToLong(csmallint) >= cbigint) or (UDFToDouble(cfloat) = cdouble))) (type: boolean)
                     Statistics: Num rows: 4778 Data size: 640688 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
                       expressions: cfloat (type: float), cstring1 (type: string), cint (type: int), ctimestamp1 (type: timestamp), cdouble (type: double), cbigint (type: bigint), (UDFToDouble(cfloat) / UDFToDouble(ctinyint)) (type: double), (UDFToLong(cint) % cbigint) (type: bigint), (- cdouble) (type: double), (cdouble + (UDFToDouble(cfloat) / UDFToDouble(ctinyint))) (type: double), (cdouble / UDFToDouble(cint)) (type: double), (- (- cdouble)) (type: double), (9763215.5639 % CAST( cbigint AS decimal(19,0))) (type: decimal(11,4)), (2563.58 + (- (- cdouble))) (type: double)
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [4, 6, 2, 8, 5, 3, 14, 15, 13, 16, 18, 19, 21, 17]
+                          selectExpressions: DoubleColDivideDoubleColumn(col 4, col 13)(children: col 4, CastLongToDouble(col 0) -> 13:double) -> 14:double, LongColModuloLongColumn(col 2, col 3)(children: col 2) -> 15:long, DoubleColUnaryMinus(col 5) -> 13:double, DoubleColAddDoubleColumn(col 5, col 17)(children: DoubleColDivideDoubleColumn(col 4, col 16)(children: col 4, CastLongToDouble(col 0) -> 16:double) -> 17:double) -> 16:double, DoubleColDivideDoubleColumn(col 5, col 17)(children: CastLongToDouble(col 2) -> 17:double) -> 18:double, DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 5) -> 17:double) -> 19:double, DecimalScalarModuloDecimalColumn(val 9763215.5639, col 20)(children: CastLongToDecimal(col 3) -> 20:decimal(19,0)) -> 21:decimal(11,4), DoubleScalarAddDoubleColumn(val 2563.58, col 22)(children: DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 5) -> 17:double) -> 22:double) -> 17:double
                       Statistics: Num rows: 4778 Data size: 1414848 Basic stats: COMPLETE Column stats: COMPLETE
                       Reduce Output Operator
                         key expressions: _col5 (type: bigint), _col0 (type: float)
                         sort order: ++
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            keyColumns: [3, 4]
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [6, 2, 8, 5, 14, 15, 13, 16, 18, 19, 21, 17]
                         Statistics: Num rows: 4778 Data size: 1414848 Basic stats: COMPLETE Column stats: COMPLETE
                         value expressions: _col1 (type: string), _col2 (type: int), _col3 (type: timestamp), _col4 (type: double), _col6 (type: double), _col7 (type: bigint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: double), _col12 (type: decimal(11,4)), _col13 (type: double)
             Execution mode: vectorized, llap
@@ -89,22 +107,41 @@ STAGE PLANS:
                 allNative: true
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 6, 8]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: decimal(13,3), double, double, bigint, double, double, double, double, decimal(19,0), decimal(11,4), double
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: aa
+                reduceColumnSortOrder: ++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 14
+                    dataColumns: KEY.reducesinkkey0:bigint, KEY.reducesinkkey1:float, VALUE._col0:string, VALUE._col1:int, VALUE._col2:timestamp, VALUE._col3:double, VALUE._col4:double, VALUE._col5:bigint, VALUE._col6:double, VALUE._col7:double, VALUE._col8:double, VALUE._col9:double, VALUE._col10:decimal(11,4), VALUE._col11:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey1 (type: float), VALUE._col0 (type: string), VALUE._col1 (type: int), VALUE._col2 (type: timestamp), VALUE._col3 (type: double), KEY.reducesinkkey0 (type: bigint), VALUE._col4 (type: double), VALUE._col5 (type: bigint), VALUE._col6 (type: double), VALUE._col7 (type: double), VALUE._col8 (type: double), VALUE._col9 (type: double), VALUE._col10 (type: decimal(11,4)), VALUE._col11 (type: double)
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 2, 3, 4, 5, 0, 6, 7, 8, 9, 10, 11, 12, 13]
                 Statistics: Num rows: 4778 Data size: 1414848 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 4778 Data size: 1414848 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_2.q.out b/ql/src/test/results/clientpositive/llap/vectorization_2.q.out
index 709a75f2ab..affd1b6d7b 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_2.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_2.q.out
@@ -1,3 +1,182 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(csmallint),
+       (AVG(csmallint) % -563),
+       (AVG(csmallint) + 762),
+       SUM(cfloat),
+       VAR_POP(cbigint),
+       (-(VAR_POP(cbigint))),
+       (SUM(cfloat) - AVG(csmallint)),
+       COUNT(*),
+       (-((SUM(cfloat) - AVG(csmallint)))),
+       (VAR_POP(cbigint) - 762),
+       MIN(ctinyint),
+       ((-(VAR_POP(cbigint))) + MIN(ctinyint)),
+       AVG(cdouble),
+       (((-(VAR_POP(cbigint))) + MIN(ctinyint)) - SUM(cfloat))
+FROM   alltypesorc
+WHERE  (((ctimestamp1 < ctimestamp2)
+         AND ((cstring2 LIKE 'b%')
+              AND (cfloat <= -5638.15)))
+        OR ((cdouble < ctinyint)
+            AND ((-10669 != ctimestamp2)
+                 OR (359 > cint))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(csmallint),
+       (AVG(csmallint) % -563),
+       (AVG(csmallint) + 762),
+       SUM(cfloat),
+       VAR_POP(cbigint),
+       (-(VAR_POP(cbigint))),
+       (SUM(cfloat) - AVG(csmallint)),
+       COUNT(*),
+       (-((SUM(cfloat) - AVG(csmallint)))),
+       (VAR_POP(cbigint) - 762),
+       MIN(ctinyint),
+       ((-(VAR_POP(cbigint))) + MIN(ctinyint)),
+       AVG(cdouble),
+       (((-(VAR_POP(cbigint))) + MIN(ctinyint)) - SUM(cfloat))
+FROM   alltypesorc
+WHERE  (((ctimestamp1 < ctimestamp2)
+         AND ((cstring2 LIKE 'b%')
+              AND (cfloat <= -5638.15)))
+        OR ((cdouble < ctinyint)
+            AND ((-10669 != ctimestamp2)
+                 OR (359 > cint))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 2157324 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: FilterTimestampColLessTimestampColumn(col 8, col 9) -> boolean, FilterStringColLikeStringScalar(col 7, pattern b%) -> boolean, FilterDoubleColLessEqualDoubleScalar(col 4, val -5638.14990234375) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColLessDoubleColumn(col 5, col 12)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterExprOrExpr(children: FilterDoubleScalarNotEqualDoubleColumn(val -10669.0, col 12)(children: CastTimestampToDouble(col 9) -> 12:double) -> boolean, FilterLongScalarGreaterLongColumn(val 359, col 2) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: (((ctimestamp1 < ctimestamp2) and (cstring2 like 'b%') and (cfloat <= -5638.15)) or ((cdouble < UDFToDouble(ctinyint)) and ((-10669.0 <> UDFToDouble(ctimestamp2)) or (359 > cint)))) (type: boolean)
+                    Statistics: Num rows: 4096 Data size: 719232 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), csmallint (type: smallint), cbigint (type: bigint), cfloat (type: float), cdouble (type: double)
+                      outputColumnNames: ctinyint, csmallint, cbigint, cfloat, cdouble
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1, 3, 4, 5]
+                      Statistics: Num rows: 4096 Data size: 719232 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: avg(csmallint), sum(cfloat), var_pop(cbigint), count(), min(ctinyint), avg(cdouble)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFSumDouble(col 4) -> double, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                        Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4, 5]
+                          Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: struct<count:bigint,sum:double,input:smallint>), _col1 (type: double), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: bigint), _col4 (type: tinyint), _col5 (type: struct<count:bigint,sum:double,input:double>)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 7, 8, 9]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,input:smallint>, VALUE._col1:double, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:bigint, VALUE._col4:tinyint, VALUE._col5:struct<count:bigint,sum:double,input:double>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: avg(VALUE._col0), sum(VALUE._col1), var_pop(VALUE._col2), count(VALUE._col3), min(VALUE._col4), avg(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFSumDouble(col 1) -> double, VectorUDAFVarPopFinal(col 2) -> double, VectorUDAFCountMerge(col 3) -> bigint, VectorUDAFMinLong(col 4) -> tinyint, VectorUDAFAvgFinal(col 5) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                Statistics: Num rows: 1 Data size: 44 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col0 (type: double), (_col0 % -563.0) (type: double), (_col0 + 762.0) (type: double), _col1 (type: double), _col2 (type: double), (- _col2) (type: double), (_col1 - _col0) (type: double), _col3 (type: bigint), (- (_col1 - _col0)) (type: double), (_col2 - 762.0) (type: double), _col4 (type: tinyint), ((- _col2) + UDFToDouble(_col4)) (type: double), _col5 (type: double), (((- _col2) + UDFToDouble(_col4)) - _col1) (type: double)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 7, 1, 2, 8, 9, 3, 11, 10, 4, 14, 5, 12]
+                      selectExpressions: DoubleColModuloDoubleScalar(col 0, val -563.0) -> 6:double, DoubleColAddDoubleScalar(col 0, val 762.0) -> 7:double, DoubleColUnaryMinus(col 2) -> 8:double, DoubleColSubtractDoubleColumn(col 1, col 0) -> 9:double, DoubleColUnaryMinus(col 10)(children: DoubleColSubtractDoubleColumn(col 1, col 0) -> 10:double) -> 11:double, DoubleColSubtractDoubleScalar(col 2, val 762.0) -> 10:double, DoubleColAddDoubleColumn(col 12, col 13)(children: DoubleColUnaryMinus(col 2) -> 12:double, CastLongToDouble(col 4) -> 13:double) -> 14:double, DoubleColSubtractDoubleColumn(col 15, col 1)(children: DoubleColAddDoubleColumn(col 12, col 13)(children: DoubleColUnaryMinus(col 2) -> 12:double, CastLongToDouble(col 4) -> 13:double) -> 15:double) -> 12:double
+                  Statistics: Num rows: 1 Data size: 108 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 108 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT AVG(csmallint),
        (AVG(csmallint) % -563),
        (AVG(csmallint) + 762),
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_3.q.out b/ql/src/test/results/clientpositive/llap/vectorization_3.q.out
index 2398dee7bc..415474665b 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_3.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_3.q.out
@@ -1,3 +1,187 @@
+WARNING: Comparing a bigint and a double may result in a loss of precision.
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT STDDEV_SAMP(csmallint),
+       (STDDEV_SAMP(csmallint) - 10.175),
+       STDDEV_POP(ctinyint),
+       (STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)),
+       (-(STDDEV_POP(ctinyint))),
+       (STDDEV_SAMP(csmallint) % 79.553),
+       (-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))),
+       STDDEV_SAMP(cfloat),
+       (-(STDDEV_SAMP(csmallint))),
+       SUM(cfloat),
+       ((-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))) / (STDDEV_SAMP(csmallint) - 10.175)),
+       (-((STDDEV_SAMP(csmallint) - 10.175))),
+       AVG(cint),
+       (-3728 - STDDEV_SAMP(csmallint)),
+       STDDEV_POP(cint),
+       (AVG(cint) / STDDEV_SAMP(cfloat))
+FROM   alltypesorc
+WHERE  (((cint <= cfloat)
+         AND ((79.553 != cbigint)
+              AND (ctimestamp2 = -29071)))
+        OR ((cbigint > cdouble)
+            AND ((79.553 <= csmallint)
+                 AND (ctimestamp1 > ctimestamp2))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT STDDEV_SAMP(csmallint),
+       (STDDEV_SAMP(csmallint) - 10.175),
+       STDDEV_POP(ctinyint),
+       (STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)),
+       (-(STDDEV_POP(ctinyint))),
+       (STDDEV_SAMP(csmallint) % 79.553),
+       (-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))),
+       STDDEV_SAMP(cfloat),
+       (-(STDDEV_SAMP(csmallint))),
+       SUM(cfloat),
+       ((-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))) / (STDDEV_SAMP(csmallint) - 10.175)),
+       (-((STDDEV_SAMP(csmallint) - 10.175))),
+       AVG(cint),
+       (-3728 - STDDEV_SAMP(csmallint)),
+       STDDEV_POP(cint),
+       (AVG(cint) / STDDEV_SAMP(cfloat))
+FROM   alltypesorc
+WHERE  (((cint <= cfloat)
+         AND ((79.553 != cbigint)
+              AND (ctimestamp2 = -29071)))
+        OR ((cbigint > cdouble)
+            AND ((79.553 <= csmallint)
+                 AND (ctimestamp1 > ctimestamp2))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 1276620 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: FilterDoubleColLessEqualDoubleColumn(col 12, col 4)(children: CastLongToFloatViaLongToDouble(col 2) -> 12:double) -> boolean, FilterDecimalScalarNotEqualDecimalColumn(val 79.553, col 13)(children: CastLongToDecimal(col 3) -> 13:decimal(22,3)) -> boolean, FilterDoubleColEqualDoubleScalar(col 12, val -29071.0)(children: CastTimestampToDouble(col 9) -> 12:double) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColGreaterDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 3) -> 12:double) -> boolean, FilterDecimalScalarLessEqualDecimalColumn(val 79.553, col 14)(children: CastLongToDecimal(col 1) -> 14:decimal(8,3)) -> boolean, FilterTimestampColGreaterTimestampColumn(col 8, col 9) -> boolean) -> boolean) -> boolean
+                    predicate: (((UDFToFloat(cint) <= cfloat) and (79.553 <> CAST( cbigint AS decimal(22,3))) and (UDFToDouble(ctimestamp2) = -29071.0)) or ((UDFToDouble(cbigint) > cdouble) and (79.553 <= CAST( csmallint AS decimal(8,3))) and (ctimestamp1 > ctimestamp2))) (type: boolean)
+                    Statistics: Num rows: 2503 Data size: 260060 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), csmallint (type: smallint), cint (type: int), cfloat (type: float)
+                      outputColumnNames: ctinyint, csmallint, cint, cfloat
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1, 2, 4]
+                      Statistics: Num rows: 2503 Data size: 260060 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: stddev_samp(csmallint), stddev_pop(ctinyint), stddev_samp(cfloat), sum(cfloat), avg(cint), stddev_pop(cint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumDouble(col 4) -> double, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                        Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4, 5]
+                          Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: double), _col4 (type: struct<count:bigint,sum:double,input:int>), _col5 (type: struct<count:bigint,sum:double,variance:double>)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 8, 9]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, decimal(22,3), decimal(8,3)
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:double, VALUE._col4:struct<count:bigint,sum:double,input:int>, VALUE._col5:struct<count:bigint,sum:double,variance:double>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: stddev_samp(VALUE._col0), stddev_pop(VALUE._col1), stddev_samp(VALUE._col2), sum(VALUE._col3), avg(VALUE._col4), stddev_pop(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFStdSampFinal(col 0) -> double, VectorUDAFStdPopFinal(col 1) -> double, VectorUDAFStdSampFinal(col 2) -> double, VectorUDAFSumDouble(col 3) -> double, VectorUDAFAvgFinal(col 4) -> double, VectorUDAFStdPopFinal(col 5) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                Statistics: Num rows: 1 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col0 (type: double), (_col0 - 10.175) (type: double), _col1 (type: double), (_col0 * (_col0 - 10.175)) (type: double), (- _col1) (type: double), (_col0 % 79.553) (type: double), (- (_col0 * (_col0 - 10.175))) (type: double), _col2 (type: double), (- _col0) (type: double), _col3 (type: double), ((- (_col0 * (_col0 - 10.175))) / (_col0 - 10.175)) (type: double), (- (_col0 - 10.175)) (type: double), _col4 (type: double), (-3728.0 - _col0) (type: double), _col5 (type: double), (_col4 / _col2) (type: double)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 1, 8, 7, 9, 10, 2, 11, 3, 14, 13, 4, 12, 5, 15]
+                      selectExpressions: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 6:double, DoubleColMultiplyDoubleColumn(col 0, col 7)(children: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 7:double) -> 8:double, DoubleColUnaryMinus(col 1) -> 7:double, DoubleColModuloDoubleScalar(col 0, val 79.553) -> 9:double, DoubleColUnaryMinus(col 11)(children: DoubleColMultiplyDoubleColumn(col 0, col 10)(children: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 10:double) -> 11:double) -> 10:double, DoubleColUnaryMinus(col 0) -> 11:double, DoubleColDivideDoubleColumn(col 12, col 13)(children: DoubleColUnaryMinus(col 13)(children: DoubleColMultiplyDoubleColumn(col 0, col 12)(children: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 12:double) -> 13:double) -> 12:double, DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 13:double) -> 14:double, DoubleColUnaryMinus(col 12)(children: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 12:double) -> 13:double, DoubleScalarSubtractDoubleColumn(val -3728.0, col 0) -> 12:double, DoubleColDivideDoubleColumn(col 4, col 2) -> 15:double
+                  Statistics: Num rows: 1 Data size: 128 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 128 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 WARNING: Comparing a bigint and a double may result in a loss of precision.
 PREHOOK: query: SELECT STDDEV_SAMP(csmallint),
        (STDDEV_SAMP(csmallint) - 10.175),
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_4.q.out b/ql/src/test/results/clientpositive/llap/vectorization_4.q.out
index 0d6829f6d8..0b2adade2c 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_4.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_4.q.out
@@ -1,3 +1,181 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT SUM(cint),
+       (SUM(cint) * -563),
+       (-3728 + SUM(cint)),
+       STDDEV_POP(cdouble),
+       (-(STDDEV_POP(cdouble))),
+       AVG(cdouble),
+       ((SUM(cint) * -563) % SUM(cint)),
+       (((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)),
+       VAR_POP(cdouble),
+       (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))),
+       ((-3728 + SUM(cint)) - (SUM(cint) * -563)),
+       MIN(ctinyint),
+       MIN(ctinyint),
+       (MIN(ctinyint) * (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))))
+FROM   alltypesorc
+WHERE  (((csmallint >= cint)
+         OR ((-89010 >= ctinyint)
+             AND (cdouble > 79.553)))
+        OR ((-563 != cbigint)
+            AND ((ctinyint != cbigint)
+                 OR (-3728 >= cdouble))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT SUM(cint),
+       (SUM(cint) * -563),
+       (-3728 + SUM(cint)),
+       STDDEV_POP(cdouble),
+       (-(STDDEV_POP(cdouble))),
+       AVG(cdouble),
+       ((SUM(cint) * -563) % SUM(cint)),
+       (((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)),
+       VAR_POP(cdouble),
+       (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))),
+       ((-3728 + SUM(cint)) - (SUM(cint) * -563)),
+       MIN(ctinyint),
+       MIN(ctinyint),
+       (MIN(ctinyint) * (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))))
+FROM   alltypesorc
+WHERE  (((csmallint >= cint)
+         OR ((-89010 >= ctinyint)
+             AND (cdouble > 79.553)))
+        OR ((-563 != cbigint)
+            AND ((ctinyint != cbigint)
+                 OR (-3728 >= cdouble))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 256884 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterLongColGreaterEqualLongColumn(col 1, col 2)(children: col 1) -> boolean, FilterExprAndExpr(children: FilterLongScalarGreaterEqualLongColumn(val -89010, col 0)(children: col 0) -> boolean, FilterDoubleColGreaterDoubleScalar(col 5, val 79.553) -> boolean) -> boolean, FilterExprAndExpr(children: FilterLongScalarNotEqualLongColumn(val -563, col 3) -> boolean, FilterExprOrExpr(children: FilterLongColNotEqualLongColumn(col 0, col 3)(children: col 0) -> boolean, FilterDoubleScalarGreaterEqualDoubleColumn(val -3728.0, col 5) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: ((UDFToInteger(csmallint) >= cint) or ((-89010 >= UDFToInteger(ctinyint)) and (cdouble > 79.553)) or ((-563 <> cbigint) and ((UDFToLong(ctinyint) <> cbigint) or (-3728.0 >= cdouble)))) (type: boolean)
+                    Statistics: Num rows: 12288 Data size: 256884 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), cint (type: int), cdouble (type: double)
+                      outputColumnNames: ctinyint, cint, cdouble
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 2, 5]
+                      Statistics: Num rows: 12288 Data size: 256884 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: sum(cint), stddev_pop(cdouble), avg(cdouble), var_pop(cdouble), min(ctinyint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFSumLong(col 2) -> bigint, VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>, VectorUDAFVarPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinLong(col 0) -> tinyint
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                        Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4]
+                          Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: bigint), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,input:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: tinyint)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 5]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    dataColumns: VALUE._col0:bigint, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:struct<count:bigint,sum:double,input:double>, VALUE._col3:struct<count:bigint,sum:double,variance:double>, VALUE._col4:tinyint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0), stddev_pop(VALUE._col1), avg(VALUE._col2), var_pop(VALUE._col3), min(VALUE._col4)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFStdPopFinal(col 1) -> double, VectorUDAFAvgFinal(col 2) -> double, VectorUDAFVarPopFinal(col 3) -> double, VectorUDAFMinLong(col 4) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Statistics: Num rows: 1 Data size: 36 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col0 (type: bigint), (_col0 * -563) (type: bigint), (-3728 + _col0) (type: bigint), _col1 (type: double), (- _col1) (type: double), _col2 (type: double), ((_col0 * -563) % _col0) (type: bigint), (UDFToDouble(((_col0 * -563) % _col0)) / _col2) (type: double), _col3 (type: double), (- (UDFToDouble(((_col0 * -563) % _col0)) / _col2)) (type: double), ((-3728 + _col0) - (_col0 * -563)) (type: bigint), _col4 (type: tinyint), _col4 (type: tinyint), (UDFToDouble(_col4) * (- (UDFToDouble(((_col0 * -563) % _col0)) / _col2))) (type: double)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 5, 6, 1, 7, 2, 9, 12, 3, 11, 14, 4, 4, 16]
+                      selectExpressions: LongColMultiplyLongScalar(col 0, val -563) -> 5:long, LongScalarAddLongColumn(val -3728, col 0) -> 6:long, DoubleColUnaryMinus(col 1) -> 7:double, LongColModuloLongColumn(col 8, col 0)(children: LongColMultiplyLongScalar(col 0, val -563) -> 8:long) -> 9:long, DoubleColDivideDoubleColumn(col 11, col 2)(children: CastLongToDouble(col 10)(children: LongColModuloLongColumn(col 8, col 0)(children: LongColMultiplyLongScalar(col 0, val -563) -> 8:long) -> 10:long) -> 11:double) -> 12:double, DoubleColUnaryMinus(col 13)(children: DoubleColDivideDoubleColumn(col 11, col 2)(children: CastLongToDouble(col 10)(children: LongColModuloLongColumn(col 8, col 0)(children: LongColMultiplyLongScalar(col 0, val -563) -> 8:long) -> 10:long) -> 11:double) -> 13:double) -> 11:double, LongColSubtractLongColumn(col 8, col 10)(children: LongScalarAddLongColumn(val -3728, col 0) -> 8:long, LongColMultiplyLongScalar(col 0, val -563) -> 10:long) -> 14:long, DoubleColMultiplyDoubleColumn(col 13, col 15)(children: CastLongToDouble(col 4) -> 13:double, DoubleColUnaryMinus(col 16)(children: DoubleColDivideDoubleColumn(col 15, col 2)(children: CastLongToDouble(col 10)(children: LongColModuloLongColumn(col 8, col 0)(children: LongColMultiplyLongScalar(col 0, val -563) -> 8:long) -> 10:long) -> 15:double) -> 16:double) -> 15:double) -> 16:double
+                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT SUM(cint),
        (SUM(cint) * -563),
        (-3728 + SUM(cint)),
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_5.q.out b/ql/src/test/results/clientpositive/llap/vectorization_5.q.out
index 914a626872..230078058a 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_5.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_5.q.out
@@ -1,3 +1,176 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT MAX(csmallint),
+       (MAX(csmallint) * -75),
+       COUNT(*),
+       ((MAX(csmallint) * -75) / COUNT(*)),
+       (6981 * MAX(csmallint)),
+       MIN(csmallint),
+       (-(MIN(csmallint))),
+       (197 % ((MAX(csmallint) * -75) / COUNT(*))),
+       SUM(cint),
+       MAX(ctinyint),
+       (-(MAX(ctinyint))),
+       ((-(MAX(ctinyint))) + MAX(ctinyint))
+FROM   alltypesorc
+WHERE  (((cboolean2 IS NOT NULL)
+         AND (cstring1 LIKE '%b%'))
+        OR ((ctinyint = cdouble)
+            AND ((ctimestamp2 IS NOT NULL)
+                 AND (cstring2 LIKE 'a'))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT MAX(csmallint),
+       (MAX(csmallint) * -75),
+       COUNT(*),
+       ((MAX(csmallint) * -75) / COUNT(*)),
+       (6981 * MAX(csmallint)),
+       MIN(csmallint),
+       (-(MIN(csmallint))),
+       (197 % ((MAX(csmallint) * -75) / COUNT(*))),
+       SUM(cint),
+       MAX(ctinyint),
+       (-(MAX(ctinyint))),
+       ((-(MAX(ctinyint))) + MAX(ctinyint))
+FROM   alltypesorc
+WHERE  (((cboolean2 IS NOT NULL)
+         AND (cstring1 LIKE '%b%'))
+        OR ((ctinyint = cdouble)
+            AND ((ctimestamp2 IS NOT NULL)
+                 AND (cstring2 LIKE 'a'))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 2454862 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: SelectColumnIsNotNull(col 11) -> boolean, FilterStringColLikeStringScalar(col 6, pattern %b%) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColEqualDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, SelectColumnIsNotNull(col 9) -> boolean, FilterStringColLikeStringScalar(col 7, pattern a) -> boolean) -> boolean) -> boolean
+                    predicate: ((cboolean2 is not null and (cstring1 like '%b%')) or ((UDFToDouble(ctinyint) = cdouble) and ctimestamp2 is not null and (cstring2 like 'a'))) (type: boolean)
+                    Statistics: Num rows: 7658 Data size: 1529972 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), csmallint (type: smallint), cint (type: int)
+                      outputColumnNames: ctinyint, csmallint, cint
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1, 2]
+                      Statistics: Num rows: 7658 Data size: 1529972 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: max(csmallint), count(), min(csmallint), sum(cint), max(ctinyint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFMaxLong(col 1) -> smallint, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 1) -> smallint, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFMaxLong(col 0) -> tinyint
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                        Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4]
+                          Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: smallint), _col1 (type: bigint), _col2 (type: smallint), _col3 (type: bigint), _col4 (type: tinyint)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 5, 6, 7, 9, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    dataColumns: VALUE._col0:smallint, VALUE._col1:bigint, VALUE._col2:smallint, VALUE._col3:bigint, VALUE._col4:tinyint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: max(VALUE._col0), count(VALUE._col1), min(VALUE._col2), sum(VALUE._col3), max(VALUE._col4)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxLong(col 0) -> smallint, VectorUDAFCountMerge(col 1) -> bigint, VectorUDAFMinLong(col 2) -> smallint, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFMaxLong(col 4) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col0 (type: smallint), (UDFToInteger(_col0) * -75) (type: int), _col1 (type: bigint), (UDFToDouble((UDFToInteger(_col0) * -75)) / UDFToDouble(_col1)) (type: double), (6981 * UDFToInteger(_col0)) (type: int), _col2 (type: smallint), (- _col2) (type: smallint), (197.0 % (UDFToDouble((UDFToInteger(_col0) * -75)) / UDFToDouble(_col1))) (type: double), _col3 (type: bigint), _col4 (type: tinyint), (- _col4) (type: tinyint), ((- _col4) + _col4) (type: tinyint)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 5, 1, 9, 6, 2, 10, 7, 3, 4, 11, 14]
+                      selectExpressions: LongColMultiplyLongScalar(col 0, val -75)(children: col 0) -> 5:long, DoubleColDivideDoubleColumn(col 7, col 8)(children: CastLongToDouble(col 6)(children: LongColMultiplyLongScalar(col 0, val -75)(children: col 0) -> 6:long) -> 7:double, CastLongToDouble(col 1) -> 8:double) -> 9:double, LongScalarMultiplyLongColumn(val 6981, col 0)(children: col 0) -> 6:long, LongColUnaryMinus(col 2) -> 10:long, DoubleScalarModuloDoubleColumn(val 197.0, col 12)(children: DoubleColDivideDoubleColumn(col 7, col 8)(children: CastLongToDouble(col 11)(children: LongColMultiplyLongScalar(col 0, val -75)(children: col 0) -> 11:long) -> 7:double, CastLongToDouble(col 1) -> 8:double) -> 12:double) -> 7:double, LongColUnaryMinus(col 4) -> 11:long, LongColAddLongColumn(col 13, col 4)(children: LongColUnaryMinus(col 4) -> 13:long) -> 14:long
+                  Statistics: Num rows: 1 Data size: 64 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 64 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT MAX(csmallint),
        (MAX(csmallint) * -75),
        COUNT(*),
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_6.q.out b/ql/src/test/results/clientpositive/llap/vectorization_6.q.out
index 13897f6f93..33814f258d 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_6.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_6.q.out
@@ -1,3 +1,115 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cboolean1,
+       cfloat,
+       cstring1,
+       (988888 * csmallint),
+       (-(csmallint)),
+       (-(cfloat)),
+       (-26.28 / cfloat),
+       (cfloat * 359),
+       (cint % ctinyint),
+       (-(cdouble)),
+       (ctinyint - -75),
+       (762 * (cint % ctinyint))
+FROM   alltypesorc
+WHERE  ((ctinyint != 0)
+        AND ((((cboolean1 <= 0)
+          AND (cboolean2 >= cboolean1))
+          OR ((cbigint IS NOT NULL)
+              AND ((cstring2 LIKE '%a')
+                   OR (cfloat <= -257))))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cboolean1,
+       cfloat,
+       cstring1,
+       (988888 * csmallint),
+       (-(csmallint)),
+       (-(cfloat)),
+       (-26.28 / cfloat),
+       (cfloat * 359),
+       (cint % ctinyint),
+       (-(cdouble)),
+       (ctinyint - -75),
+       (762 * (cint % ctinyint))
+FROM   alltypesorc
+WHERE  ((ctinyint != 0)
+        AND ((((cboolean1 <= 0)
+          AND (cboolean2 >= cboolean1))
+          OR ((cbigint IS NOT NULL)
+              AND ((cstring2 LIKE '%a')
+                   OR (cfloat <= -257))))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 2110130 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterLongColNotEqualLongScalar(col 0, val 0) -> boolean, FilterExprOrExpr(children: FilterExprAndExpr(children: FilterLongColLessEqualLongScalar(col 10, val 0) -> boolean, FilterLongColGreaterEqualLongColumn(col 11, col 10) -> boolean) -> boolean, FilterExprAndExpr(children: SelectColumnIsNotNull(col 3) -> boolean, FilterExprOrExpr(children: FilterStringColLikeStringScalar(col 7, pattern %a) -> boolean, FilterDoubleColLessEqualDoubleScalar(col 4, val -257.0) -> boolean) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: ((ctinyint <> 0) and (((cboolean1 <= 0) and (cboolean2 >= cboolean1)) or (cbigint is not null and ((cstring2 like '%a') or (cfloat <= -257))))) (type: boolean)
+                    Statistics: Num rows: 5951 Data size: 1022000 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: cboolean1 (type: boolean), cfloat (type: float), cstring1 (type: string), (988888 * UDFToInteger(csmallint)) (type: int), (- csmallint) (type: smallint), (- cfloat) (type: float), (-26.28 / UDFToDouble(cfloat)) (type: double), (cfloat * 359.0) (type: float), (cint % UDFToInteger(ctinyint)) (type: int), (- cdouble) (type: double), (UDFToInteger(ctinyint) - -75) (type: int), (762 * (cint % UDFToInteger(ctinyint))) (type: int)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [10, 4, 6, 12, 13, 14, 15, 16, 17, 18, 19, 21]
+                          selectExpressions: LongScalarMultiplyLongColumn(val 988888, col 1)(children: col 1) -> 12:long, LongColUnaryMinus(col 1) -> 13:long, DoubleColUnaryMinus(col 4) -> 14:double, DoubleScalarDivideDoubleColumn(val -26.28, col 4)(children: col 4) -> 15:double, DoubleColMultiplyDoubleScalar(col 4, val 359.0) -> 16:double, LongColModuloLongColumn(col 2, col 0)(children: col 0) -> 17:long, DoubleColUnaryMinus(col 5) -> 18:double, LongColSubtractLongScalar(col 0, val -75)(children: col 0) -> 19:long, LongScalarMultiplyLongColumn(val 762, col 20)(children: LongColModuloLongColumn(col 2, col 0)(children: col 0) -> 20:long) -> 21:long
+                      Statistics: Num rows: 5951 Data size: 715128 Basic stats: COMPLETE Column stats: COMPLETE
+                      File Output Operator
+                        compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
+                        Statistics: Num rows: 5951 Data size: 715128 Basic stats: COMPLETE Column stats: COMPLETE
+                        table:
+                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 6, 7, 10, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint, bigint, double, double, double, bigint, double, bigint, bigint, bigint
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT cboolean1,
        cfloat,
        cstring1,
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_7.q.out b/ql/src/test/results/clientpositive/llap/vectorization_7.q.out
index ba49bed5c8..6c32ccf42d 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_7.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_7.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT cboolean1,
        cbigint,
        csmallint,
@@ -25,7 +25,7 @@ WHERE  ((ctinyint != 0)
 ORDER BY cboolean1, cbigint, csmallint, ctinyint, ctimestamp1, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9
 LIMIT 25
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT cboolean1,
        cbigint,
        csmallint,
@@ -97,8 +97,10 @@ STAGE PLANS:
                         sort order: +++++++++++++++
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkObjectHashOperator
+                            keyColumns: [10, 3, 1, 0, 8, 6, 13, 14, 15, 16, 18, 19, 17, 20, 22]
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: []
                         Statistics: Num rows: 7281 Data size: 1231410 Basic stats: COMPLETE Column stats: COMPLETE
                         TopN Hash Memory Usage: 0.1
             Execution mode: vectorized, llap
@@ -111,15 +113,27 @@ STAGE PLANS:
                 allNative: true
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, bigint, bigint, bigint, bigint, bigint, bigint, bigint, bigint, bigint, bigint
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaaaaaaaaaaaaa
+                reduceColumnSortOrder: +++++++++++++++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 15
+                    dataColumns: KEY.reducesinkkey0:boolean, KEY.reducesinkkey1:bigint, KEY.reducesinkkey2:smallint, KEY.reducesinkkey3:tinyint, KEY.reducesinkkey4:timestamp, KEY.reducesinkkey5:string, KEY.reducesinkkey6:bigint, KEY.reducesinkkey7:int, KEY.reducesinkkey8:smallint, KEY.reducesinkkey9:tinyint, KEY.reducesinkkey10:int, KEY.reducesinkkey11:bigint, KEY.reducesinkkey12:int, KEY.reducesinkkey13:tinyint, KEY.reducesinkkey14:tinyint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: boolean), KEY.reducesinkkey1 (type: bigint), KEY.reducesinkkey2 (type: smallint), KEY.reducesinkkey3 (type: tinyint), KEY.reducesinkkey4 (type: timestamp), KEY.reducesinkkey5 (type: string), KEY.reducesinkkey6 (type: bigint), KEY.reducesinkkey7 (type: int), KEY.reducesinkkey8 (type: smallint), KEY.reducesinkkey9 (type: tinyint), KEY.reducesinkkey10 (type: int), KEY.reducesinkkey11 (type: bigint), KEY.reducesinkkey12 (type: int), KEY.reducesinkkey9 (type: tinyint), KEY.reducesinkkey14 (type: tinyint)
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_8.q.out b/ql/src/test/results/clientpositive/llap/vectorization_8.q.out
index 9e9f2c7cbe..0d5b6d53e0 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_8.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_8.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ctimestamp1,
        cdouble,
        cboolean1,
@@ -23,7 +23,7 @@ WHERE  (((cstring2 IS NOT NULL)
 ORDER BY ctimestamp1, cdouble, cboolean1, cstring1, cfloat, c1, c2, c3, c4, c5, c6, c7, c8, c9
 LIMIT 20
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ctimestamp1,
        cdouble,
        cboolean1,
@@ -93,8 +93,10 @@ STAGE PLANS:
                         sort order: ++++++++++++++
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkObjectHashOperator
+                            keyColumns: [8, 5, 10, 6, 4, 12, 13, 14, 16, 18, 15, 17, 19, 21]
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: []
                         Statistics: Num rows: 3060 Data size: 557456 Basic stats: COMPLETE Column stats: COMPLETE
                         TopN Hash Memory Usage: 0.1
             Execution mode: vectorized, llap
@@ -107,15 +109,27 @@ STAGE PLANS:
                 allNative: true
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [2, 3, 4, 5, 6, 7, 8, 9, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, double, double, double, double, double, double, double, double, double, double
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaaaaaaaaaaaa
+                reduceColumnSortOrder: ++++++++++++++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 14
+                    dataColumns: KEY.reducesinkkey0:timestamp, KEY.reducesinkkey1:double, KEY.reducesinkkey2:boolean, KEY.reducesinkkey3:string, KEY.reducesinkkey4:float, KEY.reducesinkkey5:double, KEY.reducesinkkey6:double, KEY.reducesinkkey7:double, KEY.reducesinkkey8:float, KEY.reducesinkkey9:double, KEY.reducesinkkey10:double, KEY.reducesinkkey11:float, KEY.reducesinkkey12:float, KEY.reducesinkkey13:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: timestamp), KEY.reducesinkkey1 (type: double), KEY.reducesinkkey2 (type: boolean), KEY.reducesinkkey3 (type: string), KEY.reducesinkkey4 (type: float), KEY.reducesinkkey5 (type: double), KEY.reducesinkkey6 (type: double), KEY.reducesinkkey7 (type: double), KEY.reducesinkkey8 (type: float), KEY.reducesinkkey9 (type: double), KEY.reducesinkkey5 (type: double), KEY.reducesinkkey11 (type: float), KEY.reducesinkkey12 (type: float), KEY.reducesinkkey13 (type: double)
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_9.q.out b/ql/src/test/results/clientpositive/llap/vectorization_9.q.out
index 686b16cbef..d961af2513 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_9.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_9.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -18,7 +18,7 @@ WHERE    ((cstring2 LIKE '%b%')
               OR (cstring1 < 'a')))
 GROUP BY cstring1, cdouble, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -59,15 +59,35 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 2308074 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterStringColLikeStringScalar(col 7, pattern %b%) -> boolean, FilterExprOrExpr(children: FilterDoubleColGreaterEqualDoubleScalar(col 5, val -1.389) -> boolean, FilterStringGroupColLessStringScalar(col 6, val a) -> boolean) -> boolean) -> boolean
                     predicate: ((cstring2 like '%b%') and ((cdouble >= -1.389) or (cstring1 < 'a'))) (type: boolean)
                     Statistics: Num rows: 4096 Data size: 769522 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
                       expressions: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                       outputColumnNames: cdouble, cstring1, ctimestamp1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [5, 6, 8]
                       Statistics: Num rows: 4096 Data size: 769522 Basic stats: COMPLETE Column stats: COMPLETE
                       Group By Operator
                         aggregations: count(cdouble), stddev_samp(cdouble), min(cdouble)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFCount(col 5) -> bigint, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinDouble(col 5) -> double
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 5, col 6, col 8
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2]
                         keys: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -76,6 +96,12 @@ STAGE PLANS:
                           key expressions: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
                           sort order: +++
                           Map-reduce partition columns: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkMultiKeyOperator
+                              keyColumns: [0, 1, 2]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [3, 4, 5]
                           Statistics: Num rows: 2048 Data size: 434588 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: double)
             Execution mode: vectorized, llap
@@ -83,21 +109,43 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [5, 6, 7, 8]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col1] not supported
-                vectorized: false
+                reduceColumnNullOrder: aaa
+                reduceColumnSortOrder: +++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: KEY._col0:double, KEY._col1:string, KEY._col2:timestamp, VALUE._col0:bigint, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), stddev_samp(VALUE._col1), min(VALUE._col2)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 3) -> bigint, VectorUDAFStdSampFinal(col 4) -> double, VectorUDAFMinDouble(col 5) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2]
                 keys: KEY._col0 (type: double), KEY._col1 (type: string), KEY._col2 (type: timestamp)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -105,9 +153,17 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col1 (type: string), _col0 (type: double), _col2 (type: timestamp), (_col0 - 9763215.5639) (type: double), (- (_col0 - 9763215.5639)) (type: double), _col3 (type: bigint), _col4 (type: double), (- _col4) (type: double), (_col4 * UDFToDouble(_col3)) (type: double), _col5 (type: double), (9763215.5639 / _col0) (type: double), (CAST( _col3 AS decimal(19,0)) / -1.389) (type: decimal(28,6)), _col4 (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [1, 0, 2, 6, 8, 3, 4, 7, 10, 5, 9, 12, 4]
+                      selectExpressions: DoubleColSubtractDoubleScalar(col 0, val 9763215.5639) -> 6:double, DoubleColUnaryMinus(col 7)(children: DoubleColSubtractDoubleScalar(col 0, val 9763215.5639) -> 7:double) -> 8:double, DoubleColUnaryMinus(col 4) -> 7:double, DoubleColMultiplyDoubleColumn(col 4, col 9)(children: CastLongToDouble(col 3) -> 9:double) -> 10:double, DoubleScalarDivideDoubleColumn(val 9763215.5639, col 0) -> 9:double, DecimalColDivideDecimalScalar(col 11, val -1.389)(children: CastLongToDecimal(col 3) -> 11:decimal(19,0)) -> 12:decimal(28,6)
                   Statistics: Num rows: 1024 Data size: 307406 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1024 Data size: 307406 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_limit.q.out b/ql/src/test/results/clientpositive/llap/vectorization_limit.q.out
index 99d84fec8f..c7fdb6504e 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_limit.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_limit.q.out
@@ -260,13 +260,14 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: avg(_col1)
                       Group By Vectorization:
-                          aggregators: VectorUDAFAvgDouble(col 12) -> struct<count:bigint,sum:double>
+                          aggregators: VectorUDAFAvgDouble(col 12) -> struct<count:bigint,sum:double,input:double>
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDouble(col 12) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                       keys: _col0 (type: tinyint)
                       mode: hash
                       outputColumnNames: _col0, _col1
@@ -275,6 +276,13 @@ STAGE PLANS:
                         key expressions: _col0 (type: tinyint)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: tinyint)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            keyColumns: [0]
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            partitionColumns: [0]
+                            valueColumns: [1]
                         Statistics: Num rows: 95 Data size: 7888 Basic stats: COMPLETE Column stats: COMPLETE
                         TopN Hash Memory Usage: 0.3
                         value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>)
@@ -283,7 +291,7 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
@@ -295,24 +303,47 @@ STAGE PLANS:
                     partitionColumnCount: 0
                     scratchColumnTypeNames: double
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    dataColumns: KEY._col0:tinyint, VALUE._col0:struct<count:bigint,sum:double,input:double>
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 1) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0]
                 keys: KEY._col0 (type: tinyint)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 95 Data size: 1048 Basic stats: COMPLETE Column stats: COMPLETE
                 Limit
                   Number of rows: 20
+                  Limit Vectorization:
+                      className: VectorLimitOperator
+                      native: true
                   Statistics: Num rows: 20 Data size: 224 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 20 Data size: 224 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -394,9 +425,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: ctinyint (type: tinyint)
                       mode: hash
@@ -448,9 +481,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: tinyint)
                 mode: mergepartial
@@ -548,9 +583,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 5
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: ctinyint (type: tinyint), cdouble (type: double)
                       mode: hash
@@ -602,9 +639,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: tinyint), KEY._col1 (type: double)
                 mode: mergepartial
@@ -615,9 +654,11 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFCount(col 1) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: COMPLETE
                       vectorOutput: true
                       keyExpressions: col 0
                       native: false
+                      vectorProcessingMode: STREAMING
                       projectedOutputColumns: [0]
                   keys: _col0 (type: tinyint)
                   mode: complete
@@ -745,9 +786,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 5
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: cdouble (type: double)
                       mode: hash
@@ -801,9 +844,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: double)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_pushdown.q.out b/ql/src/test/results/clientpositive/llap/vectorization_pushdown.q.out
index 6a99fc36db..f068ad4fe2 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_pushdown.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_pushdown.q.out
@@ -46,18 +46,20 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:bigint> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0)
diff --git a/ql/src/test/results/clientpositive/llap/vectorization_short_regress.q.out b/ql/src/test/results/clientpositive/llap/vectorization_short_regress.q.out
index 465f4ea624..00577620d8 100644
--- a/ql/src/test/results/clientpositive/llap/vectorization_short_regress.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorization_short_regress.q.out
@@ -114,17 +114,22 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: avg(cint), sum(cdouble), stddev_pop(cint), stddev_samp(csmallint), var_samp(cint), avg(cfloat), stddev_samp(cint), min(ctinyint), count(csmallint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double>, VectorUDAFSumDouble(col 5) -> double, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double>, VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFCount(col 1) -> bigint
+                            aggregators: VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFSumDouble(col 5) -> double, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double,input:double>, VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFCount(col 1) -> bigint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                         Statistics: Num rows: 1 Data size: 492 Basic stats: COMPLETE Column stats: COMPLETE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 1 Data size: 492 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col0 (type: struct<count:bigint,sum:double,input:int>), _col1 (type: double), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: struct<count:bigint,sum:double,input:float>), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: tinyint), _col8 (type: bigint)
             Execution mode: vectorized, llap
@@ -132,30 +137,48 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:int> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), sum(VALUE._col1), stddev_pop(VALUE._col2), stddev_samp(VALUE._col3), var_samp(VALUE._col4), avg(VALUE._col5), stddev_samp(VALUE._col6), min(VALUE._col7), count(VALUE._col8)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFSumDouble(col 1) -> double, VectorUDAFStdPopFinal(col 2) -> double, VectorUDAFStdSampFinal(col 3) -> double, VectorUDAFVarSampFinal(col 4) -> double, VectorUDAFAvgFinal(col 5) -> double, VectorUDAFStdSampFinal(col 6) -> double, VectorUDAFMinLong(col 7) -> tinyint, VectorUDAFCountMerge(col 8) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                 Statistics: Num rows: 1 Data size: 68 Basic stats: COMPLETE Column stats: COMPLETE
                 Select Operator
                   expressions: _col0 (type: double), (_col0 + -3728.0) (type: double), (- (_col0 + -3728.0)) (type: double), (- (- (_col0 + -3728.0))) (type: double), ((- (- (_col0 + -3728.0))) * (_col0 + -3728.0)) (type: double), _col1 (type: double), (- _col0) (type: double), _col2 (type: double), (((- (- (_col0 + -3728.0))) * (_col0 + -3728.0)) * (- (- (_col0 + -3728.0)))) (type: double), _col3 (type: double), (- _col2) (type: double), (_col2 - (- (- (_col0 + -3728.0)))) (type: double), ((_col2 - (- (- (_col0 + -3728.0)))) * _col2) (type: double), _col4 (type: double), _col5 (type: double), (10.175 - _col4) (type: double), (- (10.175 - _col4)) (type: double), ((- _col2) / -563.0) (type: double), _col6 (type: double), (- ((- _col2) / -563.0)) (type: double), (_col0 / _col1) (type: double), _col7 (type: tinyint), _col8 (type: bigint), (UDFToDouble(_col7) / ((- _col2) / -563.0)) (type: double), (- (_col0 / _col1)) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 9, 11, 10, 14, 1, 12, 2, 15, 3, 13, 17, 16, 4, 5, 18, 20, 21, 6, 19, 22, 7, 8, 24, 25]
+                      selectExpressions: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 9:double, DoubleColUnaryMinus(col 10)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 10:double) -> 11:double, DoubleColUnaryMinus(col 12)(children: DoubleColUnaryMinus(col 10)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 10:double) -> 12:double) -> 10:double, DoubleColMultiplyDoubleColumn(col 12, col 13)(children: DoubleColUnaryMinus(col 13)(children: DoubleColUnaryMinus(col 12)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 12:double) -> 13:double) -> 12:double, DoubleColAddDoubleScalar(col 0, val -3728.0) -> 13:double) -> 14:double, DoubleColUnaryMinus(col 0) -> 12:double, DoubleColMultiplyDoubleColumn(col 16, col 13)(children: DoubleColMultiplyDoubleColumn(col 13, col 15)(children: DoubleColUnaryMinus(col 15)(children: DoubleColUnaryMinus(col 13)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 13:double) -> 15:double) -> 13:double, DoubleColAddDoubleScalar(col 0, val -3728.0) -> 15:double) -> 16:double, DoubleColUnaryMinus(col 15)(children: DoubleColUnaryMinus(col 13)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 13:double) -> 15:double) -> 13:double) -> 15:double, DoubleColUnaryMinus(col 2) -> 13:double, DoubleColSubtractDoubleColumn(col 2, col 16)(children: DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 16)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 16:double) -> 17:double) -> 16:double) -> 17:double, DoubleColMultiplyDoubleColumn(col 18, col 2)(children: DoubleColSubtractDoubleColumn(col 2, col 16)(children: DoubleColUnaryMinus(col 18)(children: DoubleColUnaryMinus(col 16)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 16:double) -> 18:double) -> 16:double) -> 18:double) -> 16:double, DoubleScalarSubtractDoubleColumn(val 10.175, col 4) -> 18:double, DoubleColUnaryMinus(col 19)(children: DoubleScalarSubtractDoubleColumn(val 10.175, col 4) -> 19:double) -> 20:double, DoubleColDivideDoubleScalar(col 19, val -563.0)(children: DoubleColUnaryMinus(col 2) -> 19:double) -> 21:double, DoubleColUnaryMinus(col 22)(children: DoubleColDivideDoubleScalar(col 19, val -563.0)(children: DoubleColUnaryMinus(col 2) -> 19:double) -> 22:double) -> 19:double, DoubleColDivideDoubleColumn(col 0, col 1) -> 22:double, DoubleColDivideDoubleColumn(col 23, col 25)(children: CastLongToDouble(col 7) -> 23:double, DoubleColDivideDoubleScalar(col 24, val -563.0)(children: DoubleColUnaryMinus(col 2) -> 24:double) -> 25:double) -> 24:double, DoubleColUnaryMinus(col 23)(children: DoubleColDivideDoubleColumn(col 0, col 1) -> 23:double) -> 25:double
                   Statistics: Num rows: 1 Data size: 196 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 196 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -355,17 +378,22 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: max(cint), var_pop(cbigint), stddev_pop(csmallint), max(cdouble), avg(ctinyint), min(cint), min(cdouble), stddev_samp(csmallint), var_samp(cint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFMaxLong(col 2) -> int, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 5) -> double, VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double>, VectorUDAFMinLong(col 2) -> int, VectorUDAFMinDouble(col 5) -> double, VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>
+                            aggregators: VectorUDAFMaxLong(col 2) -> int, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 5) -> double, VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFMinLong(col 2) -> int, VectorUDAFMinDouble(col 5) -> double, VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                         Statistics: Num rows: 1 Data size: 420 Basic stats: COMPLETE Column stats: COMPLETE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 1 Data size: 420 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: double), _col4 (type: struct<count:bigint,sum:double,input:tinyint>), _col5 (type: int), _col6 (type: double), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,variance:double>)
             Execution mode: vectorized, llap
@@ -373,30 +401,48 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF var_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col1] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), var_pop(VALUE._col1), stddev_pop(VALUE._col2), max(VALUE._col3), avg(VALUE._col4), min(VALUE._col5), min(VALUE._col6), stddev_samp(VALUE._col7), var_samp(VALUE._col8)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxLong(col 0) -> int, VectorUDAFVarPopFinal(col 1) -> double, VectorUDAFStdPopFinal(col 2) -> double, VectorUDAFMaxDouble(col 3) -> double, VectorUDAFAvgFinal(col 4) -> double, VectorUDAFMinLong(col 5) -> int, VectorUDAFMinDouble(col 6) -> double, VectorUDAFStdSampFinal(col 7) -> double, VectorUDAFVarSampFinal(col 8) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                 Statistics: Num rows: 1 Data size: 64 Basic stats: COMPLETE Column stats: COMPLETE
                 Select Operator
                   expressions: _col0 (type: int), (UDFToDouble(_col0) / -3728.0) (type: double), (_col0 * -3728) (type: int), _col1 (type: double), (- (_col0 * -3728)) (type: int), _col2 (type: double), (-563 % (_col0 * -3728)) (type: int), (_col1 / _col2) (type: double), (- _col2) (type: double), _col3 (type: double), _col4 (type: double), (_col2 - 10.175) (type: double), _col5 (type: int), (UDFToDouble((_col0 * -3728)) % (_col2 - 10.175)) (type: double), (- _col3) (type: double), _col6 (type: double), (_col3 % -26.28) (type: double), _col7 (type: double), (- (UDFToDouble(_col0) / -3728.0)) (type: double), ((- (_col0 * -3728)) % (-563 % (_col0 * -3728))) (type: int), ((UDFToDouble(_col0) / -3728.0) - _col4) (type: double), (- (_col0 * -3728)) (type: int), _col8 (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 10, 11, 1, 13, 2, 14, 9, 15, 3, 4, 16, 5, 19, 17, 6, 18, 7, 20, 12, 21, 23, 8]
+                      selectExpressions: DoubleColDivideDoubleScalar(col 9, val -3728.0)(children: CastLongToDouble(col 0) -> 9:double) -> 10:double, LongColMultiplyLongScalar(col 0, val -3728) -> 11:long, LongColUnaryMinus(col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 13:long, LongScalarModuloLongColumn(val -563, col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 14:long, DoubleColDivideDoubleColumn(col 1, col 2) -> 9:double, DoubleColUnaryMinus(col 2) -> 15:double, DoubleColSubtractDoubleScalar(col 2, val 10.175) -> 16:double, DoubleColModuloDoubleColumn(col 17, col 18)(children: CastLongToDouble(col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 17:double, DoubleColSubtractDoubleScalar(col 2, val 10.175) -> 18:double) -> 19:double, DoubleColUnaryMinus(col 3) -> 17:double, DoubleColModuloDoubleScalar(col 3, val -26.28) -> 18:double, DoubleColUnaryMinus(col 21)(children: DoubleColDivideDoubleScalar(col 20, val -3728.0)(children: CastLongToDouble(col 0) -> 20:double) -> 21:double) -> 20:double, LongColModuloLongColumn(col 22, col 23)(children: LongColUnaryMinus(col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 22:long, LongScalarModuloLongColumn(val -563, col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 23:long) -> 12:long, DoubleColSubtractDoubleColumn(col 24, col 4)(children: DoubleColDivideDoubleScalar(col 21, val -3728.0)(children: CastLongToDouble(col 0) -> 21:double) -> 24:double) -> 21:double, LongColUnaryMinus(col 22)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 22:long) -> 23:long
                   Statistics: Num rows: 1 Data size: 156 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 156 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -479,7 +525,7 @@ WHERE  (((cbigint <= 197)
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
--20301111	5445.576984978541	-1626869520	7.9684972882908944E16	1626869520	NULL	-563	NULL	NULL	NULL	-8.935323383084578	NULL	-1069736047	NULL	NULL	NULL	NULL	NULL	-5445.576984978541	511	5454.512308361625	1626869520	7.2647256545687792E16
+-20301111	5445.576984978541	-1626869520	7.9684972882908944E16	1626869520	NULL	-563	NULL	NULL	NULL	-8.935323383084578	NULL	-1069736047	NULL	NULL	NULL	NULL	NULL	-5445.576984978541	-58	5454.512308361625	1626869520	7.2647256545687792E16
 PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
 SELECT VAR_POP(cbigint),
        (-(VAR_POP(cbigint))),
@@ -588,17 +634,22 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: var_pop(cbigint), count(), max(ctinyint), stddev_pop(csmallint), max(cint), stddev_samp(cdouble), count(ctinyint), avg(ctinyint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxLong(col 2) -> int, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 0) -> bigint, VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double>
+                            aggregators: VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxLong(col 2) -> int, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 0) -> bigint, VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double,input:bigint>
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                         Statistics: Num rows: 1 Data size: 340 Basic stats: COMPLETE Column stats: COMPLETE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 1 Data size: 340 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: bigint), _col2 (type: tinyint), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: int), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: bigint), _col7 (type: struct<count:bigint,sum:double,input:tinyint>)
             Execution mode: vectorized, llap
@@ -606,30 +657,48 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF var_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: var_pop(VALUE._col0), count(VALUE._col1), max(VALUE._col2), stddev_pop(VALUE._col3), max(VALUE._col4), stddev_samp(VALUE._col5), count(VALUE._col6), avg(VALUE._col7)
+                Group By Vectorization:
+                    aggregators: VectorUDAFVarPopFinal(col 0) -> double, VectorUDAFCountMerge(col 1) -> bigint, VectorUDAFMaxLong(col 2) -> tinyint, VectorUDAFStdPopFinal(col 3) -> double, VectorUDAFMaxLong(col 4) -> int, VectorUDAFStdSampFinal(col 5) -> double, VectorUDAFCountMerge(col 6) -> bigint, VectorUDAFAvgFinal(col 7) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                 Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: COMPLETE
                 Select Operator
                   expressions: _col0 (type: double), (- _col0) (type: double), (_col0 - (- _col0)) (type: double), _col1 (type: bigint), (CAST( _col1 AS decimal(19,0)) % 79.553) (type: decimal(5,3)), _col2 (type: tinyint), (UDFToDouble(_col1) - (- _col0)) (type: double), (- (- _col0)) (type: double), (-1.0 % (- _col0)) (type: double), _col1 (type: bigint), (- _col1) (type: bigint), _col3 (type: double), (- (- (- _col0))) (type: double), (762 * (- _col1)) (type: bigint), _col4 (type: int), (UDFToLong(_col2) + (762 * (- _col1))) (type: bigint), ((- _col0) + UDFToDouble(_col4)) (type: double), _col5 (type: double), ((- _col1) % _col1) (type: bigint), _col6 (type: bigint), _col7 (type: double), (-3728 % (UDFToLong(_col2) + (762 * (- _col1)))) (type: bigint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 8, 10, 1, 12, 2, 14, 13, 15, 1, 16, 3, 9, 19, 4, 18, 22, 5, 23, 6, 7, 24]
+                      selectExpressions: DoubleColUnaryMinus(col 0) -> 8:double, DoubleColSubtractDoubleColumn(col 0, col 9)(children: DoubleColUnaryMinus(col 0) -> 9:double) -> 10:double, DecimalColModuloDecimalScalar(col 11, val 79.553)(children: CastLongToDecimal(col 1) -> 11:decimal(19,0)) -> 12:decimal(5,3), DoubleColSubtractDoubleColumn(col 9, col 13)(children: CastLongToDouble(col 1) -> 9:double, DoubleColUnaryMinus(col 0) -> 13:double) -> 14:double, DoubleColUnaryMinus(col 9)(children: DoubleColUnaryMinus(col 0) -> 9:double) -> 13:double, DoubleScalarModuloDoubleColumn(val -1.0, col 9)(children: DoubleColUnaryMinus(col 0) -> 9:double) -> 15:double, LongColUnaryMinus(col 1) -> 16:long, DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 9)(children: DoubleColUnaryMinus(col 0) -> 9:double) -> 17:double) -> 9:double, LongScalarMultiplyLongColumn(val 762, col 18)(children: LongColUnaryMinus(col 1) -> 18:long) -> 19:long, LongColAddLongColumn(col 2, col 20)(children: col 2, LongScalarMultiplyLongColumn(val 762, col 18)(children: LongColUnaryMinus(col 1) -> 18:long) -> 20:long) -> 18:long, DoubleColAddDoubleColumn(col 17, col 21)(children: DoubleColUnaryMinus(col 0) -> 17:double, CastLongToDouble(col 4) -> 21:double) -> 22:double, LongColModuloLongColumn(col 20, col 1)(children: LongColUnaryMinus(col 1) -> 20:long) -> 23:long, LongScalarModuloLongColumn(val -3728, col 20)(children: LongColAddLongColumn(col 2, col 24)(children: col 2, LongScalarMultiplyLongColumn(val 762, col 20)(children: LongColUnaryMinus(col 1) -> 20:long) -> 24:long) -> 20:long) -> 24:long
                   Statistics: Num rows: 1 Data size: 272 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 272 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -800,17 +869,22 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: avg(ctinyint), max(cbigint), stddev_samp(cint), var_pop(cint), var_pop(cbigint), max(cfloat)
                         Group By Vectorization:
-                            aggregators: VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double>, VectorUDAFMaxLong(col 3) -> bigint, VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float
+                            aggregators: VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFMaxLong(col 3) -> bigint, VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                         Statistics: Num rows: 1 Data size: 328 Basic stats: COMPLETE Column stats: COMPLETE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 1 Data size: 328 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col0 (type: struct<count:bigint,sum:double,input:tinyint>), _col1 (type: bigint), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: float)
             Execution mode: vectorized, llap
@@ -818,30 +892,48 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:tinyint> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), max(VALUE._col1), stddev_samp(VALUE._col2), var_pop(VALUE._col3), var_pop(VALUE._col4), max(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFMaxLong(col 1) -> bigint, VectorUDAFStdSampFinal(col 2) -> double, VectorUDAFVarPopFinal(col 3) -> double, VectorUDAFVarPopFinal(col 4) -> double, VectorUDAFMaxDouble(col 5) -> float
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                 Statistics: Num rows: 1 Data size: 44 Basic stats: COMPLETE Column stats: COMPLETE
                 Select Operator
                   expressions: _col0 (type: double), (_col0 + 6981.0) (type: double), ((_col0 + 6981.0) + _col0) (type: double), _col1 (type: bigint), (((_col0 + 6981.0) + _col0) / _col0) (type: double), (- (_col0 + 6981.0)) (type: double), _col2 (type: double), (_col0 % (- (_col0 + 6981.0))) (type: double), _col3 (type: double), _col4 (type: double), (- _col1) (type: bigint), (UDFToDouble((- _col1)) / _col2) (type: double), _col5 (type: float), (_col4 * -26.28) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 8, 1, 7, 10, 2, 9, 3, 4, 12, 14, 5, 11]
+                      selectExpressions: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 6:double, DoubleColAddDoubleColumn(col 7, col 0)(children: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 7:double) -> 8:double, DoubleColDivideDoubleColumn(col 9, col 0)(children: DoubleColAddDoubleColumn(col 7, col 0)(children: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 7:double) -> 9:double) -> 7:double, DoubleColUnaryMinus(col 9)(children: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 9:double) -> 10:double, DoubleColModuloDoubleColumn(col 0, col 11)(children: DoubleColUnaryMinus(col 9)(children: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 9:double) -> 11:double) -> 9:double, LongColUnaryMinus(col 1) -> 12:long, DoubleColDivideDoubleColumn(col 11, col 2)(children: CastLongToDouble(col 13)(children: LongColUnaryMinus(col 1) -> 13:long) -> 11:double) -> 14:double, DoubleColMultiplyDoubleScalar(col 4, val -26.28) -> 11:double
                   Statistics: Num rows: 1 Data size: 108 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 108 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -2119,11 +2211,12 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: csmallint (type: smallint)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4
@@ -2132,6 +2225,10 @@ STAGE PLANS:
                           key expressions: _col0 (type: smallint)
                           sort order: +
                           Map-reduce partition columns: _col0 (type: smallint)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkLongOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 1128 Data size: 201900 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: bigint), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: bigint)
             Execution mode: vectorized, llap
@@ -2139,21 +2236,32 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev_samp(VALUE._col0), sum(VALUE._col1), var_pop(VALUE._col2), count(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFStdSampFinal(col 1) -> double, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFVarPopFinal(col 3) -> double, VectorUDAFCountMerge(col 4) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3]
                 keys: KEY._col0 (type: smallint)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
@@ -2161,10 +2269,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: smallint), (UDFToInteger(_col0) % -75) (type: int), _col1 (type: double), (-1.389 / CAST( _col0 AS decimal(5,0))) (type: decimal(10,9)), _col2 (type: bigint), (UDFToDouble((UDFToInteger(_col0) % -75)) / UDFToDouble(_col2)) (type: double), (- (UDFToInteger(_col0) % -75)) (type: int), _col3 (type: double), (- (- (UDFToInteger(_col0) % -75))) (type: int), _col4 (type: bigint), (_col4 - -89010) (type: bigint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 5, 1, 7, 2, 11, 12, 3, 8, 4, 13]
+                      selectExpressions: LongColModuloLongScalar(col 0, val -75)(children: col 0) -> 5:long, DecimalScalarDivideDecimalColumn(val -1.389, col 6)(children: CastLongToDecimal(col 0) -> 6:decimal(5,0)) -> 7:decimal(10,9), DoubleColDivideDoubleColumn(col 9, col 10)(children: CastLongToDouble(col 8)(children: LongColModuloLongScalar(col 0, val -75)(children: col 0) -> 8:long) -> 9:double, CastLongToDouble(col 2) -> 10:double) -> 11:double, LongColUnaryMinus(col 8)(children: LongColModuloLongScalar(col 0, val -75)(children: col 0) -> 8:long) -> 12:long, LongColUnaryMinus(col 13)(children: LongColUnaryMinus(col 8)(children: LongColModuloLongScalar(col 0, val -75)(children: col 0) -> 8:long) -> 13:long) -> 8:long, LongColSubtractLongScalar(col 4, val -89010) -> 13:long
                   Statistics: Num rows: 1128 Data size: 197388 Basic stats: COMPLETE Column stats: COMPLETE
                   Reduce Output Operator
                     key expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: double), _col3 (type: decimal(10,9)), _col4 (type: bigint), _col5 (type: double), _col6 (type: int), _col7 (type: double), _col8 (type: int), _col9 (type: bigint), _col10 (type: bigint)
                     sort order: +++++++++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 1128 Data size: 197388 Basic stats: COMPLETE Column stats: COMPLETE
                     TopN Hash Memory Usage: 0.1
         Reducer 3 
@@ -2374,11 +2491,12 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFVarSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 4) -> bigint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFVarPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumDouble(col 5) -> double
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 5
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFVarSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: cdouble (type: double)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
@@ -2387,6 +2505,10 @@ STAGE PLANS:
                           key expressions: _col0 (type: double)
                           sort order: +
                           Map-reduce partition columns: _col0 (type: double)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkMultiKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 870 Data size: 234888 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: bigint), _col3 (type: double), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: double)
             Execution mode: vectorized, llap
@@ -2394,21 +2516,32 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF var_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: var_samp(VALUE._col0), count(VALUE._col1), sum(VALUE._col2), var_pop(VALUE._col3), stddev_pop(VALUE._col4), sum(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFVarSampFinal(col 1) -> double, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFSumDouble(col 3) -> double, VectorUDAFVarPopFinal(col 4) -> double, VectorUDAFStdPopFinal(col 5) -> double, VectorUDAFSumDouble(col 6) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 keys: KEY._col0 (type: double)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
@@ -2416,10 +2549,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: double), _col1 (type: double), (2563.58 * _col1) (type: double), (- _col1) (type: double), _col2 (type: bigint), ((2563.58 * _col1) + -5638.15) (type: double), ((- _col1) * ((2563.58 * _col1) + -5638.15)) (type: double), _col3 (type: double), _col4 (type: double), (_col0 - (- _col1)) (type: double), _col5 (type: double), (_col0 + _col1) (type: double), (_col0 * 762.0) (type: double), _col6 (type: double), (-863.257 % (_col0 * 762.0)) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 7, 8, 2, 10, 11, 3, 4, 12, 5, 9, 13, 6, 15]
+                      selectExpressions: DoubleScalarMultiplyDoubleColumn(val 2563.58, col 1) -> 7:double, DoubleColUnaryMinus(col 1) -> 8:double, DoubleColAddDoubleScalar(col 9, val -5638.15)(children: DoubleScalarMultiplyDoubleColumn(val 2563.58, col 1) -> 9:double) -> 10:double, DoubleColMultiplyDoubleColumn(col 9, col 12)(children: DoubleColUnaryMinus(col 1) -> 9:double, DoubleColAddDoubleScalar(col 11, val -5638.15)(children: DoubleScalarMultiplyDoubleColumn(val 2563.58, col 1) -> 11:double) -> 12:double) -> 11:double, DoubleColSubtractDoubleColumn(col 0, col 9)(children: DoubleColUnaryMinus(col 1) -> 9:double) -> 12:double, DoubleColAddDoubleColumn(col 0, col 1) -> 9:double, DoubleColMultiplyDoubleScalar(col 0, val 762.0) -> 13:double, DoubleScalarModuloDoubleColumn(val -863.257, col 14)(children: DoubleColMultiplyDoubleScalar(col 0, val 762.0) -> 14:double) -> 15:double
                   Statistics: Num rows: 870 Data size: 109608 Basic stats: COMPLETE Column stats: COMPLETE
                   Reduce Output Operator
                     key expressions: _col0 (type: double)
                     sort order: +
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 870 Data size: 109608 Basic stats: COMPLETE Column stats: COMPLETE
                     value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: double), _col4 (type: bigint), _col5 (type: double), _col6 (type: double), _col7 (type: double), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: double), _col12 (type: double), _col13 (type: double), _col14 (type: double)
         Reducer 3 
@@ -2671,13 +2813,14 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: stddev_pop(cint), avg(csmallint), count(), min(ctinyint), var_samp(csmallint), var_pop(cfloat), avg(cint), var_samp(cfloat), avg(cfloat), min(cdouble), var_pop(csmallint), stddev_pop(ctinyint), sum(cint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFVarSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double>, VectorUDAFVarSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double>, VectorUDAFMinDouble(col 5) -> double, VectorUDAFVarPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 2) -> bigint
+                            aggregators: VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFVarSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFVarSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double,input:double>, VectorUDAFMinDouble(col 5) -> double, VectorUDAFVarPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 2) -> bigint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 8, col 6
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: ctimestamp1 (type: timestamp), cstring1 (type: string)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
@@ -2686,6 +2829,10 @@ STAGE PLANS:
                           key expressions: _col0 (type: timestamp), _col1 (type: string)
                           sort order: ++
                           Map-reduce partition columns: _col0 (type: timestamp), _col1 (type: string)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkMultiKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 6144 Data size: 5199016 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,input:smallint>), _col4 (type: bigint), _col5 (type: tinyint), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,input:int>), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,input:float>), _col11 (type: double), _col12 (type: struct<count:bigint,sum:double,variance:double>), _col13 (type: struct<count:bigint,sum:double,variance:double>), _col14 (type: bigint)
             Execution mode: vectorized, llap
@@ -2693,21 +2840,32 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev_pop(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), min(VALUE._col3), var_samp(VALUE._col4), var_pop(VALUE._col5), avg(VALUE._col6), var_samp(VALUE._col7), avg(VALUE._col8), min(VALUE._col9), var_pop(VALUE._col10), stddev_pop(VALUE._col11), sum(VALUE._col12)
+                Group By Vectorization:
+                    aggregators: VectorUDAFStdPopFinal(col 2) -> double, VectorUDAFAvgFinal(col 3) -> double, VectorUDAFCountMerge(col 4) -> bigint, VectorUDAFMinLong(col 5) -> tinyint, VectorUDAFVarSampFinal(col 6) -> double, VectorUDAFVarPopFinal(col 7) -> double, VectorUDAFAvgFinal(col 8) -> double, VectorUDAFVarSampFinal(col 9) -> double, VectorUDAFAvgFinal(col 10) -> double, VectorUDAFMinDouble(col 11) -> double, VectorUDAFVarPopFinal(col 12) -> double, VectorUDAFStdPopFinal(col 13) -> double, VectorUDAFSumLong(col 14) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
                 keys: KEY._col0 (type: timestamp), KEY._col1 (type: string)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
@@ -2715,10 +2873,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: timestamp), _col1 (type: string), _col2 (type: double), (_col2 * 10.175) (type: double), (- _col2) (type: double), _col3 (type: double), (- _col2) (type: double), (-26.28 - _col2) (type: double), _col4 (type: bigint), (- _col4) (type: bigint), ((-26.28 - _col2) * (- _col2)) (type: double), _col5 (type: tinyint), (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4))) (type: double), (- (_col2 * 10.175)) (type: double), _col6 (type: double), (_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) (type: double), (- (- _col2)) (type: double), (UDFToDouble((- _col4)) / _col2) (type: double), _col7 (type: double), (10.175 / _col3) (type: double), _col8 (type: double), _col9 (type: double), ((_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) - (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) (type: double), (- (- (_col2 * 10.175))) (type: double), _col10 (type: double), (((_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) - (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) * 10.175) (type: double), (10.175 % (10.175 / _col3)) (type: double), (- _col5) (type: tinyint), _col11 (type: double), _col12 (type: double), (- ((-26.28 - _col2) * (- _col2))) (type: double), ((- _col2) % _col10) (type: double), (-26.28 / CAST( (- _col5) AS decimal(3,0))) (type: decimal(8,6)), _col13 (type: double), _col14 (type: bigint), ((_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) / _col7) (type: double), (- (- _col4)) (type: bigint), _col4 (type: bigint), ((_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) % -26.28) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25, _col26, _col27, _col28, _col29, _col30, _col31, _col32, _col33, _col34, _col35, _col36, _col37, _col38
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 15, 16, 3, 17, 18, 4, 19, 22, 5, 21, 23, 6, 20, 26, 27, 7, 25, 8, 9, 29, 28, 10, 30, 32, 24, 11, 12, 31, 34, 37, 13, 14, 38, 40, 4, 39]
+                      selectExpressions: DoubleColMultiplyDoubleScalar(col 2, val 10.175) -> 15:double, DoubleColUnaryMinus(col 2) -> 16:double, DoubleColUnaryMinus(col 2) -> 17:double, DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 18:double, LongColUnaryMinus(col 4) -> 19:long, DoubleColMultiplyDoubleColumn(col 20, col 21)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 20:double, DoubleColUnaryMinus(col 2) -> 21:double) -> 22:double, DoubleColMultiplyDoubleColumn(col 23, col 20)(children: DoubleColMultiplyDoubleColumn(col 20, col 21)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 20:double, DoubleColUnaryMinus(col 2) -> 21:double) -> 23:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 20:double) -> 21:double, DoubleColUnaryMinus(col 20)(children: DoubleColMultiplyDoubleScalar(col 2, val 10.175) -> 20:double) -> 23:double, DoubleColAddDoubleColumn(col 6, col 25)(children: DoubleColMultiplyDoubleColumn(col 26, col 20)(children: DoubleColMultiplyDoubleColumn(col 20, col 25)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 20:double, DoubleColUnaryMinus(col 2) -> 25:double) -> 26:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 20:double) -> 25:double) -> 20:double, DoubleColUnaryMinus(col 25)(children: DoubleColUnaryMinus(col 2) -> 25:double) -> 26:double, DoubleColDivideDoubleColumn(col 25, col 2)(children: CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 25:double) -> 27:double, DoubleScalarDivideDoubleColumn(val 10.175, col 3) -> 25:double, DoubleColSubtractDoubleColumn(col 28, col 30)(children: DoubleColAddDoubleColumn(col 6, col 29)(children: DoubleColMultiplyDoubleColumn(col 30, col 28)(children: DoubleColMultiplyDoubleColumn(col 28, col 29)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 28:double, DoubleColUnaryMinus(col 2) -> 29:double) -> 30:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 28:double) -> 29:double) -> 28:double, DoubleColMultiplyDoubleColumn(col 31, col 29)(children: DoubleColMultiplyDoubleColumn(col 29, col 30)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 29:double, DoubleColUnaryMinus(col 2) -> 30:double) -> 31:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 29:double) -> 30:double) -> 29:double, DoubleColUnaryMinus(col 30)(children: DoubleColUnaryMinus(col 28)(children: DoubleColMultiplyDoubleScalar(col 2, val 10.175) -> 28:double) -> 30:double) -> 28:double, DoubleColMultiplyDoubleScalar(col 31, val 10.175)(children: DoubleColSubtractDoubleColumn(col 30, col 32)(children: DoubleColAddDoubleColumn(col 6, col 31)(children: DoubleColMultiplyDoubleColumn(col 32, col 30)(children: DoubleColMultiplyDoubleColumn(col 30, col 31)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 30:double, DoubleColUnaryMinus(col 2) -> 31:double) -> 32:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 30:double) -> 31:double) -> 30:double, DoubleColMultiplyDoubleColumn(col 33, col 31)(children: DoubleColMultiplyDoubleColumn(col 31, col 32)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 31:double, DoubleColUnaryMinus(col 2) -> 32:double) -> 33:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 31:double) -> 32:double) -> 31:double) -> 30:double, DoubleScalarModuloDoubleColumn(val 10.175, col 31)(children: DoubleScalarDivideDoubleColumn(val 10.175, col 3) -> 31:double) -> 32:double, LongColUnaryMinus(col 5) -> 24:long, DoubleColUnaryMinus(col 34)(children: DoubleColMultiplyDoubleColumn(col 31, col 33)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 31:double, DoubleColUnaryMinus(col 2) -> 33:double) -> 34:double) -> 31:double, DoubleColModuloDoubleColumn(col 33, col 10)(children: DoubleColUnaryMinus(col 2) -> 33:double) -> 34:double, DecimalScalarDivideDecimalColumn(val -26.28, col 36)(children: CastLongToDecimal(col 35)(children: LongColUnaryMinus(col 5) -> 35:long) -> 36:decimal(3,0)) -> 37:decimal(8,6), DoubleColDivideDoubleColumn(col 33, col 7)(children: DoubleColAddDoubleColumn(col 6, col 38)(children: DoubleColMultiplyDoubleColumn(col 39, col 33)(children: DoubleColMultiplyDoubleColumn(col 33, col 38)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 33:double, DoubleColUnaryMinus(col 2) -> 38:double) -> 39:double, CastLongToDouble(col 35)(children: LongColUnaryMinus(col 4) -> 35:long) -> 33:double) -> 38:double) -> 33:double) -> 38:double, LongColUnaryMinus(col 35)(children: LongColUnaryMinus(col 4) -> 35:long) -> 40:long, DoubleColModuloDoubleScalar(col 33, val -26.28)(children: DoubleColAddDoubleColumn(col 6, col 39)(children: DoubleColMultiplyDoubleColumn(col 41, col 33)(children: DoubleColMultiplyDoubleColumn(col 33, col 39)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 33:double, DoubleColUnaryMinus(col 2) -> 39:double) -> 41:double, CastLongToDouble(col 35)(children: LongColUnaryMinus(col 4) -> 35:long) -> 33:double) -> 39:double) -> 33:double) -> 39:double
                   Statistics: Num rows: 3072 Data size: 1542740 Basic stats: COMPLETE Column stats: COMPLETE
                   Reduce Output Operator
                     key expressions: _col0 (type: timestamp), _col1 (type: string), _col2 (type: double), _col3 (type: double), _col4 (type: double), _col5 (type: double), _col6 (type: double), _col7 (type: double), _col8 (type: bigint), _col9 (type: bigint), _col10 (type: double), _col11 (type: tinyint), _col12 (type: double), _col13 (type: double), _col14 (type: double), _col15 (type: double), _col16 (type: double), _col17 (type: double), _col18 (type: double), _col19 (type: double), _col20 (type: double), _col21 (type: double), _col22 (type: double), _col23 (type: double), _col24 (type: double), _col25 (type: double), _col26 (type: double), _col27 (type: tinyint), _col28 (type: double), _col29 (type: double), _col30 (type: double), _col31 (type: double), _col32 (type: decimal(8,6)), _col33 (type: double), _col34 (type: bigint), _col35 (type: double), _col36 (type: bigint), _col37 (type: bigint), _col38 (type: double)
                     sort order: +++++++++++++++++++++++++++++++++++++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 3072 Data size: 1542740 Basic stats: COMPLETE Column stats: COMPLETE
                     TopN Hash Memory Usage: 0.1
         Reducer 3 
@@ -3051,13 +3218,14 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: max(cfloat), sum(cbigint), var_samp(cint), avg(cdouble), min(cbigint), var_pop(cbigint), sum(cint), stddev_samp(ctinyint), stddev_pop(csmallint), avg(cint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFMaxDouble(col 4) -> float, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double>, VectorUDAFMinLong(col 3) -> bigint, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFStdSampLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double>
+                            aggregators: VectorUDAFMaxDouble(col 4) -> float, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>, VectorUDAFMinLong(col 3) -> bigint, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFStdSampLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 10
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                         keys: cboolean1 (type: boolean)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -3066,6 +3234,10 @@ STAGE PLANS:
                           key expressions: _col0 (type: boolean)
                           sort order: +
                           Map-reduce partition columns: _col0 (type: boolean)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkLongOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 3 Data size: 1524 Basic stats: COMPLETE Column stats: COMPLETE
                           value expressions: _col1 (type: float), _col2 (type: bigint), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,input:double>), _col5 (type: bigint), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: bigint), _col8 (type: struct<count:bigint,sum:double,variance:double>), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,input:int>)
             Execution mode: vectorized, llap
@@ -3073,21 +3245,32 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF var_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col2] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), sum(VALUE._col1), var_samp(VALUE._col2), avg(VALUE._col3), min(VALUE._col4), var_pop(VALUE._col5), sum(VALUE._col6), stddev_samp(VALUE._col7), stddev_pop(VALUE._col8), avg(VALUE._col9)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxDouble(col 1) -> float, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFVarSampFinal(col 3) -> double, VectorUDAFAvgFinal(col 4) -> double, VectorUDAFMinLong(col 5) -> bigint, VectorUDAFVarPopFinal(col 6) -> double, VectorUDAFSumLong(col 7) -> bigint, VectorUDAFStdSampFinal(col 8) -> double, VectorUDAFStdPopFinal(col 9) -> double, VectorUDAFAvgFinal(col 10) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -3095,10 +3278,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: boolean), _col1 (type: float), (- _col1) (type: float), (-26.28 / UDFToDouble(_col1)) (type: double), _col2 (type: bigint), (CAST( _col2 AS decimal(19,0)) - 10.175) (type: decimal(23,3)), _col3 (type: double), (_col3 % UDFToDouble(_col1)) (type: double), (10.175 + (- _col1)) (type: float), _col4 (type: double), (UDFToDouble((CAST( _col2 AS decimal(19,0)) - 10.175)) + _col3) (type: double), _col5 (type: bigint), _col6 (type: double), (- (10.175 + (- _col1))) (type: float), (79.553 / _col6) (type: double), (_col3 % (79.553 / _col6)) (type: double), _col7 (type: bigint), _col8 (type: double), (-1.389 * CAST( _col5 AS decimal(19,0))) (type: decimal(24,3)), (CAST( _col7 AS decimal(19,0)) - (-1.389 * CAST( _col5 AS decimal(19,0)))) (type: decimal(25,3)), _col9 (type: double), (- (CAST( _col7 AS decimal(19,0)) - (-1.389 * CAST( _col5 AS decimal(19,0))))) (type: decimal(25,3)), _col10 (type: double), (- _col10) (type: double), (_col10 * UDFToDouble(_col7)) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 11, 12, 2, 14, 3, 15, 17, 4, 19, 5, 6, 16, 20, 22, 7, 8, 23, 26, 9, 28, 10, 21, 30]
+                      selectExpressions: DoubleColUnaryMinus(col 1) -> 11:double, DoubleScalarDivideDoubleColumn(val -26.28, col 1)(children: col 1) -> 12:double, DecimalColSubtractDecimalScalar(col 13, val 10.175)(children: CastLongToDecimal(col 2) -> 13:decimal(19,0)) -> 14:decimal(23,3), DoubleColModuloDoubleColumn(col 3, col 1)(children: col 1) -> 15:double, DoubleScalarAddDoubleColumn(val 10.175000190734863, col 16)(children: DoubleColUnaryMinus(col 1) -> 16:double) -> 17:double, DoubleColAddDoubleColumn(col 16, col 3)(children: CastDecimalToDouble(col 18)(children: DecimalColSubtractDecimalScalar(col 13, val 10.175)(children: CastLongToDecimal(col 2) -> 13:decimal(19,0)) -> 18:decimal(23,3)) -> 16:double) -> 19:double, DoubleColUnaryMinus(col 20)(children: DoubleScalarAddDoubleColumn(val 10.175000190734863, col 16)(children: DoubleColUnaryMinus(col 1) -> 16:double) -> 20:double) -> 16:double, DoubleScalarDivideDoubleColumn(val 79.553, col 6) -> 20:double, DoubleColModuloDoubleColumn(col 3, col 21)(children: DoubleScalarDivideDoubleColumn(val 79.553, col 6) -> 21:double) -> 22:double, DecimalScalarMultiplyDecimalColumn(val -1.389, col 13)(children: CastLongToDecimal(col 5) -> 13:decimal(19,0)) -> 23:decimal(24,3), DecimalColSubtractDecimalColumn(col 13, col 25)(children: CastLongToDecimal(col 7) -> 13:decimal(19,0), DecimalScalarMultiplyDecimalColumn(val -1.389, col 24)(children: CastLongToDecimal(col 5) -> 24:decimal(19,0)) -> 25:decimal(24,3)) -> 26:decimal(25,3), FuncNegateDecimalToDecimal(col 27)(children: DecimalColSubtractDecimalColumn(col 13, col 25)(children: CastLongToDecimal(col 7) -> 13:decimal(19,0), DecimalScalarMultiplyDecimalColumn(val -1.389, col 24)(children: CastLongToDecimal(col 5) -> 24:decimal(19,0)) -> 25:decimal(24,3)) -> 27:decimal(25,3)) -> 28:decimal(25,3), DoubleColUnaryMinus(col 10) -> 21:double, DoubleColMultiplyDoubleColumn(col 10, col 29)(children: CastLongToDouble(col 7) -> 29:double) -> 30:double
                   Statistics: Num rows: 3 Data size: 1800 Basic stats: COMPLETE Column stats: COMPLETE
                   Reduce Output Operator
                     key expressions: _col0 (type: boolean)
                     sort order: +
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 3 Data size: 1800 Basic stats: COMPLETE Column stats: COMPLETE
                     value expressions: _col1 (type: float), _col2 (type: float), _col3 (type: double), _col4 (type: bigint), _col5 (type: decimal(23,3)), _col6 (type: double), _col7 (type: double), _col8 (type: float), _col9 (type: double), _col10 (type: double), _col11 (type: bigint), _col12 (type: double), _col13 (type: float), _col14 (type: double), _col15 (type: double), _col17 (type: bigint), _col18 (type: double), _col19 (type: decimal(24,3)), _col20 (type: decimal(25,3)), _col21 (type: double), _col22 (type: decimal(25,3)), _col23 (type: double), _col24 (type: double), _col25 (type: double)
         Reducer 3 
@@ -3271,8 +3463,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3310,8 +3504,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3385,8 +3581,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3424,8 +3622,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3571,8 +3771,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3610,8 +3812,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3685,8 +3889,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3724,8 +3930,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3799,8 +4007,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 2) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3838,8 +4048,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3913,8 +4125,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 4) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3952,8 +4166,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -4027,8 +4243,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 6) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -4066,8 +4284,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -4141,8 +4361,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 10) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -4180,8 +4402,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_case.q.out b/ql/src/test/results/clientpositive/llap/vectorized_case.q.out
index 0beaba82b7..80add939e7 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_case.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_case.q.out
@@ -290,8 +290,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 12) -> bigint, VectorUDAFSumLong(col 13) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1]
                       mode: hash
                       outputColumnNames: _col0, _col1
@@ -329,8 +331,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
@@ -417,8 +421,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 12) -> bigint, VectorUDAFSumLong(col 13) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1]
                       mode: hash
                       outputColumnNames: _col0, _col1
@@ -456,8 +462,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_date_funcs.q.out b/ql/src/test/results/clientpositive/llap/vectorized_date_funcs.q.out
index 59badfbd93..39e8096bed 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_date_funcs.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_date_funcs.q.out
@@ -1261,8 +1261,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinLong(col 0) -> date, VectorUDAFMaxLong(col 0) -> date, VectorUDAFCount(col 0) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -1300,8 +1302,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> date, VectorUDAFMaxLong(col 1) -> date, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_distinct_gby.q.out b/ql/src/test/results/clientpositive/llap/vectorized_distinct_gby.q.out
index cf2db94991..c3e5f7c90d 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_distinct_gby.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_distinct_gby.q.out
@@ -16,9 +16,11 @@ POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dtest
 POSTHOOK: Lineage: dtest.a SCRIPT []
 POSTHOOK: Lineage: dtest.b SIMPLE []
-PREHOOK: query: explain vectorization select sum(distinct a), count(distinct a) from dtest
+PREHOOK: query: explain vectorization detail
+select sum(distinct a), count(distinct a) from dtest
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization select sum(distinct a), count(distinct a) from dtest
+POSTHOOK: query: explain vectorization detail
+select sum(distinct a), count(distinct a) from dtest
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
   enabled: true
@@ -41,22 +43,51 @@ STAGE PLANS:
                 TableScan
                   alias: dtest
                   Statistics: Num rows: 5 Data size: 40 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
                   Select Operator
                     expressions: a (type: int)
                     outputColumnNames: a
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
                     Statistics: Num rows: 5 Data size: 40 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
+                      Group By Vectorization:
+                          className: VectorGroupByOperator
+                          groupByMode: FINAL
+                          vectorOutput: true
+                          keyExpressions: col 0
+                          native: false
+                          vectorProcessingMode: STREAMING
+                          projectedOutputColumns: []
                       keys: a (type: int)
                       mode: final
                       outputColumnNames: _col0
                       Statistics: Num rows: 2 Data size: 16 Basic stats: COMPLETE Column stats: NONE
                       Group By Operator
                         aggregations: sum(_col0), count(_col0)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFCount(col 0) -> bigint
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1]
                         mode: hash
                         outputColumnNames: _col0, _col1
                         Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1]
                           Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col0 (type: bigint), _col1 (type: bigint)
             Execution mode: vectorized, llap
@@ -69,23 +100,45 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    includeColumns: [0]
+                    dataColumns: a:int, b:int
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    dataColumns: VALUE._col0:bigint, VALUE._col1:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: sum(VALUE._col0), count(VALUE._col1)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFCountMerge(col 1) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -107,9 +160,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dtest
 #### A masked pattern was here ####
 300	1
-PREHOOK: query: explain vectorization select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc
+PREHOOK: query: explain vectorization detail
+select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc
+POSTHOOK: query: explain vectorization detail
+select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
   enabled: true
@@ -133,11 +188,26 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 36696 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Select Operator
                     expressions: cint (type: int)
                     outputColumnNames: cint
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [2]
                     Statistics: Num rows: 12288 Data size: 36696 Basic stats: COMPLETE Column stats: COMPLETE
                     Group By Operator
+                      Group By Vectorization:
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          keyExpressions: col 2
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: []
                       keys: cint (type: int)
                       mode: hash
                       outputColumnNames: _col0
@@ -146,6 +216,12 @@ STAGE PLANS:
                         key expressions: _col0 (type: int)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: int)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkLongOperator
+                            keyColumns: [0]
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: []
                         Statistics: Num rows: 5775 Data size: 17248 Basic stats: COMPLETE Column stats: COMPLETE
             Execution mode: vectorized, llap
             LLAP IO: all inputs
@@ -157,45 +233,97 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [2]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                groupByVectorOutput: false
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
+                groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: KEY._col0:int
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
+                Group By Vectorization:
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
                 outputColumnNames: _col0
                 Statistics: Num rows: 5775 Data size: 17248 Basic stats: COMPLETE Column stats: COMPLETE
                 Group By Operator
                   aggregations: sum(_col0), count(_col0), avg(_col0), std(_col0)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2, 3]
                   mode: hash
                   outputColumnNames: _col0, _col1, _col2, _col3
                   Statistics: Num rows: 1 Data size: 172 Basic stats: COMPLETE Column stats: COMPLETE
                   Reduce Output Operator
                     sort order: 
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkEmptyKeyOperator
+                        keyColumns: []
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        valueColumns: [0, 1, 2, 3]
                     Statistics: Num rows: 1 Data size: 172 Basic stats: COMPLETE Column stats: COMPLETE
                     value expressions: _col0 (type: bigint), _col1 (type: bigint), _col2 (type: struct<count:bigint,sum:double,input:int>), _col3 (type: struct<count:bigint,sum:double,variance:double>)
         Reducer 3 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:int> of Column[VALUE._col2] not supported
-                vectorized: false
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:bigint, VALUE._col1:bigint, VALUE._col2:struct<count:bigint,sum:double,input:int>, VALUE._col3:struct<count:bigint,sum:double,variance:double>
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: sum(VALUE._col0), count(VALUE._col1), avg(VALUE._col2), std(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFCountMerge(col 1) -> bigint, VectorUDAFAvgFinal(col 2) -> double, VectorUDAFStdPopFinal(col 3) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 1 Data size: 32 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 1 Data size: 32 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_dynamic_semijoin_reduction.q.out b/ql/src/test/results/clientpositive/llap/vectorized_dynamic_semijoin_reduction.q.out
index c9e3e60460..9a1c44c3e6 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_dynamic_semijoin_reduction.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_dynamic_semijoin_reduction.q.out
@@ -136,8 +136,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFMinLong(col 1) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilter(col 1) -> binary
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1, 2]
                           mode: hash
                           outputColumnNames: _col0, _col1, _col2
@@ -173,8 +175,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: count()
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -198,8 +202,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -229,8 +235,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilterMerge(col 2) -> binary
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: [0, 1, 2]
                 mode: final
                 outputColumnNames: _col0, _col1, _col2
@@ -373,8 +381,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFMinString(col 0) -> string, VectorUDAFMaxString(col 0) -> string, VectorUDAFBloomFilter(col 0) -> binary
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1, 2]
                           mode: hash
                           outputColumnNames: _col0, _col1, _col2
@@ -410,8 +420,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: count()
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -435,8 +447,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -466,8 +480,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinString(col 0) -> string, VectorUDAFMaxString(col 1) -> string, VectorUDAFBloomFilterMerge(col 2) -> binary
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: [0, 1, 2]
                 mode: final
                 outputColumnNames: _col0, _col1, _col2
@@ -610,8 +626,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFMinString(col 0) -> string, VectorUDAFMaxString(col 0) -> string, VectorUDAFBloomFilter(col 0) -> binary
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1, 2]
                           mode: hash
                           outputColumnNames: _col0, _col1, _col2
@@ -647,8 +665,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: count()
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -672,8 +692,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -703,8 +725,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinString(col 0) -> string, VectorUDAFMaxString(col 1) -> string, VectorUDAFBloomFilterMerge(col 2) -> binary
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: [0, 1, 2]
                 mode: final
                 outputColumnNames: _col0, _col1, _col2
@@ -848,8 +872,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFMinLong(col 1) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilter(col 1) -> binary
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1, 2]
                           mode: hash
                           outputColumnNames: _col0, _col1, _col2
@@ -918,8 +944,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFMinLong(col 1) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilter(col 1) -> binary
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1, 2]
                           mode: hash
                           outputColumnNames: _col0, _col1, _col2
@@ -957,8 +985,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: count()
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -982,8 +1012,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1013,8 +1045,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilterMerge(col 2) -> binary
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: [0, 1, 2]
                 mode: final
                 outputColumnNames: _col0, _col1, _col2
@@ -1042,8 +1076,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilterMerge(col 2) -> binary
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: [0, 1, 2]
                 mode: final
                 outputColumnNames: _col0, _col1, _col2
@@ -1187,8 +1223,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFMinString(col 0) -> string, VectorUDAFMaxString(col 0) -> string, VectorUDAFBloomFilter(col 0) -> binary
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1, 2]
                           mode: hash
                           outputColumnNames: _col0, _col1, _col2
@@ -1214,8 +1252,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFMinLong(col 1) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilter(col 1) -> binary
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1, 2]
                           mode: hash
                           outputColumnNames: _col0, _col1, _col2
@@ -1251,8 +1291,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: count()
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -1276,8 +1318,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1307,8 +1351,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinString(col 0) -> string, VectorUDAFMaxString(col 1) -> string, VectorUDAFBloomFilterMerge(col 2) -> binary
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: [0, 1, 2]
                 mode: final
                 outputColumnNames: _col0, _col1, _col2
@@ -1336,8 +1382,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilterMerge(col 2) -> binary
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: [0, 1, 2]
                 mode: final
                 outputColumnNames: _col0, _col1, _col2
@@ -1480,8 +1528,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFMinLong(col 1) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilter(col 1) -> binary
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1, 2]
                           mode: hash
                           outputColumnNames: _col0, _col1, _col2
@@ -1517,8 +1567,10 @@ STAGE PLANS:
                 Group By Operator
                   aggregations: count()
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   mode: hash
                   outputColumnNames: _col0
@@ -1542,8 +1594,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1573,8 +1627,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> int, VectorUDAFMaxLong(col 1) -> int, VectorUDAFBloomFilterMerge(col 2) -> binary
                     className: VectorGroupByOperator
+                    groupByMode: FINAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: STREAMING
                     projectedOutputColumns: [0, 1, 2]
                 mode: final
                 outputColumnNames: _col0, _col1, _col2
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_mapjoin.q.out b/ql/src/test/results/clientpositive/llap/vectorized_mapjoin.q.out
index 0f02856691..e56800ad40 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_mapjoin.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_mapjoin.q.out
@@ -72,17 +72,22 @@ STAGE PLANS:
                           Group By Operator
                             aggregations: count(_col0), max(_col1), min(_col0), avg(_col2)
                             Group By Vectorization:
-                                aggregators: VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxLong(col 2) -> int, VectorUDAFMinLong(col 2) -> int, VectorUDAFAvgLong(col 12) -> struct<count:bigint,sum:double>
+                                aggregators: VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxLong(col 2) -> int, VectorUDAFMinLong(col 2) -> int, VectorUDAFAvgLong(col 12) -> struct<count:bigint,sum:double,input:bigint>
                                 className: VectorGroupByOperator
-                                vectorOutput: false
+                                groupByMode: HASH
+                                vectorOutput: true
                                 native: false
+                                vectorProcessingMode: HASH
                                 projectedOutputColumns: [0, 1, 2, 3]
-                                vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(col 12) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                             mode: hash
                             outputColumnNames: _col0, _col1, _col2, _col3
                             Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: COMPLETE
                             Reduce Output Operator
                               sort order: 
+                              Reduce Sink Vectorization:
+                                  className: VectorReduceSinkEmptyKeyOperator
+                                  native: true
+                                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                               Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: COMPLETE
                               value expressions: _col0 (type: bigint), _col1 (type: int), _col2 (type: int), _col3 (type: struct<count:bigint,sum:double,input:int>)
             Execution mode: vectorized, llap
@@ -90,7 +95,7 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
@@ -138,20 +143,33 @@ STAGE PLANS:
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:int> of Column[VALUE._col3] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), avg(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 0) -> bigint, VectorUDAFMaxLong(col 1) -> int, VectorUDAFMinLong(col 2) -> int, VectorUDAFAvgFinal(col 3) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_mapjoin2.q.out b/ql/src/test/results/clientpositive/llap/vectorized_mapjoin2.q.out
index 2769e66fec..26c377f205 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_mapjoin2.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_mapjoin2.q.out
@@ -92,8 +92,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 1:long) -> bigint
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0]
                           mode: hash
                           outputColumnNames: _col0
@@ -173,8 +175,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_parquet.q.out b/ql/src/test/results/clientpositive/llap/vectorized_parquet.q.out
index 13eae75119..e904286cb4 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_parquet.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_parquet.q.out
@@ -163,18 +163,20 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:float> of Column[VALUE._col3] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), min(VALUE._col1), count(VALUE._col2), avg(VALUE._col3), stddev_pop(VALUE._col4)
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_parquet_types.q.out b/ql/src/test/results/clientpositive/llap/vectorized_parquet_types.q.out
index 6cd31dbce3..cdf6b3d225 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_parquet_types.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_parquet_types.q.out
@@ -289,13 +289,14 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: max(cint), min(csmallint), count(cstring1), avg(cfloat), stddev_pop(cdouble), max(cdecimal)
                       Group By Vectorization:
-                          aggregators: VectorUDAFMaxLong(col 0) -> int, VectorUDAFMinLong(col 2) -> smallint, VectorUDAFCount(col 5) -> bigint, VectorUDAFAvgDouble(col 3) -> struct<count:bigint,sum:double>, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDecimal(col 10) -> decimal(4,2)
+                          aggregators: VectorUDAFMaxLong(col 0) -> int, VectorUDAFMinLong(col 2) -> smallint, VectorUDAFCount(col 5) -> bigint, VectorUDAFAvgDouble(col 3) -> struct<count:bigint,sum:double,input:double>, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDecimal(col 10) -> decimal(4,2)
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           keyExpressions: col 1
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDouble(col 3) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                       keys: ctinyint (type: tinyint)
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
@@ -304,6 +305,10 @@ STAGE PLANS:
                         key expressions: _col0 (type: tinyint)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: tinyint)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkLongOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col1 (type: int), _col2 (type: smallint), _col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,input:float>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: decimal(4,2))
             Execution mode: vectorized, llap
@@ -311,21 +316,32 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:float> of Column[VALUE._col3] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), min(VALUE._col1), count(VALUE._col2), avg(VALUE._col3), stddev_pop(VALUE._col4), max(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxLong(col 1) -> int, VectorUDAFMinLong(col 2) -> smallint, VectorUDAFCountMerge(col 3) -> bigint, VectorUDAFAvgFinal(col 4) -> double, VectorUDAFStdPopFinal(col 5) -> double, VectorUDAFMaxDecimal(col 6) -> decimal(4,2)
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 keys: KEY._col0 (type: tinyint)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
@@ -333,6 +349,10 @@ STAGE PLANS:
                 Reduce Output Operator
                   key expressions: _col0 (type: tinyint)
                   sort order: +
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkObjectHashOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                   Statistics: Num rows: 11 Data size: 121 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col1 (type: int), _col2 (type: smallint), _col3 (type: bigint), _col4 (type: double), _col5 (type: double), _col6 (type: decimal(4,2))
         Reducer 3 
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_ptf.q.out b/ql/src/test/results/clientpositive/llap/vectorized_ptf.q.out
index 5c849f67cb..0a6d87a484 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_ptf.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_ptf.q.out
@@ -3365,9 +3365,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1, col 2
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: int)
                 mode: mergepartial
@@ -3504,9 +3506,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumDouble(col 7) -> double
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 2, col 3
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: p_mfgr (type: string), p_brand (type: string)
                       mode: hash
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_shufflejoin.q.out b/ql/src/test/results/clientpositive/llap/vectorized_shufflejoin.q.out
index 3c972cc7d5..a750d9fd01 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_shufflejoin.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_shufflejoin.q.out
@@ -126,8 +126,10 @@ STAGE PLANS:
                   Group By Operator
                     aggregations: count(_col0), max(_col1), min(_col0), avg(_col2)
                     Group By Vectorization:
+                        groupByMode: HASH
                         vectorOutput: false
                         native: false
+                        vectorProcessingMode: NONE
                         projectedOutputColumns: null
                     mode: hash
                     outputColumnNames: _col0, _col1, _col2, _col3
@@ -137,21 +139,35 @@ STAGE PLANS:
                       Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: COMPLETE
                       value expressions: _col0 (type: bigint), _col1 (type: int), _col2 (type: int), _col3 (type: struct<count:bigint,sum:double,input:int>)
         Reducer 3 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:int> of Column[VALUE._col3] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), avg(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 0) -> bigint, VectorUDAFMaxLong(col 1) -> int, VectorUDAFMinLong(col 2) -> int, VectorUDAFAvgFinal(col 3) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                 Reduce Output Operator
                   key expressions: _col0 (type: bigint)
                   sort order: +
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkObjectHashOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                   Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
                   value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: double)
         Reducer 4 
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_timestamp.q.out b/ql/src/test/results/clientpositive/llap/vectorized_timestamp.q.out
index 82d551848c..24f8d36912 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_timestamp.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_timestamp.q.out
@@ -17,24 +17,49 @@ POSTHOOK: query: INSERT INTO TABLE test VALUES ('0001-01-01 00:00:00.000000000')
 POSTHOOK: type: QUERY
 POSTHOOK: Output: default@test
 POSTHOOK: Lineage: test.ts EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test
 POSTHOOK: type: QUERY
-Plan optimized by CBO.
-
-Stage-0
-  Fetch Operator
-    limit:-1
-    Stage-1
-      Map 1 llap
-      File Output Operator [FS_2]
-        Select Operator [SEL_1] (rows=2 width=40)
-          Output:["_col0"]
-          TableScan [TS_0] (rows=2 width=40)
-            default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
+PLAN VECTORIZATION:
+  enabled: false
+  enabledConditionsNotMet: [hive.vectorized.execution.enabled IS false]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: test
+                  Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: ts (type: timestamp)
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                      table:
+                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: llap
+            LLAP IO: all inputs
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
 
 PREHOOK: query: SELECT ts FROM test
 PREHOOK: type: QUERY
@@ -46,36 +71,6 @@ POSTHOOK: Input: default@test
 #### A masked pattern was here ####
 0001-01-01 00:00:00
 9999-12-31 23:59:59.999999999
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
-PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
-POSTHOOK: type: QUERY
-Plan optimized by CBO.
-
-Vertex dependency in root stage
-Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
-
-Stage-0
-  Fetch Operator
-    limit:-1
-    Stage-1
-      Reducer 2 llap
-      File Output Operator [FS_6]
-        Select Operator [SEL_5] (rows=1 width=80)
-          Output:["_col0","_col1","_col2"]
-          Group By Operator [GBY_4] (rows=1 width=80)
-            Output:["_col0","_col1"],aggregations:["min(VALUE._col0)","max(VALUE._col1)"]
-          <-Map 1 [CUSTOM_SIMPLE_EDGE] llap
-            PARTITION_ONLY_SHUFFLE [RS_3]
-              Group By Operator [GBY_2] (rows=1 width=80)
-                Output:["_col0","_col1"],aggregations:["min(ts)","max(ts)"]
-                Select Operator [SEL_1] (rows=2 width=40)
-                  Output:["ts"]
-                  TableScan [TS_0] (rows=2 width=40)
-                    default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
-
 PREHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test
@@ -85,27 +80,6 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test
 #### A masked pattern was here ####
 0001-01-01 00:00:00	9999-12-31 23:59:59.999999999	3652060 23:59:59.999999999
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
-PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
-POSTHOOK: type: QUERY
-Plan optimized by CBO.
-
-Stage-0
-  Fetch Operator
-    limit:-1
-    Stage-1
-      Map 1 llap
-      File Output Operator [FS_3]
-        Select Operator [SEL_2] (rows=1 width=40)
-          Output:["_col0"]
-          Filter Operator [FIL_4] (rows=1 width=40)
-            predicate:(ts) IN (0001-01-01 00:00:00.0, 0002-02-02 00:00:00.0)
-            TableScan [TS_0] (rows=2 width=40)
-              default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
-
 PREHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test
@@ -115,25 +89,6 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test
 #### A masked pattern was here ####
 0001-01-01 00:00:00
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test
-PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test
-POSTHOOK: type: QUERY
-Plan optimized by CBO.
-
-Stage-0
-  Fetch Operator
-    limit:-1
-    Stage-1
-      Map 1 vectorized, llap
-      File Output Operator [FS_4]
-        Select Operator [SEL_3] (rows=2 width=40)
-          Output:["_col0"]
-          TableScan [TS_0] (rows=2 width=40)
-            default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
-
 PREHOOK: query: SELECT ts FROM test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test
@@ -144,35 +99,136 @@ POSTHOOK: Input: default@test
 #### A masked pattern was here ####
 0001-01-01 00:00:00
 9999-12-31 23:59:59.999999999
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
 POSTHOOK: type: QUERY
-Plan optimized by CBO.
-
-Vertex dependency in root stage
-Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
-
-Stage-0
-  Fetch Operator
-    limit:-1
-    Stage-1
-      Reducer 2 vectorized, llap
-      File Output Operator [FS_12]
-        Select Operator [SEL_11] (rows=1 width=80)
-          Output:["_col0","_col1","_col2"]
-          Group By Operator [GBY_10] (rows=1 width=80)
-            Output:["_col0","_col1"],aggregations:["min(VALUE._col0)","max(VALUE._col1)"]
-          <-Map 1 [CUSTOM_SIMPLE_EDGE] vectorized, llap
-            PARTITION_ONLY_SHUFFLE [RS_9]
-              Group By Operator [GBY_8] (rows=1 width=80)
-                Output:["_col0","_col1"],aggregations:["min(ts)","max(ts)"]
-                Select Operator [SEL_7] (rows=2 width=40)
-                  Output:["ts"]
-                  TableScan [TS_0] (rows=2 width=40)
-                    default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: test
+                  Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0]
+                  Select Operator
+                    expressions: ts (type: timestamp)
+                    outputColumnNames: ts
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: min(ts), max(ts)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 0) -> timestamp
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0, 1]
+                      mode: hash
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1]
+                        Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: timestamp), _col1 (type: timestamp)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    includeColumns: [0]
+                    dataColumns: ts:timestamp
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    dataColumns: VALUE._col0:timestamp, VALUE._col1:timestamp
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 1) -> timestamp
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: timestamp), _col1 (type: timestamp), (_col1 - _col0) (type: interval_day_time)
+                  outputColumnNames: _col0, _col1, _col2
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 2]
+                      selectExpressions: TimestampColSubtractTimestampColumn(col 1, col 0) -> 2:interval_day_time
+                  Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
 
 PREHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
 PREHOOK: type: QUERY
@@ -183,26 +239,79 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test
 #### A masked pattern was here ####
 0001-01-01 00:00:00	9999-12-31 23:59:59.999999999	3652060 23:59:59.999999999
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
 POSTHOOK: type: QUERY
-Plan optimized by CBO.
-
-Stage-0
-  Fetch Operator
-    limit:-1
-    Stage-1
-      Map 1 vectorized, llap
-      File Output Operator [FS_7]
-        Select Operator [SEL_6] (rows=1 width=40)
-          Output:["_col0"]
-          Filter Operator [FIL_5] (rows=1 width=40)
-            predicate:(ts) IN (0001-01-01 00:00:00.0, 0002-02-02 00:00:00.0)
-            TableScan [TS_0] (rows=2 width=40)
-              default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: test
+                  Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterTimestampColumnInList(col 0, values [0001-01-01 00:00:00.0, 0002-02-02 00:00:00.0]) -> boolean
+                    predicate: (ts) IN (0001-01-01 00:00:00.0, 0002-02-02 00:00:00.0) (type: boolean)
+                    Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ts (type: timestamp)
+                      outputColumnNames: _col0
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0]
+                      Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
+                        Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    includeColumns: [0]
+                    dataColumns: ts:timestamp
+                    partitionColumnCount: 0
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
 
 PREHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
 PREHOOK: type: QUERY
@@ -213,3 +322,274 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test
 #### A masked pattern was here ####
 0001-01-01 00:00:00
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: test
+                  Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0]
+                  Select Operator
+                    expressions: ts (type: timestamp)
+                    outputColumnNames: ts
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: avg(ts)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double,input:timestamp>
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0]
+                      mode: hash
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0]
+                        Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: struct<count:bigint,sum:double,input:timestamp>)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    includeColumns: [0]
+                    dataColumns: ts:timestamp
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,input:timestamp>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: avg(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: double), CAST( _col0 AS TIMESTAMP) (type: timestamp)
+                  outputColumnNames: _col0, _col1
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1]
+                      selectExpressions: CastDoubleToTimestamp(col 0) -> 1:timestamp
+                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+9.56332944E10	5000-07-01 13:00:00
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: test
+                  Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0]
+                  Select Operator
+                    expressions: ts (type: timestamp)
+                    outputColumnNames: ts
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
+                    Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6]
+                      mode: hash
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                      Statistics: Num rows: 1 Data size: 560 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3, 4, 5, 6]
+                        Statistics: Num rows: 1 Data size: 560 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: struct<count:bigint,sum:double,variance:double>)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    includeColumns: [0]
+                    dataColumns: ts:timestamp
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 7
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:struct<count:bigint,sum:double,variance:double>, VALUE._col4:struct<count:bigint,sum:double,variance:double>, VALUE._col5:struct<count:bigint,sum:double,variance:double>, VALUE._col6:struct<count:bigint,sum:double,variance:double>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: variance(VALUE._col0), var_pop(VALUE._col1), var_samp(VALUE._col2), std(VALUE._col3), stddev(VALUE._col4), stddev_pop(VALUE._col5), stddev_samp(VALUE._col6)
+                Group By Vectorization:
+                    aggregators: VectorUDAFVarPopFinal(col 0) -> double, VectorUDAFVarPopFinal(col 1) -> double, VectorUDAFVarSampFinal(col 2) -> double, VectorUDAFStdPopFinal(col 3) -> double, VectorUDAFStdPopFinal(col 4) -> double, VectorUDAFStdPopFinal(col 5) -> double, VectorUDAFStdSampFinal(col 6) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                Statistics: Num rows: 1 Data size: 560 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 560 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+2.489106846793884E22	2.489106846793884E22	4.978213693587768E22	1.577690352E11	1.577690352E11	1.577690352E11	2.2311910930235822E11
diff --git a/ql/src/test/results/clientpositive/llap/vectorized_timestamp_funcs.q.out b/ql/src/test/results/clientpositive/llap/vectorized_timestamp_funcs.q.out
index e326f5f79e..f6dcb7cc54 100644
--- a/ql/src/test/results/clientpositive/llap/vectorized_timestamp_funcs.q.out
+++ b/ql/src/test/results/clientpositive/llap/vectorized_timestamp_funcs.q.out
@@ -809,8 +809,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 0) -> timestamp, VectorUDAFCount(col 0) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -848,8 +850,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 1) -> timestamp, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -919,27 +923,48 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc_string
                   Statistics: Num rows: 40 Data size: 84 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
                   Select Operator
                     expressions: ctimestamp1 (type: timestamp)
                     outputColumnNames: ctimestamp1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
                     Statistics: Num rows: 40 Data size: 84 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
                       aggregations: sum(ctimestamp1)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFSumTimestamp(col 0) -> double
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
                       Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: double)
-            Execution mode: llap
+            Execution mode: vectorized, llap
             LLAP IO: all inputs
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Aggregation Function expression for GROUPBY operator: Vectorization of aggreation should have succeeded org.apache.hadoop.hive.ql.metadata.HiveException: Vector aggregate not implemented: "sum" for type: "TIMESTAMP (UDAF evaluator mode = PARTIAL1)
-                vectorized: false
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
         Reducer 2 
             Execution mode: vectorized, llap
             Reduce Vectorization:
@@ -955,8 +980,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumDouble(col 0) -> double
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1057,17 +1084,22 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: avg(ctimestamp1), variance(ctimestamp1), var_pop(ctimestamp1), var_samp(ctimestamp1), std(ctimestamp1), stddev(ctimestamp1), stddev_pop(ctimestamp1), stddev_samp(ctimestamp1)
                       Group By Vectorization:
-                          aggregators: VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>
+                          aggregators: VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double,input:timestamp>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                       Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: struct<count:bigint,sum:double,input:timestamp>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: struct<count:bigint,sum:double,variance:double>)
             Execution mode: vectorized, llap
@@ -1075,30 +1107,48 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:timestamp> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), variance(VALUE._col1), var_pop(VALUE._col2), var_samp(VALUE._col3), std(VALUE._col4), stddev(VALUE._col5), stddev_pop(VALUE._col6), stddev_samp(VALUE._col7)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFVarPopFinal(col 1) -> double, VectorUDAFVarPopFinal(col 2) -> double, VectorUDAFVarSampFinal(col 3) -> double, VectorUDAFStdPopFinal(col 4) -> double, VectorUDAFStdPopFinal(col 5) -> double, VectorUDAFStdPopFinal(col 6) -> double, VectorUDAFStdSampFinal(col 7) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                 Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
                   expressions: round(_col0, 0) (type: double), _col1 BETWEEN 8.97077295279421E19 AND 8.97077295279422E19 (type: boolean), _col2 BETWEEN 8.97077295279421E19 AND 8.97077295279422E19 (type: boolean), _col3 BETWEEN 9.20684592523616E19 AND 9.20684592523617E19 (type: boolean), round(_col4, 3) (type: double), round(_col5, 3) (type: double), round(_col6, 3) (type: double), round(_col7, 3) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [8, 9, 10, 11, 12, 13, 14, 15]
+                      selectExpressions: RoundWithNumDigitsDoubleToDouble(col 0, decimalPlaces 0) -> 8:double, VectorUDFAdaptor(_col1 BETWEEN 8.97077295279421E19 AND 8.97077295279422E19) -> 9:boolean, VectorUDFAdaptor(_col2 BETWEEN 8.97077295279421E19 AND 8.97077295279422E19) -> 10:boolean, VectorUDFAdaptor(_col3 BETWEEN 9.20684592523616E19 AND 9.20684592523617E19) -> 11:boolean, RoundWithNumDigitsDoubleToDouble(col 4, decimalPlaces 3) -> 12:double, RoundWithNumDigitsDoubleToDouble(col 5, decimalPlaces 3) -> 13:double, RoundWithNumDigitsDoubleToDouble(col 6, decimalPlaces 3) -> 14:double, RoundWithNumDigitsDoubleToDouble(col 7, decimalPlaces 3) -> 15:double
                   Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/vector_between_in.q.out b/ql/src/test/results/clientpositive/spark/vector_between_in.q.out
index 9329ba7bd2..2f878414ac 100644
--- a/ql/src/test/results/clientpositive/spark/vector_between_in.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_between_in.q.out
@@ -151,8 +151,10 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFCountStar(*) -> bigint
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0]
                         mode: hash
                         outputColumnNames: _col0
@@ -189,8 +191,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -351,8 +355,10 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFCountStar(*) -> bigint
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0]
                         mode: hash
                         outputColumnNames: _col0
@@ -389,8 +395,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -739,8 +747,10 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFCountStar(*) -> bigint
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0]
                         mode: hash
                         outputColumnNames: _col0
@@ -777,8 +787,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1087,9 +1099,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 5:long) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 4
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: boolean)
                       mode: hash
@@ -1129,9 +1143,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
@@ -1223,9 +1239,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 5:long) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 4
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: boolean)
                       mode: hash
@@ -1265,9 +1283,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
@@ -1359,9 +1379,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 5:long) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 4
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: boolean)
                       mode: hash
@@ -1401,9 +1423,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
@@ -1495,9 +1519,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 5:long) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 4
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: _col0 (type: boolean)
                       mode: hash
@@ -1537,9 +1563,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.out b/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.out
index 0aa347bcda..c69bc810d9 100644
--- a/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.out
@@ -144,13 +144,14 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: avg(50), avg(50.0), avg(50)
                       Group By Vectorization:
-                          aggregators: VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 11:long) -> struct<count:bigint,sum:double>, VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 12:double) -> struct<count:bigint,sum:double>, VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 13:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0)>
+                          aggregators: VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 11:long) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 12:double) -> struct<count:bigint,sum:double,input:double>, VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 13:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0),input:decimal(20,0)>
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           keyExpressions: col 2
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 11:long) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 12:double) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 13:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0)> output type STRUCT requires PRIMITIVE IS false
                       keys: _col0 (type: int)
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -159,6 +160,10 @@ STAGE PLANS:
                         key expressions: _col0 (type: int)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: int)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                         TopN Hash Memory Usage: 0.1
                         value expressions: _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: struct<count:bigint,sum:double,input:double>), _col3 (type: struct<count:bigint,sum:decimal(12,0),input:decimal(10,0)>)
@@ -166,20 +171,32 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:int> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), avg(VALUE._col1), avg(VALUE._col2)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 1) -> double, VectorUDAFAvgFinal(col 2) -> double, VectorUDAFAvgDecimalFinal(col 3) -> decimal(16,4)
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2]
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -187,6 +204,10 @@ STAGE PLANS:
                 Reduce Output Operator
                   key expressions: _col0 (type: int)
                   sort order: +
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkObjectHashOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                   Statistics: Num rows: 524 Data size: 155436 Basic stats: COMPLETE Column stats: NONE
                   TopN Hash Memory Usage: 0.1
                   value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: decimal(14,4))
diff --git a/ql/src/test/results/clientpositive/spark/vector_count_distinct.q.out b/ql/src/test/results/clientpositive/spark/vector_count_distinct.q.out
index b66383191e..9af0786fb7 100644
--- a/ql/src/test/results/clientpositive/spark/vector_count_distinct.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_count_distinct.q.out
@@ -1266,9 +1266,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 16
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: ws_order_number (type: int)
                       mode: hash
@@ -1305,9 +1307,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -1318,8 +1322,10 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFCount(col 0) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: [0]
                   mode: hash
                   outputColumnNames: _col0
@@ -1347,8 +1353,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/spark/vector_decimal_aggregate.q.out b/ql/src/test/results/clientpositive/spark/vector_decimal_aggregate.q.out
index edda91909d..9994f2be90 100644
--- a/ql/src/test/results/clientpositive/spark/vector_decimal_aggregate.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_decimal_aggregate.q.out
@@ -70,9 +70,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFMaxDecimal(col 1) -> decimal(20,10), VectorUDAFMinDecimal(col 1) -> decimal(20,10), VectorUDAFSumDecimal(col 1) -> decimal(38,18), VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(23,14), VectorUDAFMinDecimal(col 2) -> decimal(23,14), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 3
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
                       keys: cint (type: int)
                       mode: hash
@@ -112,9 +114,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 1) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(20,10), VectorUDAFMinDecimal(col 3) -> decimal(20,10), VectorUDAFSumDecimal(col 4) -> decimal(38,18), VectorUDAFCountMerge(col 5) -> bigint, VectorUDAFMaxDecimal(col 6) -> decimal(23,14), VectorUDAFMinDecimal(col 7) -> decimal(23,14), VectorUDAFSumDecimal(col 8) -> decimal(38,18), VectorUDAFCountMerge(col 9) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -226,13 +230,14 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: count(cdecimal1), max(cdecimal1), min(cdecimal1), sum(cdecimal1), avg(cdecimal1), stddev_pop(cdecimal1), stddev_samp(cdecimal1), count(cdecimal2), max(cdecimal2), min(cdecimal2), sum(cdecimal2), avg(cdecimal2), stddev_pop(cdecimal2), stddev_samp(cdecimal2), count()
                       Group By Vectorization:
-                          aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFMaxDecimal(col 1) -> decimal(20,10), VectorUDAFMinDecimal(col 1) -> decimal(20,10), VectorUDAFSumDecimal(col 1) -> decimal(38,18), VectorUDAFAvgDecimal(col 1) -> struct<count:bigint,sum:decimal(30,10)>, VectorUDAFStdPopDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(23,14), VectorUDAFMinDecimal(col 2) -> decimal(23,14), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFAvgDecimal(col 2) -> struct<count:bigint,sum:decimal(33,14)>, VectorUDAFStdPopDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint
+                          aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFMaxDecimal(col 1) -> decimal(20,10), VectorUDAFMinDecimal(col 1) -> decimal(20,10), VectorUDAFSumDecimal(col 1) -> decimal(38,18), VectorUDAFAvgDecimal(col 1) -> struct<count:bigint,sum:decimal(30,10),input:decimal(30,10)>, VectorUDAFStdPopDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(23,14), VectorUDAFMinDecimal(col 2) -> decimal(23,14), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFAvgDecimal(col 2) -> struct<count:bigint,sum:decimal(33,14),input:decimal(33,14)>, VectorUDAFStdPopDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           keyExpressions: col 3
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDecimal(col 1) -> struct<count:bigint,sum:decimal(30,10)> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDecimal(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampDecimal(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDecimal(col 2) -> struct<count:bigint,sum:decimal(33,14)> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDecimal(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampDecimal(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                       keys: cint (type: int)
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
@@ -241,39 +246,66 @@ STAGE PLANS:
                         key expressions: _col0 (type: int)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: int)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 12288 Data size: 2165060 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col1 (type: bigint), _col2 (type: decimal(20,10)), _col3 (type: decimal(20,10)), _col4 (type: decimal(30,10)), _col5 (type: struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)>), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: bigint), _col9 (type: decimal(23,14)), _col10 (type: decimal(23,14)), _col11 (type: decimal(33,14)), _col12 (type: struct<count:bigint,sum:decimal(33,14),input:decimal(23,14)>), _col13 (type: struct<count:bigint,sum:double,variance:double>), _col14 (type: struct<count:bigint,sum:double,variance:double>), _col15 (type: bigint)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)> of Column[VALUE._col4] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), sum(VALUE._col3), avg(VALUE._col4), stddev_pop(VALUE._col5), stddev_samp(VALUE._col6), count(VALUE._col7), max(VALUE._col8), min(VALUE._col9), sum(VALUE._col10), avg(VALUE._col11), stddev_pop(VALUE._col12), stddev_samp(VALUE._col13), count(VALUE._col14)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 1) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(20,10), VectorUDAFMinDecimal(col 3) -> decimal(20,10), VectorUDAFSumDecimal(col 4) -> decimal(38,18), VectorUDAFAvgDecimalFinal(col 5) -> decimal(34,14), VectorUDAFStdPopFinal(col 6) -> double, VectorUDAFStdSampFinal(col 7) -> double, VectorUDAFCountMerge(col 8) -> bigint, VectorUDAFMaxDecimal(col 9) -> decimal(23,14), VectorUDAFMinDecimal(col 10) -> decimal(23,14), VectorUDAFSumDecimal(col 11) -> decimal(38,18), VectorUDAFAvgDecimalFinal(col 12) -> decimal(37,18), VectorUDAFStdPopFinal(col 13) -> double, VectorUDAFStdSampFinal(col 14) -> double, VectorUDAFCountMerge(col 15) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
                 Statistics: Num rows: 6144 Data size: 1082530 Basic stats: COMPLETE Column stats: NONE
                 Filter Operator
+                  Filter Vectorization:
+                      className: VectorFilterOperator
+                      native: true
+                      predicateExpression: FilterLongColGreaterLongScalar(col 15, val 1) -> boolean
                   predicate: (_col15 > 1) (type: boolean)
                   Statistics: Num rows: 2048 Data size: 360843 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
                     expressions: _col0 (type: int), _col1 (type: bigint), _col2 (type: decimal(20,10)), _col3 (type: decimal(20,10)), _col4 (type: decimal(30,10)), _col5 (type: decimal(24,14)), _col6 (type: double), _col7 (type: double), _col8 (type: bigint), _col9 (type: decimal(23,14)), _col10 (type: decimal(23,14)), _col11 (type: decimal(33,14)), _col12 (type: decimal(27,18)), _col13 (type: double), _col14 (type: double)
                     outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
                     Statistics: Num rows: 2048 Data size: 360843 Basic stats: COMPLETE Column stats: NONE
                     File Output Operator
                       compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
                       Statistics: Num rows: 2048 Data size: 360843 Basic stats: COMPLETE Column stats: NONE
                       table:
                           input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/vector_distinct_2.q.out b/ql/src/test/results/clientpositive/spark/vector_distinct_2.q.out
index 59dcf7c1f9..aff53a60c6 100644
--- a/ql/src/test/results/clientpositive/spark/vector_distinct_2.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_distinct_2.q.out
@@ -141,9 +141,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 8
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: t (type: tinyint), s (type: string)
                       mode: hash
@@ -180,9 +182,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: tinyint), KEY._col1 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/spark/vector_groupby_3.q.out b/ql/src/test/results/clientpositive/spark/vector_groupby_3.q.out
index 94b3ef6ffb..83f860486b 100644
--- a/ql/src/test/results/clientpositive/spark/vector_groupby_3.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_groupby_3.q.out
@@ -143,9 +143,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMaxLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 8
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: t (type: tinyint), s (type: string)
                       mode: hash
@@ -185,9 +187,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMaxLong(col 2) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: tinyint), KEY._col1 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/spark/vector_inner_join.q.out b/ql/src/test/results/clientpositive/spark/vector_inner_join.q.out
index 3a9f97b4b7..62383c459d 100644
--- a/ql/src/test/results/clientpositive/spark/vector_inner_join.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_inner_join.q.out
@@ -238,9 +238,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
diff --git a/ql/src/test/results/clientpositive/spark/vector_mapjoin_reduce.q.out b/ql/src/test/results/clientpositive/spark/vector_mapjoin_reduce.q.out
index 2f2609f03e..433b9a2880 100644
--- a/ql/src/test/results/clientpositive/spark/vector_mapjoin_reduce.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_mapjoin_reduce.q.out
@@ -91,9 +91,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int)
                         mode: hash
@@ -142,9 +144,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 1
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: l_partkey (type: int)
                       mode: hash
@@ -183,9 +187,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
@@ -362,9 +368,11 @@ STAGE PLANS:
                       Group By Operator
                         Group By Vectorization:
                             className: VectorGroupByOperator
+                            groupByMode: HASH
                             vectorOutput: true
                             keyExpressions: col 0, col 3
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: []
                         keys: _col0 (type: int), _col1 (type: int)
                         mode: hash
@@ -413,9 +421,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 1
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: l_partkey (type: int)
                       mode: hash
@@ -454,9 +464,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: int)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/spark/vector_orderby_5.q.out b/ql/src/test/results/clientpositive/spark/vector_orderby_5.q.out
index fd3469c1c6..dc394c8f32 100644
--- a/ql/src/test/results/clientpositive/spark/vector_orderby_5.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_orderby_5.q.out
@@ -144,9 +144,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMaxLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 7
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: bo (type: boolean)
                       mode: hash
@@ -186,9 +188,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMaxLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/spark/vector_outer_join1.q.out b/ql/src/test/results/clientpositive/spark/vector_outer_join1.q.out
index 03e3a476f1..55547884e4 100644
--- a/ql/src/test/results/clientpositive/spark/vector_outer_join1.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_outer_join1.q.out
@@ -817,8 +817,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFCountStar(*) -> bigint, VectorUDAFSumLong(col 0) -> bigint
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1]
                           mode: hash
                           outputColumnNames: _col0, _col1
@@ -870,8 +872,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint, VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/spark/vector_outer_join2.q.out b/ql/src/test/results/clientpositive/spark/vector_outer_join2.q.out
index c91f175be0..8ca54f9533 100644
--- a/ql/src/test/results/clientpositive/spark/vector_outer_join2.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_outer_join2.q.out
@@ -398,8 +398,10 @@ STAGE PLANS:
                           Group By Vectorization:
                               aggregators: VectorUDAFCountStar(*) -> bigint, VectorUDAFSumLong(col 3) -> bigint
                               className: VectorGroupByOperator
+                              groupByMode: HASH
                               vectorOutput: true
                               native: false
+                              vectorProcessingMode: HASH
                               projectedOutputColumns: [0, 1]
                           mode: hash
                           outputColumnNames: _col0, _col1
@@ -451,8 +453,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint, VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/spark/vector_string_concat.q.out b/ql/src/test/results/clientpositive/spark/vector_string_concat.q.out
index b361ec0cee..17c79a5db2 100644
--- a/ql/src/test/results/clientpositive/spark/vector_string_concat.q.out
+++ b/ql/src/test/results/clientpositive/spark/vector_string_concat.q.out
@@ -354,9 +354,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 19
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: _col0 (type: string)
                       mode: hash
@@ -394,9 +396,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/spark/vector_tablesample_rows.q.out b/ql/src/test/results/clientpositive/spark/vector_tablesample_rows.q.out
new file mode 100644
index 0000000000..734cf6312b
--- /dev/null
+++ b/ql/src/test/results/clientpositive/spark/vector_tablesample_rows.q.out
@@ -0,0 +1,424 @@
+PREHOOK: query: explain vectorization detail
+select 'key1', 'value1' from alltypesorc tablesample (1 rows)
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization detail
+select 'key1', 'value1' from alltypesorc tablesample (1 rows)
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Row Limit Per Split: 1
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Select Operator
+                    expressions: 'key1' (type: string), 'value1' (type: string)
+                    outputColumnNames: _col0, _col1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [12, 13]
+                        selectExpressions: ConstantVectorExpression(val key1) -> 12:string, ConstantVectorExpression(val value1) -> 13:string
+                    Statistics: Num rows: 12288 Data size: 2187264 Basic stats: COMPLETE Column stats: COMPLETE
+                    File Output Operator
+                      compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
+                      Statistics: Num rows: 12288 Data size: 2187264 Basic stats: COMPLETE Column stats: COMPLETE
+                      table:
+                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: []
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: string, string
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select 'key1', 'value1' from alltypesorc tablesample (1 rows)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@alltypesorc
+#### A masked pattern was here ####
+POSTHOOK: query: select 'key1', 'value1' from alltypesorc tablesample (1 rows)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@alltypesorc
+#### A masked pattern was here ####
+_c0	_c1
+key1	value1
+PREHOOK: query: create table decimal_2 (t decimal(18,9)) stored as orc
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@decimal_2
+POSTHOOK: query: create table decimal_2 (t decimal(18,9)) stored as orc
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@decimal_2
+PREHOOK: query: explain vectorization detail
+insert overwrite table decimal_2
+  select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows)
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization detail
+insert overwrite table decimal_2
+  select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows)
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Row Limit Per Split: 1
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: COMPLETE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Select Operator
+                    expressions: 17.29 (type: decimal(18,9))
+                    outputColumnNames: _col0
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [12]
+                        selectExpressions: ConstantVectorExpression(val 17.29) -> 12:decimal(18,9)
+                    Statistics: Num rows: 12288 Data size: 1376256 Basic stats: COMPLETE Column stats: COMPLETE
+                    File Output Operator
+                      compressed: false
+                      File Sink Vectorization:
+                          className: VectorFileSinkOperator
+                          native: false
+                      Statistics: Num rows: 12288 Data size: 1376256 Basic stats: COMPLETE Column stats: COMPLETE
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                          serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                          name: default.decimal_2
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: []
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: decimal(18,9)
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.decimal_2
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table decimal_2
+  select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@alltypesorc
+PREHOOK: Output: default@decimal_2
+POSTHOOK: query: insert overwrite table decimal_2
+  select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@alltypesorc
+POSTHOOK: Output: default@decimal_2
+POSTHOOK: Lineage: decimal_2.t EXPRESSION []
+_col0
+PREHOOK: query: select count(*) from decimal_2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@decimal_2
+#### A masked pattern was here ####
+POSTHOOK: query: select count(*) from decimal_2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@decimal_2
+#### A masked pattern was here ####
+_c0
+1
+PREHOOK: query: drop table decimal_2
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@decimal_2
+PREHOOK: Output: default@decimal_2
+POSTHOOK: query: drop table decimal_2
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@decimal_2
+POSTHOOK: Output: default@decimal_2
+PREHOOK: query: explain vectorization detail
+select count(1) from (select * from (Select 1 a) x order by x.a) y
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization detail
+select count(1) from (select * from (Select 1 a) x order by x.a) y
+POSTHOOK: type: QUERY
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+      Edges:
+        Reducer 2 <- Map 1 (SORT, 1)
+        Reducer 3 <- Reducer 2 (GROUP, 1)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: _dummy_table
+                  Row Limit Per Split: 1
+                  Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: 1 (type: int)
+                      sort order: +
+                      Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+            Map Vectorization:
+                enabled: false
+#### A masked pattern was here ####
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: KEY.reducesinkkey0:int
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint
+            Reduce Operator Tree:
+              Select Operator
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: []
+                Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+                Group By Operator
+                  aggregations: count(1)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 1:long) -> bigint
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0]
+                  mode: hash
+                  outputColumnNames: _col0
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                  Reduce Output Operator
+                    sort order: 
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkEmptyKeyOperator
+                        keyColumns: []
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        valueColumns: [0]
+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                    value expressions: _col0 (type: bigint)
+        Reducer 3 
+            Execution mode: vectorized
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:bigint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0]
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select count(1) from (select * from (Select 1 a) x order by x.a) y
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+POSTHOOK: query: select count(1) from (select * from (Select 1 a) x order by x.a) y
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+_c0
+1
+PREHOOK: query: explain vectorization detail
+create temporary table dual as select 1
+PREHOOK: type: CREATETABLE_AS_SELECT
+POSTHOOK: query: explain vectorization detail
+create temporary table dual as select 1
+POSTHOOK: type: CREATETABLE_AS_SELECT
+Explain
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-3 depends on stages: Stage-0
+  Stage-2 depends on stages: Stage-3
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: _dummy_table
+                  Row Limit Per Split: 1
+                  Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: 1 (type: int)
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+                    File Output Operator
+                      compressed: false
+                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+                      table:
+                          input format: org.apache.hadoop.mapred.TextInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                          name: default.dual
+            Map Vectorization:
+                enabled: false
+#### A masked pattern was here ####
+
+  Stage: Stage-0
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
+  Stage: Stage-3
+      Create Table Operator:
+        Create Table
+          columns: _c0 int
+          input format: org.apache.hadoop.mapred.TextInputFormat
+#### A masked pattern was here ####
+          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+          serde name: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          name: default.dual
+          isTemporary: true
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: create temporary table dual as select 1
+PREHOOK: type: CREATETABLE_AS_SELECT
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: database:default
+PREHOOK: Output: default@dual
+POSTHOOK: query: create temporary table dual as select 1
+POSTHOOK: type: CREATETABLE_AS_SELECT
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@dual
+_c0
+PREHOOK: query: select * from dual
+PREHOOK: type: QUERY
+PREHOOK: Input: default@dual
+#### A masked pattern was here ####
+POSTHOOK: query: select * from dual
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@dual
+#### A masked pattern was here ####
+dual._c0
+1
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_0.q.out b/ql/src/test/results/clientpositive/spark/vectorization_0.q.out
index 3f3c664c5e..9c39b33f4d 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_0.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_0.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ctinyint) as c1,
        MAX(ctinyint),
        COUNT(ctinyint),
@@ -6,7 +6,7 @@ SELECT MIN(ctinyint) as c1,
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ctinyint) as c1,
        MAX(ctinyint),
        COUNT(ctinyint),
@@ -51,8 +51,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -61,8 +63,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3]
                         Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: tinyint), _col1 (type: tinyint), _col2 (type: bigint), _col3 (type: bigint)
             Execution mode: vectorized
@@ -74,23 +78,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:tinyint, VALUE._col1:tinyint, VALUE._col2:bigint, VALUE._col3:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3)
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFMaxLong(col 1) -> tinyint, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -100,8 +117,10 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: [1, 2, 3]
                   Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col1 (type: tinyint), _col2 (type: bigint), _col3 (type: bigint)
         Reducer 3 
@@ -109,10 +128,16 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: KEY.reducesinkkey0:tinyint, VALUE._col0:tinyint, VALUE._col1:bigint, VALUE._col2:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: tinyint), VALUE._col0 (type: tinyint), VALUE._col1 (type: bigint), VALUE._col2 (type: bigint)
@@ -158,12 +183,12 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -64	62	9173	12288
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(ctinyint) as c1
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(ctinyint) as c1
 FROM   alltypesorc
 ORDER BY c1
@@ -205,8 +230,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -215,8 +242,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0]
                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: bigint)
             Execution mode: vectorized
@@ -228,23 +257,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: sum(VALUE._col0)
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -254,18 +296,26 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: []
                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
         Reducer 3 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: KEY.reducesinkkey0:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: bigint)
@@ -369,17 +419,20 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:tinyint> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), variance(VALUE._col1), var_pop(VALUE._col2), var_samp(VALUE._col3), std(VALUE._col4), stddev(VALUE._col5), stddev_pop(VALUE._col6), stddev_samp(VALUE._col7)
@@ -448,7 +501,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -4.344925324321378	1158.3003004768184	1158.3003004768184	1158.4265870337827	34.033811136527426	34.033811136527426	34.033811136527426	34.03566639620536
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cbigint) as c1,
        MAX(cbigint),
        COUNT(cbigint),
@@ -456,7 +509,7 @@ SELECT MIN(cbigint) as c1,
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cbigint) as c1,
        MAX(cbigint),
        COUNT(cbigint),
@@ -501,8 +554,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinLong(col 3) -> bigint, VectorUDAFMaxLong(col 3) -> bigint, VectorUDAFCount(col 3) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -511,8 +566,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3]
                         Statistics: Num rows: 1 Data size: 32 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: bigint), _col1 (type: bigint), _col2 (type: bigint), _col3 (type: bigint)
             Execution mode: vectorized
@@ -524,23 +581,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [3]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:bigint, VALUE._col1:bigint, VALUE._col2:bigint, VALUE._col3:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3)
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> bigint, VectorUDAFMaxLong(col 1) -> bigint, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -550,8 +620,10 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: [1, 2, 3]
                   Statistics: Num rows: 1 Data size: 32 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: bigint)
         Reducer 3 
@@ -559,10 +631,16 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: KEY.reducesinkkey0:bigint, VALUE._col0:bigint, VALUE._col1:bigint, VALUE._col2:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: bigint), VALUE._col0 (type: bigint), VALUE._col1 (type: bigint), VALUE._col2 (type: bigint)
@@ -608,12 +686,12 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -2147311592	2145498388	9173	12288
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cbigint) as c1
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cbigint) as c1
 FROM   alltypesorc
 ORDER BY c1
@@ -655,8 +733,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 3) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -665,8 +745,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0]
                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: bigint)
             Execution mode: vectorized
@@ -678,23 +760,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [3]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: sum(VALUE._col0)
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -704,18 +799,26 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: []
                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
         Reducer 3 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: KEY.reducesinkkey0:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: bigint)
@@ -819,17 +922,20 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:bigint> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), variance(VALUE._col1), var_pop(VALUE._col2), var_samp(VALUE._col3), std(VALUE._col4), stddev(VALUE._col5), stddev_pop(VALUE._col6), stddev_samp(VALUE._col7)
@@ -898,7 +1004,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -1.8515862077935246E8	2.07689300543081907E18	2.07689300543081907E18	2.07711944383088768E18	1.441142951074188E9	1.441142951074188E9	1.441142951074188E9	1.4412215110214279E9
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cfloat) as c1,
        MAX(cfloat),
        COUNT(cfloat),
@@ -906,7 +1012,7 @@ SELECT MIN(cfloat) as c1,
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(cfloat) as c1,
        MAX(cfloat),
        COUNT(cfloat),
@@ -951,8 +1057,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinDouble(col 4) -> float, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFCount(col 4) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -961,8 +1069,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0, 1, 2, 3]
                         Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: float), _col1 (type: float), _col2 (type: bigint), _col3 (type: bigint)
             Execution mode: vectorized
@@ -974,23 +1084,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [4]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: VALUE._col0:float, VALUE._col1:float, VALUE._col2:bigint, VALUE._col3:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3)
                 Group By Vectorization:
                     aggregators: VectorUDAFMinDouble(col 0) -> float, VectorUDAFMaxDouble(col 1) -> float, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -1000,8 +1123,10 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: [1, 2, 3]
                   Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col1 (type: float), _col2 (type: bigint), _col3 (type: bigint)
         Reducer 3 
@@ -1009,10 +1134,16 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: KEY.reducesinkkey0:float, VALUE._col0:float, VALUE._col1:bigint, VALUE._col2:bigint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: float), VALUE._col0 (type: float), VALUE._col1 (type: bigint), VALUE._col2 (type: bigint)
@@ -1058,12 +1189,12 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -64.0	79.553	9173	12288
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cfloat) as c1
 FROM   alltypesorc
 ORDER BY c1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT SUM(cfloat) as c1
 FROM   alltypesorc
 ORDER BY c1
@@ -1105,8 +1236,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumDouble(col 4) -> double
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -1115,8 +1248,10 @@ STAGE PLANS:
                         sort order: 
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkEmptyKeyOperator
+                            keyColumns: []
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [0]
                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: double)
             Execution mode: vectorized
@@ -1128,23 +1263,36 @@ STAGE PLANS:
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [4]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: VALUE._col0:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: sum(VALUE._col0)
                 Group By Vectorization:
                     aggregators: VectorUDAFSumDouble(col 0) -> double
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1154,18 +1302,26 @@ STAGE PLANS:
                   sort order: +
                   Reduce Sink Vectorization:
                       className: VectorReduceSinkObjectHashOperator
+                      keyColumns: [0]
                       native: true
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      valueColumns: []
                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
         Reducer 3 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 1
+                    dataColumns: KEY.reducesinkkey0:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: double)
@@ -1269,17 +1425,20 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:float> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), variance(VALUE._col1), var_pop(VALUE._col2), var_samp(VALUE._col3), std(VALUE._col4), stddev(VALUE._col5), stddev_pop(VALUE._col6), stddev_samp(VALUE._col7)
@@ -1349,7 +1508,7 @@ POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
 -4.303895780321011	1163.8972588604984	1163.8972588604984	1164.0241556397025	34.115938487171924	34.115938487171924	34.115938487171924	34.11779822379666
 WARNING: Comparing a bigint and a double may result in a loss of precision.
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT AVG(cbigint),
        (-(AVG(cbigint))),
        (-6432 + AVG(cbigint)),
@@ -1376,7 +1535,7 @@ WHERE  (((cstring2 LIKE '%b%')
             AND ((cboolean2 = 1)
                  AND (3569 = ctinyint))))
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT AVG(cbigint),
        (-(AVG(cbigint))),
        (-6432 + AVG(cbigint)),
@@ -1444,46 +1603,84 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: avg(cbigint), stddev_pop(cbigint), var_samp(cbigint), count(), sum(cfloat), min(ctinyint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFAvgLong(col 3) -> struct<count:bigint,sum:double>, VectorUDAFStdPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFMinLong(col 0) -> tinyint
+                            aggregators: VectorUDAFAvgLong(col 3) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFStdPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFMinLong(col 0) -> tinyint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(col 3) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                         Statistics: Num rows: 1 Data size: 260 Basic stats: COMPLETE Column stats: NONE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4, 5]
                           Statistics: Num rows: 1 Data size: 260 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col0 (type: struct<count:bigint,sum:double,input:bigint>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: bigint), _col4 (type: double), _col5 (type: tinyint)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 7, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: decimal(13,3), double
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:bigint> of Column[VALUE._col0] not supported
-                vectorized: false
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,input:bigint>, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:bigint, VALUE._col4:double, VALUE._col5:tinyint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), stddev_pop(VALUE._col1), var_samp(VALUE._col2), count(VALUE._col3), sum(VALUE._col4), min(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFStdPopFinal(col 1) -> double, VectorUDAFVarSampFinal(col 2) -> double, VectorUDAFCountMerge(col 3) -> bigint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFMinLong(col 5) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                 Statistics: Num rows: 1 Data size: 260 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
                   expressions: _col0 (type: double), (- _col0) (type: double), (-6432.0 + _col0) (type: double), _col1 (type: double), (- (-6432.0 + _col0)) (type: double), ((- (-6432.0 + _col0)) + (-6432.0 + _col0)) (type: double), _col2 (type: double), (- (-6432.0 + _col0)) (type: double), (-6432.0 + (- (-6432.0 + _col0))) (type: double), (- (-6432.0 + _col0)) (type: double), ((- (-6432.0 + _col0)) / (- (-6432.0 + _col0))) (type: double), _col3 (type: bigint), _col4 (type: double), (_col2 % _col1) (type: double), (- _col2) (type: double), ((- (-6432.0 + _col0)) * (- _col0)) (type: double), _col5 (type: tinyint), (- _col5) (type: tinyint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 7, 1, 9, 11, 2, 10, 8, 13, 12, 3, 4, 14, 15, 18, 5, 19]
+                      selectExpressions: DoubleColUnaryMinus(col 0) -> 6:double, DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 7:double, DoubleColUnaryMinus(col 8)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 9:double, DoubleColAddDoubleColumn(col 10, col 8)(children: DoubleColUnaryMinus(col 8)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 10:double, DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 11:double, DoubleColUnaryMinus(col 8)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 10:double, DoubleScalarAddDoubleColumn(val -6432.0, col 12)(children: DoubleColUnaryMinus(col 8)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 8:double) -> 12:double) -> 8:double, DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 12:double) -> 13:double, DoubleColDivideDoubleColumn(col 14, col 15)(children: DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 12:double) -> 14:double, DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 12:double) -> 15:double) -> 12:double, DoubleColModuloDoubleColumn(col 2, col 1) -> 14:double, DoubleColUnaryMinus(col 2) -> 15:double, DoubleColMultiplyDoubleColumn(col 17, col 16)(children: DoubleColUnaryMinus(col 16)(children: DoubleScalarAddDoubleColumn(val -6432.0, col 0) -> 16:double) -> 17:double, DoubleColUnaryMinus(col 0) -> 16:double) -> 18:double, LongColUnaryMinus(col 5) -> 19:long
                   Statistics: Num rows: 1 Data size: 260 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 260 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_1.q.out b/ql/src/test/results/clientpositive/spark/vectorization_1.q.out
index e0a434480b..78f15170f6 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_1.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_1.q.out
@@ -1,3 +1,176 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT VAR_POP(ctinyint),
+       (VAR_POP(ctinyint) / -26.28),
+       SUM(cfloat),
+       (-1.389 + SUM(cfloat)),
+       (SUM(cfloat) * (-1.389 + SUM(cfloat))),
+       MAX(ctinyint),
+       (-((SUM(cfloat) * (-1.389 + SUM(cfloat))))),
+       MAX(cint),
+       (MAX(cint) * 79.553),
+       VAR_SAMP(cdouble),
+       (10.175 % (-((SUM(cfloat) * (-1.389 + SUM(cfloat)))))),
+       COUNT(cint),
+       (-563 % MAX(cint))
+FROM   alltypesorc
+WHERE  (((cdouble > ctinyint)
+         AND (cboolean2 > 0))
+        OR ((cbigint < ctinyint)
+            OR ((cint > cbigint)
+                OR (cboolean1 < 0))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT VAR_POP(ctinyint),
+       (VAR_POP(ctinyint) / -26.28),
+       SUM(cfloat),
+       (-1.389 + SUM(cfloat)),
+       (SUM(cfloat) * (-1.389 + SUM(cfloat))),
+       MAX(ctinyint),
+       (-((SUM(cfloat) * (-1.389 + SUM(cfloat))))),
+       MAX(cint),
+       (MAX(cint) * 79.553),
+       VAR_SAMP(cdouble),
+       (10.175 % (-((SUM(cfloat) * (-1.389 + SUM(cfloat)))))),
+       COUNT(cint),
+       (-563 % MAX(cint))
+FROM   alltypesorc
+WHERE  (((cdouble > ctinyint)
+         AND (cboolean2 > 0))
+        OR ((cbigint < ctinyint)
+            OR ((cint > cbigint)
+                OR (cboolean1 < 0))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+      Edges:
+        Reducer 2 <- Map 1 (GROUP, 1)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: FilterDoubleColGreaterDoubleColumn(col 5, col 12)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterLongColGreaterLongScalar(col 11, val 0) -> boolean) -> boolean, FilterLongColLessLongColumn(col 3, col 0)(children: col 0) -> boolean, FilterLongColGreaterLongColumn(col 2, col 3)(children: col 2) -> boolean, FilterLongColLessLongScalar(col 10, val 0) -> boolean) -> boolean
+                    predicate: (((cdouble > UDFToDouble(ctinyint)) and (cboolean2 > 0)) or (cbigint < UDFToLong(ctinyint)) or (UDFToLong(cint) > cbigint) or (cboolean1 < 0)) (type: boolean)
+                    Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), cint (type: int), cfloat (type: float), cdouble (type: double)
+                      outputColumnNames: ctinyint, cint, cfloat, cdouble
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 2, 4, 5]
+                      Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: var_pop(ctinyint), sum(cfloat), max(ctinyint), max(cint), var_samp(cdouble), count(cint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumDouble(col 4) -> double, VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFMaxLong(col 2) -> int, VectorUDAFVarSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 2) -> bigint
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                        Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4, 5]
+                          Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: double), _col2 (type: tinyint), _col3 (type: int), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: bigint)
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 2, 3, 4, 5, 10, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:double, VALUE._col2:tinyint, VALUE._col3:int, VALUE._col4:struct<count:bigint,sum:double,variance:double>, VALUE._col5:bigint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: var_pop(VALUE._col0), sum(VALUE._col1), max(VALUE._col2), max(VALUE._col3), var_samp(VALUE._col4), count(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFVarPopFinal(col 0) -> double, VectorUDAFSumDouble(col 1) -> double, VectorUDAFMaxLong(col 2) -> tinyint, VectorUDAFMaxLong(col 3) -> int, VectorUDAFVarSampFinal(col 4) -> double, VectorUDAFCountMerge(col 5) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: double), (_col0 / -26.28) (type: double), _col1 (type: double), (-1.389 + _col1) (type: double), (_col1 * (-1.389 + _col1)) (type: double), _col2 (type: tinyint), (- (_col1 * (-1.389 + _col1))) (type: double), _col3 (type: int), (CAST( _col3 AS decimal(10,0)) * 79.553) (type: decimal(16,3)), _col4 (type: double), (10.175 % (- (_col1 * (-1.389 + _col1)))) (type: double), _col5 (type: bigint), (-563 % _col3) (type: int)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 1, 7, 9, 2, 8, 3, 12, 4, 13, 5, 14]
+                      selectExpressions: DoubleColDivideDoubleScalar(col 0, val -26.28) -> 6:double, DoubleScalarAddDoubleColumn(val -1.389, col 1) -> 7:double, DoubleColMultiplyDoubleColumn(col 1, col 8)(children: DoubleScalarAddDoubleColumn(val -1.389, col 1) -> 8:double) -> 9:double, DoubleColUnaryMinus(col 10)(children: DoubleColMultiplyDoubleColumn(col 1, col 8)(children: DoubleScalarAddDoubleColumn(val -1.389, col 1) -> 8:double) -> 10:double) -> 8:double, DecimalColMultiplyDecimalScalar(col 11, val 79.553)(children: CastLongToDecimal(col 3) -> 11:decimal(10,0)) -> 12:decimal(16,3), DoubleScalarModuloDoubleColumn(val 10.175, col 10)(children: DoubleColUnaryMinus(col 13)(children: DoubleColMultiplyDoubleColumn(col 1, col 10)(children: DoubleScalarAddDoubleColumn(val -1.389, col 1) -> 10:double) -> 13:double) -> 10:double) -> 13:double, LongScalarModuloLongColumn(val -563, col 3) -> 14:long
+                  Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT VAR_POP(ctinyint),
        (VAR_POP(ctinyint) / -26.28),
        SUM(cfloat),
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_10.q.out b/ql/src/test/results/clientpositive/spark/vectorization_10.q.out
index 9dad4c440d..4e9cce3264 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_10.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_10.q.out
@@ -1,3 +1,120 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cdouble,
+       ctimestamp1,
+       ctinyint,
+       cboolean1,
+       cstring1,
+       (-(cdouble)),
+       (cdouble + csmallint),
+       ((cdouble + csmallint) % 33),
+       (-(cdouble)),
+       (ctinyint % cdouble),
+       (ctinyint % csmallint),
+       (-(cdouble)),
+       (cbigint * (ctinyint % csmallint)),
+       (9763215.5639 - (cdouble + csmallint)),
+       (-((-(cdouble))))
+FROM   alltypesorc
+WHERE  (((cstring2 <= '10')
+         OR ((ctinyint > cdouble)
+             AND (-5638.15 >= ctinyint)))
+        OR ((cdouble > 6981)
+            AND ((csmallint = 9763215.5639)
+                 OR (cstring1 LIKE '%a'))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cdouble,
+       ctimestamp1,
+       ctinyint,
+       cboolean1,
+       cstring1,
+       (-(cdouble)),
+       (cdouble + csmallint),
+       ((cdouble + csmallint) % 33),
+       (-(cdouble)),
+       (ctinyint % cdouble),
+       (ctinyint % csmallint),
+       (-(cdouble)),
+       (cbigint * (ctinyint % csmallint)),
+       (9763215.5639 - (cdouble + csmallint)),
+       (-((-(cdouble))))
+FROM   alltypesorc
+WHERE  (((cstring2 <= '10')
+         OR ((ctinyint > cdouble)
+             AND (-5638.15 >= ctinyint)))
+        OR ((cdouble > 6981)
+            AND ((csmallint = 9763215.5639)
+                 OR (cstring1 LIKE '%a'))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterStringGroupColLessEqualStringScalar(col 7, val 10) -> boolean, FilterExprAndExpr(children: FilterDoubleColGreaterDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterDecimalScalarGreaterEqualDecimalColumn(val -5638.15, col 13)(children: CastLongToDecimal(col 0) -> 13:decimal(6,2)) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColGreaterDoubleScalar(col 5, val 6981.0) -> boolean, FilterExprOrExpr(children: FilterDecimalColEqualDecimalScalar(col 14, val 9763215.5639)(children: CastLongToDecimal(col 1) -> 14:decimal(11,4)) -> boolean, FilterStringColLikeStringScalar(col 6, pattern %a) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: ((cstring2 <= '10') or ((UDFToDouble(ctinyint) > cdouble) and (-5638.15 >= CAST( ctinyint AS decimal(6,2)))) or ((cdouble > 6981.0) and ((CAST( csmallint AS decimal(11,4)) = 9763215.5639) or (cstring1 like '%a')))) (type: boolean)
+                    Statistics: Num rows: 5461 Data size: 167650 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: cdouble (type: double), ctimestamp1 (type: timestamp), ctinyint (type: tinyint), cboolean1 (type: boolean), cstring1 (type: string), (- cdouble) (type: double), (cdouble + UDFToDouble(csmallint)) (type: double), ((cdouble + UDFToDouble(csmallint)) % 33.0) (type: double), (- cdouble) (type: double), (UDFToDouble(ctinyint) % cdouble) (type: double), (UDFToShort(ctinyint) % csmallint) (type: smallint), (- cdouble) (type: double), (cbigint * UDFToLong((UDFToShort(ctinyint) % csmallint))) (type: bigint), (9763215.5639 - (cdouble + UDFToDouble(csmallint))) (type: double), (- (- cdouble)) (type: double)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [5, 8, 0, 10, 6, 12, 16, 15, 17, 19, 20, 18, 22, 23, 25]
+                          selectExpressions: DoubleColUnaryMinus(col 5) -> 12:double, DoubleColAddDoubleColumn(col 5, col 15)(children: CastLongToDouble(col 1) -> 15:double) -> 16:double, DoubleColModuloDoubleScalar(col 17, val 33.0)(children: DoubleColAddDoubleColumn(col 5, col 15)(children: CastLongToDouble(col 1) -> 15:double) -> 17:double) -> 15:double, DoubleColUnaryMinus(col 5) -> 17:double, DoubleColModuloDoubleColumn(col 18, col 5)(children: CastLongToDouble(col 0) -> 18:double) -> 19:double, LongColModuloLongColumn(col 0, col 1)(children: col 0) -> 20:long, DoubleColUnaryMinus(col 5) -> 18:double, LongColMultiplyLongColumn(col 3, col 21)(children: col 21) -> 22:long, DoubleScalarSubtractDoubleColumn(val 9763215.5639, col 24)(children: DoubleColAddDoubleColumn(col 5, col 23)(children: CastLongToDouble(col 1) -> 23:double) -> 24:double) -> 23:double, DoubleColUnaryMinus(col 24)(children: DoubleColUnaryMinus(col 5) -> 24:double) -> 25:double
+                      Statistics: Num rows: 5461 Data size: 167650 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
+                        Statistics: Num rows: 5461 Data size: 167650 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 3, 5, 6, 7, 8, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, decimal(6,2), decimal(11,4), double, double, double, double, double, bigint, bigint, bigint, double, double, double
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT cdouble,
        ctimestamp1,
        ctinyint,
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_11.q.out b/ql/src/test/results/clientpositive/spark/vectorization_11.q.out
index dff58dab42..f79c3a0a44 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_11.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_11.q.out
@@ -1,3 +1,102 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cstring1,
+       cboolean1,
+       cdouble,
+       ctimestamp1,
+       (-3728 * csmallint),
+       (cdouble - 9763215.5639),
+       (-(cdouble)),
+       ((-(cdouble)) + 6981),
+       (cdouble * -5638.15)
+FROM   alltypesorc
+WHERE  ((cstring2 = cstring1)
+        OR ((ctimestamp1 IS NULL)
+            AND (cstring1 LIKE '%a')))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cstring1,
+       cboolean1,
+       cdouble,
+       ctimestamp1,
+       (-3728 * csmallint),
+       (cdouble - 9763215.5639),
+       (-(cdouble)),
+       ((-(cdouble)) + 6981),
+       (cdouble * -5638.15)
+FROM   alltypesorc
+WHERE  ((cstring2 = cstring1)
+        OR ((ctimestamp1 IS NULL)
+            AND (cstring1 LIKE '%a')))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterStringGroupColEqualStringGroupColumn(col 7, col 6) -> boolean, FilterExprAndExpr(children: SelectColumnIsNull(col 8) -> boolean, FilterStringColLikeStringScalar(col 6, pattern %a) -> boolean) -> boolean) -> boolean
+                    predicate: ((cstring2 = cstring1) or (ctimestamp1 is null and (cstring1 like '%a'))) (type: boolean)
+                    Statistics: Num rows: 9216 Data size: 282927 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: cstring1 (type: string), cboolean1 (type: boolean), cdouble (type: double), ctimestamp1 (type: timestamp), (-3728 * UDFToInteger(csmallint)) (type: int), (cdouble - 9763215.5639) (type: double), (- cdouble) (type: double), ((- cdouble) + 6981.0) (type: double), (cdouble * -5638.15) (type: double)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [6, 10, 5, 8, 12, 13, 14, 16, 15]
+                          selectExpressions: LongScalarMultiplyLongColumn(val -3728, col 1)(children: col 1) -> 12:long, DoubleColSubtractDoubleScalar(col 5, val 9763215.5639) -> 13:double, DoubleColUnaryMinus(col 5) -> 14:double, DoubleColAddDoubleScalar(col 15, val 6981.0)(children: DoubleColUnaryMinus(col 5) -> 15:double) -> 16:double, DoubleColMultiplyDoubleScalar(col 5, val -5638.15) -> 15:double
+                      Statistics: Num rows: 9216 Data size: 282927 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
+                        Statistics: Num rows: 9216 Data size: 282927 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [1, 5, 6, 7, 8, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint, double, double, double, double
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT cstring1,
        cboolean1,
        cdouble,
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_12.q.out b/ql/src/test/results/clientpositive/spark/vectorization_12.q.out
index 6a7f69c698..c17043bdd0 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_12.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_12.q.out
@@ -1,3 +1,242 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT   cbigint,
+         cboolean1,
+         cstring1,
+         ctimestamp1,
+         cdouble,
+         (-6432 * cdouble),
+         (-(cbigint)),
+         COUNT(cbigint),
+         (cbigint * COUNT(cbigint)),
+         STDDEV_SAMP(cbigint),
+         ((-6432 * cdouble) / -6432),
+         (-(((-6432 * cdouble) / -6432))),
+         AVG(cdouble),
+         (-((-6432 * cdouble))),
+         (-5638.15 + cbigint),
+         SUM(cbigint),
+         (AVG(cdouble) / (-6432 * cdouble)),
+         AVG(cdouble),
+         (-((-(((-6432 * cdouble) / -6432))))),
+         (((-6432 * cdouble) / -6432) + (-((-6432 * cdouble)))),
+         STDDEV_POP(cdouble)
+FROM     alltypesorc
+WHERE    (((ctimestamp1 IS NULL)
+           AND ((cboolean1 >= cboolean2)
+                OR (ctinyint != csmallint)))
+          AND ((cstring1 LIKE '%a')
+              OR ((cboolean2 <= 1)
+                  AND (cbigint >= csmallint))))
+GROUP BY cbigint, cboolean1, cstring1, ctimestamp1, cdouble
+ORDER BY ctimestamp1, cdouble, cbigint, cstring1
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT   cbigint,
+         cboolean1,
+         cstring1,
+         ctimestamp1,
+         cdouble,
+         (-6432 * cdouble),
+         (-(cbigint)),
+         COUNT(cbigint),
+         (cbigint * COUNT(cbigint)),
+         STDDEV_SAMP(cbigint),
+         ((-6432 * cdouble) / -6432),
+         (-(((-6432 * cdouble) / -6432))),
+         AVG(cdouble),
+         (-((-6432 * cdouble))),
+         (-5638.15 + cbigint),
+         SUM(cbigint),
+         (AVG(cdouble) / (-6432 * cdouble)),
+         AVG(cdouble),
+         (-((-(((-6432 * cdouble) / -6432))))),
+         (((-6432 * cdouble) / -6432) + (-((-6432 * cdouble)))),
+         STDDEV_POP(cdouble)
+FROM     alltypesorc
+WHERE    (((ctimestamp1 IS NULL)
+           AND ((cboolean1 >= cboolean2)
+                OR (ctinyint != csmallint)))
+          AND ((cstring1 LIKE '%a')
+              OR ((cboolean2 <= 1)
+                  AND (cbigint >= csmallint))))
+GROUP BY cbigint, cboolean1, cstring1, ctimestamp1, cdouble
+ORDER BY ctimestamp1, cdouble, cbigint, cstring1
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+      Edges:
+        Reducer 2 <- Map 1 (GROUP, 2)
+        Reducer 3 <- Reducer 2 (SORT, 1)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: SelectColumnIsNull(col 8) -> boolean, FilterExprOrExpr(children: FilterLongColGreaterEqualLongColumn(col 10, col 11) -> boolean, FilterLongColNotEqualLongColumn(col 0, col 1)(children: col 0) -> boolean) -> boolean, FilterExprOrExpr(children: FilterStringColLikeStringScalar(col 6, pattern %a) -> boolean, FilterExprAndExpr(children: FilterLongColLessEqualLongScalar(col 11, val 1) -> boolean, FilterLongColGreaterEqualLongColumn(col 3, col 1)(children: col 1) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: (ctimestamp1 is null and ((cboolean1 >= cboolean2) or (UDFToShort(ctinyint) <> csmallint)) and ((cstring1 like '%a') or ((cboolean2 <= 1) and (cbigint >= UDFToLong(csmallint))))) (type: boolean)
+                    Statistics: Num rows: 5006 Data size: 153682 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: cbigint (type: bigint), cdouble (type: double), cstring1 (type: string), cboolean1 (type: boolean)
+                      outputColumnNames: cbigint, cdouble, cstring1, cboolean1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [3, 5, 6, 10]
+                      Statistics: Num rows: 5006 Data size: 153682 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: count(cbigint), stddev_samp(cbigint), avg(cdouble), sum(cbigint), stddev_pop(cdouble)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFCount(col 3) -> bigint, VectorUDAFStdSampLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 5, col 3, col 6, col 10
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4]
+                        keys: cdouble (type: double), cbigint (type: bigint), cstring1 (type: string), cboolean1 (type: boolean)
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+                        Statistics: Num rows: 5006 Data size: 153682 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          key expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: string), _col3 (type: boolean)
+                          sort order: ++++
+                          Map-reduce partition columns: _col0 (type: double), _col1 (type: bigint), _col2 (type: string), _col3 (type: boolean)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              keyColumns: [0, 1, 2, 3]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              partitionColumns: [0, 1, 2, 3]
+                              valueColumns: [4, 5, 6, 7, 8]
+                          Statistics: Num rows: 5006 Data size: 153682 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col4 (type: bigint), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: struct<count:bigint,sum:double,input:double>), _col7 (type: bigint), _col8 (type: struct<count:bigint,sum:double,variance:double>)
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 3, 5, 6, 8, 10, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaa
+                reduceColumnSortOrder: ++++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 9
+                    dataColumns: KEY._col0:double, KEY._col1:bigint, KEY._col2:string, KEY._col3:boolean, VALUE._col0:bigint, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:struct<count:bigint,sum:double,input:double>, VALUE._col3:bigint, VALUE._col4:struct<count:bigint,sum:double,variance:double>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0), stddev_samp(VALUE._col1), avg(VALUE._col2), sum(VALUE._col3), stddev_pop(VALUE._col4)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 4) -> bigint, VectorUDAFStdSampFinal(col 5) -> double, VectorUDAFAvgFinal(col 6) -> double, VectorUDAFSumLong(col 7) -> bigint, VectorUDAFStdPopFinal(col 8) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2, col 3
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4]
+                keys: KEY._col0 (type: double), KEY._col1 (type: bigint), KEY._col2 (type: string), KEY._col3 (type: boolean)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+                Statistics: Num rows: 2503 Data size: 76841 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col1 (type: bigint), _col3 (type: boolean), _col2 (type: string), _col0 (type: double), (-6432.0 * _col0) (type: double), (- _col1) (type: bigint), _col4 (type: bigint), (_col1 * _col4) (type: bigint), _col5 (type: double), ((-6432.0 * _col0) / -6432.0) (type: double), (- ((-6432.0 * _col0) / -6432.0)) (type: double), _col6 (type: double), (- (-6432.0 * _col0)) (type: double), (-5638.15 + CAST( _col1 AS decimal(19,0))) (type: decimal(22,2)), _col7 (type: bigint), (_col6 / (-6432.0 * _col0)) (type: double), (- (- ((-6432.0 * _col0) / -6432.0))) (type: double), (((-6432.0 * _col0) / -6432.0) + (- (-6432.0 * _col0))) (type: double), _col8 (type: double)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col17, _col18, _col19
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [1, 3, 2, 0, 9, 10, 4, 11, 5, 13, 12, 6, 15, 17, 7, 18, 19, 14, 8]
+                      selectExpressions: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 9:double, LongColUnaryMinus(col 1) -> 10:long, LongColMultiplyLongColumn(col 1, col 4) -> 11:long, DoubleColDivideDoubleScalar(col 12, val -6432.0)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 12:double) -> 13:double, DoubleColUnaryMinus(col 14)(children: DoubleColDivideDoubleScalar(col 12, val -6432.0)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 12:double) -> 14:double) -> 12:double, DoubleColUnaryMinus(col 14)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 15:double, DecimalScalarAddDecimalColumn(val -5638.15, col 16)(children: CastLongToDecimal(col 1) -> 16:decimal(19,0)) -> 17:decimal(22,2), DoubleColDivideDoubleColumn(col 6, col 14)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 18:double, DoubleColUnaryMinus(col 14)(children: DoubleColUnaryMinus(col 19)(children: DoubleColDivideDoubleScalar(col 14, val -6432.0)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 19:double) -> 14:double) -> 19:double, DoubleColAddDoubleColumn(col 20, col 21)(children: DoubleColDivideDoubleScalar(col 14, val -6432.0)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 20:double, DoubleColUnaryMinus(col 14)(children: DoubleScalarMultiplyDoubleColumn(val -6432.0, col 0) -> 14:double) -> 21:double) -> 14:double
+                  Statistics: Num rows: 2503 Data size: 76841 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col3 (type: double), _col0 (type: bigint), _col2 (type: string)
+                    sort order: +++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        keyColumns: [0, 1, 2]
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        valueColumns: [3, 9, 10, 4, 11, 5, 13, 12, 6, 15, 17, 7, 18, 19, 14, 8]
+                    Statistics: Num rows: 2503 Data size: 76841 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col1 (type: boolean), _col4 (type: double), _col5 (type: bigint), _col6 (type: bigint), _col7 (type: bigint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: double), _col12 (type: double), _col13 (type: decimal(22,2)), _col14 (type: bigint), _col15 (type: double), _col17 (type: double), _col18 (type: double), _col19 (type: double)
+        Reducer 3 
+            Execution mode: vectorized
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: aaa
+                reduceColumnSortOrder: +++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 19
+                    dataColumns: KEY.reducesinkkey0:double, KEY.reducesinkkey1:bigint, KEY.reducesinkkey2:string, VALUE._col0:boolean, VALUE._col1:double, VALUE._col2:bigint, VALUE._col3:bigint, VALUE._col4:bigint, VALUE._col5:double, VALUE._col6:double, VALUE._col7:double, VALUE._col8:double, VALUE._col9:double, VALUE._col10:decimal(22,2), VALUE._col11:bigint, VALUE._col12:double, VALUE._col13:double, VALUE._col14:double, VALUE._col15:double
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: timestamp
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey1 (type: bigint), VALUE._col0 (type: boolean), KEY.reducesinkkey2 (type: string), null (type: timestamp), KEY.reducesinkkey0 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: bigint), VALUE._col3 (type: bigint), VALUE._col4 (type: bigint), VALUE._col5 (type: double), VALUE._col6 (type: double), VALUE._col7 (type: double), VALUE._col8 (type: double), VALUE._col9 (type: double), VALUE._col10 (type: decimal(22,2)), VALUE._col11 (type: bigint), VALUE._col12 (type: double), VALUE._col8 (type: double), VALUE._col13 (type: double), VALUE._col14 (type: double), VALUE._col15 (type: double)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 3, 2, 19, 0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 11, 16, 17, 18]
+                    selectExpressions: ConstantVectorExpression(val null) -> 19:timestamp
+                Statistics: Num rows: 2503 Data size: 76841 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 2503 Data size: 76841 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT   cbigint,
          cboolean1,
          cstring1,
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_13.q.out b/ql/src/test/results/clientpositive/spark/vectorization_13.q.out
index 9a4c6c306e..bae304b99a 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_13.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_13.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cboolean1,
          ctinyint,
          ctimestamp1,
@@ -31,7 +31,7 @@ GROUP BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1
 ORDER BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16
 LIMIT 40
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cboolean1,
          ctinyint,
          ctimestamp1,
@@ -108,11 +108,12 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFMinLong(col 0) -> tinyint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 10, col 0, col 8, col 4, col 6
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: cboolean1 (type: boolean), ctinyint (type: tinyint), ctimestamp1 (type: timestamp), cfloat (type: float), cstring1 (type: string)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -121,26 +122,57 @@ STAGE PLANS:
                           key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
                           sort order: +++++
                           Map-reduce partition columns: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              keyColumns: [0, 1, 2, 3, 4]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              partitionColumns: [0, 1, 2, 3, 4]
+                              valueColumns: [5, 6, 7, 8, 9, 10]
                           Statistics: Num rows: 2730 Data size: 83809 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col5 (type: tinyint), _col6 (type: double), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,variance:double>), _col9 (type: float), _col10 (type: tinyint)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 4, 5, 6, 8, 9, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, decimal(11,4)
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col2] not supported
-                vectorized: false
+                reduceColumnNullOrder: aaaaa
+                reduceColumnSortOrder: +++++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 11
+                    dataColumns: KEY._col0:boolean, KEY._col1:tinyint, KEY._col2:timestamp, KEY._col3:float, KEY._col4:string, VALUE._col0:tinyint, VALUE._col1:double, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:struct<count:bigint,sum:double,variance:double>, VALUE._col4:float, VALUE._col5:tinyint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), sum(VALUE._col1), stddev_pop(VALUE._col2), stddev_pop(VALUE._col3), max(VALUE._col4), min(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxLong(col 5) -> tinyint, VectorUDAFSumDouble(col 6) -> double, VectorUDAFStdPopFinal(col 7) -> double, VectorUDAFStdPopFinal(col 8) -> double, VectorUDAFMaxDouble(col 9) -> float, VectorUDAFMinLong(col 10) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2, col 3, col 4
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 keys: KEY._col0 (type: boolean), KEY._col1 (type: tinyint), KEY._col2 (type: timestamp), KEY._col3 (type: float), KEY._col4 (type: string)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -148,10 +180,21 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string), (- _col1) (type: tinyint), _col5 (type: tinyint), ((- _col1) + _col5) (type: tinyint), _col6 (type: double), (_col6 * UDFToDouble(((- _col1) + _col5))) (type: double), (- _col6) (type: double), (79.553 * _col3) (type: float), _col7 (type: double), (- _col6) (type: double), _col8 (type: double), (CAST( ((- _col1) + _col5) AS decimal(3,0)) - 10.175) (type: decimal(7,3)), (- (- _col6)) (type: double), (-26.28 / (- (- _col6))) (type: double), _col9 (type: float), ((_col6 * UDFToDouble(((- _col1) + _col5))) / UDFToDouble(_col1)) (type: double), _col10 (type: tinyint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 11, 5, 13, 6, 16, 15, 17, 7, 18, 8, 20, 22, 21, 9, 25, 10]
+                      selectExpressions: LongColUnaryMinus(col 1) -> 11:long, LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 13:long, DoubleColMultiplyDoubleColumn(col 6, col 15)(children: CastLongToDouble(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 15:double) -> 16:double, DoubleColUnaryMinus(col 6) -> 15:double, DoubleScalarMultiplyDoubleColumn(val 79.5530014038086, col 3) -> 17:double, DoubleColUnaryMinus(col 6) -> 18:double, DecimalColSubtractDecimalScalar(col 19, val 10.175)(children: CastLongToDecimal(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 19:decimal(3,0)) -> 20:decimal(7,3), DoubleColUnaryMinus(col 21)(children: DoubleColUnaryMinus(col 6) -> 21:double) -> 22:double, DoubleScalarDivideDoubleColumn(val -26.28, col 23)(children: DoubleColUnaryMinus(col 21)(children: DoubleColUnaryMinus(col 6) -> 21:double) -> 23:double) -> 21:double, DoubleColDivideDoubleColumn(col 24, col 23)(children: DoubleColMultiplyDoubleColumn(col 6, col 23)(children: CastLongToDouble(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 23:double) -> 24:double, CastLongToDouble(col 1) -> 23:double) -> 25:double
                   Statistics: Num rows: 1365 Data size: 41904 Basic stats: COMPLETE Column stats: NONE
                   Reduce Output Operator
                     key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string), _col5 (type: tinyint), _col6 (type: tinyint), _col7 (type: tinyint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: double), _col15 (type: decimal(7,3)), _col16 (type: double), _col17 (type: double), _col18 (type: float), _col19 (type: double), _col20 (type: tinyint)
                     sort order: +++++++++++++++++++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        keyColumns: [0, 1, 2, 3, 4, 11, 5, 13, 6, 16, 15, 17, 7, 18, 8, 20, 22, 21, 9, 25, 10]
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        valueColumns: []
                     Statistics: Num rows: 1365 Data size: 41904 Basic stats: COMPLETE Column stats: NONE
                     TopN Hash Memory Usage: 0.1
         Reducer 3 
@@ -159,10 +202,16 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaaaaaaaaaaaaaaaaaaa
+                reduceColumnSortOrder: +++++++++++++++++++++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 21
+                    dataColumns: KEY.reducesinkkey0:boolean, KEY.reducesinkkey1:tinyint, KEY.reducesinkkey2:timestamp, KEY.reducesinkkey3:float, KEY.reducesinkkey4:string, KEY.reducesinkkey5:tinyint, KEY.reducesinkkey6:tinyint, KEY.reducesinkkey7:tinyint, KEY.reducesinkkey8:double, KEY.reducesinkkey9:double, KEY.reducesinkkey10:double, KEY.reducesinkkey11:float, KEY.reducesinkkey12:double, KEY.reducesinkkey13:double, KEY.reducesinkkey14:double, KEY.reducesinkkey15:decimal(7,3), KEY.reducesinkkey16:double, KEY.reducesinkkey17:double, KEY.reducesinkkey18:float, KEY.reducesinkkey19:double, KEY.reducesinkkey20:tinyint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: boolean), KEY.reducesinkkey1 (type: tinyint), KEY.reducesinkkey2 (type: timestamp), KEY.reducesinkkey3 (type: float), KEY.reducesinkkey4 (type: string), KEY.reducesinkkey5 (type: tinyint), KEY.reducesinkkey6 (type: tinyint), KEY.reducesinkkey7 (type: tinyint), KEY.reducesinkkey8 (type: double), KEY.reducesinkkey9 (type: double), KEY.reducesinkkey10 (type: double), KEY.reducesinkkey11 (type: float), KEY.reducesinkkey12 (type: double), KEY.reducesinkkey10 (type: double), KEY.reducesinkkey14 (type: double), KEY.reducesinkkey15 (type: decimal(7,3)), KEY.reducesinkkey16 (type: double), KEY.reducesinkkey17 (type: double), KEY.reducesinkkey18 (type: float), KEY.reducesinkkey19 (type: double), KEY.reducesinkkey20 (type: tinyint)
@@ -413,11 +462,12 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFMinLong(col 0) -> tinyint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 10, col 0, col 8, col 4, col 6
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: cboolean1 (type: boolean), ctinyint (type: tinyint), ctimestamp1 (type: timestamp), cfloat (type: float), cstring1 (type: string)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -426,26 +476,42 @@ STAGE PLANS:
                           key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
                           sort order: +++++
                           Map-reduce partition columns: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 2730 Data size: 83809 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col5 (type: tinyint), _col6 (type: double), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,variance:double>), _col9 (type: float), _col10 (type: tinyint)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col2] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), sum(VALUE._col1), stddev_pop(VALUE._col2), stddev_pop(VALUE._col3), max(VALUE._col4), min(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxLong(col 5) -> tinyint, VectorUDAFSumDouble(col 6) -> double, VectorUDAFStdPopFinal(col 7) -> double, VectorUDAFStdPopFinal(col 8) -> double, VectorUDAFMaxDouble(col 9) -> float, VectorUDAFMinLong(col 10) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2, col 3, col 4
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 keys: KEY._col0 (type: boolean), KEY._col1 (type: tinyint), KEY._col2 (type: timestamp), KEY._col3 (type: float), KEY._col4 (type: string)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -453,10 +519,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string), (- _col1) (type: tinyint), _col5 (type: tinyint), ((- _col1) + _col5) (type: tinyint), _col6 (type: double), (_col6 * UDFToDouble(((- _col1) + _col5))) (type: double), (- _col6) (type: double), (79.553 * _col3) (type: float), _col7 (type: double), (- _col6) (type: double), _col8 (type: double), (CAST( ((- _col1) + _col5) AS decimal(3,0)) - 10.175) (type: decimal(7,3)), (- (- _col6)) (type: double), (-26.28 / (- (- _col6))) (type: double), _col9 (type: float), ((_col6 * UDFToDouble(((- _col1) + _col5))) / UDFToDouble(_col1)) (type: double), _col10 (type: tinyint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 11, 5, 13, 6, 16, 15, 17, 7, 18, 8, 20, 22, 21, 9, 25, 10]
+                      selectExpressions: LongColUnaryMinus(col 1) -> 11:long, LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 13:long, DoubleColMultiplyDoubleColumn(col 6, col 15)(children: CastLongToDouble(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 15:double) -> 16:double, DoubleColUnaryMinus(col 6) -> 15:double, DoubleScalarMultiplyDoubleColumn(val 79.5530014038086, col 3) -> 17:double, DoubleColUnaryMinus(col 6) -> 18:double, DecimalColSubtractDecimalScalar(col 19, val 10.175)(children: CastLongToDecimal(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 19:decimal(3,0)) -> 20:decimal(7,3), DoubleColUnaryMinus(col 21)(children: DoubleColUnaryMinus(col 6) -> 21:double) -> 22:double, DoubleScalarDivideDoubleColumn(val -26.28, col 23)(children: DoubleColUnaryMinus(col 21)(children: DoubleColUnaryMinus(col 6) -> 21:double) -> 23:double) -> 21:double, DoubleColDivideDoubleColumn(col 24, col 23)(children: DoubleColMultiplyDoubleColumn(col 6, col 23)(children: CastLongToDouble(col 14)(children: LongColAddLongColumn(col 12, col 5)(children: LongColUnaryMinus(col 1) -> 12:long) -> 14:long) -> 23:double) -> 24:double, CastLongToDouble(col 1) -> 23:double) -> 25:double
                   Statistics: Num rows: 1365 Data size: 41904 Basic stats: COMPLETE Column stats: NONE
                   Reduce Output Operator
                     key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string), _col5 (type: tinyint), _col6 (type: tinyint), _col7 (type: tinyint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: double), _col15 (type: decimal(7,3)), _col16 (type: double), _col17 (type: double), _col18 (type: float), _col19 (type: double), _col20 (type: tinyint)
                     sort order: +++++++++++++++++++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 1365 Data size: 41904 Basic stats: COMPLETE Column stats: NONE
                     TopN Hash Memory Usage: 0.1
         Reducer 3 
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_14.q.out b/ql/src/test/results/clientpositive/spark/vectorization_14.q.out
index 154190857b..9d52abecdc 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_14.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_14.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   ctimestamp1,
          cfloat,
          cstring1,
@@ -31,7 +31,7 @@ WHERE    (((ctinyint <= cbigint)
 GROUP BY ctimestamp1, cfloat, cstring1, cboolean1, cdouble
 ORDER BY cstring1, cfloat, cdouble, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   ctimestamp1,
          cfloat,
          cstring1,
@@ -85,15 +85,36 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterLongColLessEqualLongColumn(col 0, col 3)(children: col 0) -> boolean, FilterExprOrExpr(children: FilterDoubleColLessEqualDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 2) -> 12:double) -> boolean, FilterTimestampColLessTimestampColumn(col 9, col 8) -> boolean) -> boolean, FilterDoubleColLessDoubleColumn(col 5, col 12)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterExprOrExpr(children: FilterLongColGreaterLongScalar(col 3, val -257) -> boolean, FilterDoubleColLessDoubleColumn(col 4, col 12)(children: CastLongToFloatViaLongToDouble(col 2) -> 12:double) -> boolean) -> boolean) -> boolean
                     predicate: ((UDFToLong(ctinyint) <= cbigint) and ((UDFToDouble(cint) <= cdouble) or (ctimestamp2 < ctimestamp1)) and (cdouble < UDFToDouble(ctinyint)) and ((cbigint > -257) or (cfloat < UDFToFloat(cint)))) (type: boolean)
                     Statistics: Num rows: 606 Data size: 18603 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: ctimestamp1 (type: timestamp), cfloat (type: float), cstring1 (type: string), cboolean1 (type: boolean), cdouble (type: double), (- (-26.28 + cdouble)) (type: double)
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [8, 4, 6, 10, 5, 13]
+                          selectExpressions: DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -26.28, col 5) -> 12:double) -> 13:double
                       Statistics: Num rows: 606 Data size: 18603 Basic stats: COMPLETE Column stats: NONE
                       Group By Operator
                         aggregations: stddev_samp(_col5), max(_col1), stddev_pop(_col1), count(_col1), var_pop(_col1), var_samp(_col1)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFStdSampDouble(col 13) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 4) -> bigint, VectorUDAFVarPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 6, col 4, col 5, col 8, col 10
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                         keys: _col2 (type: string), _col1 (type: float), _col4 (type: double), _col0 (type: timestamp), _col3 (type: boolean)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -102,26 +123,57 @@ STAGE PLANS:
                           key expressions: _col0 (type: string), _col1 (type: float), _col2 (type: double), _col3 (type: timestamp), _col4 (type: boolean)
                           sort order: +++++
                           Map-reduce partition columns: _col0 (type: string), _col1 (type: float), _col2 (type: double), _col3 (type: timestamp), _col4 (type: boolean)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              keyColumns: [0, 1, 2, 3, 4]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              partitionColumns: [0, 1, 2, 3, 4]
+                              valueColumns: [5, 6, 7, 8, 9, 10]
                           Statistics: Num rows: 606 Data size: 18603 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: float), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: bigint), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,variance:double>)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 2, 3, 4, 5, 6, 8, 9, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, double
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                reduceColumnNullOrder: aaaaa
+                reduceColumnSortOrder: +++++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 11
+                    dataColumns: KEY._col0:string, KEY._col1:float, KEY._col2:double, KEY._col3:timestamp, KEY._col4:boolean, VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:float, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:bigint, VALUE._col4:struct<count:bigint,sum:double,variance:double>, VALUE._col5:struct<count:bigint,sum:double,variance:double>
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev_samp(VALUE._col0), max(VALUE._col1), stddev_pop(VALUE._col2), count(VALUE._col3), var_pop(VALUE._col4), var_samp(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFStdSampFinal(col 5) -> double, VectorUDAFMaxDouble(col 6) -> float, VectorUDAFStdPopFinal(col 7) -> double, VectorUDAFCountMerge(col 8) -> bigint, VectorUDAFVarPopFinal(col 9) -> double, VectorUDAFVarSampFinal(col 10) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2, col 3, col 4
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 keys: KEY._col0 (type: string), KEY._col1 (type: float), KEY._col2 (type: double), KEY._col3 (type: timestamp), KEY._col4 (type: boolean)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -129,10 +181,21 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col3 (type: timestamp), _col1 (type: float), _col0 (type: string), _col4 (type: boolean), _col2 (type: double), (-26.28 + _col2) (type: double), (- (-26.28 + _col2)) (type: double), _col5 (type: double), (_col1 * -26.28) (type: float), _col6 (type: float), (- _col1) (type: float), (- _col6) (type: float), ((- (-26.28 + _col2)) / 10.175) (type: double), _col7 (type: double), _col8 (type: bigint), (- ((- (-26.28 + _col2)) / 10.175)) (type: double), (-1.389 % _col5) (type: double), (UDFToDouble(_col1) - _col2) (type: double), _col9 (type: double), (_col9 % 10.175) (type: double), _col10 (type: double), (- (UDFToDouble(_col1) - _col2)) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [3, 1, 0, 4, 2, 11, 13, 5, 12, 6, 14, 15, 16, 7, 8, 18, 17, 19, 9, 20, 10, 22]
+                      selectExpressions: DoubleScalarAddDoubleColumn(val -26.28, col 2) -> 11:double, DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -26.28, col 2) -> 12:double) -> 13:double, DoubleColMultiplyDoubleScalar(col 1, val -26.280000686645508) -> 12:double, DoubleColUnaryMinus(col 1) -> 14:double, DoubleColUnaryMinus(col 6) -> 15:double, DoubleColDivideDoubleScalar(col 17, val 10.175)(children: DoubleColUnaryMinus(col 16)(children: DoubleScalarAddDoubleColumn(val -26.28, col 2) -> 16:double) -> 17:double) -> 16:double, DoubleColUnaryMinus(col 17)(children: DoubleColDivideDoubleScalar(col 18, val 10.175)(children: DoubleColUnaryMinus(col 17)(children: DoubleScalarAddDoubleColumn(val -26.28, col 2) -> 17:double) -> 18:double) -> 17:double) -> 18:double, DoubleScalarModuloDoubleColumn(val -1.389, col 5) -> 17:double, DoubleColSubtractDoubleColumn(col 1, col 2)(children: col 1) -> 19:double, DoubleColModuloDoubleScalar(col 9, val 10.175) -> 20:double, DoubleColUnaryMinus(col 21)(children: DoubleColSubtractDoubleColumn(col 1, col 2)(children: col 1) -> 21:double) -> 22:double
                   Statistics: Num rows: 303 Data size: 9301 Basic stats: COMPLETE Column stats: NONE
                   Reduce Output Operator
                     key expressions: _col2 (type: string), _col1 (type: float), _col4 (type: double), _col0 (type: timestamp)
                     sort order: ++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        keyColumns: [0, 1, 2, 3]
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        valueColumns: [4, 11, 13, 5, 12, 6, 14, 15, 16, 7, 8, 18, 17, 19, 9, 20, 10, 22]
                     Statistics: Num rows: 303 Data size: 9301 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col3 (type: boolean), _col5 (type: double), _col6 (type: double), _col7 (type: double), _col8 (type: float), _col9 (type: float), _col10 (type: float), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: bigint), _col15 (type: double), _col16 (type: double), _col17 (type: double), _col18 (type: double), _col19 (type: double), _col20 (type: double), _col21 (type: double)
         Reducer 3 
@@ -140,17 +203,30 @@ STAGE PLANS:
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaa
+                reduceColumnSortOrder: ++++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 22
+                    dataColumns: KEY.reducesinkkey0:string, KEY.reducesinkkey1:float, KEY.reducesinkkey2:double, KEY.reducesinkkey3:timestamp, VALUE._col0:boolean, VALUE._col1:double, VALUE._col2:double, VALUE._col3:double, VALUE._col4:float, VALUE._col5:float, VALUE._col6:float, VALUE._col7:float, VALUE._col8:double, VALUE._col9:double, VALUE._col10:bigint, VALUE._col11:double, VALUE._col12:double, VALUE._col13:double, VALUE._col14:double, VALUE._col15:double, VALUE._col16:double, VALUE._col17:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey3 (type: timestamp), KEY.reducesinkkey1 (type: float), KEY.reducesinkkey0 (type: string), VALUE._col0 (type: boolean), KEY.reducesinkkey2 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: double), VALUE._col3 (type: double), VALUE._col4 (type: float), VALUE._col5 (type: float), VALUE._col6 (type: float), VALUE._col7 (type: float), VALUE._col8 (type: double), VALUE._col9 (type: double), VALUE._col10 (type: bigint), VALUE._col11 (type: double), VALUE._col12 (type: double), VALUE._col13 (type: double), VALUE._col14 (type: double), VALUE._col15 (type: double), VALUE._col16 (type: double), VALUE._col17 (type: double)
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [3, 1, 0, 4, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
                 Statistics: Num rows: 303 Data size: 9301 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 303 Data size: 9301 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_15.q.out b/ql/src/test/results/clientpositive/spark/vectorization_15.q.out
index b2d8aaffde..cc9ae1d813 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_15.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_15.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cboolean1,
          cdouble,
@@ -29,7 +29,7 @@ WHERE    (((cstring2 LIKE '%ss%')
 GROUP BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1
 ORDER BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cboolean1,
          cdouble,
@@ -81,15 +81,35 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterStringColLikeStringScalar(col 7, pattern %ss%) -> boolean, FilterStringColLikeStringScalar(col 6, pattern 10%) -> boolean, FilterExprAndExpr(children: FilterLongColGreaterEqualLongScalar(col 2, val -75) -> boolean, FilterLongColEqualLongColumn(col 0, col 1)(children: col 0) -> boolean, FilterDoubleColGreaterEqualDoubleScalar(col 5, val -3728.0) -> boolean) -> boolean) -> boolean
                     predicate: ((cstring2 like '%ss%') or (cstring1 like '10%') or ((cint >= -75) and (UDFToShort(ctinyint) = csmallint) and (cdouble >= -3728.0))) (type: boolean)
                     Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: ctinyint (type: tinyint), cint (type: int), cfloat (type: float), cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp), cboolean1 (type: boolean)
                       outputColumnNames: ctinyint, cint, cfloat, cdouble, cstring1, ctimestamp1, cboolean1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 2, 4, 5, 6, 8, 10]
                       Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
                       Group By Operator
                         aggregations: stddev_samp(cfloat), min(cdouble), stddev_samp(ctinyint), var_pop(ctinyint), var_samp(cint), stddev_pop(cint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFStdSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinDouble(col 5) -> double, VectorUDAFStdSampLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 4, col 10, col 5, col 6, col 0, col 2, col 8
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                         keys: cfloat (type: float), cboolean1 (type: boolean), cdouble (type: double), cstring1 (type: string), ctinyint (type: tinyint), cint (type: int), ctimestamp1 (type: timestamp)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
@@ -98,26 +118,43 @@ STAGE PLANS:
                           key expressions: _col0 (type: float), _col1 (type: boolean), _col2 (type: double), _col3 (type: string), _col4 (type: tinyint), _col5 (type: int), _col6 (type: timestamp)
                           sort order: +++++++
                           Map-reduce partition columns: _col0 (type: float), _col1 (type: boolean), _col2 (type: double), _col3 (type: string), _col4 (type: tinyint), _col5 (type: int), _col6 (type: timestamp)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              keyColumns: [0, 1, 2, 3, 4, 5, 6]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              partitionColumns: [0, 1, 2, 3, 4, 5, 6]
+                              valueColumns: [7, 8, 9, 10, 11, 12]
                           Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: double), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,variance:double>), _col11 (type: struct<count:bigint,sum:double,variance:double>), _col12 (type: struct<count:bigint,sum:double,variance:double>)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 4, 5, 6, 7, 8, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
             Reduce Vectorization:
-                enabled: true
-                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                enabled: false
+                enableConditionsMet: hive.execution.engine spark IN [tez, spark] IS true
+                enableConditionsNotMet: hive.vectorized.execution.reduce.enabled IS false
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev_samp(VALUE._col0), min(VALUE._col1), stddev_samp(VALUE._col2), var_pop(VALUE._col3), var_samp(VALUE._col4), stddev_pop(VALUE._col5)
+                Group By Vectorization:
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: false
+                    native: false
+                    vectorProcessingMode: NONE
+                    projectedOutputColumns: null
                 keys: KEY._col0 (type: float), KEY._col1 (type: boolean), KEY._col2 (type: double), KEY._col3 (type: string), KEY._col4 (type: tinyint), KEY._col5 (type: int), KEY._col6 (type: timestamp)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
@@ -132,14 +169,10 @@ STAGE PLANS:
                     Statistics: Num rows: 6144 Data size: 188618 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col7 (type: double), _col8 (type: decimal(13,2)), _col9 (type: double), _col10 (type: double), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: double), _col15 (type: tinyint), _col16 (type: double), _col17 (type: float), _col18 (type: int), _col19 (type: decimal(13,2)), _col20 (type: double)
         Reducer 3 
-            Execution mode: vectorized
             Reduce Vectorization:
-                enabled: true
-                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                groupByVectorOutput: true
-                allNative: false
-                usesVectorUDFAdaptor: false
-                vectorized: true
+                enabled: false
+                enableConditionsMet: hive.execution.engine spark IN [tez, spark] IS true
+                enableConditionsNotMet: hive.vectorized.execution.reduce.enabled IS false
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: float), KEY.reducesinkkey1 (type: boolean), KEY.reducesinkkey2 (type: double), KEY.reducesinkkey3 (type: string), KEY.reducesinkkey4 (type: tinyint), KEY.reducesinkkey5 (type: int), KEY.reducesinkkey6 (type: timestamp), VALUE._col0 (type: double), VALUE._col1 (type: decimal(13,2)), VALUE._col2 (type: double), VALUE._col3 (type: double), VALUE._col4 (type: float), VALUE._col5 (type: double), VALUE._col6 (type: double), VALUE._col7 (type: double), VALUE._col8 (type: tinyint), VALUE._col9 (type: double), VALUE._col10 (type: float), VALUE._col11 (type: int), VALUE._col12 (type: decimal(13,2)), VALUE._col13 (type: double)
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_16.q.out b/ql/src/test/results/clientpositive/spark/vectorization_16.q.out
index e731c2d9a8..d5235aa30f 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_16.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_16.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -18,7 +18,7 @@ WHERE    ((cstring2 LIKE '%b%')
               OR (cstring1 < 'a')))
 GROUP BY cstring1, cdouble, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -58,15 +58,35 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterStringColLikeStringScalar(col 7, pattern %b%) -> boolean, FilterExprOrExpr(children: FilterDoubleColGreaterEqualDoubleScalar(col 5, val -1.389) -> boolean, FilterStringGroupColLessStringScalar(col 6, val a) -> boolean) -> boolean) -> boolean
                     predicate: ((cstring2 like '%b%') and ((cdouble >= -1.389) or (cstring1 < 'a'))) (type: boolean)
                     Statistics: Num rows: 4096 Data size: 125745 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                       outputColumnNames: cdouble, cstring1, ctimestamp1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [5, 6, 8]
                       Statistics: Num rows: 4096 Data size: 125745 Basic stats: COMPLETE Column stats: NONE
                       Group By Operator
                         aggregations: count(cdouble), stddev_samp(cdouble), min(cdouble)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFCount(col 5) -> bigint, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinDouble(col 5) -> double
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 5, col 6, col 8
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2]
                         keys: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -75,26 +95,56 @@ STAGE PLANS:
                           key expressions: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
                           sort order: +++
                           Map-reduce partition columns: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              keyColumns: [0, 1, 2]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              partitionColumns: [0, 1, 2]
+                              valueColumns: [3, 4, 5]
                           Statistics: Num rows: 4096 Data size: 125745 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: double)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [5, 6, 7, 8]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col1] not supported
-                vectorized: false
+                reduceColumnNullOrder: aaa
+                reduceColumnSortOrder: +++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: KEY._col0:double, KEY._col1:string, KEY._col2:timestamp, VALUE._col0:bigint, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), stddev_samp(VALUE._col1), min(VALUE._col2)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 3) -> bigint, VectorUDAFStdSampFinal(col 4) -> double, VectorUDAFMinDouble(col 5) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2]
                 keys: KEY._col0 (type: double), KEY._col1 (type: string), KEY._col2 (type: timestamp)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -102,9 +152,17 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col1 (type: string), _col0 (type: double), _col2 (type: timestamp), (_col0 - 9763215.5639) (type: double), (- (_col0 - 9763215.5639)) (type: double), _col3 (type: bigint), _col4 (type: double), (- _col4) (type: double), (_col4 * UDFToDouble(_col3)) (type: double), _col5 (type: double), (9763215.5639 / _col0) (type: double), (CAST( _col3 AS decimal(19,0)) / -1.389) (type: decimal(28,6)), _col4 (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [1, 0, 2, 6, 8, 3, 4, 7, 10, 5, 9, 12, 4]
+                      selectExpressions: DoubleColSubtractDoubleScalar(col 0, val 9763215.5639) -> 6:double, DoubleColUnaryMinus(col 7)(children: DoubleColSubtractDoubleScalar(col 0, val 9763215.5639) -> 7:double) -> 8:double, DoubleColUnaryMinus(col 4) -> 7:double, DoubleColMultiplyDoubleColumn(col 4, col 9)(children: CastLongToDouble(col 3) -> 9:double) -> 10:double, DoubleScalarDivideDoubleColumn(val 9763215.5639, col 0) -> 9:double, DecimalColDivideDecimalScalar(col 11, val -1.389)(children: CastLongToDecimal(col 3) -> 11:decimal(19,0)) -> 12:decimal(28,6)
                   Statistics: Num rows: 2048 Data size: 62872 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 2048 Data size: 62872 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_17.q.out b/ql/src/test/results/clientpositive/spark/vectorization_17.q.out
index 32d1c0b9a3..9395a01eb4 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_17.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_17.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cstring1,
          cint,
@@ -22,7 +22,7 @@ WHERE    (((cbigint > -23)
                   OR (cfloat = cdouble))))
 ORDER BY cbigint, cfloat
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cstring1,
          cint,
@@ -66,16 +66,34 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterLongColGreaterLongScalar(col 3, val -23) -> boolean, FilterExprOrExpr(children: FilterDoubleColNotEqualDoubleScalar(col 5, val 988888.0) -> boolean, FilterDecimalColGreaterDecimalScalar(col 12, val -863.257)(children: CastLongToDecimal(col 2) -> 12:decimal(13,3)) -> boolean) -> boolean, FilterExprOrExpr(children: FilterLongColGreaterEqualLongScalar(col 0, val 33) -> boolean, FilterLongColGreaterEqualLongColumn(col 1, col 3)(children: col 1) -> boolean, FilterDoubleColEqualDoubleColumn(col 4, col 5)(children: col 4) -> boolean) -> boolean) -> boolean
                     predicate: ((cbigint > -23) and ((cdouble <> 988888.0) or (CAST( cint AS decimal(13,3)) > -863.257)) and ((ctinyint >= 33) or (UDFToLong(csmallint) >= cbigint) or (UDFToDouble(cfloat) = cdouble))) (type: boolean)
                     Statistics: Num rows: 4778 Data size: 146682 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: cfloat (type: float), cstring1 (type: string), cint (type: int), ctimestamp1 (type: timestamp), cdouble (type: double), cbigint (type: bigint), (UDFToDouble(cfloat) / UDFToDouble(ctinyint)) (type: double), (UDFToLong(cint) % cbigint) (type: bigint), (- cdouble) (type: double), (cdouble + (UDFToDouble(cfloat) / UDFToDouble(ctinyint))) (type: double), (cdouble / UDFToDouble(cint)) (type: double), (- (- cdouble)) (type: double), (9763215.5639 % CAST( cbigint AS decimal(19,0))) (type: decimal(11,4)), (2563.58 + (- (- cdouble))) (type: double)
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [4, 6, 2, 8, 5, 3, 14, 15, 13, 16, 18, 19, 21, 17]
+                          selectExpressions: DoubleColDivideDoubleColumn(col 4, col 13)(children: col 4, CastLongToDouble(col 0) -> 13:double) -> 14:double, LongColModuloLongColumn(col 2, col 3)(children: col 2) -> 15:long, DoubleColUnaryMinus(col 5) -> 13:double, DoubleColAddDoubleColumn(col 5, col 17)(children: DoubleColDivideDoubleColumn(col 4, col 16)(children: col 4, CastLongToDouble(col 0) -> 16:double) -> 17:double) -> 16:double, DoubleColDivideDoubleColumn(col 5, col 17)(children: CastLongToDouble(col 2) -> 17:double) -> 18:double, DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 5) -> 17:double) -> 19:double, DecimalScalarModuloDecimalColumn(val 9763215.5639, col 20)(children: CastLongToDecimal(col 3) -> 20:decimal(19,0)) -> 21:decimal(11,4), DoubleScalarAddDoubleColumn(val 2563.58, col 22)(children: DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 5) -> 17:double) -> 22:double) -> 17:double
                       Statistics: Num rows: 4778 Data size: 146682 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         key expressions: _col5 (type: bigint), _col0 (type: float)
                         sort order: ++
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            keyColumns: [3, 4]
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: [6, 2, 8, 5, 14, 15, 13, 16, 18, 19, 21, 17]
                         Statistics: Num rows: 4778 Data size: 146682 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col1 (type: string), _col2 (type: int), _col3 (type: timestamp), _col4 (type: double), _col6 (type: double), _col7 (type: bigint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: double), _col12 (type: decimal(11,4)), _col13 (type: double)
             Execution mode: vectorized
@@ -87,22 +105,41 @@ STAGE PLANS:
                 allNative: true
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 6, 8]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: decimal(13,3), double, double, bigint, double, double, double, double, decimal(19,0), decimal(11,4), double
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: aa
+                reduceColumnSortOrder: ++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 14
+                    dataColumns: KEY.reducesinkkey0:bigint, KEY.reducesinkkey1:float, VALUE._col0:string, VALUE._col1:int, VALUE._col2:timestamp, VALUE._col3:double, VALUE._col4:double, VALUE._col5:bigint, VALUE._col6:double, VALUE._col7:double, VALUE._col8:double, VALUE._col9:double, VALUE._col10:decimal(11,4), VALUE._col11:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey1 (type: float), VALUE._col0 (type: string), VALUE._col1 (type: int), VALUE._col2 (type: timestamp), VALUE._col3 (type: double), KEY.reducesinkkey0 (type: bigint), VALUE._col4 (type: double), VALUE._col5 (type: bigint), VALUE._col6 (type: double), VALUE._col7 (type: double), VALUE._col8 (type: double), VALUE._col9 (type: double), VALUE._col10 (type: decimal(11,4)), VALUE._col11 (type: double)
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [1, 2, 3, 4, 5, 0, 6, 7, 8, 9, 10, 11, 12, 13]
                 Statistics: Num rows: 4778 Data size: 146682 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 4778 Data size: 146682 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_2.q.out b/ql/src/test/results/clientpositive/spark/vectorization_2.q.out
index 709a75f2ab..a3c70bb5b1 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_2.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_2.q.out
@@ -1,3 +1,180 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(csmallint),
+       (AVG(csmallint) % -563),
+       (AVG(csmallint) + 762),
+       SUM(cfloat),
+       VAR_POP(cbigint),
+       (-(VAR_POP(cbigint))),
+       (SUM(cfloat) - AVG(csmallint)),
+       COUNT(*),
+       (-((SUM(cfloat) - AVG(csmallint)))),
+       (VAR_POP(cbigint) - 762),
+       MIN(ctinyint),
+       ((-(VAR_POP(cbigint))) + MIN(ctinyint)),
+       AVG(cdouble),
+       (((-(VAR_POP(cbigint))) + MIN(ctinyint)) - SUM(cfloat))
+FROM   alltypesorc
+WHERE  (((ctimestamp1 < ctimestamp2)
+         AND ((cstring2 LIKE 'b%')
+              AND (cfloat <= -5638.15)))
+        OR ((cdouble < ctinyint)
+            AND ((-10669 != ctimestamp2)
+                 OR (359 > cint))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(csmallint),
+       (AVG(csmallint) % -563),
+       (AVG(csmallint) + 762),
+       SUM(cfloat),
+       VAR_POP(cbigint),
+       (-(VAR_POP(cbigint))),
+       (SUM(cfloat) - AVG(csmallint)),
+       COUNT(*),
+       (-((SUM(cfloat) - AVG(csmallint)))),
+       (VAR_POP(cbigint) - 762),
+       MIN(ctinyint),
+       ((-(VAR_POP(cbigint))) + MIN(ctinyint)),
+       AVG(cdouble),
+       (((-(VAR_POP(cbigint))) + MIN(ctinyint)) - SUM(cfloat))
+FROM   alltypesorc
+WHERE  (((ctimestamp1 < ctimestamp2)
+         AND ((cstring2 LIKE 'b%')
+              AND (cfloat <= -5638.15)))
+        OR ((cdouble < ctinyint)
+            AND ((-10669 != ctimestamp2)
+                 OR (359 > cint))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+      Edges:
+        Reducer 2 <- Map 1 (GROUP, 1)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: FilterTimestampColLessTimestampColumn(col 8, col 9) -> boolean, FilterStringColLikeStringScalar(col 7, pattern b%) -> boolean, FilterDoubleColLessEqualDoubleScalar(col 4, val -5638.14990234375) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColLessDoubleColumn(col 5, col 12)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterExprOrExpr(children: FilterDoubleScalarNotEqualDoubleColumn(val -10669.0, col 12)(children: CastTimestampToDouble(col 9) -> 12:double) -> boolean, FilterLongScalarGreaterLongColumn(val 359, col 2) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: (((ctimestamp1 < ctimestamp2) and (cstring2 like 'b%') and (cfloat <= -5638.15)) or ((cdouble < UDFToDouble(ctinyint)) and ((-10669.0 <> UDFToDouble(ctimestamp2)) or (359 > cint)))) (type: boolean)
+                    Statistics: Num rows: 4778 Data size: 146682 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), csmallint (type: smallint), cbigint (type: bigint), cfloat (type: float), cdouble (type: double)
+                      outputColumnNames: ctinyint, csmallint, cbigint, cfloat, cdouble
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1, 3, 4, 5]
+                      Statistics: Num rows: 4778 Data size: 146682 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: avg(csmallint), sum(cfloat), var_pop(cbigint), count(), min(ctinyint), avg(cdouble)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFSumDouble(col 4) -> double, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                        Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4, 5]
+                          Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col0 (type: struct<count:bigint,sum:double,input:smallint>), _col1 (type: double), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: bigint), _col4 (type: tinyint), _col5 (type: struct<count:bigint,sum:double,input:double>)
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 7, 8, 9]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,input:smallint>, VALUE._col1:double, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:bigint, VALUE._col4:tinyint, VALUE._col5:struct<count:bigint,sum:double,input:double>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: avg(VALUE._col0), sum(VALUE._col1), var_pop(VALUE._col2), count(VALUE._col3), min(VALUE._col4), avg(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFSumDouble(col 1) -> double, VectorUDAFVarPopFinal(col 2) -> double, VectorUDAFCountMerge(col 3) -> bigint, VectorUDAFMinLong(col 4) -> tinyint, VectorUDAFAvgFinal(col 5) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: double), (_col0 % -563.0) (type: double), (_col0 + 762.0) (type: double), _col1 (type: double), _col2 (type: double), (- _col2) (type: double), (_col1 - _col0) (type: double), _col3 (type: bigint), (- (_col1 - _col0)) (type: double), (_col2 - 762.0) (type: double), _col4 (type: tinyint), ((- _col2) + UDFToDouble(_col4)) (type: double), _col5 (type: double), (((- _col2) + UDFToDouble(_col4)) - _col1) (type: double)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 7, 1, 2, 8, 9, 3, 11, 10, 4, 14, 5, 12]
+                      selectExpressions: DoubleColModuloDoubleScalar(col 0, val -563.0) -> 6:double, DoubleColAddDoubleScalar(col 0, val 762.0) -> 7:double, DoubleColUnaryMinus(col 2) -> 8:double, DoubleColSubtractDoubleColumn(col 1, col 0) -> 9:double, DoubleColUnaryMinus(col 10)(children: DoubleColSubtractDoubleColumn(col 1, col 0) -> 10:double) -> 11:double, DoubleColSubtractDoubleScalar(col 2, val 762.0) -> 10:double, DoubleColAddDoubleColumn(col 12, col 13)(children: DoubleColUnaryMinus(col 2) -> 12:double, CastLongToDouble(col 4) -> 13:double) -> 14:double, DoubleColSubtractDoubleColumn(col 15, col 1)(children: DoubleColAddDoubleColumn(col 12, col 13)(children: DoubleColUnaryMinus(col 2) -> 12:double, CastLongToDouble(col 4) -> 13:double) -> 15:double) -> 12:double
+                  Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT AVG(csmallint),
        (AVG(csmallint) % -563),
        (AVG(csmallint) + 762),
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_3.q.out b/ql/src/test/results/clientpositive/spark/vectorization_3.q.out
index 2398dee7bc..a335c7d441 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_3.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_3.q.out
@@ -1,3 +1,185 @@
+WARNING: Comparing a bigint and a double may result in a loss of precision.
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT STDDEV_SAMP(csmallint),
+       (STDDEV_SAMP(csmallint) - 10.175),
+       STDDEV_POP(ctinyint),
+       (STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)),
+       (-(STDDEV_POP(ctinyint))),
+       (STDDEV_SAMP(csmallint) % 79.553),
+       (-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))),
+       STDDEV_SAMP(cfloat),
+       (-(STDDEV_SAMP(csmallint))),
+       SUM(cfloat),
+       ((-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))) / (STDDEV_SAMP(csmallint) - 10.175)),
+       (-((STDDEV_SAMP(csmallint) - 10.175))),
+       AVG(cint),
+       (-3728 - STDDEV_SAMP(csmallint)),
+       STDDEV_POP(cint),
+       (AVG(cint) / STDDEV_SAMP(cfloat))
+FROM   alltypesorc
+WHERE  (((cint <= cfloat)
+         AND ((79.553 != cbigint)
+              AND (ctimestamp2 = -29071)))
+        OR ((cbigint > cdouble)
+            AND ((79.553 <= csmallint)
+                 AND (ctimestamp1 > ctimestamp2))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT STDDEV_SAMP(csmallint),
+       (STDDEV_SAMP(csmallint) - 10.175),
+       STDDEV_POP(ctinyint),
+       (STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)),
+       (-(STDDEV_POP(ctinyint))),
+       (STDDEV_SAMP(csmallint) % 79.553),
+       (-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))),
+       STDDEV_SAMP(cfloat),
+       (-(STDDEV_SAMP(csmallint))),
+       SUM(cfloat),
+       ((-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))) / (STDDEV_SAMP(csmallint) - 10.175)),
+       (-((STDDEV_SAMP(csmallint) - 10.175))),
+       AVG(cint),
+       (-3728 - STDDEV_SAMP(csmallint)),
+       STDDEV_POP(cint),
+       (AVG(cint) / STDDEV_SAMP(cfloat))
+FROM   alltypesorc
+WHERE  (((cint <= cfloat)
+         AND ((79.553 != cbigint)
+              AND (ctimestamp2 = -29071)))
+        OR ((cbigint > cdouble)
+            AND ((79.553 <= csmallint)
+                 AND (ctimestamp1 > ctimestamp2))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+      Edges:
+        Reducer 2 <- Map 1 (GROUP, 1)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: FilterDoubleColLessEqualDoubleColumn(col 12, col 4)(children: CastLongToFloatViaLongToDouble(col 2) -> 12:double) -> boolean, FilterDecimalScalarNotEqualDecimalColumn(val 79.553, col 13)(children: CastLongToDecimal(col 3) -> 13:decimal(22,3)) -> boolean, FilterDoubleColEqualDoubleScalar(col 12, val -29071.0)(children: CastTimestampToDouble(col 9) -> 12:double) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColGreaterDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 3) -> 12:double) -> boolean, FilterDecimalScalarLessEqualDecimalColumn(val 79.553, col 14)(children: CastLongToDecimal(col 1) -> 14:decimal(8,3)) -> boolean, FilterTimestampColGreaterTimestampColumn(col 8, col 9) -> boolean) -> boolean) -> boolean
+                    predicate: (((UDFToFloat(cint) <= cfloat) and (79.553 <> CAST( cbigint AS decimal(22,3))) and (UDFToDouble(ctimestamp2) = -29071.0)) or ((UDFToDouble(cbigint) > cdouble) and (79.553 <= CAST( csmallint AS decimal(8,3))) and (ctimestamp1 > ctimestamp2))) (type: boolean)
+                    Statistics: Num rows: 2503 Data size: 76841 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), csmallint (type: smallint), cint (type: int), cfloat (type: float)
+                      outputColumnNames: ctinyint, csmallint, cint, cfloat
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1, 2, 4]
+                      Statistics: Num rows: 2503 Data size: 76841 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: stddev_samp(csmallint), stddev_pop(ctinyint), stddev_samp(cfloat), sum(cfloat), avg(cint), stddev_pop(cint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumDouble(col 4) -> double, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                        Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4, 5]
+                          Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: double), _col4 (type: struct<count:bigint,sum:double,input:int>), _col5 (type: struct<count:bigint,sum:double,variance:double>)
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 8, 9]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, decimal(22,3), decimal(8,3)
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:struct<count:bigint,sum:double,variance:double>, VALUE._col3:double, VALUE._col4:struct<count:bigint,sum:double,input:int>, VALUE._col5:struct<count:bigint,sum:double,variance:double>
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: stddev_samp(VALUE._col0), stddev_pop(VALUE._col1), stddev_samp(VALUE._col2), sum(VALUE._col3), avg(VALUE._col4), stddev_pop(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFStdSampFinal(col 0) -> double, VectorUDAFStdPopFinal(col 1) -> double, VectorUDAFStdSampFinal(col 2) -> double, VectorUDAFSumDouble(col 3) -> double, VectorUDAFAvgFinal(col 4) -> double, VectorUDAFStdPopFinal(col 5) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: double), (_col0 - 10.175) (type: double), _col1 (type: double), (_col0 * (_col0 - 10.175)) (type: double), (- _col1) (type: double), (_col0 % 79.553) (type: double), (- (_col0 * (_col0 - 10.175))) (type: double), _col2 (type: double), (- _col0) (type: double), _col3 (type: double), ((- (_col0 * (_col0 - 10.175))) / (_col0 - 10.175)) (type: double), (- (_col0 - 10.175)) (type: double), _col4 (type: double), (-3728.0 - _col0) (type: double), _col5 (type: double), (_col4 / _col2) (type: double)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 1, 8, 7, 9, 10, 2, 11, 3, 14, 13, 4, 12, 5, 15]
+                      selectExpressions: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 6:double, DoubleColMultiplyDoubleColumn(col 0, col 7)(children: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 7:double) -> 8:double, DoubleColUnaryMinus(col 1) -> 7:double, DoubleColModuloDoubleScalar(col 0, val 79.553) -> 9:double, DoubleColUnaryMinus(col 11)(children: DoubleColMultiplyDoubleColumn(col 0, col 10)(children: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 10:double) -> 11:double) -> 10:double, DoubleColUnaryMinus(col 0) -> 11:double, DoubleColDivideDoubleColumn(col 12, col 13)(children: DoubleColUnaryMinus(col 13)(children: DoubleColMultiplyDoubleColumn(col 0, col 12)(children: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 12:double) -> 13:double) -> 12:double, DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 13:double) -> 14:double, DoubleColUnaryMinus(col 12)(children: DoubleColSubtractDoubleScalar(col 0, val 10.175) -> 12:double) -> 13:double, DoubleScalarSubtractDoubleColumn(val -3728.0, col 0) -> 12:double, DoubleColDivideDoubleColumn(col 4, col 2) -> 15:double
+                  Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 WARNING: Comparing a bigint and a double may result in a loss of precision.
 PREHOOK: query: SELECT STDDEV_SAMP(csmallint),
        (STDDEV_SAMP(csmallint) - 10.175),
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_4.q.out b/ql/src/test/results/clientpositive/spark/vectorization_4.q.out
index 0d6829f6d8..3d0e700f3d 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_4.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_4.q.out
@@ -1,3 +1,179 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT SUM(cint),
+       (SUM(cint) * -563),
+       (-3728 + SUM(cint)),
+       STDDEV_POP(cdouble),
+       (-(STDDEV_POP(cdouble))),
+       AVG(cdouble),
+       ((SUM(cint) * -563) % SUM(cint)),
+       (((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)),
+       VAR_POP(cdouble),
+       (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))),
+       ((-3728 + SUM(cint)) - (SUM(cint) * -563)),
+       MIN(ctinyint),
+       MIN(ctinyint),
+       (MIN(ctinyint) * (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))))
+FROM   alltypesorc
+WHERE  (((csmallint >= cint)
+         OR ((-89010 >= ctinyint)
+             AND (cdouble > 79.553)))
+        OR ((-563 != cbigint)
+            AND ((ctinyint != cbigint)
+                 OR (-3728 >= cdouble))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT SUM(cint),
+       (SUM(cint) * -563),
+       (-3728 + SUM(cint)),
+       STDDEV_POP(cdouble),
+       (-(STDDEV_POP(cdouble))),
+       AVG(cdouble),
+       ((SUM(cint) * -563) % SUM(cint)),
+       (((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)),
+       VAR_POP(cdouble),
+       (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))),
+       ((-3728 + SUM(cint)) - (SUM(cint) * -563)),
+       MIN(ctinyint),
+       MIN(ctinyint),
+       (MIN(ctinyint) * (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))))
+FROM   alltypesorc
+WHERE  (((csmallint >= cint)
+         OR ((-89010 >= ctinyint)
+             AND (cdouble > 79.553)))
+        OR ((-563 != cbigint)
+            AND ((ctinyint != cbigint)
+                 OR (-3728 >= cdouble))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+      Edges:
+        Reducer 2 <- Map 1 (GROUP, 1)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterLongColGreaterEqualLongColumn(col 1, col 2)(children: col 1) -> boolean, FilterExprAndExpr(children: FilterLongScalarGreaterEqualLongColumn(val -89010, col 0)(children: col 0) -> boolean, FilterDoubleColGreaterDoubleScalar(col 5, val 79.553) -> boolean) -> boolean, FilterExprAndExpr(children: FilterLongScalarNotEqualLongColumn(val -563, col 3) -> boolean, FilterExprOrExpr(children: FilterLongColNotEqualLongColumn(col 0, col 3)(children: col 0) -> boolean, FilterDoubleScalarGreaterEqualDoubleColumn(val -3728.0, col 5) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: ((UDFToInteger(csmallint) >= cint) or ((-89010 >= UDFToInteger(ctinyint)) and (cdouble > 79.553)) or ((-563 <> cbigint) and ((UDFToLong(ctinyint) <> cbigint) or (-3728.0 >= cdouble)))) (type: boolean)
+                    Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), cint (type: int), cdouble (type: double)
+                      outputColumnNames: ctinyint, cint, cdouble
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 2, 5]
+                      Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: sum(cint), stddev_pop(cdouble), avg(cdouble), var_pop(cdouble), min(ctinyint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFSumLong(col 2) -> bigint, VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>, VectorUDAFVarPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinLong(col 0) -> tinyint
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                        Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4]
+                          Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col0 (type: bigint), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,input:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: tinyint)
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 5]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    dataColumns: VALUE._col0:bigint, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:struct<count:bigint,sum:double,input:double>, VALUE._col3:struct<count:bigint,sum:double,variance:double>, VALUE._col4:tinyint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0), stddev_pop(VALUE._col1), avg(VALUE._col2), var_pop(VALUE._col3), min(VALUE._col4)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFStdPopFinal(col 1) -> double, VectorUDAFAvgFinal(col 2) -> double, VectorUDAFVarPopFinal(col 3) -> double, VectorUDAFMinLong(col 4) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: bigint), (_col0 * -563) (type: bigint), (-3728 + _col0) (type: bigint), _col1 (type: double), (- _col1) (type: double), _col2 (type: double), ((_col0 * -563) % _col0) (type: bigint), (UDFToDouble(((_col0 * -563) % _col0)) / _col2) (type: double), _col3 (type: double), (- (UDFToDouble(((_col0 * -563) % _col0)) / _col2)) (type: double), ((-3728 + _col0) - (_col0 * -563)) (type: bigint), _col4 (type: tinyint), _col4 (type: tinyint), (UDFToDouble(_col4) * (- (UDFToDouble(((_col0 * -563) % _col0)) / _col2))) (type: double)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 5, 6, 1, 7, 2, 9, 12, 3, 11, 14, 4, 4, 16]
+                      selectExpressions: LongColMultiplyLongScalar(col 0, val -563) -> 5:long, LongScalarAddLongColumn(val -3728, col 0) -> 6:long, DoubleColUnaryMinus(col 1) -> 7:double, LongColModuloLongColumn(col 8, col 0)(children: LongColMultiplyLongScalar(col 0, val -563) -> 8:long) -> 9:long, DoubleColDivideDoubleColumn(col 11, col 2)(children: CastLongToDouble(col 10)(children: LongColModuloLongColumn(col 8, col 0)(children: LongColMultiplyLongScalar(col 0, val -563) -> 8:long) -> 10:long) -> 11:double) -> 12:double, DoubleColUnaryMinus(col 13)(children: DoubleColDivideDoubleColumn(col 11, col 2)(children: CastLongToDouble(col 10)(children: LongColModuloLongColumn(col 8, col 0)(children: LongColMultiplyLongScalar(col 0, val -563) -> 8:long) -> 10:long) -> 11:double) -> 13:double) -> 11:double, LongColSubtractLongColumn(col 8, col 10)(children: LongScalarAddLongColumn(val -3728, col 0) -> 8:long, LongColMultiplyLongScalar(col 0, val -563) -> 10:long) -> 14:long, DoubleColMultiplyDoubleColumn(col 13, col 15)(children: CastLongToDouble(col 4) -> 13:double, DoubleColUnaryMinus(col 16)(children: DoubleColDivideDoubleColumn(col 15, col 2)(children: CastLongToDouble(col 10)(children: LongColModuloLongColumn(col 8, col 0)(children: LongColMultiplyLongScalar(col 0, val -563) -> 8:long) -> 10:long) -> 15:double) -> 16:double) -> 15:double) -> 16:double
+                  Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT SUM(cint),
        (SUM(cint) * -563),
        (-3728 + SUM(cint)),
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_5.q.out b/ql/src/test/results/clientpositive/spark/vectorization_5.q.out
index 914a626872..2737d9bc94 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_5.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_5.q.out
@@ -1,3 +1,174 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT MAX(csmallint),
+       (MAX(csmallint) * -75),
+       COUNT(*),
+       ((MAX(csmallint) * -75) / COUNT(*)),
+       (6981 * MAX(csmallint)),
+       MIN(csmallint),
+       (-(MIN(csmallint))),
+       (197 % ((MAX(csmallint) * -75) / COUNT(*))),
+       SUM(cint),
+       MAX(ctinyint),
+       (-(MAX(ctinyint))),
+       ((-(MAX(ctinyint))) + MAX(ctinyint))
+FROM   alltypesorc
+WHERE  (((cboolean2 IS NOT NULL)
+         AND (cstring1 LIKE '%b%'))
+        OR ((ctinyint = cdouble)
+            AND ((ctimestamp2 IS NOT NULL)
+                 AND (cstring2 LIKE 'a'))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT MAX(csmallint),
+       (MAX(csmallint) * -75),
+       COUNT(*),
+       ((MAX(csmallint) * -75) / COUNT(*)),
+       (6981 * MAX(csmallint)),
+       MIN(csmallint),
+       (-(MIN(csmallint))),
+       (197 % ((MAX(csmallint) * -75) / COUNT(*))),
+       SUM(cint),
+       MAX(ctinyint),
+       (-(MAX(ctinyint))),
+       ((-(MAX(ctinyint))) + MAX(ctinyint))
+FROM   alltypesorc
+WHERE  (((cboolean2 IS NOT NULL)
+         AND (cstring1 LIKE '%b%'))
+        OR ((ctinyint = cdouble)
+            AND ((ctimestamp2 IS NOT NULL)
+                 AND (cstring2 LIKE 'a'))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+      Edges:
+        Reducer 2 <- Map 1 (GROUP, 1)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: SelectColumnIsNotNull(col 11) -> boolean, FilterStringColLikeStringScalar(col 6, pattern %b%) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColEqualDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, SelectColumnIsNotNull(col 9) -> boolean, FilterStringColLikeStringScalar(col 7, pattern a) -> boolean) -> boolean) -> boolean
+                    predicate: ((cboolean2 is not null and (cstring1 like '%b%')) or ((UDFToDouble(ctinyint) = cdouble) and ctimestamp2 is not null and (cstring2 like 'a'))) (type: boolean)
+                    Statistics: Num rows: 9216 Data size: 282927 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ctinyint (type: tinyint), csmallint (type: smallint), cint (type: int)
+                      outputColumnNames: ctinyint, csmallint, cint
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [0, 1, 2]
+                      Statistics: Num rows: 9216 Data size: 282927 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: max(csmallint), count(), min(csmallint), sum(cint), max(ctinyint)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFMaxLong(col 1) -> smallint, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 1) -> smallint, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFMaxLong(col 0) -> tinyint
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2, 3, 4]
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                        Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              keyColumns: []
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              valueColumns: [0, 1, 2, 3, 4]
+                          Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col0 (type: smallint), _col1 (type: bigint), _col2 (type: smallint), _col3 (type: bigint), _col4 (type: tinyint)
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 5, 6, 7, 9, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Vectorization:
+                enabled: true
+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: 
+                reduceColumnSortOrder: 
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 5
+                    dataColumns: VALUE._col0:smallint, VALUE._col1:bigint, VALUE._col2:smallint, VALUE._col3:bigint, VALUE._col4:tinyint
+                    partitionColumnCount: 0
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: max(VALUE._col0), count(VALUE._col1), min(VALUE._col2), sum(VALUE._col3), max(VALUE._col4)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxLong(col 0) -> smallint, VectorUDAFCountMerge(col 1) -> bigint, VectorUDAFMinLong(col 2) -> smallint, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFMaxLong(col 4) -> tinyint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4]
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: smallint), (UDFToInteger(_col0) * -75) (type: int), _col1 (type: bigint), (UDFToDouble((UDFToInteger(_col0) * -75)) / UDFToDouble(_col1)) (type: double), (6981 * UDFToInteger(_col0)) (type: int), _col2 (type: smallint), (- _col2) (type: smallint), (197.0 % (UDFToDouble((UDFToInteger(_col0) * -75)) / UDFToDouble(_col1))) (type: double), _col3 (type: bigint), _col4 (type: tinyint), (- _col4) (type: tinyint), ((- _col4) + _col4) (type: tinyint)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 5, 1, 9, 6, 2, 10, 7, 3, 4, 11, 14]
+                      selectExpressions: LongColMultiplyLongScalar(col 0, val -75)(children: col 0) -> 5:long, DoubleColDivideDoubleColumn(col 7, col 8)(children: CastLongToDouble(col 6)(children: LongColMultiplyLongScalar(col 0, val -75)(children: col 0) -> 6:long) -> 7:double, CastLongToDouble(col 1) -> 8:double) -> 9:double, LongScalarMultiplyLongColumn(val 6981, col 0)(children: col 0) -> 6:long, LongColUnaryMinus(col 2) -> 10:long, DoubleScalarModuloDoubleColumn(val 197.0, col 12)(children: DoubleColDivideDoubleColumn(col 7, col 8)(children: CastLongToDouble(col 11)(children: LongColMultiplyLongScalar(col 0, val -75)(children: col 0) -> 11:long) -> 7:double, CastLongToDouble(col 1) -> 8:double) -> 12:double) -> 7:double, LongColUnaryMinus(col 4) -> 11:long, LongColAddLongColumn(col 13, col 4)(children: LongColUnaryMinus(col 4) -> 13:long) -> 14:long
+                  Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
+                    Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT MAX(csmallint),
        (MAX(csmallint) * -75),
        COUNT(*),
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_6.q.out b/ql/src/test/results/clientpositive/spark/vectorization_6.q.out
index 13897f6f93..4906328285 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_6.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_6.q.out
@@ -1,3 +1,114 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cboolean1,
+       cfloat,
+       cstring1,
+       (988888 * csmallint),
+       (-(csmallint)),
+       (-(cfloat)),
+       (-26.28 / cfloat),
+       (cfloat * 359),
+       (cint % ctinyint),
+       (-(cdouble)),
+       (ctinyint - -75),
+       (762 * (cint % ctinyint))
+FROM   alltypesorc
+WHERE  ((ctinyint != 0)
+        AND ((((cboolean1 <= 0)
+          AND (cboolean2 >= cboolean1))
+          OR ((cbigint IS NOT NULL)
+              AND ((cstring2 LIKE '%a')
+                   OR (cfloat <= -257))))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cboolean1,
+       cfloat,
+       cstring1,
+       (988888 * csmallint),
+       (-(csmallint)),
+       (-(cfloat)),
+       (-26.28 / cfloat),
+       (cfloat * 359),
+       (cint % ctinyint),
+       (-(cdouble)),
+       (ctinyint - -75),
+       (762 * (cint % ctinyint))
+FROM   alltypesorc
+WHERE  ((ctinyint != 0)
+        AND ((((cboolean1 <= 0)
+          AND (cboolean2 >= cboolean1))
+          OR ((cbigint IS NOT NULL)
+              AND ((cstring2 LIKE '%a')
+                   OR (cfloat <= -257))))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: alltypesorc
+                  Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+                  Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterLongColNotEqualLongScalar(col 0, val 0) -> boolean, FilterExprOrExpr(children: FilterExprAndExpr(children: FilterLongColLessEqualLongScalar(col 10, val 0) -> boolean, FilterLongColGreaterEqualLongColumn(col 11, col 10) -> boolean) -> boolean, FilterExprAndExpr(children: SelectColumnIsNotNull(col 3) -> boolean, FilterExprOrExpr(children: FilterStringColLikeStringScalar(col 7, pattern %a) -> boolean, FilterDoubleColLessEqualDoubleScalar(col 4, val -257.0) -> boolean) -> boolean) -> boolean) -> boolean) -> boolean
+                    predicate: ((ctinyint <> 0) and (((cboolean1 <= 0) and (cboolean2 >= cboolean1)) or (cbigint is not null and ((cstring2 like '%a') or (cfloat <= -257))))) (type: boolean)
+                    Statistics: Num rows: 11605 Data size: 356269 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: cboolean1 (type: boolean), cfloat (type: float), cstring1 (type: string), (988888 * UDFToInteger(csmallint)) (type: int), (- csmallint) (type: smallint), (- cfloat) (type: float), (-26.28 / UDFToDouble(cfloat)) (type: double), (cfloat * 359.0) (type: float), (cint % UDFToInteger(ctinyint)) (type: int), (- cdouble) (type: double), (UDFToInteger(ctinyint) - -75) (type: int), (762 * (cint % UDFToInteger(ctinyint))) (type: int)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [10, 4, 6, 12, 13, 14, 15, 16, 17, 18, 19, 21]
+                          selectExpressions: LongScalarMultiplyLongColumn(val 988888, col 1)(children: col 1) -> 12:long, LongColUnaryMinus(col 1) -> 13:long, DoubleColUnaryMinus(col 4) -> 14:double, DoubleScalarDivideDoubleColumn(val -26.28, col 4)(children: col 4) -> 15:double, DoubleColMultiplyDoubleScalar(col 4, val 359.0) -> 16:double, LongColModuloLongColumn(col 2, col 0)(children: col 0) -> 17:long, DoubleColUnaryMinus(col 5) -> 18:double, LongColSubtractLongScalar(col 0, val -75)(children: col 0) -> 19:long, LongScalarMultiplyLongColumn(val 762, col 20)(children: LongColModuloLongColumn(col 2, col 0)(children: col 0) -> 20:long) -> 21:long
+                      Statistics: Num rows: 11605 Data size: 356269 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        File Sink Vectorization:
+                            className: VectorFileSinkOperator
+                            native: false
+                        Statistics: Num rows: 11605 Data size: 356269 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized
+            Map Vectorization:
+                enabled: true
+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
+                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 4, 5, 6, 7, 10, 11]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint, bigint, double, double, double, bigint, double, bigint, bigint, bigint
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT cboolean1,
        cfloat,
        cstring1,
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_7.q.out b/ql/src/test/results/clientpositive/spark/vectorization_7.q.out
index d2ff35303b..d49af0cecf 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_7.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_7.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT cboolean1,
        cbigint,
        csmallint,
@@ -25,7 +25,7 @@ WHERE  ((ctinyint != 0)
 ORDER BY cboolean1, cbigint, csmallint, ctinyint, ctimestamp1, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9
 LIMIT 25
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT cboolean1,
        cbigint,
        csmallint,
@@ -96,8 +96,10 @@ STAGE PLANS:
                         sort order: +++++++++++++++
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkObjectHashOperator
+                            keyColumns: [10, 3, 1, 0, 8, 6, 13, 14, 15, 16, 18, 19, 17, 20, 22]
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: []
                         Statistics: Num rows: 7281 Data size: 223523 Basic stats: COMPLETE Column stats: NONE
                         TopN Hash Memory Usage: 0.1
             Execution mode: vectorized
@@ -109,15 +111,27 @@ STAGE PLANS:
                 allNative: true
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, bigint, bigint, bigint, bigint, bigint, bigint, bigint, bigint, bigint, bigint
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaaaaaaaaaaaaa
+                reduceColumnSortOrder: +++++++++++++++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 15
+                    dataColumns: KEY.reducesinkkey0:boolean, KEY.reducesinkkey1:bigint, KEY.reducesinkkey2:smallint, KEY.reducesinkkey3:tinyint, KEY.reducesinkkey4:timestamp, KEY.reducesinkkey5:string, KEY.reducesinkkey6:bigint, KEY.reducesinkkey7:int, KEY.reducesinkkey8:smallint, KEY.reducesinkkey9:tinyint, KEY.reducesinkkey10:int, KEY.reducesinkkey11:bigint, KEY.reducesinkkey12:int, KEY.reducesinkkey13:tinyint, KEY.reducesinkkey14:tinyint
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: boolean), KEY.reducesinkkey1 (type: bigint), KEY.reducesinkkey2 (type: smallint), KEY.reducesinkkey3 (type: tinyint), KEY.reducesinkkey4 (type: timestamp), KEY.reducesinkkey5 (type: string), KEY.reducesinkkey6 (type: bigint), KEY.reducesinkkey7 (type: int), KEY.reducesinkkey8 (type: smallint), KEY.reducesinkkey9 (type: tinyint), KEY.reducesinkkey10 (type: int), KEY.reducesinkkey11 (type: bigint), KEY.reducesinkkey12 (type: int), KEY.reducesinkkey9 (type: tinyint), KEY.reducesinkkey14 (type: tinyint)
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_8.q.out b/ql/src/test/results/clientpositive/spark/vectorization_8.q.out
index 927ee59223..3eed209849 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_8.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_8.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ctimestamp1,
        cdouble,
        cboolean1,
@@ -23,7 +23,7 @@ WHERE  (((cstring2 IS NOT NULL)
 ORDER BY ctimestamp1, cdouble, cboolean1, cstring1, cfloat, c1, c2, c3, c4, c5, c6, c7, c8, c9
 LIMIT 20
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ctimestamp1,
        cdouble,
        cboolean1,
@@ -92,8 +92,10 @@ STAGE PLANS:
                         sort order: ++++++++++++++
                         Reduce Sink Vectorization:
                             className: VectorReduceSinkObjectHashOperator
+                            keyColumns: [8, 5, 10, 6, 4, 12, 13, 14, 16, 18, 15, 17, 19, 21]
                             native: true
                             nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            valueColumns: []
                         Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
                         TopN Hash Memory Usage: 0.1
             Execution mode: vectorized
@@ -105,15 +107,27 @@ STAGE PLANS:
                 allNative: true
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [2, 3, 4, 5, 6, 7, 8, 9, 10]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: double, double, double, double, double, double, double, double, double, double, double
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
+                reduceColumnNullOrder: aaaaaaaaaaaaaa
+                reduceColumnSortOrder: ++++++++++++++
                 groupByVectorOutput: true
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 14
+                    dataColumns: KEY.reducesinkkey0:timestamp, KEY.reducesinkkey1:double, KEY.reducesinkkey2:boolean, KEY.reducesinkkey3:string, KEY.reducesinkkey4:float, KEY.reducesinkkey5:double, KEY.reducesinkkey6:double, KEY.reducesinkkey7:double, KEY.reducesinkkey8:float, KEY.reducesinkkey9:double, KEY.reducesinkkey10:double, KEY.reducesinkkey11:float, KEY.reducesinkkey12:float, KEY.reducesinkkey13:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: timestamp), KEY.reducesinkkey1 (type: double), KEY.reducesinkkey2 (type: boolean), KEY.reducesinkkey3 (type: string), KEY.reducesinkkey4 (type: float), KEY.reducesinkkey5 (type: double), KEY.reducesinkkey6 (type: double), KEY.reducesinkkey7 (type: double), KEY.reducesinkkey8 (type: float), KEY.reducesinkkey9 (type: double), KEY.reducesinkkey5 (type: double), KEY.reducesinkkey11 (type: float), KEY.reducesinkkey12 (type: float), KEY.reducesinkkey13 (type: double)
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_9.q.out b/ql/src/test/results/clientpositive/spark/vectorization_9.q.out
index e731c2d9a8..d5235aa30f 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_9.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_9.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -18,7 +18,7 @@ WHERE    ((cstring2 LIKE '%b%')
               OR (cstring1 < 'a')))
 GROUP BY cstring1, cdouble, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -58,15 +58,35 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc
                   Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                   Filter Operator
+                    Filter Vectorization:
+                        className: VectorFilterOperator
+                        native: true
+                        predicateExpression: FilterExprAndExpr(children: FilterStringColLikeStringScalar(col 7, pattern %b%) -> boolean, FilterExprOrExpr(children: FilterDoubleColGreaterEqualDoubleScalar(col 5, val -1.389) -> boolean, FilterStringGroupColLessStringScalar(col 6, val a) -> boolean) -> boolean) -> boolean
                     predicate: ((cstring2 like '%b%') and ((cdouble >= -1.389) or (cstring1 < 'a'))) (type: boolean)
                     Statistics: Num rows: 4096 Data size: 125745 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                       outputColumnNames: cdouble, cstring1, ctimestamp1
+                      Select Vectorization:
+                          className: VectorSelectOperator
+                          native: true
+                          projectedOutputColumns: [5, 6, 8]
                       Statistics: Num rows: 4096 Data size: 125745 Basic stats: COMPLETE Column stats: NONE
                       Group By Operator
                         aggregations: count(cdouble), stddev_samp(cdouble), min(cdouble)
+                        Group By Vectorization:
+                            aggregators: VectorUDAFCount(col 5) -> bigint, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinDouble(col 5) -> double
+                            className: VectorGroupByOperator
+                            groupByMode: HASH
+                            vectorOutput: true
+                            keyExpressions: col 5, col 6, col 8
+                            native: false
+                            vectorProcessingMode: HASH
+                            projectedOutputColumns: [0, 1, 2]
                         keys: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -75,26 +95,56 @@ STAGE PLANS:
                           key expressions: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
                           sort order: +++
                           Map-reduce partition columns: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              keyColumns: [0, 1, 2]
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                              partitionColumns: [0, 1, 2]
+                              valueColumns: [3, 4, 5]
                           Statistics: Num rows: 4096 Data size: 125745 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: double)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 12
+                    includeColumns: [5, 6, 7, 8]
+                    dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+                    partitionColumnCount: 0
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col1] not supported
-                vectorized: false
+                reduceColumnNullOrder: aaa
+                reduceColumnSortOrder: +++
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 6
+                    dataColumns: KEY._col0:double, KEY._col1:string, KEY._col2:timestamp, VALUE._col0:bigint, VALUE._col1:struct<count:bigint,sum:double,variance:double>, VALUE._col2:double
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), stddev_samp(VALUE._col1), min(VALUE._col2)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 3) -> bigint, VectorUDAFStdSampFinal(col 4) -> double, VectorUDAFMinDouble(col 5) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, col 2
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2]
                 keys: KEY._col0 (type: double), KEY._col1 (type: string), KEY._col2 (type: timestamp)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -102,9 +152,17 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col1 (type: string), _col0 (type: double), _col2 (type: timestamp), (_col0 - 9763215.5639) (type: double), (- (_col0 - 9763215.5639)) (type: double), _col3 (type: bigint), _col4 (type: double), (- _col4) (type: double), (_col4 * UDFToDouble(_col3)) (type: double), _col5 (type: double), (9763215.5639 / _col0) (type: double), (CAST( _col3 AS decimal(19,0)) / -1.389) (type: decimal(28,6)), _col4 (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [1, 0, 2, 6, 8, 3, 4, 7, 10, 5, 9, 12, 4]
+                      selectExpressions: DoubleColSubtractDoubleScalar(col 0, val 9763215.5639) -> 6:double, DoubleColUnaryMinus(col 7)(children: DoubleColSubtractDoubleScalar(col 0, val 9763215.5639) -> 7:double) -> 8:double, DoubleColUnaryMinus(col 4) -> 7:double, DoubleColMultiplyDoubleColumn(col 4, col 9)(children: CastLongToDouble(col 3) -> 9:double) -> 10:double, DoubleScalarDivideDoubleColumn(val 9763215.5639, col 0) -> 9:double, DecimalColDivideDecimalScalar(col 11, val -1.389)(children: CastLongToDecimal(col 3) -> 11:decimal(19,0)) -> 12:decimal(28,6)
                   Statistics: Num rows: 2048 Data size: 62872 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 2048 Data size: 62872 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_pushdown.q.out b/ql/src/test/results/clientpositive/spark/vectorization_pushdown.q.out
index 1f1bb30684..0b901befff 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_pushdown.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_pushdown.q.out
@@ -44,17 +44,20 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:bigint> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0)
diff --git a/ql/src/test/results/clientpositive/spark/vectorization_short_regress.q.out b/ql/src/test/results/clientpositive/spark/vectorization_short_regress.q.out
index 2c7c57ac22..4d3e41ac72 100644
--- a/ql/src/test/results/clientpositive/spark/vectorization_short_regress.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorization_short_regress.q.out
@@ -113,46 +113,70 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: avg(cint), sum(cdouble), stddev_pop(cint), stddev_samp(csmallint), var_samp(cint), avg(cfloat), stddev_samp(cint), min(ctinyint), count(csmallint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double>, VectorUDAFSumDouble(col 5) -> double, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double>, VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFCount(col 1) -> bigint
+                            aggregators: VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFSumDouble(col 5) -> double, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double,input:double>, VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFCount(col 1) -> bigint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                         Statistics: Num rows: 1 Data size: 492 Basic stats: COMPLETE Column stats: NONE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 1 Data size: 492 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col0 (type: struct<count:bigint,sum:double,input:int>), _col1 (type: double), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: struct<count:bigint,sum:double,input:float>), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: tinyint), _col8 (type: bigint)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:int> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), sum(VALUE._col1), stddev_pop(VALUE._col2), stddev_samp(VALUE._col3), var_samp(VALUE._col4), avg(VALUE._col5), stddev_samp(VALUE._col6), min(VALUE._col7), count(VALUE._col8)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFSumDouble(col 1) -> double, VectorUDAFStdPopFinal(col 2) -> double, VectorUDAFStdSampFinal(col 3) -> double, VectorUDAFVarSampFinal(col 4) -> double, VectorUDAFAvgFinal(col 5) -> double, VectorUDAFStdSampFinal(col 6) -> double, VectorUDAFMinLong(col 7) -> tinyint, VectorUDAFCountMerge(col 8) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                 Statistics: Num rows: 1 Data size: 492 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
                   expressions: _col0 (type: double), (_col0 + -3728.0) (type: double), (- (_col0 + -3728.0)) (type: double), (- (- (_col0 + -3728.0))) (type: double), ((- (- (_col0 + -3728.0))) * (_col0 + -3728.0)) (type: double), _col1 (type: double), (- _col0) (type: double), _col2 (type: double), (((- (- (_col0 + -3728.0))) * (_col0 + -3728.0)) * (- (- (_col0 + -3728.0)))) (type: double), _col3 (type: double), (- _col2) (type: double), (_col2 - (- (- (_col0 + -3728.0)))) (type: double), ((_col2 - (- (- (_col0 + -3728.0)))) * _col2) (type: double), _col4 (type: double), _col5 (type: double), (10.175 - _col4) (type: double), (- (10.175 - _col4)) (type: double), ((- _col2) / -563.0) (type: double), _col6 (type: double), (- ((- _col2) / -563.0)) (type: double), (_col0 / _col1) (type: double), _col7 (type: tinyint), _col8 (type: bigint), (UDFToDouble(_col7) / ((- _col2) / -563.0)) (type: double), (- (_col0 / _col1)) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 9, 11, 10, 14, 1, 12, 2, 15, 3, 13, 17, 16, 4, 5, 18, 20, 21, 6, 19, 22, 7, 8, 24, 25]
+                      selectExpressions: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 9:double, DoubleColUnaryMinus(col 10)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 10:double) -> 11:double, DoubleColUnaryMinus(col 12)(children: DoubleColUnaryMinus(col 10)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 10:double) -> 12:double) -> 10:double, DoubleColMultiplyDoubleColumn(col 12, col 13)(children: DoubleColUnaryMinus(col 13)(children: DoubleColUnaryMinus(col 12)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 12:double) -> 13:double) -> 12:double, DoubleColAddDoubleScalar(col 0, val -3728.0) -> 13:double) -> 14:double, DoubleColUnaryMinus(col 0) -> 12:double, DoubleColMultiplyDoubleColumn(col 16, col 13)(children: DoubleColMultiplyDoubleColumn(col 13, col 15)(children: DoubleColUnaryMinus(col 15)(children: DoubleColUnaryMinus(col 13)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 13:double) -> 15:double) -> 13:double, DoubleColAddDoubleScalar(col 0, val -3728.0) -> 15:double) -> 16:double, DoubleColUnaryMinus(col 15)(children: DoubleColUnaryMinus(col 13)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 13:double) -> 15:double) -> 13:double) -> 15:double, DoubleColUnaryMinus(col 2) -> 13:double, DoubleColSubtractDoubleColumn(col 2, col 16)(children: DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 16)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 16:double) -> 17:double) -> 16:double) -> 17:double, DoubleColMultiplyDoubleColumn(col 18, col 2)(children: DoubleColSubtractDoubleColumn(col 2, col 16)(children: DoubleColUnaryMinus(col 18)(children: DoubleColUnaryMinus(col 16)(children: DoubleColAddDoubleScalar(col 0, val -3728.0) -> 16:double) -> 18:double) -> 16:double) -> 18:double) -> 16:double, DoubleScalarSubtractDoubleColumn(val 10.175, col 4) -> 18:double, DoubleColUnaryMinus(col 19)(children: DoubleScalarSubtractDoubleColumn(val 10.175, col 4) -> 19:double) -> 20:double, DoubleColDivideDoubleScalar(col 19, val -563.0)(children: DoubleColUnaryMinus(col 2) -> 19:double) -> 21:double, DoubleColUnaryMinus(col 22)(children: DoubleColDivideDoubleScalar(col 19, val -563.0)(children: DoubleColUnaryMinus(col 2) -> 19:double) -> 22:double) -> 19:double, DoubleColDivideDoubleColumn(col 0, col 1) -> 22:double, DoubleColDivideDoubleColumn(col 23, col 25)(children: CastLongToDouble(col 7) -> 23:double, DoubleColDivideDoubleScalar(col 24, val -563.0)(children: DoubleColUnaryMinus(col 2) -> 24:double) -> 25:double) -> 24:double, DoubleColUnaryMinus(col 23)(children: DoubleColDivideDoubleColumn(col 0, col 1) -> 23:double) -> 25:double
                   Statistics: Num rows: 1 Data size: 492 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 492 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -351,46 +375,70 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: max(cint), var_pop(cbigint), stddev_pop(csmallint), max(cdouble), avg(ctinyint), min(cint), min(cdouble), stddev_samp(csmallint), var_samp(cint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFMaxLong(col 2) -> int, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 5) -> double, VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double>, VectorUDAFMinLong(col 2) -> int, VectorUDAFMinDouble(col 5) -> double, VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>
+                            aggregators: VectorUDAFMaxLong(col 2) -> int, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 5) -> double, VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFMinLong(col 2) -> int, VectorUDAFMinDouble(col 5) -> double, VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                         Statistics: Num rows: 1 Data size: 420 Basic stats: COMPLETE Column stats: NONE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 1 Data size: 420 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: double), _col4 (type: struct<count:bigint,sum:double,input:tinyint>), _col5 (type: int), _col6 (type: double), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,variance:double>)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF var_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col1] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), var_pop(VALUE._col1), stddev_pop(VALUE._col2), max(VALUE._col3), avg(VALUE._col4), min(VALUE._col5), min(VALUE._col6), stddev_samp(VALUE._col7), var_samp(VALUE._col8)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxLong(col 0) -> int, VectorUDAFVarPopFinal(col 1) -> double, VectorUDAFStdPopFinal(col 2) -> double, VectorUDAFMaxDouble(col 3) -> double, VectorUDAFAvgFinal(col 4) -> double, VectorUDAFMinLong(col 5) -> int, VectorUDAFMinDouble(col 6) -> double, VectorUDAFStdSampFinal(col 7) -> double, VectorUDAFVarSampFinal(col 8) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                 Statistics: Num rows: 1 Data size: 420 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
                   expressions: _col0 (type: int), (UDFToDouble(_col0) / -3728.0) (type: double), (_col0 * -3728) (type: int), _col1 (type: double), (- (_col0 * -3728)) (type: int), _col2 (type: double), (-563 % (_col0 * -3728)) (type: int), (_col1 / _col2) (type: double), (- _col2) (type: double), _col3 (type: double), _col4 (type: double), (_col2 - 10.175) (type: double), _col5 (type: int), (UDFToDouble((_col0 * -3728)) % (_col2 - 10.175)) (type: double), (- _col3) (type: double), _col6 (type: double), (_col3 % -26.28) (type: double), _col7 (type: double), (- (UDFToDouble(_col0) / -3728.0)) (type: double), ((- (_col0 * -3728)) % (-563 % (_col0 * -3728))) (type: int), ((UDFToDouble(_col0) / -3728.0) - _col4) (type: double), (- (_col0 * -3728)) (type: int), _col8 (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 10, 11, 1, 13, 2, 14, 9, 15, 3, 4, 16, 5, 19, 17, 6, 18, 7, 20, 12, 21, 23, 8]
+                      selectExpressions: DoubleColDivideDoubleScalar(col 9, val -3728.0)(children: CastLongToDouble(col 0) -> 9:double) -> 10:double, LongColMultiplyLongScalar(col 0, val -3728) -> 11:long, LongColUnaryMinus(col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 13:long, LongScalarModuloLongColumn(val -563, col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 14:long, DoubleColDivideDoubleColumn(col 1, col 2) -> 9:double, DoubleColUnaryMinus(col 2) -> 15:double, DoubleColSubtractDoubleScalar(col 2, val 10.175) -> 16:double, DoubleColModuloDoubleColumn(col 17, col 18)(children: CastLongToDouble(col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 17:double, DoubleColSubtractDoubleScalar(col 2, val 10.175) -> 18:double) -> 19:double, DoubleColUnaryMinus(col 3) -> 17:double, DoubleColModuloDoubleScalar(col 3, val -26.28) -> 18:double, DoubleColUnaryMinus(col 21)(children: DoubleColDivideDoubleScalar(col 20, val -3728.0)(children: CastLongToDouble(col 0) -> 20:double) -> 21:double) -> 20:double, LongColModuloLongColumn(col 22, col 23)(children: LongColUnaryMinus(col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 22:long, LongScalarModuloLongColumn(val -563, col 12)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 12:long) -> 23:long) -> 12:long, DoubleColSubtractDoubleColumn(col 24, col 4)(children: DoubleColDivideDoubleScalar(col 21, val -3728.0)(children: CastLongToDouble(col 0) -> 21:double) -> 24:double) -> 21:double, LongColUnaryMinus(col 22)(children: LongColMultiplyLongScalar(col 0, val -3728) -> 22:long) -> 23:long
                   Statistics: Num rows: 1 Data size: 420 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 420 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -473,7 +521,7 @@ WHERE  (((cbigint <= 197)
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alltypesorc
 #### A masked pattern was here ####
--20301111	5445.576984978541	-1626869520	7.9684972882908944E16	1626869520	NULL	-563	NULL	NULL	NULL	-8.935323383084578	NULL	-1069736047	NULL	NULL	NULL	NULL	NULL	-5445.576984978541	511	5454.512308361625	1626869520	7.2647256545687792E16
+-20301111	5445.576984978541	-1626869520	7.9684972882908944E16	1626869520	NULL	-563	NULL	NULL	NULL	-8.935323383084578	NULL	-1069736047	NULL	NULL	NULL	NULL	NULL	-5445.576984978541	-58	5454.512308361625	1626869520	7.2647256545687792E16
 PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
 SELECT VAR_POP(cbigint),
        (-(VAR_POP(cbigint))),
@@ -581,46 +629,70 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: var_pop(cbigint), count(), max(ctinyint), stddev_pop(csmallint), max(cint), stddev_samp(cdouble), count(ctinyint), avg(ctinyint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxLong(col 2) -> int, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 0) -> bigint, VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double>
+                            aggregators: VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxLong(col 2) -> int, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 0) -> bigint, VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double,input:bigint>
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                         Statistics: Num rows: 1 Data size: 340 Basic stats: COMPLETE Column stats: NONE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 1 Data size: 340 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: bigint), _col2 (type: tinyint), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: int), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: bigint), _col7 (type: struct<count:bigint,sum:double,input:tinyint>)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF var_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: var_pop(VALUE._col0), count(VALUE._col1), max(VALUE._col2), stddev_pop(VALUE._col3), max(VALUE._col4), stddev_samp(VALUE._col5), count(VALUE._col6), avg(VALUE._col7)
+                Group By Vectorization:
+                    aggregators: VectorUDAFVarPopFinal(col 0) -> double, VectorUDAFCountMerge(col 1) -> bigint, VectorUDAFMaxLong(col 2) -> tinyint, VectorUDAFStdPopFinal(col 3) -> double, VectorUDAFMaxLong(col 4) -> int, VectorUDAFStdSampFinal(col 5) -> double, VectorUDAFCountMerge(col 6) -> bigint, VectorUDAFAvgFinal(col 7) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                 Statistics: Num rows: 1 Data size: 340 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
                   expressions: _col0 (type: double), (- _col0) (type: double), (_col0 - (- _col0)) (type: double), _col1 (type: bigint), (CAST( _col1 AS decimal(19,0)) % 79.553) (type: decimal(5,3)), _col2 (type: tinyint), (UDFToDouble(_col1) - (- _col0)) (type: double), (- (- _col0)) (type: double), (-1.0 % (- _col0)) (type: double), _col1 (type: bigint), (- _col1) (type: bigint), _col3 (type: double), (- (- (- _col0))) (type: double), (762 * (- _col1)) (type: bigint), _col4 (type: int), (UDFToLong(_col2) + (762 * (- _col1))) (type: bigint), ((- _col0) + UDFToDouble(_col4)) (type: double), _col5 (type: double), ((- _col1) % _col1) (type: bigint), _col6 (type: bigint), _col7 (type: double), (-3728 % (UDFToLong(_col2) + (762 * (- _col1)))) (type: bigint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 8, 10, 1, 12, 2, 14, 13, 15, 1, 16, 3, 9, 19, 4, 18, 22, 5, 23, 6, 7, 24]
+                      selectExpressions: DoubleColUnaryMinus(col 0) -> 8:double, DoubleColSubtractDoubleColumn(col 0, col 9)(children: DoubleColUnaryMinus(col 0) -> 9:double) -> 10:double, DecimalColModuloDecimalScalar(col 11, val 79.553)(children: CastLongToDecimal(col 1) -> 11:decimal(19,0)) -> 12:decimal(5,3), DoubleColSubtractDoubleColumn(col 9, col 13)(children: CastLongToDouble(col 1) -> 9:double, DoubleColUnaryMinus(col 0) -> 13:double) -> 14:double, DoubleColUnaryMinus(col 9)(children: DoubleColUnaryMinus(col 0) -> 9:double) -> 13:double, DoubleScalarModuloDoubleColumn(val -1.0, col 9)(children: DoubleColUnaryMinus(col 0) -> 9:double) -> 15:double, LongColUnaryMinus(col 1) -> 16:long, DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 9)(children: DoubleColUnaryMinus(col 0) -> 9:double) -> 17:double) -> 9:double, LongScalarMultiplyLongColumn(val 762, col 18)(children: LongColUnaryMinus(col 1) -> 18:long) -> 19:long, LongColAddLongColumn(col 2, col 20)(children: col 2, LongScalarMultiplyLongColumn(val 762, col 18)(children: LongColUnaryMinus(col 1) -> 18:long) -> 20:long) -> 18:long, DoubleColAddDoubleColumn(col 17, col 21)(children: DoubleColUnaryMinus(col 0) -> 17:double, CastLongToDouble(col 4) -> 21:double) -> 22:double, LongColModuloLongColumn(col 20, col 1)(children: LongColUnaryMinus(col 1) -> 20:long) -> 23:long, LongScalarModuloLongColumn(val -3728, col 20)(children: LongColAddLongColumn(col 2, col 24)(children: col 2, LongScalarMultiplyLongColumn(val 762, col 20)(children: LongColUnaryMinus(col 1) -> 20:long) -> 24:long) -> 20:long) -> 24:long
                   Statistics: Num rows: 1 Data size: 340 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 340 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -790,46 +862,70 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: avg(ctinyint), max(cbigint), stddev_samp(cint), var_pop(cint), var_pop(cbigint), max(cfloat)
                         Group By Vectorization:
-                            aggregators: VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double>, VectorUDAFMaxLong(col 3) -> bigint, VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float
+                            aggregators: VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFMaxLong(col 3) -> bigint, VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(col 0) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                         Statistics: Num rows: 1 Data size: 328 Basic stats: COMPLETE Column stats: NONE
                         Reduce Output Operator
                           sort order: 
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkEmptyKeyOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 1 Data size: 328 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col0 (type: struct<count:bigint,sum:double,input:tinyint>), _col1 (type: bigint), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: float)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:tinyint> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), max(VALUE._col1), stddev_samp(VALUE._col2), var_pop(VALUE._col3), var_pop(VALUE._col4), max(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFMaxLong(col 1) -> bigint, VectorUDAFStdSampFinal(col 2) -> double, VectorUDAFVarPopFinal(col 3) -> double, VectorUDAFVarPopFinal(col 4) -> double, VectorUDAFMaxDouble(col 5) -> float
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                 Statistics: Num rows: 1 Data size: 328 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
                   expressions: _col0 (type: double), (_col0 + 6981.0) (type: double), ((_col0 + 6981.0) + _col0) (type: double), _col1 (type: bigint), (((_col0 + 6981.0) + _col0) / _col0) (type: double), (- (_col0 + 6981.0)) (type: double), _col2 (type: double), (_col0 % (- (_col0 + 6981.0))) (type: double), _col3 (type: double), _col4 (type: double), (- _col1) (type: bigint), (UDFToDouble((- _col1)) / _col2) (type: double), _col5 (type: float), (_col4 * -26.28) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 6, 8, 1, 7, 10, 2, 9, 3, 4, 12, 14, 5, 11]
+                      selectExpressions: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 6:double, DoubleColAddDoubleColumn(col 7, col 0)(children: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 7:double) -> 8:double, DoubleColDivideDoubleColumn(col 9, col 0)(children: DoubleColAddDoubleColumn(col 7, col 0)(children: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 7:double) -> 9:double) -> 7:double, DoubleColUnaryMinus(col 9)(children: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 9:double) -> 10:double, DoubleColModuloDoubleColumn(col 0, col 11)(children: DoubleColUnaryMinus(col 9)(children: DoubleColAddDoubleScalar(col 0, val 6981.0) -> 9:double) -> 11:double) -> 9:double, LongColUnaryMinus(col 1) -> 12:long, DoubleColDivideDoubleColumn(col 11, col 2)(children: CastLongToDouble(col 13)(children: LongColUnaryMinus(col 1) -> 13:long) -> 11:double) -> 14:double, DoubleColMultiplyDoubleScalar(col 4, val -26.28) -> 11:double
                   Statistics: Num rows: 1 Data size: 328 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 328 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -2098,11 +2194,12 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 1
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: csmallint (type: smallint)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4
@@ -2111,26 +2208,42 @@ STAGE PLANS:
                           key expressions: _col0 (type: smallint)
                           sort order: +
                           Map-reduce partition columns: _col0 (type: smallint)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 2503 Data size: 76841 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: bigint), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: bigint)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev_samp(VALUE._col0), sum(VALUE._col1), var_pop(VALUE._col2), count(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFStdSampFinal(col 1) -> double, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFVarPopFinal(col 3) -> double, VectorUDAFCountMerge(col 4) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3]
                 keys: KEY._col0 (type: smallint)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
@@ -2138,10 +2251,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: smallint), (UDFToInteger(_col0) % -75) (type: int), _col1 (type: double), (-1.389 / CAST( _col0 AS decimal(5,0))) (type: decimal(10,9)), _col2 (type: bigint), (UDFToDouble((UDFToInteger(_col0) % -75)) / UDFToDouble(_col2)) (type: double), (- (UDFToInteger(_col0) % -75)) (type: int), _col3 (type: double), (- (- (UDFToInteger(_col0) % -75))) (type: int), _col4 (type: bigint), (_col4 - -89010) (type: bigint)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 5, 1, 7, 2, 11, 12, 3, 8, 4, 13]
+                      selectExpressions: LongColModuloLongScalar(col 0, val -75)(children: col 0) -> 5:long, DecimalScalarDivideDecimalColumn(val -1.389, col 6)(children: CastLongToDecimal(col 0) -> 6:decimal(5,0)) -> 7:decimal(10,9), DoubleColDivideDoubleColumn(col 9, col 10)(children: CastLongToDouble(col 8)(children: LongColModuloLongScalar(col 0, val -75)(children: col 0) -> 8:long) -> 9:double, CastLongToDouble(col 2) -> 10:double) -> 11:double, LongColUnaryMinus(col 8)(children: LongColModuloLongScalar(col 0, val -75)(children: col 0) -> 8:long) -> 12:long, LongColUnaryMinus(col 13)(children: LongColUnaryMinus(col 8)(children: LongColModuloLongScalar(col 0, val -75)(children: col 0) -> 8:long) -> 13:long) -> 8:long, LongColSubtractLongScalar(col 4, val -89010) -> 13:long
                   Statistics: Num rows: 1251 Data size: 38405 Basic stats: COMPLETE Column stats: NONE
                   Reduce Output Operator
                     key expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: double), _col3 (type: decimal(10,9)), _col4 (type: bigint), _col5 (type: double), _col6 (type: int), _col7 (type: double), _col8 (type: int), _col9 (type: bigint), _col10 (type: bigint)
                     sort order: +++++++++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 1251 Data size: 38405 Basic stats: COMPLETE Column stats: NONE
                     TopN Hash Memory Usage: 0.1
         Reducer 3 
@@ -2350,11 +2472,12 @@ STAGE PLANS:
                         Group By Vectorization:
                             aggregators: VectorUDAFVarSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 4) -> bigint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFVarPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumDouble(col 5) -> double
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 5
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFVarSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: cdouble (type: double)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
@@ -2363,26 +2486,42 @@ STAGE PLANS:
                           key expressions: _col0 (type: double)
                           sort order: +
                           Map-reduce partition columns: _col0 (type: double)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 2654 Data size: 81476 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: bigint), _col3 (type: double), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: double)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF var_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: var_samp(VALUE._col0), count(VALUE._col1), sum(VALUE._col2), var_pop(VALUE._col3), stddev_pop(VALUE._col4), sum(VALUE._col5)
+                Group By Vectorization:
+                    aggregators: VectorUDAFVarSampFinal(col 1) -> double, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFSumDouble(col 3) -> double, VectorUDAFVarPopFinal(col 4) -> double, VectorUDAFStdPopFinal(col 5) -> double, VectorUDAFSumDouble(col 6) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                 keys: KEY._col0 (type: double)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
@@ -2390,10 +2529,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: double), _col1 (type: double), (2563.58 * _col1) (type: double), (- _col1) (type: double), _col2 (type: bigint), ((2563.58 * _col1) + -5638.15) (type: double), ((- _col1) * ((2563.58 * _col1) + -5638.15)) (type: double), _col3 (type: double), _col4 (type: double), (_col0 - (- _col1)) (type: double), _col5 (type: double), (_col0 + _col1) (type: double), (_col0 * 762.0) (type: double), _col6 (type: double), (-863.257 % (_col0 * 762.0)) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 7, 8, 2, 10, 11, 3, 4, 12, 5, 9, 13, 6, 15]
+                      selectExpressions: DoubleScalarMultiplyDoubleColumn(val 2563.58, col 1) -> 7:double, DoubleColUnaryMinus(col 1) -> 8:double, DoubleColAddDoubleScalar(col 9, val -5638.15)(children: DoubleScalarMultiplyDoubleColumn(val 2563.58, col 1) -> 9:double) -> 10:double, DoubleColMultiplyDoubleColumn(col 9, col 12)(children: DoubleColUnaryMinus(col 1) -> 9:double, DoubleColAddDoubleScalar(col 11, val -5638.15)(children: DoubleScalarMultiplyDoubleColumn(val 2563.58, col 1) -> 11:double) -> 12:double) -> 11:double, DoubleColSubtractDoubleColumn(col 0, col 9)(children: DoubleColUnaryMinus(col 1) -> 9:double) -> 12:double, DoubleColAddDoubleColumn(col 0, col 1) -> 9:double, DoubleColMultiplyDoubleScalar(col 0, val 762.0) -> 13:double, DoubleScalarModuloDoubleColumn(val -863.257, col 14)(children: DoubleColMultiplyDoubleScalar(col 0, val 762.0) -> 14:double) -> 15:double
                   Statistics: Num rows: 1327 Data size: 40738 Basic stats: COMPLETE Column stats: NONE
                   Reduce Output Operator
                     key expressions: _col0 (type: double)
                     sort order: +
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 1327 Data size: 40738 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: double), _col4 (type: bigint), _col5 (type: double), _col6 (type: double), _col7 (type: double), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: double), _col12 (type: double), _col13 (type: double), _col14 (type: double)
         Reducer 3 
@@ -2644,13 +2792,14 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: stddev_pop(cint), avg(csmallint), count(), min(ctinyint), var_samp(csmallint), var_pop(cfloat), avg(cint), var_samp(cfloat), avg(cfloat), min(cdouble), var_pop(csmallint), stddev_pop(ctinyint), sum(cint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFVarSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double>, VectorUDAFVarSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double>, VectorUDAFMinDouble(col 5) -> double, VectorUDAFVarPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 2) -> bigint
+                            aggregators: VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFVarSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFVarSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double,input:double>, VectorUDAFMinDouble(col 5) -> double, VectorUDAFVarPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 2) -> bigint
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 8, col 6
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDouble(col 4) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                         keys: ctimestamp1 (type: timestamp), cstring1 (type: string)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
@@ -2659,26 +2808,42 @@ STAGE PLANS:
                           key expressions: _col0 (type: timestamp), _col1 (type: string)
                           sort order: ++
                           Map-reduce partition columns: _col0 (type: timestamp), _col1 (type: string)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 12288 Data size: 377237 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,input:smallint>), _col4 (type: bigint), _col5 (type: tinyint), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,input:int>), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,input:float>), _col11 (type: double), _col12 (type: struct<count:bigint,sum:double,variance:double>), _col13 (type: struct<count:bigint,sum:double,variance:double>), _col14 (type: bigint)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF stddev_pop parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: stddev_pop(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), min(VALUE._col3), var_samp(VALUE._col4), var_pop(VALUE._col5), avg(VALUE._col6), var_samp(VALUE._col7), avg(VALUE._col8), min(VALUE._col9), var_pop(VALUE._col10), stddev_pop(VALUE._col11), sum(VALUE._col12)
+                Group By Vectorization:
+                    aggregators: VectorUDAFStdPopFinal(col 2) -> double, VectorUDAFAvgFinal(col 3) -> double, VectorUDAFCountMerge(col 4) -> bigint, VectorUDAFMinLong(col 5) -> tinyint, VectorUDAFVarSampFinal(col 6) -> double, VectorUDAFVarPopFinal(col 7) -> double, VectorUDAFAvgFinal(col 8) -> double, VectorUDAFVarSampFinal(col 9) -> double, VectorUDAFAvgFinal(col 10) -> double, VectorUDAFMinDouble(col 11) -> double, VectorUDAFVarPopFinal(col 12) -> double, VectorUDAFStdPopFinal(col 13) -> double, VectorUDAFSumLong(col 14) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
                 keys: KEY._col0 (type: timestamp), KEY._col1 (type: string)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
@@ -2686,10 +2851,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: timestamp), _col1 (type: string), _col2 (type: double), (_col2 * 10.175) (type: double), (- _col2) (type: double), _col3 (type: double), (- _col2) (type: double), (-26.28 - _col2) (type: double), _col4 (type: bigint), (- _col4) (type: bigint), ((-26.28 - _col2) * (- _col2)) (type: double), _col5 (type: tinyint), (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4))) (type: double), (- (_col2 * 10.175)) (type: double), _col6 (type: double), (_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) (type: double), (- (- _col2)) (type: double), (UDFToDouble((- _col4)) / _col2) (type: double), _col7 (type: double), (10.175 / _col3) (type: double), _col8 (type: double), _col9 (type: double), ((_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) - (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) (type: double), (- (- (_col2 * 10.175))) (type: double), _col10 (type: double), (((_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) - (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) * 10.175) (type: double), (10.175 % (10.175 / _col3)) (type: double), (- _col5) (type: tinyint), _col11 (type: double), _col12 (type: double), (- ((-26.28 - _col2) * (- _col2))) (type: double), ((- _col2) % _col10) (type: double), (-26.28 / CAST( (- _col5) AS decimal(3,0))) (type: decimal(8,6)), _col13 (type: double), _col14 (type: bigint), ((_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) / _col7) (type: double), (- (- _col4)) (type: bigint), _col4 (type: bigint), ((_col6 + (((-26.28 - _col2) * (- _col2)) * UDFToDouble((- _col4)))) % -26.28) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25, _col26, _col27, _col28, _col29, _col30, _col31, _col32, _col33, _col34, _col35, _col36, _col37, _col38
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 2, 15, 16, 3, 17, 18, 4, 19, 22, 5, 21, 23, 6, 20, 26, 27, 7, 25, 8, 9, 29, 28, 10, 30, 32, 24, 11, 12, 31, 34, 37, 13, 14, 38, 40, 4, 39]
+                      selectExpressions: DoubleColMultiplyDoubleScalar(col 2, val 10.175) -> 15:double, DoubleColUnaryMinus(col 2) -> 16:double, DoubleColUnaryMinus(col 2) -> 17:double, DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 18:double, LongColUnaryMinus(col 4) -> 19:long, DoubleColMultiplyDoubleColumn(col 20, col 21)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 20:double, DoubleColUnaryMinus(col 2) -> 21:double) -> 22:double, DoubleColMultiplyDoubleColumn(col 23, col 20)(children: DoubleColMultiplyDoubleColumn(col 20, col 21)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 20:double, DoubleColUnaryMinus(col 2) -> 21:double) -> 23:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 20:double) -> 21:double, DoubleColUnaryMinus(col 20)(children: DoubleColMultiplyDoubleScalar(col 2, val 10.175) -> 20:double) -> 23:double, DoubleColAddDoubleColumn(col 6, col 25)(children: DoubleColMultiplyDoubleColumn(col 26, col 20)(children: DoubleColMultiplyDoubleColumn(col 20, col 25)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 20:double, DoubleColUnaryMinus(col 2) -> 25:double) -> 26:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 20:double) -> 25:double) -> 20:double, DoubleColUnaryMinus(col 25)(children: DoubleColUnaryMinus(col 2) -> 25:double) -> 26:double, DoubleColDivideDoubleColumn(col 25, col 2)(children: CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 25:double) -> 27:double, DoubleScalarDivideDoubleColumn(val 10.175, col 3) -> 25:double, DoubleColSubtractDoubleColumn(col 28, col 30)(children: DoubleColAddDoubleColumn(col 6, col 29)(children: DoubleColMultiplyDoubleColumn(col 30, col 28)(children: DoubleColMultiplyDoubleColumn(col 28, col 29)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 28:double, DoubleColUnaryMinus(col 2) -> 29:double) -> 30:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 28:double) -> 29:double) -> 28:double, DoubleColMultiplyDoubleColumn(col 31, col 29)(children: DoubleColMultiplyDoubleColumn(col 29, col 30)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 29:double, DoubleColUnaryMinus(col 2) -> 30:double) -> 31:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 29:double) -> 30:double) -> 29:double, DoubleColUnaryMinus(col 30)(children: DoubleColUnaryMinus(col 28)(children: DoubleColMultiplyDoubleScalar(col 2, val 10.175) -> 28:double) -> 30:double) -> 28:double, DoubleColMultiplyDoubleScalar(col 31, val 10.175)(children: DoubleColSubtractDoubleColumn(col 30, col 32)(children: DoubleColAddDoubleColumn(col 6, col 31)(children: DoubleColMultiplyDoubleColumn(col 32, col 30)(children: DoubleColMultiplyDoubleColumn(col 30, col 31)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 30:double, DoubleColUnaryMinus(col 2) -> 31:double) -> 32:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 30:double) -> 31:double) -> 30:double, DoubleColMultiplyDoubleColumn(col 33, col 31)(children: DoubleColMultiplyDoubleColumn(col 31, col 32)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 31:double, DoubleColUnaryMinus(col 2) -> 32:double) -> 33:double, CastLongToDouble(col 24)(children: LongColUnaryMinus(col 4) -> 24:long) -> 31:double) -> 32:double) -> 31:double) -> 30:double, DoubleScalarModuloDoubleColumn(val 10.175, col 31)(children: DoubleScalarDivideDoubleColumn(val 10.175, col 3) -> 31:double) -> 32:double, LongColUnaryMinus(col 5) -> 24:long, DoubleColUnaryMinus(col 34)(children: DoubleColMultiplyDoubleColumn(col 31, col 33)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 31:double, DoubleColUnaryMinus(col 2) -> 33:double) -> 34:double) -> 31:double, DoubleColModuloDoubleColumn(col 33, col 10)(children: DoubleColUnaryMinus(col 2) -> 33:double) -> 34:double, DecimalScalarDivideDecimalColumn(val -26.28, col 36)(children: CastLongToDecimal(col 35)(children: LongColUnaryMinus(col 5) -> 35:long) -> 36:decimal(3,0)) -> 37:decimal(8,6), DoubleColDivideDoubleColumn(col 33, col 7)(children: DoubleColAddDoubleColumn(col 6, col 38)(children: DoubleColMultiplyDoubleColumn(col 39, col 33)(children: DoubleColMultiplyDoubleColumn(col 33, col 38)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 33:double, DoubleColUnaryMinus(col 2) -> 38:double) -> 39:double, CastLongToDouble(col 35)(children: LongColUnaryMinus(col 4) -> 35:long) -> 33:double) -> 38:double) -> 33:double) -> 38:double, LongColUnaryMinus(col 35)(children: LongColUnaryMinus(col 4) -> 35:long) -> 40:long, DoubleColModuloDoubleScalar(col 33, val -26.28)(children: DoubleColAddDoubleColumn(col 6, col 39)(children: DoubleColMultiplyDoubleColumn(col 41, col 33)(children: DoubleColMultiplyDoubleColumn(col 33, col 39)(children: DoubleScalarSubtractDoubleColumn(val -26.28, col 2) -> 33:double, DoubleColUnaryMinus(col 2) -> 39:double) -> 41:double, CastLongToDouble(col 35)(children: LongColUnaryMinus(col 4) -> 35:long) -> 33:double) -> 39:double) -> 33:double) -> 39:double
                   Statistics: Num rows: 6144 Data size: 188618 Basic stats: COMPLETE Column stats: NONE
                   Reduce Output Operator
                     key expressions: _col0 (type: timestamp), _col1 (type: string), _col2 (type: double), _col3 (type: double), _col4 (type: double), _col5 (type: double), _col6 (type: double), _col7 (type: double), _col8 (type: bigint), _col9 (type: bigint), _col10 (type: double), _col11 (type: tinyint), _col12 (type: double), _col13 (type: double), _col14 (type: double), _col15 (type: double), _col16 (type: double), _col17 (type: double), _col18 (type: double), _col19 (type: double), _col20 (type: double), _col21 (type: double), _col22 (type: double), _col23 (type: double), _col24 (type: double), _col25 (type: double), _col26 (type: double), _col27 (type: tinyint), _col28 (type: double), _col29 (type: double), _col30 (type: double), _col31 (type: double), _col32 (type: decimal(8,6)), _col33 (type: double), _col34 (type: bigint), _col35 (type: double), _col36 (type: bigint), _col37 (type: bigint), _col38 (type: double)
                     sort order: +++++++++++++++++++++++++++++++++++++++
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 6144 Data size: 188618 Basic stats: COMPLETE Column stats: NONE
                     TopN Hash Memory Usage: 0.1
         Reducer 3 
@@ -3021,13 +3195,14 @@ STAGE PLANS:
                       Group By Operator
                         aggregations: max(cfloat), sum(cbigint), var_samp(cint), avg(cdouble), min(cbigint), var_pop(cbigint), sum(cint), stddev_samp(ctinyint), stddev_pop(csmallint), avg(cint)
                         Group By Vectorization:
-                            aggregators: VectorUDAFMaxDouble(col 4) -> float, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double>, VectorUDAFMinLong(col 3) -> bigint, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFStdSampLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double>
+                            aggregators: VectorUDAFMaxDouble(col 4) -> float, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>, VectorUDAFMinLong(col 3) -> bigint, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFStdSampLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>
                             className: VectorGroupByOperator
-                            vectorOutput: false
+                            groupByMode: HASH
+                            vectorOutput: true
                             keyExpressions: col 10
                             native: false
+                            vectorProcessingMode: HASH
                             projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
-                            vectorOutputConditionsNotMet: Vector output of VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                         keys: cboolean1 (type: boolean)
                         mode: hash
                         outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -3036,26 +3211,42 @@ STAGE PLANS:
                           key expressions: _col0 (type: boolean)
                           sort order: +
                           Map-reduce partition columns: _col0 (type: boolean)
+                          Reduce Sink Vectorization:
+                              className: VectorReduceSinkObjectHashOperator
+                              native: true
+                              nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                           Statistics: Num rows: 10239 Data size: 314333 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col1 (type: float), _col2 (type: bigint), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,input:double>), _col5 (type: bigint), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: bigint), _col8 (type: struct<count:bigint,sum:double,variance:double>), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,input:int>)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF var_samp parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,variance:double> of Column[VALUE._col2] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: max(VALUE._col0), sum(VALUE._col1), var_samp(VALUE._col2), avg(VALUE._col3), min(VALUE._col4), var_pop(VALUE._col5), sum(VALUE._col6), stddev_samp(VALUE._col7), stddev_pop(VALUE._col8), avg(VALUE._col9)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMaxDouble(col 1) -> float, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFVarSampFinal(col 3) -> double, VectorUDAFAvgFinal(col 4) -> double, VectorUDAFMinLong(col 5) -> bigint, VectorUDAFVarPopFinal(col 6) -> double, VectorUDAFSumLong(col 7) -> bigint, VectorUDAFStdSampFinal(col 8) -> double, VectorUDAFStdPopFinal(col 9) -> double, VectorUDAFAvgFinal(col 10) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
                 keys: KEY._col0 (type: boolean)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -3063,10 +3254,19 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: boolean), _col1 (type: float), (- _col1) (type: float), (-26.28 / UDFToDouble(_col1)) (type: double), _col2 (type: bigint), (CAST( _col2 AS decimal(19,0)) - 10.175) (type: decimal(23,3)), _col3 (type: double), (_col3 % UDFToDouble(_col1)) (type: double), (10.175 + (- _col1)) (type: float), _col4 (type: double), (UDFToDouble((CAST( _col2 AS decimal(19,0)) - 10.175)) + _col3) (type: double), _col5 (type: bigint), _col6 (type: double), (- (10.175 + (- _col1))) (type: float), (79.553 / _col6) (type: double), (_col3 % (79.553 / _col6)) (type: double), _col7 (type: bigint), _col8 (type: double), (-1.389 * CAST( _col5 AS decimal(19,0))) (type: decimal(24,3)), (CAST( _col7 AS decimal(19,0)) - (-1.389 * CAST( _col5 AS decimal(19,0)))) (type: decimal(25,3)), _col9 (type: double), (- (CAST( _col7 AS decimal(19,0)) - (-1.389 * CAST( _col5 AS decimal(19,0))))) (type: decimal(25,3)), _col10 (type: double), (- _col10) (type: double), (_col10 * UDFToDouble(_col7)) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 11, 12, 2, 14, 3, 15, 17, 4, 19, 5, 6, 16, 20, 22, 7, 8, 23, 26, 9, 28, 10, 21, 30]
+                      selectExpressions: DoubleColUnaryMinus(col 1) -> 11:double, DoubleScalarDivideDoubleColumn(val -26.28, col 1)(children: col 1) -> 12:double, DecimalColSubtractDecimalScalar(col 13, val 10.175)(children: CastLongToDecimal(col 2) -> 13:decimal(19,0)) -> 14:decimal(23,3), DoubleColModuloDoubleColumn(col 3, col 1)(children: col 1) -> 15:double, DoubleScalarAddDoubleColumn(val 10.175000190734863, col 16)(children: DoubleColUnaryMinus(col 1) -> 16:double) -> 17:double, DoubleColAddDoubleColumn(col 16, col 3)(children: CastDecimalToDouble(col 18)(children: DecimalColSubtractDecimalScalar(col 13, val 10.175)(children: CastLongToDecimal(col 2) -> 13:decimal(19,0)) -> 18:decimal(23,3)) -> 16:double) -> 19:double, DoubleColUnaryMinus(col 20)(children: DoubleScalarAddDoubleColumn(val 10.175000190734863, col 16)(children: DoubleColUnaryMinus(col 1) -> 16:double) -> 20:double) -> 16:double, DoubleScalarDivideDoubleColumn(val 79.553, col 6) -> 20:double, DoubleColModuloDoubleColumn(col 3, col 21)(children: DoubleScalarDivideDoubleColumn(val 79.553, col 6) -> 21:double) -> 22:double, DecimalScalarMultiplyDecimalColumn(val -1.389, col 13)(children: CastLongToDecimal(col 5) -> 13:decimal(19,0)) -> 23:decimal(24,3), DecimalColSubtractDecimalColumn(col 13, col 25)(children: CastLongToDecimal(col 7) -> 13:decimal(19,0), DecimalScalarMultiplyDecimalColumn(val -1.389, col 24)(children: CastLongToDecimal(col 5) -> 24:decimal(19,0)) -> 25:decimal(24,3)) -> 26:decimal(25,3), FuncNegateDecimalToDecimal(col 27)(children: DecimalColSubtractDecimalColumn(col 13, col 25)(children: CastLongToDecimal(col 7) -> 13:decimal(19,0), DecimalScalarMultiplyDecimalColumn(val -1.389, col 24)(children: CastLongToDecimal(col 5) -> 24:decimal(19,0)) -> 25:decimal(24,3)) -> 27:decimal(25,3)) -> 28:decimal(25,3), DoubleColUnaryMinus(col 10) -> 21:double, DoubleColMultiplyDoubleColumn(col 10, col 29)(children: CastLongToDouble(col 7) -> 29:double) -> 30:double
                   Statistics: Num rows: 5119 Data size: 157151 Basic stats: COMPLETE Column stats: NONE
                   Reduce Output Operator
                     key expressions: _col0 (type: boolean)
                     sort order: +
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkObjectHashOperator
+                        native: true
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                     Statistics: Num rows: 5119 Data size: 157151 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col1 (type: float), _col2 (type: float), _col3 (type: double), _col4 (type: bigint), _col5 (type: decimal(23,3)), _col6 (type: double), _col7 (type: double), _col8 (type: float), _col9 (type: double), _col10 (type: double), _col11 (type: bigint), _col12 (type: double), _col13 (type: float), _col14 (type: double), _col15 (type: double), _col17 (type: bigint), _col18 (type: double), _col19 (type: decimal(24,3)), _col20 (type: decimal(25,3)), _col21 (type: double), _col22 (type: decimal(25,3)), _col23 (type: double), _col24 (type: double), _col25 (type: double)
         Reducer 3 
@@ -3238,8 +3438,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3276,8 +3478,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3350,8 +3554,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3388,8 +3594,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3534,8 +3742,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3572,8 +3782,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3646,8 +3858,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3684,8 +3898,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3758,8 +3974,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 2) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3796,8 +4014,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3870,8 +4090,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 4) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -3908,8 +4130,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -3982,8 +4206,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 6) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -4020,8 +4246,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -4094,8 +4322,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFCount(col 10) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -4132,8 +4362,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountMerge(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/spark/vectorized_case.q.out b/ql/src/test/results/clientpositive/spark/vectorized_case.q.out
index ffac02c80d..66dcdad30d 100644
--- a/ql/src/test/results/clientpositive/spark/vectorized_case.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorized_case.q.out
@@ -287,8 +287,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 12) -> bigint, VectorUDAFSumLong(col 13) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1]
                       mode: hash
                       outputColumnNames: _col0, _col1
@@ -325,8 +327,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
@@ -412,8 +416,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 12) -> bigint, VectorUDAFSumLong(col 13) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1]
                       mode: hash
                       outputColumnNames: _col0, _col1
@@ -450,8 +456,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/spark/vectorized_mapjoin.q.out b/ql/src/test/results/clientpositive/spark/vectorized_mapjoin.q.out
index 030a71bb6a..9fd16c97c9 100644
--- a/ql/src/test/results/clientpositive/spark/vectorized_mapjoin.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorized_mapjoin.q.out
@@ -117,24 +117,29 @@ STAGE PLANS:
                           Group By Operator
                             aggregations: count(_col0), max(_col1), min(_col0), avg(_col2)
                             Group By Vectorization:
-                                aggregators: VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxLong(col 2) -> int, VectorUDAFMinLong(col 2) -> int, VectorUDAFAvgLong(col 12) -> struct<count:bigint,sum:double>
+                                aggregators: VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxLong(col 2) -> int, VectorUDAFMinLong(col 2) -> int, VectorUDAFAvgLong(col 12) -> struct<count:bigint,sum:double,input:bigint>
                                 className: VectorGroupByOperator
-                                vectorOutput: false
+                                groupByMode: HASH
+                                vectorOutput: true
                                 native: false
+                                vectorProcessingMode: HASH
                                 projectedOutputColumns: [0, 1, 2, 3]
-                                vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(col 12) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                             mode: hash
                             outputColumnNames: _col0, _col1, _col2, _col3
                             Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
                             Reduce Output Operator
                               sort order: 
+                              Reduce Sink Vectorization:
+                                  className: VectorReduceSinkEmptyKeyOperator
+                                  native: true
+                                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                               Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
                               value expressions: _col0 (type: bigint), _col1 (type: int), _col2 (type: int), _col3 (type: struct<count:bigint,sum:double,input:int>)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
@@ -142,19 +147,33 @@ STAGE PLANS:
             Local Work:
               Map Reduce Local Work
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:int> of Column[VALUE._col3] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), avg(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 0) -> bigint, VectorUDAFMaxLong(col 1) -> int, VectorUDAFMinLong(col 2) -> int, VectorUDAFAvgFinal(col 3) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/vectorized_ptf.q.out b/ql/src/test/results/clientpositive/spark/vectorized_ptf.q.out
index 2395091f38..4972677835 100644
--- a/ql/src/test/results/clientpositive/spark/vectorized_ptf.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorized_ptf.q.out
@@ -3323,9 +3323,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1, col 2
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: int)
                 mode: mergepartial
@@ -3461,9 +3463,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumDouble(col 7) -> double
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 2, col 3
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: p_mfgr (type: string), p_brand (type: string)
                       mode: hash
diff --git a/ql/src/test/results/clientpositive/spark/vectorized_shufflejoin.q.out b/ql/src/test/results/clientpositive/spark/vectorized_shufflejoin.q.out
index b6e7519d63..18c7db10c6 100644
--- a/ql/src/test/results/clientpositive/spark/vectorized_shufflejoin.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorized_shufflejoin.q.out
@@ -134,20 +134,35 @@ STAGE PLANS:
                       Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
                       value expressions: _col0 (type: bigint), _col1 (type: int), _col2 (type: int), _col3 (type: struct<count:bigint,sum:double,input:int>)
         Reducer 3 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:int> of Column[VALUE._col3] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), avg(VALUE._col3)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 0) -> bigint, VectorUDAFMaxLong(col 1) -> int, VectorUDAFMinLong(col 2) -> int, VectorUDAFAvgFinal(col 3) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   key expressions: _col0 (type: bigint)
                   sort order: +
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkObjectHashOperator
+                      native: true
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                   Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: double)
         Reducer 4 
diff --git a/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out b/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out
index 864b82f382..a992f41dcd 100644
--- a/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out
+++ b/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out
@@ -800,8 +800,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 0) -> timestamp, VectorUDAFCount(col 0) -> bigint, VectorUDAFCountStar(*) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
@@ -838,8 +840,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 1) -> timestamp, VectorUDAFCountMerge(col 2) -> bigint, VectorUDAFCountMerge(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -908,25 +912,47 @@ STAGE PLANS:
                 TableScan
                   alias: alltypesorc_string
                   Statistics: Num rows: 40 Data size: 84 Basic stats: COMPLETE Column stats: NONE
+                  TableScan Vectorization:
+                      native: true
+                      projectedOutputColumns: [0, 1]
                   Select Operator
                     expressions: ctimestamp1 (type: timestamp)
                     outputColumnNames: ctimestamp1
+                    Select Vectorization:
+                        className: VectorSelectOperator
+                        native: true
+                        projectedOutputColumns: [0]
                     Statistics: Num rows: 40 Data size: 84 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
                       aggregations: sum(ctimestamp1)
+                      Group By Vectorization:
+                          aggregators: VectorUDAFSumTimestamp(col 0) -> double
+                          className: VectorGroupByOperator
+                          groupByMode: HASH
+                          vectorOutput: true
+                          native: false
+                          vectorProcessingMode: HASH
+                          projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
                       Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: double)
+            Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                notVectorizedReason: Aggregation Function expression for GROUPBY operator: Vectorization of aggreation should have succeeded org.apache.hadoop.hive.ql.metadata.HiveException: Vector aggregate not implemented: "sum" for type: "TIMESTAMP (UDAF evaluator mode = PARTIAL1)
-                vectorized: false
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
         Reducer 2 
             Execution mode: vectorized
             Reduce Vectorization:
@@ -942,8 +968,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumDouble(col 0) -> double
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: GLOBAL
                     projectedOutputColumns: [0]
                 mode: mergepartial
                 outputColumnNames: _col0
@@ -1043,46 +1071,70 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: avg(ctimestamp1), variance(ctimestamp1), var_pop(ctimestamp1), var_samp(ctimestamp1), std(ctimestamp1), stddev(ctimestamp1), stddev_pop(ctimestamp1), stddev_samp(ctimestamp1)
                       Group By Vectorization:
-                          aggregators: VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>
+                          aggregators: VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double,input:timestamp>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                       Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkEmptyKeyOperator
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                         Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: struct<count:bigint,sum:double,input:timestamp>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: struct<count:bigint,sum:double,variance:double>)
             Execution mode: vectorized
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
                 vectorized: true
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:timestamp> of Column[VALUE._col0] not supported
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: true
+                vectorized: true
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0), variance(VALUE._col1), var_pop(VALUE._col2), var_samp(VALUE._col3), std(VALUE._col4), stddev(VALUE._col5), stddev_pop(VALUE._col6), stddev_samp(VALUE._col7)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 0) -> double, VectorUDAFVarPopFinal(col 1) -> double, VectorUDAFVarPopFinal(col 2) -> double, VectorUDAFVarSampFinal(col 3) -> double, VectorUDAFStdPopFinal(col 4) -> double, VectorUDAFStdPopFinal(col 5) -> double, VectorUDAFStdPopFinal(col 6) -> double, VectorUDAFStdSampFinal(col 7) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: GLOBAL
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                 Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
                   expressions: round(_col0, 0) (type: double), _col1 BETWEEN 8.97077295279421E19 AND 8.97077295279422E19 (type: boolean), _col2 BETWEEN 8.97077295279421E19 AND 8.97077295279422E19 (type: boolean), _col3 BETWEEN 9.20684592523616E19 AND 9.20684592523617E19 (type: boolean), round(_col4, 3) (type: double), round(_col5, 3) (type: double), round(_col6, 3) (type: double), round(_col7, 3) (type: double)
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [8, 9, 10, 11, 12, 13, 14, 15]
+                      selectExpressions: RoundWithNumDigitsDoubleToDouble(col 0, decimalPlaces 0) -> 8:double, VectorUDFAdaptor(_col1 BETWEEN 8.97077295279421E19 AND 8.97077295279422E19) -> 9:boolean, VectorUDFAdaptor(_col2 BETWEEN 8.97077295279421E19 AND 8.97077295279422E19) -> 10:boolean, VectorUDFAdaptor(_col3 BETWEEN 9.20684592523616E19 AND 9.20684592523617E19) -> 11:boolean, RoundWithNumDigitsDoubleToDouble(col 4, decimalPlaces 3) -> 12:double, RoundWithNumDigitsDoubleToDouble(col 5, decimalPlaces 3) -> 13:double, RoundWithNumDigitsDoubleToDouble(col 6, decimalPlaces 3) -> 14:double, RoundWithNumDigitsDoubleToDouble(col 7, decimalPlaces 3) -> 15:double
                   Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
diff --git a/ql/src/test/results/clientpositive/tez/vectorization_limit.q.out b/ql/src/test/results/clientpositive/tez/vectorization_limit.q.out
index efc2489656..afcae8c34c 100644
--- a/ql/src/test/results/clientpositive/tez/vectorization_limit.q.out
+++ b/ql/src/test/results/clientpositive/tez/vectorization_limit.q.out
@@ -258,13 +258,14 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: avg(_col1)
                       Group By Vectorization:
-                          aggregators: VectorUDAFAvgDouble(col 12) -> struct<count:bigint,sum:double>
+                          aggregators: VectorUDAFAvgDouble(col 12) -> struct<count:bigint,sum:double,input:double>
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDouble(col 12) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                       keys: _col0 (type: tinyint)
                       mode: hash
                       outputColumnNames: _col0, _col1
@@ -273,6 +274,13 @@ STAGE PLANS:
                         key expressions: _col0 (type: tinyint)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: tinyint)
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkObjectHashOperator
+                            keyColumns: [0]
+                            native: true
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            partitionColumns: [0]
+                            valueColumns: [1]
                         Statistics: Num rows: 95 Data size: 7888 Basic stats: COMPLETE Column stats: COMPLETE
                         TopN Hash Memory Usage: 0.3
                         value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>)
@@ -280,7 +288,7 @@ STAGE PLANS:
             Map Vectorization:
                 enabled: true
                 enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-                groupByVectorOutput: false
+                groupByVectorOutput: true
                 inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                 allNative: false
                 usesVectorUDFAdaptor: false
@@ -292,23 +300,47 @@ STAGE PLANS:
                     partitionColumnCount: 0
                     scratchColumnTypeNames: double
         Reducer 2 
+            Execution mode: vectorized
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Aggregation Function UDF avg parameter expression for GROUPBY operator: Data type struct<count:bigint,sum:double,input:double> of Column[VALUE._col0] not supported
-                vectorized: false
+                reduceColumnNullOrder: a
+                reduceColumnSortOrder: +
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 2
+                    dataColumns: KEY._col0:tinyint, VALUE._col0:struct<count:bigint,sum:double,input:double>
+                    partitionColumnCount: 0
             Reduce Operator Tree:
               Group By Operator
                 aggregations: avg(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFAvgFinal(col 1) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: MERGE_PARTIAL
+                    projectedOutputColumns: [0]
                 keys: KEY._col0 (type: tinyint)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 95 Data size: 1048 Basic stats: COMPLETE Column stats: COMPLETE
                 Limit
                   Number of rows: 20
+                  Limit Vectorization:
+                      className: VectorLimitOperator
+                      native: true
                   Statistics: Num rows: 20 Data size: 224 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 20 Data size: 224 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -390,9 +422,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: ctinyint (type: tinyint)
                       mode: hash
@@ -443,9 +477,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: tinyint)
                 mode: mergepartial
@@ -543,9 +579,11 @@ STAGE PLANS:
                     Group By Operator
                       Group By Vectorization:
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 0, col 5
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: []
                       keys: ctinyint (type: tinyint), cdouble (type: double)
                       mode: hash
@@ -596,9 +634,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0, col 1
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: []
                 keys: KEY._col0 (type: tinyint), KEY._col1 (type: double)
                 mode: mergepartial
@@ -609,9 +649,11 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFCount(col 1) -> bigint
                       className: VectorGroupByOperator
+                      groupByMode: COMPLETE
                       vectorOutput: true
                       keyExpressions: col 0
                       native: false
+                      vectorProcessingMode: STREAMING
                       projectedOutputColumns: [0]
                   keys: _col0 (type: tinyint)
                   mode: complete
@@ -739,9 +781,11 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 0) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           keyExpressions: col 5
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       keys: cdouble (type: double)
                       mode: hash
@@ -794,9 +838,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 1) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: MERGEPARTIAL
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: MERGE_PARTIAL
                     projectedOutputColumns: [0]
                 keys: KEY._col0 (type: double)
                 mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_aggregate_9.q.out b/ql/src/test/results/clientpositive/vector_aggregate_9.q.out
index 3ad29ef455..0f4855cefd 100644
--- a/ql/src/test/results/clientpositive/vector_aggregate_9.q.out
+++ b/ql/src/test/results/clientpositive/vector_aggregate_9.q.out
@@ -101,10 +101,10 @@ POSTHOOK: Lineage: vectortab2korc.si SIMPLE [(vectortab2k)vectortab2k.FieldSchem
 POSTHOOK: Lineage: vectortab2korc.t SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:t, type:tinyint, comment:null), ]
 POSTHOOK: Lineage: vectortab2korc.ts SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:ts, type:timestamp, comment:null), ]
 POSTHOOK: Lineage: vectortab2korc.ts2 SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:ts2, type:timestamp, comment:null), ]
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
@@ -136,28 +136,39 @@ STAGE PLANS:
               Group By Operator
                 aggregations: min(dc), max(dc), sum(dc), avg(dc)
                 Group By Vectorization:
-                    aggregators: VectorUDAFMinDecimal(col 6) -> decimal(38,18), VectorUDAFMaxDecimal(col 6) -> decimal(38,18), VectorUDAFSumDecimal(col 6) -> decimal(38,18), VectorUDAFAvgDecimal(col 6) -> struct<count:bigint,sum:decimal(38,18)>
+                    aggregators: VectorUDAFMinDecimal(col 6) -> decimal(38,18), VectorUDAFMaxDecimal(col 6) -> decimal(38,18), VectorUDAFSumDecimal(col 6) -> decimal(38,18), VectorUDAFAvgDecimal(col 6) -> struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>
                     className: VectorGroupByOperator
-                    vectorOutput: false
+                    groupByMode: HASH
+                    vectorOutput: true
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2, 3]
-                    vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDecimal(col 6) -> struct<count:bigint,sum:decimal(38,18)> output type STRUCT requires PRIMITIVE IS false
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   sort order: 
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col0 (type: decimal(38,18)), _col1 (type: decimal(38,18)), _col2 (type: decimal(38,18)), _col3 (type: struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 13
+              includeColumns: [6]
+              dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -166,8 +177,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), avg(VALUE._col3)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3
@@ -195,3 +208,217 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@vectortab2korc
 #### A masked pattern was here ####
 -4997414117561.546875000000000000	4994550248722.298828000000000000	-10252745435816.024410000000000000	-5399023399.587163986308583465
+PREHOOK: query: explain vectorization detail
+select min(d), max(d), sum(d), avg(d) from vectortab2korc
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization detail
+select min(d), max(d), sum(d), avg(d) from vectortab2korc
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: vectortab2korc
+            Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
+            Select Operator
+              expressions: d (type: double)
+              outputColumnNames: d
+              Select Vectorization:
+                  className: VectorSelectOperator
+                  native: true
+                  projectedOutputColumns: [5]
+              Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                aggregations: min(d), max(d), sum(d), avg(d)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMinDouble(col 5) -> double, VectorUDAFMaxDouble(col 5) -> double, VectorUDAFSumDouble(col 5) -> double, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>
+                    className: VectorGroupByOperator
+                    groupByMode: HASH
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: HASH
+                    projectedOutputColumns: [0, 1, 2, 3]
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  sort order: 
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col0 (type: double), _col1 (type: double), _col2 (type: double), _col3 (type: struct<count:bigint,sum:double,input:double>)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 13
+              includeColumns: [5]
+              dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
+              partitionColumnCount: 0
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), avg(VALUE._col3)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3
+          Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select min(d), max(d), sum(d), avg(d) from vectortab2korc
+PREHOOK: type: QUERY
+PREHOOK: Input: default@vectortab2korc
+#### A masked pattern was here ####
+POSTHOOK: query: select min(d), max(d), sum(d), avg(d) from vectortab2korc
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@vectortab2korc
+#### A masked pattern was here ####
+-4999829.07	4997627.14	-1.7516847286999977E8	-92193.93308947356
+PREHOOK: query: explain vectorization detail
+select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
+PREHOOK: type: QUERY
+POSTHOOK: query: explain vectorization detail
+select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: vectortab2korc
+            Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
+            Select Operator
+              expressions: ts (type: timestamp)
+              outputColumnNames: ts
+              Select Vectorization:
+                  className: VectorSelectOperator
+                  native: true
+                  projectedOutputColumns: [10]
+              Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                aggregations: min(ts), max(ts), sum(ts), avg(ts)
+                Group By Vectorization:
+                    aggregators: VectorUDAFMinTimestamp(col 10) -> timestamp, VectorUDAFMaxTimestamp(col 10) -> timestamp, VectorUDAFSumTimestamp(col 10) -> double, VectorUDAFAvgTimestamp(col 10) -> struct<count:bigint,sum:double,input:timestamp>
+                    className: VectorGroupByOperator
+                    groupByMode: HASH
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: HASH
+                    projectedOutputColumns: [0, 1, 2, 3]
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  sort order: 
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                  Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col0 (type: timestamp), _col1 (type: timestamp), _col2 (type: double), _col3 (type: struct<count:bigint,sum:double,input:timestamp>)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 13
+              includeColumns: [10]
+              dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
+              partitionColumnCount: 0
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), avg(VALUE._col3)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3
+          Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
+PREHOOK: type: QUERY
+PREHOOK: Input: default@vectortab2korc
+#### A masked pattern was here ####
+POSTHOOK: query: select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@vectortab2korc
+#### A masked pattern was here ####
+2013-02-18 21:06:48	2081-02-22 01:21:53	4.591384881081E12	2.4254542425150557E9
diff --git a/ql/src/test/results/clientpositive/vector_binary_join_groupby.q.out b/ql/src/test/results/clientpositive/vector_binary_join_groupby.q.out
index 3519a87044..e234c0a411 100644
--- a/ql/src/test/results/clientpositive/vector_binary_join_groupby.q.out
+++ b/ql/src/test/results/clientpositive/vector_binary_join_groupby.q.out
@@ -190,8 +190,10 @@ STAGE PLANS:
                       Group By Vectorization:
                           aggregators: VectorUDAFSumLong(col 22) -> bigint
                           className: VectorGroupByOperator
+                          groupByMode: HASH
                           vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0]
                       mode: hash
                       outputColumnNames: _col0
@@ -224,8 +226,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: sum(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0
@@ -342,9 +346,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountStar(*) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 10
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0]
                 keys: bin (type: binary)
                 mode: hash
@@ -378,8 +384,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: binary)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_cast_constant.q.out b/ql/src/test/results/clientpositive/vector_cast_constant.q.out
index 7afdb721de..3cd708b5f8 100644
--- a/ql/src/test/results/clientpositive/vector_cast_constant.q.out
+++ b/ql/src/test/results/clientpositive/vector_cast_constant.q.out
@@ -139,13 +139,14 @@ STAGE PLANS:
               Group By Operator
                 aggregations: avg(50), avg(50.0), avg(50)
                 Group By Vectorization:
-                    aggregators: VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 11:long) -> struct<count:bigint,sum:double>, VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 12:double) -> struct<count:bigint,sum:double>, VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 13:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0)>
+                    aggregators: VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 11:long) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 12:double) -> struct<count:bigint,sum:double,input:double>, VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 13:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0),input:decimal(20,0)>
                     className: VectorGroupByOperator
-                    vectorOutput: false
+                    groupByMode: HASH
+                    vectorOutput: true
                     keyExpressions: col 2
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2]
-                    vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 11:long) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 12:double) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 13:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0)> output type STRUCT requires PRIMITIVE IS false
                 keys: _col0 (type: int)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -154,6 +155,11 @@ STAGE PLANS:
                   key expressions: _col0 (type: int)
                   sort order: +
                   Map-reduce partition columns: _col0 (type: int)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                   TopN Hash Memory Usage: 0.1
                   value expressions: _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: struct<count:bigint,sum:double,input:double>), _col3 (type: struct<count:bigint,sum:decimal(12,0),input:decimal(10,0)>)
@@ -161,7 +167,7 @@ STAGE PLANS:
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
@@ -174,8 +180,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: avg(VALUE._col0), avg(VALUE._col1), avg(VALUE._col2)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_char_2.q.out b/ql/src/test/results/clientpositive/vector_char_2.q.out
index 03bf43688d..26dfad1a16 100644
--- a/ql/src/test/results/clientpositive/vector_char_2.q.out
+++ b/ql/src/test/results/clientpositive/vector_char_2.q.out
@@ -92,9 +92,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 2) -> bigint, VectorUDAFCountStar(*) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 1
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1]
                 keys: _col0 (type: char(20))
                 mode: hash
@@ -129,8 +131,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: sum(VALUE._col0), count(VALUE._col1)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: char(20))
           mode: mergepartial
@@ -283,9 +287,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 2) -> bigint, VectorUDAFCountStar(*) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 1
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1]
                 keys: _col0 (type: char(20))
                 mode: hash
@@ -320,8 +326,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: sum(VALUE._col0), count(VALUE._col1)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: char(20))
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_coalesce_2.q.out b/ql/src/test/results/clientpositive/vector_coalesce_2.q.out
index 431cfdc206..336ae040ab 100644
--- a/ql/src/test/results/clientpositive/vector_coalesce_2.q.out
+++ b/ql/src/test/results/clientpositive/vector_coalesce_2.q.out
@@ -48,8 +48,10 @@ STAGE PLANS:
               Group By Operator
                 aggregations: sum(_col1)
                 Group By Vectorization:
+                    groupByMode: HASH
                     vectorOutput: false
                     native: false
+                    vectorProcessingMode: NONE
                     projectedOutputColumns: null
                 keys: _col0 (type: string)
                 mode: hash
@@ -65,8 +67,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: sum(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string)
           mode: mergepartial
@@ -205,9 +209,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 4) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 1
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0]
                 keys: _col0 (type: string)
                 mode: hash
@@ -241,8 +247,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: sum(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_complex_all.q.out b/ql/src/test/results/clientpositive/vector_complex_all.q.out
index e6f0307903..7125df87b9 100644
--- a/ql/src/test/results/clientpositive/vector_complex_all.q.out
+++ b/ql/src/test/results/clientpositive/vector_complex_all.q.out
@@ -75,7 +75,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
 orc_create_complex.str	orc_create_complex.mp	orc_create_complex.lst	orc_create_complex.strct	orc_create_complex.val
-line1	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]	{"a":"one","b":"two"}	0
+line1	{"key11":"value11","key12":"value12","key13":"value13"}	["a","b","c"]	{"a":"one","b":"two"}	0
 line2	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]	{"a":"three","b":"four"}	0
 line3	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]	{"a":"five","b":"six"}	0
 PREHOOK: query: SELECT str FROM orc_create_complex
@@ -99,7 +99,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
 strct	mp	lst
-{"a":"one","b":"two"}	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]
+{"a":"one","b":"two"}	{"key11":"value11","key12":"value12","key13":"value13"}	["a","b","c"]
 {"a":"three","b":"four"}	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]
 {"a":"five","b":"six"}	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]
 PREHOOK: query: SELECT lst, str FROM orc_create_complex
@@ -123,7 +123,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
 mp	str
-{"key13":"value13","key11":"value11","key12":"value12"}	line1
+{"key11":"value11","key12":"value12","key13":"value13"}	line1
 {"key21":"value21","key22":"value22","key23":"value23"}	line2
 {"key31":"value31","key32":"value32","key33":"value33"}	line3
 PREHOOK: query: SELECT strct, str FROM orc_create_complex
diff --git a/ql/src/test/results/clientpositive/vector_complex_join.q.out b/ql/src/test/results/clientpositive/vector_complex_join.q.out
index 513c1592ba..dfc30e4fdf 100644
--- a/ql/src/test/results/clientpositive/vector_complex_join.q.out
+++ b/ql/src/test/results/clientpositive/vector_complex_join.q.out
@@ -63,12 +63,23 @@ STAGE PLANS:
           TableScan
             alias: alltypesorc
             Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
             Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: SelectColumnIsNotNull(col 2) -> boolean
               predicate: cint is not null (type: boolean)
               Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: ctinyint (type: tinyint), csmallint (type: smallint), cint (type: int), cbigint (type: bigint), cfloat (type: float), cdouble (type: double), cstring1 (type: string), cstring2 (type: string), ctimestamp1 (type: timestamp), ctimestamp2 (type: timestamp), cboolean1 (type: boolean), cboolean2 (type: boolean)
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
                 Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
                 Map Join Operator
                   condition map:
@@ -76,21 +87,32 @@ STAGE PLANS:
                   keys:
                     0 _col2 (type: int)
                     1 _col0 (type: int)
+                  Map Join Vectorization:
+                      className: VectorMapJoinOperator
+                      native: false
+                      nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true, Optimized Table and Supports Key Types IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
                   Statistics: Num rows: 13516 Data size: 2906160 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 13516 Data size: 2906160 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+      Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-          notVectorizedReason: Small Table expression for MAPJOIN operator: Data type map<int,string> of Column[_col1] not supported
-          vectorized: false
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
       Local Work:
         Map Reduce Local Work
 
@@ -214,7 +236,7 @@ STAGE PLANS:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-          notVectorizedReason: Predicate expression for FILTER operator: Data type array<int> of Column[a] not supported
+          notVectorizedReason: Predicate expression for FILTER operator: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected hive type name array<int>
           vectorized: false
       Local Work:
         Map Reduce Local Work
diff --git a/ql/src/test/results/clientpositive/vector_count.q.out b/ql/src/test/results/clientpositive/vector_count.q.out
index ff6993ef3b..1068c785dd 100644
--- a/ql/src/test/results/clientpositive/vector_count.q.out
+++ b/ql/src/test/results/clientpositive/vector_count.q.out
@@ -78,9 +78,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 2) -> bigint, VectorUDAFSumLong(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 0, col 1, col 2
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2]
                 keys: a (type: int), b (type: int), c (type: int)
                 mode: hash
@@ -114,8 +116,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(DISTINCT KEY._col1:0._col0), count(DISTINCT KEY._col1:1._col0), sum(VALUE._col2)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: mergepartial
@@ -182,9 +186,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 4:long) -> bigint, VectorUDAFCountStar(*) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 2) -> bigint, VectorUDAFCount(col 3) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 2) -> bigint, VectorUDAFCount(col 3) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 2) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 1) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 0) -> bigint, VectorUDAFCount(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 0, col 1, col 2, col 3
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
                 keys: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)
                 mode: hash
@@ -217,8 +223,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0), count(VALUE._col1), count(VALUE._col2), count(VALUE._col3), count(VALUE._col4), count(VALUE._col5), count(DISTINCT KEY._col0:0._col0), count(DISTINCT KEY._col0:1._col0), count(DISTINCT KEY._col0:2._col0), count(DISTINCT KEY._col0:3._col0), count(DISTINCT KEY._col0:4._col0, KEY._col0:4._col1), count(DISTINCT KEY._col0:5._col0, KEY._col0:5._col1), count(DISTINCT KEY._col0:6._col0, KEY._col0:6._col1), count(DISTINCT KEY._col0:7._col0, KEY._col0:7._col1), count(DISTINCT KEY._col0:8._col0, KEY._col0:8._col1), count(DISTINCT KEY._col0:9._col0, KEY._col0:9._col1), count(DISTINCT KEY._col0:10._col0, KEY._col0:10._col1, KEY._col0:10._col2), count(DISTINCT KEY._col0:11._col0, KEY._col0:11._col1, KEY._col0:11._col2), count(DISTINCT KEY._col0:12._col0, KEY._col0:12._col1, KEY._col0:12._col2), count(DISTINCT KEY._col0:13._col0, KEY._col0:13._col1, KEY._col0:13._col2), count(DISTINCT KEY._col0:14._col0, KEY._col0:14._col1, KEY._col0:14._col2, KEY._col0:14._col3)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
@@ -304,8 +312,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(DISTINCT KEY._col1:0._col0), count(DISTINCT KEY._col1:1._col0), sum(VALUE._col0)
           Group By Vectorization:
+              groupByMode: COMPLETE
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: complete
@@ -393,8 +403,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(1), count(), count(KEY._col0:0._col0), count(KEY._col0:1._col0), count(KEY._col0:2._col0), count(KEY._col0:3._col0), count(DISTINCT KEY._col0:0._col0), count(DISTINCT KEY._col0:1._col0), count(DISTINCT KEY._col0:2._col0), count(DISTINCT KEY._col0:3._col0), count(DISTINCT KEY._col0:4._col0, KEY._col0:4._col1), count(DISTINCT KEY._col0:5._col0, KEY._col0:5._col1), count(DISTINCT KEY._col0:6._col0, KEY._col0:6._col1), count(DISTINCT KEY._col0:7._col0, KEY._col0:7._col1), count(DISTINCT KEY._col0:8._col0, KEY._col0:8._col1), count(DISTINCT KEY._col0:9._col0, KEY._col0:9._col1), count(DISTINCT KEY._col0:10._col0, KEY._col0:10._col1, KEY._col0:10._col2), count(DISTINCT KEY._col0:11._col0, KEY._col0:11._col1, KEY._col0:11._col2), count(DISTINCT KEY._col0:12._col0, KEY._col0:12._col1, KEY._col0:12._col2), count(DISTINCT KEY._col0:13._col0, KEY._col0:13._col1, KEY._col0:13._col2), count(DISTINCT KEY._col0:14._col0, KEY._col0:14._col1, KEY._col0:14._col2, KEY._col0:14._col3)
           Group By Vectorization:
+              groupByMode: COMPLETE
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: complete
           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
diff --git a/ql/src/test/results/clientpositive/vector_decimal_aggregate.q.out b/ql/src/test/results/clientpositive/vector_decimal_aggregate.q.out
index 34c60c0c8a..04c90a2334 100644
--- a/ql/src/test/results/clientpositive/vector_decimal_aggregate.q.out
+++ b/ql/src/test/results/clientpositive/vector_decimal_aggregate.q.out
@@ -65,9 +65,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFMaxDecimal(col 1) -> decimal(20,10), VectorUDAFMinDecimal(col 1) -> decimal(20,10), VectorUDAFSumDecimal(col 1) -> decimal(38,18), VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(23,14), VectorUDAFMinDecimal(col 2) -> decimal(23,14), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFCountStar(*) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 3
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8]
                 keys: cint (type: int)
                 mode: hash
@@ -101,8 +103,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), sum(VALUE._col3), count(VALUE._col4), max(VALUE._col5), min(VALUE._col6), sum(VALUE._col7), count(VALUE._col8)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: mergepartial
@@ -198,13 +202,14 @@ STAGE PLANS:
               Group By Operator
                 aggregations: count(cdecimal1), max(cdecimal1), min(cdecimal1), sum(cdecimal1), avg(cdecimal1), stddev_pop(cdecimal1), stddev_samp(cdecimal1), count(cdecimal2), max(cdecimal2), min(cdecimal2), sum(cdecimal2), avg(cdecimal2), stddev_pop(cdecimal2), stddev_samp(cdecimal2), count()
                 Group By Vectorization:
-                    aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFMaxDecimal(col 1) -> decimal(20,10), VectorUDAFMinDecimal(col 1) -> decimal(20,10), VectorUDAFSumDecimal(col 1) -> decimal(38,18), VectorUDAFAvgDecimal(col 1) -> struct<count:bigint,sum:decimal(30,10)>, VectorUDAFStdPopDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(23,14), VectorUDAFMinDecimal(col 2) -> decimal(23,14), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFAvgDecimal(col 2) -> struct<count:bigint,sum:decimal(33,14)>, VectorUDAFStdPopDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint
+                    aggregators: VectorUDAFCount(col 1) -> bigint, VectorUDAFMaxDecimal(col 1) -> decimal(20,10), VectorUDAFMinDecimal(col 1) -> decimal(20,10), VectorUDAFSumDecimal(col 1) -> decimal(38,18), VectorUDAFAvgDecimal(col 1) -> struct<count:bigint,sum:decimal(30,10),input:decimal(30,10)>, VectorUDAFStdPopDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 2) -> bigint, VectorUDAFMaxDecimal(col 2) -> decimal(23,14), VectorUDAFMinDecimal(col 2) -> decimal(23,14), VectorUDAFSumDecimal(col 2) -> decimal(38,18), VectorUDAFAvgDecimal(col 2) -> struct<count:bigint,sum:decimal(33,14),input:decimal(33,14)>, VectorUDAFStdPopDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDecimal(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint
                     className: VectorGroupByOperator
-                    vectorOutput: false
+                    groupByMode: HASH
+                    vectorOutput: true
                     keyExpressions: col 3
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
-                    vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDecimal(col 1) -> struct<count:bigint,sum:decimal(30,10)> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDecimal(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampDecimal(col 1) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFAvgDecimal(col 2) -> struct<count:bigint,sum:decimal(33,14)> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDecimal(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampDecimal(col 2) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                 keys: cint (type: int)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
@@ -213,13 +218,18 @@ STAGE PLANS:
                   key expressions: _col0 (type: int)
                   sort order: +
                   Map-reduce partition columns: _col0 (type: int)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 12288 Data size: 2165060 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col1 (type: bigint), _col2 (type: decimal(20,10)), _col3 (type: decimal(20,10)), _col4 (type: decimal(30,10)), _col5 (type: struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)>), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: bigint), _col9 (type: decimal(23,14)), _col10 (type: decimal(23,14)), _col11 (type: decimal(33,14)), _col12 (type: struct<count:bigint,sum:decimal(33,14),input:decimal(23,14)>), _col13 (type: struct<count:bigint,sum:double,variance:double>), _col14 (type: struct<count:bigint,sum:double,variance:double>), _col15 (type: bigint)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
@@ -232,8 +242,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), sum(VALUE._col3), avg(VALUE._col4), stddev_pop(VALUE._col5), stddev_samp(VALUE._col6), count(VALUE._col7), max(VALUE._col8), min(VALUE._col9), sum(VALUE._col10), avg(VALUE._col11), stddev_pop(VALUE._col12), stddev_samp(VALUE._col13), count(VALUE._col14)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_decimal_precision.q.out b/ql/src/test/results/clientpositive/vector_decimal_precision.q.out
index 690441f3cc..0dc5a6733c 100644
--- a/ql/src/test/results/clientpositive/vector_decimal_precision.q.out
+++ b/ql/src/test/results/clientpositive/vector_decimal_precision.q.out
@@ -578,24 +578,30 @@ STAGE PLANS:
               Group By Operator
                 aggregations: avg(dec), sum(dec)
                 Group By Vectorization:
-                    aggregators: VectorUDAFAvgDecimal(col 0) -> struct<count:bigint,sum:decimal(30,10)>, VectorUDAFSumDecimal(col 0) -> decimal(38,18)
+                    aggregators: VectorUDAFAvgDecimal(col 0) -> struct<count:bigint,sum:decimal(30,10),input:decimal(30,10)>, VectorUDAFSumDecimal(col 0) -> decimal(38,18)
                     className: VectorGroupByOperator
-                    vectorOutput: false
+                    groupByMode: HASH
+                    vectorOutput: true
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1]
-                    vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDecimal(col 0) -> struct<count:bigint,sum:decimal(30,10)> output type STRUCT requires PRIMITIVE IS false
                 mode: hash
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   sort order: 
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col0 (type: struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)>), _col1 (type: decimal(30,10))
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
@@ -608,8 +614,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: avg(VALUE._col0), sum(VALUE._col1)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/vector_distinct_2.q.out b/ql/src/test/results/clientpositive/vector_distinct_2.q.out
index b6e9527aa3..db688bfdde 100644
--- a/ql/src/test/results/clientpositive/vector_distinct_2.q.out
+++ b/ql/src/test/results/clientpositive/vector_distinct_2.q.out
@@ -136,9 +136,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 0, col 8
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: t (type: tinyint), s (type: string)
                 mode: hash
@@ -170,8 +172,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: tinyint), KEY._col1 (type: string)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_empty_where.q.out b/ql/src/test/results/clientpositive/vector_empty_where.q.out
index b7580f34b5..a95fdf6e85 100644
--- a/ql/src/test/results/clientpositive/vector_empty_where.q.out
+++ b/ql/src/test/results/clientpositive/vector_empty_where.q.out
@@ -41,9 +41,11 @@ STAGE PLANS:
                 Group By Operator
                   Group By Vectorization:
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       keyExpressions: col 2
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: []
                   keys: cint (type: int)
                   mode: hash
@@ -75,8 +77,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: PARTIAL2
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: partial2
@@ -85,8 +89,10 @@ STAGE PLANS:
           Group By Operator
             aggregations: count(_col0)
             Group By Vectorization:
+                groupByMode: PARTIAL2
                 vectorOutput: false
                 native: false
+                vectorProcessingMode: NONE
                 projectedOutputColumns: null
             mode: partial2
             outputColumnNames: _col0
@@ -131,8 +137,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0
@@ -195,9 +203,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 2
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: cint (type: int)
                 mode: hash
@@ -229,8 +239,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: PARTIAL2
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: partial2
@@ -239,8 +251,10 @@ STAGE PLANS:
           Group By Operator
             aggregations: count(_col0)
             Group By Vectorization:
+                groupByMode: PARTIAL2
                 vectorOutput: false
                 native: false
+                vectorProcessingMode: NONE
                 projectedOutputColumns: null
             mode: partial2
             outputColumnNames: _col0
@@ -285,8 +299,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0
@@ -357,9 +373,11 @@ STAGE PLANS:
                 Group By Operator
                   Group By Vectorization:
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       keyExpressions: col 2
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: []
                   keys: cint (type: int)
                   mode: hash
@@ -391,8 +409,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: PARTIAL2
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: partial2
@@ -401,8 +421,10 @@ STAGE PLANS:
           Group By Operator
             aggregations: count(_col0)
             Group By Vectorization:
+                groupByMode: PARTIAL2
                 vectorOutput: false
                 native: false
+                vectorProcessingMode: NONE
                 projectedOutputColumns: null
             mode: partial2
             outputColumnNames: _col0
@@ -447,8 +469,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0
@@ -519,9 +543,11 @@ STAGE PLANS:
                 Group By Operator
                   Group By Vectorization:
                       className: VectorGroupByOperator
+                      groupByMode: HASH
                       vectorOutput: true
                       keyExpressions: col 2
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: []
                   keys: cint (type: int)
                   mode: hash
@@ -553,8 +579,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: PARTIAL2
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: partial2
@@ -563,8 +591,10 @@ STAGE PLANS:
           Group By Operator
             aggregations: count(_col0)
             Group By Vectorization:
+                groupByMode: PARTIAL2
                 vectorOutput: false
                 native: false
+                vectorProcessingMode: NONE
                 projectedOutputColumns: null
             mode: partial2
             outputColumnNames: _col0
@@ -609,8 +639,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/vector_groupby4.q.out b/ql/src/test/results/clientpositive/vector_groupby4.q.out
index 9de8e6eea7..34b571e32d 100644
--- a/ql/src/test/results/clientpositive/vector_groupby4.q.out
+++ b/ql/src/test/results/clientpositive/vector_groupby4.q.out
@@ -81,8 +81,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: PARTIAL1
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string)
           mode: partial1
@@ -128,8 +130,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: FINAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string)
           mode: final
diff --git a/ql/src/test/results/clientpositive/vector_groupby6.q.out b/ql/src/test/results/clientpositive/vector_groupby6.q.out
index 25cf5b2a4f..bc86c15137 100644
--- a/ql/src/test/results/clientpositive/vector_groupby6.q.out
+++ b/ql/src/test/results/clientpositive/vector_groupby6.q.out
@@ -81,8 +81,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: PARTIAL1
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string)
           mode: partial1
@@ -128,8 +130,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: FINAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string)
           mode: final
diff --git a/ql/src/test/results/clientpositive/vector_groupby_3.q.out b/ql/src/test/results/clientpositive/vector_groupby_3.q.out
index 9a1256b8d9..d360e44385 100644
--- a/ql/src/test/results/clientpositive/vector_groupby_3.q.out
+++ b/ql/src/test/results/clientpositive/vector_groupby_3.q.out
@@ -138,9 +138,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMaxLong(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 0, col 8
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0]
                 keys: t (type: tinyint), s (type: string)
                 mode: hash
@@ -174,8 +176,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: max(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: tinyint), KEY._col1 (type: string)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_groupby_mapjoin.q.out b/ql/src/test/results/clientpositive/vector_groupby_mapjoin.q.out
index f0c87c47da..17ebb087e6 100644
--- a/ql/src/test/results/clientpositive/vector_groupby_mapjoin.q.out
+++ b/ql/src/test/results/clientpositive/vector_groupby_mapjoin.q.out
@@ -52,8 +52,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCountStar(*) -> bigint, VectorUDAFCount(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1]
                 mode: hash
                 outputColumnNames: _col0, _col1
@@ -84,8 +86,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0), count(VALUE._col1)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1
@@ -334,9 +338,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: key (type: string)
                 mode: hash
@@ -368,8 +374,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_groupby_reduce.q.out b/ql/src/test/results/clientpositive/vector_groupby_reduce.q.out
index bc59510121..5fb42b142c 100644
--- a/ql/src/test/results/clientpositive/vector_groupby_reduce.q.out
+++ b/ql/src/test/results/clientpositive/vector_groupby_reduce.q.out
@@ -11,20 +11,20 @@ PREHOOK: query: create table store_sales_txt
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float                  
+    ss_wholesale_cost         double,
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
-row format delimited fields terminated by '|' 
+row format delimited fields terminated by '|'
 stored as textfile
 PREHOOK: type: CREATETABLE
 PREHOOK: Output: database:default
@@ -42,20 +42,20 @@ POSTHOOK: query: create table store_sales_txt
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float                  
+    ss_wholesale_cost         double,
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
-row format delimited fields terminated by '|' 
+row format delimited fields terminated by '|'
 stored as textfile
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: database:default
@@ -81,18 +81,19 @@ PREHOOK: query: create table store_sales
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float
+    ss_wholesale_cost         double,
+    ss_wholesale_cost_decimal     decimal(38,18),
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
 stored as orc
 tblproperties ("orc.stripe.size"="33554432", "orc.compress.size"="16384")
@@ -112,18 +113,19 @@ POSTHOOK: query: create table store_sales
     ss_promo_sk               int,
     ss_ticket_number          int,
     ss_quantity               int,
-    ss_wholesale_cost         float,
-    ss_list_price             float,
-    ss_sales_price            float,
-    ss_ext_discount_amt       float,
-    ss_ext_sales_price        float,
-    ss_ext_wholesale_cost     float,
-    ss_ext_list_price         float,
-    ss_ext_tax                float,
-    ss_coupon_amt             float,
-    ss_net_paid               float,
-    ss_net_paid_inc_tax       float,
-    ss_net_profit             float
+    ss_wholesale_cost         double,
+    ss_wholesale_cost_decimal     decimal(38,18),
+    ss_list_price             double,
+    ss_sales_price            double,
+    ss_ext_discount_amt       double,
+    ss_ext_sales_price        double,
+    ss_ext_wholesale_cost     double,
+    ss_ext_list_price         double,
+    ss_ext_tax                double,
+    ss_coupon_amt             double,
+    ss_net_paid               double,
+    ss_net_paid_inc_tax       double,
+    ss_net_profit             double
 )
 stored as orc
 tblproperties ("orc.stripe.size"="33554432", "orc.compress.size"="16384")
@@ -144,6 +146,7 @@ ss_sold_date_sk           ,
     ss_ticket_number      ,
     ss_quantity           ,
     ss_wholesale_cost     ,
+    cast(ss_wholesale_cost as decimal(38,18)),
     ss_list_price         ,
     ss_sales_price        ,
     ss_ext_discount_amt   ,
@@ -173,6 +176,7 @@ ss_sold_date_sk           ,
     ss_ticket_number      ,
     ss_quantity           ,
     ss_wholesale_cost     ,
+    cast(ss_wholesale_cost as decimal(38,18)),
     ss_list_price         ,
     ss_sales_price        ,
     ss_ext_discount_amt   ,
@@ -190,27 +194,28 @@ POSTHOOK: Input: default@store_sales_txt
 POSTHOOK: Output: default@store_sales
 POSTHOOK: Lineage: store_sales.ss_addr_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_addr_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_cdemo_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_cdemo_sk, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_coupon_amt SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_coupon_amt, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_coupon_amt SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_coupon_amt, type:double, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_customer_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_customer_sk, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_discount_amt SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_discount_amt, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_list_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_list_price, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_sales_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_sales_price, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_tax SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_tax, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_ext_wholesale_cost SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_wholesale_cost, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_discount_amt SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_discount_amt, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_list_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_list_price, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_sales_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_sales_price, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_tax SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_tax, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_ext_wholesale_cost SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ext_wholesale_cost, type:double, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_hdemo_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_hdemo_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_item_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_item_sk, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_list_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_list_price, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_net_paid SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_paid, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_net_paid_inc_tax SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_net_profit SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_list_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_list_price, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_net_paid SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_paid, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_net_paid_inc_tax SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_paid_inc_tax, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_net_profit SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_net_profit, type:double, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_promo_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_promo_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_quantity SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_quantity, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_sales_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_sales_price, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_sales_price SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_sales_price, type:double, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_sold_date_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_sold_date_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_sold_time_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_sold_time_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_store_sk SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_store_sk, type:int, comment:null), ]
 POSTHOOK: Lineage: store_sales.ss_ticket_number SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_ticket_number, type:int, comment:null), ]
-POSTHOOK: Lineage: store_sales.ss_wholesale_cost SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_wholesale_cost, type:float, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_wholesale_cost SIMPLE [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_wholesale_cost, type:double, comment:null), ]
+POSTHOOK: Lineage: store_sales.ss_wholesale_cost_decimal EXPRESSION [(store_sales_txt)store_sales_txt.FieldSchema(name:ss_wholesale_cost, type:double, comment:null), ]
 PREHOOK: query: explain vectorization expression
 select 
   ss_ticket_number
@@ -244,10 +249,10 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: store_sales
-            Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
             TableScan Vectorization:
                 native: true
-                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
             Select Operator
               expressions: ss_ticket_number (type: int)
               outputColumnNames: ss_ticket_number
@@ -255,18 +260,20 @@ STAGE PLANS:
                   className: VectorSelectOperator
                   native: true
                   projectedOutputColumns: [9]
-              Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 9
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: ss_ticket_number (type: int)
                 mode: hash
                 outputColumnNames: _col0
-                Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   key expressions: _col0 (type: int)
                   sort order: +
@@ -276,7 +283,7 @@ STAGE PLANS:
                       native: false
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                       nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
-                  Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                   TopN Hash Memory Usage: 0.1
       Execution mode: vectorized
       Map Vectorization:
@@ -294,13 +301,15 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: mergepartial
           outputColumnNames: _col0
-          Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+          Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
           File Output Operator
             compressed: false
             table:
@@ -323,7 +332,7 @@ STAGE PLANS:
                   native: false
                   nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                   nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
-              Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
               TopN Hash Memory Usage: 0.1
       Execution mode: vectorized
       Map Vectorization:
@@ -342,13 +351,13 @@ STAGE PLANS:
         Select Operator
           expressions: KEY.reducesinkkey0 (type: int)
           outputColumnNames: _col0
-          Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+          Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
           Limit
             Number of rows: 20
-            Statistics: Num rows: 20 Data size: 1760 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 20 Data size: 4820 Basic stats: COMPLETE Column stats: NONE
             File Output Operator
               compressed: false
-              Statistics: Num rows: 20 Data size: 1760 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 20 Data size: 4820 Basic stats: COMPLETE Column stats: NONE
               table:
                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -439,10 +448,10 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: store_sales
-            Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
             TableScan Vectorization:
                 native: true
-                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
             Select Operator
               expressions: ss_ticket_number (type: int)
               outputColumnNames: ss_ticket_number
@@ -450,18 +459,20 @@ STAGE PLANS:
                   className: VectorSelectOperator
                   native: true
                   projectedOutputColumns: [9]
-              Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 9
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: ss_ticket_number (type: int)
                 mode: hash
                 outputColumnNames: _col0
-                Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   key expressions: _col0 (type: int)
                   sort order: +
@@ -471,7 +482,7 @@ STAGE PLANS:
                       native: false
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                       nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
-                  Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
@@ -488,27 +499,31 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: mergepartial
           outputColumnNames: _col0
-          Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+          Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
           Group By Operator
             aggregations: min(_col0)
             Group By Vectorization:
+                groupByMode: COMPLETE
                 vectorOutput: false
                 native: false
+                vectorProcessingMode: NONE
                 projectedOutputColumns: null
             keys: _col0 (type: int)
             mode: complete
             outputColumnNames: _col0, _col1
-            Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: _col1 (type: int)
               outputColumnNames: _col0
-              Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
               File Output Operator
                 compressed: false
                 table:
@@ -531,7 +546,7 @@ STAGE PLANS:
                   native: false
                   nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                   nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
-              Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
@@ -549,10 +564,10 @@ STAGE PLANS:
         Select Operator
           expressions: KEY.reducesinkkey0 (type: int)
           outputColumnNames: _col0
-          Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+          Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
           File Output Operator
             compressed: false
-            Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
             table:
                 input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                 output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -674,24 +689,26 @@ POSTHOOK: Input: default@store_sales
 82
 PREHOOK: query: explain vectorization expression
 select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number
 PREHOOK: type: QUERY
 POSTHOOK: query: explain vectorization expression
 select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number
@@ -711,42 +728,51 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: store_sales
-            Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
             TableScan Vectorization:
                 native: true
-                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
-            Select Operator
-              expressions: ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int)
-              outputColumnNames: ss_item_sk, ss_ticket_number, ss_quantity
-              Select Vectorization:
-                  className: VectorSelectOperator
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
                   native: true
-                  projectedOutputColumns: [2, 9, 10]
-              Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: min(ss_quantity)
-                Group By Vectorization:
-                    aggregators: VectorUDAFMinLong(col 10) -> int
-                    className: VectorGroupByOperator
-                    vectorOutput: true
-                    keyExpressions: col 9, col 2
-                    native: false
-                    projectedOutputColumns: [0]
-                keys: ss_ticket_number (type: int), ss_item_sk (type: int)
-                mode: hash
-                outputColumnNames: _col0, _col1, _col2
-                Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  key expressions: _col0 (type: int), _col1 (type: int)
-                  sort order: ++
-                  Map-reduce partition columns: _col0 (type: int)
-                  Reduce Sink Vectorization:
-                      className: VectorReduceSinkOperator
+                  predicateExpression: FilterLongColEqualLongScalar(col 9, val 1) -> boolean
+              predicate: (ss_ticket_number = 1) (type: boolean)
+              Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ss_item_sk (type: int), ss_quantity (type: int), ss_wholesale_cost_decimal (type: decimal(38,18)), ss_net_profit (type: double)
+                outputColumnNames: ss_item_sk, ss_quantity, ss_wholesale_cost_decimal, ss_net_profit
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [2, 10, 12, 23]
+                Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: min(ss_quantity), max(ss_net_profit), max(ss_wholesale_cost_decimal)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFMinLong(col 10) -> int, VectorUDAFMaxDouble(col 23) -> double, VectorUDAFMaxDecimal(col 12) -> decimal(38,18)
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      keyExpressions: col 2
                       native: false
-                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
-                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
-                  Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col2 (type: int)
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2]
+                  keys: ss_item_sk (type: int)
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col0 (type: int)
+                    sort order: +
+                    Map-reduce partition columns: _col0 (type: int)
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                    Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col1 (type: int), _col2 (type: double), _col3 (type: decimal(38,18))
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
@@ -762,29 +788,33 @@ STAGE PLANS:
           enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
       Reduce Operator Tree:
         Group By Operator
-          aggregations: min(VALUE._col0)
+          aggregations: min(VALUE._col0), max(VALUE._col1), max(VALUE._col2)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
-          keys: KEY._col0 (type: int), KEY._col1 (type: int)
+          keys: KEY._col0 (type: int)
           mode: mergepartial
-          outputColumnNames: _col0, _col1, _col2
-          Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+          outputColumnNames: _col0, _col1, _col2, _col3
+          Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
           Select Operator
-            expressions: _col1 (type: int), _col0 (type: int), _col2 (type: int)
-            outputColumnNames: _col0, _col1, _col2
-            Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+            expressions: _col0 (type: int), _col1 (type: int), _col2 (type: double), _col3 (type: decimal(38,18))
+            outputColumnNames: _col1, _col2, _col3, _col4
+            Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
             Group By Operator
-              aggregations: sum(_col0), sum(_col2)
+              aggregations: sum(_col1), sum(_col2), avg(_col2), sum(_col3), avg(_col3), sum(_col4), avg(_col4)
               Group By Vectorization:
+                  groupByMode: HASH
                   vectorOutput: false
                   native: false
+                  vectorProcessingMode: NONE
                   projectedOutputColumns: null
-              keys: _col1 (type: int)
-              mode: complete
-              outputColumnNames: _col0, _col1, _col2
-              Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+              keys: 1 (type: int)
+              mode: hash
+              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+              Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
               File Output Operator
                 compressed: false
                 table:
@@ -798,17 +828,18 @@ STAGE PLANS:
           TableScan
             TableScan Vectorization:
                 native: true
-                projectedOutputColumns: [0, 1, 2]
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
             Reduce Output Operator
               key expressions: _col0 (type: int)
               sort order: +
+              Map-reduce partition columns: _col0 (type: int)
               Reduce Sink Vectorization:
                   className: VectorReduceSinkOperator
                   native: false
                   nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                   nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
-              Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-              value expressions: _col1 (type: bigint), _col2 (type: bigint)
+              Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
+              value expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: struct<count:bigint,sum:double,input:int>), _col4 (type: double), _col5 (type: struct<count:bigint,sum:double,input:double>), _col6 (type: decimal(38,18)), _col7 (type: struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
@@ -823,17 +854,29 @@ STAGE PLANS:
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
           enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
       Reduce Operator Tree:
-        Select Operator
-          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: bigint), VALUE._col1 (type: bigint)
-          outputColumnNames: _col0, _col1, _col2
-          Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Group By Operator
+          aggregations: sum(VALUE._col0), sum(VALUE._col1), avg(VALUE._col2), sum(VALUE._col3), avg(VALUE._col4), sum(VALUE._col5), avg(VALUE._col6)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          keys: KEY._col0 (type: int)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+          Statistics: Num rows: 125 Data size: 30150 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: 1 (type: int), _col1 (type: bigint), _col2 (type: bigint), _col3 (type: double), _col4 (type: double), _col5 (type: double), _col6 (type: decimal(38,18)), _col7 (type: decimal(38,18))
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+            Statistics: Num rows: 125 Data size: 30150 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 125 Data size: 30150 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
 
   Stage: Stage-0
     Fetch Operator
@@ -842,12 +885,13 @@ STAGE PLANS:
         ListSink
 
 PREHOOK: query: select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number
@@ -855,106 +899,26 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@store_sales
 #### A masked pattern was here ####
 POSTHOOK: query: select
-    ss_ticket_number, sum(ss_item_sk), sum(q)
+    ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
+    where ss_ticket_number = 1
     group by ss_ticket_number, ss_item_sk) a
 group by ss_ticket_number
 order by ss_ticket_number
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@store_sales
 #### A masked pattern was here ####
-1	85411	816
-2	157365	812
-3	147948	710
-4	69545	411
-5	163232	840
-6	86307	627
-7	114874	563
-8	117953	662
-9	173250	690
-10	60338	602
-11	138545	657
-12	97181	586
-13	109484	555
-14	137333	442
-15	176829	652
-16	115004	654
-17	105008	460
-18	165135	738
-19	128252	831
-20	104789	374
-21	72771	469
-22	128153	449
-23	110253	603
-24	100662	1029
-25	118714	760
-26	81596	502
-27	164068	871
-28	58632	409
-29	133777	417
-30	130451	772
-31	114967	586
-32	142021	592
-33	151818	691
-34	112559	662
-35	137027	780
-36	118285	538
-37	94528	401
-38	81368	521
-39	101064	937
-40	84435	480
-41	112444	688
-42	95731	840
-43	57298	410
-44	159880	839
-45	68919	474
-46	111212	374
-47	78210	416
-48	94459	445
-49	90879	589
-50	37821	407
-51	124927	612
-52	98099	489
-53	138706	609
-54	87478	354
-55	90290	406
-56	78812	372
-57	101175	597
-58	88044	202
-59	104582	753
-60	99218	900
-61	66514	392
-62	126713	527
-63	98778	648
-64	131659	380
-65	86990	494
-66	108808	492
-67	75250	711
-68	91671	548
-69	92821	405
-70	75021	319
-71	124484	748
-72	161470	744
-73	104358	621
-74	88609	688
-75	92940	649
-76	75853	580
-77	124755	873
-78	98285	573
-79	160595	581
-80	151471	704
-81	105109	429
-82	55611	254
+1	85411	816	58.285714285714285	-5080.17	-362.8692857142857	621.350000000000000000	44.382142857142857143
 PREHOOK: query: explain vectorization expression
 select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
@@ -963,10 +927,10 @@ order by ss_ticket_number, ss_item_sk
 PREHOOK: type: QUERY
 POSTHOOK: query: explain vectorization expression
 select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
@@ -988,31 +952,33 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: store_sales
-            Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
             TableScan Vectorization:
                 native: true
-                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
             Select Operator
-              expressions: ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int)
-              outputColumnNames: ss_item_sk, ss_ticket_number, ss_quantity
+              expressions: ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int), ss_wholesale_cost_decimal (type: decimal(38,18)), ss_net_profit (type: double)
+              outputColumnNames: ss_item_sk, ss_ticket_number, ss_quantity, ss_wholesale_cost_decimal, ss_net_profit
               Select Vectorization:
                   className: VectorSelectOperator
                   native: true
-                  projectedOutputColumns: [2, 9, 10]
-              Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                  projectedOutputColumns: [2, 9, 10, 12, 23]
+              Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: min(ss_quantity)
+                aggregations: min(ss_quantity), max(ss_net_profit), max(ss_wholesale_cost_decimal)
                 Group By Vectorization:
-                    aggregators: VectorUDAFMinLong(col 10) -> int
+                    aggregators: VectorUDAFMinLong(col 10) -> int, VectorUDAFMaxDouble(col 23) -> double, VectorUDAFMaxDecimal(col 12) -> decimal(38,18)
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 9, col 2
                     native: false
-                    projectedOutputColumns: [0]
+                    vectorProcessingMode: HASH
+                    projectedOutputColumns: [0, 1, 2]
                 keys: ss_ticket_number (type: int), ss_item_sk (type: int)
                 mode: hash
-                outputColumnNames: _col0, _col1, _col2
-                Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   key expressions: _col0 (type: int), _col1 (type: int)
                   sort order: ++
@@ -1022,8 +988,8 @@ STAGE PLANS:
                       native: false
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                       nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
-                  Statistics: Num rows: 1000 Data size: 88276 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col2 (type: int)
+                  Statistics: Num rows: 1000 Data size: 241204 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col2 (type: int), _col3 (type: double), _col4 (type: decimal(38,18))
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
@@ -1039,35 +1005,43 @@ STAGE PLANS:
           enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
       Reduce Operator Tree:
         Group By Operator
-          aggregations: min(VALUE._col0)
+          aggregations: min(VALUE._col0), max(VALUE._col1), max(VALUE._col2)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int), KEY._col1 (type: int)
           mode: mergepartial
-          outputColumnNames: _col0, _col1, _col2
-          Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4
+          Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
           Select Operator
-            expressions: _col1 (type: int), _col0 (type: int), _col2 (type: int)
-            outputColumnNames: _col0, _col1, _col2
-            Statistics: Num rows: 500 Data size: 44138 Basic stats: COMPLETE Column stats: NONE
+            expressions: _col1 (type: int), _col0 (type: int), _col2 (type: int), _col3 (type: double), _col4 (type: decimal(38,18))
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4
+            Statistics: Num rows: 500 Data size: 120602 Basic stats: COMPLETE Column stats: NONE
             Group By Operator
-              aggregations: sum(_col2)
+              aggregations: sum(_col2), avg(_col2), sum(_col3), avg(_col3), sum(_col4), avg(_col4)
               Group By Vectorization:
+                  groupByMode: COMPLETE
                   vectorOutput: false
                   native: false
+                  vectorProcessingMode: NONE
                   projectedOutputColumns: null
               keys: _col1 (type: int), _col0 (type: int)
               mode: complete
-              outputColumnNames: _col0, _col1, _col2
-              Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-              File Output Operator
-                compressed: false
-                table:
-                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+              Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: double), _col4 (type: double), _col5 (type: double), _col6 (type: decimal(38,18)), _col7 (type: decimal(38,18))
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+                Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
 
   Stage: Stage-2
     Map Reduce
@@ -1075,7 +1049,7 @@ STAGE PLANS:
           TableScan
             TableScan Vectorization:
                 native: true
-                projectedOutputColumns: [0, 1, 2]
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
             Reduce Output Operator
               key expressions: _col0 (type: int), _col1 (type: int)
               sort order: ++
@@ -1084,8 +1058,8 @@ STAGE PLANS:
                   native: false
                   nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                   nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
-              Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
-              value expressions: _col2 (type: bigint)
+              Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
+              value expressions: _col2 (type: bigint), _col3 (type: double), _col4 (type: double), _col5 (type: double), _col6 (type: decimal(38,18)), _col7 (type: decimal(38,18))
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
@@ -1101,12 +1075,12 @@ STAGE PLANS:
           enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
       Reduce Operator Tree:
         Select Operator
-          expressions: KEY.reducesinkkey0 (type: int), KEY.reducesinkkey1 (type: int), VALUE._col0 (type: bigint)
-          outputColumnNames: _col0, _col1, _col2
-          Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+          expressions: KEY.reducesinkkey0 (type: int), KEY.reducesinkkey1 (type: int), VALUE._col0 (type: bigint), VALUE._col1 (type: double), VALUE._col2 (type: double), VALUE._col3 (type: double), VALUE._col4 (type: decimal(38,18)), VALUE._col5 (type: decimal(38,18))
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+          Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
           File Output Operator
             compressed: false
-            Statistics: Num rows: 250 Data size: 22069 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 250 Data size: 60301 Basic stats: COMPLETE Column stats: NONE
             table:
                 input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                 output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -1119,10 +1093,10 @@ STAGE PLANS:
         ListSink
 
 PREHOOK: query: select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(wc), avg(wc), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_wholesale_cost) wc, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
@@ -1132,10 +1106,10 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@store_sales
 #### A masked pattern was here ####
 POSTHOOK: query: select
-    ss_ticket_number, ss_item_sk, sum(q)
+    ss_ticket_number, ss_item_sk, sum(q), avg(q), sum(wc), avg(wc), sum(decwc), avg(decwc)
 from
     (select
-        ss_ticket_number, ss_item_sk, min(ss_quantity) q
+        ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_wholesale_cost) wc, max(ss_wholesale_cost_decimal) decwc
     from
         store_sales
     group by ss_ticket_number, ss_item_sk) a
@@ -1144,1003 +1118,1003 @@ order by ss_ticket_number, ss_item_sk
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@store_sales
 #### A masked pattern was here ####
-1	49	5
-1	173	65
-1	1553	50
-1	3248	58
-1	3617	79
-1	4553	100
-1	4583	72
-1	4682	44
-1	5527	88
-1	5981	14
-1	10993	91
-1	13283	37
-1	13538	14
-1	13631	99
-2	1363	4
-2	2930	36
-2	3740	49
-2	6928	65
-2	7654	25
-2	9436	79
-2	10768	30
-2	12068	74
-2	12223	78
-2	13340	71
-2	13927	93
-2	14701	58
-2	15085	88
-2	15782	62
-2	17420	NULL
-3	246	96
-3	1531	NULL
-3	3525	42
-3	4698	98
-3	5355	53
-3	10693	27
-3	12447	82
-3	13021	64
-3	14100	79
-3	14443	4
-3	15786	56
-3	16869	4
-3	17263	17
-3	17971	88
-4	163	17
-4	1576	74
-4	5350	86
-4	5515	23
-4	6988	23
-4	7990	56
-4	8452	27
-4	9685	21
-4	11036	41
-4	12790	43
-5	1808	NULL
-5	1940	60
-5	5842	50
-5	6068	76
-5	6466	36
-5	11324	52
-5	11590	15
-5	12650	66
-5	13562	64
-5	13958	60
-5	14599	83
-5	14686	91
-5	15752	66
-5	16195	50
-5	16792	71
-6	2549	62
-6	2647	100
-6	3049	31
-6	3291	100
-6	6437	72
-6	8621	NULL
-6	10355	94
-6	10895	1
-6	11705	61
-6	13245	64
-6	13513	42
-7	4627	9
-7	4795	73
-7	4833	88
-7	5183	51
-7	5905	69
-7	8955	54
-7	9751	4
-7	10487	52
-7	12571	82
-7	15179	12
-7	15333	NULL
-7	17255	69
-8	665	31
-8	4183	90
-8	5929	83
-8	7115	54
-8	11365	7
-8	11893	95
-8	12041	95
-8	13427	87
-8	16671	20
-8	17119	51
-8	17545	49
-9	69	11
-9	889	6
-9	1185	62
-9	4623	34
-9	7945	83
-9	8334	71
-9	12027	27
-9	12969	59
-9	13483	NULL
-9	13717	53
-9	15133	15
-9	16083	32
-9	16363	54
-9	16461	66
-9	16659	84
-9	17310	33
-10	755	74
-10	1425	92
-10	1511	76
-10	3433	83
-10	3933	52
-10	4357	17
-10	5863	47
-10	9811	28
-10	13803	66
-10	15447	67
-11	157	84
-11	1315	70
-11	7519	68
-11	7608	66
-11	9901	57
-11	10699	33
-11	11490	NULL
-11	11991	38
-11	12438	16
-11	15157	96
-11	15649	33
-11	17226	11
-11	17395	85
-12	373	57
-12	1591	82
-12	4888	56
-12	6148	36
-12	6248	36
-12	9616	66
-12	9788	73
-12	13399	46
-12	14746	26
-12	14944	9
-12	15440	99
-13	868	NULL
-13	1760	12
-13	1898	NULL
-13	2108	9
-13	2191	NULL
-13	4430	73
-13	5971	80
-13	6085	58
-13	6140	15
-13	6682	80
-13	7640	48
-13	7723	27
-13	10096	12
-13	11758	34
-13	16894	87
-13	17240	20
-14	177	41
-14	769	20
-14	4507	4
-14	10175	19
-14	11549	6
-14	11653	60
-14	11817	81
-14	12587	NULL
-14	13069	77
-14	13515	57
-14	13845	17
-14	16741	46
-14	16929	14
-15	4241	21
-15	4505	59
-15	4777	28
-15	7391	98
-15	8336	15
-15	8353	NULL
-15	8690	32
-15	8707	21
-15	10361	39
-15	11659	80
-15	13172	25
-15	16619	81
-15	17267	7
-15	17330	82
-15	17564	26
-15	17857	38
-16	457	60
-16	1888	4
-16	4144	94
-16	6008	59
-16	7504	51
-16	8887	35
-16	9769	42
-16	9790	17
-16	9997	94
-16	11168	86
-16	11920	29
-16	16226	13
-16	17246	70
-17	2092	37
-17	4678	34
-17	6811	70
-17	9214	57
-17	10543	54
-17	11203	21
-17	13177	45
-17	13826	32
-17	15781	76
-17	17683	34
-18	2440	40
-18	5251	41
-18	7378	94
-18	8779	9
-18	8884	18
-18	9886	62
-18	11584	76
-18	11890	7
-18	12602	81
-18	12826	93
-18	12860	18
-18	14011	95
-18	14372	76
-18	14377	15
-18	17995	13
-19	1094	48
-19	3133	96
-19	3376	84
-19	4882	84
-19	6772	97
-19	7087	1
-19	7814	29
-19	8662	97
-19	9094	49
-19	9346	39
-19	10558	82
-19	10651	46
-19	11914	59
-19	16330	NULL
-19	17539	20
-20	1451	89
-20	2618	4
-20	5312	9
-20	5425	15
-20	5483	8
-20	6026	21
-20	7207	90
-20	8714	NULL
-20	9086	4
-20	9800	32
-20	13601	17
-20	14935	NULL
-20	15131	85
-21	230	48
-21	1810	59
-21	2870	50
-21	5170	45
-21	5998	51
-21	6476	49
-21	9187	14
-21	12266	47
-21	14368	18
-21	14396	88
-22	9985	70
-22	10474	31
-22	11599	66
-22	12415	10
-22	15310	15
-22	16396	85
-22	16922	88
-22	17392	14
-22	17660	70
-23	319	86
-23	7242	37
-23	8181	13
-23	8413	1
-23	9093	38
-23	9097	81
-23	11220	91
-23	11257	64
-23	12397	80
-23	15403	96
-23	17631	16
-24	407	53
-24	1389	72
-24	1795	21
-24	2497	85
-24	3103	73
-24	4425	57
-24	4749	28
-24	4873	41
-24	5653	92
-24	6043	1
-24	6751	82
-24	7375	97
-24	10265	93
-24	11551	48
-24	13303	97
-24	16483	89
-25	1333	55
-25	2150	100
-25	2608	76
-25	3454	100
-25	4880	29
-25	5954	34
-25	6955	40
-25	7874	65
-25	9472	48
-25	10159	24
-25	14488	26
-25	14635	68
-25	17000	40
-25	17752	55
-26	1989	26
-26	5053	4
-26	5385	97
-26	5721	81
-26	6647	64
-26	7337	45
-26	9679	18
-26	11895	77
-26	12851	56
-26	15039	34
-27	1305	44
-27	2137	96
-27	2671	92
-27	5831	61
-27	7139	59
-27	8167	28
-27	10757	15
-27	11441	15
-27	11509	65
-27	12237	89
-27	12749	31
-27	13885	66
-27	15025	26
-27	16029	59
-27	16419	65
-27	16767	60
-28	1807	98
-28	2817	8
-28	2967	29
-28	4483	78
-28	5437	15
-28	6411	3
-28	7965	93
-28	8043	58
-28	8407	14
-28	10295	13
-29	20	18
-29	1363	75
-29	2930	23
-29	3740	5
-29	7654	20
-29	9458	33
-29	10795	33
-29	12068	37
-29	12223	59
-29	13340	21
-29	13693	NULL
-29	15085	40
-29	15626	NULL
-29	15782	53
-30	217	91
-30	1951	59
-30	3238	16
-30	3506	15
-30	3928	87
-30	5431	77
-30	6752	69
-30	7870	7
-30	8666	21
-30	12572	33
-30	12670	20
-30	13579	75
-30	14848	62
-30	17348	62
-30	17875	78
-31	913	54
-31	4963	67
-31	6617	11
-31	6917	4
-31	7513	82
-31	11739	95
-31	14575	97
-31	14727	41
-31	15341	31
-31	15411	53
-31	16251	51
-32	1115	61
-32	2095	34
-32	2887	8
-32	4339	6
-32	4537	22
-32	4808	NULL
-32	5798	87
-32	7547	24
-32	9683	26
-32	11005	46
-32	11348	41
-32	12134	21
-32	15001	57
-32	15644	34
-32	16421	74
-32	17659	51
-33	4798	27
-33	7300	3
-33	9649	36
-33	10376	21
-33	11119	92
-33	11756	26
-33	12643	89
-33	12760	54
-33	12964	80
-33	14125	66
-33	14158	82
-33	14692	93
-33	15478	22
-34	1526	91
-34	1717	53
-34	2312	6
-34	4118	88
-34	5197	63
-34	5449	9
-34	6193	61
-34	9325	3
-34	9766	83
-34	12016	42
-34	12290	53
-34	12512	60
-34	13814	20
-34	16324	30
-35	411	51
-35	2377	52
-35	3667	97
-35	4325	56
-35	5179	83
-35	11635	87
-35	11661	81
-35	14239	55
-35	15619	45
-35	15757	9
-35	17341	92
-35	17365	65
-35	17451	7
-36	1115	80
-36	2095	43
-36	2887	31
-36	7547	46
-36	11005	49
-36	11349	80
-36	15001	54
-36	15645	23
-36	16421	25
-36	17561	16
-36	17659	91
-37	2997	94
-37	7283	87
-37	10715	52
-37	10929	88
-37	13171	6
-37	15337	62
-37	16971	12
-37	17125	NULL
-38	757	2
-38	2164	17
-38	3439	84
-38	4154	35
-38	5113	73
-38	6220	98
-38	7018	15
-38	7784	56
-38	8870	15
-38	9710	7
-38	10441	62
-38	15698	57
-39	386	89
-39	1598	64
-39	3476	73
-39	3943	64
-39	4190	86
-39	4957	24
-39	5393	98
-39	7097	78
-39	7118	67
-39	7604	49
-39	7697	24
-39	8078	54
-39	8411	96
-39	15491	54
-39	15625	17
-40	2854	71
-40	3490	65
-40	3985	63
-40	5098	35
-40	5318	87
-40	10094	80
-40	10912	23
-40	12050	NULL
-40	13658	53
-40	16976	3
-41	10	50
-41	64	29
-41	3380	88
-41	5566	11
-41	6310	90
-41	7402	69
-41	7603	94
-41	9322	8
-41	10915	81
-41	14788	15
-41	15242	87
-41	15328	46
-41	16514	20
-42	619	69
-42	976	100
-42	1436	94
-42	2314	74
-42	2392	14
-42	2602	30
-42	3346	74
-42	3613	30
-42	6058	30
-42	6134	92
-42	8462	23
-42	9740	52
-42	10016	57
-42	10471	19
-42	12550	41
-42	15002	41
-43	2923	16
-43	3344	22
-43	3911	26
-43	4364	77
-43	4691	41
-43	5773	85
-43	5852	16
-43	11771	30
-43	14669	97
-44	2351	56
-44	2623	18
-44	7303	14
-44	7527	67
-44	9059	68
-44	11707	83
-44	12341	20
-44	13331	98
-44	13449	45
-44	14149	80
-44	15803	81
-44	16491	56
-44	16837	92
-44	16909	61
-45	811	62
-45	1479	49
-45	3265	98
-45	5309	18
-45	7363	87
-45	10115	68
-45	11095	40
-45	13133	46
-45	16349	6
-46	1960	12
-46	3010	67
-46	7040	33
-46	8065	NULL
-46	11426	72
-46	13042	58
-46	15595	32
-46	16540	30
-46	17150	57
-46	17384	13
-47	254	NULL
-47	481	30
-47	1132	66
-47	1916	71
-47	3085	51
-47	3202	7
-47	3878	NULL
-47	4774	11
-47	5008	82
-47	5305	NULL
-47	5468	7
-47	7214	1
-47	9770	33
-47	13246	47
-47	13477	10
-48	1761	22
-48	2820	4
-48	2829	65
-48	4431	39
-48	5971	29
-48	6085	1
-48	6684	44
-48	9199	88
-48	11259	NULL
-48	12468	62
-48	13153	74
-48	17799	17
-49	749	60
-49	2135	4
-49	5342	69
-49	5852	47
-49	6805	40
-49	7141	94
-49	9049	68
-49	9553	71
-49	12737	48
-49	15155	84
-49	16361	4
-50	1280	69
-50	1312	30
-50	1909	53
-50	1984	40
-50	3097	64
-50	5023	NULL
-50	7135	69
-50	16081	82
-51	422	21
-51	3091	28
-51	4687	6
-51	5029	12
-51	5059	51
-51	6565	33
-51	8384	79
-51	9311	90
-51	10133	54
-51	11234	NULL
-51	12625	53
-51	13199	97
-51	17483	22
-51	17705	66
-52	2420	90
-52	3334	73
-52	6098	NULL
-52	7606	45
-52	11488	76
-52	15649	29
-52	16646	48
-52	17402	91
-52	17456	37
-53	1114	40
-53	2095	62
-53	2786	70
-53	2887	39
-53	7546	58
-53	11348	38
-53	13220	76
-53	13795	38
-53	15991	37
-53	16420	14
-53	16648	79
-53	17296	43
-53	17560	15
-54	702	40
-54	825	50
-54	1165	62
-54	3861	NULL
-54	6517	40
-54	9159	75
-54	14737	38
-54	16059	15
-54	16974	NULL
-54	17479	34
-55	1339	16
-55	3001	7
-55	5137	33
-55	9703	44
-55	12170	92
-55	12205	90
-55	14135	36
-55	14923	71
-55	17677	17
-56	4242	2
-56	4506	57
-56	8353	35
-56	8691	59
-56	8707	68
-56	10362	54
-56	16620	23
-56	17331	74
-57	3253	71
-57	4028	88
-57	4933	22
-57	12596	91
-57	12721	62
-57	12740	52
-57	15182	86
-57	17729	26
-57	17993	99
-58	1829	52
-58	3848	6
-58	5117	2
-58	7649	19
-58	9743	62
-58	10802	14
-58	15635	6
-58	16472	6
-58	16949	35
-59	3133	92
-59	3546	22
-59	5772	70
-59	7087	80
-59	8010	46
-59	8335	36
-59	9348	62
-59	9397	92
-59	10651	100
-59	11916	19
-59	12858	90
-59	14529	44
-60	97	50
-60	555	62
-60	633	71
-60	999	43
-60	1117	78
-60	1573	90
-60	4041	25
-60	4235	28
-60	4513	72
-60	4937	22
-60	7231	95
-60	10277	62
-60	10393	75
-60	13975	14
-60	16887	25
-60	17755	88
-61	1106	4
-61	2264	36
-61	3362	48
-61	4567	26
-61	5528	78
-61	6380	77
-61	7591	78
-61	8924	11
-61	10330	8
-61	16462	26
-62	4093	94
-62	6403	NULL
-62	8457	37
-62	10149	75
-62	12163	29
-62	12199	5
-62	12407	NULL
-62	13559	80
-62	15399	74
-62	15733	40
-62	16151	93
-63	4488	73
-63	5079	79
-63	5217	66
-63	5658	99
-63	9319	80
-63	11370	38
-63	11946	85
-63	13339	19
-63	15793	40
-63	16569	69
-64	1213	NULL
-64	3090	87
-64	3963	NULL
-64	11835	82
-64	13224	NULL
-64	14407	8
-64	15867	59
-64	15936	30
-64	16921	19
-64	17586	78
-64	17617	17
-65	2287	100
-65	4227	42
-65	9625	51
-65	9847	54
-65	13897	40
-65	14905	85
-65	15177	55
-65	17025	67
-66	6507	76
-66	7033	65
-66	7227	66
-66	8197	41
-66	9237	29
-66	10019	10
-66	11419	66
-66	15629	20
-66	16745	91
-66	16795	28
-67	757	77
-67	2133	74
-67	3439	73
-67	4155	87
-67	5113	NULL
-67	7020	79
-67	7507	77
-67	8469	59
-67	8871	71
-67	12087	70
-67	15699	44
-68	1387	74
-68	1603	57
-68	1820	54
-68	2035	22
-68	2296	52
-68	2564	83
-68	5162	23
-68	6763	77
-68	7765	NULL
-68	12526	3
-68	12724	88
-68	17426	2
-68	17600	13
-69	322	45
-69	337	34
-69	4208	9
-69	4267	10
-69	6136	7
-69	7264	67
-69	7822	30
-69	8599	53
-69	11137	68
-69	13489	66
-69	13792	NULL
-69	15448	16
-70	1592	53
-70	2462	NULL
-70	3296	48
-70	3947	NULL
-70	6185	82
-70	6425	NULL
-70	8893	17
-70	9857	20
-70	14549	4
-70	17815	95
-71	457	75
-71	1888	4
-71	2098	51
-71	4144	49
-71	5858	NULL
-71	6008	54
-71	7504	3
-71	8887	10
-71	9274	36
-71	9769	79
-71	9790	96
-71	9997	26
-71	10108	66
-71	10288	30
-71	11168	79
-71	17246	90
-72	1535	9
-72	5917	85
-72	6113	45
-72	6671	13
-72	9860	26
-72	10427	66
-72	10753	16
-72	11741	62
-72	12788	29
-72	12901	57
-72	13085	94
-72	13423	62
-72	13904	37
-72	15587	87
-72	16765	56
-73	247	53
-73	1063	37
-73	3205	82
-73	4946	54
-73	6862	58
-73	10051	49
-73	12502	75
-73	15109	38
-73	16519	97
-73	16585	38
-73	17269	40
-74	326	29
-74	3104	78
-74	3175	23
-74	3278	NULL
-74	3542	96
-74	3754	26
-74	5492	54
-74	7694	17
-74	8653	12
-74	9620	95
-74	10069	99
-74	13208	87
-74	16694	72
-75	607	20
-75	2948	25
-75	4625	73
-75	6938	89
-75	6953	71
-75	8726	6
-75	9905	54
-75	10217	85
-75	11039	70
-75	14186	63
-75	16796	93
-76	257	5
-76	465	2
-76	1107	16
-76	1503	97
-76	2265	98
-76	2869	32
-76	3363	25
-76	4237	48
-76	4567	40
-76	5529	78
-76	6381	50
-76	7591	27
-76	8925	6
-76	10331	3
-76	16463	53
-77	992	62
-77	1399	34
-77	2713	85
-77	3868	89
-77	6289	30
-77	7339	88
-77	7448	95
-77	7486	49
-77	8686	38
-77	9220	90
-77	11918	36
-77	12439	95
-77	13456	48
-77	14815	18
-77	16687	16
-78	901	3
-78	3304	50
-78	3856	27
-78	5965	78
-78	6044	59
-78	6110	43
-78	6500	76
-78	7576	87
-78	8611	79
-78	10507	6
-78	11209	7
-78	12706	19
-78	14996	39
-79	247	NULL
-79	1063	85
-79	3205	48
-79	4947	35
-79	6864	1
-79	10051	10
-79	10524	36
-79	12504	81
-79	14322	41
-79	15109	NULL
-79	15498	3
-79	15888	58
-79	16519	9
-79	16585	93
-79	17269	81
-80	998	93
-80	1519	25
-80	1573	40
-80	4040	66
-80	4513	NULL
-80	4622	1
-80	7231	49
-80	7610	37
-80	10393	5
-80	12968	NULL
-80	13717	91
-80	13975	13
-80	16363	84
-80	16886	77
-80	17308	29
-80	17755	94
-81	4486	31
-81	5078	75
-81	5216	64
-81	5656	24
-81	7166	7
-81	7663	79
-81	8918	37
-81	9319	36
-81	11107	36
-81	11368	26
-81	13339	6
-81	15793	8
-82	2572	53
-82	7862	75
-82	13138	59
-82	14998	49
-82	17041	18
+1	49	5	5.0	10.68	10.68	10.680000000000000000	10.680000000000000000
+1	173	65	65.0	27.16	27.16	27.160000000000000000	27.160000000000000000
+1	1553	50	50.0	67.71	67.71	67.710000000000000000	67.710000000000000000
+1	3248	58	58.0	4.57	4.57	4.570000000000000000	4.570000000000000000
+1	3617	79	79.0	11.41	11.41	11.410000000000000000	11.410000000000000000
+1	4553	100	100.0	25.08	25.08	25.080000000000000000	25.080000000000000000
+1	4583	72	72.0	84.72	84.72	84.720000000000000000	84.720000000000000000
+1	4682	44	44.0	31.07	31.07	31.070000000000000000	31.070000000000000000
+1	5527	88	88.0	52.41	52.41	52.410000000000000000	52.410000000000000000
+1	5981	14	14.0	57.37	57.37	57.370000000000000000	57.370000000000000000
+1	10993	91	91.0	93.48	93.48	93.480000000000000000	93.480000000000000000
+1	13283	37	37.0	63.63	63.63	63.630000000000000000	63.630000000000000000
+1	13538	14	14.0	11.54	11.54	11.540000000000000000	11.540000000000000000
+1	13631	99	99.0	80.52	80.52	80.520000000000000000	80.520000000000000000
+2	1363	4	4.0	13.46	13.46	13.460000000000000000	13.460000000000000000
+2	2930	36	36.0	61.23	61.23	61.230000000000000000	61.230000000000000000
+2	3740	49	49.0	6.55	6.55	6.550000000000000000	6.550000000000000000
+2	6928	65	65.0	93.86	93.86	93.860000000000000000	93.860000000000000000
+2	7654	25	25.0	74.26	74.26	74.260000000000000000	74.260000000000000000
+2	9436	79	79.0	88.02	88.02	88.020000000000000000	88.020000000000000000
+2	10768	30	30.0	2.27	2.27	2.270000000000000000	2.270000000000000000
+2	12068	74	74.0	16.55	16.55	16.550000000000000000	16.550000000000000000
+2	12223	78	78.0	65.71	65.71	65.710000000000000000	65.710000000000000000
+2	13340	71	71.0	36.01	36.01	36.010000000000000000	36.010000000000000000
+2	13927	93	93.0	35.87	35.87	35.870000000000000000	35.870000000000000000
+2	14701	58	58.0	53.09	53.09	53.090000000000000000	53.090000000000000000
+2	15085	88	88.0	64.43	64.43	64.430000000000000000	64.430000000000000000
+2	15782	62	62.0	77.97	77.97	77.970000000000000000	77.970000000000000000
+2	17420	NULL	NULL	17.12	17.12	17.120000000000000000	17.120000000000000000
+3	246	96	96.0	98.02	98.02	98.020000000000000000	98.020000000000000000
+3	1531	NULL	NULL	NULL	NULL	NULL	NULL
+3	3525	42	42.0	97.03	97.03	97.030000000000000000	97.030000000000000000
+3	4698	98	98.0	85.0	85.0	85.000000000000000000	85.000000000000000000
+3	5355	53	53.0	23.04	23.04	23.040000000000000000	23.040000000000000000
+3	10693	27	27.0	37.04	37.04	37.040000000000000000	37.040000000000000000
+3	12447	82	82.0	56.14	56.14	56.140000000000000000	56.140000000000000000
+3	13021	64	64.0	74.69	74.69	74.690000000000000000	74.690000000000000000
+3	14100	79	79.0	44.66	44.66	44.660000000000000000	44.660000000000000000
+3	14443	4	4.0	95.75	95.75	95.750000000000000000	95.750000000000000000
+3	15786	56	56.0	4.31	4.31	4.310000000000000000	4.310000000000000000
+3	16869	4	4.0	75.67	75.67	75.670000000000000000	75.670000000000000000
+3	17263	17	17.0	72.38	72.38	72.380000000000000000	72.380000000000000000
+3	17971	88	88.0	27.95	27.95	27.950000000000000000	27.950000000000000000
+4	163	17	17.0	54.26	54.26	54.260000000000000000	54.260000000000000000
+4	1576	74	74.0	81.81	81.81	81.810000000000000000	81.810000000000000000
+4	5350	86	86.0	64.67	64.67	64.670000000000000000	64.670000000000000000
+4	5515	23	23.0	2.91	2.91	2.910000000000000000	2.910000000000000000
+4	6988	23	23.0	53.28	53.28	53.280000000000000000	53.280000000000000000
+4	7990	56	56.0	64.68	64.68	64.680000000000000000	64.680000000000000000
+4	8452	27	27.0	26.21	26.21	26.210000000000000000	26.210000000000000000
+4	9685	21	21.0	40.39	40.39	40.390000000000000000	40.390000000000000000
+4	11036	41	41.0	67.18	67.18	67.180000000000000000	67.180000000000000000
+4	12790	43	43.0	54.34	54.34	54.340000000000000000	54.340000000000000000
+5	1808	NULL	NULL	NULL	NULL	NULL	NULL
+5	1940	60	60.0	69.54	69.54	69.540000000000000000	69.540000000000000000
+5	5842	50	50.0	30.69	30.69	30.690000000000000000	30.690000000000000000
+5	6068	76	76.0	89.78	89.78	89.780000000000000000	89.780000000000000000
+5	6466	36	36.0	7.93	7.93	7.930000000000000000	7.930000000000000000
+5	11324	52	52.0	16.33	16.33	16.330000000000000000	16.330000000000000000
+5	11590	15	15.0	21.21	21.21	21.210000000000000000	21.210000000000000000
+5	12650	66	66.0	21.01	21.01	21.010000000000000000	21.010000000000000000
+5	13562	64	64.0	87.9	87.9	87.900000000000000000	87.900000000000000000
+5	13958	60	60.0	41.72	41.72	41.720000000000000000	41.720000000000000000
+5	14599	83	83.0	74.15	74.15	74.150000000000000000	74.150000000000000000
+5	14686	91	91.0	27.68	27.68	27.680000000000000000	27.680000000000000000
+5	15752	66	66.0	71.06	71.06	71.060000000000000000	71.060000000000000000
+5	16195	50	50.0	30.96	30.96	30.960000000000000000	30.960000000000000000
+5	16792	71	71.0	22.1	22.1	22.100000000000000000	22.100000000000000000
+6	2549	62	62.0	85.07	85.07	85.070000000000000000	85.070000000000000000
+6	2647	100	100.0	4.45	4.45	4.450000000000000000	4.450000000000000000
+6	3049	31	31.0	49.78	49.78	49.780000000000000000	49.780000000000000000
+6	3291	100	100.0	41.08	41.08	41.080000000000000000	41.080000000000000000
+6	6437	72	72.0	55.49	55.49	55.490000000000000000	55.490000000000000000
+6	8621	NULL	NULL	NULL	NULL	NULL	NULL
+6	10355	94	94.0	62.67	62.67	62.670000000000000000	62.670000000000000000
+6	10895	1	1.0	71.1	71.1	71.100000000000000000	71.100000000000000000
+6	11705	61	61.0	48.18	48.18	48.180000000000000000	48.180000000000000000
+6	13245	64	64.0	86.35	86.35	86.350000000000000000	86.350000000000000000
+6	13513	42	42.0	64.46	64.46	64.460000000000000000	64.460000000000000000
+7	4627	9	9.0	56.13	56.13	56.130000000000000000	56.130000000000000000
+7	4795	73	73.0	12.17	12.17	12.170000000000000000	12.170000000000000000
+7	4833	88	88.0	38.23	38.23	38.230000000000000000	38.230000000000000000
+7	5183	51	51.0	84.65	84.65	84.650000000000000000	84.650000000000000000
+7	5905	69	69.0	99.85	99.85	99.850000000000000000	99.850000000000000000
+7	8955	54	54.0	42.82	42.82	42.820000000000000000	42.820000000000000000
+7	9751	4	4.0	NULL	NULL	NULL	NULL
+7	10487	52	52.0	63.8	63.8	63.800000000000000000	63.800000000000000000
+7	12571	82	82.0	69.53	69.53	69.530000000000000000	69.530000000000000000
+7	15179	12	12.0	47.6	47.6	47.600000000000000000	47.600000000000000000
+7	15333	NULL	NULL	NULL	NULL	NULL	NULL
+7	17255	69	69.0	34.19	34.19	34.190000000000000000	34.190000000000000000
+8	665	31	31.0	15.64	15.64	15.640000000000000000	15.640000000000000000
+8	4183	90	90.0	81.63	81.63	81.630000000000000000	81.630000000000000000
+8	5929	83	83.0	14.11	14.11	14.110000000000000000	14.110000000000000000
+8	7115	54	54.0	36.99	36.99	36.990000000000000000	36.990000000000000000
+8	11365	7	7.0	18.65	18.65	18.650000000000000000	18.650000000000000000
+8	11893	95	95.0	21.29	21.29	21.290000000000000000	21.290000000000000000
+8	12041	95	95.0	91.8	91.8	91.800000000000000000	91.800000000000000000
+8	13427	87	87.0	31.78	31.78	31.780000000000000000	31.780000000000000000
+8	16671	20	20.0	18.95	18.95	18.950000000000000000	18.950000000000000000
+8	17119	51	51.0	8.04	8.04	8.040000000000000000	8.040000000000000000
+8	17545	49	49.0	72.15	72.15	72.150000000000000000	72.150000000000000000
+9	69	11	11.0	31.7	31.7	31.700000000000000000	31.700000000000000000
+9	889	6	6.0	27.17	27.17	27.170000000000000000	27.170000000000000000
+9	1185	62	62.0	55.68	55.68	55.680000000000000000	55.680000000000000000
+9	4623	34	34.0	2.97	2.97	2.970000000000000000	2.970000000000000000
+9	7945	83	83.0	8.1	8.1	8.100000000000000000	8.100000000000000000
+9	8334	71	71.0	34.79	34.79	34.790000000000000000	34.790000000000000000
+9	12027	27	27.0	98.68	98.68	98.680000000000000000	98.680000000000000000
+9	12969	59	59.0	88.31	88.31	88.310000000000000000	88.310000000000000000
+9	13483	NULL	NULL	59.14	59.14	59.140000000000000000	59.140000000000000000
+9	13717	53	53.0	75.37	75.37	75.370000000000000000	75.370000000000000000
+9	15133	15	15.0	35.89	35.89	35.890000000000000000	35.890000000000000000
+9	16083	32	32.0	99.1	99.1	99.100000000000000000	99.100000000000000000
+9	16363	54	54.0	NULL	NULL	NULL	NULL
+9	16461	66	66.0	15.21	15.21	15.210000000000000000	15.210000000000000000
+9	16659	84	84.0	76.71	76.71	76.710000000000000000	76.710000000000000000
+9	17310	33	33.0	27.13	27.13	27.130000000000000000	27.130000000000000000
+10	755	74	74.0	82.24	82.24	82.240000000000000000	82.240000000000000000
+10	1425	92	92.0	NULL	NULL	NULL	NULL
+10	1511	76	76.0	31.47	31.47	31.470000000000000000	31.470000000000000000
+10	3433	83	83.0	10.26	10.26	10.260000000000000000	10.260000000000000000
+10	3933	52	52.0	52.19	52.19	52.190000000000000000	52.190000000000000000
+10	4357	17	17.0	88.36	88.36	88.360000000000000000	88.360000000000000000
+10	5863	47	47.0	11.71	11.71	11.710000000000000000	11.710000000000000000
+10	9811	28	28.0	47.85	47.85	47.850000000000000000	47.850000000000000000
+10	13803	66	66.0	82.35	82.35	82.350000000000000000	82.350000000000000000
+10	15447	67	67.0	33.28	33.28	33.280000000000000000	33.280000000000000000
+11	157	84	84.0	64.63	64.63	64.630000000000000000	64.630000000000000000
+11	1315	70	70.0	45.84	45.84	45.840000000000000000	45.840000000000000000
+11	7519	68	68.0	7.16	7.16	7.160000000000000000	7.160000000000000000
+11	7608	66	66.0	8.34	8.34	8.340000000000000000	8.340000000000000000
+11	9901	57	57.0	46.93	46.93	46.930000000000000000	46.930000000000000000
+11	10699	33	33.0	73.77	73.77	73.770000000000000000	73.770000000000000000
+11	11490	NULL	NULL	NULL	NULL	NULL	NULL
+11	11991	38	38.0	3.27	3.27	3.270000000000000000	3.270000000000000000
+11	12438	16	16.0	92.94	92.94	92.940000000000000000	92.940000000000000000
+11	15157	96	96.0	15.52	15.52	15.520000000000000000	15.520000000000000000
+11	15649	33	33.0	66.11	66.11	66.110000000000000000	66.110000000000000000
+11	17226	11	11.0	34.03	34.03	34.030000000000000000	34.030000000000000000
+11	17395	85	85.0	38.04	38.04	38.040000000000000000	38.040000000000000000
+12	373	57	57.0	13.95	13.95	13.950000000000000000	13.950000000000000000
+12	1591	82	82.0	45.84	45.84	45.840000000000000000	45.840000000000000000
+12	4888	56	56.0	75.74	75.74	75.740000000000000000	75.740000000000000000
+12	6148	36	36.0	97.62	97.62	97.620000000000000000	97.620000000000000000
+12	6248	36	36.0	75.17	75.17	75.170000000000000000	75.170000000000000000
+12	9616	66	66.0	99.06	99.06	99.060000000000000000	99.060000000000000000
+12	9788	73	73.0	79.42	79.42	79.420000000000000000	79.420000000000000000
+12	13399	46	46.0	45.27	45.27	45.270000000000000000	45.270000000000000000
+12	14746	26	26.0	58.74	58.74	58.740000000000000000	58.740000000000000000
+12	14944	9	9.0	7.33	7.33	7.330000000000000000	7.330000000000000000
+12	15440	99	99.0	27.09	27.09	27.090000000000000000	27.090000000000000000
+13	868	NULL	NULL	62.85	62.85	62.850000000000000000	62.850000000000000000
+13	1760	12	12.0	80.96	80.96	80.960000000000000000	80.960000000000000000
+13	1898	NULL	NULL	96.46	96.46	96.460000000000000000	96.460000000000000000
+13	2108	9	9.0	NULL	NULL	NULL	NULL
+13	2191	NULL	NULL	NULL	NULL	NULL	NULL
+13	4430	73	73.0	5.86	5.86	5.860000000000000000	5.860000000000000000
+13	5971	80	80.0	72.61	72.61	72.610000000000000000	72.610000000000000000
+13	6085	58	58.0	21.45	21.45	21.450000000000000000	21.450000000000000000
+13	6140	15	15.0	89.9	89.9	89.900000000000000000	89.900000000000000000
+13	6682	80	80.0	32.05	32.05	32.050000000000000000	32.050000000000000000
+13	7640	48	48.0	17.06	17.06	17.060000000000000000	17.060000000000000000
+13	7723	27	27.0	59.09	59.09	59.090000000000000000	59.090000000000000000
+13	10096	12	12.0	17.14	17.14	17.140000000000000000	17.140000000000000000
+13	11758	34	34.0	72.24	72.24	72.240000000000000000	72.240000000000000000
+13	16894	87	87.0	20.99	20.99	20.990000000000000000	20.990000000000000000
+13	17240	20	20.0	93.85	93.85	93.850000000000000000	93.850000000000000000
+14	177	41	41.0	13.05	13.05	13.050000000000000000	13.050000000000000000
+14	769	20	20.0	26.29	26.29	26.290000000000000000	26.290000000000000000
+14	4507	4	4.0	45.45	45.45	45.450000000000000000	45.450000000000000000
+14	10175	19	19.0	39.97	39.97	39.970000000000000000	39.970000000000000000
+14	11549	6	6.0	19.33	19.33	19.330000000000000000	19.330000000000000000
+14	11653	60	60.0	86.94	86.94	86.940000000000000000	86.940000000000000000
+14	11817	81	81.0	60.77	60.77	60.770000000000000000	60.770000000000000000
+14	12587	NULL	NULL	NULL	NULL	NULL	NULL
+14	13069	77	77.0	93.6	93.6	93.600000000000000000	93.600000000000000000
+14	13515	57	57.0	87.32	87.32	87.320000000000000000	87.320000000000000000
+14	13845	17	17.0	52.3	52.3	52.300000000000000000	52.300000000000000000
+14	16741	46	46.0	76.43	76.43	76.430000000000000000	76.430000000000000000
+14	16929	14	14.0	54.76	54.76	54.760000000000000000	54.760000000000000000
+15	4241	21	21.0	89.07	89.07	89.070000000000000000	89.070000000000000000
+15	4505	59	59.0	77.35	77.35	77.350000000000000000	77.350000000000000000
+15	4777	28	28.0	36.86	36.86	36.860000000000000000	36.860000000000000000
+15	7391	98	98.0	53.76	53.76	53.760000000000000000	53.760000000000000000
+15	8336	15	15.0	44.09	44.09	44.090000000000000000	44.090000000000000000
+15	8353	NULL	NULL	NULL	NULL	NULL	NULL
+15	8690	32	32.0	67.37	67.37	67.370000000000000000	67.370000000000000000
+15	8707	21	21.0	48.54	48.54	48.540000000000000000	48.540000000000000000
+15	10361	39	39.0	74.88	74.88	74.880000000000000000	74.880000000000000000
+15	11659	80	80.0	86.23	86.23	86.230000000000000000	86.230000000000000000
+15	13172	25	25.0	47.11	47.11	47.110000000000000000	47.110000000000000000
+15	16619	81	81.0	80.21	80.21	80.210000000000000000	80.210000000000000000
+15	17267	7	7.0	30.61	30.61	30.610000000000000000	30.610000000000000000
+15	17330	82	82.0	67.45	67.45	67.450000000000000000	67.450000000000000000
+15	17564	26	26.0	63.52	63.52	63.520000000000000000	63.520000000000000000
+15	17857	38	38.0	96.35	96.35	96.350000000000000000	96.350000000000000000
+16	457	60	60.0	91.53	91.53	91.530000000000000000	91.530000000000000000
+16	1888	4	4.0	47.64	47.64	47.640000000000000000	47.640000000000000000
+16	4144	94	94.0	19.91	19.91	19.910000000000000000	19.910000000000000000
+16	6008	59	59.0	59.62	59.62	59.620000000000000000	59.620000000000000000
+16	7504	51	51.0	31.35	31.35	31.350000000000000000	31.350000000000000000
+16	8887	35	35.0	59.82	59.82	59.820000000000000000	59.820000000000000000
+16	9769	42	42.0	29.53	29.53	29.530000000000000000	29.530000000000000000
+16	9790	17	17.0	36.95	36.95	36.950000000000000000	36.950000000000000000
+16	9997	94	94.0	64.76	64.76	64.760000000000000000	64.760000000000000000
+16	11168	86	86.0	62.85	62.85	62.850000000000000000	62.850000000000000000
+16	11920	29	29.0	94.31	94.31	94.310000000000000000	94.310000000000000000
+16	16226	13	13.0	31.3	31.3	31.300000000000000000	31.300000000000000000
+16	17246	70	70.0	80.85	80.85	80.850000000000000000	80.850000000000000000
+17	2092	37	37.0	31.71	31.71	31.710000000000000000	31.710000000000000000
+17	4678	34	34.0	32.47	32.47	32.470000000000000000	32.470000000000000000
+17	6811	70	70.0	62.96	62.96	62.960000000000000000	62.960000000000000000
+17	9214	57	57.0	14.2	14.2	14.200000000000000000	14.200000000000000000
+17	10543	54	54.0	57.11	57.11	57.110000000000000000	57.110000000000000000
+17	11203	21	21.0	93.44	93.44	93.440000000000000000	93.440000000000000000
+17	13177	45	45.0	44.18	44.18	44.180000000000000000	44.180000000000000000
+17	13826	32	32.0	58.61	58.61	58.610000000000000000	58.610000000000000000
+17	15781	76	76.0	24.79	24.79	24.790000000000000000	24.790000000000000000
+17	17683	34	34.0	81.48	81.48	81.480000000000000000	81.480000000000000000
+18	2440	40	40.0	15.39	15.39	15.390000000000000000	15.390000000000000000
+18	5251	41	41.0	45.83	45.83	45.830000000000000000	45.830000000000000000
+18	7378	94	94.0	61.01	61.01	61.010000000000000000	61.010000000000000000
+18	8779	9	9.0	75.19	75.19	75.190000000000000000	75.190000000000000000
+18	8884	18	18.0	43.49	43.49	43.490000000000000000	43.490000000000000000
+18	9886	62	62.0	9.59	9.59	9.590000000000000000	9.590000000000000000
+18	11584	76	76.0	4.26	4.26	4.260000000000000000	4.260000000000000000
+18	11890	7	7.0	82.36	82.36	82.360000000000000000	82.360000000000000000
+18	12602	81	81.0	11.32	11.32	11.320000000000000000	11.320000000000000000
+18	12826	93	93.0	82.82	82.82	82.820000000000000000	82.820000000000000000
+18	12860	18	18.0	19.89	19.89	19.890000000000000000	19.890000000000000000
+18	14011	95	95.0	55.01	55.01	55.010000000000000000	55.010000000000000000
+18	14372	76	76.0	89.58	89.58	89.580000000000000000	89.580000000000000000
+18	14377	15	15.0	15.47	15.47	15.470000000000000000	15.470000000000000000
+18	17995	13	13.0	46.79	46.79	46.790000000000000000	46.790000000000000000
+19	1094	48	48.0	19.55	19.55	19.550000000000000000	19.550000000000000000
+19	3133	96	96.0	68.89	68.89	68.890000000000000000	68.890000000000000000
+19	3376	84	84.0	63.07	63.07	63.070000000000000000	63.070000000000000000
+19	4882	84	84.0	41.48	41.48	41.480000000000000000	41.480000000000000000
+19	6772	97	97.0	36.04	36.04	36.040000000000000000	36.040000000000000000
+19	7087	1	1.0	48.67	48.67	48.670000000000000000	48.670000000000000000
+19	7814	29	29.0	61.78	61.78	61.780000000000000000	61.780000000000000000
+19	8662	97	97.0	72.78	72.78	72.780000000000000000	72.780000000000000000
+19	9094	49	49.0	61.82	61.82	61.820000000000000000	61.820000000000000000
+19	9346	39	39.0	84.06	84.06	84.060000000000000000	84.060000000000000000
+19	10558	82	82.0	12.34	12.34	12.340000000000000000	12.340000000000000000
+19	10651	46	46.0	57.69	57.69	57.690000000000000000	57.690000000000000000
+19	11914	59	59.0	88.03	88.03	88.030000000000000000	88.030000000000000000
+19	16330	NULL	NULL	79.15	79.15	79.150000000000000000	79.150000000000000000
+19	17539	20	20.0	69.2	69.2	69.200000000000000000	69.200000000000000000
+20	1451	89	89.0	84.34	84.34	84.340000000000000000	84.340000000000000000
+20	2618	4	4.0	69.47	69.47	69.470000000000000000	69.470000000000000000
+20	5312	9	9.0	29.45	29.45	29.450000000000000000	29.450000000000000000
+20	5425	15	15.0	28.19	28.19	28.190000000000000000	28.190000000000000000
+20	5483	8	8.0	30.74	30.74	30.740000000000000000	30.740000000000000000
+20	6026	21	21.0	80.56	80.56	80.560000000000000000	80.560000000000000000
+20	7207	90	90.0	83.12	83.12	83.120000000000000000	83.120000000000000000
+20	8714	NULL	NULL	8.15	8.15	8.150000000000000000	8.150000000000000000
+20	9086	4	4.0	98.99	98.99	98.990000000000000000	98.990000000000000000
+20	9800	32	32.0	18.09	18.09	18.090000000000000000	18.090000000000000000
+20	13601	17	17.0	1.4	1.4	1.400000000000000000	1.400000000000000000
+20	14935	NULL	NULL	NULL	NULL	NULL	NULL
+20	15131	85	85.0	42.56	42.56	42.560000000000000000	42.560000000000000000
+21	230	48	48.0	13.37	13.37	13.370000000000000000	13.370000000000000000
+21	1810	59	59.0	66.37	66.37	66.370000000000000000	66.370000000000000000
+21	2870	50	50.0	91.94	91.94	91.940000000000000000	91.940000000000000000
+21	5170	45	45.0	90.0	90.0	90.000000000000000000	90.000000000000000000
+21	5998	51	51.0	9.41	9.41	9.410000000000000000	9.410000000000000000
+21	6476	49	49.0	20.29	20.29	20.290000000000000000	20.290000000000000000
+21	9187	14	14.0	35.49	35.49	35.490000000000000000	35.490000000000000000
+21	12266	47	47.0	11.55	11.55	11.550000000000000000	11.550000000000000000
+21	14368	18	18.0	51.29	51.29	51.290000000000000000	51.290000000000000000
+21	14396	88	88.0	45.26	45.26	45.260000000000000000	45.260000000000000000
+22	9985	70	70.0	21.46	21.46	21.460000000000000000	21.460000000000000000
+22	10474	31	31.0	45.65	45.65	45.650000000000000000	45.650000000000000000
+22	11599	66	66.0	5.01	5.01	5.010000000000000000	5.010000000000000000
+22	12415	10	10.0	38.97	38.97	38.970000000000000000	38.970000000000000000
+22	15310	15	15.0	82.24	82.24	82.240000000000000000	82.240000000000000000
+22	16396	85	85.0	86.46	86.46	86.460000000000000000	86.460000000000000000
+22	16922	88	88.0	28.0	28.0	28.000000000000000000	28.000000000000000000
+22	17392	14	14.0	51.86	51.86	51.860000000000000000	51.860000000000000000
+22	17660	70	70.0	95.56	95.56	95.560000000000000000	95.560000000000000000
+23	319	86	86.0	66.36	66.36	66.360000000000000000	66.360000000000000000
+23	7242	37	37.0	54.82	54.82	54.820000000000000000	54.820000000000000000
+23	8181	13	13.0	4.63	4.63	4.630000000000000000	4.630000000000000000
+23	8413	1	1.0	14.2	14.2	14.200000000000000000	14.200000000000000000
+23	9093	38	38.0	80.2	80.2	80.200000000000000000	80.200000000000000000
+23	9097	81	81.0	72.51	72.51	72.510000000000000000	72.510000000000000000
+23	11220	91	91.0	71.3	71.3	71.300000000000000000	71.300000000000000000
+23	11257	64	64.0	29.95	29.95	29.950000000000000000	29.950000000000000000
+23	12397	80	80.0	78.73	78.73	78.730000000000000000	78.730000000000000000
+23	15403	96	96.0	51.96	51.96	51.960000000000000000	51.960000000000000000
+23	17631	16	16.0	22.06	22.06	22.060000000000000000	22.060000000000000000
+24	407	53	53.0	98.05	98.05	98.050000000000000000	98.050000000000000000
+24	1389	72	72.0	60.01	60.01	60.010000000000000000	60.010000000000000000
+24	1795	21	21.0	76.67	76.67	76.670000000000000000	76.670000000000000000
+24	2497	85	85.0	57.93	57.93	57.930000000000000000	57.930000000000000000
+24	3103	73	73.0	44.96	44.96	44.960000000000000000	44.960000000000000000
+24	4425	57	57.0	29.31	29.31	29.310000000000000000	29.310000000000000000
+24	4749	28	28.0	18.17	18.17	18.170000000000000000	18.170000000000000000
+24	4873	41	41.0	40.34	40.34	40.340000000000000000	40.340000000000000000
+24	5653	92	92.0	64.99	64.99	64.990000000000000000	64.990000000000000000
+24	6043	1	1.0	33.41	33.41	33.410000000000000000	33.410000000000000000
+24	6751	82	82.0	7.48	7.48	7.480000000000000000	7.480000000000000000
+24	7375	97	97.0	78.55	78.55	78.550000000000000000	78.550000000000000000
+24	10265	93	93.0	12.03	12.03	12.030000000000000000	12.030000000000000000
+24	11551	48	48.0	30.8	30.8	30.800000000000000000	30.800000000000000000
+24	13303	97	97.0	94.48	94.48	94.480000000000000000	94.480000000000000000
+24	16483	89	89.0	13.84	13.84	13.840000000000000000	13.840000000000000000
+25	1333	55	55.0	30.82	30.82	30.820000000000000000	30.820000000000000000
+25	2150	100	100.0	67.24	67.24	67.240000000000000000	67.240000000000000000
+25	2608	76	76.0	87.75	87.75	87.750000000000000000	87.750000000000000000
+25	3454	100	100.0	1.61	1.61	1.610000000000000000	1.610000000000000000
+25	4880	29	29.0	15.35	15.35	15.350000000000000000	15.350000000000000000
+25	5954	34	34.0	76.57	76.57	76.570000000000000000	76.570000000000000000
+25	6955	40	40.0	87.12	87.12	87.120000000000000000	87.120000000000000000
+25	7874	65	65.0	2.75	2.75	2.750000000000000000	2.750000000000000000
+25	9472	48	48.0	4.97	4.97	4.970000000000000000	4.970000000000000000
+25	10159	24	24.0	76.64	76.64	76.640000000000000000	76.640000000000000000
+25	14488	26	26.0	68.17	68.17	68.170000000000000000	68.170000000000000000
+25	14635	68	68.0	45.79	45.79	45.790000000000000000	45.790000000000000000
+25	17000	40	40.0	89.34	89.34	89.340000000000000000	89.340000000000000000
+25	17752	55	55.0	11.49	11.49	11.490000000000000000	11.490000000000000000
+26	1989	26	26.0	83.31	83.31	83.310000000000000000	83.310000000000000000
+26	5053	4	4.0	19.63	19.63	19.630000000000000000	19.630000000000000000
+26	5385	97	97.0	51.89	51.89	51.890000000000000000	51.890000000000000000
+26	5721	81	81.0	74.96	74.96	74.960000000000000000	74.960000000000000000
+26	6647	64	64.0	57.04	57.04	57.040000000000000000	57.040000000000000000
+26	7337	45	45.0	37.59	37.59	37.590000000000000000	37.590000000000000000
+26	9679	18	18.0	77.54	77.54	77.540000000000000000	77.540000000000000000
+26	11895	77	77.0	36.85	36.85	36.850000000000000000	36.850000000000000000
+26	12851	56	56.0	14.02	14.02	14.020000000000000000	14.020000000000000000
+26	15039	34	34.0	22.65	22.65	22.650000000000000000	22.650000000000000000
+27	1305	44	44.0	8.35	8.35	8.350000000000000000	8.350000000000000000
+27	2137	96	96.0	3.07	3.07	3.070000000000000000	3.070000000000000000
+27	2671	92	92.0	4.35	4.35	4.350000000000000000	4.350000000000000000
+27	5831	61	61.0	8.79	8.79	8.790000000000000000	8.790000000000000000
+27	7139	59	59.0	6.17	6.17	6.170000000000000000	6.170000000000000000
+27	8167	28	28.0	38.83	38.83	38.830000000000000000	38.830000000000000000
+27	10757	15	15.0	8.7	8.7	8.700000000000000000	8.700000000000000000
+27	11441	15	15.0	14.45	14.45	14.450000000000000000	14.450000000000000000
+27	11509	65	65.0	80.34	80.34	80.340000000000000000	80.340000000000000000
+27	12237	89	89.0	73.9	73.9	73.900000000000000000	73.900000000000000000
+27	12749	31	31.0	80.27	80.27	80.270000000000000000	80.270000000000000000
+27	13885	66	66.0	40.62	40.62	40.620000000000000000	40.620000000000000000
+27	15025	26	26.0	35.56	35.56	35.560000000000000000	35.560000000000000000
+27	16029	59	59.0	2.11	2.11	2.110000000000000000	2.110000000000000000
+27	16419	65	65.0	80.1	80.1	80.100000000000000000	80.100000000000000000
+27	16767	60	60.0	68.33	68.33	68.330000000000000000	68.330000000000000000
+28	1807	98	98.0	78.91	78.91	78.910000000000000000	78.910000000000000000
+28	2817	8	8.0	98.75	98.75	98.750000000000000000	98.750000000000000000
+28	2967	29	29.0	47.87	47.87	47.870000000000000000	47.870000000000000000
+28	4483	78	78.0	73.9	73.9	73.900000000000000000	73.900000000000000000
+28	5437	15	15.0	7.49	7.49	7.490000000000000000	7.490000000000000000
+28	6411	3	3.0	67.26	67.26	67.260000000000000000	67.260000000000000000
+28	7965	93	93.0	77.74	77.74	77.740000000000000000	77.740000000000000000
+28	8043	58	58.0	60.26	60.26	60.260000000000000000	60.260000000000000000
+28	8407	14	14.0	95.01	95.01	95.010000000000000000	95.010000000000000000
+28	10295	13	13.0	31.83	31.83	31.830000000000000000	31.830000000000000000
+29	20	18	18.0	66.26	66.26	66.260000000000000000	66.260000000000000000
+29	1363	75	75.0	NULL	NULL	NULL	NULL
+29	2930	23	23.0	64.78	64.78	64.780000000000000000	64.780000000000000000
+29	3740	5	5.0	90.13	90.13	90.130000000000000000	90.130000000000000000
+29	7654	20	20.0	98.14	98.14	98.140000000000000000	98.140000000000000000
+29	9458	33	33.0	52.33	52.33	52.330000000000000000	52.330000000000000000
+29	10795	33	33.0	68.24	68.24	68.240000000000000000	68.240000000000000000
+29	12068	37	37.0	80.75	80.75	80.750000000000000000	80.750000000000000000
+29	12223	59	59.0	12.89	12.89	12.890000000000000000	12.890000000000000000
+29	13340	21	21.0	40.5	40.5	40.500000000000000000	40.500000000000000000
+29	13693	NULL	NULL	95.63	95.63	95.630000000000000000	95.630000000000000000
+29	15085	40	40.0	NULL	NULL	NULL	NULL
+29	15626	NULL	NULL	17.61	17.61	17.610000000000000000	17.610000000000000000
+29	15782	53	53.0	57.11	57.11	57.110000000000000000	57.110000000000000000
+30	217	91	91.0	52.03	52.03	52.030000000000000000	52.030000000000000000
+30	1951	59	59.0	17.14	17.14	17.140000000000000000	17.140000000000000000
+30	3238	16	16.0	9.84	9.84	9.840000000000000000	9.840000000000000000
+30	3506	15	15.0	16.31	16.31	16.310000000000000000	16.310000000000000000
+30	3928	87	87.0	27.01	27.01	27.010000000000000000	27.010000000000000000
+30	5431	77	77.0	52.37	52.37	52.370000000000000000	52.370000000000000000
+30	6752	69	69.0	40.8	40.8	40.800000000000000000	40.800000000000000000
+30	7870	7	7.0	4.51	4.51	4.510000000000000000	4.510000000000000000
+30	8666	21	21.0	64.0	64.0	64.000000000000000000	64.000000000000000000
+30	12572	33	33.0	61.96	61.96	61.960000000000000000	61.960000000000000000
+30	12670	20	20.0	6.44	6.44	6.440000000000000000	6.440000000000000000
+30	13579	75	75.0	62.71	62.71	62.710000000000000000	62.710000000000000000
+30	14848	62	62.0	64.03	64.03	64.030000000000000000	64.030000000000000000
+30	17348	62	62.0	88.74	88.74	88.740000000000000000	88.740000000000000000
+30	17875	78	78.0	2.91	2.91	2.910000000000000000	2.910000000000000000
+31	913	54	54.0	79.11	79.11	79.110000000000000000	79.110000000000000000
+31	4963	67	67.0	56.37	56.37	56.370000000000000000	56.370000000000000000
+31	6617	11	11.0	86.78	86.78	86.780000000000000000	86.780000000000000000
+31	6917	4	4.0	49.76	49.76	49.760000000000000000	49.760000000000000000
+31	7513	82	82.0	44.95	44.95	44.950000000000000000	44.950000000000000000
+31	11739	95	95.0	6.99	6.99	6.990000000000000000	6.990000000000000000
+31	14575	97	97.0	59.9	59.9	59.900000000000000000	59.900000000000000000
+31	14727	41	41.0	48.1	48.1	48.100000000000000000	48.100000000000000000
+31	15341	31	31.0	16.15	16.15	16.150000000000000000	16.150000000000000000
+31	15411	53	53.0	47.64	47.64	47.640000000000000000	47.640000000000000000
+31	16251	51	51.0	91.49	91.49	91.490000000000000000	91.490000000000000000
+32	1115	61	61.0	97.03	97.03	97.030000000000000000	97.030000000000000000
+32	2095	34	34.0	89.33	89.33	89.330000000000000000	89.330000000000000000
+32	2887	8	8.0	48.71	48.71	48.710000000000000000	48.710000000000000000
+32	4339	6	6.0	88.27	88.27	88.270000000000000000	88.270000000000000000
+32	4537	22	22.0	65.72	65.72	65.720000000000000000	65.720000000000000000
+32	4808	NULL	NULL	57.01	57.01	57.010000000000000000	57.010000000000000000
+32	5798	87	87.0	46.23	46.23	46.230000000000000000	46.230000000000000000
+32	7547	24	24.0	43.33	43.33	43.330000000000000000	43.330000000000000000
+32	9683	26	26.0	NULL	NULL	NULL	NULL
+32	11005	46	46.0	51.48	51.48	51.480000000000000000	51.480000000000000000
+32	11348	41	41.0	55.14	55.14	55.140000000000000000	55.140000000000000000
+32	12134	21	21.0	51.01	51.01	51.010000000000000000	51.010000000000000000
+32	15001	57	57.0	30.07	30.07	30.070000000000000000	30.070000000000000000
+32	15644	34	34.0	80.54	80.54	80.540000000000000000	80.540000000000000000
+32	16421	74	74.0	89.89	89.89	89.890000000000000000	89.890000000000000000
+32	17659	51	51.0	23.88	23.88	23.880000000000000000	23.880000000000000000
+33	4798	27	27.0	28.56	28.56	28.560000000000000000	28.560000000000000000
+33	7300	3	3.0	3.13	3.13	3.130000000000000000	3.130000000000000000
+33	9649	36	36.0	18.91	18.91	18.910000000000000000	18.910000000000000000
+33	10376	21	21.0	55.09	55.09	55.090000000000000000	55.090000000000000000
+33	11119	92	92.0	3.49	3.49	3.490000000000000000	3.490000000000000000
+33	11756	26	26.0	58.87	58.87	58.870000000000000000	58.870000000000000000
+33	12643	89	89.0	35.74	35.74	35.740000000000000000	35.740000000000000000
+33	12760	54	54.0	48.97	48.97	48.970000000000000000	48.970000000000000000
+33	12964	80	80.0	83.86	83.86	83.860000000000000000	83.860000000000000000
+33	14125	66	66.0	44.03	44.03	44.030000000000000000	44.030000000000000000
+33	14158	82	82.0	48.07	48.07	48.070000000000000000	48.070000000000000000
+33	14692	93	93.0	56.78	56.78	56.780000000000000000	56.780000000000000000
+33	15478	22	22.0	95.96	95.96	95.960000000000000000	95.960000000000000000
+34	1526	91	91.0	78.12	78.12	78.120000000000000000	78.120000000000000000
+34	1717	53	53.0	99.68	99.68	99.680000000000000000	99.680000000000000000
+34	2312	6	6.0	51.4	51.4	51.400000000000000000	51.400000000000000000
+34	4118	88	88.0	38.38	38.38	38.380000000000000000	38.380000000000000000
+34	5197	63	63.0	13.5	13.5	13.500000000000000000	13.500000000000000000
+34	5449	9	9.0	21.24	21.24	21.240000000000000000	21.240000000000000000
+34	6193	61	61.0	54.55	54.55	54.550000000000000000	54.550000000000000000
+34	9325	3	3.0	92.35	92.35	92.350000000000000000	92.350000000000000000
+34	9766	83	83.0	68.57	68.57	68.570000000000000000	68.570000000000000000
+34	12016	42	42.0	42.44	42.44	42.440000000000000000	42.440000000000000000
+34	12290	53	53.0	88.61	88.61	88.610000000000000000	88.610000000000000000
+34	12512	60	60.0	40.48	40.48	40.480000000000000000	40.480000000000000000
+34	13814	20	20.0	22.82	22.82	22.820000000000000000	22.820000000000000000
+34	16324	30	30.0	37.27	37.27	37.270000000000000000	37.270000000000000000
+35	411	51	51.0	NULL	NULL	NULL	NULL
+35	2377	52	52.0	98.03	98.03	98.030000000000000000	98.030000000000000000
+35	3667	97	97.0	59.31	59.31	59.310000000000000000	59.310000000000000000
+35	4325	56	56.0	67.43	67.43	67.430000000000000000	67.430000000000000000
+35	5179	83	83.0	90.54	90.54	90.540000000000000000	90.540000000000000000
+35	11635	87	87.0	92.02	92.02	92.020000000000000000	92.020000000000000000
+35	11661	81	81.0	NULL	NULL	NULL	NULL
+35	14239	55	55.0	8.27	8.27	8.270000000000000000	8.270000000000000000
+35	15619	45	45.0	90.28	90.28	90.280000000000000000	90.280000000000000000
+35	15757	9	9.0	14.83	14.83	14.830000000000000000	14.830000000000000000
+35	17341	92	92.0	59.48	59.48	59.480000000000000000	59.480000000000000000
+35	17365	65	65.0	76.2	76.2	76.200000000000000000	76.200000000000000000
+35	17451	7	7.0	45.66	45.66	45.660000000000000000	45.660000000000000000
+36	1115	80	80.0	11.13	11.13	11.130000000000000000	11.130000000000000000
+36	2095	43	43.0	91.17	91.17	91.170000000000000000	91.170000000000000000
+36	2887	31	31.0	24.53	24.53	24.530000000000000000	24.530000000000000000
+36	7547	46	46.0	8.04	8.04	8.040000000000000000	8.040000000000000000
+36	11005	49	49.0	70.6	70.6	70.600000000000000000	70.600000000000000000
+36	11349	80	80.0	58.17	58.17	58.170000000000000000	58.170000000000000000
+36	15001	54	54.0	16.24	16.24	16.240000000000000000	16.240000000000000000
+36	15645	23	23.0	32.35	32.35	32.350000000000000000	32.350000000000000000
+36	16421	25	25.0	69.67	69.67	69.670000000000000000	69.670000000000000000
+36	17561	16	16.0	82.46	82.46	82.460000000000000000	82.460000000000000000
+36	17659	91	91.0	44.83	44.83	44.830000000000000000	44.830000000000000000
+37	2997	94	94.0	85.67	85.67	85.670000000000000000	85.670000000000000000
+37	7283	87	87.0	54.25	54.25	54.250000000000000000	54.250000000000000000
+37	10715	52	52.0	89.22	89.22	89.220000000000000000	89.220000000000000000
+37	10929	88	88.0	65.45	65.45	65.450000000000000000	65.450000000000000000
+37	13171	6	6.0	84.14	84.14	84.140000000000000000	84.140000000000000000
+37	15337	62	62.0	16.64	16.64	16.640000000000000000	16.640000000000000000
+37	16971	12	12.0	53.97	53.97	53.970000000000000000	53.970000000000000000
+37	17125	NULL	NULL	NULL	NULL	NULL	NULL
+38	757	2	2.0	NULL	NULL	NULL	NULL
+38	2164	17	17.0	72.04	72.04	72.040000000000000000	72.040000000000000000
+38	3439	84	84.0	11.71	11.71	11.710000000000000000	11.710000000000000000
+38	4154	35	35.0	10.28	10.28	10.280000000000000000	10.280000000000000000
+38	5113	73	73.0	50.59	50.59	50.590000000000000000	50.590000000000000000
+38	6220	98	98.0	14.54	14.54	14.540000000000000000	14.540000000000000000
+38	7018	15	15.0	69.78	69.78	69.780000000000000000	69.780000000000000000
+38	7784	56	56.0	31.89	31.89	31.890000000000000000	31.890000000000000000
+38	8870	15	15.0	46.69	46.69	46.690000000000000000	46.690000000000000000
+38	9710	7	7.0	82.77	82.77	82.770000000000000000	82.770000000000000000
+38	10441	62	62.0	80.37	80.37	80.370000000000000000	80.370000000000000000
+38	15698	57	57.0	11.4	11.4	11.400000000000000000	11.400000000000000000
+39	386	89	89.0	28.08	28.08	28.080000000000000000	28.080000000000000000
+39	1598	64	64.0	44.63	44.63	44.630000000000000000	44.630000000000000000
+39	3476	73	73.0	80.57	80.57	80.570000000000000000	80.570000000000000000
+39	3943	64	64.0	59.68	59.68	59.680000000000000000	59.680000000000000000
+39	4190	86	86.0	35.56	35.56	35.560000000000000000	35.560000000000000000
+39	4957	24	24.0	16.1	16.1	16.100000000000000000	16.100000000000000000
+39	5393	98	98.0	58.75	58.75	58.750000000000000000	58.750000000000000000
+39	7097	78	78.0	33.1	33.1	33.100000000000000000	33.100000000000000000
+39	7118	67	67.0	68.99	68.99	68.990000000000000000	68.990000000000000000
+39	7604	49	49.0	46.49	46.49	46.490000000000000000	46.490000000000000000
+39	7697	24	24.0	44.89	44.89	44.890000000000000000	44.890000000000000000
+39	8078	54	54.0	73.6	73.6	73.600000000000000000	73.600000000000000000
+39	8411	96	96.0	35.69	35.69	35.690000000000000000	35.690000000000000000
+39	15491	54	54.0	3.2	3.2	3.200000000000000000	3.200000000000000000
+39	15625	17	17.0	96.62	96.62	96.620000000000000000	96.620000000000000000
+40	2854	71	71.0	10.62	10.62	10.620000000000000000	10.620000000000000000
+40	3490	65	65.0	41.24	41.24	41.240000000000000000	41.240000000000000000
+40	3985	63	63.0	22.94	22.94	22.940000000000000000	22.940000000000000000
+40	5098	35	35.0	33.91	33.91	33.910000000000000000	33.910000000000000000
+40	5318	87	87.0	32.66	32.66	32.660000000000000000	32.660000000000000000
+40	10094	80	80.0	8.63	8.63	8.630000000000000000	8.630000000000000000
+40	10912	23	23.0	2.46	2.46	2.460000000000000000	2.460000000000000000
+40	12050	NULL	NULL	38.12	38.12	38.120000000000000000	38.120000000000000000
+40	13658	53	53.0	56.42	56.42	56.420000000000000000	56.420000000000000000
+40	16976	3	3.0	20.7	20.7	20.700000000000000000	20.700000000000000000
+41	10	50	50.0	54.36	54.36	54.360000000000000000	54.360000000000000000
+41	64	29	29.0	27.18	27.18	27.180000000000000000	27.180000000000000000
+41	3380	88	88.0	14.11	14.11	14.110000000000000000	14.110000000000000000
+41	5566	11	11.0	50.45	50.45	50.450000000000000000	50.450000000000000000
+41	6310	90	90.0	60.1	60.1	60.100000000000000000	60.100000000000000000
+41	7402	69	69.0	57.23	57.23	57.230000000000000000	57.230000000000000000
+41	7603	94	94.0	6.12	6.12	6.120000000000000000	6.120000000000000000
+41	9322	8	8.0	59.4	59.4	59.400000000000000000	59.400000000000000000
+41	10915	81	81.0	91.63	91.63	91.630000000000000000	91.630000000000000000
+41	14788	15	15.0	90.04	90.04	90.040000000000000000	90.040000000000000000
+41	15242	87	87.0	48.25	48.25	48.250000000000000000	48.250000000000000000
+41	15328	46	46.0	84.03	84.03	84.030000000000000000	84.030000000000000000
+41	16514	20	20.0	5.05	5.05	5.050000000000000000	5.050000000000000000
+42	619	69	69.0	56.85	56.85	56.850000000000000000	56.850000000000000000
+42	976	100	100.0	12.59	12.59	12.590000000000000000	12.590000000000000000
+42	1436	94	94.0	54.21	54.21	54.210000000000000000	54.210000000000000000
+42	2314	74	74.0	24.46	24.46	24.460000000000000000	24.460000000000000000
+42	2392	14	14.0	49.48	49.48	49.480000000000000000	49.480000000000000000
+42	2602	30	30.0	55.77	55.77	55.770000000000000000	55.770000000000000000
+42	3346	74	74.0	29.72	29.72	29.720000000000000000	29.720000000000000000
+42	3613	30	30.0	56.33	56.33	56.330000000000000000	56.330000000000000000
+42	6058	30	30.0	81.1	81.1	81.100000000000000000	81.100000000000000000
+42	6134	92	92.0	18.91	18.91	18.910000000000000000	18.910000000000000000
+42	8462	23	23.0	27.88	27.88	27.880000000000000000	27.880000000000000000
+42	9740	52	52.0	52.46	52.46	52.460000000000000000	52.460000000000000000
+42	10016	57	57.0	12.47	12.47	12.470000000000000000	12.470000000000000000
+42	10471	19	19.0	42.67	42.67	42.670000000000000000	42.670000000000000000
+42	12550	41	41.0	17.09	17.09	17.090000000000000000	17.090000000000000000
+42	15002	41	41.0	58.33	58.33	58.330000000000000000	58.330000000000000000
+43	2923	16	16.0	82.12	82.12	82.120000000000000000	82.120000000000000000
+43	3344	22	22.0	88.77	88.77	88.770000000000000000	88.770000000000000000
+43	3911	26	26.0	21.75	21.75	21.750000000000000000	21.750000000000000000
+43	4364	77	77.0	82.92	82.92	82.920000000000000000	82.920000000000000000
+43	4691	41	41.0	2.24	2.24	2.240000000000000000	2.240000000000000000
+43	5773	85	85.0	66.42	66.42	66.420000000000000000	66.420000000000000000
+43	5852	16	16.0	81.99	81.99	81.990000000000000000	81.990000000000000000
+43	11771	30	30.0	41.13	41.13	41.130000000000000000	41.130000000000000000
+43	14669	97	97.0	52.94	52.94	52.940000000000000000	52.940000000000000000
+44	2351	56	56.0	55.53	55.53	55.530000000000000000	55.530000000000000000
+44	2623	18	18.0	39.17	39.17	39.170000000000000000	39.170000000000000000
+44	7303	14	14.0	36.13	36.13	36.130000000000000000	36.130000000000000000
+44	7527	67	67.0	90.05	90.05	90.050000000000000000	90.050000000000000000
+44	9059	68	68.0	30.11	30.11	30.110000000000000000	30.110000000000000000
+44	11707	83	83.0	85.49	85.49	85.490000000000000000	85.490000000000000000
+44	12341	20	20.0	82.28	82.28	82.280000000000000000	82.280000000000000000
+44	13331	98	98.0	3.53	3.53	3.530000000000000000	3.530000000000000000
+44	13449	45	45.0	50.83	50.83	50.830000000000000000	50.830000000000000000
+44	14149	80	80.0	18.83	18.83	18.830000000000000000	18.830000000000000000
+44	15803	81	81.0	43.81	43.81	43.810000000000000000	43.810000000000000000
+44	16491	56	56.0	32.28	32.28	32.280000000000000000	32.280000000000000000
+44	16837	92	92.0	30.11	30.11	30.110000000000000000	30.110000000000000000
+44	16909	61	61.0	92.15	92.15	92.150000000000000000	92.150000000000000000
+45	811	62	62.0	23.41	23.41	23.410000000000000000	23.410000000000000000
+45	1479	49	49.0	5.01	5.01	5.010000000000000000	5.010000000000000000
+45	3265	98	98.0	27.12	27.12	27.120000000000000000	27.120000000000000000
+45	5309	18	18.0	51.16	51.16	51.160000000000000000	51.160000000000000000
+45	7363	87	87.0	85.95	85.95	85.950000000000000000	85.950000000000000000
+45	10115	68	68.0	38.09	38.09	38.090000000000000000	38.090000000000000000
+45	11095	40	40.0	52.97	52.97	52.970000000000000000	52.970000000000000000
+45	13133	46	46.0	85.87	85.87	85.870000000000000000	85.870000000000000000
+45	16349	6	6.0	94.59	94.59	94.590000000000000000	94.590000000000000000
+46	1960	12	12.0	53.47	53.47	53.470000000000000000	53.470000000000000000
+46	3010	67	67.0	66.87	66.87	66.870000000000000000	66.870000000000000000
+46	7040	33	33.0	90.87	90.87	90.870000000000000000	90.870000000000000000
+46	8065	NULL	NULL	43.04	43.04	43.040000000000000000	43.040000000000000000
+46	11426	72	72.0	53.81	53.81	53.810000000000000000	53.810000000000000000
+46	13042	58	58.0	41.38	41.38	41.380000000000000000	41.380000000000000000
+46	15595	32	32.0	29.12	29.12	29.120000000000000000	29.120000000000000000
+46	16540	30	30.0	54.36	54.36	54.360000000000000000	54.360000000000000000
+46	17150	57	57.0	71.68	71.68	71.680000000000000000	71.680000000000000000
+46	17384	13	13.0	93.68	93.68	93.680000000000000000	93.680000000000000000
+47	254	NULL	NULL	NULL	NULL	NULL	NULL
+47	481	30	30.0	36.51	36.51	36.510000000000000000	36.510000000000000000
+47	1132	66	66.0	53.46	53.46	53.460000000000000000	53.460000000000000000
+47	1916	71	71.0	47.62	47.62	47.620000000000000000	47.620000000000000000
+47	3085	51	51.0	63.55	63.55	63.550000000000000000	63.550000000000000000
+47	3202	7	7.0	26.06	26.06	26.060000000000000000	26.060000000000000000
+47	3878	NULL	NULL	NULL	NULL	NULL	NULL
+47	4774	11	11.0	63.71	63.71	63.710000000000000000	63.710000000000000000
+47	5008	82	82.0	1.76	1.76	1.760000000000000000	1.760000000000000000
+47	5305	NULL	NULL	84.7	84.7	84.700000000000000000	84.700000000000000000
+47	5468	7	7.0	5.03	5.03	5.030000000000000000	5.030000000000000000
+47	7214	1	1.0	12.8	12.8	12.800000000000000000	12.800000000000000000
+47	9770	33	33.0	69.12	69.12	69.120000000000000000	69.120000000000000000
+47	13246	47	47.0	11.71	11.71	11.710000000000000000	11.710000000000000000
+47	13477	10	10.0	78.83	78.83	78.830000000000000000	78.830000000000000000
+48	1761	22	22.0	55.73	55.73	55.730000000000000000	55.730000000000000000
+48	2820	4	4.0	6.46	6.46	6.460000000000000000	6.460000000000000000
+48	2829	65	65.0	22.1	22.1	22.100000000000000000	22.100000000000000000
+48	4431	39	39.0	97.07	97.07	97.070000000000000000	97.070000000000000000
+48	5971	29	29.0	40.46	40.46	40.460000000000000000	40.460000000000000000
+48	6085	1	1.0	58.13	58.13	58.130000000000000000	58.130000000000000000
+48	6684	44	44.0	20.22	20.22	20.220000000000000000	20.220000000000000000
+48	9199	88	88.0	37.89	37.89	37.890000000000000000	37.890000000000000000
+48	11259	NULL	NULL	NULL	NULL	NULL	NULL
+48	12468	62	62.0	43.72	43.72	43.720000000000000000	43.720000000000000000
+48	13153	74	74.0	34.26	34.26	34.260000000000000000	34.260000000000000000
+48	17799	17	17.0	80.36	80.36	80.360000000000000000	80.360000000000000000
+49	749	60	60.0	42.11	42.11	42.110000000000000000	42.110000000000000000
+49	2135	4	4.0	15.8	15.8	15.800000000000000000	15.800000000000000000
+49	5342	69	69.0	46.41	46.41	46.410000000000000000	46.410000000000000000
+49	5852	47	47.0	74.9	74.9	74.900000000000000000	74.900000000000000000
+49	6805	40	40.0	12.9	12.9	12.900000000000000000	12.900000000000000000
+49	7141	94	94.0	50.5	50.5	50.500000000000000000	50.500000000000000000
+49	9049	68	68.0	75.38	75.38	75.380000000000000000	75.380000000000000000
+49	9553	71	71.0	29.28	29.28	29.280000000000000000	29.280000000000000000
+49	12737	48	48.0	2.17	2.17	2.170000000000000000	2.170000000000000000
+49	15155	84	84.0	4.4	4.4	4.400000000000000000	4.400000000000000000
+49	16361	4	4.0	79.85	79.85	79.850000000000000000	79.850000000000000000
+50	1280	69	69.0	8.66	8.66	8.660000000000000000	8.660000000000000000
+50	1312	30	30.0	25.84	25.84	25.840000000000000000	25.840000000000000000
+50	1909	53	53.0	56.01	56.01	56.010000000000000000	56.010000000000000000
+50	1984	40	40.0	8.81	8.81	8.810000000000000000	8.810000000000000000
+50	3097	64	64.0	33.17	33.17	33.170000000000000000	33.170000000000000000
+50	5023	NULL	NULL	16.24	16.24	16.240000000000000000	16.240000000000000000
+50	7135	69	69.0	12.68	12.68	12.680000000000000000	12.680000000000000000
+50	16081	82	82.0	99.55	99.55	99.550000000000000000	99.550000000000000000
+51	422	21	21.0	69.89	69.89	69.890000000000000000	69.890000000000000000
+51	3091	28	28.0	92.87	92.87	92.870000000000000000	92.870000000000000000
+51	4687	6	6.0	93.02	93.02	93.020000000000000000	93.020000000000000000
+51	5029	12	12.0	34.53	34.53	34.530000000000000000	34.530000000000000000
+51	5059	51	51.0	48.54	48.54	48.540000000000000000	48.540000000000000000
+51	6565	33	33.0	32.44	32.44	32.440000000000000000	32.440000000000000000
+51	8384	79	79.0	15.35	15.35	15.350000000000000000	15.350000000000000000
+51	9311	90	90.0	39.48	39.48	39.480000000000000000	39.480000000000000000
+51	10133	54	54.0	46.71	46.71	46.710000000000000000	46.710000000000000000
+51	11234	NULL	NULL	NULL	NULL	NULL	NULL
+51	12625	53	53.0	97.27	97.27	97.270000000000000000	97.270000000000000000
+51	13199	97	97.0	99.32	99.32	99.320000000000000000	99.320000000000000000
+51	17483	22	22.0	31.99	31.99	31.990000000000000000	31.990000000000000000
+51	17705	66	66.0	46.11	46.11	46.110000000000000000	46.110000000000000000
+52	2420	90	90.0	22.31	22.31	22.310000000000000000	22.310000000000000000
+52	3334	73	73.0	29.2	29.2	29.200000000000000000	29.200000000000000000
+52	6098	NULL	NULL	4.83	4.83	4.830000000000000000	4.830000000000000000
+52	7606	45	45.0	42.51	42.51	42.510000000000000000	42.510000000000000000
+52	11488	76	76.0	78.68	78.68	78.680000000000000000	78.680000000000000000
+52	15649	29	29.0	22.86	22.86	22.860000000000000000	22.860000000000000000
+52	16646	48	48.0	95.82	95.82	95.820000000000000000	95.820000000000000000
+52	17402	91	91.0	81.94	81.94	81.940000000000000000	81.940000000000000000
+52	17456	37	37.0	7.93	7.93	7.930000000000000000	7.930000000000000000
+53	1114	40	40.0	28.34	28.34	28.340000000000000000	28.340000000000000000
+53	2095	62	62.0	23.98	23.98	23.980000000000000000	23.980000000000000000
+53	2786	70	70.0	76.55	76.55	76.550000000000000000	76.550000000000000000
+53	2887	39	39.0	66.68	66.68	66.680000000000000000	66.680000000000000000
+53	7546	58	58.0	73.79	73.79	73.790000000000000000	73.790000000000000000
+53	11348	38	38.0	5.54	5.54	5.540000000000000000	5.540000000000000000
+53	13220	76	76.0	27.93	27.93	27.930000000000000000	27.930000000000000000
+53	13795	38	38.0	93.96	93.96	93.960000000000000000	93.960000000000000000
+53	15991	37	37.0	77.75	77.75	77.750000000000000000	77.750000000000000000
+53	16420	14	14.0	36.72	36.72	36.720000000000000000	36.720000000000000000
+53	16648	79	79.0	55.29	55.29	55.290000000000000000	55.290000000000000000
+53	17296	43	43.0	21.4	21.4	21.400000000000000000	21.400000000000000000
+53	17560	15	15.0	46.39	46.39	46.390000000000000000	46.390000000000000000
+54	702	40	40.0	16.76	16.76	16.760000000000000000	16.760000000000000000
+54	825	50	50.0	99.64	99.64	99.640000000000000000	99.640000000000000000
+54	1165	62	62.0	69.84	69.84	69.840000000000000000	69.840000000000000000
+54	3861	NULL	NULL	NULL	NULL	NULL	NULL
+54	6517	40	40.0	23.38	23.38	23.380000000000000000	23.380000000000000000
+54	9159	75	75.0	55.47	55.47	55.470000000000000000	55.470000000000000000
+54	14737	38	38.0	29.2	29.2	29.200000000000000000	29.200000000000000000
+54	16059	15	15.0	7.9	7.9	7.900000000000000000	7.900000000000000000
+54	16974	NULL	NULL	NULL	NULL	NULL	NULL
+54	17479	34	34.0	94.14	94.14	94.140000000000000000	94.140000000000000000
+55	1339	16	16.0	71.32	71.32	71.320000000000000000	71.320000000000000000
+55	3001	7	7.0	57.58	57.58	57.580000000000000000	57.580000000000000000
+55	5137	33	33.0	57.28	57.28	57.280000000000000000	57.280000000000000000
+55	9703	44	44.0	57.21	57.21	57.210000000000000000	57.210000000000000000
+55	12170	92	92.0	69.53	69.53	69.530000000000000000	69.530000000000000000
+55	12205	90	90.0	56.92	56.92	56.920000000000000000	56.920000000000000000
+55	14135	36	36.0	26.4	26.4	26.400000000000000000	26.400000000000000000
+55	14923	71	71.0	30.04	30.04	30.040000000000000000	30.040000000000000000
+55	17677	17	17.0	26.59	26.59	26.590000000000000000	26.590000000000000000
+56	4242	2	2.0	88.74	88.74	88.740000000000000000	88.740000000000000000
+56	4506	57	57.0	69.45	69.45	69.450000000000000000	69.450000000000000000
+56	8353	35	35.0	80.42	80.42	80.420000000000000000	80.420000000000000000
+56	8691	59	59.0	98.91	98.91	98.910000000000000000	98.910000000000000000
+56	8707	68	68.0	79.7	79.7	79.700000000000000000	79.700000000000000000
+56	10362	54	54.0	82.62	82.62	82.620000000000000000	82.620000000000000000
+56	16620	23	23.0	9.94	9.94	9.940000000000000000	9.940000000000000000
+56	17331	74	74.0	32.12	32.12	32.120000000000000000	32.120000000000000000
+57	3253	71	71.0	91.02	91.02	91.020000000000000000	91.020000000000000000
+57	4028	88	88.0	82.23	82.23	82.230000000000000000	82.230000000000000000
+57	4933	22	22.0	93.86	93.86	93.860000000000000000	93.860000000000000000
+57	12596	91	91.0	36.67	36.67	36.670000000000000000	36.670000000000000000
+57	12721	62	62.0	76.4	76.4	76.400000000000000000	76.400000000000000000
+57	12740	52	52.0	55.58	55.58	55.580000000000000000	55.580000000000000000
+57	15182	86	86.0	84.85	84.85	84.850000000000000000	84.850000000000000000
+57	17729	26	26.0	97.2	97.2	97.200000000000000000	97.200000000000000000
+57	17993	99	99.0	NULL	NULL	NULL	NULL
+58	1829	52	52.0	19.97	19.97	19.970000000000000000	19.970000000000000000
+58	3848	6	6.0	45.41	45.41	45.410000000000000000	45.410000000000000000
+58	5117	2	2.0	56.01	56.01	56.010000000000000000	56.010000000000000000
+58	7649	19	19.0	44.04	44.04	44.040000000000000000	44.040000000000000000
+58	9743	62	62.0	73.14	73.14	73.140000000000000000	73.140000000000000000
+58	10802	14	14.0	79.64	79.64	79.640000000000000000	79.640000000000000000
+58	15635	6	6.0	82.45	82.45	82.450000000000000000	82.450000000000000000
+58	16472	6	6.0	7.58	7.58	7.580000000000000000	7.580000000000000000
+58	16949	35	35.0	25.76	25.76	25.760000000000000000	25.760000000000000000
+59	3133	92	92.0	14.57	14.57	14.570000000000000000	14.570000000000000000
+59	3546	22	22.0	64.21	64.21	64.210000000000000000	64.210000000000000000
+59	5772	70	70.0	56.19	56.19	56.190000000000000000	56.190000000000000000
+59	7087	80	80.0	58.71	58.71	58.710000000000000000	58.710000000000000000
+59	8010	46	46.0	20.15	20.15	20.150000000000000000	20.150000000000000000
+59	8335	36	36.0	32.82	32.82	32.820000000000000000	32.820000000000000000
+59	9348	62	62.0	83.62	83.62	83.620000000000000000	83.620000000000000000
+59	9397	92	92.0	70.69	70.69	70.690000000000000000	70.690000000000000000
+59	10651	100	100.0	35.78	35.78	35.780000000000000000	35.780000000000000000
+59	11916	19	19.0	34.31	34.31	34.310000000000000000	34.310000000000000000
+59	12858	90	90.0	61.18	61.18	61.180000000000000000	61.180000000000000000
+59	14529	44	44.0	42.76	42.76	42.760000000000000000	42.760000000000000000
+60	97	50	50.0	37.49	37.49	37.490000000000000000	37.490000000000000000
+60	555	62	62.0	49.17	49.17	49.170000000000000000	49.170000000000000000
+60	633	71	71.0	96.74	96.74	96.740000000000000000	96.740000000000000000
+60	999	43	43.0	22.13	22.13	22.130000000000000000	22.130000000000000000
+60	1117	78	78.0	46.63	46.63	46.630000000000000000	46.630000000000000000
+60	1573	90	90.0	19.02	19.02	19.020000000000000000	19.020000000000000000
+60	4041	25	25.0	36.26	36.26	36.260000000000000000	36.260000000000000000
+60	4235	28	28.0	29.67	29.67	29.670000000000000000	29.670000000000000000
+60	4513	72	72.0	79.56	79.56	79.560000000000000000	79.560000000000000000
+60	4937	22	22.0	27.75	27.75	27.750000000000000000	27.750000000000000000
+60	7231	95	95.0	45.42	45.42	45.420000000000000000	45.420000000000000000
+60	10277	62	62.0	28.05	28.05	28.050000000000000000	28.050000000000000000
+60	10393	75	75.0	98.86	98.86	98.860000000000000000	98.860000000000000000
+60	13975	14	14.0	76.01	76.01	76.010000000000000000	76.010000000000000000
+60	16887	25	25.0	17.92	17.92	17.920000000000000000	17.920000000000000000
+60	17755	88	88.0	52.17	52.17	52.170000000000000000	52.170000000000000000
+61	1106	4	4.0	78.21	78.21	78.210000000000000000	78.210000000000000000
+61	2264	36	36.0	60.94	60.94	60.940000000000000000	60.940000000000000000
+61	3362	48	48.0	67.92	67.92	67.920000000000000000	67.920000000000000000
+61	4567	26	26.0	29.6	29.6	29.600000000000000000	29.600000000000000000
+61	5528	78	78.0	13.85	13.85	13.850000000000000000	13.850000000000000000
+61	6380	77	77.0	69.52	69.52	69.520000000000000000	69.520000000000000000
+61	7591	78	78.0	91.99	91.99	91.990000000000000000	91.990000000000000000
+61	8924	11	11.0	86.51	86.51	86.510000000000000000	86.510000000000000000
+61	10330	8	8.0	46.45	46.45	46.450000000000000000	46.450000000000000000
+61	16462	26	26.0	24.34	24.34	24.340000000000000000	24.340000000000000000
+62	4093	94	94.0	5.53	5.53	5.530000000000000000	5.530000000000000000
+62	6403	NULL	NULL	92.02	92.02	92.020000000000000000	92.020000000000000000
+62	8457	37	37.0	99.97	99.97	99.970000000000000000	99.970000000000000000
+62	10149	75	75.0	48.36	48.36	48.360000000000000000	48.360000000000000000
+62	12163	29	29.0	16.7	16.7	16.700000000000000000	16.700000000000000000
+62	12199	5	5.0	85.54	85.54	85.540000000000000000	85.540000000000000000
+62	12407	NULL	NULL	NULL	NULL	NULL	NULL
+62	13559	80	80.0	52.56	52.56	52.560000000000000000	52.560000000000000000
+62	15399	74	74.0	71.7	71.7	71.700000000000000000	71.700000000000000000
+62	15733	40	40.0	28.03	28.03	28.030000000000000000	28.030000000000000000
+62	16151	93	93.0	84.72	84.72	84.720000000000000000	84.720000000000000000
+63	4488	73	73.0	22.85	22.85	22.850000000000000000	22.850000000000000000
+63	5079	79	79.0	36.05	36.05	36.050000000000000000	36.050000000000000000
+63	5217	66	66.0	15.71	15.71	15.710000000000000000	15.710000000000000000
+63	5658	99	99.0	88.78	88.78	88.780000000000000000	88.780000000000000000
+63	9319	80	80.0	9.27	9.27	9.270000000000000000	9.270000000000000000
+63	11370	38	38.0	56.43	56.43	56.430000000000000000	56.430000000000000000
+63	11946	85	85.0	94.28	94.28	94.280000000000000000	94.280000000000000000
+63	13339	19	19.0	19.44	19.44	19.440000000000000000	19.440000000000000000
+63	15793	40	40.0	75.62	75.62	75.620000000000000000	75.620000000000000000
+63	16569	69	69.0	NULL	NULL	NULL	NULL
+64	1213	NULL	NULL	38.46	38.46	38.460000000000000000	38.460000000000000000
+64	3090	87	87.0	78.06	78.06	78.060000000000000000	78.060000000000000000
+64	3963	NULL	NULL	NULL	NULL	NULL	NULL
+64	11835	82	82.0	30.65	30.65	30.650000000000000000	30.650000000000000000
+64	13224	NULL	NULL	NULL	NULL	NULL	NULL
+64	14407	8	8.0	44.36	44.36	44.360000000000000000	44.360000000000000000
+64	15867	59	59.0	43.77	43.77	43.770000000000000000	43.770000000000000000
+64	15936	30	30.0	56.24	56.24	56.240000000000000000	56.240000000000000000
+64	16921	19	19.0	98.61	98.61	98.610000000000000000	98.610000000000000000
+64	17586	78	78.0	77.26	77.26	77.260000000000000000	77.260000000000000000
+64	17617	17	17.0	91.67	91.67	91.670000000000000000	91.670000000000000000
+65	2287	100	100.0	91.8	91.8	91.800000000000000000	91.800000000000000000
+65	4227	42	42.0	45.38	45.38	45.380000000000000000	45.380000000000000000
+65	9625	51	51.0	40.95	40.95	40.950000000000000000	40.950000000000000000
+65	9847	54	54.0	64.26	64.26	64.260000000000000000	64.260000000000000000
+65	13897	40	40.0	52.84	52.84	52.840000000000000000	52.840000000000000000
+65	14905	85	85.0	81.24	81.24	81.240000000000000000	81.240000000000000000
+65	15177	55	55.0	89.19	89.19	89.190000000000000000	89.190000000000000000
+65	17025	67	67.0	25.52	25.52	25.520000000000000000	25.520000000000000000
+66	6507	76	76.0	43.81	43.81	43.810000000000000000	43.810000000000000000
+66	7033	65	65.0	4.08	4.08	4.080000000000000000	4.080000000000000000
+66	7227	66	66.0	92.15	92.15	92.150000000000000000	92.150000000000000000
+66	8197	41	41.0	84.22	84.22	84.220000000000000000	84.220000000000000000
+66	9237	29	29.0	76.94	76.94	76.940000000000000000	76.940000000000000000
+66	10019	10	10.0	48.77	48.77	48.770000000000000000	48.770000000000000000
+66	11419	66	66.0	10.12	10.12	10.120000000000000000	10.120000000000000000
+66	15629	20	20.0	22.04	22.04	22.040000000000000000	22.040000000000000000
+66	16745	91	91.0	9.53	9.53	9.530000000000000000	9.530000000000000000
+66	16795	28	28.0	42.0	42.0	42.000000000000000000	42.000000000000000000
+67	757	77	77.0	94.12	94.12	94.120000000000000000	94.120000000000000000
+67	2133	74	74.0	71.99	71.99	71.990000000000000000	71.990000000000000000
+67	3439	73	73.0	23.52	23.52	23.520000000000000000	23.520000000000000000
+67	4155	87	87.0	87.74	87.74	87.740000000000000000	87.740000000000000000
+67	5113	NULL	NULL	49.59	49.59	49.590000000000000000	49.590000000000000000
+67	7020	79	79.0	97.01	97.01	97.010000000000000000	97.010000000000000000
+67	7507	77	77.0	26.78	26.78	26.780000000000000000	26.780000000000000000
+67	8469	59	59.0	NULL	NULL	NULL	NULL
+67	8871	71	71.0	78.59	78.59	78.590000000000000000	78.590000000000000000
+67	12087	70	70.0	80.71	80.71	80.710000000000000000	80.710000000000000000
+67	15699	44	44.0	34.59	34.59	34.590000000000000000	34.590000000000000000
+68	1387	74	74.0	90.2	90.2	90.200000000000000000	90.200000000000000000
+68	1603	57	57.0	21.03	21.03	21.030000000000000000	21.030000000000000000
+68	1820	54	54.0	55.82	55.82	55.820000000000000000	55.820000000000000000
+68	2035	22	22.0	54.35	54.35	54.350000000000000000	54.350000000000000000
+68	2296	52	52.0	98.9	98.9	98.900000000000000000	98.900000000000000000
+68	2564	83	83.0	77.32	77.32	77.320000000000000000	77.320000000000000000
+68	5162	23	23.0	83.48	83.48	83.480000000000000000	83.480000000000000000
+68	6763	77	77.0	96.29	96.29	96.290000000000000000	96.290000000000000000
+68	7765	NULL	NULL	69.58	69.58	69.580000000000000000	69.580000000000000000
+68	12526	3	3.0	13.06	13.06	13.060000000000000000	13.060000000000000000
+68	12724	88	88.0	9.63	9.63	9.630000000000000000	9.630000000000000000
+68	17426	2	2.0	48.36	48.36	48.360000000000000000	48.360000000000000000
+68	17600	13	13.0	52.66	52.66	52.660000000000000000	52.660000000000000000
+69	322	45	45.0	NULL	NULL	NULL	NULL
+69	337	34	34.0	20.99	20.99	20.990000000000000000	20.990000000000000000
+69	4208	9	9.0	99.77	99.77	99.770000000000000000	99.770000000000000000
+69	4267	10	10.0	72.37	72.37	72.370000000000000000	72.370000000000000000
+69	6136	7	7.0	49.79	49.79	49.790000000000000000	49.790000000000000000
+69	7264	67	67.0	78.29	78.29	78.290000000000000000	78.290000000000000000
+69	7822	30	30.0	78.1	78.1	78.100000000000000000	78.100000000000000000
+69	8599	53	53.0	56.42	56.42	56.420000000000000000	56.420000000000000000
+69	11137	68	68.0	22.04	22.04	22.040000000000000000	22.040000000000000000
+69	13489	66	66.0	2.68	2.68	2.680000000000000000	2.680000000000000000
+69	13792	NULL	NULL	85.64	85.64	85.640000000000000000	85.640000000000000000
+69	15448	16	16.0	94.38	94.38	94.380000000000000000	94.380000000000000000
+70	1592	53	53.0	99.59	99.59	99.590000000000000000	99.590000000000000000
+70	2462	NULL	NULL	92.7	92.7	92.700000000000000000	92.700000000000000000
+70	3296	48	48.0	10.23	10.23	10.230000000000000000	10.230000000000000000
+70	3947	NULL	NULL	63.8	63.8	63.800000000000000000	63.800000000000000000
+70	6185	82	82.0	84.6	84.6	84.600000000000000000	84.600000000000000000
+70	6425	NULL	NULL	NULL	NULL	NULL	NULL
+70	8893	17	17.0	63.51	63.51	63.510000000000000000	63.510000000000000000
+70	9857	20	20.0	54.96	54.96	54.960000000000000000	54.960000000000000000
+70	14549	4	4.0	35.39	35.39	35.390000000000000000	35.390000000000000000
+70	17815	95	95.0	36.89	36.89	36.890000000000000000	36.890000000000000000
+71	457	75	75.0	27.02	27.02	27.020000000000000000	27.020000000000000000
+71	1888	4	4.0	40.47	40.47	40.470000000000000000	40.470000000000000000
+71	2098	51	51.0	57.87	57.87	57.870000000000000000	57.870000000000000000
+71	4144	49	49.0	26.75	26.75	26.750000000000000000	26.750000000000000000
+71	5858	NULL	NULL	NULL	NULL	NULL	NULL
+71	6008	54	54.0	38.98	38.98	38.980000000000000000	38.980000000000000000
+71	7504	3	3.0	78.44	78.44	78.440000000000000000	78.440000000000000000
+71	8887	10	10.0	61.4	61.4	61.400000000000000000	61.400000000000000000
+71	9274	36	36.0	12.39	12.39	12.390000000000000000	12.390000000000000000
+71	9769	79	79.0	52.15	52.15	52.150000000000000000	52.150000000000000000
+71	9790	96	96.0	37.78	37.78	37.780000000000000000	37.780000000000000000
+71	9997	26	26.0	53.28	53.28	53.280000000000000000	53.280000000000000000
+71	10108	66	66.0	9.49	9.49	9.490000000000000000	9.490000000000000000
+71	10288	30	30.0	29.57	29.57	29.570000000000000000	29.570000000000000000
+71	11168	79	79.0	24.66	24.66	24.660000000000000000	24.660000000000000000
+71	17246	90	90.0	50.57	50.57	50.570000000000000000	50.570000000000000000
+72	1535	9	9.0	69.06	69.06	69.060000000000000000	69.060000000000000000
+72	5917	85	85.0	NULL	NULL	NULL	NULL
+72	6113	45	45.0	59.65	59.65	59.650000000000000000	59.650000000000000000
+72	6671	13	13.0	42.82	42.82	42.820000000000000000	42.820000000000000000
+72	9860	26	26.0	69.92	69.92	69.920000000000000000	69.920000000000000000
+72	10427	66	66.0	55.31	55.31	55.310000000000000000	55.310000000000000000
+72	10753	16	16.0	32.01	32.01	32.010000000000000000	32.010000000000000000
+72	11741	62	62.0	79.25	79.25	79.250000000000000000	79.250000000000000000
+72	12788	29	29.0	34.57	34.57	34.570000000000000000	34.570000000000000000
+72	12901	57	57.0	1.64	1.64	1.640000000000000000	1.640000000000000000
+72	13085	94	94.0	85.13	85.13	85.130000000000000000	85.130000000000000000
+72	13423	62	62.0	34.39	34.39	34.390000000000000000	34.390000000000000000
+72	13904	37	37.0	40.39	40.39	40.390000000000000000	40.390000000000000000
+72	15587	87	87.0	19.04	19.04	19.040000000000000000	19.040000000000000000
+72	16765	56	56.0	89.44	89.44	89.440000000000000000	89.440000000000000000
+73	247	53	53.0	5.22	5.22	5.220000000000000000	5.220000000000000000
+73	1063	37	37.0	34.93	34.93	34.930000000000000000	34.930000000000000000
+73	3205	82	82.0	44.64	44.64	44.640000000000000000	44.640000000000000000
+73	4946	54	54.0	71.08	71.08	71.080000000000000000	71.080000000000000000
+73	6862	58	58.0	86.48	86.48	86.480000000000000000	86.480000000000000000
+73	10051	49	49.0	97.28	97.28	97.280000000000000000	97.280000000000000000
+73	12502	75	75.0	21.63	21.63	21.630000000000000000	21.630000000000000000
+73	15109	38	38.0	53.9	53.9	53.900000000000000000	53.900000000000000000
+73	16519	97	97.0	82.11	82.11	82.110000000000000000	82.110000000000000000
+73	16585	38	38.0	69.27	69.27	69.270000000000000000	69.270000000000000000
+73	17269	40	40.0	NULL	NULL	NULL	NULL
+74	326	29	29.0	76.73	76.73	76.730000000000000000	76.730000000000000000
+74	3104	78	78.0	52.23	52.23	52.230000000000000000	52.230000000000000000
+74	3175	23	23.0	50.69	50.69	50.690000000000000000	50.690000000000000000
+74	3278	NULL	NULL	NULL	NULL	NULL	NULL
+74	3542	96	96.0	93.18	93.18	93.180000000000000000	93.180000000000000000
+74	3754	26	26.0	89.35	89.35	89.350000000000000000	89.350000000000000000
+74	5492	54	54.0	31.24	31.24	31.240000000000000000	31.240000000000000000
+74	7694	17	17.0	36.61	36.61	36.610000000000000000	36.610000000000000000
+74	8653	12	12.0	4.33	4.33	4.330000000000000000	4.330000000000000000
+74	9620	95	95.0	56.35	56.35	56.350000000000000000	56.350000000000000000
+74	10069	99	99.0	99.98	99.98	99.980000000000000000	99.980000000000000000
+74	13208	87	87.0	82.61	82.61	82.610000000000000000	82.610000000000000000
+74	16694	72	72.0	36.04	36.04	36.040000000000000000	36.040000000000000000
+75	607	20	20.0	88.61	88.61	88.610000000000000000	88.610000000000000000
+75	2948	25	25.0	7.48	7.48	7.480000000000000000	7.480000000000000000
+75	4625	73	73.0	76.04	76.04	76.040000000000000000	76.040000000000000000
+75	6938	89	89.0	20.73	20.73	20.730000000000000000	20.730000000000000000
+75	6953	71	71.0	33.34	33.34	33.340000000000000000	33.340000000000000000
+75	8726	6	6.0	25.87	25.87	25.870000000000000000	25.870000000000000000
+75	9905	54	54.0	63.01	63.01	63.010000000000000000	63.010000000000000000
+75	10217	85	85.0	83.25	83.25	83.250000000000000000	83.250000000000000000
+75	11039	70	70.0	87.84	87.84	87.840000000000000000	87.840000000000000000
+75	14186	63	63.0	82.77	82.77	82.770000000000000000	82.770000000000000000
+75	16796	93	93.0	5.19	5.19	5.190000000000000000	5.190000000000000000
+76	257	5	5.0	8.47	8.47	8.470000000000000000	8.470000000000000000
+76	465	2	2.0	95.45	95.45	95.450000000000000000	95.450000000000000000
+76	1107	16	16.0	NULL	NULL	NULL	NULL
+76	1503	97	97.0	30.22	30.22	30.220000000000000000	30.220000000000000000
+76	2265	98	98.0	89.7	89.7	89.700000000000000000	89.700000000000000000
+76	2869	32	32.0	NULL	NULL	NULL	NULL
+76	3363	25	25.0	89.9	89.9	89.900000000000000000	89.900000000000000000
+76	4237	48	48.0	60.58	60.58	60.580000000000000000	60.580000000000000000
+76	4567	40	40.0	2.19	2.19	2.190000000000000000	2.190000000000000000
+76	5529	78	78.0	49.64	49.64	49.640000000000000000	49.640000000000000000
+76	6381	50	50.0	34.93	34.93	34.930000000000000000	34.930000000000000000
+76	7591	27	27.0	61.86	61.86	61.860000000000000000	61.860000000000000000
+76	8925	6	6.0	80.04	80.04	80.040000000000000000	80.040000000000000000
+76	10331	3	3.0	29.09	29.09	29.090000000000000000	29.090000000000000000
+76	16463	53	53.0	86.06	86.06	86.060000000000000000	86.060000000000000000
+77	992	62	62.0	21.65	21.65	21.650000000000000000	21.650000000000000000
+77	1399	34	34.0	96.21	96.21	96.210000000000000000	96.210000000000000000
+77	2713	85	85.0	85.72	85.72	85.720000000000000000	85.720000000000000000
+77	3868	89	89.0	3.72	3.72	3.720000000000000000	3.720000000000000000
+77	6289	30	30.0	26.16	26.16	26.160000000000000000	26.160000000000000000
+77	7339	88	88.0	31.13	31.13	31.130000000000000000	31.130000000000000000
+77	7448	95	95.0	29.07	29.07	29.070000000000000000	29.070000000000000000
+77	7486	49	49.0	NULL	NULL	NULL	NULL
+77	8686	38	38.0	45.3	45.3	45.300000000000000000	45.300000000000000000
+77	9220	90	90.0	87.41	87.41	87.410000000000000000	87.410000000000000000
+77	11918	36	36.0	25.95	25.95	25.950000000000000000	25.950000000000000000
+77	12439	95	95.0	74.32	74.32	74.320000000000000000	74.320000000000000000
+77	13456	48	48.0	85.61	85.61	85.610000000000000000	85.610000000000000000
+77	14815	18	18.0	69.28	69.28	69.280000000000000000	69.280000000000000000
+77	16687	16	16.0	67.63	67.63	67.630000000000000000	67.630000000000000000
+78	901	3	3.0	54.5	54.5	54.500000000000000000	54.500000000000000000
+78	3304	50	50.0	61.49	61.49	61.490000000000000000	61.490000000000000000
+78	3856	27	27.0	60.1	60.1	60.100000000000000000	60.100000000000000000
+78	5965	78	78.0	7.47	7.47	7.470000000000000000	7.470000000000000000
+78	6044	59	59.0	15.94	15.94	15.940000000000000000	15.940000000000000000
+78	6110	43	43.0	28.45	28.45	28.450000000000000000	28.450000000000000000
+78	6500	76	76.0	38.42	38.42	38.420000000000000000	38.420000000000000000
+78	7576	87	87.0	60.62	60.62	60.620000000000000000	60.620000000000000000
+78	8611	79	79.0	95.42	95.42	95.420000000000000000	95.420000000000000000
+78	10507	6	6.0	50.83	50.83	50.830000000000000000	50.830000000000000000
+78	11209	7	7.0	88.17	88.17	88.170000000000000000	88.170000000000000000
+78	12706	19	19.0	81.1	81.1	81.100000000000000000	81.100000000000000000
+78	14996	39	39.0	36.47	36.47	36.470000000000000000	36.470000000000000000
+79	247	NULL	NULL	NULL	NULL	NULL	NULL
+79	1063	85	85.0	77.51	77.51	77.510000000000000000	77.510000000000000000
+79	3205	48	48.0	21.34	21.34	21.340000000000000000	21.340000000000000000
+79	4947	35	35.0	99.77	99.77	99.770000000000000000	99.770000000000000000
+79	6864	1	1.0	86.12	86.12	86.120000000000000000	86.120000000000000000
+79	10051	10	10.0	94.53	94.53	94.530000000000000000	94.530000000000000000
+79	10524	36	36.0	21.73	21.73	21.730000000000000000	21.730000000000000000
+79	12504	81	81.0	14.87	14.87	14.870000000000000000	14.870000000000000000
+79	14322	41	41.0	58.88	58.88	58.880000000000000000	58.880000000000000000
+79	15109	NULL	NULL	45.07	45.07	45.070000000000000000	45.070000000000000000
+79	15498	3	3.0	94.64	94.64	94.640000000000000000	94.640000000000000000
+79	15888	58	58.0	99.75	99.75	99.750000000000000000	99.750000000000000000
+79	16519	9	9.0	69.84	69.84	69.840000000000000000	69.840000000000000000
+79	16585	93	93.0	70.7	70.7	70.700000000000000000	70.700000000000000000
+79	17269	81	81.0	5.88	5.88	5.880000000000000000	5.880000000000000000
+80	998	93	93.0	69.32	69.32	69.320000000000000000	69.320000000000000000
+80	1519	25	25.0	66.36	66.36	66.360000000000000000	66.360000000000000000
+80	1573	40	40.0	43.33	43.33	43.330000000000000000	43.330000000000000000
+80	4040	66	66.0	15.01	15.01	15.010000000000000000	15.010000000000000000
+80	4513	NULL	NULL	76.02	76.02	76.020000000000000000	76.020000000000000000
+80	4622	1	1.0	60.1	60.1	60.100000000000000000	60.100000000000000000
+80	7231	49	49.0	76.07	76.07	76.070000000000000000	76.070000000000000000
+80	7610	37	37.0	24.62	24.62	24.620000000000000000	24.620000000000000000
+80	10393	5	5.0	71.37	71.37	71.370000000000000000	71.370000000000000000
+80	12968	NULL	NULL	NULL	NULL	NULL	NULL
+80	13717	91	91.0	60.42	60.42	60.420000000000000000	60.420000000000000000
+80	13975	13	13.0	83.81	83.81	83.810000000000000000	83.810000000000000000
+80	16363	84	84.0	84.8	84.8	84.800000000000000000	84.800000000000000000
+80	16886	77	77.0	89.22	89.22	89.220000000000000000	89.220000000000000000
+80	17308	29	29.0	94.38	94.38	94.380000000000000000	94.380000000000000000
+80	17755	94	94.0	56.04	56.04	56.040000000000000000	56.040000000000000000
+81	4486	31	31.0	63.84	63.84	63.840000000000000000	63.840000000000000000
+81	5078	75	75.0	33.72	33.72	33.720000000000000000	33.720000000000000000
+81	5216	64	64.0	4.59	4.59	4.590000000000000000	4.590000000000000000
+81	5656	24	24.0	40.61	40.61	40.610000000000000000	40.610000000000000000
+81	7166	7	7.0	22.87	22.87	22.870000000000000000	22.870000000000000000
+81	7663	79	79.0	52.07	52.07	52.070000000000000000	52.070000000000000000
+81	8918	37	37.0	86.54	86.54	86.540000000000000000	86.540000000000000000
+81	9319	36	36.0	91.74	91.74	91.740000000000000000	91.740000000000000000
+81	11107	36	36.0	47.86	47.86	47.860000000000000000	47.860000000000000000
+81	11368	26	26.0	NULL	NULL	NULL	NULL
+81	13339	6	6.0	4.63	4.63	4.630000000000000000	4.630000000000000000
+81	15793	8	8.0	5.61	5.61	5.610000000000000000	5.610000000000000000
+82	2572	53	53.0	55.41	55.41	55.410000000000000000	55.410000000000000000
+82	7862	75	75.0	21.65	21.65	21.650000000000000000	21.650000000000000000
+82	13138	59	59.0	31.81	31.81	31.810000000000000000	31.810000000000000000
+82	14998	49	49.0	52.59	52.59	52.590000000000000000	52.590000000000000000
+82	17041	18	18.0	4.71	4.71	4.710000000000000000	4.710000000000000000
diff --git a/ql/src/test/results/clientpositive/vector_grouping_sets.q.out b/ql/src/test/results/clientpositive/vector_grouping_sets.q.out
index 3d35fbfcdb..8a8d1effa3 100644
--- a/ql/src/test/results/clientpositive/vector_grouping_sets.q.out
+++ b/ql/src/test/results/clientpositive/vector_grouping_sets.q.out
@@ -162,9 +162,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 1, ConstantVectorExpression(val 0) -> 29:long
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: s_store_id (type: string), 0 (type: int)
                 mode: hash
@@ -196,8 +198,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string), KEY._col1 (type: int)
           mode: mergepartial
@@ -276,9 +280,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 1, ConstantVectorExpression(val 0) -> 29:long
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: _col0 (type: string), 0 (type: int)
                 mode: hash
@@ -310,8 +316,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string), KEY._col1 (type: int)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_include_no_sel.q.out b/ql/src/test/results/clientpositive/vector_include_no_sel.q.out
index 8c8ef803f6..7f97f54413 100644
--- a/ql/src/test/results/clientpositive/vector_include_no_sel.q.out
+++ b/ql/src/test/results/clientpositive/vector_include_no_sel.q.out
@@ -239,8 +239,10 @@ STAGE PLANS:
                     Group By Vectorization:
                         aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 3:long) -> bigint
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: [0]
                     mode: hash
                     outputColumnNames: _col0
@@ -273,8 +275,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/vector_mapjoin_reduce.q.out b/ql/src/test/results/clientpositive/vector_mapjoin_reduce.q.out
index 82bef24230..0ff11df8a3 100644
--- a/ql/src/test/results/clientpositive/vector_mapjoin_reduce.q.out
+++ b/ql/src/test/results/clientpositive/vector_mapjoin_reduce.q.out
@@ -40,9 +40,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 1
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: l_partkey (type: int)
                 mode: hash
@@ -74,8 +76,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: mergepartial
@@ -126,8 +130,10 @@ STAGE PLANS:
                 Statistics: Num rows: 50 Data size: 5999 Basic stats: COMPLETE Column stats: NONE
                 Group By Operator
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   keys: _col0 (type: int)
                   mode: hash
@@ -273,9 +279,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 1
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: l_partkey (type: int)
                 mode: hash
@@ -307,8 +315,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int)
           mode: mergepartial
@@ -359,8 +369,10 @@ STAGE PLANS:
                 Statistics: Num rows: 25 Data size: 2999 Basic stats: COMPLETE Column stats: NONE
                 Group By Operator
                   Group By Vectorization:
+                      groupByMode: HASH
                       vectorOutput: false
                       native: false
+                      vectorProcessingMode: NONE
                       projectedOutputColumns: null
                   keys: _col0 (type: int), _col1 (type: int)
                   mode: hash
diff --git a/ql/src/test/results/clientpositive/vector_null_projection.q.out b/ql/src/test/results/clientpositive/vector_null_projection.q.out
index 94aea2fa62..bf3984f4b2 100644
--- a/ql/src/test/results/clientpositive/vector_null_projection.q.out
+++ b/ql/src/test/results/clientpositive/vector_null_projection.q.out
@@ -64,7 +64,7 @@ STAGE PLANS:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-          notVectorizedReason: Select expression for SELECT operator: Data type void of Const void null not supported
+          notVectorizedReason: Select expression for SELECT operator: Vectorizing data type void not supported when mode = PROJECTION
           vectorized: false
 
   Stage: Stage-0
@@ -111,8 +111,10 @@ STAGE PLANS:
                   Statistics: Num rows: 2 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                   Group By Operator
                     Group By Vectorization:
+                        groupByMode: HASH
                         vectorOutput: false
                         native: false
+                        vectorProcessingMode: NONE
                         projectedOutputColumns: null
                     keys: null (type: void)
                     mode: hash
@@ -134,8 +136,10 @@ STAGE PLANS:
                   Statistics: Num rows: 2 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                   Group By Operator
                     Group By Vectorization:
+                        groupByMode: HASH
                         vectorOutput: false
                         native: false
+                        vectorProcessingMode: NONE
                         projectedOutputColumns: null
                     keys: null (type: void)
                     mode: hash
@@ -156,8 +160,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: void)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_nvl.q.out b/ql/src/test/results/clientpositive/vector_nvl.q.out
index 08cc168f81..f8de13334c 100644
--- a/ql/src/test/results/clientpositive/vector_nvl.q.out
+++ b/ql/src/test/results/clientpositive/vector_nvl.q.out
@@ -317,7 +317,7 @@ STAGE PLANS:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-          notVectorizedReason: Select expression for SELECT operator: Data type void of Const void null not supported
+          notVectorizedReason: Select expression for SELECT operator: Vectorizing data type void not supported when mode = PROJECTION
           vectorized: false
 
   Stage: Stage-0
diff --git a/ql/src/test/results/clientpositive/vector_orderby_5.q.out b/ql/src/test/results/clientpositive/vector_orderby_5.q.out
index b85eb755c0..9a7295088e 100644
--- a/ql/src/test/results/clientpositive/vector_orderby_5.q.out
+++ b/ql/src/test/results/clientpositive/vector_orderby_5.q.out
@@ -139,9 +139,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMaxLong(col 3) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 7
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0]
                 keys: bo (type: boolean)
                 mode: hash
@@ -175,8 +177,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: max(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: boolean)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_outer_join1.q.out b/ql/src/test/results/clientpositive/vector_outer_join1.q.out
index 7a92befa2d..70bce01c76 100644
--- a/ql/src/test/results/clientpositive/vector_outer_join1.q.out
+++ b/ql/src/test/results/clientpositive/vector_outer_join1.q.out
@@ -688,8 +688,10 @@ STAGE PLANS:
                     Group By Vectorization:
                         aggregators: VectorUDAFCountStar(*) -> bigint, VectorUDAFSumLong(col 0) -> bigint
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: [0, 1]
                     mode: hash
                     outputColumnNames: _col0, _col1
@@ -727,8 +729,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0), sum(VALUE._col1)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/vector_outer_join2.q.out b/ql/src/test/results/clientpositive/vector_outer_join2.q.out
index 6b5148951b..2265cb83bc 100644
--- a/ql/src/test/results/clientpositive/vector_outer_join2.q.out
+++ b/ql/src/test/results/clientpositive/vector_outer_join2.q.out
@@ -335,8 +335,10 @@ STAGE PLANS:
                     Group By Vectorization:
                         aggregators: VectorUDAFCountStar(*) -> bigint, VectorUDAFSumLong(col 0) -> bigint
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: [0, 1]
                     mode: hash
                     outputColumnNames: _col0, _col1
@@ -374,8 +376,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0), sum(VALUE._col1)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/vector_outer_join3.q.out b/ql/src/test/results/clientpositive/vector_outer_join3.q.out
index d299be448e..e4e482500f 100644
--- a/ql/src/test/results/clientpositive/vector_outer_join3.q.out
+++ b/ql/src/test/results/clientpositive/vector_outer_join3.q.out
@@ -242,7 +242,7 @@ left outer join small_alltypesorc_a hd
   on hd.cstring1 = c.cstring1
 ) t1
 POSTHOOK: type: QUERY
-{"PLAN VECTORIZATION":{"enabled":true,"enabledConditionsMet":["hive.vectorized.execution.enabled IS true"]},"STAGE DEPENDENCIES":{"Stage-8":{"ROOT STAGE":"TRUE"},"Stage-3":{"DEPENDENT STAGES":"Stage-8"},"Stage-0":{"DEPENDENT STAGES":"Stage-3"}},"STAGE PLANS":{"Stage-8":{"Map Reduce Local Work":{"Alias -> Map Local Tables:":{"$hdt$_1:cd":{"Fetch Operator":{"limit:":"-1"}},"$hdt$_2:hd":{"Fetch Operator":{"limit:":"-1"}}},"Alias -> Map Local Operator Tree:":{"$hdt$_1:cd":{"TableScan":{"alias:":"cd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_2","children":{"Select Operator":{"expressions:":"cint (type: int)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_3","children":{"HashTable Sink Operator":{"keys:":{"0":"_col0 (type: int)","1":"_col0 (type: int)"},"OperatorId:":"HASHTABLESINK_26"}}}}}},"$hdt$_2:hd":{"TableScan":{"alias:":"hd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_4","children":{"Select Operator":{"expressions:":"cstring1 (type: string)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_5","children":{"HashTable Sink Operator":{"keys:":{"0":"_col1 (type: string)","1":"_col0 (type: string)"},"OperatorId:":"HASHTABLESINK_24"}}}}}}}}},"Stage-3":{"Map Reduce":{"Map Operator Tree:":[{"TableScan":{"alias:":"c","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","TableScan Vectorization:":{"native:":"true","projectedOutputColumns:":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"},"OperatorId:":"TS_0","children":{"Select Operator":{"expressions:":"cint (type: int), cstring1 (type: string)","outputColumnNames:":["_col0","_col1"],"Select Vectorization:":{"className:":"VectorSelectOperator","native:":"true","projectedOutputColumns:":"[2, 6]"},"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_28","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col0 (type: int)","1":"_col0 (type: int)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"outputColumnNames:":["_col1"],"Statistics:":"Num rows: 22 Data size: 4840 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_29","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col1 (type: string)","1":"_col0 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 24 Data size: 5324 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_30","children":{"Group By Operator":{"aggregations:":["count()"],"Group By Vectorization:":{"aggregators:":["VectorUDAFCountStar(*) -> bigint"],"className:":"VectorGroupByOperator","vectorOutput:":"true","native:":"false","projectedOutputColumns:":"[0]"},"mode:":"hash","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_31","children":{"Reduce Output Operator":{"sort order:":"","Reduce Sink Vectorization:":{"className:":"VectorReduceSinkOperator","native:":"false","nativeConditionsMet:":["hive.vectorized.execution.reducesink.new.enabled IS true","No PTF TopN IS true","No DISTINCT columns IS true","BinarySortableSerDe for keys IS true","LazyBinarySerDe for values IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","value expressions:":"_col0 (type: bigint)","OperatorId:":"RS_32"}}}}}}}}}}}}],"Execution mode:":"vectorized","Map Vectorization:":{"enabled:":"true","enabledConditionsMet:":["hive.vectorized.use.vectorized.input.format IS true"],"groupByVectorOutput:":"true","inputFileFormats:":["org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"],"allNative:":"false","usesVectorUDFAdaptor:":"false","vectorized:":"true","rowBatchContext:":{"dataColumnCount:":"12","includeColumns:":"[2, 6]","dataColumns:":["ctinyint:tinyint","csmallint:smallint","cint:int","cbigint:bigint","cfloat:float","cdouble:double","cstring1:string","cstring2:string","ctimestamp1:timestamp","ctimestamp2:timestamp","cboolean1:boolean","cboolean2:boolean"],"partitionColumnCount:":"0"}},"Local Work:":{"Map Reduce Local Work":{}},"Reduce Vectorization:":{"enabled:":"false","enableConditionsMet:":["hive.vectorized.execution.reduce.enabled IS true"],"enableConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Reduce Operator Tree:":{"Group By Operator":{"aggregations:":["count(VALUE._col0)"],"Group By Vectorization:":{"vectorOutput:":"false","native:":"false","projectedOutputColumns:":"null"},"mode:":"mergepartial","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_15","children":{"File Output Operator":{"compressed:":"false","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","table:":{"input format:":"org.apache.hadoop.mapred.SequenceFileInputFormat","output format:":"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat","serde:":"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"},"OperatorId:":"FS_17"}}}}}},"Stage-0":{"Fetch Operator":{"limit:":"-1","Processor Tree:":{"ListSink":{"OperatorId:":"LIST_SINK_33"}}}}}}
+{"PLAN VECTORIZATION":{"enabled":true,"enabledConditionsMet":["hive.vectorized.execution.enabled IS true"]},"STAGE DEPENDENCIES":{"Stage-8":{"ROOT STAGE":"TRUE"},"Stage-3":{"DEPENDENT STAGES":"Stage-8"},"Stage-0":{"DEPENDENT STAGES":"Stage-3"}},"STAGE PLANS":{"Stage-8":{"Map Reduce Local Work":{"Alias -> Map Local Tables:":{"$hdt$_1:cd":{"Fetch Operator":{"limit:":"-1"}},"$hdt$_2:hd":{"Fetch Operator":{"limit:":"-1"}}},"Alias -> Map Local Operator Tree:":{"$hdt$_1:cd":{"TableScan":{"alias:":"cd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_2","children":{"Select Operator":{"expressions:":"cint (type: int)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_3","children":{"HashTable Sink Operator":{"keys:":{"0":"_col0 (type: int)","1":"_col0 (type: int)"},"OperatorId:":"HASHTABLESINK_26"}}}}}},"$hdt$_2:hd":{"TableScan":{"alias:":"hd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_4","children":{"Select Operator":{"expressions:":"cstring1 (type: string)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_5","children":{"HashTable Sink Operator":{"keys:":{"0":"_col1 (type: string)","1":"_col0 (type: string)"},"OperatorId:":"HASHTABLESINK_24"}}}}}}}}},"Stage-3":{"Map Reduce":{"Map Operator Tree:":[{"TableScan":{"alias:":"c","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","TableScan Vectorization:":{"native:":"true","projectedOutputColumns:":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"},"OperatorId:":"TS_0","children":{"Select Operator":{"expressions:":"cint (type: int), cstring1 (type: string)","outputColumnNames:":["_col0","_col1"],"Select Vectorization:":{"className:":"VectorSelectOperator","native:":"true","projectedOutputColumns:":"[2, 6]"},"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_28","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col0 (type: int)","1":"_col0 (type: int)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"outputColumnNames:":["_col1"],"Statistics:":"Num rows: 22 Data size: 4840 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_29","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col1 (type: string)","1":"_col0 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 24 Data size: 5324 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_30","children":{"Group By Operator":{"aggregations:":["count()"],"Group By Vectorization:":{"aggregators:":["VectorUDAFCountStar(*) -> bigint"],"className:":"VectorGroupByOperator","groupByMode:":"HASH","vectorOutput:":"true","native:":"false","vectorProcessingMode:":"HASH","projectedOutputColumns:":"[0]"},"mode:":"hash","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_31","children":{"Reduce Output Operator":{"sort order:":"","Reduce Sink Vectorization:":{"className:":"VectorReduceSinkOperator","native:":"false","nativeConditionsMet:":["hive.vectorized.execution.reducesink.new.enabled IS true","No PTF TopN IS true","No DISTINCT columns IS true","BinarySortableSerDe for keys IS true","LazyBinarySerDe for values IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","value expressions:":"_col0 (type: bigint)","OperatorId:":"RS_32"}}}}}}}}}}}}],"Execution mode:":"vectorized","Map Vectorization:":{"enabled:":"true","enabledConditionsMet:":["hive.vectorized.use.vectorized.input.format IS true"],"groupByVectorOutput:":"true","inputFileFormats:":["org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"],"allNative:":"false","usesVectorUDFAdaptor:":"false","vectorized:":"true","rowBatchContext:":{"dataColumnCount:":"12","includeColumns:":"[2, 6]","dataColumns:":["ctinyint:tinyint","csmallint:smallint","cint:int","cbigint:bigint","cfloat:float","cdouble:double","cstring1:string","cstring2:string","ctimestamp1:timestamp","ctimestamp2:timestamp","cboolean1:boolean","cboolean2:boolean"],"partitionColumnCount:":"0"}},"Local Work:":{"Map Reduce Local Work":{}},"Reduce Vectorization:":{"enabled:":"false","enableConditionsMet:":["hive.vectorized.execution.reduce.enabled IS true"],"enableConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Reduce Operator Tree:":{"Group By Operator":{"aggregations:":["count(VALUE._col0)"],"Group By Vectorization:":{"groupByMode:":"MERGEPARTIAL","vectorOutput:":"false","native:":"false","vectorProcessingMode:":"NONE","projectedOutputColumns:":"null"},"mode:":"mergepartial","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_15","children":{"File Output Operator":{"compressed:":"false","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","table:":{"input format:":"org.apache.hadoop.mapred.SequenceFileInputFormat","output format:":"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat","serde:":"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"},"OperatorId:":"FS_17"}}}}}},"Stage-0":{"Fetch Operator":{"limit:":"-1","Processor Tree:":{"ListSink":{"OperatorId:":"LIST_SINK_33"}}}}}}
 PREHOOK: query: select count(*) from (select c.cstring1
 from small_alltypesorc_a c
 left outer join small_alltypesorc_a cd
@@ -282,7 +282,7 @@ left outer join small_alltypesorc_a hd
   on hd.cstring1 = c.cstring1
 ) t1
 POSTHOOK: type: QUERY
-{"PLAN VECTORIZATION":{"enabled":true,"enabledConditionsMet":["hive.vectorized.execution.enabled IS true"]},"STAGE DEPENDENCIES":{"Stage-8":{"ROOT STAGE":"TRUE"},"Stage-3":{"DEPENDENT STAGES":"Stage-8"},"Stage-0":{"DEPENDENT STAGES":"Stage-3"}},"STAGE PLANS":{"Stage-8":{"Map Reduce Local Work":{"Alias -> Map Local Tables:":{"$hdt$_1:cd":{"Fetch Operator":{"limit:":"-1"}},"$hdt$_2:hd":{"Fetch Operator":{"limit:":"-1"}}},"Alias -> Map Local Operator Tree:":{"$hdt$_1:cd":{"TableScan":{"alias:":"cd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_2","children":{"Select Operator":{"expressions:":"cstring2 (type: string)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_3","children":{"HashTable Sink Operator":{"keys:":{"0":"_col1 (type: string)","1":"_col0 (type: string)"},"OperatorId:":"HASHTABLESINK_26"}}}}}},"$hdt$_2:hd":{"TableScan":{"alias:":"hd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_4","children":{"Select Operator":{"expressions:":"cstring1 (type: string)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_5","children":{"HashTable Sink Operator":{"keys:":{"0":"_col0 (type: string)","1":"_col0 (type: string)"},"OperatorId:":"HASHTABLESINK_24"}}}}}}}}},"Stage-3":{"Map Reduce":{"Map Operator Tree:":[{"TableScan":{"alias:":"c","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","TableScan Vectorization:":{"native:":"true","projectedOutputColumns:":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"},"OperatorId:":"TS_0","children":{"Select Operator":{"expressions:":"cstring1 (type: string), cstring2 (type: string)","outputColumnNames:":["_col0","_col1"],"Select Vectorization:":{"className:":"VectorSelectOperator","native:":"true","projectedOutputColumns:":"[6, 7]"},"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_28","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col1 (type: string)","1":"_col0 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"outputColumnNames:":["_col0"],"Statistics:":"Num rows: 22 Data size: 4840 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_29","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col0 (type: string)","1":"_col0 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 24 Data size: 5324 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_30","children":{"Group By Operator":{"aggregations:":["count()"],"Group By Vectorization:":{"aggregators:":["VectorUDAFCountStar(*) -> bigint"],"className:":"VectorGroupByOperator","vectorOutput:":"true","native:":"false","projectedOutputColumns:":"[0]"},"mode:":"hash","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_31","children":{"Reduce Output Operator":{"sort order:":"","Reduce Sink Vectorization:":{"className:":"VectorReduceSinkOperator","native:":"false","nativeConditionsMet:":["hive.vectorized.execution.reducesink.new.enabled IS true","No PTF TopN IS true","No DISTINCT columns IS true","BinarySortableSerDe for keys IS true","LazyBinarySerDe for values IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","value expressions:":"_col0 (type: bigint)","OperatorId:":"RS_32"}}}}}}}}}}}}],"Execution mode:":"vectorized","Map Vectorization:":{"enabled:":"true","enabledConditionsMet:":["hive.vectorized.use.vectorized.input.format IS true"],"groupByVectorOutput:":"true","inputFileFormats:":["org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"],"allNative:":"false","usesVectorUDFAdaptor:":"false","vectorized:":"true","rowBatchContext:":{"dataColumnCount:":"12","includeColumns:":"[6, 7]","dataColumns:":["ctinyint:tinyint","csmallint:smallint","cint:int","cbigint:bigint","cfloat:float","cdouble:double","cstring1:string","cstring2:string","ctimestamp1:timestamp","ctimestamp2:timestamp","cboolean1:boolean","cboolean2:boolean"],"partitionColumnCount:":"0"}},"Local Work:":{"Map Reduce Local Work":{}},"Reduce Vectorization:":{"enabled:":"false","enableConditionsMet:":["hive.vectorized.execution.reduce.enabled IS true"],"enableConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Reduce Operator Tree:":{"Group By Operator":{"aggregations:":["count(VALUE._col0)"],"Group By Vectorization:":{"vectorOutput:":"false","native:":"false","projectedOutputColumns:":"null"},"mode:":"mergepartial","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_15","children":{"File Output Operator":{"compressed:":"false","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","table:":{"input format:":"org.apache.hadoop.mapred.SequenceFileInputFormat","output format:":"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat","serde:":"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"},"OperatorId:":"FS_17"}}}}}},"Stage-0":{"Fetch Operator":{"limit:":"-1","Processor Tree:":{"ListSink":{"OperatorId:":"LIST_SINK_33"}}}}}}
+{"PLAN VECTORIZATION":{"enabled":true,"enabledConditionsMet":["hive.vectorized.execution.enabled IS true"]},"STAGE DEPENDENCIES":{"Stage-8":{"ROOT STAGE":"TRUE"},"Stage-3":{"DEPENDENT STAGES":"Stage-8"},"Stage-0":{"DEPENDENT STAGES":"Stage-3"}},"STAGE PLANS":{"Stage-8":{"Map Reduce Local Work":{"Alias -> Map Local Tables:":{"$hdt$_1:cd":{"Fetch Operator":{"limit:":"-1"}},"$hdt$_2:hd":{"Fetch Operator":{"limit:":"-1"}}},"Alias -> Map Local Operator Tree:":{"$hdt$_1:cd":{"TableScan":{"alias:":"cd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_2","children":{"Select Operator":{"expressions:":"cstring2 (type: string)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_3","children":{"HashTable Sink Operator":{"keys:":{"0":"_col1 (type: string)","1":"_col0 (type: string)"},"OperatorId:":"HASHTABLESINK_26"}}}}}},"$hdt$_2:hd":{"TableScan":{"alias:":"hd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_4","children":{"Select Operator":{"expressions:":"cstring1 (type: string)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_5","children":{"HashTable Sink Operator":{"keys:":{"0":"_col0 (type: string)","1":"_col0 (type: string)"},"OperatorId:":"HASHTABLESINK_24"}}}}}}}}},"Stage-3":{"Map Reduce":{"Map Operator Tree:":[{"TableScan":{"alias:":"c","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","TableScan Vectorization:":{"native:":"true","projectedOutputColumns:":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"},"OperatorId:":"TS_0","children":{"Select Operator":{"expressions:":"cstring1 (type: string), cstring2 (type: string)","outputColumnNames:":["_col0","_col1"],"Select Vectorization:":{"className:":"VectorSelectOperator","native:":"true","projectedOutputColumns:":"[6, 7]"},"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_28","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col1 (type: string)","1":"_col0 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"outputColumnNames:":["_col0"],"Statistics:":"Num rows: 22 Data size: 4840 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_29","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col0 (type: string)","1":"_col0 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 24 Data size: 5324 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_30","children":{"Group By Operator":{"aggregations:":["count()"],"Group By Vectorization:":{"aggregators:":["VectorUDAFCountStar(*) -> bigint"],"className:":"VectorGroupByOperator","groupByMode:":"HASH","vectorOutput:":"true","native:":"false","vectorProcessingMode:":"HASH","projectedOutputColumns:":"[0]"},"mode:":"hash","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_31","children":{"Reduce Output Operator":{"sort order:":"","Reduce Sink Vectorization:":{"className:":"VectorReduceSinkOperator","native:":"false","nativeConditionsMet:":["hive.vectorized.execution.reducesink.new.enabled IS true","No PTF TopN IS true","No DISTINCT columns IS true","BinarySortableSerDe for keys IS true","LazyBinarySerDe for values IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","value expressions:":"_col0 (type: bigint)","OperatorId:":"RS_32"}}}}}}}}}}}}],"Execution mode:":"vectorized","Map Vectorization:":{"enabled:":"true","enabledConditionsMet:":["hive.vectorized.use.vectorized.input.format IS true"],"groupByVectorOutput:":"true","inputFileFormats:":["org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"],"allNative:":"false","usesVectorUDFAdaptor:":"false","vectorized:":"true","rowBatchContext:":{"dataColumnCount:":"12","includeColumns:":"[6, 7]","dataColumns:":["ctinyint:tinyint","csmallint:smallint","cint:int","cbigint:bigint","cfloat:float","cdouble:double","cstring1:string","cstring2:string","ctimestamp1:timestamp","ctimestamp2:timestamp","cboolean1:boolean","cboolean2:boolean"],"partitionColumnCount:":"0"}},"Local Work:":{"Map Reduce Local Work":{}},"Reduce Vectorization:":{"enabled:":"false","enableConditionsMet:":["hive.vectorized.execution.reduce.enabled IS true"],"enableConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Reduce Operator Tree:":{"Group By Operator":{"aggregations:":["count(VALUE._col0)"],"Group By Vectorization:":{"groupByMode:":"MERGEPARTIAL","vectorOutput:":"false","native:":"false","vectorProcessingMode:":"NONE","projectedOutputColumns:":"null"},"mode:":"mergepartial","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_15","children":{"File Output Operator":{"compressed:":"false","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","table:":{"input format:":"org.apache.hadoop.mapred.SequenceFileInputFormat","output format:":"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat","serde:":"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"},"OperatorId:":"FS_17"}}}}}},"Stage-0":{"Fetch Operator":{"limit:":"-1","Processor Tree:":{"ListSink":{"OperatorId:":"LIST_SINK_33"}}}}}}
 PREHOOK: query: select count(*) from (select c.cstring1
 from small_alltypesorc_a c
 left outer join small_alltypesorc_a cd
@@ -322,7 +322,7 @@ left outer join small_alltypesorc_a hd
   on hd.cstring1 = c.cstring1 and hd.cint = c.cint
 ) t1
 POSTHOOK: type: QUERY
-{"PLAN VECTORIZATION":{"enabled":true,"enabledConditionsMet":["hive.vectorized.execution.enabled IS true"]},"STAGE DEPENDENCIES":{"Stage-8":{"ROOT STAGE":"TRUE"},"Stage-3":{"DEPENDENT STAGES":"Stage-8"},"Stage-0":{"DEPENDENT STAGES":"Stage-3"}},"STAGE PLANS":{"Stage-8":{"Map Reduce Local Work":{"Alias -> Map Local Tables:":{"$hdt$_1:cd":{"Fetch Operator":{"limit:":"-1"}},"$hdt$_2:hd":{"Fetch Operator":{"limit:":"-1"}}},"Alias -> Map Local Operator Tree:":{"$hdt$_1:cd":{"TableScan":{"alias:":"cd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_2","children":{"Select Operator":{"expressions:":"cbigint (type: bigint), cstring2 (type: string)","outputColumnNames:":["_col0","_col1"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_3","children":{"HashTable Sink Operator":{"keys:":{"0":"_col1 (type: bigint), _col3 (type: string)","1":"_col0 (type: bigint), _col1 (type: string)"},"OperatorId:":"HASHTABLESINK_26"}}}}}},"$hdt$_2:hd":{"TableScan":{"alias:":"hd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_4","children":{"Select Operator":{"expressions:":"cint (type: int), cstring1 (type: string)","outputColumnNames:":["_col0","_col1"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_5","children":{"HashTable Sink Operator":{"keys:":{"0":"_col0 (type: int), _col2 (type: string)","1":"_col0 (type: int), _col1 (type: string)"},"OperatorId:":"HASHTABLESINK_24"}}}}}}}}},"Stage-3":{"Map Reduce":{"Map Operator Tree:":[{"TableScan":{"alias:":"c","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","TableScan Vectorization:":{"native:":"true","projectedOutputColumns:":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"},"OperatorId:":"TS_0","children":{"Select Operator":{"expressions:":"cint (type: int), cbigint (type: bigint), cstring1 (type: string), cstring2 (type: string)","outputColumnNames:":["_col0","_col1","_col2","_col3"],"Select Vectorization:":{"className:":"VectorSelectOperator","native:":"true","projectedOutputColumns:":"[2, 3, 6, 7]"},"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_28","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col1 (type: bigint), _col3 (type: string)","1":"_col0 (type: bigint), _col1 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"outputColumnNames:":["_col0","_col2"],"Statistics:":"Num rows: 22 Data size: 4840 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_29","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col0 (type: int), _col2 (type: string)","1":"_col0 (type: int), _col1 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 24 Data size: 5324 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_30","children":{"Group By Operator":{"aggregations:":["count()"],"Group By Vectorization:":{"aggregators:":["VectorUDAFCountStar(*) -> bigint"],"className:":"VectorGroupByOperator","vectorOutput:":"true","native:":"false","projectedOutputColumns:":"[0]"},"mode:":"hash","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_31","children":{"Reduce Output Operator":{"sort order:":"","Reduce Sink Vectorization:":{"className:":"VectorReduceSinkOperator","native:":"false","nativeConditionsMet:":["hive.vectorized.execution.reducesink.new.enabled IS true","No PTF TopN IS true","No DISTINCT columns IS true","BinarySortableSerDe for keys IS true","LazyBinarySerDe for values IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","value expressions:":"_col0 (type: bigint)","OperatorId:":"RS_32"}}}}}}}}}}}}],"Execution mode:":"vectorized","Map Vectorization:":{"enabled:":"true","enabledConditionsMet:":["hive.vectorized.use.vectorized.input.format IS true"],"groupByVectorOutput:":"true","inputFileFormats:":["org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"],"allNative:":"false","usesVectorUDFAdaptor:":"false","vectorized:":"true","rowBatchContext:":{"dataColumnCount:":"12","includeColumns:":"[2, 3, 6, 7]","dataColumns:":["ctinyint:tinyint","csmallint:smallint","cint:int","cbigint:bigint","cfloat:float","cdouble:double","cstring1:string","cstring2:string","ctimestamp1:timestamp","ctimestamp2:timestamp","cboolean1:boolean","cboolean2:boolean"],"partitionColumnCount:":"0"}},"Local Work:":{"Map Reduce Local Work":{}},"Reduce Vectorization:":{"enabled:":"false","enableConditionsMet:":["hive.vectorized.execution.reduce.enabled IS true"],"enableConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Reduce Operator Tree:":{"Group By Operator":{"aggregations:":["count(VALUE._col0)"],"Group By Vectorization:":{"vectorOutput:":"false","native:":"false","projectedOutputColumns:":"null"},"mode:":"mergepartial","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_15","children":{"File Output Operator":{"compressed:":"false","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","table:":{"input format:":"org.apache.hadoop.mapred.SequenceFileInputFormat","output format:":"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat","serde:":"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"},"OperatorId:":"FS_17"}}}}}},"Stage-0":{"Fetch Operator":{"limit:":"-1","Processor Tree:":{"ListSink":{"OperatorId:":"LIST_SINK_33"}}}}}}
+{"PLAN VECTORIZATION":{"enabled":true,"enabledConditionsMet":["hive.vectorized.execution.enabled IS true"]},"STAGE DEPENDENCIES":{"Stage-8":{"ROOT STAGE":"TRUE"},"Stage-3":{"DEPENDENT STAGES":"Stage-8"},"Stage-0":{"DEPENDENT STAGES":"Stage-3"}},"STAGE PLANS":{"Stage-8":{"Map Reduce Local Work":{"Alias -> Map Local Tables:":{"$hdt$_1:cd":{"Fetch Operator":{"limit:":"-1"}},"$hdt$_2:hd":{"Fetch Operator":{"limit:":"-1"}}},"Alias -> Map Local Operator Tree:":{"$hdt$_1:cd":{"TableScan":{"alias:":"cd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_2","children":{"Select Operator":{"expressions:":"cbigint (type: bigint), cstring2 (type: string)","outputColumnNames:":["_col0","_col1"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_3","children":{"HashTable Sink Operator":{"keys:":{"0":"_col1 (type: bigint), _col3 (type: string)","1":"_col0 (type: bigint), _col1 (type: string)"},"OperatorId:":"HASHTABLESINK_26"}}}}}},"$hdt$_2:hd":{"TableScan":{"alias:":"hd","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_4","children":{"Select Operator":{"expressions:":"cint (type: int), cstring1 (type: string)","outputColumnNames:":["_col0","_col1"],"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_5","children":{"HashTable Sink Operator":{"keys:":{"0":"_col0 (type: int), _col2 (type: string)","1":"_col0 (type: int), _col1 (type: string)"},"OperatorId:":"HASHTABLESINK_24"}}}}}}}}},"Stage-3":{"Map Reduce":{"Map Operator Tree:":[{"TableScan":{"alias:":"c","Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","TableScan Vectorization:":{"native:":"true","projectedOutputColumns:":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"},"OperatorId:":"TS_0","children":{"Select Operator":{"expressions:":"cint (type: int), cbigint (type: bigint), cstring1 (type: string), cstring2 (type: string)","outputColumnNames:":["_col0","_col1","_col2","_col3"],"Select Vectorization:":{"className:":"VectorSelectOperator","native:":"true","projectedOutputColumns:":"[2, 3, 6, 7]"},"Statistics:":"Num rows: 20 Data size: 4400 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_28","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col1 (type: bigint), _col3 (type: string)","1":"_col0 (type: bigint), _col1 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"outputColumnNames:":["_col0","_col2"],"Statistics:":"Num rows: 22 Data size: 4840 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_29","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col0 (type: int), _col2 (type: string)","1":"_col0 (type: int), _col1 (type: string)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 24 Data size: 5324 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_30","children":{"Group By Operator":{"aggregations:":["count()"],"Group By Vectorization:":{"aggregators:":["VectorUDAFCountStar(*) -> bigint"],"className:":"VectorGroupByOperator","groupByMode:":"HASH","vectorOutput:":"true","native:":"false","vectorProcessingMode:":"HASH","projectedOutputColumns:":"[0]"},"mode:":"hash","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_31","children":{"Reduce Output Operator":{"sort order:":"","Reduce Sink Vectorization:":{"className:":"VectorReduceSinkOperator","native:":"false","nativeConditionsMet:":["hive.vectorized.execution.reducesink.new.enabled IS true","No PTF TopN IS true","No DISTINCT columns IS true","BinarySortableSerDe for keys IS true","LazyBinarySerDe for values IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","value expressions:":"_col0 (type: bigint)","OperatorId:":"RS_32"}}}}}}}}}}}}],"Execution mode:":"vectorized","Map Vectorization:":{"enabled:":"true","enabledConditionsMet:":["hive.vectorized.use.vectorized.input.format IS true"],"groupByVectorOutput:":"true","inputFileFormats:":["org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"],"allNative:":"false","usesVectorUDFAdaptor:":"false","vectorized:":"true","rowBatchContext:":{"dataColumnCount:":"12","includeColumns:":"[2, 3, 6, 7]","dataColumns:":["ctinyint:tinyint","csmallint:smallint","cint:int","cbigint:bigint","cfloat:float","cdouble:double","cstring1:string","cstring2:string","ctimestamp1:timestamp","ctimestamp2:timestamp","cboolean1:boolean","cboolean2:boolean"],"partitionColumnCount:":"0"}},"Local Work:":{"Map Reduce Local Work":{}},"Reduce Vectorization:":{"enabled:":"false","enableConditionsMet:":["hive.vectorized.execution.reduce.enabled IS true"],"enableConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Reduce Operator Tree:":{"Group By Operator":{"aggregations:":["count(VALUE._col0)"],"Group By Vectorization:":{"groupByMode:":"MERGEPARTIAL","vectorOutput:":"false","native:":"false","vectorProcessingMode:":"NONE","projectedOutputColumns:":"null"},"mode:":"mergepartial","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_15","children":{"File Output Operator":{"compressed:":"false","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","table:":{"input format:":"org.apache.hadoop.mapred.SequenceFileInputFormat","output format:":"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat","serde:":"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"},"OperatorId:":"FS_17"}}}}}},"Stage-0":{"Fetch Operator":{"limit:":"-1","Processor Tree:":{"ListSink":{"OperatorId:":"LIST_SINK_33"}}}}}}
 PREHOOK: query: select count(*) from (select c.cstring1
 from small_alltypesorc_a c
 left outer join small_alltypesorc_a cd
diff --git a/ql/src/test/results/clientpositive/vector_outer_join4.q.out b/ql/src/test/results/clientpositive/vector_outer_join4.q.out
index 136e386fc5..125ec07353 100644
--- a/ql/src/test/results/clientpositive/vector_outer_join4.q.out
+++ b/ql/src/test/results/clientpositive/vector_outer_join4.q.out
@@ -780,7 +780,7 @@ left outer join small_alltypesorc_b hd
   on hd.ctinyint = c.ctinyint
 ) t1
 POSTHOOK: type: QUERY
-{"PLAN VECTORIZATION":{"enabled":true,"enabledConditionsMet":["hive.vectorized.execution.enabled IS true"]},"STAGE DEPENDENCIES":{"Stage-8":{"ROOT STAGE":"TRUE"},"Stage-3":{"DEPENDENT STAGES":"Stage-8"},"Stage-0":{"DEPENDENT STAGES":"Stage-3"}},"STAGE PLANS":{"Stage-8":{"Map Reduce Local Work":{"Alias -> Map Local Tables:":{"$hdt$_1:cd":{"Fetch Operator":{"limit:":"-1"}},"$hdt$_2:hd":{"Fetch Operator":{"limit:":"-1"}}},"Alias -> Map Local Operator Tree:":{"$hdt$_1:cd":{"TableScan":{"alias:":"cd","Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_2","children":{"Select Operator":{"expressions:":"cint (type: int)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_3","children":{"HashTable Sink Operator":{"keys:":{"0":"_col1 (type: int)","1":"_col0 (type: int)"},"OperatorId:":"HASHTABLESINK_26"}}}}}},"$hdt$_2:hd":{"TableScan":{"alias:":"hd","Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_4","children":{"Select Operator":{"expressions:":"ctinyint (type: tinyint)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_5","children":{"HashTable Sink Operator":{"keys:":{"0":"_col0 (type: tinyint)","1":"_col0 (type: tinyint)"},"OperatorId:":"HASHTABLESINK_24"}}}}}}}}},"Stage-3":{"Map Reduce":{"Map Operator Tree:":[{"TableScan":{"alias:":"c","Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","TableScan Vectorization:":{"native:":"true","projectedOutputColumns:":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"},"OperatorId:":"TS_0","children":{"Select Operator":{"expressions:":"ctinyint (type: tinyint), cint (type: int)","outputColumnNames:":["_col0","_col1"],"Select Vectorization:":{"className:":"VectorSelectOperator","native:":"true","projectedOutputColumns:":"[0, 2]"},"Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_28","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col1 (type: int)","1":"_col0 (type: int)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"outputColumnNames:":["_col0"],"Statistics:":"Num rows: 33 Data size: 7348 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_29","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col0 (type: tinyint)","1":"_col0 (type: tinyint)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 36 Data size: 8082 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_30","children":{"Group By Operator":{"aggregations:":["count()"],"Group By Vectorization:":{"aggregators:":["VectorUDAFCountStar(*) -> bigint"],"className:":"VectorGroupByOperator","vectorOutput:":"true","native:":"false","projectedOutputColumns:":"[0]"},"mode:":"hash","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_31","children":{"Reduce Output Operator":{"sort order:":"","Reduce Sink Vectorization:":{"className:":"VectorReduceSinkOperator","native:":"false","nativeConditionsMet:":["hive.vectorized.execution.reducesink.new.enabled IS true","No PTF TopN IS true","No DISTINCT columns IS true","BinarySortableSerDe for keys IS true","LazyBinarySerDe for values IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","value expressions:":"_col0 (type: bigint)","OperatorId:":"RS_32"}}}}}}}}}}}}],"Execution mode:":"vectorized","Map Vectorization:":{"enabled:":"true","enabledConditionsMet:":["hive.vectorized.use.vectorized.input.format IS true"],"groupByVectorOutput:":"true","inputFileFormats:":["org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"],"allNative:":"false","usesVectorUDFAdaptor:":"false","vectorized:":"true","rowBatchContext:":{"dataColumnCount:":"12","includeColumns:":"[0, 2]","dataColumns:":["ctinyint:tinyint","csmallint:smallint","cint:int","cbigint:bigint","cfloat:float","cdouble:double","cstring1:string","cstring2:string","ctimestamp1:timestamp","ctimestamp2:timestamp","cboolean1:boolean","cboolean2:boolean"],"partitionColumnCount:":"0"}},"Local Work:":{"Map Reduce Local Work":{}},"Reduce Vectorization:":{"enabled:":"false","enableConditionsMet:":["hive.vectorized.execution.reduce.enabled IS true"],"enableConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Reduce Operator Tree:":{"Group By Operator":{"aggregations:":["count(VALUE._col0)"],"Group By Vectorization:":{"vectorOutput:":"false","native:":"false","projectedOutputColumns:":"null"},"mode:":"mergepartial","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_15","children":{"File Output Operator":{"compressed:":"false","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","table:":{"input format:":"org.apache.hadoop.mapred.SequenceFileInputFormat","output format:":"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat","serde:":"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"},"OperatorId:":"FS_17"}}}}}},"Stage-0":{"Fetch Operator":{"limit:":"-1","Processor Tree:":{"ListSink":{"OperatorId:":"LIST_SINK_33"}}}}}}
+{"PLAN VECTORIZATION":{"enabled":true,"enabledConditionsMet":["hive.vectorized.execution.enabled IS true"]},"STAGE DEPENDENCIES":{"Stage-8":{"ROOT STAGE":"TRUE"},"Stage-3":{"DEPENDENT STAGES":"Stage-8"},"Stage-0":{"DEPENDENT STAGES":"Stage-3"}},"STAGE PLANS":{"Stage-8":{"Map Reduce Local Work":{"Alias -> Map Local Tables:":{"$hdt$_1:cd":{"Fetch Operator":{"limit:":"-1"}},"$hdt$_2:hd":{"Fetch Operator":{"limit:":"-1"}}},"Alias -> Map Local Operator Tree:":{"$hdt$_1:cd":{"TableScan":{"alias:":"cd","Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_2","children":{"Select Operator":{"expressions:":"cint (type: int)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_3","children":{"HashTable Sink Operator":{"keys:":{"0":"_col1 (type: int)","1":"_col0 (type: int)"},"OperatorId:":"HASHTABLESINK_26"}}}}}},"$hdt$_2:hd":{"TableScan":{"alias:":"hd","Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"TS_4","children":{"Select Operator":{"expressions:":"ctinyint (type: tinyint)","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_5","children":{"HashTable Sink Operator":{"keys:":{"0":"_col0 (type: tinyint)","1":"_col0 (type: tinyint)"},"OperatorId:":"HASHTABLESINK_24"}}}}}}}}},"Stage-3":{"Map Reduce":{"Map Operator Tree:":[{"TableScan":{"alias:":"c","Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","TableScan Vectorization:":{"native:":"true","projectedOutputColumns:":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"},"OperatorId:":"TS_0","children":{"Select Operator":{"expressions:":"ctinyint (type: tinyint), cint (type: int)","outputColumnNames:":["_col0","_col1"],"Select Vectorization:":{"className:":"VectorSelectOperator","native:":"true","projectedOutputColumns:":"[0, 2]"},"Statistics:":"Num rows: 30 Data size: 6680 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"SEL_28","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col1 (type: int)","1":"_col0 (type: int)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"outputColumnNames:":["_col0"],"Statistics:":"Num rows: 33 Data size: 7348 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_29","children":{"Map Join Operator":{"condition map:":[{"":"Left Outer Join 0 to 1"}],"keys:":{"0":"_col0 (type: tinyint)","1":"_col0 (type: tinyint)"},"Map Join Vectorization:":{"className:":"VectorMapJoinOperator","native:":"false","nativeConditionsMet:":["hive.mapjoin.optimized.hashtable IS true","hive.vectorized.execution.mapjoin.native.enabled IS true","One MapJoin Condition IS true","No nullsafe IS true","Small table vectorizes IS true","Optimized Table and Supports Key Types IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 36 Data size: 8082 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"MAPJOIN_30","children":{"Group By Operator":{"aggregations:":["count()"],"Group By Vectorization:":{"aggregators:":["VectorUDAFCountStar(*) -> bigint"],"className:":"VectorGroupByOperator","groupByMode:":"HASH","vectorOutput:":"true","native:":"false","vectorProcessingMode:":"HASH","projectedOutputColumns:":"[0]"},"mode:":"hash","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_31","children":{"Reduce Output Operator":{"sort order:":"","Reduce Sink Vectorization:":{"className:":"VectorReduceSinkOperator","native:":"false","nativeConditionsMet:":["hive.vectorized.execution.reducesink.new.enabled IS true","No PTF TopN IS true","No DISTINCT columns IS true","BinarySortableSerDe for keys IS true","LazyBinarySerDe for values IS true"],"nativeConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","value expressions:":"_col0 (type: bigint)","OperatorId:":"RS_32"}}}}}}}}}}}}],"Execution mode:":"vectorized","Map Vectorization:":{"enabled:":"true","enabledConditionsMet:":["hive.vectorized.use.vectorized.input.format IS true"],"groupByVectorOutput:":"true","inputFileFormats:":["org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"],"allNative:":"false","usesVectorUDFAdaptor:":"false","vectorized:":"true","rowBatchContext:":{"dataColumnCount:":"12","includeColumns:":"[0, 2]","dataColumns:":["ctinyint:tinyint","csmallint:smallint","cint:int","cbigint:bigint","cfloat:float","cdouble:double","cstring1:string","cstring2:string","ctimestamp1:timestamp","ctimestamp2:timestamp","cboolean1:boolean","cboolean2:boolean"],"partitionColumnCount:":"0"}},"Local Work:":{"Map Reduce Local Work":{}},"Reduce Vectorization:":{"enabled:":"false","enableConditionsMet:":["hive.vectorized.execution.reduce.enabled IS true"],"enableConditionsNotMet:":["hive.execution.engine mr IN [tez, spark] IS false"]},"Reduce Operator Tree:":{"Group By Operator":{"aggregations:":["count(VALUE._col0)"],"Group By Vectorization:":{"groupByMode:":"MERGEPARTIAL","vectorOutput:":"false","native:":"false","vectorProcessingMode:":"NONE","projectedOutputColumns:":"null"},"mode:":"mergepartial","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","OperatorId:":"GBY_15","children":{"File Output Operator":{"compressed:":"false","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE","table:":{"input format:":"org.apache.hadoop.mapred.SequenceFileInputFormat","output format:":"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat","serde:":"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"},"OperatorId:":"FS_17"}}}}}},"Stage-0":{"Fetch Operator":{"limit:":"-1","Processor Tree:":{"ListSink":{"OperatorId:":"LIST_SINK_33"}}}}}}
 PREHOOK: query: select count(*) from (select c.ctinyint
 from small_alltypesorc_b c
 left outer join small_alltypesorc_b cd
diff --git a/ql/src/test/results/clientpositive/vector_reduce_groupby_decimal.q.out b/ql/src/test/results/clientpositive/vector_reduce_groupby_decimal.q.out
index 724ef450a8..f90100d1d7 100644
--- a/ql/src/test/results/clientpositive/vector_reduce_groupby_decimal.q.out
+++ b/ql/src/test/results/clientpositive/vector_reduce_groupby_decimal.q.out
@@ -59,9 +59,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinDecimal(col 2) -> decimal(20,10)
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 0, col 1, col 2, col 3
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0]
                 keys: cint (type: int), cdouble (type: double), cdecimal1 (type: decimal(20,10)), cdecimal2 (type: decimal(23,14))
                 mode: hash
@@ -96,8 +98,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: min(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: int), KEY._col1 (type: double), KEY._col2 (type: decimal(20,10)), KEY._col3 (type: decimal(23,14))
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_string_concat.q.out b/ql/src/test/results/clientpositive/vector_string_concat.q.out
index 00f9b38d14..9f6fe7d1f7 100644
--- a/ql/src/test/results/clientpositive/vector_string_concat.q.out
+++ b/ql/src/test/results/clientpositive/vector_string_concat.q.out
@@ -346,9 +346,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 19
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: _col0 (type: string)
                 mode: hash
@@ -381,8 +383,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out b/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out
index c96ea00a86..fd9908f4e0 100644
--- a/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out
+++ b/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out
@@ -1,7 +1,7 @@
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 select 'key1', 'value1' from alltypesorc tablesample (1 rows)
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 select 'key1', 'value1' from alltypesorc tablesample (1 rows)
 POSTHOOK: type: QUERY
 Explain
@@ -52,6 +52,12 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: []
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: string, string
 
   Stage: Stage-0
     Fetch Operator
@@ -77,11 +83,11 @@ POSTHOOK: query: create table decimal_2 (t decimal(18,9)) stored as orc
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: database:default
 POSTHOOK: Output: default@decimal_2
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 insert overwrite table decimal_2
   select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows)
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 insert overwrite table decimal_2
   select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows)
 POSTHOOK: type: QUERY
@@ -140,6 +146,12 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: []
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: decimal(18,9)
 
   Stage: Stage-7
     Conditional Operator
@@ -213,10 +225,10 @@ POSTHOOK: query: drop table decimal_2
 POSTHOOK: type: DROPTABLE
 POSTHOOK: Input: default@decimal_2
 POSTHOOK: Output: default@decimal_2
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 select count(1) from (select * from (Select 1 a) x order by x.a) y
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 select count(1) from (select * from (Select 1 a) x order by x.a) y
 POSTHOOK: type: QUERY
 Explain
@@ -256,8 +268,10 @@ STAGE PLANS:
           Group By Operator
             aggregations: count(1)
             Group By Vectorization:
+                groupByMode: HASH
                 vectorOutput: false
                 native: false
+                vectorProcessingMode: NONE
                 projectedOutputColumns: null
             mode: hash
             outputColumnNames: _col0
@@ -294,6 +308,11 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 1
+              includeColumns: [0]
+              dataColumns: _col0:bigint
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -302,8 +321,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0
@@ -332,10 +353,10 @@ POSTHOOK: Input: _dummy_database@_dummy_table
 #### A masked pattern was here ####
 _c0
 1
-PREHOOK: query: explain vectorization expression
+PREHOOK: query: explain vectorization detail
 create temporary table dual as select 1
 PREHOOK: type: CREATETABLE_AS_SELECT
-POSTHOOK: query: explain vectorization expression
+POSTHOOK: query: explain vectorization detail
 create temporary table dual as select 1
 POSTHOOK: type: CREATETABLE_AS_SELECT
 Explain
diff --git a/ql/src/test/results/clientpositive/vector_when_case_null.q.out b/ql/src/test/results/clientpositive/vector_when_case_null.q.out
index 5ae4b99b55..e0023361c5 100644
--- a/ql/src/test/results/clientpositive/vector_when_case_null.q.out
+++ b/ql/src/test/results/clientpositive/vector_when_case_null.q.out
@@ -52,9 +52,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCount(col 5) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0]
                 keys: _col0 (type: string)
                 mode: hash
@@ -88,8 +90,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: string)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vectorization_1.q.out b/ql/src/test/results/clientpositive/vectorization_1.q.out
index e0a434480b..9c2ce2a174 100644
--- a/ql/src/test/results/clientpositive/vectorization_1.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_1.q.out
@@ -1,3 +1,149 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT VAR_POP(ctinyint),
+       (VAR_POP(ctinyint) / -26.28),
+       SUM(cfloat),
+       (-1.389 + SUM(cfloat)),
+       (SUM(cfloat) * (-1.389 + SUM(cfloat))),
+       MAX(ctinyint),
+       (-((SUM(cfloat) * (-1.389 + SUM(cfloat))))),
+       MAX(cint),
+       (MAX(cint) * 79.553),
+       VAR_SAMP(cdouble),
+       (10.175 % (-((SUM(cfloat) * (-1.389 + SUM(cfloat)))))),
+       COUNT(cint),
+       (-563 % MAX(cint))
+FROM   alltypesorc
+WHERE  (((cdouble > ctinyint)
+         AND (cboolean2 > 0))
+        OR ((cbigint < ctinyint)
+            OR ((cint > cbigint)
+                OR (cboolean1 < 0))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT VAR_POP(ctinyint),
+       (VAR_POP(ctinyint) / -26.28),
+       SUM(cfloat),
+       (-1.389 + SUM(cfloat)),
+       (SUM(cfloat) * (-1.389 + SUM(cfloat))),
+       MAX(ctinyint),
+       (-((SUM(cfloat) * (-1.389 + SUM(cfloat))))),
+       MAX(cint),
+       (MAX(cint) * 79.553),
+       VAR_SAMP(cdouble),
+       (10.175 % (-((SUM(cfloat) * (-1.389 + SUM(cfloat)))))),
+       COUNT(cint),
+       (-563 % MAX(cint))
+FROM   alltypesorc
+WHERE  (((cdouble > ctinyint)
+         AND (cboolean2 > 0))
+        OR ((cbigint < ctinyint)
+            OR ((cint > cbigint)
+                OR (cboolean1 < 0))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: alltypesorc
+            Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: FilterDoubleColGreaterDoubleColumn(col 5, col 12)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterLongColGreaterLongScalar(col 11, val 0) -> boolean) -> boolean, FilterLongColLessLongColumn(col 3, col 0)(children: col 0) -> boolean, FilterLongColGreaterLongColumn(col 2, col 3)(children: col 2) -> boolean, FilterLongColLessLongScalar(col 10, val 0) -> boolean) -> boolean
+              predicate: (((cdouble > UDFToDouble(ctinyint)) and (cboolean2 > 0)) or (cbigint < UDFToLong(ctinyint)) or (UDFToLong(cint) > cbigint) or (cboolean1 < 0)) (type: boolean)
+              Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ctinyint (type: tinyint), cint (type: int), cfloat (type: float), cdouble (type: double)
+                outputColumnNames: ctinyint, cint, cfloat, cdouble
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0, 2, 4, 5]
+                Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: var_pop(ctinyint), sum(cfloat), max(ctinyint), max(cint), var_samp(cdouble), count(cint)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumDouble(col 4) -> double, VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFMaxLong(col 2) -> int, VectorUDAFVarSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 2) -> bigint
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                  Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    sort order: 
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                    Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: double), _col2 (type: tinyint), _col3 (type: int), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: bigint)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 2, 3, 4, 5, 10, 11]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: double
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: var_pop(VALUE._col0), sum(VALUE._col1), max(VALUE._col2), max(VALUE._col3), var_samp(VALUE._col4), count(VALUE._col5)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+          Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col0 (type: double), (_col0 / -26.28) (type: double), _col1 (type: double), (-1.389 + _col1) (type: double), (_col1 * (-1.389 + _col1)) (type: double), _col2 (type: tinyint), (- (_col1 * (-1.389 + _col1))) (type: double), _col3 (type: int), (CAST( _col3 AS decimal(10,0)) * 79.553) (type: decimal(16,3)), _col4 (type: double), (10.175 % (- (_col1 * (-1.389 + _col1)))) (type: double), _col5 (type: bigint), (-563 % _col3) (type: int)
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+            Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT VAR_POP(ctinyint),
        (VAR_POP(ctinyint) / -26.28),
        SUM(cfloat),
diff --git a/ql/src/test/results/clientpositive/vectorization_10.q.out b/ql/src/test/results/clientpositive/vectorization_10.q.out
index 9dad4c440d..d2d9bf644b 100644
--- a/ql/src/test/results/clientpositive/vectorization_10.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_10.q.out
@@ -1,3 +1,117 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cdouble,
+       ctimestamp1,
+       ctinyint,
+       cboolean1,
+       cstring1,
+       (-(cdouble)),
+       (cdouble + csmallint),
+       ((cdouble + csmallint) % 33),
+       (-(cdouble)),
+       (ctinyint % cdouble),
+       (ctinyint % csmallint),
+       (-(cdouble)),
+       (cbigint * (ctinyint % csmallint)),
+       (9763215.5639 - (cdouble + csmallint)),
+       (-((-(cdouble))))
+FROM   alltypesorc
+WHERE  (((cstring2 <= '10')
+         OR ((ctinyint > cdouble)
+             AND (-5638.15 >= ctinyint)))
+        OR ((cdouble > 6981)
+            AND ((csmallint = 9763215.5639)
+                 OR (cstring1 LIKE '%a'))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cdouble,
+       ctimestamp1,
+       ctinyint,
+       cboolean1,
+       cstring1,
+       (-(cdouble)),
+       (cdouble + csmallint),
+       ((cdouble + csmallint) % 33),
+       (-(cdouble)),
+       (ctinyint % cdouble),
+       (ctinyint % csmallint),
+       (-(cdouble)),
+       (cbigint * (ctinyint % csmallint)),
+       (9763215.5639 - (cdouble + csmallint)),
+       (-((-(cdouble))))
+FROM   alltypesorc
+WHERE  (((cstring2 <= '10')
+         OR ((ctinyint > cdouble)
+             AND (-5638.15 >= ctinyint)))
+        OR ((cdouble > 6981)
+            AND ((csmallint = 9763215.5639)
+                 OR (cstring1 LIKE '%a'))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: alltypesorc
+            Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprOrExpr(children: FilterStringGroupColLessEqualStringScalar(col 7, val 10) -> boolean, FilterExprAndExpr(children: FilterDoubleColGreaterDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterDecimalScalarGreaterEqualDecimalColumn(val -5638.15, col 13)(children: CastLongToDecimal(col 0) -> 13:decimal(6,2)) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColGreaterDoubleScalar(col 5, val 6981.0) -> boolean, FilterExprOrExpr(children: FilterDecimalColEqualDecimalScalar(col 14, val 9763215.5639)(children: CastLongToDecimal(col 1) -> 14:decimal(11,4)) -> boolean, FilterStringColLikeStringScalar(col 6, pattern %a) -> boolean) -> boolean) -> boolean) -> boolean
+              predicate: ((cstring2 <= '10') or ((UDFToDouble(ctinyint) > cdouble) and (-5638.15 >= CAST( ctinyint AS decimal(6,2)))) or ((cdouble > 6981.0) and ((CAST( csmallint AS decimal(11,4)) = 9763215.5639) or (cstring1 like '%a')))) (type: boolean)
+              Statistics: Num rows: 5461 Data size: 1174134 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: cdouble (type: double), ctimestamp1 (type: timestamp), ctinyint (type: tinyint), cboolean1 (type: boolean), cstring1 (type: string), (- cdouble) (type: double), (cdouble + UDFToDouble(csmallint)) (type: double), ((cdouble + UDFToDouble(csmallint)) % 33.0) (type: double), (- cdouble) (type: double), (UDFToDouble(ctinyint) % cdouble) (type: double), (UDFToShort(ctinyint) % csmallint) (type: smallint), (- cdouble) (type: double), (cbigint * UDFToLong((UDFToShort(ctinyint) % csmallint))) (type: bigint), (9763215.5639 - (cdouble + UDFToDouble(csmallint))) (type: double), (- (- cdouble)) (type: double)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [5, 8, 0, 10, 6, 12, 16, 15, 17, 19, 20, 18, 22, 23, 25]
+                    selectExpressions: DoubleColUnaryMinus(col 5) -> 12:double, DoubleColAddDoubleColumn(col 5, col 15)(children: CastLongToDouble(col 1) -> 15:double) -> 16:double, DoubleColModuloDoubleScalar(col 17, val 33.0)(children: DoubleColAddDoubleColumn(col 5, col 15)(children: CastLongToDouble(col 1) -> 15:double) -> 17:double) -> 15:double, DoubleColUnaryMinus(col 5) -> 17:double, DoubleColModuloDoubleColumn(col 18, col 5)(children: CastLongToDouble(col 0) -> 18:double) -> 19:double, LongColModuloLongColumn(col 0, col 1)(children: col 0) -> 20:long, DoubleColUnaryMinus(col 5) -> 18:double, LongColMultiplyLongColumn(col 3, col 21)(children: col 21) -> 22:long, DoubleScalarSubtractDoubleColumn(val 9763215.5639, col 24)(children: DoubleColAddDoubleColumn(col 5, col 23)(children: CastLongToDouble(col 1) -> 23:double) -> 24:double) -> 23:double, DoubleColUnaryMinus(col 24)(children: DoubleColUnaryMinus(col 5) -> 24:double) -> 25:double
+                Statistics: Num rows: 5461 Data size: 1174134 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 5461 Data size: 1174134 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 3, 5, 6, 7, 8, 10]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: double, decimal(6,2), decimal(11,4), double, double, double, double, double, bigint, bigint, bigint, double, double, double
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT cdouble,
        ctimestamp1,
        ctinyint,
diff --git a/ql/src/test/results/clientpositive/vectorization_11.q.out b/ql/src/test/results/clientpositive/vectorization_11.q.out
index dff58dab42..bc03170554 100644
--- a/ql/src/test/results/clientpositive/vectorization_11.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_11.q.out
@@ -1,3 +1,99 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cstring1,
+       cboolean1,
+       cdouble,
+       ctimestamp1,
+       (-3728 * csmallint),
+       (cdouble - 9763215.5639),
+       (-(cdouble)),
+       ((-(cdouble)) + 6981),
+       (cdouble * -5638.15)
+FROM   alltypesorc
+WHERE  ((cstring2 = cstring1)
+        OR ((ctimestamp1 IS NULL)
+            AND (cstring1 LIKE '%a')))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cstring1,
+       cboolean1,
+       cdouble,
+       ctimestamp1,
+       (-3728 * csmallint),
+       (cdouble - 9763215.5639),
+       (-(cdouble)),
+       ((-(cdouble)) + 6981),
+       (cdouble * -5638.15)
+FROM   alltypesorc
+WHERE  ((cstring2 = cstring1)
+        OR ((ctimestamp1 IS NULL)
+            AND (cstring1 LIKE '%a')))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: alltypesorc
+            Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprOrExpr(children: FilterStringGroupColEqualStringGroupColumn(col 7, col 6) -> boolean, FilterExprAndExpr(children: SelectColumnIsNull(col 8) -> boolean, FilterStringColLikeStringScalar(col 6, pattern %a) -> boolean) -> boolean) -> boolean
+              predicate: ((cstring2 = cstring1) or (ctimestamp1 is null and (cstring1 like '%a'))) (type: boolean)
+              Statistics: Num rows: 9216 Data size: 1981473 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: cstring1 (type: string), cboolean1 (type: boolean), cdouble (type: double), ctimestamp1 (type: timestamp), (-3728 * UDFToInteger(csmallint)) (type: int), (cdouble - 9763215.5639) (type: double), (- cdouble) (type: double), ((- cdouble) + 6981.0) (type: double), (cdouble * -5638.15) (type: double)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [6, 10, 5, 8, 12, 13, 14, 16, 15]
+                    selectExpressions: LongScalarMultiplyLongColumn(val -3728, col 1)(children: col 1) -> 12:long, DoubleColSubtractDoubleScalar(col 5, val 9763215.5639) -> 13:double, DoubleColUnaryMinus(col 5) -> 14:double, DoubleColAddDoubleScalar(col 15, val 6981.0)(children: DoubleColUnaryMinus(col 5) -> 15:double) -> 16:double, DoubleColMultiplyDoubleScalar(col 5, val -5638.15) -> 15:double
+                Statistics: Num rows: 9216 Data size: 1981473 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 9216 Data size: 1981473 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [1, 5, 6, 7, 8, 10]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: bigint, double, double, double, double
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT cstring1,
        cboolean1,
        cdouble,
diff --git a/ql/src/test/results/clientpositive/vectorization_12.q.out b/ql/src/test/results/clientpositive/vectorization_12.q.out
index 6a7f69c698..df3f0472de 100644
--- a/ql/src/test/results/clientpositive/vectorization_12.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_12.q.out
@@ -1,3 +1,223 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT   cbigint,
+         cboolean1,
+         cstring1,
+         ctimestamp1,
+         cdouble,
+         (-6432 * cdouble),
+         (-(cbigint)),
+         COUNT(cbigint),
+         (cbigint * COUNT(cbigint)),
+         STDDEV_SAMP(cbigint),
+         ((-6432 * cdouble) / -6432),
+         (-(((-6432 * cdouble) / -6432))),
+         AVG(cdouble),
+         (-((-6432 * cdouble))),
+         (-5638.15 + cbigint),
+         SUM(cbigint),
+         (AVG(cdouble) / (-6432 * cdouble)),
+         AVG(cdouble),
+         (-((-(((-6432 * cdouble) / -6432))))),
+         (((-6432 * cdouble) / -6432) + (-((-6432 * cdouble)))),
+         STDDEV_POP(cdouble)
+FROM     alltypesorc
+WHERE    (((ctimestamp1 IS NULL)
+           AND ((cboolean1 >= cboolean2)
+                OR (ctinyint != csmallint)))
+          AND ((cstring1 LIKE '%a')
+              OR ((cboolean2 <= 1)
+                  AND (cbigint >= csmallint))))
+GROUP BY cbigint, cboolean1, cstring1, ctimestamp1, cdouble
+ORDER BY ctimestamp1, cdouble, cbigint, cstring1
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT   cbigint,
+         cboolean1,
+         cstring1,
+         ctimestamp1,
+         cdouble,
+         (-6432 * cdouble),
+         (-(cbigint)),
+         COUNT(cbigint),
+         (cbigint * COUNT(cbigint)),
+         STDDEV_SAMP(cbigint),
+         ((-6432 * cdouble) / -6432),
+         (-(((-6432 * cdouble) / -6432))),
+         AVG(cdouble),
+         (-((-6432 * cdouble))),
+         (-5638.15 + cbigint),
+         SUM(cbigint),
+         (AVG(cdouble) / (-6432 * cdouble)),
+         AVG(cdouble),
+         (-((-(((-6432 * cdouble) / -6432))))),
+         (((-6432 * cdouble) / -6432) + (-((-6432 * cdouble)))),
+         STDDEV_POP(cdouble)
+FROM     alltypesorc
+WHERE    (((ctimestamp1 IS NULL)
+           AND ((cboolean1 >= cboolean2)
+                OR (ctinyint != csmallint)))
+          AND ((cstring1 LIKE '%a')
+              OR ((cboolean2 <= 1)
+                  AND (cbigint >= csmallint))))
+GROUP BY cbigint, cboolean1, cstring1, ctimestamp1, cdouble
+ORDER BY ctimestamp1, cdouble, cbigint, cstring1
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: alltypesorc
+            Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprAndExpr(children: SelectColumnIsNull(col 8) -> boolean, FilterExprOrExpr(children: FilterLongColGreaterEqualLongColumn(col 10, col 11) -> boolean, FilterLongColNotEqualLongColumn(col 0, col 1)(children: col 0) -> boolean) -> boolean, FilterExprOrExpr(children: FilterStringColLikeStringScalar(col 6, pattern %a) -> boolean, FilterExprAndExpr(children: FilterLongColLessEqualLongScalar(col 11, val 1) -> boolean, FilterLongColGreaterEqualLongColumn(col 3, col 1)(children: col 1) -> boolean) -> boolean) -> boolean) -> boolean
+              predicate: (ctimestamp1 is null and ((cboolean1 >= cboolean2) or (UDFToShort(ctinyint) <> csmallint)) and ((cstring1 like '%a') or ((cboolean2 <= 1) and (cbigint >= UDFToLong(csmallint))))) (type: boolean)
+              Statistics: Num rows: 5006 Data size: 1076307 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: cbigint (type: bigint), cdouble (type: double), cstring1 (type: string), cboolean1 (type: boolean)
+                outputColumnNames: cbigint, cdouble, cstring1, cboolean1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [3, 5, 6, 10]
+                Statistics: Num rows: 5006 Data size: 1076307 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: count(cbigint), stddev_samp(cbigint), avg(cdouble), sum(cbigint), stddev_pop(cdouble)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFCount(col 3) -> bigint, VectorUDAFStdSampLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>, VectorUDAFSumLong(col 3) -> bigint, VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      keyExpressions: col 5, col 3, col 6, col 10
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  keys: cdouble (type: double), cbigint (type: bigint), cstring1 (type: string), cboolean1 (type: boolean)
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+                  Statistics: Num rows: 5006 Data size: 1076307 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: string), _col3 (type: boolean)
+                    sort order: ++++
+                    Map-reduce partition columns: _col0 (type: double), _col1 (type: bigint), _col2 (type: string), _col3 (type: boolean)
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                    Statistics: Num rows: 5006 Data size: 1076307 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col4 (type: bigint), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: struct<count:bigint,sum:double,input:double>), _col7 (type: bigint), _col8 (type: struct<count:bigint,sum:double,variance:double>)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 3, 5, 6, 8, 10, 11]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: count(VALUE._col0), stddev_samp(VALUE._col1), avg(VALUE._col2), sum(VALUE._col3), stddev_pop(VALUE._col4)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          keys: KEY._col0 (type: double), KEY._col1 (type: bigint), KEY._col2 (type: string), KEY._col3 (type: boolean)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+          Statistics: Num rows: 2503 Data size: 538153 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col1 (type: bigint), _col3 (type: boolean), _col2 (type: string), _col0 (type: double), (-6432.0 * _col0) (type: double), (- _col1) (type: bigint), _col4 (type: bigint), (_col1 * _col4) (type: bigint), _col5 (type: double), ((-6432.0 * _col0) / -6432.0) (type: double), (- ((-6432.0 * _col0) / -6432.0)) (type: double), _col6 (type: double), (- (-6432.0 * _col0)) (type: double), (-5638.15 + CAST( _col1 AS decimal(19,0))) (type: decimal(22,2)), _col7 (type: bigint), (_col6 / (-6432.0 * _col0)) (type: double), (- (- ((-6432.0 * _col0) / -6432.0))) (type: double), (((-6432.0 * _col0) / -6432.0) + (- (-6432.0 * _col0))) (type: double), _col8 (type: double)
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col17, _col18, _col19
+            Statistics: Num rows: 2503 Data size: 538153 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+
+  Stage: Stage-2
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
+            Reduce Output Operator
+              key expressions: _col3 (type: double), _col0 (type: bigint), _col2 (type: string)
+              sort order: +++
+              Reduce Sink Vectorization:
+                  className: VectorReduceSinkOperator
+                  native: false
+                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+              Statistics: Num rows: 2503 Data size: 538153 Basic stats: COMPLETE Column stats: NONE
+              value expressions: _col1 (type: boolean), _col4 (type: double), _col5 (type: bigint), _col6 (type: bigint), _col7 (type: bigint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: double), _col12 (type: double), _col13 (type: decimal(22,2)), _col14 (type: bigint), _col15 (type: double), _col17 (type: double), _col18 (type: double), _col19 (type: double)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.mapred.SequenceFileInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 19
+              includeColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
+              dataColumns: _col0:bigint, _col1:boolean, _col2:string, _col3:double, _col4:double, _col5:bigint, _col6:bigint, _col7:bigint, _col8:double, _col9:double, _col10:double, _col11:double, _col12:double, _col13:decimal(22,2), _col14:bigint, _col15:double, _col17:double, _col18:double, _col19:double
+              partitionColumnCount: 0
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey1 (type: bigint), VALUE._col0 (type: boolean), KEY.reducesinkkey2 (type: string), null (type: timestamp), KEY.reducesinkkey0 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: bigint), VALUE._col3 (type: bigint), VALUE._col4 (type: bigint), VALUE._col5 (type: double), VALUE._col6 (type: double), VALUE._col7 (type: double), VALUE._col8 (type: double), VALUE._col9 (type: double), VALUE._col10 (type: decimal(22,2)), VALUE._col11 (type: bigint), VALUE._col12 (type: double), VALUE._col8 (type: double), VALUE._col13 (type: double), VALUE._col14 (type: double), VALUE._col15 (type: double)
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
+          Statistics: Num rows: 2503 Data size: 538153 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 2503 Data size: 538153 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT   cbigint,
          cboolean1,
          cstring1,
diff --git a/ql/src/test/results/clientpositive/vectorization_13.q.out b/ql/src/test/results/clientpositive/vectorization_13.q.out
index 35c704ef10..58529720dd 100644
--- a/ql/src/test/results/clientpositive/vectorization_13.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_13.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cboolean1,
          ctinyint,
          ctimestamp1,
@@ -31,7 +31,7 @@ GROUP BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1
 ORDER BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16
 LIMIT 40
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cboolean1,
          ctinyint,
          ctimestamp1,
@@ -103,11 +103,12 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFMinLong(col 0) -> tinyint
                       className: VectorGroupByOperator
-                      vectorOutput: false
+                      groupByMode: HASH
+                      vectorOutput: true
                       keyExpressions: col 10, col 0, col 8, col 4, col 6
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                      vectorOutputConditionsNotMet: Vector output of VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                   keys: cboolean1 (type: boolean), ctinyint (type: tinyint), ctimestamp1 (type: timestamp), cfloat (type: float), cstring1 (type: string)
                   mode: hash
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -116,17 +117,28 @@ STAGE PLANS:
                     key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
                     sort order: +++++
                     Map-reduce partition columns: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                     Statistics: Num rows: 2730 Data size: 586959 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col5 (type: tinyint), _col6 (type: double), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,variance:double>), _col9 (type: float), _col10 (type: tinyint)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 4, 5, 6, 8, 9, 10]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: double, decimal(11,4)
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -135,8 +147,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: max(VALUE._col0), sum(VALUE._col1), stddev_pop(VALUE._col2), stddev_pop(VALUE._col3), max(VALUE._col4), min(VALUE._col5)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: boolean), KEY._col1 (type: tinyint), KEY._col2 (type: timestamp), KEY._col3 (type: float), KEY._col4 (type: string)
           mode: mergepartial
@@ -179,6 +193,11 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 21
+              includeColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
+              dataColumns: _col0:boolean, _col1:tinyint, _col2:timestamp, _col3:float, _col4:string, _col5:tinyint, _col6:tinyint, _col7:tinyint, _col8:double, _col9:double, _col10:double, _col11:float, _col12:double, _col13:double, _col14:double, _col15:decimal(7,3), _col16:double, _col17:double, _col18:float, _col19:double, _col20:tinyint
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -418,11 +437,12 @@ STAGE PLANS:
                   Group By Vectorization:
                       aggregators: VectorUDAFMaxLong(col 0) -> tinyint, VectorUDAFSumDouble(col 4) -> double, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFMinLong(col 0) -> tinyint
                       className: VectorGroupByOperator
-                      vectorOutput: false
+                      groupByMode: HASH
+                      vectorOutput: true
                       keyExpressions: col 10, col 0, col 8, col 4, col 6
                       native: false
+                      vectorProcessingMode: HASH
                       projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                      vectorOutputConditionsNotMet: Vector output of VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                   keys: cboolean1 (type: boolean), ctinyint (type: tinyint), ctimestamp1 (type: timestamp), cfloat (type: float), cstring1 (type: string)
                   mode: hash
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -431,13 +451,18 @@ STAGE PLANS:
                     key expressions: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
                     sort order: +++++
                     Map-reduce partition columns: _col0 (type: boolean), _col1 (type: tinyint), _col2 (type: timestamp), _col3 (type: float), _col4 (type: string)
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                     Statistics: Num rows: 2730 Data size: 586959 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col5 (type: tinyint), _col6 (type: double), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: struct<count:bigint,sum:double,variance:double>), _col9 (type: float), _col10 (type: tinyint)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
@@ -450,8 +475,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: max(VALUE._col0), sum(VALUE._col1), stddev_pop(VALUE._col2), stddev_pop(VALUE._col3), max(VALUE._col4), min(VALUE._col5)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: boolean), KEY._col1 (type: tinyint), KEY._col2 (type: timestamp), KEY._col3 (type: float), KEY._col4 (type: string)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vectorization_14.q.out b/ql/src/test/results/clientpositive/vectorization_14.q.out
index ec4f7cd701..c6bd7cfec5 100644
--- a/ql/src/test/results/clientpositive/vectorization_14.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_14.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   ctimestamp1,
          cfloat,
          cstring1,
@@ -31,7 +31,7 @@ WHERE    (((ctinyint <= cbigint)
 GROUP BY ctimestamp1, cfloat, cstring1, cboolean1, cdouble
 ORDER BY cstring1, cfloat, cdouble, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   ctimestamp1,
          cfloat,
          cstring1,
@@ -80,15 +80,36 @@ STAGE PLANS:
           TableScan
             alias: alltypesorc
             Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
             Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprAndExpr(children: FilterLongColLessEqualLongColumn(col 0, col 3)(children: col 0) -> boolean, FilterExprOrExpr(children: FilterDoubleColLessEqualDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 2) -> 12:double) -> boolean, FilterTimestampColLessTimestampColumn(col 9, col 8) -> boolean) -> boolean, FilterDoubleColLessDoubleColumn(col 5, col 12)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterExprOrExpr(children: FilterLongColGreaterLongScalar(col 3, val -257) -> boolean, FilterDoubleColLessDoubleColumn(col 4, col 12)(children: CastLongToFloatViaLongToDouble(col 2) -> 12:double) -> boolean) -> boolean) -> boolean
               predicate: ((UDFToLong(ctinyint) <= cbigint) and ((UDFToDouble(cint) <= cdouble) or (ctimestamp2 < ctimestamp1)) and (cdouble < UDFToDouble(ctinyint)) and ((cbigint > -257) or (cfloat < UDFToFloat(cint)))) (type: boolean)
               Statistics: Num rows: 606 Data size: 130292 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: ctimestamp1 (type: timestamp), cfloat (type: float), cstring1 (type: string), cboolean1 (type: boolean), cdouble (type: double), (- (-26.28 + cdouble)) (type: double)
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [8, 4, 6, 10, 5, 13]
+                    selectExpressions: DoubleColUnaryMinus(col 12)(children: DoubleScalarAddDoubleColumn(val -26.28, col 5) -> 12:double) -> 13:double
                 Statistics: Num rows: 606 Data size: 130292 Basic stats: COMPLETE Column stats: NONE
                 Group By Operator
                   aggregations: stddev_samp(_col5), max(_col1), stddev_pop(_col1), count(_col1), var_pop(_col1), var_samp(_col1)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFStdSampDouble(col 13) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDouble(col 4) -> float, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCount(col 4) -> bigint, VectorUDAFVarPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      keyExpressions: col 6, col 4, col 5, col 8, col 10
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                   keys: _col2 (type: string), _col1 (type: float), _col4 (type: double), _col0 (type: timestamp), _col3 (type: boolean)
                   mode: hash
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -97,17 +118,28 @@ STAGE PLANS:
                     key expressions: _col0 (type: string), _col1 (type: float), _col2 (type: double), _col3 (type: timestamp), _col4 (type: boolean)
                     sort order: +++++
                     Map-reduce partition columns: _col0 (type: string), _col1 (type: float), _col2 (type: double), _col3 (type: timestamp), _col4 (type: boolean)
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                     Statistics: Num rows: 606 Data size: 130292 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: float), _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: bigint), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,variance:double>)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 2, 3, 4, 5, 6, 8, 9, 10]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: double, double
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -115,6 +147,12 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           aggregations: stddev_samp(VALUE._col0), max(VALUE._col1), stddev_pop(VALUE._col2), count(VALUE._col3), var_pop(VALUE._col4), var_samp(VALUE._col5)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
           keys: KEY._col0 (type: string), KEY._col1 (type: float), KEY._col2 (type: double), KEY._col3 (type: timestamp), KEY._col4 (type: boolean)
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
@@ -134,9 +172,17 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
             Reduce Output Operator
               key expressions: _col2 (type: string), _col1 (type: float), _col4 (type: double), _col0 (type: timestamp)
               sort order: ++++
+              Reduce Sink Vectorization:
+                  className: VectorReduceSinkOperator
+                  native: false
+                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
               Statistics: Num rows: 303 Data size: 65146 Basic stats: COMPLETE Column stats: NONE
               value expressions: _col3 (type: boolean), _col5 (type: double), _col6 (type: double), _col7 (type: double), _col8 (type: float), _col9 (type: float), _col10 (type: float), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: bigint), _col15 (type: double), _col16 (type: double), _col17 (type: double), _col18 (type: double), _col19 (type: double), _col20 (type: double), _col21 (type: double)
       Execution mode: vectorized
@@ -148,6 +194,11 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 22
+              includeColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
+              dataColumns: _col0:timestamp, _col1:float, _col2:string, _col3:boolean, _col4:double, _col5:double, _col6:double, _col7:double, _col8:float, _col9:float, _col10:float, _col11:float, _col12:double, _col13:double, _col14:bigint, _col15:double, _col16:double, _col17:double, _col18:double, _col19:double, _col20:double, _col21:double
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
diff --git a/ql/src/test/results/clientpositive/vectorization_15.q.out b/ql/src/test/results/clientpositive/vectorization_15.q.out
index 05b8b14baf..8f0a879a64 100644
--- a/ql/src/test/results/clientpositive/vectorization_15.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_15.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cboolean1,
          cdouble,
@@ -29,7 +29,7 @@ WHERE    (((cstring2 LIKE '%ss%')
 GROUP BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1
 ORDER BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cboolean1,
          cdouble,
@@ -76,15 +76,35 @@ STAGE PLANS:
           TableScan
             alias: alltypesorc
             Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
             Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprOrExpr(children: FilterStringColLikeStringScalar(col 7, pattern %ss%) -> boolean, FilterStringColLikeStringScalar(col 6, pattern 10%) -> boolean, FilterExprAndExpr(children: FilterLongColGreaterEqualLongScalar(col 2, val -75) -> boolean, FilterLongColEqualLongColumn(col 0, col 1)(children: col 0) -> boolean, FilterDoubleColGreaterEqualDoubleScalar(col 5, val -3728.0) -> boolean) -> boolean) -> boolean
               predicate: ((cstring2 like '%ss%') or (cstring1 like '10%') or ((cint >= -75) and (UDFToShort(ctinyint) = csmallint) and (cdouble >= -3728.0))) (type: boolean)
               Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: ctinyint (type: tinyint), cint (type: int), cfloat (type: float), cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp), cboolean1 (type: boolean)
                 outputColumnNames: ctinyint, cint, cfloat, cdouble, cstring1, ctimestamp1, cboolean1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0, 2, 4, 5, 6, 8, 10]
                 Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
                 Group By Operator
                   aggregations: stddev_samp(cfloat), min(cdouble), stddev_samp(ctinyint), var_pop(ctinyint), var_samp(cint), stddev_pop(cint)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFStdSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinDouble(col 5) -> double, VectorUDAFStdSampLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampLong(col 2) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      keyExpressions: col 4, col 10, col 5, col 6, col 0, col 2, col 8
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5]
                   keys: cfloat (type: float), cboolean1 (type: boolean), cdouble (type: double), cstring1 (type: string), ctinyint (type: tinyint), cint (type: int), ctimestamp1 (type: timestamp)
                   mode: hash
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
@@ -93,24 +113,39 @@ STAGE PLANS:
                     key expressions: _col0 (type: float), _col1 (type: boolean), _col2 (type: double), _col3 (type: string), _col4 (type: tinyint), _col5 (type: int), _col6 (type: timestamp)
                     sort order: +++++++
                     Map-reduce partition columns: _col0 (type: float), _col1 (type: boolean), _col2 (type: double), _col3 (type: string), _col4 (type: tinyint), _col5 (type: int), _col6 (type: timestamp)
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                     Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col7 (type: struct<count:bigint,sum:double,variance:double>), _col8 (type: double), _col9 (type: struct<count:bigint,sum:double,variance:double>), _col10 (type: struct<count:bigint,sum:double,variance:double>), _col11 (type: struct<count:bigint,sum:double,variance:double>), _col12 (type: struct<count:bigint,sum:double,variance:double>)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 2, 4, 5, 6, 7, 8, 10]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
-          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
-          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+          enableConditionsNotMet: hive.vectorized.execution.reduce.enabled IS false, hive.execution.engine mr IN [tez, spark] IS false
       Reduce Operator Tree:
         Group By Operator
           aggregations: stddev_samp(VALUE._col0), min(VALUE._col1), stddev_samp(VALUE._col2), var_pop(VALUE._col3), var_samp(VALUE._col4), stddev_pop(VALUE._col5)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
           keys: KEY._col0 (type: float), KEY._col1 (type: boolean), KEY._col2 (type: double), KEY._col3 (type: string), KEY._col4 (type: tinyint), KEY._col5 (type: int), KEY._col6 (type: timestamp)
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
@@ -130,9 +165,17 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
             Reduce Output Operator
               key expressions: _col0 (type: float), _col1 (type: boolean), _col2 (type: double), _col3 (type: string), _col4 (type: tinyint), _col5 (type: int), _col6 (type: timestamp)
               sort order: +++++++
+              Reduce Sink Vectorization:
+                  className: VectorReduceSinkOperator
+                  native: false
+                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
               Statistics: Num rows: 6144 Data size: 1320982 Basic stats: COMPLETE Column stats: NONE
               value expressions: _col7 (type: double), _col8 (type: decimal(13,2)), _col9 (type: double), _col10 (type: double), _col11 (type: float), _col12 (type: double), _col13 (type: double), _col14 (type: double), _col15 (type: tinyint), _col16 (type: double), _col17 (type: float), _col18 (type: int), _col19 (type: decimal(13,2)), _col20 (type: double)
       Execution mode: vectorized
@@ -144,10 +187,14 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 21
+              includeColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
+              dataColumns: _col0:float, _col1:boolean, _col2:double, _col3:string, _col4:tinyint, _col5:int, _col6:timestamp, _col7:double, _col8:decimal(13,2), _col9:double, _col10:double, _col11:float, _col12:double, _col13:double, _col14:double, _col15:tinyint, _col16:double, _col17:float, _col18:int, _col19:decimal(13,2), _col20:double
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
-          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
-          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+          enableConditionsNotMet: hive.vectorized.execution.reduce.enabled IS false, hive.execution.engine mr IN [tez, spark] IS false
       Reduce Operator Tree:
         Select Operator
           expressions: KEY.reducesinkkey0 (type: float), KEY.reducesinkkey1 (type: boolean), KEY.reducesinkkey2 (type: double), KEY.reducesinkkey3 (type: string), KEY.reducesinkkey4 (type: tinyint), KEY.reducesinkkey5 (type: int), KEY.reducesinkkey6 (type: timestamp), VALUE._col0 (type: double), VALUE._col1 (type: decimal(13,2)), VALUE._col2 (type: double), VALUE._col3 (type: double), VALUE._col4 (type: float), VALUE._col5 (type: double), VALUE._col6 (type: double), VALUE._col7 (type: double), VALUE._col8 (type: tinyint), VALUE._col9 (type: double), VALUE._col10 (type: float), VALUE._col11 (type: int), VALUE._col12 (type: decimal(13,2)), VALUE._col13 (type: double)
diff --git a/ql/src/test/results/clientpositive/vectorization_16.q.out b/ql/src/test/results/clientpositive/vectorization_16.q.out
index 2e3a34d907..930b476f06 100644
--- a/ql/src/test/results/clientpositive/vectorization_16.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_16.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -18,7 +18,7 @@ WHERE    ((cstring2 LIKE '%b%')
               OR (cstring1 < 'a')))
 GROUP BY cstring1, cdouble, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -53,15 +53,35 @@ STAGE PLANS:
           TableScan
             alias: alltypesorc
             Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
             Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprAndExpr(children: FilterStringColLikeStringScalar(col 7, pattern %b%) -> boolean, FilterExprOrExpr(children: FilterDoubleColGreaterEqualDoubleScalar(col 5, val -1.389) -> boolean, FilterStringGroupColLessStringScalar(col 6, val a) -> boolean) -> boolean) -> boolean
               predicate: ((cstring2 like '%b%') and ((cdouble >= -1.389) or (cstring1 < 'a'))) (type: boolean)
               Statistics: Num rows: 4096 Data size: 880654 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                 outputColumnNames: cdouble, cstring1, ctimestamp1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [5, 6, 8]
                 Statistics: Num rows: 4096 Data size: 880654 Basic stats: COMPLETE Column stats: NONE
                 Group By Operator
                   aggregations: count(cdouble), stddev_samp(cdouble), min(cdouble)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFCount(col 5) -> bigint, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinDouble(col 5) -> double
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      keyExpressions: col 5, col 6, col 8
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2]
                   keys: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                   mode: hash
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -70,17 +90,27 @@ STAGE PLANS:
                     key expressions: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
                     sort order: +++
                     Map-reduce partition columns: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                     Statistics: Num rows: 4096 Data size: 880654 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: double)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [5, 6, 7, 8]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -88,6 +118,12 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           aggregations: count(VALUE._col0), stddev_samp(VALUE._col1), min(VALUE._col2)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
           keys: KEY._col0 (type: double), KEY._col1 (type: string), KEY._col2 (type: timestamp)
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
diff --git a/ql/src/test/results/clientpositive/vectorization_17.q.out b/ql/src/test/results/clientpositive/vectorization_17.q.out
index 75907034af..c0186082df 100644
--- a/ql/src/test/results/clientpositive/vectorization_17.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_17.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cstring1,
          cint,
@@ -22,7 +22,7 @@ WHERE    (((cbigint > -23)
                   OR (cfloat = cdouble))))
 ORDER BY cbigint, cfloat
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cfloat,
          cstring1,
          cint,
@@ -61,16 +61,33 @@ STAGE PLANS:
           TableScan
             alias: alltypesorc
             Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
             Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprAndExpr(children: FilterLongColGreaterLongScalar(col 3, val -23) -> boolean, FilterExprOrExpr(children: FilterDoubleColNotEqualDoubleScalar(col 5, val 988888.0) -> boolean, FilterDecimalColGreaterDecimalScalar(col 12, val -863.257)(children: CastLongToDecimal(col 2) -> 12:decimal(13,3)) -> boolean) -> boolean, FilterExprOrExpr(children: FilterLongColGreaterEqualLongScalar(col 0, val 33) -> boolean, FilterLongColGreaterEqualLongColumn(col 1, col 3)(children: col 1) -> boolean, FilterDoubleColEqualDoubleColumn(col 4, col 5)(children: col 4) -> boolean) -> boolean) -> boolean
               predicate: ((cbigint > -23) and ((cdouble <> 988888.0) or (CAST( cint AS decimal(13,3)) > -863.257)) and ((ctinyint >= 33) or (UDFToLong(csmallint) >= cbigint) or (UDFToDouble(cfloat) = cdouble))) (type: boolean)
               Statistics: Num rows: 4778 Data size: 1027287 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: cfloat (type: float), cstring1 (type: string), cint (type: int), ctimestamp1 (type: timestamp), cdouble (type: double), cbigint (type: bigint), (UDFToDouble(cfloat) / UDFToDouble(ctinyint)) (type: double), (UDFToLong(cint) % cbigint) (type: bigint), (- cdouble) (type: double), (cdouble + (UDFToDouble(cfloat) / UDFToDouble(ctinyint))) (type: double), (cdouble / UDFToDouble(cint)) (type: double), (- (- cdouble)) (type: double), (9763215.5639 % CAST( cbigint AS decimal(19,0))) (type: decimal(11,4)), (2563.58 + (- (- cdouble))) (type: double)
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [4, 6, 2, 8, 5, 3, 14, 15, 13, 16, 18, 19, 21, 17]
+                    selectExpressions: DoubleColDivideDoubleColumn(col 4, col 13)(children: col 4, CastLongToDouble(col 0) -> 13:double) -> 14:double, LongColModuloLongColumn(col 2, col 3)(children: col 2) -> 15:long, DoubleColUnaryMinus(col 5) -> 13:double, DoubleColAddDoubleColumn(col 5, col 17)(children: DoubleColDivideDoubleColumn(col 4, col 16)(children: col 4, CastLongToDouble(col 0) -> 16:double) -> 17:double) -> 16:double, DoubleColDivideDoubleColumn(col 5, col 17)(children: CastLongToDouble(col 2) -> 17:double) -> 18:double, DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 5) -> 17:double) -> 19:double, DecimalScalarModuloDecimalColumn(val 9763215.5639, col 20)(children: CastLongToDecimal(col 3) -> 20:decimal(19,0)) -> 21:decimal(11,4), DoubleScalarAddDoubleColumn(val 2563.58, col 22)(children: DoubleColUnaryMinus(col 17)(children: DoubleColUnaryMinus(col 5) -> 17:double) -> 22:double) -> 17:double
                 Statistics: Num rows: 4778 Data size: 1027287 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   key expressions: _col5 (type: bigint), _col0 (type: float)
                   sort order: ++
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 4778 Data size: 1027287 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col1 (type: string), _col2 (type: int), _col3 (type: timestamp), _col4 (type: double), _col6 (type: double), _col7 (type: bigint), _col8 (type: double), _col9 (type: double), _col10 (type: double), _col11 (type: double), _col12 (type: decimal(11,4)), _col13 (type: double)
       Execution mode: vectorized
@@ -82,6 +99,12 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 2, 3, 4, 5, 6, 8]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: decimal(13,3), double, double, bigint, double, double, double, double, decimal(19,0), decimal(11,4), double
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
diff --git a/ql/src/test/results/clientpositive/vectorization_2.q.out b/ql/src/test/results/clientpositive/vectorization_2.q.out
index 709a75f2ab..47ff8d48ba 100644
--- a/ql/src/test/results/clientpositive/vectorization_2.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_2.q.out
@@ -1,3 +1,153 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(csmallint),
+       (AVG(csmallint) % -563),
+       (AVG(csmallint) + 762),
+       SUM(cfloat),
+       VAR_POP(cbigint),
+       (-(VAR_POP(cbigint))),
+       (SUM(cfloat) - AVG(csmallint)),
+       COUNT(*),
+       (-((SUM(cfloat) - AVG(csmallint)))),
+       (VAR_POP(cbigint) - 762),
+       MIN(ctinyint),
+       ((-(VAR_POP(cbigint))) + MIN(ctinyint)),
+       AVG(cdouble),
+       (((-(VAR_POP(cbigint))) + MIN(ctinyint)) - SUM(cfloat))
+FROM   alltypesorc
+WHERE  (((ctimestamp1 < ctimestamp2)
+         AND ((cstring2 LIKE 'b%')
+              AND (cfloat <= -5638.15)))
+        OR ((cdouble < ctinyint)
+            AND ((-10669 != ctimestamp2)
+                 OR (359 > cint))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(csmallint),
+       (AVG(csmallint) % -563),
+       (AVG(csmallint) + 762),
+       SUM(cfloat),
+       VAR_POP(cbigint),
+       (-(VAR_POP(cbigint))),
+       (SUM(cfloat) - AVG(csmallint)),
+       COUNT(*),
+       (-((SUM(cfloat) - AVG(csmallint)))),
+       (VAR_POP(cbigint) - 762),
+       MIN(ctinyint),
+       ((-(VAR_POP(cbigint))) + MIN(ctinyint)),
+       AVG(cdouble),
+       (((-(VAR_POP(cbigint))) + MIN(ctinyint)) - SUM(cfloat))
+FROM   alltypesorc
+WHERE  (((ctimestamp1 < ctimestamp2)
+         AND ((cstring2 LIKE 'b%')
+              AND (cfloat <= -5638.15)))
+        OR ((cdouble < ctinyint)
+            AND ((-10669 != ctimestamp2)
+                 OR (359 > cint))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: alltypesorc
+            Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: FilterTimestampColLessTimestampColumn(col 8, col 9) -> boolean, FilterStringColLikeStringScalar(col 7, pattern b%) -> boolean, FilterDoubleColLessEqualDoubleScalar(col 4, val -5638.14990234375) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColLessDoubleColumn(col 5, col 12)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, FilterExprOrExpr(children: FilterDoubleScalarNotEqualDoubleColumn(val -10669.0, col 12)(children: CastTimestampToDouble(col 9) -> 12:double) -> boolean, FilterLongScalarGreaterLongColumn(val 359, col 2) -> boolean) -> boolean) -> boolean) -> boolean
+              predicate: (((ctimestamp1 < ctimestamp2) and (cstring2 like 'b%') and (cfloat <= -5638.15)) or ((cdouble < UDFToDouble(ctinyint)) and ((-10669.0 <> UDFToDouble(ctimestamp2)) or (359 > cint)))) (type: boolean)
+              Statistics: Num rows: 4778 Data size: 1027287 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ctinyint (type: tinyint), csmallint (type: smallint), cbigint (type: bigint), cfloat (type: float), cdouble (type: double)
+                outputColumnNames: ctinyint, csmallint, cbigint, cfloat, cdouble
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0, 1, 3, 4, 5]
+                Statistics: Num rows: 4778 Data size: 1027287 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: avg(csmallint), sum(cfloat), var_pop(cbigint), count(), min(ctinyint), avg(cdouble)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFAvgLong(col 1) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFSumDouble(col 4) -> double, VectorUDAFVarPopLong(col 3) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 0) -> tinyint, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                  Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    sort order: 
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                    Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: struct<count:bigint,sum:double,input:smallint>), _col1 (type: double), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: bigint), _col4 (type: tinyint), _col5 (type: struct<count:bigint,sum:double,input:double>)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 2, 3, 4, 5, 7, 8, 9]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: double
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: avg(VALUE._col0), sum(VALUE._col1), var_pop(VALUE._col2), count(VALUE._col3), min(VALUE._col4), avg(VALUE._col5)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+          Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col0 (type: double), (_col0 % -563.0) (type: double), (_col0 + 762.0) (type: double), _col1 (type: double), _col2 (type: double), (- _col2) (type: double), (_col1 - _col0) (type: double), _col3 (type: bigint), (- (_col1 - _col0)) (type: double), (_col2 - 762.0) (type: double), _col4 (type: tinyint), ((- _col2) + UDFToDouble(_col4)) (type: double), _col5 (type: double), (((- _col2) + UDFToDouble(_col4)) - _col1) (type: double)
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+            Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 1 Data size: 256 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT AVG(csmallint),
        (AVG(csmallint) % -563),
        (AVG(csmallint) + 762),
diff --git a/ql/src/test/results/clientpositive/vectorization_3.q.out b/ql/src/test/results/clientpositive/vectorization_3.q.out
index 2398dee7bc..a730ca6f56 100644
--- a/ql/src/test/results/clientpositive/vectorization_3.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_3.q.out
@@ -1,3 +1,158 @@
+WARNING: Comparing a bigint and a double may result in a loss of precision.
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT STDDEV_SAMP(csmallint),
+       (STDDEV_SAMP(csmallint) - 10.175),
+       STDDEV_POP(ctinyint),
+       (STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)),
+       (-(STDDEV_POP(ctinyint))),
+       (STDDEV_SAMP(csmallint) % 79.553),
+       (-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))),
+       STDDEV_SAMP(cfloat),
+       (-(STDDEV_SAMP(csmallint))),
+       SUM(cfloat),
+       ((-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))) / (STDDEV_SAMP(csmallint) - 10.175)),
+       (-((STDDEV_SAMP(csmallint) - 10.175))),
+       AVG(cint),
+       (-3728 - STDDEV_SAMP(csmallint)),
+       STDDEV_POP(cint),
+       (AVG(cint) / STDDEV_SAMP(cfloat))
+FROM   alltypesorc
+WHERE  (((cint <= cfloat)
+         AND ((79.553 != cbigint)
+              AND (ctimestamp2 = -29071)))
+        OR ((cbigint > cdouble)
+            AND ((79.553 <= csmallint)
+                 AND (ctimestamp1 > ctimestamp2))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT STDDEV_SAMP(csmallint),
+       (STDDEV_SAMP(csmallint) - 10.175),
+       STDDEV_POP(ctinyint),
+       (STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)),
+       (-(STDDEV_POP(ctinyint))),
+       (STDDEV_SAMP(csmallint) % 79.553),
+       (-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))),
+       STDDEV_SAMP(cfloat),
+       (-(STDDEV_SAMP(csmallint))),
+       SUM(cfloat),
+       ((-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))) / (STDDEV_SAMP(csmallint) - 10.175)),
+       (-((STDDEV_SAMP(csmallint) - 10.175))),
+       AVG(cint),
+       (-3728 - STDDEV_SAMP(csmallint)),
+       STDDEV_POP(cint),
+       (AVG(cint) / STDDEV_SAMP(cfloat))
+FROM   alltypesorc
+WHERE  (((cint <= cfloat)
+         AND ((79.553 != cbigint)
+              AND (ctimestamp2 = -29071)))
+        OR ((cbigint > cdouble)
+            AND ((79.553 <= csmallint)
+                 AND (ctimestamp1 > ctimestamp2))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: alltypesorc
+            Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: FilterDoubleColLessEqualDoubleColumn(col 12, col 4)(children: CastLongToFloatViaLongToDouble(col 2) -> 12:double) -> boolean, FilterDecimalScalarNotEqualDecimalColumn(val 79.553, col 13)(children: CastLongToDecimal(col 3) -> 13:decimal(22,3)) -> boolean, FilterDoubleColEqualDoubleScalar(col 12, val -29071.0)(children: CastTimestampToDouble(col 9) -> 12:double) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColGreaterDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 3) -> 12:double) -> boolean, FilterDecimalScalarLessEqualDecimalColumn(val 79.553, col 14)(children: CastLongToDecimal(col 1) -> 14:decimal(8,3)) -> boolean, FilterTimestampColGreaterTimestampColumn(col 8, col 9) -> boolean) -> boolean) -> boolean
+              predicate: (((UDFToFloat(cint) <= cfloat) and (79.553 <> CAST( cbigint AS decimal(22,3))) and (UDFToDouble(ctimestamp2) = -29071.0)) or ((UDFToDouble(cbigint) > cdouble) and (79.553 <= CAST( csmallint AS decimal(8,3))) and (ctimestamp1 > ctimestamp2))) (type: boolean)
+              Statistics: Num rows: 2503 Data size: 538153 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ctinyint (type: tinyint), csmallint (type: smallint), cint (type: int), cfloat (type: float)
+                outputColumnNames: ctinyint, csmallint, cint, cfloat
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0, 1, 2, 4]
+                Statistics: Num rows: 2503 Data size: 538153 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: stddev_samp(csmallint), stddev_pop(ctinyint), stddev_samp(cfloat), sum(cfloat), avg(cint), stddev_pop(cint)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFStdSampLong(col 1) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopLong(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFSumDouble(col 4) -> double, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2, 3, 4, 5]
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                  Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    sort order: 
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                    Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: double), _col4 (type: struct<count:bigint,sum:double,input:int>), _col5 (type: struct<count:bigint,sum:double,variance:double>)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 2, 3, 4, 5, 8, 9]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: double, decimal(22,3), decimal(8,3)
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: stddev_samp(VALUE._col0), stddev_pop(VALUE._col1), stddev_samp(VALUE._col2), sum(VALUE._col3), avg(VALUE._col4), stddev_pop(VALUE._col5)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+          Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col0 (type: double), (_col0 - 10.175) (type: double), _col1 (type: double), (_col0 * (_col0 - 10.175)) (type: double), (- _col1) (type: double), (_col0 % 79.553) (type: double), (- (_col0 * (_col0 - 10.175))) (type: double), _col2 (type: double), (- _col0) (type: double), _col3 (type: double), ((- (_col0 * (_col0 - 10.175))) / (_col0 - 10.175)) (type: double), (- (_col0 - 10.175)) (type: double), _col4 (type: double), (-3728.0 - _col0) (type: double), _col5 (type: double), (_col4 / _col2) (type: double)
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
+            Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 WARNING: Comparing a bigint and a double may result in a loss of precision.
 PREHOOK: query: SELECT STDDEV_SAMP(csmallint),
        (STDDEV_SAMP(csmallint) - 10.175),
diff --git a/ql/src/test/results/clientpositive/vectorization_4.q.out b/ql/src/test/results/clientpositive/vectorization_4.q.out
index 0d6829f6d8..0199d7db16 100644
--- a/ql/src/test/results/clientpositive/vectorization_4.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_4.q.out
@@ -1,3 +1,152 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT SUM(cint),
+       (SUM(cint) * -563),
+       (-3728 + SUM(cint)),
+       STDDEV_POP(cdouble),
+       (-(STDDEV_POP(cdouble))),
+       AVG(cdouble),
+       ((SUM(cint) * -563) % SUM(cint)),
+       (((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)),
+       VAR_POP(cdouble),
+       (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))),
+       ((-3728 + SUM(cint)) - (SUM(cint) * -563)),
+       MIN(ctinyint),
+       MIN(ctinyint),
+       (MIN(ctinyint) * (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))))
+FROM   alltypesorc
+WHERE  (((csmallint >= cint)
+         OR ((-89010 >= ctinyint)
+             AND (cdouble > 79.553)))
+        OR ((-563 != cbigint)
+            AND ((ctinyint != cbigint)
+                 OR (-3728 >= cdouble))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT SUM(cint),
+       (SUM(cint) * -563),
+       (-3728 + SUM(cint)),
+       STDDEV_POP(cdouble),
+       (-(STDDEV_POP(cdouble))),
+       AVG(cdouble),
+       ((SUM(cint) * -563) % SUM(cint)),
+       (((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)),
+       VAR_POP(cdouble),
+       (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))),
+       ((-3728 + SUM(cint)) - (SUM(cint) * -563)),
+       MIN(ctinyint),
+       MIN(ctinyint),
+       (MIN(ctinyint) * (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))))
+FROM   alltypesorc
+WHERE  (((csmallint >= cint)
+         OR ((-89010 >= ctinyint)
+             AND (cdouble > 79.553)))
+        OR ((-563 != cbigint)
+            AND ((ctinyint != cbigint)
+                 OR (-3728 >= cdouble))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: alltypesorc
+            Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprOrExpr(children: FilterLongColGreaterEqualLongColumn(col 1, col 2)(children: col 1) -> boolean, FilterExprAndExpr(children: FilterLongScalarGreaterEqualLongColumn(val -89010, col 0)(children: col 0) -> boolean, FilterDoubleColGreaterDoubleScalar(col 5, val 79.553) -> boolean) -> boolean, FilterExprAndExpr(children: FilterLongScalarNotEqualLongColumn(val -563, col 3) -> boolean, FilterExprOrExpr(children: FilterLongColNotEqualLongColumn(col 0, col 3)(children: col 0) -> boolean, FilterDoubleScalarGreaterEqualDoubleColumn(val -3728.0, col 5) -> boolean) -> boolean) -> boolean) -> boolean
+              predicate: ((UDFToInteger(csmallint) >= cint) or ((-89010 >= UDFToInteger(ctinyint)) and (cdouble > 79.553)) or ((-563 <> cbigint) and ((UDFToLong(ctinyint) <> cbigint) or (-3728.0 >= cdouble)))) (type: boolean)
+              Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ctinyint (type: tinyint), cint (type: int), cdouble (type: double)
+                outputColumnNames: ctinyint, cint, cdouble
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0, 2, 5]
+                Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: sum(cint), stddev_pop(cdouble), avg(cdouble), var_pop(cdouble), min(ctinyint)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFSumLong(col 2) -> bigint, VectorUDAFStdPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>, VectorUDAFVarPopDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinLong(col 0) -> tinyint
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                  Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    sort order: 
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                    Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: bigint), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,input:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: tinyint)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 2, 3, 5]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: sum(VALUE._col0), stddev_pop(VALUE._col1), avg(VALUE._col2), var_pop(VALUE._col3), min(VALUE._col4)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4
+          Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col0 (type: bigint), (_col0 * -563) (type: bigint), (-3728 + _col0) (type: bigint), _col1 (type: double), (- _col1) (type: double), _col2 (type: double), ((_col0 * -563) % _col0) (type: bigint), (UDFToDouble(((_col0 * -563) % _col0)) / _col2) (type: double), _col3 (type: double), (- (UDFToDouble(((_col0 * -563) % _col0)) / _col2)) (type: double), ((-3728 + _col0) - (_col0 * -563)) (type: bigint), _col4 (type: tinyint), _col4 (type: tinyint), (UDFToDouble(_col4) * (- (UDFToDouble(((_col0 * -563) % _col0)) / _col2))) (type: double)
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+            Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 1 Data size: 252 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT SUM(cint),
        (SUM(cint) * -563),
        (-3728 + SUM(cint)),
diff --git a/ql/src/test/results/clientpositive/vectorization_5.q.out b/ql/src/test/results/clientpositive/vectorization_5.q.out
index 914a626872..33707c7006 100644
--- a/ql/src/test/results/clientpositive/vectorization_5.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_5.q.out
@@ -1,3 +1,147 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT MAX(csmallint),
+       (MAX(csmallint) * -75),
+       COUNT(*),
+       ((MAX(csmallint) * -75) / COUNT(*)),
+       (6981 * MAX(csmallint)),
+       MIN(csmallint),
+       (-(MIN(csmallint))),
+       (197 % ((MAX(csmallint) * -75) / COUNT(*))),
+       SUM(cint),
+       MAX(ctinyint),
+       (-(MAX(ctinyint))),
+       ((-(MAX(ctinyint))) + MAX(ctinyint))
+FROM   alltypesorc
+WHERE  (((cboolean2 IS NOT NULL)
+         AND (cstring1 LIKE '%b%'))
+        OR ((ctinyint = cdouble)
+            AND ((ctimestamp2 IS NOT NULL)
+                 AND (cstring2 LIKE 'a'))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT MAX(csmallint),
+       (MAX(csmallint) * -75),
+       COUNT(*),
+       ((MAX(csmallint) * -75) / COUNT(*)),
+       (6981 * MAX(csmallint)),
+       MIN(csmallint),
+       (-(MIN(csmallint))),
+       (197 % ((MAX(csmallint) * -75) / COUNT(*))),
+       SUM(cint),
+       MAX(ctinyint),
+       (-(MAX(ctinyint))),
+       ((-(MAX(ctinyint))) + MAX(ctinyint))
+FROM   alltypesorc
+WHERE  (((cboolean2 IS NOT NULL)
+         AND (cstring1 LIKE '%b%'))
+        OR ((ctinyint = cdouble)
+            AND ((ctimestamp2 IS NOT NULL)
+                 AND (cstring2 LIKE 'a'))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: alltypesorc
+            Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprOrExpr(children: FilterExprAndExpr(children: SelectColumnIsNotNull(col 11) -> boolean, FilterStringColLikeStringScalar(col 6, pattern %b%) -> boolean) -> boolean, FilterExprAndExpr(children: FilterDoubleColEqualDoubleColumn(col 12, col 5)(children: CastLongToDouble(col 0) -> 12:double) -> boolean, SelectColumnIsNotNull(col 9) -> boolean, FilterStringColLikeStringScalar(col 7, pattern a) -> boolean) -> boolean) -> boolean
+              predicate: ((cboolean2 is not null and (cstring1 like '%b%')) or ((UDFToDouble(ctinyint) = cdouble) and ctimestamp2 is not null and (cstring2 like 'a'))) (type: boolean)
+              Statistics: Num rows: 9216 Data size: 1981473 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ctinyint (type: tinyint), csmallint (type: smallint), cint (type: int)
+                outputColumnNames: ctinyint, csmallint, cint
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0, 1, 2]
+                Statistics: Num rows: 9216 Data size: 1981473 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  aggregations: max(csmallint), count(), min(csmallint), sum(cint), max(ctinyint)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFMaxLong(col 1) -> smallint, VectorUDAFCountStar(*) -> bigint, VectorUDAFMinLong(col 1) -> smallint, VectorUDAFSumLong(col 2) -> bigint, VectorUDAFMaxLong(col 0) -> tinyint
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2, 3, 4]
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                  Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    sort order: 
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                    Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: smallint), _col1 (type: bigint), _col2 (type: smallint), _col3 (type: bigint), _col4 (type: tinyint)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 2, 5, 6, 7, 9, 11]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: double
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: max(VALUE._col0), count(VALUE._col1), min(VALUE._col2), sum(VALUE._col3), max(VALUE._col4)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4
+          Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col0 (type: smallint), (UDFToInteger(_col0) * -75) (type: int), _col1 (type: bigint), (UDFToDouble((UDFToInteger(_col0) * -75)) / UDFToDouble(_col1)) (type: double), (6981 * UDFToInteger(_col0)) (type: int), _col2 (type: smallint), (- _col2) (type: smallint), (197.0 % (UDFToDouble((UDFToInteger(_col0) * -75)) / UDFToDouble(_col1))) (type: double), _col3 (type: bigint), _col4 (type: tinyint), (- _col4) (type: tinyint), ((- _col4) + _col4) (type: tinyint)
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
+            Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT MAX(csmallint),
        (MAX(csmallint) * -75),
        COUNT(*),
diff --git a/ql/src/test/results/clientpositive/vectorization_6.q.out b/ql/src/test/results/clientpositive/vectorization_6.q.out
index 13897f6f93..9f3da46212 100644
--- a/ql/src/test/results/clientpositive/vectorization_6.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_6.q.out
@@ -1,3 +1,111 @@
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cboolean1,
+       cfloat,
+       cstring1,
+       (988888 * csmallint),
+       (-(csmallint)),
+       (-(cfloat)),
+       (-26.28 / cfloat),
+       (cfloat * 359),
+       (cint % ctinyint),
+       (-(cdouble)),
+       (ctinyint - -75),
+       (762 * (cint % ctinyint))
+FROM   alltypesorc
+WHERE  ((ctinyint != 0)
+        AND ((((cboolean1 <= 0)
+          AND (cboolean2 >= cboolean1))
+          OR ((cbigint IS NOT NULL)
+              AND ((cstring2 LIKE '%a')
+                   OR (cfloat <= -257))))))
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT cboolean1,
+       cfloat,
+       cstring1,
+       (988888 * csmallint),
+       (-(csmallint)),
+       (-(cfloat)),
+       (-26.28 / cfloat),
+       (cfloat * 359),
+       (cint % ctinyint),
+       (-(cdouble)),
+       (ctinyint - -75),
+       (762 * (cint % ctinyint))
+FROM   alltypesorc
+WHERE  ((ctinyint != 0)
+        AND ((((cboolean1 <= 0)
+          AND (cboolean2 >= cboolean1))
+          OR ((cbigint IS NOT NULL)
+              AND ((cstring2 LIKE '%a')
+                   OR (cfloat <= -257))))))
+POSTHOOK: type: QUERY
+PLAN VECTORIZATION:
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: alltypesorc
+            Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
+            Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprAndExpr(children: FilterLongColNotEqualLongScalar(col 0, val 0) -> boolean, FilterExprOrExpr(children: FilterExprAndExpr(children: FilterLongColLessEqualLongScalar(col 10, val 0) -> boolean, FilterLongColGreaterEqualLongColumn(col 11, col 10) -> boolean) -> boolean, FilterExprAndExpr(children: SelectColumnIsNotNull(col 3) -> boolean, FilterExprOrExpr(children: FilterStringColLikeStringScalar(col 7, pattern %a) -> boolean, FilterDoubleColLessEqualDoubleScalar(col 4, val -257.0) -> boolean) -> boolean) -> boolean) -> boolean) -> boolean
+              predicate: ((ctinyint <> 0) and (((cboolean1 <= 0) and (cboolean2 >= cboolean1)) or (cbigint is not null and ((cstring2 like '%a') or (cfloat <= -257))))) (type: boolean)
+              Statistics: Num rows: 11605 Data size: 2495116 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: cboolean1 (type: boolean), cfloat (type: float), cstring1 (type: string), (988888 * UDFToInteger(csmallint)) (type: int), (- csmallint) (type: smallint), (- cfloat) (type: float), (-26.28 / UDFToDouble(cfloat)) (type: double), (cfloat * 359.0) (type: float), (cint % UDFToInteger(ctinyint)) (type: int), (- cdouble) (type: double), (UDFToInteger(ctinyint) - -75) (type: int), (762 * (cint % UDFToInteger(ctinyint))) (type: int)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [10, 4, 6, 12, 13, 14, 15, 16, 17, 18, 19, 21]
+                    selectExpressions: LongScalarMultiplyLongColumn(val 988888, col 1)(children: col 1) -> 12:long, LongColUnaryMinus(col 1) -> 13:long, DoubleColUnaryMinus(col 4) -> 14:double, DoubleScalarDivideDoubleColumn(val -26.28, col 4)(children: col 4) -> 15:double, DoubleColMultiplyDoubleScalar(col 4, val 359.0) -> 16:double, LongColModuloLongColumn(col 2, col 0)(children: col 0) -> 17:long, DoubleColUnaryMinus(col 5) -> 18:double, LongColSubtractLongScalar(col 0, val -75)(children: col 0) -> 19:long, LongScalarMultiplyLongColumn(val 762, col 20)(children: LongColModuloLongColumn(col 2, col 0)(children: col 0) -> 20:long) -> 21:long
+                Statistics: Num rows: 11605 Data size: 2495116 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
+                  Statistics: Num rows: 11605 Data size: 2495116 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 2, 3, 4, 5, 6, 7, 10, 11]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: bigint, bigint, double, double, double, bigint, double, bigint, bigint, bigint
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
 PREHOOK: query: SELECT cboolean1,
        cfloat,
        cstring1,
diff --git a/ql/src/test/results/clientpositive/vectorization_7.q.out b/ql/src/test/results/clientpositive/vectorization_7.q.out
index c05fee0e96..218d307915 100644
--- a/ql/src/test/results/clientpositive/vectorization_7.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_7.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT cboolean1,
        cbigint,
        csmallint,
@@ -25,7 +25,7 @@ WHERE  ((ctinyint != 0)
 ORDER BY cboolean1, cbigint, csmallint, ctinyint, ctimestamp1, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9
 LIMIT 25
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT cboolean1,
        cbigint,
        csmallint,
@@ -105,6 +105,12 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: double, bigint, bigint, bigint, bigint, bigint, bigint, bigint, bigint, bigint, bigint
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
diff --git a/ql/src/test/results/clientpositive/vectorization_8.q.out b/ql/src/test/results/clientpositive/vectorization_8.q.out
index ce2a4b55e6..e56fb53fb9 100644
--- a/ql/src/test/results/clientpositive/vectorization_8.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_8.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ctimestamp1,
        cdouble,
        cboolean1,
@@ -23,7 +23,7 @@ WHERE  (((cstring2 IS NOT NULL)
 ORDER BY ctimestamp1, cdouble, cboolean1, cstring1, cfloat, c1, c2, c3, c4, c5, c6, c7, c8, c9
 LIMIT 20
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ctimestamp1,
        cdouble,
        cboolean1,
@@ -101,6 +101,12 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [2, 3, 4, 5, 6, 7, 8, 9, 10]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
+              scratchColumnTypeNames: double, double, double, double, double, double, double, double, double, double, double
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
diff --git a/ql/src/test/results/clientpositive/vectorization_9.q.out b/ql/src/test/results/clientpositive/vectorization_9.q.out
index 2e3a34d907..930b476f06 100644
--- a/ql/src/test/results/clientpositive/vectorization_9.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_9.q.out
@@ -1,4 +1,4 @@
-PREHOOK: query: EXPLAIN VECTORIZATION 
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -18,7 +18,7 @@ WHERE    ((cstring2 LIKE '%b%')
               OR (cstring1 < 'a')))
 GROUP BY cstring1, cdouble, ctimestamp1
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION 
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT   cstring1,
          cdouble,
          ctimestamp1,
@@ -53,15 +53,35 @@ STAGE PLANS:
           TableScan
             alias: alltypesorc
             Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
             Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterExprAndExpr(children: FilterStringColLikeStringScalar(col 7, pattern %b%) -> boolean, FilterExprOrExpr(children: FilterDoubleColGreaterEqualDoubleScalar(col 5, val -1.389) -> boolean, FilterStringGroupColLessStringScalar(col 6, val a) -> boolean) -> boolean) -> boolean
               predicate: ((cstring2 like '%b%') and ((cdouble >= -1.389) or (cstring1 < 'a'))) (type: boolean)
               Statistics: Num rows: 4096 Data size: 880654 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                 outputColumnNames: cdouble, cstring1, ctimestamp1
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [5, 6, 8]
                 Statistics: Num rows: 4096 Data size: 880654 Basic stats: COMPLETE Column stats: NONE
                 Group By Operator
                   aggregations: count(cdouble), stddev_samp(cdouble), min(cdouble)
+                  Group By Vectorization:
+                      aggregators: VectorUDAFCount(col 5) -> bigint, VectorUDAFStdSampDouble(col 5) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMinDouble(col 5) -> double
+                      className: VectorGroupByOperator
+                      groupByMode: HASH
+                      vectorOutput: true
+                      keyExpressions: col 5, col 6, col 8
+                      native: false
+                      vectorProcessingMode: HASH
+                      projectedOutputColumns: [0, 1, 2]
                   keys: cdouble (type: double), cstring1 (type: string), ctimestamp1 (type: timestamp)
                   mode: hash
                   outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
@@ -70,17 +90,27 @@ STAGE PLANS:
                     key expressions: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
                     sort order: +++
                     Map-reduce partition columns: _col0 (type: double), _col1 (type: string), _col2 (type: timestamp)
+                    Reduce Sink Vectorization:
+                        className: VectorReduceSinkOperator
+                        native: false
+                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                        nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                     Statistics: Num rows: 4096 Data size: 880654 Basic stats: COMPLETE Column stats: NONE
                     value expressions: _col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: double)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [5, 6, 7, 8]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -88,6 +118,12 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           aggregations: count(VALUE._col0), stddev_samp(VALUE._col1), min(VALUE._col2)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
           keys: KEY._col0 (type: double), KEY._col1 (type: string), KEY._col2 (type: timestamp)
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
diff --git a/ql/src/test/results/clientpositive/vectorization_limit.q.out b/ql/src/test/results/clientpositive/vectorization_limit.q.out
index 738129458c..b46e6ef689 100644
--- a/ql/src/test/results/clientpositive/vectorization_limit.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_limit.q.out
@@ -221,13 +221,14 @@ STAGE PLANS:
               Group By Operator
                 aggregations: avg(_col1)
                 Group By Vectorization:
-                    aggregators: VectorUDAFAvgDouble(col 12) -> struct<count:bigint,sum:double>
+                    aggregators: VectorUDAFAvgDouble(col 12) -> struct<count:bigint,sum:double,input:double>
                     className: VectorGroupByOperator
-                    vectorOutput: false
+                    groupByMode: HASH
+                    vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0]
-                    vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDouble(col 12) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                 keys: _col0 (type: tinyint)
                 mode: hash
                 outputColumnNames: _col0, _col1
@@ -236,6 +237,11 @@ STAGE PLANS:
                   key expressions: _col0 (type: tinyint)
                   sort order: +
                   Map-reduce partition columns: _col0 (type: tinyint)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
                   TopN Hash Memory Usage: 0.3
                   value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>)
@@ -243,7 +249,7 @@ STAGE PLANS:
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
@@ -262,8 +268,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: avg(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: tinyint)
           mode: mergepartial
@@ -349,9 +357,11 @@ STAGE PLANS:
               Group By Operator
                 Group By Vectorization:
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 0
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: []
                 keys: ctinyint (type: tinyint)
                 mode: hash
@@ -389,8 +399,10 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: tinyint)
           mode: mergepartial
@@ -478,9 +490,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFCount(col 5) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 0, col 5
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0]
                 keys: ctinyint (type: tinyint), cdouble (type: double)
                 mode: hash
@@ -519,8 +533,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(DISTINCT KEY._col1:0._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: tinyint)
           mode: mergepartial
@@ -636,9 +652,11 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 0) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     keyExpressions: col 5
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0]
                 keys: cdouble (type: double)
                 mode: hash
@@ -677,8 +695,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: sum(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: double)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vectorization_pushdown.q.out b/ql/src/test/results/clientpositive/vectorization_pushdown.q.out
index 664a9baeed..183cbdc7a4 100644
--- a/ql/src/test/results/clientpositive/vectorization_pushdown.q.out
+++ b/ql/src/test/results/clientpositive/vectorization_pushdown.q.out
@@ -39,7 +39,7 @@ STAGE PLANS:
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
diff --git a/ql/src/test/results/clientpositive/vectorized_case.q.out b/ql/src/test/results/clientpositive/vectorized_case.q.out
index b1b5e54fa3..0a2e803670 100644
--- a/ql/src/test/results/clientpositive/vectorized_case.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_case.q.out
@@ -276,8 +276,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 12) -> bigint, VectorUDAFSumLong(col 13) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1]
                 mode: hash
                 outputColumnNames: _col0, _col1
@@ -308,8 +310,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: sum(VALUE._col0), sum(VALUE._col1)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1
@@ -387,8 +391,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFSumLong(col 12) -> bigint, VectorUDAFSumLong(col 13) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1]
                 mode: hash
                 outputColumnNames: _col0, _col1
@@ -419,8 +425,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: sum(VALUE._col0), sum(VALUE._col1)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1
diff --git a/ql/src/test/results/clientpositive/vectorized_date_funcs.q.out b/ql/src/test/results/clientpositive/vectorized_date_funcs.q.out
index 4248d0893d..b7ac3f9f65 100644
--- a/ql/src/test/results/clientpositive/vectorized_date_funcs.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_date_funcs.q.out
@@ -1239,8 +1239,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinLong(col 0) -> date, VectorUDAFMaxLong(col 0) -> date, VectorUDAFCount(col 0) -> bigint, VectorUDAFCountStar(*) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -1271,8 +1273,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3
diff --git a/ql/src/test/results/clientpositive/vectorized_distinct_gby.q.out b/ql/src/test/results/clientpositive/vectorized_distinct_gby.q.out
index 81292ec023..1fe1c69b12 100644
--- a/ql/src/test/results/clientpositive/vectorized_distinct_gby.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_distinct_gby.q.out
@@ -16,9 +16,11 @@ POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dtest
 POSTHOOK: Lineage: dtest.a SCRIPT []
 POSTHOOK: Lineage: dtest.b SIMPLE []
-PREHOOK: query: explain vectorization select sum(distinct a), count(distinct a) from dtest
+PREHOOK: query: explain vectorization detail
+select sum(distinct a), count(distinct a) from dtest
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization select sum(distinct a), count(distinct a) from dtest
+POSTHOOK: query: explain vectorization detail
+select sum(distinct a), count(distinct a) from dtest
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
   enabled: true
@@ -35,13 +37,29 @@ STAGE PLANS:
           TableScan
             alias: dtest
             Statistics: Num rows: 5 Data size: 40 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1]
             Select Operator
               expressions: a (type: int)
               outputColumnNames: a
+              Select Vectorization:
+                  className: VectorSelectOperator
+                  native: true
+                  projectedOutputColumns: [0]
               Statistics: Num rows: 5 Data size: 40 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
                 aggregations: sum(DISTINCT a), count(DISTINCT a)
                 bucketGroup: true
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 0) -> bigint, VectorUDAFCount(col 0) -> bigint
+                    className: VectorGroupByOperator
+                    groupByMode: HASH
+                    vectorOutput: true
+                    keyExpressions: col 0
+                    native: false
+                    vectorProcessingMode: HASH
+                    projectedOutputColumns: [0, 1]
                 keys: a (type: int)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
@@ -49,6 +67,11 @@ STAGE PLANS:
                 Reduce Output Operator
                   key expressions: _col0 (type: int)
                   sort order: +
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false, No DISTINCT columns IS false
                   Statistics: Num rows: 5 Data size: 40 Basic stats: COMPLETE Column stats: NONE
       Execution mode: vectorized
       Map Vectorization:
@@ -59,6 +82,11 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 2
+              includeColumns: [0]
+              dataColumns: a:int, b:int
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -66,6 +94,12 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           aggregations: sum(DISTINCT KEY._col0:0._col0), count(DISTINCT KEY._col0:1._col0)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1
           Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
@@ -92,9 +126,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dtest
 #### A masked pattern was here ####
 300	1
-PREHOOK: query: explain vectorization select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc
+PREHOOK: query: explain vectorization detail
+select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc
 PREHOOK: type: QUERY
-POSTHOOK: query: explain vectorization select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc
+POSTHOOK: query: explain vectorization detail
+select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
   enabled: true
@@ -111,12 +147,28 @@ STAGE PLANS:
           TableScan
             alias: alltypesorc
             Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
             Select Operator
               expressions: cint (type: int)
               outputColumnNames: cint
+              Select Vectorization:
+                  className: VectorSelectOperator
+                  native: true
+                  projectedOutputColumns: [2]
               Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
                 aggregations: sum(DISTINCT cint), count(DISTINCT cint), avg(DISTINCT cint), std(DISTINCT cint)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumLong(col 2) -> bigint, VectorUDAFCount(col 2) -> bigint, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>, VectorUDAFStdPopLong(col 2) -> struct<count:bigint,sum:double,variance:double>
+                    className: VectorGroupByOperator
+                    groupByMode: HASH
+                    vectorOutput: true
+                    keyExpressions: col 2
+                    native: false
+                    vectorProcessingMode: HASH
+                    projectedOutputColumns: [0, 1, 2, 3]
                 keys: cint (type: int)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
@@ -124,16 +176,26 @@ STAGE PLANS:
                 Reduce Output Operator
                   key expressions: _col0 (type: int)
                   sort order: +
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false, No DISTINCT columns IS false
                   Statistics: Num rows: 12288 Data size: 2641964 Basic stats: COMPLETE Column stats: NONE
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 12
+              includeColumns: [2]
+              dataColumns: ctinyint:tinyint, csmallint:smallint, cint:int, cbigint:bigint, cfloat:float, cdouble:double, cstring1:string, cstring2:string, ctimestamp1:timestamp, ctimestamp2:timestamp, cboolean1:boolean, cboolean2:boolean
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -141,6 +203,12 @@ STAGE PLANS:
       Reduce Operator Tree:
         Group By Operator
           aggregations: sum(DISTINCT KEY._col0:0._col0), count(DISTINCT KEY._col0:1._col0), avg(DISTINCT KEY._col0:2._col0), std(DISTINCT KEY._col0:3._col0)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3
           Statistics: Num rows: 1 Data size: 180 Basic stats: COMPLETE Column stats: NONE
diff --git a/ql/src/test/results/clientpositive/vectorized_mapjoin.q.out b/ql/src/test/results/clientpositive/vectorized_mapjoin.q.out
index 32210ad978..b915e87a7f 100644
--- a/ql/src/test/results/clientpositive/vectorized_mapjoin.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_mapjoin.q.out
@@ -88,24 +88,30 @@ STAGE PLANS:
                     Group By Operator
                       aggregations: count(_col0), max(_col1), min(_col0), avg(_col2)
                       Group By Vectorization:
-                          aggregators: VectorUDAFCount(col 0) -> bigint, VectorUDAFMaxLong(col 1) -> int, VectorUDAFMinLong(col 0) -> int, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double>
+                          aggregators: VectorUDAFCount(col 0) -> bigint, VectorUDAFMaxLong(col 1) -> int, VectorUDAFMinLong(col 0) -> int, VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double,input:bigint>
                           className: VectorGroupByOperator
-                          vectorOutput: false
+                          groupByMode: HASH
+                          vectorOutput: true
                           native: false
+                          vectorProcessingMode: HASH
                           projectedOutputColumns: [0, 1, 2, 3]
-                          vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgLong(col 2) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false
                       mode: hash
                       outputColumnNames: _col0, _col1, _col2, _col3
                       Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         sort order: 
+                        Reduce Sink Vectorization:
+                            className: VectorReduceSinkOperator
+                            native: false
+                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                            nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                         Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col0 (type: bigint), _col1 (type: int), _col2 (type: int), _col3 (type: struct<count:bigint,sum:double,input:int>)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
@@ -120,8 +126,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), avg(VALUE._col3)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3
diff --git a/ql/src/test/results/clientpositive/vectorized_mapjoin2.q.out b/ql/src/test/results/clientpositive/vectorized_mapjoin2.q.out
index 52aa05b5c3..5334c16421 100644
--- a/ql/src/test/results/clientpositive/vectorized_mapjoin2.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_mapjoin2.q.out
@@ -108,8 +108,10 @@ STAGE PLANS:
                     Group By Vectorization:
                         aggregators: VectorUDAFCount(ConstantVectorExpression(val 1) -> 0:long) -> bigint
                         className: VectorGroupByOperator
+                        groupByMode: HASH
                         vectorOutput: true
                         native: false
+                        vectorProcessingMode: HASH
                         projectedOutputColumns: [0]
                     mode: hash
                     outputColumnNames: _col0
@@ -142,8 +144,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0
diff --git a/ql/src/test/results/clientpositive/vectorized_parquet_types.q.out b/ql/src/test/results/clientpositive/vectorized_parquet_types.q.out
index 46e51f7921..e096c72212 100644
--- a/ql/src/test/results/clientpositive/vectorized_parquet_types.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_parquet_types.q.out
@@ -351,13 +351,14 @@ STAGE PLANS:
               Group By Operator
                 aggregations: max(cint), min(csmallint), count(cstring1), avg(cfloat), stddev_pop(cdouble), max(cdecimal)
                 Group By Vectorization:
-                    aggregators: VectorUDAFMaxLong(col 0) -> int, VectorUDAFMinLong(col 2) -> smallint, VectorUDAFCount(col 5) -> bigint, VectorUDAFAvgDouble(col 3) -> struct<count:bigint,sum:double>, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDecimal(col 10) -> decimal(4,2)
+                    aggregators: VectorUDAFMaxLong(col 0) -> int, VectorUDAFMinLong(col 2) -> smallint, VectorUDAFCount(col 5) -> bigint, VectorUDAFAvgDouble(col 3) -> struct<count:bigint,sum:double,input:double>, VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFMaxDecimal(col 10) -> decimal(4,2)
                     className: VectorGroupByOperator
-                    vectorOutput: false
+                    groupByMode: HASH
+                    vectorOutput: true
                     keyExpressions: col 1
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2, 3, 4, 5]
-                    vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgDouble(col 3) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopDouble(col 4) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                 keys: ctinyint (type: tinyint)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
@@ -366,13 +367,18 @@ STAGE PLANS:
                   key expressions: _col0 (type: tinyint)
                   sort order: +
                   Map-reduce partition columns: _col0 (type: tinyint)
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col1 (type: int), _col2 (type: smallint), _col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,input:float>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: decimal(4,2))
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
@@ -385,8 +391,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: max(VALUE._col0), min(VALUE._col1), count(VALUE._col2), avg(VALUE._col3), stddev_pop(VALUE._col4), max(VALUE._col5)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           keys: KEY._col0 (type: tinyint)
           mode: mergepartial
diff --git a/ql/src/test/results/clientpositive/vectorized_shufflejoin.q.out b/ql/src/test/results/clientpositive/vectorized_shufflejoin.q.out
index d42369f462..d1d5e55294 100644
--- a/ql/src/test/results/clientpositive/vectorized_shufflejoin.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_shufflejoin.q.out
@@ -73,8 +73,10 @@ STAGE PLANS:
             Group By Operator
               aggregations: count(_col0), max(_col1), min(_col0), avg(_col2)
               Group By Vectorization:
+                  groupByMode: HASH
                   vectorOutput: false
                   native: false
+                  vectorProcessingMode: NONE
                   projectedOutputColumns: null
               mode: hash
               outputColumnNames: _col0, _col1, _col2, _col3
@@ -90,16 +92,27 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1, 2, 3]
             Reduce Output Operator
               sort order: 
+              Reduce Sink Vectorization:
+                  className: VectorReduceSinkOperator
+                  native: false
+                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                  nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
               Statistics: Num rows: 1 Data size: 92 Basic stats: COMPLETE Column stats: NONE
               value expressions: _col0 (type: bigint), _col1 (type: int), _col2 (type: int), _col3 (type: struct<count:bigint,sum:double,input:int>)
+      Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.mapred.SequenceFileInputFormat
-          notVectorizedReason: Value expression for REDUCESINK operator: Data type struct<count:bigint,sum:double,input:int> of Column[_col3] not supported
-          vectorized: false
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -108,8 +121,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: count(VALUE._col0), max(VALUE._col1), min(VALUE._col2), avg(VALUE._col3)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3
diff --git a/ql/src/test/results/clientpositive/vectorized_timestamp.q.out b/ql/src/test/results/clientpositive/vectorized_timestamp.q.out
index df8297cc68..e2292154cc 100644
--- a/ql/src/test/results/clientpositive/vectorized_timestamp.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_timestamp.q.out
@@ -17,10 +17,10 @@ POSTHOOK: query: INSERT INTO TABLE test VALUES ('0001-01-01 00:00:00.000000000')
 POSTHOOK: type: QUERY
 POSTHOOK: Output: default@test
 POSTHOOK: Lineage: test.ts EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
@@ -66,15 +66,43 @@ POSTHOOK: Input: default@test
 #### A masked pattern was here ####
 0001-01-01 00:00:00
 9999-12-31 23:59:59.999999999
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+0001-01-01 00:00:00	9999-12-31 23:59:59.999999999	3652060 23:59:59.999999999
+PREHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+0001-01-01 00:00:00
+PREHOOK: query: SELECT ts FROM test
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT ts FROM test
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+0001-01-01 00:00:00
+9999-12-31 23:59:59.999999999
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
-  enabled: false
-  enabledConditionsNotMet: [hive.vectorized.execution.enabled IS false]
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -87,29 +115,65 @@ STAGE PLANS:
           TableScan
             alias: test
             Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0]
             Select Operator
               expressions: ts (type: timestamp)
               outputColumnNames: ts
+              Select Vectorization:
+                  className: VectorSelectOperator
+                  native: true
+                  projectedOutputColumns: [0]
               Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
                 aggregations: min(ts), max(ts)
                 Group By Vectorization:
-                    vectorOutput: false
+                    aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 0) -> timestamp
+                    className: VectorGroupByOperator
+                    groupByMode: HASH
+                    vectorOutput: true
                     native: false
-                    projectedOutputColumns: null
+                    vectorProcessingMode: HASH
+                    projectedOutputColumns: [0, 1]
                 mode: hash
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   sort order: 
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col0 (type: timestamp), _col1 (type: timestamp)
+      Execution mode: vectorized
+      Map Vectorization:
+          enabled: true
+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
+          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
+          rowBatchContext:
+              dataColumnCount: 1
+              includeColumns: [0]
+              dataColumns: ts:timestamp
+              partitionColumnCount: 0
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
       Reduce Operator Tree:
         Group By Operator
           aggregations: min(VALUE._col0), max(VALUE._col1)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1
@@ -141,15 +205,15 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test
 #### A masked pattern was here ####
 0001-01-01 00:00:00	9999-12-31 23:59:59.999999999	3652060 23:59:59.999999999
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
 SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
-  enabled: false
-  enabledConditionsNotMet: [hive.vectorized.execution.enabled IS false]
+  enabled: true
+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -162,78 +226,34 @@ STAGE PLANS:
           TableScan
             alias: test
             Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0]
             Filter Operator
+              Filter Vectorization:
+                  className: VectorFilterOperator
+                  native: true
+                  predicateExpression: FilterTimestampColumnInList(col 0, values [0001-01-01 00:00:00.0, 0002-02-02 00:00:00.0]) -> boolean
               predicate: (ts) IN (0001-01-01 00:00:00.0, 0002-02-02 00:00:00.0) (type: boolean)
               Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: ts (type: timestamp)
                 outputColumnNames: _col0
+                Select Vectorization:
+                    className: VectorSelectOperator
+                    native: true
+                    projectedOutputColumns: [0]
                 Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  File Sink Vectorization:
+                      className: VectorFileSinkOperator
+                      native: false
                   Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-      Processor Tree:
-        ListSink
-
-PREHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
-PREHOOK: type: QUERY
-PREHOOK: Input: default@test
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@test
-#### A masked pattern was here ####
-0001-01-01 00:00:00
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test
-PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test
-POSTHOOK: type: QUERY
-PLAN VECTORIZATION:
-  enabled: true
-  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: test
-            Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
-            TableScan Vectorization:
-                native: true
-                projectedOutputColumns: [0]
-            Select Operator
-              expressions: ts (type: timestamp)
-              outputColumnNames: _col0
-              Select Vectorization:
-                  className: VectorSelectOperator
-                  native: true
-                  projectedOutputColumns: [0]
-              Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
-              File Output Operator
-                compressed: false
-                File Sink Vectorization:
-                    className: VectorFileSinkOperator
-                    native: false
-                Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
-                table:
-                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
@@ -243,6 +263,11 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 1
+              includeColumns: [0]
+              dataColumns: ts:timestamp
+              partitionColumnCount: 0
 
   Stage: Stage-0
     Fetch Operator
@@ -250,21 +275,20 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-PREHOOK: query: SELECT ts FROM test
+PREHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT ts FROM test
+POSTHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test
 #### A masked pattern was here ####
 0001-01-01 00:00:00
-9999-12-31 23:59:59.999999999
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
   enabled: true
@@ -293,16 +317,18 @@ STAGE PLANS:
                   projectedOutputColumns: [0]
               Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: min(ts), max(ts)
+                aggregations: avg(ts)
                 Group By Vectorization:
-                    aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 0) -> timestamp
+                    aggregators: VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double,input:timestamp>
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     native: false
-                    projectedOutputColumns: [0, 1]
+                    vectorProcessingMode: HASH
+                    projectedOutputColumns: [0]
                 mode: hash
-                outputColumnNames: _col0, _col1
-                Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   sort order: 
                   Reduce Sink Vectorization:
@@ -310,8 +336,8 @@ STAGE PLANS:
                       native: false
                       nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                       nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
-                  Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: timestamp), _col1 (type: timestamp)
+                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col0 (type: struct<count:bigint,sum:double,input:timestamp>)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
@@ -321,27 +347,34 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 1
+              includeColumns: [0]
+              dataColumns: ts:timestamp
+              partitionColumnCount: 0
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
           enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
       Reduce Operator Tree:
         Group By Operator
-          aggregations: min(VALUE._col0), max(VALUE._col1)
+          aggregations: avg(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
-          outputColumnNames: _col0, _col1
-          Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+          outputColumnNames: _col0
+          Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
           Select Operator
-            expressions: _col0 (type: timestamp), _col1 (type: timestamp), (_col1 - _col0) (type: interval_day_time)
-            outputColumnNames: _col0, _col1, _col2
-            Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+            expressions: _col0 (type: double), CAST( _col0 AS TIMESTAMP) (type: timestamp)
+            outputColumnNames: _col0, _col1
+            Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
             File Output Operator
               compressed: false
-              Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
               table:
                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -353,20 +386,20 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-PREHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+PREHOOK: query: SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+POSTHOOK: query: SELECT AVG(ts), CAST(AVG(ts) AS TIMESTAMP) FROM test
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test
 #### A masked pattern was here ####
-0001-01-01 00:00:00	9999-12-31 23:59:59.999999999	3652060 23:59:59.999999999
-PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
+9.56332944E10	5000-07-01 13:00:00
+PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test
 PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION
-SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
+POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
+SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test
 POSTHOOK: type: QUERY
 PLAN VECTORIZATION:
   enabled: true
@@ -386,31 +419,36 @@ STAGE PLANS:
             TableScan Vectorization:
                 native: true
                 projectedOutputColumns: [0]
-            Filter Operator
-              Filter Vectorization:
-                  className: VectorFilterOperator
+            Select Operator
+              expressions: ts (type: timestamp)
+              outputColumnNames: ts
+              Select Vectorization:
+                  className: VectorSelectOperator
                   native: true
-                  predicateExpression: FilterTimestampColumnInList(col 0, values [0001-01-01 00:00:00.0, 0002-02-02 00:00:00.0]) -> boolean
-              predicate: (ts) IN (0001-01-01 00:00:00.0, 0002-02-02 00:00:00.0) (type: boolean)
-              Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
-              Select Operator
-                expressions: ts (type: timestamp)
-                outputColumnNames: _col0
-                Select Vectorization:
-                    className: VectorSelectOperator
-                    native: true
-                    projectedOutputColumns: [0]
-                Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  File Sink Vectorization:
-                      className: VectorFileSinkOperator
+                  projectedOutputColumns: [0]
+              Statistics: Num rows: 2 Data size: 80 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                aggregations: variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts)
+                Group By Vectorization:
+                    aggregators: VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>
+                    className: VectorGroupByOperator
+                    groupByMode: HASH
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: HASH
+                    projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6]
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                Statistics: Num rows: 1 Data size: 560 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  sort order: 
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
                       native: false
-                  Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+                  Statistics: Num rows: 1 Data size: 560 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col0 (type: struct<count:bigint,sum:double,variance:double>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: struct<count:bigint,sum:double,variance:double>)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
@@ -420,6 +458,34 @@ STAGE PLANS:
           allNative: false
           usesVectorUDFAdaptor: false
           vectorized: true
+          rowBatchContext:
+              dataColumnCount: 1
+              includeColumns: [0]
+              dataColumns: ts:timestamp
+              partitionColumnCount: 0
+      Reduce Vectorization:
+          enabled: false
+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: variance(VALUE._col0), var_pop(VALUE._col1), var_samp(VALUE._col2), std(VALUE._col3), stddev(VALUE._col4), stddev_pop(VALUE._col5), stddev_samp(VALUE._col6)
+          Group By Vectorization:
+              groupByMode: MERGEPARTIAL
+              vectorOutput: false
+              native: false
+              vectorProcessingMode: NONE
+              projectedOutputColumns: null
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+          Statistics: Num rows: 1 Data size: 560 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 1 Data size: 560 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
 
   Stage: Stage-0
     Fetch Operator
@@ -427,12 +493,12 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-PREHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
+PREHOOK: query: SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT ts FROM test WHERE ts IN (timestamp '0001-01-01 00:00:00.000000000', timestamp '0002-02-02 00:00:00.000000000')
+POSTHOOK: query: SELECT variance(ts), var_pop(ts), var_samp(ts), std(ts), stddev(ts), stddev_pop(ts), stddev_samp(ts) FROM test
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test
 #### A masked pattern was here ####
-0001-01-01 00:00:00
+2.489106846793884E22	2.489106846793884E22	4.978213693587768E22	1.577690352E11	1.577690352E11	1.577690352E11	2.2311910930235822E11
diff --git a/ql/src/test/results/clientpositive/vectorized_timestamp_funcs.q.out b/ql/src/test/results/clientpositive/vectorized_timestamp_funcs.q.out
index a4536fd810..4bb3564356 100644
--- a/ql/src/test/results/clientpositive/vectorized_timestamp_funcs.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_timestamp_funcs.q.out
@@ -731,8 +731,10 @@ STAGE PLANS:
                 Group By Vectorization:
                     aggregators: VectorUDAFMinTimestamp(col 0) -> timestamp, VectorUDAFMaxTimestamp(col 0) -> timestamp, VectorUDAFCount(col 0) -> bigint, VectorUDAFCountStar(*) -> bigint
                     className: VectorGroupByOperator
+                    groupByMode: HASH
                     vectorOutput: true
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2, 3]
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3
@@ -763,8 +765,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3
@@ -825,25 +829,48 @@ STAGE PLANS:
           TableScan
             alias: alltypesorc_string
             Statistics: Num rows: 40 Data size: 84 Basic stats: COMPLETE Column stats: NONE
+            TableScan Vectorization:
+                native: true
+                projectedOutputColumns: [0, 1]
             Select Operator
               expressions: ctimestamp1 (type: timestamp)
               outputColumnNames: ctimestamp1
+              Select Vectorization:
+                  className: VectorSelectOperator
+                  native: true
+                  projectedOutputColumns: [0]
               Statistics: Num rows: 40 Data size: 84 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
                 aggregations: sum(ctimestamp1)
+                Group By Vectorization:
+                    aggregators: VectorUDAFSumTimestamp(col 0) -> double
+                    className: VectorGroupByOperator
+                    groupByMode: HASH
+                    vectorOutput: true
+                    native: false
+                    vectorProcessingMode: HASH
+                    projectedOutputColumns: [0]
                 mode: hash
                 outputColumnNames: _col0
                 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   sort order: 
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col0 (type: double)
+      Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-          notVectorizedReason: Aggregation Function expression for GROUPBY operator: Vectorization of aggreation should have succeeded org.apache.hadoop.hive.ql.metadata.HiveException: Vector aggregate not implemented: "sum" for type: "TIMESTAMP (UDAF evaluator mode = PARTIAL1)
-          vectorized: false
+          allNative: false
+          usesVectorUDFAdaptor: false
+          vectorized: true
       Reduce Vectorization:
           enabled: false
           enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
@@ -852,8 +879,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: sum(VALUE._col0)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0
@@ -940,24 +969,30 @@ STAGE PLANS:
               Group By Operator
                 aggregations: avg(ctimestamp1), variance(ctimestamp1), var_pop(ctimestamp1), var_samp(ctimestamp1), std(ctimestamp1), stddev(ctimestamp1), stddev_pop(ctimestamp1), stddev_samp(ctimestamp1)
                 Group By Vectorization:
-                    aggregators: VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>
+                    aggregators: VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double,input:timestamp>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>, VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double>
                     className: VectorGroupByOperator
-                    vectorOutput: false
+                    groupByMode: HASH
+                    vectorOutput: true
                     native: false
+                    vectorProcessingMode: HASH
                     projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7]
-                    vectorOutputConditionsNotMet: Vector output of VectorUDAFAvgTimestamp(col 0) -> struct<count:bigint,sum:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFVarSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdPopTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false, Vector output of VectorUDAFStdSampTimestamp(col 0) -> struct<count:bigint,sum:double,variance:double> output type STRUCT requires PRIMITIVE IS false
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                 Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   sort order: 
+                  Reduce Sink Vectorization:
+                      className: VectorReduceSinkOperator
+                      native: false
+                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
+                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                   Statistics: Num rows: 1 Data size: 672 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col0 (type: struct<count:bigint,sum:double,input:timestamp>), _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>), _col3 (type: struct<count:bigint,sum:double,variance:double>), _col4 (type: struct<count:bigint,sum:double,variance:double>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: struct<count:bigint,sum:double,variance:double>), _col7 (type: struct<count:bigint,sum:double,variance:double>)
       Execution mode: vectorized
       Map Vectorization:
           enabled: true
           enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
-          groupByVectorOutput: false
+          groupByVectorOutput: true
           inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
           allNative: false
           usesVectorUDFAdaptor: false
@@ -970,8 +1005,10 @@ STAGE PLANS:
         Group By Operator
           aggregations: avg(VALUE._col0), variance(VALUE._col1), var_pop(VALUE._col2), var_samp(VALUE._col3), std(VALUE._col4), stddev(VALUE._col5), stddev_pop(VALUE._col6), stddev_samp(VALUE._col7)
           Group By Vectorization:
+              groupByMode: MERGEPARTIAL
               vectorOutput: false
               native: false
+              vectorProcessingMode: NONE
               projectedOutputColumns: null
           mode: mergepartial
           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
diff --git a/vector-code-gen/src/org/apache/hadoop/hive/tools/GenVectorCode.java b/vector-code-gen/src/org/apache/hadoop/hive/tools/GenVectorCode.java
index 926321e3fd..b87ec55c67 100644
--- a/vector-code-gen/src/org/apache/hadoop/hive/tools/GenVectorCode.java
+++ b/vector-code-gen/src/org/apache/hadoop/hive/tools/GenVectorCode.java
@@ -24,6 +24,11 @@
 import java.io.FileReader;
 import java.io.FileWriter;
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
 
 import org.apache.tools.ant.BuildException;
 import org.apache.tools.ant.Task;
@@ -1004,51 +1009,144 @@ public class GenVectorCode extends Task {
       {"VectorUDAFMinMaxIntervalDayTime", "VectorUDAFMinIntervalDayTime", ">", "min",
           "_FUNC_(expr) - Returns the minimum value of expr (vectorized, type: interval_day_time)"},
 
-        //template, <ClassName>, <ValueType>
-        {"VectorUDAFSum", "VectorUDAFSumLong", "long"},
-        {"VectorUDAFSum", "VectorUDAFSumDouble", "double"},
-        {"VectorUDAFAvg", "VectorUDAFAvgLong", "long"},
-        {"VectorUDAFAvg", "VectorUDAFAvgDouble", "double"},
+      // Template, <ClassName>, <ValueType>
+      {"VectorUDAFSum", "VectorUDAFSumLong", "long"},
+      {"VectorUDAFSum", "VectorUDAFSumDouble", "double"},
+
+      // Template, <ClassName>, <ValueType>, <IfDefined>
+      {"VectorUDAFAvg", "VectorUDAFAvgLong", "long", "PARTIAL1"},
+      {"VectorUDAFAvg", "VectorUDAFAvgLongComplete", "long", "COMPLETE"},
+
+      {"VectorUDAFAvg", "VectorUDAFAvgDouble", "double", "PARTIAL1"},
+      {"VectorUDAFAvg", "VectorUDAFAvgDoubleComplete", "double", "COMPLETE"},
+
+      {"VectorUDAFAvgDecimal", "VectorUDAFAvgDecimal", "PARTIAL1"},
+      {"VectorUDAFAvgDecimal", "VectorUDAFAvgDecimalComplete", "COMPLETE"},
+
+      {"VectorUDAFAvgTimestamp", "VectorUDAFAvgTimestamp", "PARTIAL1"},
+      {"VectorUDAFAvgTimestamp", "VectorUDAFAvgTimestampComplete", "COMPLETE"},
+
+      //template, <ClassName>, <ValueType>, <IfDefined>
+      {"VectorUDAFAvgMerge", "VectorUDAFAvgPartial2", "PARTIAL2"},
+      {"VectorUDAFAvgMerge", "VectorUDAFAvgFinal", "FINAL"},
+
+      {"VectorUDAFAvgDecimalMerge", "VectorUDAFAvgDecimalPartial2", "PARTIAL2"},
+      {"VectorUDAFAvgDecimalMerge", "VectorUDAFAvgDecimalFinal", "FINAL"},
+
+      // (since Timestamps are averaged with double, we don't need a PARTIAL2 class)
+      // (and, since Timestamps are output as double for AVG, we don't need a FINAL class, either)
+      // {"VectorUDAFAvgMerge", "VectorUDAFAvgTimestampPartial2", "PARTIAL2"},
+      // {"VectorUDAFAvgMerge", "VectorUDAFAvgTimestampFinal", "FINAL"},
 
       // template, <ClassName>, <ValueType>, <VarianceFormula>, <DescriptionName>,
       // <DescriptionValue>
-      {"VectorUDAFVar", "VectorUDAFVarPopLong", "long", "myagg.variance / myagg.count",
+      {"VectorUDAFVar", "VectorUDAFVarPopLong", "long", "PARTIAL1", "myagg.variance / myagg.count",
           "variance, var_pop",
           "_FUNC_(x) - Returns the variance of a set of numbers (vectorized, long)"},
-      {"VectorUDAFVar", "VectorUDAFVarPopDouble", "double", "myagg.variance / myagg.count",
+      {"VectorUDAFVar", "VectorUDAFVarPopLongComplete", "long", "COMPLETE,VARIANCE", "myagg.variance / myagg.count",
+        "variance, var_pop",
+        "_FUNC_(x) - Returns the variance of a set of numbers (vectorized, long)"},
+      {"VectorUDAFVar", "VectorUDAFVarPopDouble", "double", "PARTIAL1", "myagg.variance / myagg.count",
           "variance, var_pop",
           "_FUNC_(x) - Returns the variance of a set of numbers (vectorized, double)"},
-      {"VectorUDAFVarDecimal", "VectorUDAFVarPopDecimal", "myagg.variance / myagg.count",
+      {"VectorUDAFVar", "VectorUDAFVarPopDoubleComplete", "double", "COMPLETE,VARIANCE", "myagg.variance / myagg.count",
+        "variance, var_pop",
+        "_FUNC_(x) - Returns the variance of a set of numbers (vectorized, double)"},
+      {"VectorUDAFVarDecimal", "VectorUDAFVarPopDecimal", "PARTIAL1", "myagg.variance / myagg.count",
           "variance, var_pop",
           "_FUNC_(x) - Returns the variance of a set of numbers (vectorized, decimal)"},
-      {"VectorUDAFVar", "VectorUDAFVarSampLong", "long", "myagg.variance / (myagg.count-1.0)",
+      {"VectorUDAFVarDecimal", "VectorUDAFVarPopDecimalComplete", "COMPLETE,VARIANCE", "myagg.variance / myagg.count",
+        "variance, var_pop",
+        "_FUNC_(x) - Returns the variance of a set of numbers (vectorized, timestamp)"},
+      {"VectorUDAFVarTimestamp", "VectorUDAFVarPopTimestamp", "PARTIAL1", "myagg.variance / myagg.count",
+        "variance, var_pop",
+        "_FUNC_(x) - Returns the variance of a set of numbers (vectorized, timestamp)"},
+      {"VectorUDAFVarTimestamp", "VectorUDAFVarPopTimestampComplete", "COMPLETE,VARIANCE", "myagg.variance / myagg.count",
+        "variance, var_pop",
+        "_FUNC_(x) - Returns the variance of a set of numbers (vectorized, decimal)"},
+
+      {"VectorUDAFVar", "VectorUDAFVarSampLong", "long", "PARTIAL1", "myagg.variance / (myagg.count-1.0)",
           "var_samp",
           "_FUNC_(x) - Returns the sample variance of a set of numbers (vectorized, long)"},
-      {"VectorUDAFVar", "VectorUDAFVarSampDouble", "double", "myagg.variance / (myagg.count-1.0)",
+      {"VectorUDAFVar", "VectorUDAFVarSampLongComplete", "long", "COMPLETE,VARIANCE_SAMPLE", "myagg.variance / (myagg.count-1.0)",
+        "var_samp",
+        "_FUNC_(x) - Returns the sample variance of a set of numbers (vectorized, long)"},
+      {"VectorUDAFVar", "VectorUDAFVarSampDouble", "double", "PARTIAL1", "myagg.variance / (myagg.count-1.0)",
           "var_samp",
           "_FUNC_(x) - Returns the sample variance of a set of numbers (vectorized, double)"},
-      {"VectorUDAFVarDecimal", "VectorUDAFVarSampDecimal", "myagg.variance / (myagg.count-1.0)",
+      {"VectorUDAFVar", "VectorUDAFVarSampDoubleComplete", "double", "COMPLETE,VARIANCE_SAMPLE", "myagg.variance / (myagg.count-1.0)",
+        "var_samp",
+        "_FUNC_(x) - Returns the sample variance of a set of numbers (vectorized, double)"},
+      {"VectorUDAFVarDecimal", "VectorUDAFVarSampDecimal", "PARTIAL1", "myagg.variance / (myagg.count-1.0)",
           "var_samp",
           "_FUNC_(x) - Returns the sample variance of a set of numbers (vectorized, decimal)"},
-      {"VectorUDAFVar", "VectorUDAFStdPopLong", "long",
+      {"VectorUDAFVarDecimal", "VectorUDAFVarSampDecimalComplete", "COMPLETE,VARIANCE_SAMPLE", "myagg.variance / (myagg.count-1.0)",
+        "var_samp",
+        "_FUNC_(x) - Returns the sample variance of a set of numbers (vectorized, decimal)"},
+      {"VectorUDAFVarTimestamp", "VectorUDAFVarSampTimestamp", "PARTIAL1", "myagg.variance / (myagg.count-1.0)",
+        "var_samp",
+        "_FUNC_(x) - Returns the sample variance of a set of numbers (vectorized, timestamp)"},
+      {"VectorUDAFVarTimestamp", "VectorUDAFVarSampTimestampComplete", "COMPLETE,VARIANCE_SAMPLE", "myagg.variance / (myagg.count-1.0)",
+        "var_samp",
+        "_FUNC_(x) - Returns the sample variance of a set of numbers (vectorized, timestamp)"},
+
+      {"VectorUDAFVar", "VectorUDAFStdPopLong", "long", "PARTIAL1",
           "Math.sqrt(myagg.variance / (myagg.count))", "std,stddev,stddev_pop",
           "_FUNC_(x) - Returns the standard deviation of a set of numbers (vectorized, long)"},
-      {"VectorUDAFVar", "VectorUDAFStdPopDouble", "double",
+      {"VectorUDAFVar", "VectorUDAFStdPopLongComplete", "long", "COMPLETE,STD",
+        "Math.sqrt(myagg.variance / (myagg.count))", "std,stddev,stddev_pop",
+        "_FUNC_(x) - Returns the standard deviation of a set of numbers (vectorized, long)"},
+      {"VectorUDAFVar", "VectorUDAFStdPopDouble", "double", "PARTIAL1",
           "Math.sqrt(myagg.variance / (myagg.count))", "std,stddev,stddev_pop",
           "_FUNC_(x) - Returns the standard deviation of a set of numbers (vectorized, double)"},
-      {"VectorUDAFVarDecimal", "VectorUDAFStdPopDecimal",
+      {"VectorUDAFVar", "VectorUDAFStdPopDoubleComplete", "double", "COMPLETE,STD",
+        "Math.sqrt(myagg.variance / (myagg.count))", "std,stddev,stddev_pop",
+        "_FUNC_(x) - Returns the standard deviation of a set of numbers (vectorized, double)"},
+      {"VectorUDAFVarDecimal", "VectorUDAFStdPopDecimal", "PARTIAL1",
           "Math.sqrt(myagg.variance / (myagg.count))", "std,stddev,stddev_pop",
           "_FUNC_(x) - Returns the standard deviation of a set of numbers (vectorized, decimal)"},
-      {"VectorUDAFVar", "VectorUDAFStdSampLong", "long",
+      {"VectorUDAFVarDecimal", "VectorUDAFStdPopDecimalComplete", "COMPLETE,STD",
+        "Math.sqrt(myagg.variance / (myagg.count))", "std,stddev,stddev_pop",
+        "_FUNC_(x) - Returns the standard deviation of a set of numbers (vectorized, decimal)"},
+      {"VectorUDAFVarTimestamp", "VectorUDAFStdPopTimestamp", "PARTIAL1",
+        "Math.sqrt(myagg.variance / (myagg.count))", "std,stddev,stddev_pop",
+        "_FUNC_(x) - Returns the standard deviation of a set of numbers (vectorized, timestamp)"},
+      {"VectorUDAFVarTimestamp", "VectorUDAFStdPopTimestampComplete", "COMPLETE,STD",
+        "Math.sqrt(myagg.variance / (myagg.count))", "std,stddev,stddev_pop",
+        "_FUNC_(x) - Returns the standard deviation of a set of numbers (vectorized, timestamp)"},
+
+      {"VectorUDAFVar", "VectorUDAFStdSampLong", "long", "PARTIAL1",
           "Math.sqrt(myagg.variance / (myagg.count-1.0))", "stddev_samp",
           "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, long)"},
-      {"VectorUDAFVar", "VectorUDAFStdSampDouble", "double",
+      {"VectorUDAFVar", "VectorUDAFStdSampLongComplete", "long", "COMPLETE,STD_SAMPLE",
+        "Math.sqrt(myagg.variance / (myagg.count-1.0))", "stddev_samp",
+        "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, long)"},
+      {"VectorUDAFVar", "VectorUDAFStdSampDouble", "double", "PARTIAL1",
           "Math.sqrt(myagg.variance / (myagg.count-1.0))", "stddev_samp",
           "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, double)"},
-      {"VectorUDAFVarDecimal", "VectorUDAFStdSampDecimal",
+      {"VectorUDAFVar", "VectorUDAFStdSampDoubleComplete", "double", "COMPLETE,STD_SAMPLE",
+        "Math.sqrt(myagg.variance / (myagg.count-1.0))", "stddev_samp",
+        "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, double)"},
+      {"VectorUDAFVarDecimal", "VectorUDAFStdSampDecimal", "PARTIAL1",
           "Math.sqrt(myagg.variance / (myagg.count-1.0))", "stddev_samp",
           "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, decimal)"},
-
+      {"VectorUDAFVarDecimal", "VectorUDAFStdSampDecimalComplete", "COMPLETE,STD_SAMPLE",
+        "Math.sqrt(myagg.variance / (myagg.count-1.0))", "stddev_samp",
+        "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, decimal)"},
+      {"VectorUDAFVarTimestamp", "VectorUDAFStdSampTimestamp", "PARTIAL1",
+        "Math.sqrt(myagg.variance / (myagg.count-1.0))", "stddev_samp",
+        "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, timestamp)"},
+      {"VectorUDAFVarTimestamp", "VectorUDAFStdSampTimestampComplete", "COMPLETE,STD_SAMPLE",
+        "Math.sqrt(myagg.variance / (myagg.count-1.0))", "stddev_samp",
+        "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, timestamp)"},
+
+      //template, <ClassName>, <ValueType>, <IfDefined>
+      {"VectorUDAFVarMerge", "VectorUDAFVarPartial2", "PARTIAL2"},
+
+      {"VectorUDAFVarMerge", "VectorUDAFVarPopFinal", "FINAL,VARIANCE"},
+      {"VectorUDAFVarMerge", "VectorUDAFVarSampFinal", "FINAL,VARIANCE_SAMPLE"},
+      {"VectorUDAFVarMerge", "VectorUDAFStdPopFinal", "FINAL,STD"},
+      {"VectorUDAFVarMerge", "VectorUDAFStdSampFinal", "FINAL,STD_SAMPLE"},
     };
 
 
@@ -1204,10 +1302,22 @@ private void generate() throws Exception {
         generateVectorUDAFSum(tdesc);
       } else if (tdesc[0].equals("VectorUDAFAvg")) {
         generateVectorUDAFAvg(tdesc);
+      } else if (tdesc[0].equals("VectorUDAFAvgMerge")) {
+        generateVectorUDAFAvgMerge(tdesc);
+      } else if (tdesc[0].equals("VectorUDAFAvgDecimal")) {
+        generateVectorUDAFAvgObject(tdesc);
+      } else if (tdesc[0].equals("VectorUDAFAvgTimestamp")) {
+        generateVectorUDAFAvgObject(tdesc);
+      } else if (tdesc[0].equals("VectorUDAFAvgDecimalMerge")) {
+        generateVectorUDAFAvgMerge(tdesc);
       } else if (tdesc[0].equals("VectorUDAFVar")) {
         generateVectorUDAFVar(tdesc);
       } else if (tdesc[0].equals("VectorUDAFVarDecimal")) {
-        generateVectorUDAFVarDecimal(tdesc);
+        generateVectorUDAFVarObject(tdesc);
+      } else if (tdesc[0].equals("VectorUDAFVarTimestamp")) {
+        generateVectorUDAFVarObject(tdesc);
+      } else if (tdesc[0].equals("VectorUDAFVarMerge")) {
+        generateVectorUDAFVarMerge(tdesc);
       } else if (tdesc[0].equals("FilterStringGroupColumnCompareStringGroupScalarBase")) {
         generateFilterStringGroupColumnCompareStringGroupScalarBase(tdesc);
       } else if (tdesc[0].equals("FilterStringGroupColumnCompareStringScalar")) {
@@ -1565,14 +1675,50 @@ private void generateVectorUDAFSum(String[] tdesc) throws Exception {
   private void generateVectorUDAFAvg(String[] tdesc) throws Exception {
     String className = tdesc[1];
     String valueType = tdesc[2];
+    String camelValueCaseType = getCamelCaseType(valueType);
     String columnType = getColumnVectorType(valueType);
+    String ifDefined = tdesc[3];
 
     File templateFile = new File(joinPath(this.udafTemplateDirectory, tdesc[0] + ".txt"));
 
     String templateString = readFile(templateFile);
     templateString = templateString.replaceAll("<ClassName>", className);
     templateString = templateString.replaceAll("<ValueType>", valueType);
+    templateString = templateString.replaceAll("<CamelCaseValueType>", camelValueCaseType);
     templateString = templateString.replaceAll("<InputColumnVectorType>", columnType);
+
+    templateString = evaluateIfDefined(templateString, ifDefined);
+
+    writeFile(templateFile.lastModified(), udafOutputDirectory, udafClassesDirectory,
+        className, templateString);
+  }
+
+  private void generateVectorUDAFAvgMerge(String[] tdesc) throws Exception {
+    String className = tdesc[1];
+    String groupByMode = tdesc[2];
+
+    File templateFile = new File(joinPath(this.udafTemplateDirectory, tdesc[0] + ".txt"));
+
+    String templateString = readFile(templateFile);
+    templateString = templateString.replaceAll("<ClassName>", className);
+
+    templateString = evaluateIfDefined(templateString, groupByMode);
+
+    writeFile(templateFile.lastModified(), udafOutputDirectory, udafClassesDirectory,
+        className, templateString);
+  }
+
+  private void generateVectorUDAFAvgObject(String[] tdesc) throws Exception {
+    String className = tdesc[1];
+    String ifDefined = tdesc[2];
+
+    File templateFile = new File(joinPath(this.udafTemplateDirectory, tdesc[0] + ".txt"));
+
+    String templateString = readFile(templateFile);
+    templateString = templateString.replaceAll("<ClassName>", className);
+
+    templateString = evaluateIfDefined(templateString, ifDefined);
+
     writeFile(templateFile.lastModified(), udafOutputDirectory, udafClassesDirectory,
         className, templateString);
   }
@@ -1580,9 +1726,10 @@ private void generateVectorUDAFAvg(String[] tdesc) throws Exception {
   private void generateVectorUDAFVar(String[] tdesc) throws Exception {
     String className = tdesc[1];
     String valueType = tdesc[2];
-    String varianceFormula = tdesc[3];
-    String descriptionName = tdesc[4];
-    String descriptionValue = tdesc[5];
+    String ifDefined = tdesc[3];
+    String varianceFormula = tdesc[4];
+    String descriptionName = tdesc[5];
+    String descriptionValue = tdesc[6];
     String columnType = getColumnVectorType(valueType);
 
     File templateFile = new File(joinPath(this.udafTemplateDirectory, tdesc[0] + ".txt"));
@@ -1594,26 +1741,48 @@ private void generateVectorUDAFVar(String[] tdesc) throws Exception {
     templateString = templateString.replaceAll("<VarianceFormula>", varianceFormula);
     templateString = templateString.replaceAll("<DescriptionName>", descriptionName);
     templateString = templateString.replaceAll("<DescriptionValue>", descriptionValue);
+
+    templateString = evaluateIfDefined(templateString, ifDefined);
+
     writeFile(templateFile.lastModified(), udafOutputDirectory, udafClassesDirectory,
         className, templateString);
   }
 
-  private void generateVectorUDAFVarDecimal(String[] tdesc) throws Exception {
-      String className = tdesc[1];
-      String varianceFormula = tdesc[2];
-      String descriptionName = tdesc[3];
-      String descriptionValue = tdesc[4];
+  private void generateVectorUDAFVarObject(String[] tdesc) throws Exception {
+    String className = tdesc[1];
+    String ifDefined = tdesc[2];
+    String varianceFormula = tdesc[3];
+    String descriptionName = tdesc[4];
+    String descriptionValue = tdesc[5];
 
-      File templateFile = new File(joinPath(this.udafTemplateDirectory, tdesc[0] + ".txt"));
+    File templateFile = new File(joinPath(this.udafTemplateDirectory, tdesc[0] + ".txt"));
 
-      String templateString = readFile(templateFile);
-      templateString = templateString.replaceAll("<ClassName>", className);
-      templateString = templateString.replaceAll("<VarianceFormula>", varianceFormula);
-      templateString = templateString.replaceAll("<DescriptionName>", descriptionName);
-      templateString = templateString.replaceAll("<DescriptionValue>", descriptionValue);
-      writeFile(templateFile.lastModified(), udafOutputDirectory, udafClassesDirectory,
-          className, templateString);
-    }
+    String templateString = readFile(templateFile);
+    templateString = templateString.replaceAll("<ClassName>", className);
+    templateString = templateString.replaceAll("<VarianceFormula>", varianceFormula);
+    templateString = templateString.replaceAll("<DescriptionName>", descriptionName);
+    templateString = templateString.replaceAll("<DescriptionValue>", descriptionValue);
+
+    templateString = evaluateIfDefined(templateString, ifDefined);
+
+    writeFile(templateFile.lastModified(), udafOutputDirectory, udafClassesDirectory,
+        className, templateString);
+  }
+
+  private void generateVectorUDAFVarMerge(String[] tdesc) throws Exception {
+    String className = tdesc[1];
+    String groupByMode = tdesc[2];
+
+    File templateFile = new File(joinPath(this.udafTemplateDirectory, tdesc[0] + ".txt"));
+
+    String templateString = readFile(templateFile);
+    templateString = templateString.replaceAll("<ClassName>", className);
+
+    templateString = evaluateIfDefined(templateString, groupByMode);
+
+    writeFile(templateFile.lastModified(), udafOutputDirectory, udafClassesDirectory,
+        className, templateString);
+  }
 
   private void generateFilterStringGroupScalarCompareStringGroupColumnBase(String[] tdesc) throws IOException {
     String operatorName = tdesc[1];
@@ -3126,6 +3295,102 @@ private static boolean isTimestampIntervalType(String type) {
         || type.equals("interval_day_time"));
   }
 
+  private boolean containsDefinedStrings(Set<String> defineSet, String commaDefinedString) {
+    String[] definedStrings = commaDefinedString.split(",");
+    boolean result = false;
+    for (String definedString : definedStrings) {
+      if (defineSet.contains(definedString)) {
+        result = true;
+        break;
+      }
+    }
+    return result;
+  }
+
+  private int doIfDefinedStatement(String[] lines, int index, Set<String> definedSet,
+      boolean outerInclude, StringBuilder sb) {
+    String ifLine = lines[index];
+    final int ifLineNumber = index + 1;
+    String commaDefinedString = ifLine.substring("#IF ".length());
+    boolean includeBody = containsDefinedStrings(definedSet, commaDefinedString);
+    index++;
+    final int end = lines.length;
+    while (true) {
+      if (index >= end) {
+        throw new RuntimeException("Unmatched #IF at line " + index + " for " + commaDefinedString);
+      }
+      String line = lines[index];
+      if (line.length() == 0 || line.charAt(0) != '#') {
+        if (outerInclude && includeBody) {
+          sb.append(line);
+          sb.append("\n");
+        }
+        index++;
+        continue;
+      }
+
+      // A pound # statement (IF/ELSE/ENDIF).
+      if (line.startsWith("#IF ")) {
+        // Recurse.
+        index = doIfDefinedStatement(lines, index, definedSet, outerInclude && includeBody, sb);
+      } else if (line.equals("#ELSE")) {
+        // Flip inclusion.
+        includeBody = !includeBody;
+        index++;
+      } else if (line.equals("#ENDIF")) {
+        throw new RuntimeException("Missing defined strings with #ENDIF on line " + (index + 1));
+      } else if (line.startsWith("#ENDIF ")) {
+        String endCommaDefinedString = line.substring("#ENDIF ".length());
+        if (!commaDefinedString.equals(endCommaDefinedString)) {
+          throw new RuntimeException(
+              "#ENDIF defined names \"" + endCommaDefinedString + "\" (line " + ifLineNumber +
+              " do not match \"" + commaDefinedString + "\" (line " + (index + 1) + ")");
+        }
+        return ++index;
+      } else {
+        throw new RuntimeException("Problem with #IF/#ELSE/#ENDIF on line " + (index + 1) + ": " + line);
+      }
+    }
+  }
+
+  private void doEvaluateIfDefined(String[] lines, int index, Set<String> definedSet,
+      boolean outerInclude, StringBuilder sb) {
+      final int end = lines.length;
+      while (true) {
+        if (index >= end) {
+          break;
+        }
+        String line = lines[index];
+        if (line.length() == 0 || line.charAt(0) != '#') {
+          if (outerInclude) {
+            sb.append(line);
+            sb.append("\n");
+          }
+          index++;
+          continue;
+        }
+
+        // A pound # statement (IF/ELSE/ENDIF).
+        if (line.startsWith("#IF ")) {
+          index = doIfDefinedStatement(lines, index, definedSet, outerInclude, sb);
+        } else {
+          throw new RuntimeException("Problem with #IF/#ELSE/#ENDIF on line " + (index + 1) + ": " + line);
+        }
+      }
+  }
+
+  private String evaluateIfDefined(String linesString, List<String> definedList) {
+    String[] lines = linesString.split("\n");
+    Set<String> definedSet = new HashSet<String>(definedList);
+    StringBuilder sb = new StringBuilder();
+    doEvaluateIfDefined(lines, 0, definedSet, true, sb);
+    return sb.toString();
+  }
+
+  private String evaluateIfDefined(String linesString, String definedString) {
+    return evaluateIfDefined(linesString, Arrays.asList(definedString.split(",")));
+  }
+
   static void writeFile(long templateTime, String outputDir, String classesDir,
        String className, String str) throws IOException {
     File outputFile = new File(outputDir, className + ".java");
