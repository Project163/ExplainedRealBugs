diff --git a/CHANGES.txt b/CHANGES.txt
index bccc60914b..504ab6b705 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -15,9 +15,14 @@ Trunk - unreleased changes
 
   BUG FIXES
 
-    HIVE-387. Use URI from FileSystem Object instead of from HADOOPFS directly
     HIVE-381. Fix JDBC HiveResultSet's next function.
     (Kim P via namit)
+    
+    HIVE-387. Use URI from FileSystem Object instead of from HADOOPFS
+    directly. (jssarma)
+
+    HIVE-399. Fix timeout problems caused due to the bigdata test.
+    (Namit Jain via athusoo)
 
 Release 0.3.0 - Unreleased
 
diff --git a/build-common.xml b/build-common.xml
index f57c530349..32427a01f8 100644
--- a/build-common.xml
+++ b/build-common.xml
@@ -60,7 +60,7 @@
   <property name="test.include" value="Test*"/>
   <property name="test.classpath.id" value="test.classpath"/>
   <property name="test.output" value="true"/>
-  <property name="test.timeout" value="1800000"/>
+  <property name="test.timeout" value="2700000"/>
   <property name="test.junit.output.format" value="xml"/>
   <property name="test.junit.output.usefile" value="true"/>
 
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index ee965013d8..4153e7da43 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -123,6 +123,9 @@ public static enum ConfVars {
     HIVEHWILISTENPORT("hive.hwi.listen.port","9999"),
     HIVEHWIWARFILE("hive.hwi.war.file",System.getenv("HIVE_HOME")+"/lib/hive_hwi.war"),
 
+    // mapper/reducer memory in local mode
+    HIVEHADOOPMAXMEM("hive.mapred.local.mem", 0),
+
     // Optimizer
     HIVEOPTPPD("hive.optimize.ppd", false); // predicate pushdown
     
diff --git a/data/scripts/dumpdata_script.py b/data/scripts/dumpdata_script.py
index 19be0cbfbf..07f1c3fa23 100644
--- a/data/scripts/dumpdata_script.py
+++ b/data/scripts/dumpdata_script.py
@@ -1,11 +1,4 @@
-for i in xrange(100):
-   for j in xrange(10):
-      for k in xrange(42022):      
-         print 42000 * i + k
-
-
-for i in xrange(100):
-   for j in xrange(10):
-      for k in xrange(42022):      
-         print 5000000 + (42000 * i) + k
-
+for i in xrange(50):
+   for j in xrange(5):
+      for k in xrange(20022):      
+         print 20000 * i + k
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
index d4e163b6e6..ceb9be566c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
@@ -21,6 +21,7 @@
 import java.io.File;
 import java.io.FileOutputStream;
 import java.io.Serializable;
+import java.util.Map;
 
 import org.apache.hadoop.mapred.JobConf;
 
@@ -59,7 +60,7 @@ public int execute() {
       LOG.info("Generating plan file " + planFile.toString());
       FileOutputStream out = new FileOutputStream(planFile);
       Utilities.serializeMapRedWork(plan, out);
-    
+
       String cmdLine = hadoopExec + " jar " + auxJars + " " + hiveJar + " org.apache.hadoop.hive.ql.exec.ExecDriver -plan " + planFile.toString() + " " + hiveConfArgs;
       
       String files = ExecDriver.getRealFiles(conf);
@@ -68,8 +69,29 @@ public int execute() {
       }
 
       LOG.info("Executing: " + cmdLine);
-      Process executor = Runtime.getRuntime().exec(cmdLine);
-
+      Process executor = null;
+
+      // The user can specify the hadoop memory
+      int hadoopMem = conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);
+
+      if (hadoopMem == 0) 
+        executor = Runtime.getRuntime().exec(cmdLine);
+      // user specified the memory - only applicable for local mode
+      else {
+        Map<String, String> variables = System.getenv();  
+        String[] env = new String[variables.size() + 1];
+        int pos = 0;
+        
+        for (Map.Entry<String, String> entry : variables.entrySet())  
+        {  
+          String name = entry.getKey();  
+          String value = entry.getValue();  
+          env[pos++] = name + "=" + value;  
+        }  
+        
+        env[pos] = new String("HADOOP_HEAPSIZE=" + hadoopMem);
+        executor = Runtime.getRuntime().exec(cmdLine, env);
+      }
 
       StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, System.out);
       StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, System.err);
diff --git a/ql/src/test/queries/clientpositive/groupby_bigdata.q b/ql/src/test/queries/clientpositive/groupby_bigdata.q
index d45510707e..499bab9608 100644
--- a/ql/src/test/queries/clientpositive/groupby_bigdata.q
+++ b/ql/src/test/queries/clientpositive/groupby_bigdata.q
@@ -1,4 +1,5 @@
-set hive.map.aggr.hash.percentmemory = 0.4;
+set hive.map.aggr.hash.percentmemory = 0.3;
+set hive.mapred.local.mem = 256;
 
 select count(distinct subq.key) from
 (FROM src MAP src.key USING 'python ../data/scripts/dumpdata_script.py' AS key WHERE src.key = 10) subq;
diff --git a/ql/src/test/results/clientpositive/groupby_bigdata.q.out b/ql/src/test/results/clientpositive/groupby_bigdata.q.out
index 47d71862de..6dc150a9b3 100644
--- a/ql/src/test/results/clientpositive/groupby_bigdata.q.out
+++ b/ql/src/test/results/clientpositive/groupby_bigdata.q.out
@@ -1 +1 @@
-8400044
+1000022
