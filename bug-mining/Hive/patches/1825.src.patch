diff --git a/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/CreateTableHook.java b/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/CreateTableHook.java
index 791e01b9f7..36a6e0c538 100644
--- a/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/CreateTableHook.java
+++ b/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/CreateTableHook.java
@@ -49,6 +49,7 @@
 /**
  * @deprecated Use/modify {@link org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook} instead
  */
+@Deprecated
 final class CreateTableHook extends HCatSemanticAnalyzerBase {
 
   private String tableName;
@@ -216,7 +217,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,
       try {
         Table table = context.getHive().newTable(desc.getTableName());
         if (desc.getLocation() != null) {
-          table.setDataLocation(new Path(desc.getLocation()).toUri());
+          table.setDataLocation(new Path(desc.getLocation()));
         }
         if (desc.getStorageHandler() != null) {
           table.setProperty(
diff --git a/hcatalog/core/src/main/java/org/apache/hcatalog/security/HdfsAuthorizationProvider.java b/hcatalog/core/src/main/java/org/apache/hcatalog/security/HdfsAuthorizationProvider.java
index 2eba53042b..cd2d7bfdb2 100644
--- a/hcatalog/core/src/main/java/org/apache/hcatalog/security/HdfsAuthorizationProvider.java
+++ b/hcatalog/core/src/main/java/org/apache/hcatalog/security/HdfsAuthorizationProvider.java
@@ -208,7 +208,7 @@ public void authorize(Table table, Partition part, Privilege[] readRequiredPriv,
     if (part == null || part.getLocation() == null) {
       authorize(table, readRequiredPriv, writeRequiredPriv);
     } else {
-      authorize(part.getPartitionPath(), readRequiredPriv, writeRequiredPriv);
+      authorize(part.getDataLocation(), readRequiredPriv, writeRequiredPriv);
     }
   }
 
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java
index 4c3acb6da8..ec24531117 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java
@@ -202,7 +202,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,
             desc.getSerName(),
             desc.getInputFormat(),
             desc.getOutputFormat());
-        //Authorization checks are performed by the storageHandler.getAuthorizationProvider(), if  
+        //Authorization checks are performed by the storageHandler.getAuthorizationProvider(), if
         //StorageDelegationAuthorizationProvider is used.
       } catch (IOException e) {
         throw new SemanticException(e);
@@ -213,7 +213,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,
       try {
         Table table = context.getHive().newTable(desc.getTableName());
         if (desc.getLocation() != null) {
-          table.setDataLocation(new Path(desc.getLocation()).toUri());
+          table.setDataLocation(new Path(desc.getLocation()));
         }
         if (desc.getStorageHandler() != null) {
           table.setProperty(
diff --git a/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatMultiOutputFormat.java b/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatMultiOutputFormat.java
index a2be640638..f3e4037818 100644
--- a/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatMultiOutputFormat.java
+++ b/hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatMultiOutputFormat.java
@@ -74,6 +74,7 @@
 /**
  * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat} instead
  */
+@Deprecated
 public class TestHCatMultiOutputFormat {
 
   private static final Logger LOG = LoggerFactory.getLogger(TestHCatMultiOutputFormat.class);
@@ -377,13 +378,13 @@ private List<String> getTableData(String table, String database) throws Exceptio
     org.apache.hadoop.hive.ql.metadata.Table tbl = hive.getTable(database, table);
     FetchWork work;
     if (tbl.getPartCols().isEmpty()) {
-      work = new FetchWork(new Path(tbl.getDataLocation()), Utilities.getTableDesc(tbl));
+      work = new FetchWork(tbl.getDataLocation(), Utilities.getTableDesc(tbl));
     } else {
       List<Partition> partitions = hive.getPartitions(tbl);
       List<PartitionDesc> partDesc = new ArrayList<PartitionDesc>();
       List<Path> partLocs = new ArrayList<Path>();
       for (Partition part : partitions) {
-        partLocs.add(part.getPartitionPath());
+        partLocs.add(part.getDataLocation());
         partDesc.add(Utilities.getPartitionDesc(part));
       }
       work = new FetchWork(partLocs, partDesc, Utilities.getTableDesc(tbl));
diff --git a/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java b/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java
index 68c77c29ff..7e53a16333 100644
--- a/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java
+++ b/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java
@@ -381,13 +381,13 @@ private List<String> getTableData(String table, String database) throws Exceptio
       List<PartitionDesc> partDesc = new ArrayList<PartitionDesc>();
       List<Path> partLocs = new ArrayList<Path>();
       for (Partition part : partitions) {
-        partLocs.add(part.getPartitionPath());
+        partLocs.add(part.getDataLocation());
         partDesc.add(Utilities.getPartitionDesc(part));
       }
       work = new FetchWork(partLocs, partDesc, Utilities.getTableDesc(tbl));
       work.setLimit(100);
     } else {
-      work = new FetchWork(new Path(tbl.getDataLocation()), Utilities.getTableDesc(tbl));
+      work = new FetchWork(tbl.getDataLocation(), Utilities.getTableDesc(tbl));
     }
     FetchTask task = new FetchTask();
     task.setWork(work);
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
index 5cc4079da7..eb97edc960 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
@@ -22,10 +22,11 @@
 
 public class VerifyOutputTableLocationSchemeIsFileHook implements ExecuteWithHookContext {
 
+  @Override
   public void run(HookContext hookContext) {
     for (WriteEntity output : hookContext.getOutputs()) {
       if (output.getType() == WriteEntity.Type.TABLE) {
-        String scheme = output.getTable().getDataLocation().getScheme();
+        String scheme = output.getTable().getDataLocation().toUri().getScheme();
         Assert.assertTrue(output.getTable().getTableName() + " has a location which has a " +
               "scheme other than file: " + scheme, scheme.equals("file"));
       }
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java
index ce377be1c2..fd512e6124 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsNotSubdirectoryOfTableHook.java
@@ -27,6 +27,7 @@
 // not a subdirectory.
 public class VerifyPartitionIsNotSubdirectoryOfTableHook implements ExecuteWithHookContext {
 
+  @Override
   public void run(HookContext hookContext) {
     for (WriteEntity output : hookContext.getOutputs()) {
       if (output.getType() == WriteEntity.Type.PARTITION) {
@@ -44,6 +45,6 @@ public void run(HookContext hookContext) {
   private void verify(Partition partition, Table table) {
     Assert.assertFalse("The location of the partition: " + partition.getName() + " was a " +
         "subdirectory of the location of the table: " + table.getTableName(),
-        partition.getPartitionPath().toString().startsWith(table.getPath().toString()));
+        partition.getDataLocation().toString().startsWith(table.getPath().toString()));
   }
 }
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java
index 4406036169..dd1064da0a 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java
@@ -26,6 +26,7 @@
 // the location of the table.  It is a very simple check to make sure it is a subdirectory.
 public class VerifyPartitionIsSubdirectoryOfTableHook implements ExecuteWithHookContext {
 
+  @Override
   public void run(HookContext hookContext) {
     for (WriteEntity output : hookContext.getOutputs()) {
       if (output.getType() == WriteEntity.Type.PARTITION) {
@@ -43,6 +44,6 @@ public void run(HookContext hookContext) {
   private void verify(Partition partition, Table table) {
     Assert.assertTrue("The location of the partition: " + partition.getName() + " was not a " +
         "subdirectory of the location of the table: " + table.getTableName(),
-        partition.getPartitionPath().toString().startsWith(table.getPath().toString()));
+        partition.getDataLocation().toString().startsWith(table.getPath().toString()));
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java
index 9608fcd165..598be1150d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java
@@ -108,11 +108,11 @@ public Path createPath(Table tbl) throws HiveException {
       } catch (MetaException e) {
         throw new HiveException("Unable to get partitions directories prefix", e);
       }
-      URI tableDir = tbl.getDataLocation();
+      Path tableDir = tbl.getDataLocation();
       if(tableDir == null) {
         throw new HiveException("Table has no location set");
       }
-      return new Path(tableDir.toString(), prefixSubdir);
+      return new Path(tableDir, prefixSubdir);
     }
     /**
      * Generates name for prefix partial partition specification.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
index 9e4f1c7e44..dc45ea20cd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
@@ -883,8 +883,8 @@ private int alterIndex(Hive db, AlterIndexDesc alterIndex) throws HiveException
             }
             if (baseParts != null) {
               for (Partition p : baseParts) {
-                FileSystem fs = p.getPartitionPath().getFileSystem(db.getConf());
-                FileStatus fss = fs.getFileStatus(p.getPartitionPath());
+                FileSystem fs = p.getDataLocation().getFileSystem(db.getConf());
+                FileStatus fss = fs.getFileStatus(p.getDataLocation());
                 basePartTs.put(p.getSpec(), fss.getModificationTime());
               }
             }
@@ -1212,12 +1212,12 @@ boolean partitionInCustomLocation(Table tbl, Partition p)
     } catch (MetaException e) {
       throw new HiveException("Unable to get partition's directory", e);
     }
-    URI tableDir = tbl.getDataLocation();
+    Path tableDir = tbl.getDataLocation();
     if(tableDir == null) {
       throw new HiveException("Table has no location set");
     }
 
-    String standardLocation = (new Path(tableDir.toString(), subdir)).toString();
+    String standardLocation = (new Path(tableDir, subdir)).toString();
     if(ArchiveUtils.isArchived(p)) {
       return !getOriginalLocation(p).equals(standardLocation);
     } else {
@@ -1266,7 +1266,7 @@ private int archive(Hive db, AlterTableSimpleDesc simpleDesc,
       if(ArchiveUtils.isArchived(p)) {
         originalDir = new Path(getOriginalLocation(p));
       } else {
-        originalDir = p.getPartitionPath();
+        originalDir = p.getDataLocation();
       }
     }
 
@@ -1418,8 +1418,8 @@ private int archive(Hive db, AlterTableSimpleDesc simpleDesc,
     // Record this change in the metastore
     try {
       for(Partition p: partitions) {
-        URI originalPartitionUri = ArchiveUtils.addSlash(p.getPartitionPath().toUri());
-        URI test = p.getPartitionPath().toUri();
+        URI originalPartitionUri = ArchiveUtils.addSlash(p.getDataLocation().toUri());
+        URI test = p.getDataLocation().toUri();
         URI harPartitionDir = harHelper.getHarUri(originalPartitionUri, shim);
         StringBuilder authority = new StringBuilder();
         if(harPartitionDir.getUserInfo() != null) {
@@ -3204,7 +3204,7 @@ private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {
         if (part != null) {
           part.setLocation(newLocation);
         } else {
-          tbl.setDataLocation(locUri);
+          tbl.setDataLocation(new Path(locUri));
         }
       } catch (URISyntaxException e) {
         throw new HiveException(e);
@@ -3661,7 +3661,7 @@ private int createTable(Hive db, CreateTableDesc crtTbl) throws HiveException {
       tbl.setProperty("comment", crtTbl.getComment());
     }
     if (crtTbl.getLocation() != null) {
-      tbl.setDataLocation(new Path(crtTbl.getLocation()).toUri());
+      tbl.setDataLocation(new Path(crtTbl.getLocation()));
     }
 
     if (crtTbl.getSkewedColNames() != null) {
@@ -3799,7 +3799,7 @@ private int createTableLike(Hive db, CreateTableLikeDesc crtTbl) throws HiveExce
       tbl.setTableName(newTable.getTableName());
 
       if (crtTbl.getLocation() != null) {
-        tbl.setDataLocation(new Path(crtTbl.getLocation()).toUri());
+        tbl.setDataLocation(new Path(crtTbl.getLocation()));
       } else {
         tbl.unsetDataLocation();
       }
@@ -3953,14 +3953,14 @@ private List<Path> getLocations(Hive db, Table table, Map<String, String> partSp
     if (partSpec == null) {
       if (table.isPartitioned()) {
         for (Partition partition : db.getPartitions(table)) {
-          locations.add(partition.getPartitionPath());
+          locations.add(partition.getDataLocation());
         }
       } else {
         locations.add(table.getPath());
       }
     } else {
       for (Partition partition : db.getPartitionsByNames(table, partSpec)) {
-        locations.add(partition.getPartitionPath());
+        locations.add(partition.getDataLocation());
       }
     }
     return locations;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index c44e9da0f8..ed7787d7d8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -251,7 +251,7 @@ public int execute(DriverContext driverContext) {
           ArrayList<FileStatus> files;
           FileSystem fs;
           try {
-            fs = FileSystem.get(table.getDataLocation(), conf);
+            fs = table.getDataLocation().getFileSystem(conf);
             dirs = fs.globStatus(tbd.getSourcePath());
             files = new ArrayList<FileStatus>();
             for (int i = 0; (dirs != null && i < dirs.length); i++) {
@@ -460,9 +460,9 @@ private void updatePartitionBucketSortColumns(Table table, Partition partn,
 
     boolean updateBucketCols = false;
     if (bucketCols != null) {
-      FileSystem fileSys = partn.getPartitionPath().getFileSystem(conf);
+      FileSystem fileSys = partn.getDataLocation().getFileSystem(conf);
       FileStatus[] fileStatus = HiveStatsUtils.getFileStatusRecurse(
-          partn.getPartitionPath(), 1, fileSys);
+          partn.getDataLocation(), 1, fileSys);
       // Verify the number of buckets equals the number of files
       // This will not hold for dynamic partitions where not every reducer produced a file for
       // those partitions.  In this case the table is not bucketed as Hive requires a files for
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java b/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java
index f57feb9a53..637b3aef5e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java
@@ -23,8 +23,8 @@
 import java.util.Map;
 
 import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.DummyPartition;
+import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 
 /**
@@ -236,11 +236,11 @@ public URI getLocation() throws Exception {
     }
 
     if (typ == Type.TABLE) {
-      return t.getDataLocation();
+      return t.getDataLocation().toUri();
     }
 
     if (typ == Type.PARTITION) {
-      return p.getDataLocation();
+      return p.getDataLocation().toUri();
     }
 
     if (typ == Type.DFS_DIR || typ == Type.LOCAL_DIR) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/IndexMetadataChangeTask.java b/ql/src/java/org/apache/hadoop/hive/ql/index/IndexMetadataChangeTask.java
index 364fc198b2..1e0100106a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/IndexMetadataChangeTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/IndexMetadataChangeTask.java
@@ -23,7 +23,6 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.metastore.TableType;
 import org.apache.hadoop.hive.metastore.Warehouse;
-import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.DriverContext;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.metadata.Hive;
@@ -67,9 +66,9 @@ protected int execute(DriverContext driverContext) {
           return 1;
         }
 
-        Path url = new Path(part.getPartitionPath().toString());
-        FileSystem fs = url.getFileSystem(conf);
-        FileStatus fstat = fs.getFileStatus(url);
+        Path path = part.getDataLocation();
+        FileSystem fs = path.getFileSystem(conf);
+        FileStatus fstat = fs.getFileStatus(path);
 
         part.getParameters().put(HiveIndex.INDEX_TABLE_CREATETIME, Long.toString(fstat.getModificationTime()));
         db.alterPartition(tbl.getTableName(), part);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
index 2fe86e1e91..441f329939 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
@@ -1195,13 +1195,13 @@ public void loadPartition(Path loadPath, String tableName,
       Partition oldPart = getPartition(tbl, partSpec, false);
       Path oldPartPath = null;
       if(oldPart != null) {
-        oldPartPath = oldPart.getPartitionPath();
+        oldPartPath = oldPart.getDataLocation();
       }
 
       Path newPartPath = null;
 
       if (inheritTableSpecs) {
-        Path partPath = new Path(tbl.getDataLocation().getPath(),
+        Path partPath = new Path(tbl.getDataLocation(),
             Warehouse.makePartPath(partSpec));
         newPartPath = new Path(loadPath.toUri().getScheme(), loadPath.toUri().getAuthority(),
             partPath.toUri().getPath());
@@ -1227,7 +1227,7 @@ public void loadPartition(Path loadPath, String tableName,
       if (replace) {
         Hive.replaceFiles(loadPath, newPartPath, oldPartPath, getConf());
       } else {
-        FileSystem fs = FileSystem.get(tbl.getDataLocation(), getConf());
+        FileSystem fs = tbl.getDataLocation().getFileSystem(conf);
         Hive.copyFiles(conf, loadPath, newPartPath, fs);
       }
 
@@ -2395,6 +2395,7 @@ public void exchangeTablePartitions(Map<String, String> partitionSpecs,
   private IMetaStoreClient createMetaStoreClient() throws MetaException {
 
     HiveMetaHookLoader hookLoader = new HiveMetaHookLoader() {
+        @Override
         public HiveMetaHook getHook(
           org.apache.hadoop.hive.metastore.api.Table tbl)
           throws MetaException {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
index 695982f361..7803169bf0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
@@ -254,7 +254,7 @@ void checkTable(Table table, List<Partition> parts,
         // most likely the user specified an invalid partition
         continue;
       }
-      Path partPath = partition.getPartitionPath();
+      Path partPath = partition.getDataLocation();
       fs = partPath.getFileSystem(conf);
       if (!fs.exists(partPath)) {
         PartitionResult pr = new PartitionResult();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
index 83514a2626..849bbf4ba5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hive.ql.metadata;
 
 import java.io.Serializable;
-import java.net.URI;
-import java.net.URISyntaxException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
@@ -75,7 +73,6 @@ public class Partition implements Serializable {
   private Deserializer deserializer;
   private Class<? extends HiveOutputFormat> outputFormatClass;
   private Class<? extends InputFormat> inputFormatClass;
-  private URI uri;
 
   /**
    * @return The values of the partition
@@ -228,35 +225,16 @@ public String getName() {
   }
 
   public Path[] getPath() {
-    Path[] ret = new Path[]{getPartitionPath()};
+    Path[] ret = new Path[]{getDataLocation()};
     return ret;
   }
 
-  public Path getPartitionPath() {
+  public Path getDataLocation() {
     if (table.isPartitioned()) {
       return new Path(tPartition.getSd().getLocation());
     } else {
-
-      /**
-       * Table location string need to be constructed as URI first to decode
-       * the http encoded characters in the location path (because location is
-       * stored as URI in org.apache.hadoop.hive.ql.metadata.Table before saved
-       * to metastore database). This is not necessary for partition location.
-       */
-      try {
-        return new Path(new URI(table.getTTable().getSd().getLocation()));
-      } catch (URISyntaxException e) {
-        throw new RuntimeException("Invalid table path " +
-          table.getTTable().getSd().getLocation(), e);
-      }
-    }
-  }
-
-  final public URI getDataLocation() {
-    if (uri == null) {
-      uri = getPartitionPath().toUri();
+      return new Path(table.getTTable().getSd().getLocation());
     }
-    return uri;
   }
 
   final public Deserializer getDeserializer() {
@@ -392,9 +370,8 @@ public FileStatus[] getSortedPaths() {
     try {
       // Previously, this got the filesystem of the Table, which could be
       // different from the filesystem of the partition.
-      FileSystem fs = FileSystem.get(getPartitionPath().toUri(), Hive.get()
-          .getConf());
-      String pathPattern = getPartitionPath().toString();
+      FileSystem fs = getDataLocation().getFileSystem(Hive.get().getConf());
+      String pathPattern = getDataLocation().toString();
       if (getBucketCount() > 0) {
         pathPattern = pathPattern + "/*";
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
index 0180b8760e..e638ec2bba 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
@@ -20,8 +20,6 @@
 
 import java.io.IOException;
 import java.io.Serializable;
-import java.net.URI;
-import java.net.URISyntaxException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
@@ -37,7 +35,6 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.JavaUtils;
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.ProtectMode;
 import org.apache.hadoop.hive.metastore.TableType;
@@ -84,7 +81,7 @@ public class Table implements Serializable {
   private Deserializer deserializer;
   private Class<? extends HiveOutputFormat> outputFormatClass;
   private Class<? extends InputFormat> inputFormatClass;
-  private URI uri;
+  private Path path;
   private HiveStorageHandler storageHandler;
 
   /**
@@ -245,25 +242,18 @@ final public Path getPath() {
     if (location == null) {
       return null;
     }
-    try {
-      return new Path(new URI(location));
-    } catch (URISyntaxException e) {
-      throw new RuntimeException("Invalid table path " + location, e);
-    }
+    return new Path(location);
   }
 
   final public String getTableName() {
     return tTable.getTableName();
   }
 
-  final public URI getDataLocation() {
-    if (uri == null) {
-      Path path = getPath();
-      if (path != null) {
-        uri = path.toUri();
-      }
+  final public Path getDataLocation() {
+    if (path == null) {
+      path = getPath();
     }
-    return uri;
+    return path;
   }
 
   final public Deserializer getDeserializer() {
@@ -504,13 +494,13 @@ public String getBucketingDimensionId() {
     return bcols.get(0);
   }
 
-  public void setDataLocation(URI uri) {
-    this.uri = uri;
-    tTable.getSd().setLocation(uri.toString());
+  public void setDataLocation(Path path) {
+    this.path = path;
+    tTable.getSd().setLocation(path.toString());
   }
 
   public void unsetDataLocation() {
-    this.uri = null;
+    this.path = null;
     tTable.getSd().unsetLocation();
   }
 
@@ -641,7 +631,7 @@ public int getNumBuckets() {
    *          Source directory
    */
   protected void replaceFiles(Path srcf) throws HiveException {
-    Path tableDest =  new Path(getDataLocation().getPath());
+    Path tableDest = getPath();
     Hive.replaceFiles(srcf, tableDest, tableDest, Hive.get().getConf());
   }
 
@@ -654,8 +644,8 @@ protected void replaceFiles(Path srcf) throws HiveException {
   protected void copyFiles(Path srcf) throws HiveException {
     FileSystem fs;
     try {
-      fs = FileSystem.get(getDataLocation(), Hive.get().getConf());
-      Hive.copyFiles(Hive.get().getConf(), srcf, new Path(getDataLocation().getPath()), fs);
+      fs = getDataLocation().getFileSystem(Hive.get().getConf());
+      Hive.copyFiles(Hive.get().getConf(), srcf, getPath(), fs);
     } catch (IOException e) {
       throw new HiveException("addFiles: filesystem error in check phase", e);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java
index bcd75be4a3..54b02cb4cc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java
@@ -19,14 +19,15 @@
 package org.apache.hadoop.hive.ql.metadata.formatting;
 
 import java.io.DataOutputStream;
-import java.io.OutputStream;
 import java.io.IOException;
+import java.io.OutputStream;
 import java.io.UnsupportedEncodingException;
 import java.net.URLDecoder;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -39,8 +40,6 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-import org.apache.hadoop.hive.shims.ShimLoader;
 import org.codehaus.jackson.map.ObjectMapper;
 
 /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java
index b919d1ab01..9b0d4826d5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java
@@ -42,8 +42,6 @@
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-import org.apache.hadoop.hive.shims.ShimLoader;
 
 /**
  * Format table and index information for human readability using
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java
index 0991847751..2850c7f819 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java
@@ -18,7 +18,6 @@
 package org.apache.hadoop.hive.ql.optimizer;
 
 import java.io.IOException;
-import java.net.URI;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -77,10 +76,10 @@ abstract public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx proc
       Object... nodeOutputs) throws SemanticException;
 
   private static List<String> getBucketFilePathsOfPartition(
-      URI location, ParseContext pGraphContext) throws SemanticException {
+      Path location, ParseContext pGraphContext) throws SemanticException {
     List<String> fileNames = new ArrayList<String>();
     try {
-      FileSystem fs = FileSystem.get(location, pGraphContext.getConf());
+      FileSystem fs = location.getFileSystem(pGraphContext.getConf());
       FileStatus[] files = fs.listStatus(new Path(location.toString()));
       if (files != null) {
         for (FileStatus file : files) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
index 51d56ef5a4..9c8071473a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
@@ -185,9 +185,9 @@ private void handlePartialScanCommand(TableScanOperator op, GenMRProcContext ctx
         aggregationKey += Warehouse.makePartPath(part.getSpec());
       } catch (MetaException e) {
         throw new SemanticException(ErrorMsg.ANALYZE_TABLE_PARTIALSCAN_AGGKEY.getMsg(
-            part.getPartitionPath().toString() + e.getMessage()));
+            part.getDataLocation().toString() + e.getMessage()));
       }
-      inputPaths.add(part.getPartitionPath());
+      inputPaths.add(part.getDataLocation());
       break;
     default:
       assert false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/IndexUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/IndexUtils.java
index 214600930a..f1ef4cec47 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/IndexUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/IndexUtils.java
@@ -165,8 +165,8 @@ private static boolean isIndexPartitionFresh(Hive hive, Index index,
       Partition part) throws HiveException {
     LOG.info("checking index staleness...");
     try {
-      FileSystem partFs = part.getPartitionPath().getFileSystem(hive.getConf());
-      FileStatus partFss = partFs.getFileStatus(part.getPartitionPath());
+      FileSystem partFs = part.getDataLocation().getFileSystem(hive.getConf());
+      FileStatus partFss = partFs.getFileStatus(part.getDataLocation());
       String ts = index.getParameters().get(part.getSpec().toString());
       if (ts == null) {
         return false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SamplePruner.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SamplePruner.java
index ab2ed81bf3..b0f4b47db3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SamplePruner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SamplePruner.java
@@ -319,9 +319,8 @@ public static LimitPruneRetStatus limitPrune(Partition part, long sizeLimit, int
       throws SemanticException {
 
     try {
-      FileSystem fs = FileSystem.get(part.getPartitionPath().toUri(), Hive.get()
-          .getConf());
-      String pathPattern = part.getPartitionPath().toString() + "/*";
+      FileSystem fs = part.getDataLocation().getFileSystem(Hive.get().getConf());
+      String pathPattern = part.getDataLocation().toString() + "/*";
       AddPathReturnStatus ret = addPath(fs, pathPattern, sizeLimit, fileLimit, retPathList);
       if (ret == null) {
         return LimitPruneRetStatus.NotQualify;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
index c73c874c13..5e8dc418a6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
@@ -242,7 +242,7 @@ private FetchWork convertToWork() throws HiveException {
 
       for (Partition partition : partsList.getNotDeniedPartns()) {
         inputs.add(new ReadEntity(partition));
-        listP.add(partition.getPartitionPath());
+        listP.add(partition.getDataLocation());
         partP.add(Utilities.getPartitionDesc(partition));
       }
       Table sourceTable = partsList.getSourceTable();
@@ -296,7 +296,7 @@ private long calculateLength(ParseContext pctx, long remaining) throws Exception
       }
       long total = 0;
       for (Partition partition : partsList.getNotDeniedPartns()) {
-        Path path = partition.getPartitionPath();
+        Path path = partition.getDataLocation();
         total += getFileLength(jobConf, path, partition.getInputFormatClass());
       }
       return total;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SizeBasedBigTableSelectorForAutoSMJ.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SizeBasedBigTableSelectorForAutoSMJ.java
index 1a686a0f5c..f8aec84b8a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SizeBasedBigTableSelectorForAutoSMJ.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SizeBasedBigTableSelectorForAutoSMJ.java
@@ -78,7 +78,7 @@ protected long getSize(HiveConf conf, Table table) {
   }
 
   protected long getSize(HiveConf conf, Partition partition) {
-    Path path = partition.getPartitionPath();
+    Path path = partition.getDataLocation();
     String size = partition.getParameters().get("totalSize");
 
     return getSize(conf, size, path);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 713bd54693..0de3dca6c6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -758,7 +758,7 @@ private void analyzeTruncateTable(ASTNode ast) throws SemanticException {
           Partition part = db.getPartition(table, partSpec, false);
 
           Path tabPath = table.getPath();
-          Path partPath = part.getPartitionPath();
+          Path partPath = part.getDataLocation();
 
           // if the table is in a different dfs than the partition,
           // replace the partition's dfs with the table's dfs.
@@ -1402,7 +1402,7 @@ private void analyzeAlterTablePartMergeFiles(ASTNode tablePartAST, ASTNode ast,
           isArchived = ArchiveUtils.isArchived(part);
 
           Path tabPath = tblObj.getPath();
-          Path partPath = part.getPartitionPath();
+          Path partPath = part.getDataLocation();
 
           // if the table is in a different dfs than the partition,
           // replace the partition's dfs with the table's dfs.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
index be0ad62f40..7b2e2e6302 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
@@ -105,19 +105,19 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
 
     if (ts.tableHandle.isPartitioned()) {
       for (Partition partition : partitions) {
-        URI fromURI = partition.getDataLocation();
+        Path fromPath = partition.getDataLocation();
         Path toPartPath = new Path(parentPath, partition.getName());
         Task<? extends Serializable> rTask = TaskFactory.get(
-            new CopyWork(new Path(fromURI), toPartPath, false),
+            new CopyWork(fromPath, toPartPath, false),
             conf);
         rootTasks.add(rTask);
         inputs.add(new ReadEntity(partition));
       }
     } else {
-      URI fromURI = ts.tableHandle.getDataLocation();
+      Path fromPath = ts.tableHandle.getDataLocation();
       Path toDataPath = new Path(parentPath, "data");
       Task<? extends Serializable> rTask = TaskFactory.get(new CopyWork(
-          new Path(fromURI), toDataPath, false), conf);
+          fromPath, toDataPath, false), conf);
       rootTasks.add(rTask);
       inputs.add(new ReadEntity(ts.tableHandle));
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
index 1ab5a60b79..8ed5b70f47 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
@@ -399,7 +399,7 @@ private static void checkTable(Table table, CreateTableDesc tableDesc)
         if (tableDesc.getLocation() != null) { // IMPORT statement specified
                                                // location
           if (!table.getDataLocation()
-              .equals(new URI(tableDesc.getLocation()))) {
+              .equals(new Path(tableDesc.getLocation()))) {
             throw new SemanticException(
                 ErrorMsg.INCOMPATIBLE_SCHEMA.getMsg(" Location does not match"));
           }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
index 5663fca058..a22a15f9bb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
@@ -206,8 +206,8 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
       throw new SemanticException(ErrorMsg.LOAD_INTO_STORED_AS_DIR.getMsg());
     }
 
-    URI toURI = (ts.partHandle != null) ? ts.partHandle.getDataLocation()
-        : ts.tableHandle.getDataLocation();
+    URI toURI = ((ts.partHandle != null) ? ts.partHandle.getDataLocation()
+        : ts.tableHandle.getDataLocation()).toUri();
 
     List<FieldSchema> parts = ts.tableHandle.getPartitionKeys();
     if ((parts != null && parts.size() > 0)
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 8e68fcfc7c..52d7c75274 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -5418,7 +5418,7 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)
       }
 
       Path tabPath = dest_tab.getPath();
-      Path partPath = dest_part.getPartitionPath();
+      Path partPath = dest_part.getDataLocation();
 
       // if the table is in a different dfs than the partition,
       // replace the partition's dfs with the table's dfs.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java
index 55576974f6..a0cd232a70 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java
@@ -174,7 +174,7 @@ private void authorize(Table table, Partition part, Privilege[] readRequiredPriv
     if ((part == null) || (part.getLocation() == null)) {
       authorize(table, readRequiredPriv, writeRequiredPriv);
     } else {
-      authorize(part.getPartitionPath(), readRequiredPriv, writeRequiredPriv);
+      authorize(part.getDataLocation(), readRequiredPriv, writeRequiredPriv);
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
index e89e3a4165..41e237fcde 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
@@ -305,7 +305,7 @@ public static long getFileSizeForTable(HiveConf conf, Table table) {
   public static List<Long> getFileSizeForPartitions(HiveConf conf, List<Partition> parts) {
     List<Long> sizes = Lists.newArrayList();
     for (Partition part : parts) {
-      Path path = part.getPartitionPath();
+      Path path = part.getDataLocation();
       long size = 0;
       try {
         FileSystem fs = path.getFileSystem(conf);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java
index 69d18967e4..e453f569c4 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java
@@ -165,7 +165,7 @@ public void testTableCheck() throws HiveException, MetaException,
     Path fakeTable = table.getPath().getParent().suffix(
         Path.SEPARATOR + "faketable");
     fs.mkdirs(fakeTable);
-    fs.deleteOnExit(fakeTable);    
+    fs.deleteOnExit(fakeTable);
 
     // find the extra table
     result = new CheckResult();
@@ -221,7 +221,7 @@ public void testPartitionsCheck() throws HiveException, MetaException,
     List<Partition> partitions = hive.getPartitions(table);
     assertEquals(2, partitions.size());
     Partition partToRemove = partitions.get(0);
-    Path partToRemovePath = new Path(partToRemove.getDataLocation().toString());
+    Path partToRemovePath = partToRemove.getDataLocation();
     fs = partToRemovePath.getFileSystem(hive.getConf());
     fs.delete(partToRemovePath, true);
 
@@ -298,8 +298,8 @@ public void testDataDeletion() throws HiveException, MetaException,
         Path.SEPARATOR + "faketable");
     fs = fakeTable.getFileSystem(hive.getConf());
     fs.mkdirs(fakeTable);
-    fs.deleteOnExit(fakeTable);    
-    
+    fs.deleteOnExit(fakeTable);
+
     Path fakePart = new Path(table.getDataLocation().toString(),
         "fakepartition=fakevalue");
     fs.mkdirs(fakePart);
