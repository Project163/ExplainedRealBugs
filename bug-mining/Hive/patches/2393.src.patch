diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java
index 01a70e94a0..67910f48b3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java
@@ -31,7 +31,7 @@
 public class HiveMapFunction implements PairFlatMapFunction<Iterator<Tuple2<BytesWritable, BytesWritable>>,
 BytesWritable, BytesWritable> {
   private static final long serialVersionUID = 1L;
-  
+
   private transient ExecMapper mapper;
   private transient SparkCollector collector;
   private transient JobConf jobConf;
@@ -58,9 +58,9 @@ public HiveMapFunction(byte[] buffer) {
       System.out.println("mapper input: " + input._1() + ", " + input._2());
       mapper.map(input._1(), input._2(), collector, Reporter.NULL);
     }
-    
+
     mapper.close();
     return collector.getResult();
   }
-  
+
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
index 841db87337..713e327e7c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
@@ -35,7 +35,7 @@
 public class HiveReduceFunction implements PairFlatMapFunction<Iterator<Tuple2<BytesWritable,BytesWritable>>,
 BytesWritable, BytesWritable> {
   private static final long serialVersionUID = 1L;
-  
+
   private transient ExecReducer reducer;
   private transient SparkCollector collector;
   private transient JobConf jobConf;
@@ -52,7 +52,7 @@ public Iterable<Tuple2<BytesWritable, BytesWritable>> call(Iterator<Tuple2<Bytes
     if (jobConf == null) {
       jobConf = KryoSerializer.deserializeJobConf(this.buffer);
       jobConf.set("mapred.reducer.class", ExecReducer.class.getName());      
- 
+
       reducer = new ExecReducer();
       reducer.configure(jobConf);
       collector = new SparkCollector();
@@ -74,14 +74,14 @@ public Iterable<Tuple2<BytesWritable, BytesWritable>> call(Iterator<Tuple2<Bytes
       }
       valueList.add(value);
     }
-    
+
     for (Map.Entry<BytesWritable, List<BytesWritable>> entry : clusteredRows.entrySet()) {
       // pass on the clustered result to the reducer operator tree.
       reducer.reduce(entry.getKey(), entry.getValue().iterator(), collector, Reporter.NULL);
     }
-    
+
     reducer.close();
     return collector.getResult();
   }
-  
+
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveVoidFunction.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveVoidFunction.java
index 38f09e478c..c66cd80364 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveVoidFunction.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveVoidFunction.java
@@ -29,13 +29,13 @@
  */
 public class HiveVoidFunction implements VoidFunction<Tuple2<BytesWritable, BytesWritable>> {
   private static final long serialVersionUID = 1L;
-  
+
   private static HiveVoidFunction instance = new HiveVoidFunction();
 
   public static HiveVoidFunction getInstance() {
     return instance;
   }
-  
+
   private HiveVoidFunction() {
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/KryoSerializer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/KryoSerializer.java
index b088b3f73a..20a19389cd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/KryoSerializer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/KryoSerializer.java
@@ -73,7 +73,7 @@ public static byte[] serializeJobConf(JobConf jobConf) {
     return out.toByteArray();
 
   }
-  
+
   public static JobConf deserializeJobConf(byte[] buffer) {
     JobConf conf = new JobConf();
     try {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/MapTran.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/MapTran.java
index 98d08e6927..b03a51c78b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/MapTran.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/MapTran.java
@@ -22,16 +22,16 @@
 import org.apache.spark.api.java.JavaPairRDD;
 
 public class MapTran implements SparkTran {
-	private HiveMapFunction mapFunc;
+  private HiveMapFunction mapFunc;
 
-	@Override
-	public JavaPairRDD<BytesWritable, BytesWritable> transform(
-			JavaPairRDD<BytesWritable, BytesWritable> input) {
-		return input.mapPartitionsToPair(mapFunc);
-	}
+  @Override
+  public JavaPairRDD<BytesWritable, BytesWritable> transform(
+      JavaPairRDD<BytesWritable, BytesWritable> input) {
+    return input.mapPartitionsToPair(mapFunc);
+  }
 
-	public void setMapFunction(HiveMapFunction mapFunc) {
-		this.mapFunc = mapFunc;
-	}
+  public void setMapFunction(HiveMapFunction mapFunc) {
+    this.mapFunc = mapFunc;
+  }
 
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ReduceTran.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ReduceTran.java
index d1af86d370..eb925b3c7b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ReduceTran.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ReduceTran.java
@@ -23,15 +23,15 @@
 
 public class ReduceTran implements SparkTran {
   private HiveReduceFunction reduceFunc;
-  
-	@Override
-	public JavaPairRDD<BytesWritable, BytesWritable> transform(
-			JavaPairRDD<BytesWritable, BytesWritable> input) {
-		return input.mapPartitionsToPair(reduceFunc);
-	}
 
-	public void setReduceFunction(HiveReduceFunction redFunc) {
-		this.reduceFunc = redFunc;
-	}
+  @Override
+  public JavaPairRDD<BytesWritable, BytesWritable> transform(
+      JavaPairRDD<BytesWritable, BytesWritable> input) {
+    return input.mapPartitionsToPair(reduceFunc);
+  }
+
+  public void setReduceFunction(HiveReduceFunction redFunc) {
+    this.reduceFunc = redFunc;
+  }
 
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ShuffleTran.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ShuffleTran.java
index 33e7d451e7..dee9fa6e85 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ShuffleTran.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ShuffleTran.java
@@ -24,10 +24,10 @@
 
 public class ShuffleTran implements SparkTran {
 
-	@Override
-	public JavaPairRDD<BytesWritable, BytesWritable> transform(
-			JavaPairRDD<BytesWritable, BytesWritable> input) {
-		return input.partitionBy(new HashPartitioner(1));
-	}
+  @Override
+  public JavaPairRDD<BytesWritable, BytesWritable> transform(
+      JavaPairRDD<BytesWritable, BytesWritable> input) {
+    return input.partitionBy(new HashPartitioner(1));
+  }
 
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java
index 88206a7806..d5e29d6346 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java
@@ -50,14 +50,14 @@ public class SparkClient implements Serializable {
   private static final String SAPRK_DEFAULT_APP_NAME = "Hive on Spark";
 
   private static SparkClient client;
-  
+
   public static synchronized SparkClient getInstance(Configuration hiveConf) {
     if (client == null) {
       client = new SparkClient(hiveConf);
     }
     return client;
   }
-  
+
   private JavaSparkContext sc;
 
   private List<String> localJars = new ArrayList<String>();
@@ -89,7 +89,7 @@ private SparkConf initiateSparkConf(Configuration hiveConf) {
             String value = properties.getProperty(propertyName);
             sparkConf.set(propertyName, properties.getProperty(propertyName));
             LOG.info(String.format("load spark configuration from %s (%s -> %s).",
-              SPARK_DEFAULT_CONF_FILE, propertyName, value));
+                SPARK_DEFAULT_CONF_FILE, propertyName, value));
           }
         }
       }
@@ -114,7 +114,7 @@ private SparkConf initiateSparkConf(Configuration hiveConf) {
         String value = entry.getValue();
         sparkConf.set(propertyName, value);
         LOG.info(String.format("load spark configuration from hive configuration (%s -> %s).",
-          propertyName, value));
+            propertyName, value));
       }
     }
 
@@ -144,7 +144,7 @@ public int execute(DriverContext driverContext, SparkWork sparkWork) {
     } catch (IOException e) {
       e.printStackTrace();
       System.err.println("Error launching map-reduce job" + "\n"
-        + org.apache.hadoop.util.StringUtils.stringifyException(e));
+          + org.apache.hadoop.util.StringUtils.stringifyException(e));
       return 5;
     }
 
@@ -166,7 +166,7 @@ public int execute(DriverContext driverContext, SparkWork sparkWork) {
     } catch (IOException e1) {
       e1.printStackTrace();
     }
-/*
+    /*
     try {
       Path planPath = new Path(jobConf.getWorkingDirectory(), "plan.xml");
       System.out.println("Serializing plan to path: " + planPath);
@@ -177,8 +177,8 @@ public int execute(DriverContext driverContext, SparkWork sparkWork) {
       e1.printStackTrace();
       return 1;
     }
-*/  
-/*    JavaPairRDD rdd = createRDD(sc, jobConf, mapWork);
+     */  
+    /*    JavaPairRDD rdd = createRDD(sc, jobConf, mapWork);
     byte[] confBytes = KryoSerializer.serializeJobConf(jobConf);
     HiveMapFunction mf = new HiveMapFunction(confBytes);
     JavaPairRDD rdd2 = rdd.mapPartitionsToPair(mf);
@@ -206,7 +206,7 @@ public int execute(DriverContext driverContext, SparkWork sparkWork) {
         e.printStackTrace();
       }
     }
-*/ 
+     */ 
     SparkPlanGenerator gen = new SparkPlanGenerator(sc, ctx, jobConf, emptyScratchDir);
     SparkPlan plan;
     try {
@@ -219,7 +219,7 @@ public int execute(DriverContext driverContext, SparkWork sparkWork) {
     plan.execute();
     return 0;
   }
-  
+
   private void refreshLocalResources(SparkWork sparkWork, HiveConf conf) {
     // add hive-exec jar
     String hiveJar = conf.getJar();
@@ -257,7 +257,7 @@ private void refreshLocalResources(SparkWork sparkWork, HiveConf conf) {
     if (newTmpJars != null && newTmpJars.length > 0) {
       for (String tmpJar : newTmpJars) {
         if (StringUtils.isNotEmpty(tmpJar) && StringUtils.isNotBlank(tmpJar)
-          && !localJars.contains(tmpJar)) {
+            && !localJars.contains(tmpJar)) {
           localJars.add(tmpJar);
           sc.addJar(tmpJar);
         }
@@ -282,7 +282,7 @@ private void refreshLocalResources(SparkWork sparkWork, HiveConf conf) {
   private void addResources(String addedFiles, List<String> localCache) {
     for (String addedFile : addedFiles.split(",")) {
       if (StringUtils.isNotEmpty(addedFile) && StringUtils.isNotBlank(addedFile)
-        && !localCache.contains(addedFile)) {
+          && !localCache.contains(addedFile)) {
         localCache.add(addedFile);
         sc.addFile(addedFile);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkCollector.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkCollector.java
index cf09a5af6e..3894630a43 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkCollector.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkCollector.java
@@ -32,19 +32,19 @@ public class SparkCollector implements OutputCollector<BytesWritable, BytesWrita
   private static final long serialVersionUID = 1L;
 
   private List<Tuple2<BytesWritable, BytesWritable>> result = new ArrayList<Tuple2<BytesWritable, BytesWritable>>();
-  
+
   @Override
   public void collect(BytesWritable key, BytesWritable value) throws IOException {
     result.add(new Tuple2<BytesWritable, BytesWritable>(copyBytesWritable(key), copyBytesWritable(value)));
   }
-  
+
   // TODO: Move this to a utility class.
   public static BytesWritable copyBytesWritable(BytesWritable bw) {
     BytesWritable copy = new BytesWritable();
     copy.set(bw);
     return copy;
   }
-  
+
   public void clear() {
     result.clear();
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlan.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlan.java
index cf85af1a76..b24f3d0e0e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlan.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlan.java
@@ -22,27 +22,27 @@
 import org.apache.spark.api.java.JavaPairRDD;
 
 public class SparkPlan {
-	private JavaPairRDD<BytesWritable, BytesWritable> input;
-	private SparkTran tran;
-	
-	public void execute() {
-		JavaPairRDD<BytesWritable, BytesWritable> rdd = tran.transform(input);
-		rdd.foreach(HiveVoidFunction.getInstance());
-	}
-
-	public SparkTran getTran() {
-		return tran;
-	}
-
-	public void setTran(SparkTran tran) {
-		this.tran = tran;
-	}
-
-	public JavaPairRDD<BytesWritable, BytesWritable> getInput() {
-		return input;
-	}
-
-	public void setInput(JavaPairRDD<BytesWritable, BytesWritable> input) {
-		this.input = input;
-	}
+  private JavaPairRDD<BytesWritable, BytesWritable> input;
+  private SparkTran tran;
+
+  public void execute() {
+    JavaPairRDD<BytesWritable, BytesWritable> rdd = tran.transform(input);
+    rdd.foreach(HiveVoidFunction.getInstance());
+  }
+
+  public SparkTran getTran() {
+    return tran;
+  }
+
+  public void setTran(SparkTran tran) {
+    this.tran = tran;
+  }
+
+  public JavaPairRDD<BytesWritable, BytesWritable> getInput() {
+    return input;
+  }
+
+  public void setInput(JavaPairRDD<BytesWritable, BytesWritable> input) {
+    this.input = input;
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
index f78963755f..fb2559641e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
@@ -46,7 +46,7 @@ public int execute(DriverContext driverContext) {
     }
     return rc;
   }
-  
+
   /**
    * close will move the temp files into the right place for the fetch
    * task. If the job has failed it will clean up the files.
@@ -64,7 +64,7 @@ private int close(int rc) {
       if (rc == 0) {
         rc = 3;
         String mesg = "Job Commit failed with exception '"
-          + Utilities.getNameMessage(e) + "'";
+            + Utilities.getNameMessage(e) + "'";
         console.printError(mesg, "\n" + StringUtils.stringifyException(e));
       }
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTran.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTran.java
index 6aa732fe69..19894b0a3c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTran.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTran.java
@@ -1,3 +1,21 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.hadoop.hive.ql.exec.spark;
 
 import org.apache.hadoop.io.BytesWritable;
@@ -5,5 +23,5 @@
 
 public interface SparkTran {
   JavaPairRDD<BytesWritable, BytesWritable> transform(
-		  JavaPairRDD<BytesWritable, BytesWritable> input);
+      JavaPairRDD<BytesWritable, BytesWritable> input);
 }
