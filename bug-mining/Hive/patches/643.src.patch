diff --git a/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/base64/Base64TextOutputFormat.java b/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/base64/Base64TextOutputFormat.java
index c9eb36796d..5d0cdd41b4 100644
--- a/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/base64/Base64TextOutputFormat.java
+++ b/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/base64/Base64TextOutputFormat.java
@@ -36,13 +36,13 @@
 
 /**
  * FileOutputFormat for base64 encoded text files.
- * 
+ *
  * Each line is a base64-encoded record. The key is a LongWritable which is the
  * offset. The value is a BytesWritable containing the base64-decoded bytes.
- * 
+ *
  * This class accepts a configurable parameter:
  * "base64.text.output.format.signature"
- * 
+ *
  * The UTF-8 encoded signature will be prepended to each BytesWritable before we
  * do base64 encoding.
  */
@@ -75,8 +75,8 @@ public void write(Writable w) throws IOException {
         inputLength = ((Text) w).getLength();
       } else {
         assert (w instanceof BytesWritable);
-        input = ((BytesWritable) w).get();
-        inputLength = ((BytesWritable) w).getSize();
+        input = ((BytesWritable) w).getBytes();
+        inputLength = ((BytesWritable) w).getLength();
       }
 
       // Add signature
diff --git a/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/TypedBytesSerDe.java b/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/TypedBytesSerDe.java
index e6b1289233..e37c5ea552 100644
--- a/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/TypedBytesSerDe.java
+++ b/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/TypedBytesSerDe.java
@@ -40,11 +40,11 @@
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
@@ -165,7 +165,7 @@ public Class<? extends Writable> getSerializedClass() {
   public Object deserialize(Writable blob) throws SerDeException {
 
     BytesWritable data = (BytesWritable) blob;
-    inBarrStr.reset(data.get(), 0, data.getSize());
+    inBarrStr.reset(data.getBytes(), 0, data.getLength());
 
     try {
 
diff --git a/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/s3/S3LogDeserializer.java b/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/s3/S3LogDeserializer.java
index 23c0d4e220..0f6979cc68 100644
--- a/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/s3/S3LogDeserializer.java
+++ b/contrib/src/java/org/apache/hadoop/hive/contrib/serde2/s3/S3LogDeserializer.java
@@ -137,7 +137,7 @@ public Object deserialize(Writable field) throws SerDeException {
     if (field instanceof BytesWritable) {
       BytesWritable b = (BytesWritable) field;
       try {
-        row = Text.decode(b.get(), 0, b.getSize());
+        row = Text.decode(b.getBytes(), 0, b.getLength());
       } catch (CharacterCodingException e) {
         throw new SerDeException(e);
       }
diff --git a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordWriter.java b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordWriter.java
index 29c8ad0b16..3c29d47e09 100644
--- a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordWriter.java
+++ b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordWriter.java
@@ -41,7 +41,7 @@ public void initialize(OutputStream out, Configuration conf)
 
   public void write(Writable row) throws IOException {
     BytesWritable brow = (BytesWritable) row;
-    out.write(brow.get(), 0, brow.getSize());
+    out.write(brow.getBytes(), 0, brow.getLength());
   }
 
   public void close() throws IOException {
diff --git a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritable.java b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritable.java
index f181cef774..4685a938e0 100644
--- a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritable.java
+++ b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritable.java
@@ -57,7 +57,7 @@ public void setValue(Object obj) {
   /** Get the typed bytes as a Java object. */
   public Object getValue() {
     try {
-      ByteArrayInputStream bais = new ByteArrayInputStream(get());
+      ByteArrayInputStream bais = new ByteArrayInputStream(getBytes());
       TypedBytesInput tbi = TypedBytesInput.get(new DataInputStream(bais));
       Object obj = tbi.read();
       return obj;
@@ -68,7 +68,7 @@ public Object getValue() {
 
   /** Get the type code embedded in the first byte. */
   public Type getType() {
-    byte[] bytes = get();
+    byte[] bytes = getBytes();
     if (bytes == null || bytes.length == 0) {
       return null;
     }
diff --git a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
index 7bcad61fcf..7e8d5af2b5 100644
--- a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
+++ b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
@@ -149,11 +149,11 @@ public void write(Writable w) throws IOException {
   }
 
   public void writeTypedBytes(TypedBytesWritable tbw) throws IOException {
-    out.writeRaw(tbw.get(), 0, tbw.getSize());
+    out.writeRaw(tbw.getBytes(), 0, tbw.getLength());
   }
 
   public void writeBytes(BytesWritable bw) throws IOException {
-    byte[] bytes = Arrays.copyOfRange(bw.get(), 0, bw.getSize());
+    byte[] bytes = Arrays.copyOfRange(bw.getBytes(), 0, bw.getLength());
     out.writeBytes(bytes);
   }
 
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/ByteStreamTypedSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/ByteStreamTypedSerDe.java
index b2c3e2acc8..fff35354f0 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/ByteStreamTypedSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/ByteStreamTypedSerDe.java
@@ -42,7 +42,7 @@ public ByteStreamTypedSerDe(Type objectType) throws SerDeException {
   public Object deserialize(Writable field) throws SerDeException {
     Object retObj = super.deserialize(field);
     BytesWritable b = (BytesWritable) field;
-    bis.reset(b.get(), b.getSize());
+    bis.reset(b.getBytes(), b.getLength());
     return (retObj);
   }
 
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/MetadataTypedColumnsetSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/MetadataTypedColumnsetSerDe.java
index 815e4c57b5..aa23797e03 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/MetadataTypedColumnsetSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/MetadataTypedColumnsetSerDe.java
@@ -30,10 +30,10 @@
 import org.apache.hadoop.hive.serde.Constants;
 import org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
@@ -172,7 +172,7 @@ public Object deserialize(Writable field) throws SerDeException {
     if (field instanceof BytesWritable) {
       BytesWritable b = (BytesWritable) field;
       try {
-        row = Text.decode(b.get(), 0, b.getSize());
+        row = Text.decode(b.getBytes(), 0, b.getLength());
       } catch (CharacterCodingException e) {
         throw new SerDeException(e);
       }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java
index 96f65f554b..6ea4a2b6df 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java
@@ -40,10 +40,10 @@
 import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
@@ -159,7 +159,7 @@ public ObjectInspector getObjectInspector() throws SerDeException {
   @Override
   public Object deserialize(Writable blob) throws SerDeException {
     BytesWritable data = (BytesWritable) blob;
-    inputByteBuffer.reset(data.get(), 0, data.getSize());
+    inputByteBuffer.reset(data.getBytes(), 0, data.getLength());
 
     try {
       for (int i = 0; i < columnNames.size(); i++) {
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDe.java
index c778f2316e..166df2419a 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDe.java
@@ -152,7 +152,7 @@ public Object deserialize(Writable field) throws SerDeException {
         bis_.reset(b.getBytes(), b.getLength());
       } else {
         BytesWritable b = (BytesWritable) field;
-        bis_.reset(b.get(), b.getSize());
+        bis_.reset(b.getBytes(), b.getLength());
       }
       deserializeReuse = bt.deserialize(deserializeReuse, iprot_);
       return deserializeReuse;
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
index de26150f0c..0036a8ea9b 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
@@ -35,11 +35,11 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
@@ -278,8 +278,8 @@ public Object deserialize(Writable field) throws SerDeException {
     if (field instanceof BytesWritable) {
       BytesWritable b = (BytesWritable) field;
       // For backward-compatibility with hadoop 0.17
-      byteArrayRef.setData(b.get());
-      cachedLazyStruct.init(byteArrayRef, 0, b.getSize());
+      byteArrayRef.setData(b.getBytes());
+      cachedLazyStruct.init(byteArrayRef, 0, b.getLength());
     } else if (field instanceof Text) {
       Text t = (Text) field;
       byteArrayRef.setData(t.getBytes());
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java
index 5f31d0c1cb..fa34246077 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java
@@ -29,18 +29,18 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.serde.Constants;
 import org.apache.hadoop.hive.serde2.ByteStream;
-import org.apache.hadoop.hive.serde2.ByteStream.Output;
 import org.apache.hadoop.hive.serde2.SerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeStats;
+import org.apache.hadoop.hive.serde2.ByteStream.Output;
 import org.apache.hadoop.hive.serde2.lazy.ByteArrayRef;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
@@ -152,12 +152,12 @@ public Object deserialize(Writable field) throws SerDeException {
     }
     if (field instanceof BytesWritable) {
       BytesWritable b = (BytesWritable) field;
-      if (b.getSize() == 0) {
+      if (b.getLength() == 0) {
         return null;
       }
       // For backward-compatibility with hadoop 0.17
-      byteArrayRef.setData(b.get());
-      cachedLazyBinaryStruct.init(byteArrayRef, 0, b.getSize());
+      byteArrayRef.setData(b.getBytes());
+      cachedLazyBinaryStruct.init(byteArrayRef, 0, b.getLength());
     } else if (field instanceof Text) {
       Text t = (Text) field;
       if (t.getLength() == 0) {
