diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index f5e5974b6b..cf80a6cd66 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -3000,6 +3000,11 @@ public static enum ConfVars {
         -1f, "The customized fraction of JVM memory which Tez will reserve for the processor"),
     // The default is different on the client and server, so it's null here.
     LLAP_IO_ENABLED("hive.llap.io.enabled", null, "Whether the LLAP IO layer is enabled."),
+    LLAP_IO_TRACE_SIZE("hive.llap.io.trace.size", "2Mb",
+        new SizeValidator(0L, true, (long)Integer.MAX_VALUE, false),
+        "The buffer size for a per-fragment LLAP debug trace. 0 to disable."),
+    LLAP_IO_TRACE_ALWAYS_DUMP("hive.llap.io.trace.always.dump", true, // TODO#
+        "Whether to always dump the LLAP IO trace (if enabled); the default is on error."),
     LLAP_IO_NONVECTOR_WRAPPER_ENABLED("hive.llap.io.nonvector.wrapper.enabled", true,
         "Whether the LLAP IO layer is enabled for non-vectorized queries that read inputs\n" +
         "that can be vectorized"),
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java
index 53c9bae5c1..35b9d1f942 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java
@@ -58,11 +58,14 @@
 import org.apache.hadoop.hive.llap.metrics.LlapDaemonIOMetrics;
 import org.apache.hadoop.hive.llap.metrics.MetricsUtils;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.metrics2.util.MBeans;
+import org.apache.hive.common.util.FixedSizedObjectPool;
+
 
 import com.google.common.primitives.Ints;
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
@@ -184,11 +187,12 @@ private LlapIoImpl(Configuration conf) throws IOException {
         0L, TimeUnit.MILLISECONDS,
         new LinkedBlockingQueue<Runnable>(),
         new ThreadFactoryBuilder().setNameFormat("IO-Elevator-Thread-%d").setDaemon(true).build());
+    FixedSizedObjectPool<IoTrace> tracePool = IoTrace.createTracePool(conf);
     // TODO: this should depends on input format and be in a map, or something.
     this.orcCvp = new OrcColumnVectorProducer(
-        metadataCache, cache, bufferManagerOrc, conf, cacheMetrics, ioMetrics);
+        metadataCache, cache, bufferManagerOrc, conf, cacheMetrics, ioMetrics, tracePool);
     this.genericCvp = isEncodeEnabled ? new GenericColumnVectorProducer(
-        serdeCache, bufferManagerGeneric, conf, cacheMetrics, ioMetrics) : null;
+        serdeCache, bufferManagerGeneric, conf, cacheMetrics, ioMetrics, tracePool) : null;
     LOG.info("LLAP IO initialized");
 
     registerMXBeans();
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java
index 3cafad16c0..1f3f4d2b4f 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl;
 import org.apache.hadoop.hive.llap.metrics.LlapDaemonIOMetrics;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Consumer;
+import org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace;
 import org.apache.hive.common.util.FixedSizedObjectPool;
 import org.apache.orc.TypeDescription;
 
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/GenericColumnVectorProducer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/GenericColumnVectorProducer.java
index 0d9779f4c8..945aff31b3 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/GenericColumnVectorProducer.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/GenericColumnVectorProducer.java
@@ -39,6 +39,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Consumer;
+import org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.serde2.Deserializer;
@@ -48,6 +49,7 @@
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hive.common.util.FixedSizedObjectPool;
 import org.apache.orc.CompressionKind;
 import org.apache.orc.OrcProto;
 import org.apache.orc.OrcUtils;
@@ -63,16 +65,18 @@ public class GenericColumnVectorProducer implements ColumnVectorProducer {
   private final Configuration conf;
   private final LlapDaemonCacheMetrics cacheMetrics;
   private final LlapDaemonIOMetrics ioMetrics;
+  private final FixedSizedObjectPool<IoTrace> tracePool;
 
   public GenericColumnVectorProducer(SerDeLowLevelCacheImpl serdeCache,
       BufferUsageManager bufferManager, Configuration conf, LlapDaemonCacheMetrics cacheMetrics,
-      LlapDaemonIOMetrics ioMetrics) {
+      LlapDaemonIOMetrics ioMetrics, FixedSizedObjectPool<IoTrace> tracePool) {
     LlapIoImpl.LOG.info("Initializing ORC column vector producer");
     this.cache = serdeCache;
     this.bufferManager = bufferManager;
     this.conf = conf;
     this.cacheMetrics = cacheMetrics;
     this.ioMetrics = ioMetrics;
+    this.tracePool = tracePool;
   }
 
   @Override
@@ -92,10 +96,11 @@ public ReadPipeline createReadPipeline(Consumer<ColumnVectorBatch> consumer, Fil
     }
     edc.setFileMetadata(fm);
     // Note that we pass job config to the record reader, but use global config for LLAP IO.
+    // TODO: add tracing to serde reader
     SerDeEncodedDataReader reader = new SerDeEncodedDataReader(cache,
         bufferManager, conf, split, columnIds, edc, job, reporter, sourceInputFormat,
         sourceSerDe, counters, fm.getSchema(), parts);
-    edc.init(reader, reader);
+    edc.init(reader, reader, new IoTrace(0, false));
     if (LlapIoImpl.LOG.isDebugEnabled()) {
       LlapIoImpl.LOG.debug("Ignoring schema: " + schema);
     }
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
index 121e169fc6..6edd84b8b0 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
@@ -24,7 +24,9 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.Pool;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.llap.cache.BufferUsageManager;
 import org.apache.hadoop.hive.llap.cache.LowLevelCache;
 import org.apache.hadoop.hive.llap.counters.QueryFragmentCounters;
@@ -36,6 +38,7 @@
 import org.apache.hadoop.hive.llap.metrics.LlapDaemonIOMetrics;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Consumer;
+import org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.serde2.Deserializer;
@@ -43,6 +46,7 @@
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hive.common.util.FixedSizedObjectPool;
 import org.apache.orc.TypeDescription;
 import org.apache.orc.OrcConf;
 
@@ -55,10 +59,14 @@ public class OrcColumnVectorProducer implements ColumnVectorProducer {
   private boolean _skipCorrupt; // TODO: get rid of this
   private LlapDaemonCacheMetrics cacheMetrics;
   private LlapDaemonIOMetrics ioMetrics;
+  // TODO: if using in multiple places, e.g. SerDe cache, pass this in.
+  // TODO: should this rather use a threadlocal for NUMA affinity?
+  private final FixedSizedObjectPool<IoTrace> tracePool;
 
   public OrcColumnVectorProducer(OrcMetadataCache metadataCache,
       LowLevelCache lowLevelCache, BufferUsageManager bufferManager,
-      Configuration conf, LlapDaemonCacheMetrics cacheMetrics, LlapDaemonIOMetrics ioMetrics) {
+      Configuration conf, LlapDaemonCacheMetrics cacheMetrics, LlapDaemonIOMetrics ioMetrics,
+      FixedSizedObjectPool<IoTrace> tracePool) {
     LlapIoImpl.LOG.info("Initializing ORC column vector producer");
 
     this.metadataCache = metadataCache;
@@ -68,6 +76,7 @@ public OrcColumnVectorProducer(OrcMetadataCache metadataCache,
     this._skipCorrupt = OrcConf.SKIP_CORRUPT_DATA.getBoolean(conf);
     this.cacheMetrics = cacheMetrics;
     this.ioMetrics = ioMetrics;
+    this.tracePool = tracePool;
   }
 
   @Override
@@ -81,8 +90,8 @@ public ReadPipeline createReadPipeline(
         _skipCorrupt, counters, ioMetrics);
     OrcEncodedDataReader reader = new OrcEncodedDataReader(
         lowLevelCache, bufferManager, metadataCache, conf, job, split, columnIds, sarg,
-        columnNames, edc, counters, readerSchema);
-    edc.init(reader, reader);
+        columnNames, edc, counters, readerSchema, tracePool);
+    edc.init(reader, reader, reader.getTrace());
     return edc;
   }
 }
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
index 8d96e7b2c2..d048a57651 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
@@ -19,8 +19,10 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.concurrent.Callable;
 
 import org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch;
+import org.apache.hadoop.hive.llap.ConsumerFeedback;
 import org.apache.hadoop.hive.llap.counters.LlapIOCounters;
 import org.apache.hadoop.hive.llap.counters.QueryFragmentCounters;
 import org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch;
@@ -44,6 +46,7 @@
 import org.apache.hadoop.hive.ql.io.orc.encoded.Consumer;
 import org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory;
 import org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.SettableTreeReader;
+import org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace;
 import org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.OrcEncodedColumnBatch;
 import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl;
@@ -68,6 +71,7 @@ public class OrcEncodedDataConsumer
   private final QueryFragmentCounters counters;
   private boolean[] includedColumns;
   private SchemaEvolution evolution;
+  private IoTrace trace;
 
   public OrcEncodedDataConsumer(
       Consumer<ColumnVectorBatch> consumer, int colCount, boolean skipCorrupt,
@@ -152,6 +156,7 @@ protected void decodeBatch(OrcEncodedColumnBatch batch,
             // When we populate column vectors we skip over the root struct.
             cvb.cols[idx] = createColumn(schema.getChildren().get(columnMapping[idx]), batchSize);
           }
+          trace.logTreeReaderNextVector(idx);
           cvb.cols[idx].ensureSize(batchSize, false);
           reader.nextVector(cvb.cols[idx], null, batchSize);
         }
@@ -224,6 +229,7 @@ private void positionInStreams(TreeReaderFactory.TreeReader[] columnReaders,
     PositionProvider[] pps = createPositionProviders(columnReaders, batchKey, stripeMetadata);
     if (pps == null) return;
     for (int i = 0; i < columnReaders.length; i++) {
+      // TODO: we could/should trace seek destinations; pps needs a "peek" method
       columnReaders[i].seek(pps);
     }
   }
@@ -313,4 +319,10 @@ public void setSchemaEvolution(SchemaEvolution evolution) {
   public SchemaEvolution getSchemaEvolution() {
     return evolution;
   }
+
+  public void init(ConsumerFeedback<OrcEncodedColumnBatch> upstreamFeedback,
+      Callable<Void> readCallable, IoTrace trace) {
+    super.init(upstreamFeedback, readCallable);
+    this.trace = trace;
+  }
 }
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
index 2a76f5c4da..a2eb82947f 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
@@ -74,6 +74,7 @@
 import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl;
 import org.apache.hadoop.hive.ql.io.orc.encoded.EncodedOrcFile;
 import org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReader;
+import org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace;
 import org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.OrcEncodedColumnBatch;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.PoolFactory;
@@ -166,12 +167,15 @@ public Pool<OrcEncodedColumnBatch> createEncodedColumnBatchPool() {
   private volatile boolean isPaused = false;
 
   boolean[] globalIncludes = null;
+  private final IoTrace trace;
+  private Pool<IoTrace> tracePool;
 
   public OrcEncodedDataReader(LowLevelCache lowLevelCache, BufferUsageManager bufferManager,
       OrcMetadataCache metadataCache, Configuration daemonConf, Configuration jobConf,
       FileSplit split, List<Integer> columnIds, SearchArgument sarg, String[] columnNames,
       OrcEncodedDataConsumer consumer, QueryFragmentCounters counters,
-      TypeDescription readerSchema) throws IOException {
+      TypeDescription readerSchema, Pool<IoTrace> tracePool)
+          throws IOException {
     this.lowLevelCache = lowLevelCache;
     this.metadataCache = metadataCache;
     this.bufferManager = bufferManager;
@@ -185,6 +189,8 @@ public OrcEncodedDataReader(LowLevelCache lowLevelCache, BufferUsageManager buff
     this.columnNames = columnNames;
     this.consumer = consumer;
     this.counters = counters;
+    this.trace = tracePool.take();
+    this.tracePool = tracePool;
     try {
       this.ugi = UserGroupInformation.getCurrentUser();
     } catch (IOException e) {
@@ -192,7 +198,7 @@ public OrcEncodedDataReader(LowLevelCache lowLevelCache, BufferUsageManager buff
     }
 
     // moved this part of code from performDataRead as LlapInputFormat need to know the file schema
-    // to decide if schema evolution is supported or not
+    // to decide if schema evolution is supported or not.
     orcReader = null;
     // 1. Get file metadata from cache, or create the reader and read it.
     // Don't cache the filesystem object for now; Tez closes it and FS cache will fix all that
@@ -248,7 +254,7 @@ public Void run() throws Exception {
 
   protected Void performDataRead() throws IOException {
     long startTime = counters.startTimeCounter();
-    LlapIoImpl.LOG.info("Processing data for {}", split.getPath());
+    LlapIoImpl.LOG.info("Processing data for file {}: {}", fileKey, split.getPath());
     if (processStop()) {
       recordReaderTime(startTime);
       return null;
@@ -265,14 +271,14 @@ protected Void performDataRead() throws IOException {
       // 2. Determine which stripes to read based on the split.
       determineStripesToRead();
     } catch (Throwable t) {
-      recordReaderTime(startTime);
-      consumer.setError(t);
+      handleReaderError(startTime, t);
       return null;
     }
 
     if (stripeRgs.length == 0) {
       consumer.setDone();
       recordReaderTime(startTime);
+      tracePool.offer(trace);
       return null; // No data to read.
     }
     counters.setDesc(QueryFragmentCounters.Desc.STRIPES,
@@ -305,12 +311,11 @@ protected Void performDataRead() throws IOException {
       if (!hasData) {
         consumer.setDone();
         recordReaderTime(startTime);
+        tracePool.offer(trace);
         return null; // No data to read.
       }
     } catch (Throwable t) {
-      cleanupReaders();
-      consumer.setError(t);
-      recordReaderTime(startTime);
+      handleReaderError(startTime, t);
       return null;
     }
 
@@ -324,12 +329,10 @@ protected Void performDataRead() throws IOException {
       ensureOrcReader();
       // Reader creating updates HDFS counters, don't do it here.
       DataWrapperForOrc dw = new DataWrapperForOrc();
-      stripeReader = orcReader.encodedReader(fileKey, dw, dw, POOL_FACTORY);
+      stripeReader = orcReader.encodedReader(fileKey, dw, dw, POOL_FACTORY, trace);
       stripeReader.setTracing(LlapIoImpl.ORC_LOGGER.isTraceEnabled());
     } catch (Throwable t) {
-      consumer.setError(t);
-      recordReaderTime(startTime);
-      cleanupReaders();
+      handleReaderError(startTime, t);
       return null;
     }
 
@@ -351,6 +354,7 @@ protected Void performDataRead() throws IOException {
 
         LlapIoImpl.ORC_LOGGER.trace("Reading stripe {}: {}, {}", stripeIx, stripe.getOffset(),
             stripe.getLength());
+        trace.logReadingStripe(stripeIx, stripe.getOffset(), stripe.getLength());
         rgs = stripeRgs[stripeIxMod];
         if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
           LlapIoImpl.ORC_LOGGER.trace("readState[{}]: {}", stripeIxMod, Arrays.toString(rgs));
@@ -404,9 +408,7 @@ protected Void performDataRead() throws IOException {
           counters.incrCounter(LlapIOCounters.METADATA_CACHE_HIT);
         }
       } catch (Throwable t) {
-        consumer.setError(t);
-        cleanupReaders();
-        recordReaderTime(startTime);
+        handleReaderError(startTime, t);
         return null;
       }
       if (processStop()) {
@@ -425,9 +427,7 @@ protected Void performDataRead() throws IOException {
             stripeMetadata.getEncodings(), stripeMetadata.getStreams(), globalIncludes,
             rgs, consumer);
       } catch (Throwable t) {
-        consumer.setError(t);
-        cleanupReaders();
-        recordReaderTime(startTime);
+        handleReaderError(startTime, t);
         return null;
       }
     }
@@ -437,12 +437,20 @@ protected Void performDataRead() throws IOException {
     consumer.setDone();
 
     LlapIoImpl.LOG.trace("done processing {}", split);
-
+    tracePool.offer(trace);
     // Close the stripe reader, we are done reading.
     cleanupReaders();
     return null;
   }
 
+  private void handleReaderError(long startTime, Throwable t) {
+    recordReaderTime(startTime);
+    consumer.setError(t);
+    trace.dumpLog(LOG);
+    cleanupReaders();
+    tracePool.offer(trace);
+  }
+
   private void recordReaderTime(long startTime) {
     counters.incrTimeCounter(LlapIOCounters.TOTAL_IO_TIME_NS, startTime);
   }
@@ -504,6 +512,7 @@ private void validateFileMetadata() throws IOException {
   private boolean processStop() {
     if (!isStopped) return false;
     LOG.info("Encoded data reader is stopping");
+    tracePool.offer(trace);
     cleanupReaders();
     return true;
   }
@@ -733,11 +742,14 @@ private boolean determineRgsToRead(boolean[] globalIncludes, int rowIndexStride,
       if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
         if (isNone) {
           LlapIoImpl.ORC_LOGGER.trace("SARG eliminated all RGs for stripe {}", stripeIx);
+          trace.logSargResult(stripeIx, 0);
         } else if (!isAll) {
           LlapIoImpl.ORC_LOGGER.trace("SARG picked RGs for stripe {}: {}",
               stripeIx, DebugUtils.toString(rgsToRead));
+          trace.logSargResult(stripeIx, rgsToRead);
         } else {
           LlapIoImpl.ORC_LOGGER.trace("Will read all {} RGs for stripe {}", rgCount, stripeIx);
+          trace.logSargResult(stripeIx, rgCount);
         }
       }
       assert isAll || isNone || rgsToRead.length == rgCount;
@@ -836,6 +848,7 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList range,
       if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
         LlapIoImpl.ORC_LOGGER.trace("Disk ranges after data cache (file " + fileKey +
             ", base offset " + baseOffset + "): " + RecordReaderUtils.stringifyDiskRanges(result));
+        // TODO: trace ranges here? Between data cache and incomplete cb cache
       }
       if (gotAllData.value) return result;
       return (metadataCache == null) ? result
@@ -888,6 +901,7 @@ public DiskRangeList readFileData(DiskRangeList range, long baseOffset,
         LlapIoImpl.ORC_LOGGER.trace("Disk ranges after disk read (file {}, base offset {}): {}",
             fileKey, baseOffset, RecordReaderUtils.stringifyDiskRanges(result));
       }
+      trace.logRanges(fileKey, baseOffset, result, IoTrace.RangesSrc.DISK);
       return result;
     }
 
@@ -950,4 +964,8 @@ public MemoryBuffer create() {
   public TezCounters getTezCounters() {
     return counters.getTezCounters();
   }
+
+  public IoTrace getTrace() {
+    return trace;
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
index d5807b77e2..3f5f99ce35 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
@@ -40,11 +40,14 @@
 import org.apache.orc.CompressionCodec;
 import org.apache.orc.DataReader;
 import org.apache.orc.OrcConf;
+import org.apache.orc.OrcProto.ColumnEncoding;
+import org.apache.orc.OrcProto.Stream.Kind;
 import org.apache.orc.impl.OutStream;
 import org.apache.orc.impl.RecordReaderUtils;
 import org.apache.orc.impl.StreamName;
 import org.apache.orc.StripeInformation;
 import org.apache.orc.impl.BufferChunk;
+import org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace.RangesSrc;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.OrcEncodedColumnBatch;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.PoolFactory;
 import org.apache.orc.OrcProto;
@@ -122,10 +125,11 @@ public DiskRangeList createCacheChunk(MemoryBuffer buffer, long offset, long end
   private final long rowIndexStride;
   private final DataCache cacheWrapper;
   private boolean isTracingEnabled;
+  private final IoTrace trace;
 
   public EncodedReaderImpl(Object fileKey, List<OrcProto.Type> types, CompressionCodec codec,
       int bufferSize, long strideRate, DataCache cacheWrapper, DataReader dataReader,
-      PoolFactory pf) throws IOException {
+      PoolFactory pf, IoTrace trace) throws IOException {
     this.fileKey = fileKey;
     this.codec = codec;
     this.types = types;
@@ -133,6 +137,7 @@ public EncodedReaderImpl(Object fileKey, List<OrcProto.Type> types, CompressionC
     this.rowIndexStride = strideRate;
     this.cacheWrapper = cacheWrapper;
     this.dataReader = dataReader;
+    this.trace = trace;
     if (POOLS != null) return;
     if (pf == null) {
       pf = new NoopPoolFactory();
@@ -243,10 +248,12 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
     // Don't create context for the 0-s column.
     for (int i = 1; i < included.length; ++i) {
       if (!included[i]) continue;
-      colCtxs[i] = new ColumnReadContext(i, encodings.get(i), indexes[i], ++colRgIx);
+      ColumnEncoding enc = encodings.get(i);
+      colCtxs[i] = new ColumnReadContext(i, enc, indexes[i], ++colRgIx);
       if (isTracingEnabled) {
         LOG.trace("Creating context: " + colCtxs[i].toString());
       }
+      trace.logColumnRead(i, colRgIx, enc.getKind());
     }
     boolean isCompressed = (codec != null);
     CreateHelper listToRead = new CreateHelper();
@@ -263,6 +270,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
           LOG.trace("Skipping stream for column " + colIx + ": "
               + streamKind + " at " + offset + ", " + length);
         }
+        trace.logSkipStream(colIx, streamKind, offset, length);
         offset += length;
         continue;
       }
@@ -276,11 +284,13 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
             + ", " + length + ", index position " + indexIx);
       }
       if (rgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {
+        trace.logAddStream(colIx, streamKind, offset, length, indexIx, true);
         RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);
         if (isTracingEnabled) {
           LOG.trace("Will read whole stream " + streamKind + "; added to " + listToRead.getTail());
         }
       } else {
+        trace.logAddStream(colIx, streamKind, offset, length, indexIx, false);
         RecordReaderUtils.addRgFilteredStreamToRanges(stream, rgs,
             codec != null, indexes[colIx], encodings.get(colIx), types.get(colIx),
             bufferSize, hasNull[colIx], offset, length, listToRead, true);
@@ -304,18 +314,20 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
 
     // 2. Now, read all of the ranges from cache or disk.
     DiskRangeList.MutateHelper toRead = new DiskRangeList.MutateHelper(listToRead.get());
-    if (/*isTracingEnabled && */LOG.isInfoEnabled()) {
-      LOG.info("Resulting disk ranges to read (file " + fileKey + "): "
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Resulting disk ranges to read (file " + fileKey + "): "
           + RecordReaderUtils.stringifyDiskRanges(toRead.next));
     }
+    trace.logRanges(fileKey, stripeOffset, toRead.next, RangesSrc.PLAN);
     BooleanRef isAllInCache = new BooleanRef();
     if (hasFileId) {
       cacheWrapper.getFileData(fileKey, toRead.next, stripeOffset, CC_FACTORY, isAllInCache);
-      if (/*isTracingEnabled && */LOG.isInfoEnabled()) {
-        LOG.info("Disk ranges after cache (found everything " + isAllInCache.value + "; file "
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Disk ranges after cache (found everything " + isAllInCache.value + "; file "
             + fileKey + ", base offset " + stripeOffset  + "): "
             + RecordReaderUtils.stringifyDiskRanges(toRead.next));
       }
+      trace.logRanges(fileKey, stripeOffset, toRead.next, RangesSrc.CACHE);
     }
 
     // TODO: the memory release could be optimized - we could release original buffers after we
@@ -368,7 +380,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
           for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {
             StreamContext sctx = ctx.streams[streamIx];
             DiskRangeList newIter = preReadUncompressedStream(
-                stripeOffset, iter, sctx.offset, sctx.offset + sctx.length);
+                stripeOffset, iter, sctx.offset, sctx.offset + sctx.length, sctx.kind);
             if (newIter != null) {
               iter = newIter;
             }
@@ -410,6 +422,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
         boolean isLastRg = rgIx == rgCount - 1;
         // Create the batch we will use to return data for this RG.
         OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();
+        trace.logStartRg(rgIx);
         boolean hasError = true;
         try {
           ecb.init(fileKey, stripeIx, rgIx, included.length);
@@ -422,6 +435,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
             OrcProto.RowIndexEntry index = ctx.rowIndex.getEntry(rgIx),
                 nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);
             ecb.initOrcColumn(ctx.colIx);
+            trace.logStartCol(ctx.colIx);
             for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {
               StreamContext sctx = ctx.streams[streamIx];
               ColumnStreamData cb = null;
@@ -432,6 +446,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
                     LOG.trace("Getting stripe-level stream [" + sctx.kind + ", " + ctx.encoding + "] for"
                         + " column " + ctx.colIx + " RG " + rgIx + " at " + sctx.offset + ", " + sctx.length);
                   }
+                  trace.logStartStripeStream(sctx.kind);
                   if (sctx.stripeLevelStream == null) {
                     sctx.stripeLevelStream = POOLS.csdPool.take();
                     // We will be using this for each RG while also sending RGs to processing.
@@ -464,8 +479,8 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
                   // As we read, we can unlock initial refcounts for the buffers that end before
                   // the data that we need for this RG.
                   long unlockUntilCOffset = sctx.offset + nextCOffsetRel;
-                  cb = createRgColumnStreamData(
-                      rgIx, isLastRg, ctx.colIx, sctx, cOffset, endCOffset, isCompressed);
+                  cb = createRgColumnStreamData(rgIx, isLastRg, ctx.colIx, sctx,
+                      cOffset, endCOffset, isCompressed, unlockUntilCOffset);
                   boolean isStartOfStream = sctx.bufferIter == null;
                   DiskRangeList lastCached = readEncodedStream(stripeOffset,
                       (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,
@@ -498,6 +513,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
         LOG.trace("Disk ranges after preparing all the data "
             + RecordReaderUtils.stringifyDiskRanges(toRead.next));
       }
+      trace.logRanges(fileKey, stripeOffset, toRead.next, RangesSrc.PREREAD);
     } finally {
       // Release the unreleased stripe-level buffers. See class comment about refcounts.
       for (int colIx = 0; colIx < colCtxs.length; ++colIx) {
@@ -564,8 +580,9 @@ private static String arrayToString(boolean[] a) {
   }
 
 
-  private ColumnStreamData createRgColumnStreamData(int rgIx, boolean isLastRg,
-      int colIx, StreamContext sctx, long cOffset, long endCOffset, boolean isCompressed) {
+  private ColumnStreamData createRgColumnStreamData(int rgIx, boolean isLastRg, int colIx,
+      StreamContext sctx, long cOffset, long endCOffset, boolean isCompressed,
+      long unlockUntilCOffset) {
     ColumnStreamData cb = POOLS.csdPool.take();
     cb.incRef();
     if (isTracingEnabled) {
@@ -574,6 +591,7 @@ private ColumnStreamData createRgColumnStreamData(int rgIx, boolean isLastRg,
           + sctx.length + " index position " + sctx.streamIndexOffset + ": " +
           (isCompressed ? "" : "un") + "compressed [" + cOffset + ", " + endCOffset + ")");
     }
+    trace.logStartStream(sctx.kind, cOffset, endCOffset, unlockUntilCOffset);
     return cb;
   }
 
@@ -741,6 +759,7 @@ public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, lon
     if (isTracingEnabled) {
       LOG.trace("Starting read for [" + cOffset + "," + endCOffset + ") at " + current);
     }
+    trace.logStartRead(current);
 
     CacheChunk lastUncompressed = null;
 
@@ -934,9 +953,10 @@ private CacheChunk prepareRangesForUncompressedRead(long cOffset, long endCOffse
    * to handle just for this case.
    * We could avoid copy in non-zcr case and manage the buffer that was not allocated by our
    * allocator. Uncompressed case is not mainline though so let's not complicate it.
+   * @param kind 
    */
   private DiskRangeList preReadUncompressedStream(long baseOffset, DiskRangeList start,
-      long streamOffset, long streamEnd) throws IOException {
+      long streamOffset, long streamEnd, Kind kind) throws IOException {
     if (streamOffset == streamEnd) return null;
     List<UncompressedCacheChunk> toCache = null;
 
@@ -945,6 +965,8 @@ private DiskRangeList preReadUncompressedStream(long baseOffset, DiskRangeList s
     if (isTracingEnabled) {
       LOG.trace("Starting pre-read for [" + streamOffset + "," + streamEnd + ") at " + current);
     }
+    trace.logStartStream(kind, streamOffset, streamEnd, streamOffset);
+    trace.logStartRead(current);
 
     if (streamOffset > current.getOffset()) {
       // Target compression block is in the middle of the range; slice the range in two.
@@ -997,6 +1019,7 @@ private DiskRangeList preReadUncompressedStream(long baseOffset, DiskRangeList s
           LOG.trace("Processing uncompressed file data at ["
               + current.getOffset() + ", " + current.getEnd() + ")");
         }
+        trace.logUncompressedData(current.getOffset(), current.getEnd());
         BufferChunk curBc = (BufferChunk)current;
         // Track if we still have the entire part.
         long hadEntirePartTo = hasEntirePartTo;
@@ -1052,7 +1075,7 @@ private DiskRangeList preReadUncompressedStream(long baseOffset, DiskRangeList s
     for (UncompressedCacheChunk candidateCached : toCache) {
       candidateCached.setBuffer(targetBuffers[ix]);
       ByteBuffer dest = candidateCached.getBuffer().getByteBufferRaw();
-      copyAndReplaceUncompressedChunks(candidateCached, dest, candidateCached);
+      copyAndReplaceUncompressedChunks(candidateCached, dest, candidateCached, true);
       candidateCached.clear();
       lastUncompressed = candidateCached;
       ++ix;
@@ -1079,7 +1102,7 @@ private int determineUncompressedPartSize() {
   private static void copyUncompressedChunk(ByteBuffer src, ByteBuffer dest) {
     int startPos = dest.position(), startLim = dest.limit();
     dest.put(src); // Copy uncompressed data to cache.
-    // Put moves position forward by the size of the data.
+    // Put call moves position forward by the size of the data.
     int newPos = dest.position();
     if (newPos > startLim) {
       throw new AssertionError("After copying, buffer [" + startPos + ", " + startLim
@@ -1090,26 +1113,27 @@ private static void copyUncompressedChunk(ByteBuffer src, ByteBuffer dest) {
   }
 
 
-  private static CacheChunk copyAndReplaceCandidateToNonCached(
+  private CacheChunk copyAndReplaceCandidateToNonCached(
       UncompressedCacheChunk candidateCached, long partOffset,
       long candidateEnd, DataCache cacheWrapper, MemoryBuffer[] singleAlloc) {
     // We thought we had the entire part to cache, but we don't; convert start to
     // non-cached. Since we are at the first gap, the previous stuff must be contiguous.
     singleAlloc[0] = null;
+    trace.logPartialUncompressedData(partOffset, candidateEnd, true);
     cacheWrapper.getAllocator().allocateMultiple(singleAlloc, (int)(candidateEnd - partOffset));
-
     MemoryBuffer buffer = singleAlloc[0];
     cacheWrapper.reuseBuffer(buffer);
     ByteBuffer dest = buffer.getByteBufferRaw();
     CacheChunk tcc = POOLS.tccPool.take();
     tcc.init(buffer, partOffset, candidateEnd);
-    copyAndReplaceUncompressedChunks(candidateCached, dest, tcc);
+    copyAndReplaceUncompressedChunks(candidateCached, dest, tcc, false);
     return tcc;
   }
 
-  private static CacheChunk copyAndReplaceUncompressedToNonCached(
+  private CacheChunk copyAndReplaceUncompressedToNonCached(
       BufferChunk bc, DataCache cacheWrapper, MemoryBuffer[] singleAlloc) {
     singleAlloc[0] = null;
+    trace.logPartialUncompressedData(bc.getOffset(), bc.getEnd(), false);
     cacheWrapper.getAllocator().allocateMultiple(singleAlloc, bc.getLength());
     MemoryBuffer buffer = singleAlloc[0];
     cacheWrapper.reuseBuffer(buffer);
@@ -1121,13 +1145,16 @@ private static CacheChunk copyAndReplaceUncompressedToNonCached(
     return tcc;
   }
 
-  private static void copyAndReplaceUncompressedChunks(
-      UncompressedCacheChunk candidateCached, ByteBuffer dest, CacheChunk tcc) {
+  private void copyAndReplaceUncompressedChunks(UncompressedCacheChunk candidateCached,
+      ByteBuffer dest, CacheChunk tcc, boolean isValid) {
     int startPos = dest.position(), startLim = dest.limit();
     DiskRangeList next = null;
     for (int i = 0; i < candidateCached.getCount(); ++i) {
       BufferChunk chunk = (i == 0) ? candidateCached.getChunk() : (BufferChunk)next;
       dest.put(chunk.getData());
+      if (isValid) {
+        trace.logValidUncompresseedChunk(startLim - startPos, chunk);
+      }
       next = chunk.next;
       if (i == 0) {
         chunk.replaceSelfWith(tcc);
@@ -1229,6 +1256,7 @@ private void processCacheCollisions(long[] collisionMask,
           LOG.trace("Discarding data due to cache collision: " + replacedChunk.getBuffer()
               + " replaced with " + replacementBuffer);
         }
+        trace.logCacheCollision(replacedChunk, replacementBuffer);
         assert replacedChunk.getBuffer() != replacementBuffer : i + " was not replaced in the results "
             + "even though mask is [" + Long.toBinaryString(maskVal) + "]";
         replacedChunk.handleCacheCollision(cacheWrapper, replacementBuffer, cacheBuffers);
@@ -1367,7 +1395,7 @@ private ProcCacheChunk addOneCompressionBuffer(BufferChunk current,
       // Bad luck! Handle the corner cases where 3 bytes are in multiple blocks.
       int[] bytes = new int[3];
       current = readLengthBytesFromSmallBuffers(
-          current, cbStartOffset, bytes, badEstimates, isTracingEnabled);
+          current, cbStartOffset, bytes, badEstimates, isTracingEnabled, trace);
       if (current == null) return null;
       compressed = current.getChunk();
       b0 = bytes[0];
@@ -1387,16 +1415,17 @@ private ProcCacheChunk addOneCompressionBuffer(BufferChunk current,
       LOG.trace("Found CB at " + cbStartOffset + ", chunk length " + chunkLength + ", total "
           + consumedLength + ", " + (isUncompressed ? "not " : "") + "compressed");
     }
+    trace.logOrcCb(cbStartOffset, chunkLength, isUncompressed);
     if (compressed.remaining() >= chunkLength) {
       // Simple case - CB fits entirely in the disk range.
       slice = compressed.slice();
       slice.limit(chunkLength);
       return addOneCompressionBlockByteBuffer(slice, isUncompressed,
-          cbStartOffset, cbEndOffset, chunkLength, current, toDecompress, cacheBuffers);
+          cbStartOffset, cbEndOffset, chunkLength, current, toDecompress, cacheBuffers, false);
     }
     if (current.getEnd() < cbEndOffset && !current.hasContiguousNext()) {
       badEstimates.add(addIncompleteCompressionBuffer(
-          cbStartOffset, current, 0, isTracingEnabled));
+          cbStartOffset, current, 0, isTracingEnabled, trace));
       return null; // This is impossible to read from this chunk.
     }
 
@@ -1410,6 +1439,7 @@ private ProcCacheChunk addOneCompressionBuffer(BufferChunk current,
     if (isTracingEnabled) {
       LOG.trace("Removing partial CB " + current + " from ranges after copying its contents");
     }
+    trace.logPartialCb(current);
     DiskRangeList next = current.next;
     current.removeSelf();
     if (originalPos == 0 && toRelease.remove(compressed)) {
@@ -1428,8 +1458,8 @@ private ProcCacheChunk addOneCompressionBuffer(BufferChunk current,
         slice = compressed.slice();
         slice.limit(remaining);
         copy.put(slice);
-        ProcCacheChunk cc = addOneCompressionBlockByteBuffer(copy, isUncompressed,
-            cbStartOffset, cbEndOffset, remaining, (BufferChunk)next, toDecompress, cacheBuffers);
+        ProcCacheChunk cc = addOneCompressionBlockByteBuffer(copy, isUncompressed, cbStartOffset,
+            cbEndOffset, remaining, (BufferChunk)next, toDecompress, cacheBuffers, true);
         if (compressed.remaining() <= 0 && toRelease.remove(compressed)) {
           releaseBuffer(compressed, true); // We copied the entire buffer. 
         } // else there's more data to process; will be handled in next call.
@@ -1446,10 +1476,11 @@ private ProcCacheChunk addOneCompressionBuffer(BufferChunk current,
         if (isTracingEnabled) {
           LOG.trace("Removing partial CB " + tmp + " from ranges after copying its contents");
         }
+        trace.logPartialCb(tmp);
         tmp.removeSelf();
       } else {
         badEstimates.add(addIncompleteCompressionBuffer(
-            cbStartOffset, tmp, extraChunkCount, isTracingEnabled));
+            cbStartOffset, tmp, extraChunkCount, isTracingEnabled, trace));
         return null; // This is impossible to read from this chunk.
       }
     }
@@ -1458,9 +1489,11 @@ private ProcCacheChunk addOneCompressionBuffer(BufferChunk current,
 
   @VisibleForTesting
   static BufferChunk readLengthBytesFromSmallBuffers(BufferChunk first, long cbStartOffset,
-      int[] result, List<IncompleteCb> badEstimates, boolean isTracingEnabled) throws IOException {
+      int[] result, List<IncompleteCb> badEstimates, boolean isTracingEnabled, IoTrace trace)
+          throws IOException {
     if (!first.hasContiguousNext()) {
-      badEstimates.add(addIncompleteCompressionBuffer(cbStartOffset, first, 0, isTracingEnabled));
+      badEstimates.add(addIncompleteCompressionBuffer(
+          cbStartOffset, first, 0, isTracingEnabled, trace));
       return null; // This is impossible to read from this chunk.
     }
     int ix = readLengthBytes(first.getChunk(), result, 0);
@@ -1481,9 +1514,11 @@ static BufferChunk readLengthBytesFromSmallBuffers(BufferChunk first, long cbSta
         if (isTracingEnabled) {
           LOG.trace("Removing partial CB " + tmp + " from ranges after copying its contents");
         }
+        trace.logPartialCb(tmp);
         tmp.removeSelf();
       } else {
-        badEstimates.add(addIncompleteCompressionBuffer(cbStartOffset, tmp, -1, isTracingEnabled));
+        badEstimates.add(addIncompleteCompressionBuffer(
+            cbStartOffset, tmp, -1, isTracingEnabled, trace));
         return null; // This is impossible to read from this chunk.
       }
     }
@@ -1531,12 +1566,13 @@ private void releaseBuffer(ByteBuffer bb, boolean isFromDataReader) {
 
 
   private static IncompleteCb addIncompleteCompressionBuffer(long cbStartOffset,
-      DiskRangeList target, int extraChunkCountToLog, boolean isTracingEnabled) {
+      DiskRangeList target, int extraChunkCountToLog, boolean isTracingEnabled, IoTrace trace) {
     IncompleteCb icb = new IncompleteCb(cbStartOffset, target.getEnd());
     if (isTracingEnabled) {
       LOG.trace("Replacing " + target + " (and " + extraChunkCountToLog
           + " previous chunks) with " + icb + " in the buffers");
     }
+    trace.logInvalidOrcCb(cbStartOffset, target.getEnd());
     target.replaceSelfWith(icb);
     return icb;
   }
@@ -1555,7 +1591,8 @@ private static IncompleteCb addIncompleteCompressionBuffer(long cbStartOffset,
    */
   private ProcCacheChunk addOneCompressionBlockByteBuffer(ByteBuffer fullCompressionBlock,
       boolean isUncompressed, long cbStartOffset, long cbEndOffset, int lastChunkLength,
-      BufferChunk lastChunk, List<ProcCacheChunk> toDecompress, List<MemoryBuffer> cacheBuffers) {
+      BufferChunk lastChunk, List<ProcCacheChunk> toDecompress, List<MemoryBuffer> cacheBuffers,
+      boolean doTrace) {
     // Prepare future cache buffer.
     MemoryBuffer futureAlloc = cacheWrapper.getAllocator().createUnallocated();
     // Add it to result in order we are processing.
@@ -1569,6 +1606,9 @@ private ProcCacheChunk addOneCompressionBlockByteBuffer(ByteBuffer fullCompressi
     if (isTracingEnabled) {
       LOG.trace("Adjusting " + lastChunk + " to consume " + lastChunkLength + " compressed bytes");
     }
+    if (doTrace) {
+      trace.logCompositeOrcCb(lastChunkLength, lastChunk.getChunk().remaining(), cc);
+    }
     lastChunk.getChunk().position(lastChunk.getChunk().position() + lastChunkLength);
     // Finally, put it in the ranges list for future use (if shared between RGs).
     // Before anyone else accesses it, it would have been allocated and decompressed locally.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/IoTrace.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/IoTrace.java
new file mode 100644
index 0000000000..e1b5ae7cbd
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/IoTrace.java
@@ -0,0 +1,478 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.io.orc.encoded;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.Pool;
+import org.apache.hadoop.hive.common.io.DiskRange;
+import org.apache.hadoop.hive.common.io.DiskRangeList;
+import org.apache.hadoop.hive.common.io.encoded.MemoryBuffer;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.llap.DebugUtils;
+import org.apache.hive.common.util.FixedSizedObjectPool;
+import org.apache.orc.OrcProto.Stream.Kind;
+import org.apache.orc.OrcProto.*;
+import org.slf4j.Logger;
+
+/**
+ * Single threaded IO trace.
+ * Note: this can be made MTT merely by using an AtomicInteger, and storing a thread ID.
+ */
+public final class IoTrace {
+  private final static Logger LOG = org.slf4j.LoggerFactory.getLogger(IoTrace.class);
+  private final long[] log;
+  private int offset;
+  private final boolean isAlwaysDump;
+  private boolean hasDumped = false;
+
+  public static enum RangesSrc {
+    PLAN, CACHE, DISK, PREREAD
+  }
+
+  public IoTrace(int byteSize, boolean isAlwaysDump) {
+    log = (byteSize == 0) ? null : new long[byteSize >> 3];
+    this.isAlwaysDump = isAlwaysDump;
+  }
+
+  // Events.
+  public static final int TREE_READER_NEXT_VECTOR = 1, READING_STRIPE = 2,
+      SARG_RESULT = 4, RANGES = 5, COLUMN_READ = 6, SKIP_STREAM = 7,
+      ADD_STREAM = 8, START_RG = 9, START_COL = 10, START_STRIPE_STREAM = 11,
+      START_STREAM = 12, START_READ = 13, UNCOMPRESSED_DATA = 14,
+      PARTIAL_UNCOMPRESSED_DATA = 15, VALID_UNCOMPRESSEED_CHUNK = 16, CACHE_COLLISION = 17,
+      ORC_CB = 18, INVALID_ORC_CB = 19, PARTIAL_CB = 20, COMPOSITE_ORC_CB = 21, SARG_RESULT2 = 22;
+
+  public void reset() {
+    if (isAlwaysDump && !hasDumped) {
+      dumpLog(LOG);
+    }
+    offset = 0;
+    hasDumped = false;
+  }
+
+  public void dumpLog(Logger logger) {
+    hasDumped = true;
+    int ix = 0;
+    logger.info("Dumping LLAP IO trace; " + (offset << 3) + " bytes");
+    while (ix < offset) {
+      ix = dumpOneLine(ix, logger, log);
+    }
+  }
+
+  private static int dumpOneLine(int ix, Logger logger, long[] log) {
+    int event = getFirstInt(log[ix]);
+    switch (event) {
+    case TREE_READER_NEXT_VECTOR: {
+      logger.info(ix + ": TreeReader next vector " + getSecondInt(log[ix]));
+      return ix + 1;
+    }
+    case READING_STRIPE: {
+      logger.info(ix + ": Reading stripe " + getSecondInt(log[ix])
+          + " at " + log[ix + 1] + " length " + log[ix + 2]);
+      return ix + 3;
+    }
+    case SARG_RESULT: {
+      logger.info(ix + ": Reading " + log[ix + 1] + " rgs for stripe " + getSecondInt(log[ix]));
+      return ix + 2;
+    }
+    case SARG_RESULT2: {
+      int rgsLength = (int) log[ix + 1];
+      int elements = (rgsLength >> 6) + ((rgsLength & 63) == 0 ? 0 : 1);
+      boolean[] rgs = new boolean[rgsLength];
+      int rgsOffset = 0;
+      for (int i = 0; i < elements; ++i) {
+        long val = log[ix + i + 2];
+        int bitsInByte = Math.min(rgsLength - rgsOffset, 64);
+        for (int j = 0; j < rgsOffset; ++j, val >>>= 1) {
+          rgs[rgsOffset + j] = (val & 1) == 1;
+        }
+        rgsOffset += bitsInByte;
+      }
+      logger.info(ix + ": Reading filtered rgs for stripe " + getSecondInt(log[ix])
+          + ": " + DebugUtils.toString(rgs));
+      return ix + (elements + 2);
+    }
+    case RANGES: {
+      int val = getSecondInt(log[ix]);
+      RangesSrc src = RangesSrc.values()[val >>> MAX_ELEMENT_BITS];
+      int rangeCount = val & ((1 << MAX_ELEMENT_BITS) - 1);
+      int currentOffset = ix + 3;
+      StringBuilder sb = new StringBuilder();
+      for (int i = 0; i < rangeCount; ++i, currentOffset += 3) {
+        sb.append(printRange(currentOffset, log)).append(", ");
+      }
+      logger.info(ix + ": Ranges for file " + log[ix + 1] + " (base offset " + log[ix + 2]
+          + ") after " + src + ": " + sb.toString());
+      return ix + 3 + rangeCount * 3;
+    }
+    case COLUMN_READ: {
+      logger.info(ix + ": Reading column " + getSecondInt(log[ix]) + " (included index "
+          + getFirstInt(log[ix + 1]) + "; type "
+          + ColumnEncoding.Kind.values()[getSecondInt(log[ix + 1])] + ")");
+      return ix + 2;
+    }
+    case SKIP_STREAM: {
+      long streamOffset = log[ix + 1];
+      logger.info(ix + ": Skipping stream for col " + getSecondInt(log[ix]) + " [" + streamOffset
+          + ", " + (streamOffset + getFirstInt(log[ix + 2])) + ") kind "
+          + Kind.values()[getSecondInt(log[ix + 2])]);
+      return ix + 3;
+    }
+    case ADD_STREAM: {
+      long streamOffset = log[ix + 1];
+      logger.info(ix + ": Adding stream for col " + getSecondInt(log[ix]) + " [" + streamOffset
+          + ", " + (streamOffset + getFirstInt(log[ix + 2])) + ") kind "
+          + Kind.values()[getSecondInt(log[ix + 2])] + ", index " + getFirstInt(log[ix + 3])
+          + ", entire stream " + (getSecondInt(log[ix + 3]) == 1));
+      return ix + 4;
+    }
+    case START_RG: {
+      logger.info(ix + ": Starting rg " + getSecondInt(log[ix]));
+      return ix + 1;
+    }
+    case START_COL: {
+      logger.info(ix + ": Starting column " + getSecondInt(log[ix]));
+      return ix + 1;
+    }
+    case START_STRIPE_STREAM: {
+      logger.info(ix + ": Starting stripe-level stream " + Kind.values()[getSecondInt(log[ix])]);
+      return ix + 1;
+    }
+    case START_STREAM: {
+      long offset = log[ix + 1];
+      int unlockLen = getFirstInt(log[ix + 2]);
+      String unlockStr = (unlockLen == Integer.MAX_VALUE) ? "" : " unlock " + (offset + unlockLen);
+      logger.info(ix + ": Starting on stream " + Kind.values()[getSecondInt(log[ix])] + "["
+          + offset + ", "  + (offset + getSecondInt(log[ix + 2])) + ") " + unlockStr);
+      return ix + 3;
+    }
+    case START_READ: {
+      logger.info(ix + ": Starting read at 0x" + Integer.toHexString(getSecondInt(log[ix])));
+      return ix + 1;
+    }
+    case UNCOMPRESSED_DATA: {
+      long offset = log[ix + 1];
+      logger.info(ix + ": Uncompressed data ["
+          + offset + ", " + (offset + getSecondInt(log[ix])) + ")");
+      return ix + 2;
+    }
+    case PARTIAL_UNCOMPRESSED_DATA: {
+      long offset = log[ix + 1];
+      logger.info(ix + ": Partial uncompressed data ["
+          + offset + ", " + (offset + getSecondInt(log[ix])) + ")");
+      return ix + 2;
+    }
+    case VALID_UNCOMPRESSEED_CHUNK: {
+      logger.info(ix + ": Combining uncompressed data for cache buffer of length "
+          + getSecondInt(log[ix]) + " from 0x" + Integer.toHexString((int)log[ix + 1]));
+      return ix + 2;
+    }
+    case CACHE_COLLISION: {
+      logger.info(ix + ": Replacing " + printRange(ix + 1, log) + " with 0x"
+          + Integer.toHexString(getSecondInt(log[ix])));
+      return ix + 4;
+    }
+    case ORC_CB: {
+      long offset = log[ix + 1];
+      int val = getSecondInt(log[ix]);
+      boolean isUncompressed = (val & 1) == 1;
+      int cbLength = val >>> 1;
+      logger.info(ix + ": Found " + (isUncompressed ? "un" : "") + "compressed ORC CB ["
+          + offset + ", " + (offset + cbLength) + ")");
+      return ix + 2;
+    }
+    case INVALID_ORC_CB: {
+      long offset = log[ix + 1];
+      logger.info(ix + ": Found incomplete ORC CB ["
+          + offset + ", " + (offset + getSecondInt(log[ix])) + ")");
+      return ix + 2;
+    }
+    case PARTIAL_CB: {
+      logger.info(ix + ": Found buffer with a part of ORC CB " + printRange(ix + 1, log));
+      return ix + 4;
+    }
+    case COMPOSITE_ORC_CB: {
+      logger.info(ix + ": Combined ORC CB from multiple buffers " + printRange(ix + 2, log)
+          + " last chunk taken " + getSecondInt(log[ix]) + ", remaining " + log[ix + 1]);
+      return ix + 5;
+    }
+    default: throw new AssertionError("Unknown " + event);
+    }
+  }
+
+  //Utility methods used to store pairs of ints as long.
+  private static long makeIntPair(int first, int second) {
+    return ((long)first) << 32 | second;
+  }
+
+  private static int getFirstInt(long result) {
+    return (int) (result >>> 32);
+  }
+  private static int getSecondInt(long result) {
+    return (int) (result & ((1L << 32) - 1));
+  }
+
+  public void logTreeReaderNextVector(int idx) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 1 > log.length) return;
+    log[offset] = makeIntPair(TREE_READER_NEXT_VECTOR, idx);
+    this.offset += 1;
+  }
+
+  public void logReadingStripe(int stripeIx, long stripeOffset, long length) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 3 > log.length) return;
+    log[offset] = makeIntPair(READING_STRIPE, stripeIx);
+    log[offset + 1] = stripeOffset;
+    log[offset + 2] = length;
+    this.offset += 3;
+  }
+
+  public void logSargResult(int stripeIx, int rgCount) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 2 > log.length) return;
+    log[offset] = makeIntPair(SARG_RESULT, stripeIx);
+    log[offset + 1] = rgCount;
+    this.offset += 2;
+  }
+
+  public void logSargResult(int stripeIx, boolean[] rgsToRead) {
+    if (log == null) return;
+    int offset = this.offset;
+    int elements = (rgsToRead.length >> 6) + ((rgsToRead.length & 63) == 0 ? 0 : 1);
+    if (offset + elements + 2 > log.length) return;
+    log[offset] = makeIntPair(SARG_RESULT2, stripeIx);
+    log[offset + 1] = rgsToRead.length;
+    for (int i = 0, valOffset = 0; i < elements; ++i, valOffset += 64) {
+      long val = 0;
+      for (int j = 0; j < 64; ++j) {
+        int ix = valOffset + j;
+        if (rgsToRead.length == ix) break;
+        if (!rgsToRead[ix]) continue;
+        val = val | (1 << j);
+      }
+      log[offset + i + 2] = val;
+    }
+    this.offset += (elements + 2);
+  }
+
+  // Safety limit for potential list bugs.
+  private static int MAX_ELEMENT_BITS = 17, MAX_ELEMENTS = (1 << MAX_ELEMENT_BITS) - 1;
+  public void logRanges(Object fileKey, long baseOffset, DiskRangeList range, RangesSrc src) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 3 > log.length) return; // At least the header should fit.
+    log[offset + 1] = (fileKey instanceof Long) ? (long)fileKey : fileKey.hashCode();
+    log[offset + 2] = baseOffset;
+    int elementCount = 0;
+    int currentOffset = offset + 3;
+    while (range != null && elementCount < MAX_ELEMENTS) {
+      if (currentOffset + 3 > log.length) break;
+      logRange(range, currentOffset);
+      currentOffset += 3;
+      ++elementCount;
+      range = range.next;
+    }
+    log[offset] = makeIntPair(RANGES, (src.ordinal() << MAX_ELEMENT_BITS) | elementCount);
+    this.offset = currentOffset;
+  }
+
+  private void logRange(DiskRange range, int currentOffset) {
+    log[currentOffset] = range.getOffset();
+    log[currentOffset + 1] = range.getEnd();
+    log[currentOffset + 2] = range.hasData() ? System.identityHashCode(range.getData()) : 0;
+  }
+
+  private static String printRange(int ix, long[] log) {
+    return "[" + log[ix] + ", " + log[ix + 1] + "): 0x" + Integer.toHexString((int)log[ix + 2]);
+  }
+
+
+  public void logColumnRead(int colIx, int includedIx, ColumnEncoding.Kind kind) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 2 > log.length) return;
+    log[offset] = makeIntPair(COLUMN_READ, colIx);
+    log[offset + 1] = makeIntPair(includedIx, kind.ordinal());
+    this.offset += 2;
+  }
+
+  public void logSkipStream(int colIx, Stream.Kind streamKind, long streamOffset, long length) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 3 > log.length) return;
+    log[offset] = makeIntPair(SKIP_STREAM, colIx);
+    log[offset + 1] = streamOffset;
+    log[offset + 2] = makeIntPair((int)length, streamKind.ordinal());
+    this.offset += 3;
+  }
+
+  public void logAddStream(int colIx, Stream.Kind streamKind, long streamOffset,
+      long length, int indexIx, boolean isEntire) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 4 > log.length) return;
+    log[offset] = makeIntPair(ADD_STREAM, colIx);
+    log[offset + 1] = streamOffset;
+    log[offset + 2] = makeIntPair((int)length, streamKind.ordinal());
+    log[offset + 3] = makeIntPair(indexIx, isEntire ? 1 : 0);
+    this.offset += 4;
+  }
+
+  public void logStartRg(int rgIx) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 1 > log.length) return;
+    log[offset] = makeIntPair(START_RG, rgIx);
+    this.offset += 1;
+  }
+
+  public void logStartCol(int colIx) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 1 > log.length) return;
+    log[offset] = makeIntPair(START_COL, colIx);
+    this.offset += 1;
+  }
+
+  public void logStartStripeStream(Kind kind) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 1 > log.length) return;
+    log[offset] = makeIntPair(START_STRIPE_STREAM, kind.ordinal());
+    this.offset += 1;
+  }
+
+  public void logStartStream(Kind kind, long cOffset, long endCOffset,
+      long unlockUntilCOffset) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 3 > log.length) return;
+    log[offset] = makeIntPair(START_STREAM, kind.ordinal());
+    log[offset + 1] = cOffset;
+    long unlockLen = unlockUntilCOffset - cOffset;
+    int unlockLenToSave = unlockLen >= 0 && unlockLen < Integer.MAX_VALUE
+        ? (int)unlockLen : Integer.MAX_VALUE;
+    log[offset + 2] = makeIntPair(unlockLenToSave, (int)(endCOffset - cOffset));
+    this.offset += 3;
+  }
+
+  public void logStartRead(DiskRangeList current) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 1 > log.length) return;
+    log[offset] = makeIntPair(START_READ,
+        current.hasData() ? System.identityHashCode(current.getData()) : 0);
+    this.offset += 1;
+  }
+
+  public void logUncompressedData(long dataOffset, long end) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 2 > log.length) return;
+    log[offset] = makeIntPair(UNCOMPRESSED_DATA, (int)(end - dataOffset));
+    log[offset + 1] = dataOffset;
+    this.offset += 2;
+  }
+
+  public void logPartialUncompressedData(long partOffset, long candidateEnd, boolean fromCache) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 2 > log.length) return;
+    log[offset] = makeIntPair(PARTIAL_UNCOMPRESSED_DATA, (int)(candidateEnd - partOffset));
+    log[offset + 1] = partOffset;
+    this.offset += 2;
+  }
+
+  public void logValidUncompresseedChunk(int totalLength, DiskRange chunk) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 2 > log.length) return;
+    log[offset] = makeIntPair(VALID_UNCOMPRESSEED_CHUNK, totalLength);
+    log[offset + 1] = chunk.hasData() ? System.identityHashCode(chunk.getData()) : 0;
+    this.offset += 2;
+  }
+
+  public void logCacheCollision(DiskRange replacedChunk, MemoryBuffer replacementBuffer) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 4 > log.length) return;
+    log[offset] = makeIntPair(CACHE_COLLISION, System.identityHashCode(replacementBuffer));
+    logRange(replacedChunk, offset + 1);
+    this.offset += 4;
+  }
+
+  public void logOrcCb(long cbStartOffset, int cbLength, boolean isUncompressed) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 2 > log.length) return;
+    log[offset] = makeIntPair(ORC_CB, (cbLength << 1) | (isUncompressed ? 1 : 0));
+    log[offset + 1] = cbStartOffset;
+    this.offset += 2;
+  }
+
+  public void logInvalidOrcCb(long cbStartOffset, long end) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 2 > log.length) return;
+    log[offset] = makeIntPair(INVALID_ORC_CB, (int)(end - cbStartOffset));
+    log[offset + 1] = cbStartOffset;
+    this.offset += 2;
+  }
+
+  public void logPartialCb(DiskRange current) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 4 > log.length) return;
+    log[offset] = makeIntPair(PARTIAL_CB, 0);
+    logRange(current, offset + 1);
+    this.offset += 4;
+  }
+
+  public void logCompositeOrcCb(int lastChunkTaken, int lastChunkRemaining, DiskRange cc) {
+    if (log == null) return;
+    int offset = this.offset;
+    if (offset + 5 > log.length) return;
+    log[offset] = makeIntPair(COMPOSITE_ORC_CB, lastChunkTaken);
+    log[offset + 1] = lastChunkRemaining;
+    logRange(cc, offset + 2);
+    this.offset += 5;
+  }
+
+  public static FixedSizedObjectPool<IoTrace> createTracePool(Configuration conf) {
+    final int ioTraceSize = (int)HiveConf.getSizeVar(conf, ConfVars.LLAP_IO_TRACE_SIZE);
+    final boolean isAlwaysDump = HiveConf.getBoolVar(conf, ConfVars.LLAP_IO_TRACE_ALWAYS_DUMP);
+    int ioThreads = HiveConf.getIntVar(conf, ConfVars.LLAP_IO_THREADPOOL_SIZE);
+    return new FixedSizedObjectPool<>(ioThreads, new Pool.PoolObjectHelper<IoTrace>() {
+      @Override
+      public IoTrace create() {
+        return new IoTrace(ioTraceSize, isAlwaysDump);
+      }
+
+      @Override
+      public void resetBeforeOffer(IoTrace t) {
+        t.reset();
+      }
+    });
+  }
+}
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java
index 31b0609b83..cdd58df370 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java
@@ -45,7 +45,7 @@ public interface Reader extends org.apache.hadoop.hive.ql.io.orc.Reader {
    * @return The reader.
    */
   EncodedReader encodedReader(Object fileKey, DataCache dataCache, DataReader dataReader,
-      PoolFactory pf) throws IOException;
+      PoolFactory pf, IoTrace trace) throws IOException;
 
   /** The factory that can create (or return) the pools used by encoded reader. */
   public interface PoolFactory {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/ReaderImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/ReaderImpl.java
index 4856fb3ceb..d47ba6b31a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/ReaderImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/ReaderImpl.java
@@ -33,9 +33,9 @@ public ReaderImpl(Path path, ReaderOptions options) throws IOException {
   }
 
   @Override
-  public EncodedReader encodedReader(
-      Object fileKey, DataCache dataCache, DataReader dataReader, PoolFactory pf) throws IOException {
+  public EncodedReader encodedReader(Object fileKey, DataCache dataCache, DataReader dataReader,
+      PoolFactory pf, IoTrace trace) throws IOException {
     return new EncodedReaderImpl(fileKey, types,
-        codec, bufferSize, rowIndexStride, dataCache, dataReader, pf);
+        codec, bufferSize, rowIndexStride, dataCache, dataReader, pf, trace);
   }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/encoded/TestEncodedReaderImpl.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/encoded/TestEncodedReaderImpl.java
index 28ca441ae7..ad0dbcaec0 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/encoded/TestEncodedReaderImpl.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/encoded/TestEncodedReaderImpl.java
@@ -37,18 +37,20 @@ public void testReadLength() throws IOException {
     BufferChunk bc = new BufferChunk(one, 0);
     int[] result = new int[3];
     List<IncompleteCb> l = new ArrayList<>();
-    BufferChunk rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true);
+    IoTrace trace = new IoTrace(0, false);
+    BufferChunk rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(
+        bc, 0l, result, l, true, trace);
     assertNull(rv);
     one.position(0);
     bc.insertAfter(new BufferChunk(two, 1));
     Arrays.fill(result, -1);
-    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true);
+    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true, trace);
     assertNull(rv);
     one.position(0);
     two.position(0);
     bc.insertAfter(new BufferChunk(two, 1)).insertAfter(new BufferChunk(three, 2));
     Arrays.fill(result, -1);
-    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true);
+    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true, trace);
     assertNotNull(rv);
     for (int i = 0; i < result.length; ++i) {
       assertEquals(i + 1, result[i]);
@@ -56,19 +58,19 @@ public void testReadLength() throws IOException {
     one.position(0);
     bc.insertAfter(new BufferChunk(twoThree, 1));
     Arrays.fill(result, -1);
-    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true);
+    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true, trace);
     assertNotNull(rv);
     for (int i = 0; i < result.length; ++i) {
       assertEquals(i + 1, result[i]);
     }
     bc = new BufferChunk(oneTwo, 0);
     Arrays.fill(result, -1);
-    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true);
+    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true, trace);
     assertNull(rv);
     three.position(0);
     bc.insertAfter(new BufferChunk(three, 2));
     Arrays.fill(result, -1);
-    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true);
+    rv = EncodedReaderImpl.readLengthBytesFromSmallBuffers(bc, 0l, result, l, true, trace);
     assertNotNull(rv);
     for (int i = 0; i < result.length; ++i) {
       assertEquals(i + 1, result[i]);
