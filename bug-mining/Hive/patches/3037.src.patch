diff --git a/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java b/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java
index fdf667611c..7bc03c6f44 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java
@@ -110,6 +110,9 @@ public void acquireLocks(QueryPlan plan, Context ctx, String username) throws Lo
     // If a lock needs to be acquired on any partition, a read lock needs to be acquired on all
     // its parents also
     for (ReadEntity input : plan.getInputs()) {
+      if (!input.needsLock()) {
+        continue;
+      }
       LOG.debug("Adding " + input.getName() + " to list of lock inputs");
       if (input.getType() == ReadEntity.Type.DATABASE) {
         lockObjects.addAll(getLockObjects(plan, input.getDatabase(), null,
@@ -125,18 +128,18 @@ public void acquireLocks(QueryPlan plan, Context ctx, String username) throws Lo
     }
 
     for (WriteEntity output : plan.getOutputs()) {
+      HiveLockMode lockMode = getWriteEntityLockMode(output);
+      if (lockMode == null) {
+        continue;
+      }
       LOG.debug("Adding " + output.getName() + " to list of lock outputs");
       List<HiveLockObj> lockObj = null;
       if (output.getType() == WriteEntity.Type.DATABASE) {
-        lockObjects.addAll(getLockObjects(plan, output.getDatabase(), null,
-            null,
-            output.isComplete() ? HiveLockMode.EXCLUSIVE : HiveLockMode.SHARED));
+        lockObjects.addAll(getLockObjects(plan, output.getDatabase(), null, null, lockMode));
       } else if (output.getTyp() == WriteEntity.Type.TABLE) {
-        lockObj = getLockObjects(plan, null, output.getTable(), null,
-            output.isComplete() ? HiveLockMode.EXCLUSIVE : HiveLockMode.SHARED);
+        lockObj = getLockObjects(plan, null, output.getTable(), null,lockMode);
       } else if (output.getTyp() == WriteEntity.Type.PARTITION) {
-        lockObj = getLockObjects(plan, null, null, output.getPartition(),
-            HiveLockMode.EXCLUSIVE);
+        lockObj = getLockObjects(plan, null, null, output.getPartition(), lockMode);
       }
       // In case of dynamic queries, it is possible to have incomplete dummy partitions
       else if (output.getTyp() == WriteEntity.Type.DUMMYPARTITION) {
@@ -249,6 +252,22 @@ static void dedupLockObjects(List<HiveLockObj> lockObjects) {
     }
   }
 
+  private HiveLockMode getWriteEntityLockMode (WriteEntity we) {
+    HiveLockMode lockMode = we.isComplete() ? HiveLockMode.EXCLUSIVE : HiveLockMode.SHARED;
+    //but the writeEntity is complete in DDL operations, and we need check its writeType to
+    //to determine the lockMode
+    switch (we.getWriteType()) {
+      case DDL_EXCLUSIVE:
+        return HiveLockMode.EXCLUSIVE;
+      case DDL_SHARED:
+        return HiveLockMode.SHARED;
+      case DDL_NO_LOCK:
+        return null;
+      default: //other writeTypes related to DMLs
+        return lockMode;
+    }
+  }
+
   private List<HiveLockObj> getLockObjects(QueryPlan plan, Database db,
                                            Table t, Partition p,
                                            HiveLockMode mode)
