diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
index 823d40421e..7ba58b936a 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
@@ -37,6 +37,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil;
+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
@@ -321,6 +322,12 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
     TableDesc currentTable = null;
     TableScanOperator currentTableScan = null;
 
+    boolean pushDownProjection = false;
+    //Buffers to hold filter pushdown information
+    StringBuilder readColumnsBuffer = new StringBuilder(newjob.
+      get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, ""));;
+    StringBuilder readColumnNamesBuffer = new StringBuilder(newjob.
+      get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, ""));
     // for each dir, get the InputFormat, and do getSplits.
     for (Path dir : dirs) {
       PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir);
@@ -336,9 +343,13 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
         Operator op = mrwork.getAliasToWork().get(aliases.get(0));
         if ((op != null) && (op instanceof TableScanOperator)) {
           tableScan = (TableScanOperator) op;
+          //Reset buffers to store filter push down columns
+          readColumnsBuffer.setLength(0);
+          readColumnNamesBuffer.setLength(0);
           // push down projections.
-          ColumnProjectionUtils.appendReadColumns(
-              newjob, tableScan.getNeededColumnIDs(), tableScan.getNeededColumns());
+          ColumnProjectionUtils.appendReadColumns(readColumnsBuffer, readColumnNamesBuffer,
+            tableScan.getNeededColumnIDs(), tableScan.getNeededColumns());
+          pushDownProjection = true;
           // push down filters
           pushFilters(newjob, tableScan);
         }
@@ -366,6 +377,13 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
       currentTable = table;
       currentInputFormatClass = inputFormatClass;
     }
+    if (pushDownProjection) {
+      newjob.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);
+      newjob.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, readColumnsBuffer.toString());
+      newjob.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, readColumnNamesBuffer.toString());
+      LOG.info(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR + "=" + readColumnsBuffer.toString());
+      LOG.info(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR + "=" + readColumnNamesBuffer.toString());
+    }
 
     if (dirs.length != 0) {
       LOG.info("Generating splits");
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
index cfd98f2fff..e403ad9b34 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
@@ -103,6 +103,36 @@ public static void appendReadColumns(
     appendReadColumnNames(conf, names);
   }
 
+  public static void appendReadColumns(
+      StringBuilder readColumnsBuffer, StringBuilder readColumnNamesBuffer, List<Integer> ids,
+      List<String> names) {
+    appendReadColumns(readColumnsBuffer, ids);
+    appendReadColumnNames(readColumnNamesBuffer, names);
+  }
+
+  public static void appendReadColumns(StringBuilder readColumnsBuffer, List<Integer> ids) {
+    String id = toReadColumnIDString(ids);
+    String newConfStr = id;
+    if (readColumnsBuffer.length() > 0) {
+      readColumnsBuffer.append(StringUtils.COMMA_STR).append(newConfStr);
+    }
+    if (readColumnsBuffer.length() == 0) {
+      readColumnsBuffer.append(READ_COLUMN_IDS_CONF_STR_DEFAULT);
+    }
+  }
+
+  private static void appendReadColumnNames(StringBuilder readColumnNamesBuffer, List<String> cols) {
+    boolean first = readColumnNamesBuffer.length() > 0;
+    for(String col: cols) {
+      if (first) {
+        first = false;
+      } else {
+        readColumnNamesBuffer.append(',');
+      }
+      readColumnNamesBuffer.append(col);
+    }
+  }
+
   /**
    * Returns an array of column ids(start from zero) which is set in the given
    * parameter <tt>conf</tt>.
