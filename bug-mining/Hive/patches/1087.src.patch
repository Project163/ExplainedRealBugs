diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java
index 2d7c420ed6..c8d6ca816a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java
@@ -86,7 +86,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
     joinKeys = new HashMap<Byte, List<ExprNodeEvaluator>>();
 
-    JoinUtil.populateJoinKeyValue(joinKeys, conf.getKeys(),order,NOTSKIPBIGTABLE);
+    JoinUtil.populateJoinKeyValue(joinKeys, conf.getKeys(), NOTSKIPBIGTABLE);
     joinKeysObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinKeys,
         inputObjInspectors,NOTSKIPBIGTABLE);
     joinKeysStandardObjectInspectors = JoinUtil.getStandardObjectInspectors(
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
index afe978dafd..f3052d858b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
@@ -246,12 +246,13 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     nullsafes = conf.getNullSafes();
     noOuterJoin = conf.isNoOuterJoin();
 
+    Byte[] reorder = getExecContext() == null ? order : null;
     totalSz = JoinUtil.populateJoinKeyValue(joinValues, conf.getExprs(),
-        order,NOTSKIPBIGTABLE);
+        reorder,NOTSKIPBIGTABLE);
 
     //process join filters
     joinFilters = new HashMap<Byte, List<ExprNodeEvaluator>>();
-    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(),order,NOTSKIPBIGTABLE);
+    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(),reorder,NOTSKIPBIGTABLE);
 
 
     joinValuesObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinValues,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
index 588dd0df21..775c02b9ac 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
@@ -71,7 +71,6 @@ public class HashTableSinkOperator extends TerminalOperator<HashTableSinkDesc> i
    */
   protected transient Map<Byte, List<ObjectInspector>> joinKeysStandardObjectInspectors;
 
-  protected transient int posBigTableTag = -1; // one of the tables that is not in memory
   protected transient int posBigTableAlias = -1; // one of the tables that is not in memory
   transient int mapJoinRowsKey; // rows for a given key
 
@@ -186,12 +185,10 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     firstRow = true;
 
     // for small tables only; so get the big table position first
-    posBigTableTag = conf.getPosBigTable();
+    posBigTableAlias = conf.getPosBigTable();
 
     order = conf.getTagOrder();
 
-    posBigTableAlias = order[posBigTableTag];
-
     // initialize some variables, which used to be initialized in CommonJoinOperator
     numAliases = conf.getExprs().size();
     this.hconf = hconf;
@@ -202,7 +199,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
     // process join keys
     joinKeys = new HashMap<Byte, List<ExprNodeEvaluator>>();
-    JoinUtil.populateJoinKeyValue(joinKeys, conf.getKeys(), order, posBigTableAlias);
+    JoinUtil.populateJoinKeyValue(joinKeys, conf.getKeys(), posBigTableAlias);
     joinKeysObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinKeys,
         inputObjInspectors, posBigTableAlias);
     joinKeysStandardObjectInspectors = JoinUtil.getStandardObjectInspectors(
@@ -210,7 +207,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
     // process join values
     joinValues = new HashMap<Byte, List<ExprNodeEvaluator>>();
-    JoinUtil.populateJoinKeyValue(joinValues, conf.getExprs(), order, posBigTableAlias);
+    JoinUtil.populateJoinKeyValue(joinValues, conf.getExprs(), posBigTableAlias);
     joinValuesObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinValues,
         inputObjInspectors, posBigTableAlias);
     joinValuesStandardObjectInspectors = JoinUtil.getStandardObjectInspectors(
@@ -218,7 +215,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
     // process join filters
     joinFilters = new HashMap<Byte, List<ExprNodeEvaluator>>();
-    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(), order, posBigTableAlias);
+    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(), posBigTableAlias);
     joinFilterObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinFilters,
         inputObjInspectors, posBigTableAlias);
 
@@ -259,7 +256,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
     // initialize the hash tables for other tables
     for (Byte pos : order) {
-      if (pos == posBigTableTag) {
+      if (pos == posBigTableAlias) {
         continue;
       }
 
@@ -313,8 +310,7 @@ public void processOp(Object row, int tag) throws HiveException {
         setKeyMetaData();
         firstRow = false;
       }
-      alias = order[tag];
-      // alias = (byte)tag;
+      alias = (byte)tag;
 
       // compute keys and values as StandardObjects
       AbstractMapJoinKey keyMap = JoinUtil.computeMapJoinKeys(row, joinKeys.get(alias),
@@ -325,8 +321,7 @@ public void processOp(Object row, int tag) throws HiveException {
               .get(alias), filterMap == null ? null : filterMap[alias]);
 
 
-      HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = mapJoinTables
-          .get((byte) tag);
+      HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = mapJoinTables.get(alias);
 
       MapJoinObjectValue o = hashTable.get(keyMap);
       MapJoinRowContainer<Object[]> res = null;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java
index 637e9537e7..ffa397c56e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java
@@ -103,34 +103,29 @@ public static HashMap<Byte, List<ObjectInspector>> getStandardObjectInspectors(
     return result;
 
   }
+
+  public static int populateJoinKeyValue(Map<Byte, List<ExprNodeEvaluator>> outMap,
+      Map<Byte, List<ExprNodeDesc>> inputMap, int posBigTableAlias) {
+    return populateJoinKeyValue(outMap, inputMap, null, posBigTableAlias);
+  }
+
   public static int populateJoinKeyValue(Map<Byte, List<ExprNodeEvaluator>> outMap,
       Map<Byte, List<ExprNodeDesc>> inputMap,
       Byte[] order,
       int posBigTableAlias) {
-
     int total = 0;
-
-    Iterator<Map.Entry<Byte, List<ExprNodeDesc>>> entryIter = inputMap
-        .entrySet().iterator();
-    while (entryIter.hasNext()) {
-      Map.Entry<Byte, List<ExprNodeDesc>> e = entryIter.next();
-      Byte key = order[e.getKey()];
-
+    for (Entry<Byte, List<ExprNodeDesc>> e : inputMap.entrySet()) {
+      Byte key = order == null ? e.getKey() : order[e.getKey()];
       List<ExprNodeEvaluator> valueFields = new ArrayList<ExprNodeEvaluator>();
-
-      List<ExprNodeDesc> expr = e.getValue();
-      int sz = expr.size();
-      total += sz;
-
-      for (int j = 0; j < sz; j++) {
-        if(key == (byte) posBigTableAlias){
+      for (ExprNodeDesc expr : e.getValue()) {
+        if (key == (byte) posBigTableAlias) {
           valueFields.add(null);
-        }else{
-          valueFields.add(ExprNodeEvaluatorFactory.get(expr.get(j)));
+        } else {
+          valueFields.add(ExprNodeEvaluatorFactory.get(expr));
         }
       }
-
       outMap.put(key, valueFields);
+      total += valueFields.size();
     }
 
     return total;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
index 4a048f8736..1daec97032 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
@@ -225,9 +225,7 @@ public void processOp(Object row, int tag) throws HiveException {
         firstRow = false;
       }
 
-      // get alias
-      alias = order[tag];
-      // alias = (byte)tag;
+      alias = (byte)tag;
 
       if ((lastAlias == null) || (!lastAlias.equals(alias))) {
         nextSz = joinEmitInterval;
@@ -242,10 +240,10 @@ public void processOp(Object row, int tag) throws HiveException {
 
 
       // Add the value to the ArrayList
-      storage.get((byte) tag).add(value);
+      storage.get(alias).add(value);
 
       for (Byte pos : order) {
-        if (pos.intValue() != tag) {
+        if (pos.intValue() != alias) {
 
           MapJoinObjectValue o = mapJoinTables.get(pos).get(key);
           MapJoinRowContainer<ArrayList<Object>> rowContainer = rowContainerMap.get(pos);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
index 4ea799b994..710a454e45 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
@@ -302,7 +302,7 @@ public static MapJoinOperator convertMapJoin(
       newParentOps.add(grandParentOp);
     }
 
-    int pos = 0;
+    byte pos = 0;
     // Remove parent reduce-sink operators
     for (String src : joinTree.getBaseSrc()) {
       if (src != null) {
@@ -322,13 +322,12 @@ public static MapJoinOperator convertMapJoin(
     for (pos = 0; pos < newParentOps.size(); pos++) {
       ReduceSinkOperator oldPar = (ReduceSinkOperator) oldReduceSinkParentOps.get(pos);
       ReduceSinkDesc rsconf = oldPar.getConf();
-      Byte tag = (byte) rsconf.getTag();
       List<ExprNodeDesc> keys = rsconf.getKeyCols();
-      keyExprMap.put(tag, keys);
+      keyExprMap.put(pos, keys);
 
       // set column transfer
       HashMap<String, ExprNodeDesc> map = (HashMap<String, ExprNodeDesc>) oldPar.getColumnExprMap();
-      columnTransfer.put(tag, map);
+      columnTransfer.put(pos, map);
     }
 
     // create the map-join operator
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
index b05cded554..d73ce95a6e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
@@ -132,8 +132,6 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object..
 
       // get the last operator for processing big tables
       int bigTable = mapJoinOp.getConf().getPosBigTable();
-      Byte[] order = mapJoinOp.getConf().getTagOrder();
-      int bigTableAlias = (int) order[bigTable];
 
       // the parent ops for hashTableSinkOp
       List<Operator<? extends OperatorDesc>> smallTablesParentOp =
@@ -143,7 +141,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object..
       // get all parents
       List<Operator<? extends OperatorDesc>> parentsOp = mapJoinOp.getParentOperators();
       for (int i = 0; i < parentsOp.size(); i++) {
-        if (i == bigTableAlias) {
+        if (i == bigTable) {
           smallTablesParentOp.add(null);
           continue;
         }
diff --git a/ql/src/test/queries/clientpositive/join_reorder4.q b/ql/src/test/queries/clientpositive/join_reorder4.q
new file mode 100644
index 0000000000..3dabfdeb5d
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/join_reorder4.q
@@ -0,0 +1,18 @@
+CREATE TABLE T1(key1 STRING, val1 STRING) STORED AS TEXTFILE;
+CREATE TABLE T2(key2 STRING, val2 STRING) STORED AS TEXTFILE;
+CREATE TABLE T3(key3 STRING, val3 STRING) STORED AS TEXTFILE;
+
+LOAD DATA LOCAL INPATH '../data/files/T1.txt' INTO TABLE T1;
+LOAD DATA LOCAL INPATH '../data/files/T2.txt' INTO TABLE T2;
+LOAD DATA LOCAL INPATH '../data/files/T3.txt' INTO TABLE T3;
+
+set hive.auto.convert.join=true;
+
+explain select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+
+explain select /*+ STREAMTABLE(b) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+
+explain select /*+ STREAMTABLE(c) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
diff --git a/ql/src/test/results/clientpositive/join_reorder4.q.out b/ql/src/test/results/clientpositive/join_reorder4.q.out
new file mode 100644
index 0000000000..b9697a1796
--- /dev/null
+++ b/ql/src/test/results/clientpositive/join_reorder4.q.out
@@ -0,0 +1,1170 @@
+PREHOOK: query: CREATE TABLE T1(key1 STRING, val1 STRING) STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE T1(key1 STRING, val1 STRING) STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@T1
+PREHOOK: query: CREATE TABLE T2(key2 STRING, val2 STRING) STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE T2(key2 STRING, val2 STRING) STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@T2
+PREHOOK: query: CREATE TABLE T3(key3 STRING, val3 STRING) STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE T3(key3 STRING, val3 STRING) STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@T3
+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' INTO TABLE T1
+PREHOOK: type: LOAD
+PREHOOK: Output: default@t1
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' INTO TABLE T1
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@t1
+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T2.txt' INTO TABLE T2
+PREHOOK: type: LOAD
+PREHOOK: Output: default@t2
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T2.txt' INTO TABLE T2
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@t2
+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T3.txt' INTO TABLE T3
+PREHOOK: type: LOAD
+PREHOOK: Output: default@t3
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T3.txt' INTO TABLE T3
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@t3
+PREHOOK: query: explain select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME T1) a) (TOK_TABREF (TOK_TABNAME T2) b) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL b) key2))) (TOK_TABREF (TOK_TABNAME T3) c) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL c) key3)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_STREAMTABLE (TOK_HINTARGLIST a))) (TOK_SELEXPR TOK_ALLCOLREF))))
+
+STAGE DEPENDENCIES:
+  Stage-7 is a root stage , consists of Stage-8, Stage-9, Stage-10, Stage-1
+  Stage-8 has a backup stage: Stage-1
+  Stage-4 depends on stages: Stage-8
+  Stage-9 has a backup stage: Stage-1
+  Stage-5 depends on stages: Stage-9
+  Stage-10 has a backup stage: Stage-1
+  Stage-6 depends on stages: Stage-10
+  Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-7
+    Conditional Operator
+
+  Stage: Stage-8
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        b 
+          Fetch Operator
+            limit: -1
+        c 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        b 
+          TableScan
+            alias: b
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 0
+        c 
+          TableScan
+            alias: c
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 0
+
+  Stage: Stage-4
+    Map Reduce
+      Alias -> Map Operator Tree:
+        a 
+          TableScan
+            alias: a
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+                   Inner Join 0 to 2
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+              Position of Big Table: 0
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: string
+                      expr: _col8
+                      type: string
+                      expr: _col9
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-9
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        a 
+          Fetch Operator
+            limit: -1
+        c 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        a 
+          TableScan
+            alias: a
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 1
+        c 
+          TableScan
+            alias: c
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 1
+
+  Stage: Stage-5
+    Map Reduce
+      Alias -> Map Operator Tree:
+        b 
+          TableScan
+            alias: b
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+                   Inner Join 0 to 2
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+              Position of Big Table: 1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: string
+                      expr: _col8
+                      type: string
+                      expr: _col9
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-10
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        a 
+          Fetch Operator
+            limit: -1
+        b 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        a 
+          TableScan
+            alias: a
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 2
+        b 
+          TableScan
+            alias: b
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 2
+
+  Stage: Stage-6
+    Map Reduce
+      Alias -> Map Operator Tree:
+        c 
+          TableScan
+            alias: c
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+                   Inner Join 0 to 2
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+              Position of Big Table: 2
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: string
+                      expr: _col8
+                      type: string
+                      expr: _col9
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        a 
+          TableScan
+            alias: a
+            Reduce Output Operator
+              key expressions:
+                    expr: key1
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key1
+                    type: string
+              tag: 2
+              value expressions:
+                    expr: key1
+                    type: string
+                    expr: val1
+                    type: string
+        b 
+          TableScan
+            alias: b
+            Reduce Output Operator
+              key expressions:
+                    expr: key2
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key2
+                    type: string
+              tag: 1
+              value expressions:
+                    expr: key2
+                    type: string
+                    expr: val2
+                    type: string
+        c 
+          TableScan
+            alias: c
+            Reduce Output Operator
+              key expressions:
+                    expr: key3
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key3
+                    type: string
+              tag: 0
+              value expressions:
+                    expr: key3
+                    type: string
+                    expr: val3
+                    type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+               Inner Join 0 to 2
+          condition expressions:
+            0 {VALUE._col0} {VALUE._col1}
+            1 {VALUE._col0} {VALUE._col1}
+            2 {VALUE._col0} {VALUE._col1}
+          handleSkewJoin: false
+          outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: string
+                  expr: _col1
+                  type: string
+                  expr: _col4
+                  type: string
+                  expr: _col5
+                  type: string
+                  expr: _col8
+                  type: string
+                  expr: _col9
+                  type: string
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t1
+PREHOOK: Input: default@t2
+PREHOOK: Input: default@t3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t1
+POSTHOOK: Input: default@t2
+POSTHOOK: Input: default@t3
+#### A masked pattern was here ####
+2	12	2	22	2	12
+PREHOOK: query: explain select /*+ STREAMTABLE(b) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select /*+ STREAMTABLE(b) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME T1) a) (TOK_TABREF (TOK_TABNAME T2) b) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL b) key2))) (TOK_TABREF (TOK_TABNAME T3) c) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL c) key3)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_STREAMTABLE (TOK_HINTARGLIST b))) (TOK_SELEXPR TOK_ALLCOLREF))))
+
+STAGE DEPENDENCIES:
+  Stage-7 is a root stage , consists of Stage-8, Stage-9, Stage-10, Stage-1
+  Stage-8 has a backup stage: Stage-1
+  Stage-4 depends on stages: Stage-8
+  Stage-9 has a backup stage: Stage-1
+  Stage-5 depends on stages: Stage-9
+  Stage-10 has a backup stage: Stage-1
+  Stage-6 depends on stages: Stage-10
+  Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-7
+    Conditional Operator
+
+  Stage: Stage-8
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        b 
+          Fetch Operator
+            limit: -1
+        c 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        b 
+          TableScan
+            alias: b
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 0
+        c 
+          TableScan
+            alias: c
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 0
+
+  Stage: Stage-4
+    Map Reduce
+      Alias -> Map Operator Tree:
+        a 
+          TableScan
+            alias: a
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+                   Inner Join 0 to 2
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+              Position of Big Table: 0
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: string
+                      expr: _col8
+                      type: string
+                      expr: _col9
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-9
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        a 
+          Fetch Operator
+            limit: -1
+        c 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        a 
+          TableScan
+            alias: a
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 1
+        c 
+          TableScan
+            alias: c
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 1
+
+  Stage: Stage-5
+    Map Reduce
+      Alias -> Map Operator Tree:
+        b 
+          TableScan
+            alias: b
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+                   Inner Join 0 to 2
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+              Position of Big Table: 1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: string
+                      expr: _col8
+                      type: string
+                      expr: _col9
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-10
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        a 
+          Fetch Operator
+            limit: -1
+        b 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        a 
+          TableScan
+            alias: a
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 2
+        b 
+          TableScan
+            alias: b
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 2
+
+  Stage: Stage-6
+    Map Reduce
+      Alias -> Map Operator Tree:
+        c 
+          TableScan
+            alias: c
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+                   Inner Join 0 to 2
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+              Position of Big Table: 2
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: string
+                      expr: _col8
+                      type: string
+                      expr: _col9
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        a 
+          TableScan
+            alias: a
+            Reduce Output Operator
+              key expressions:
+                    expr: key1
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key1
+                    type: string
+              tag: 0
+              value expressions:
+                    expr: key1
+                    type: string
+                    expr: val1
+                    type: string
+        b 
+          TableScan
+            alias: b
+            Reduce Output Operator
+              key expressions:
+                    expr: key2
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key2
+                    type: string
+              tag: 2
+              value expressions:
+                    expr: key2
+                    type: string
+                    expr: val2
+                    type: string
+        c 
+          TableScan
+            alias: c
+            Reduce Output Operator
+              key expressions:
+                    expr: key3
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key3
+                    type: string
+              tag: 1
+              value expressions:
+                    expr: key3
+                    type: string
+                    expr: val3
+                    type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+               Inner Join 0 to 2
+          condition expressions:
+            0 {VALUE._col0} {VALUE._col1}
+            1 {VALUE._col0} {VALUE._col1}
+            2 {VALUE._col0} {VALUE._col1}
+          handleSkewJoin: false
+          outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: string
+                  expr: _col1
+                  type: string
+                  expr: _col4
+                  type: string
+                  expr: _col5
+                  type: string
+                  expr: _col8
+                  type: string
+                  expr: _col9
+                  type: string
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t1
+PREHOOK: Input: default@t2
+PREHOOK: Input: default@t3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t1
+POSTHOOK: Input: default@t2
+POSTHOOK: Input: default@t3
+#### A masked pattern was here ####
+2	12	2	22	2	12
+PREHOOK: query: explain select /*+ STREAMTABLE(c) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select /*+ STREAMTABLE(c) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME T1) a) (TOK_TABREF (TOK_TABNAME T2) b) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL b) key2))) (TOK_TABREF (TOK_TABNAME T3) c) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL c) key3)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_STREAMTABLE (TOK_HINTARGLIST c))) (TOK_SELEXPR TOK_ALLCOLREF))))
+
+STAGE DEPENDENCIES:
+  Stage-7 is a root stage , consists of Stage-8, Stage-9, Stage-10, Stage-1
+  Stage-8 has a backup stage: Stage-1
+  Stage-4 depends on stages: Stage-8
+  Stage-9 has a backup stage: Stage-1
+  Stage-5 depends on stages: Stage-9
+  Stage-10 has a backup stage: Stage-1
+  Stage-6 depends on stages: Stage-10
+  Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-7
+    Conditional Operator
+
+  Stage: Stage-8
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        b 
+          Fetch Operator
+            limit: -1
+        c 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        b 
+          TableScan
+            alias: b
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 0
+        c 
+          TableScan
+            alias: c
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 0
+
+  Stage: Stage-4
+    Map Reduce
+      Alias -> Map Operator Tree:
+        a 
+          TableScan
+            alias: a
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+                   Inner Join 0 to 2
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+              Position of Big Table: 0
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: string
+                      expr: _col8
+                      type: string
+                      expr: _col9
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-9
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        a 
+          Fetch Operator
+            limit: -1
+        c 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        a 
+          TableScan
+            alias: a
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 1
+        c 
+          TableScan
+            alias: c
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 1
+
+  Stage: Stage-5
+    Map Reduce
+      Alias -> Map Operator Tree:
+        b 
+          TableScan
+            alias: b
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+                   Inner Join 0 to 2
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+              Position of Big Table: 1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: string
+                      expr: _col8
+                      type: string
+                      expr: _col9
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-10
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        a 
+          Fetch Operator
+            limit: -1
+        b 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        a 
+          TableScan
+            alias: a
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 2
+        b 
+          TableScan
+            alias: b
+            HashTable Sink Operator
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              Position of Big Table: 2
+
+  Stage: Stage-6
+    Map Reduce
+      Alias -> Map Operator Tree:
+        c 
+          TableScan
+            alias: c
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+                   Inner Join 0 to 2
+              condition expressions:
+                0 {key1} {val1}
+                1 {key2} {val2}
+                2 {key3} {val3}
+              handleSkewJoin: false
+              keys:
+                0 [Column[key1]]
+                1 [Column[key2]]
+                2 [Column[key3]]
+              outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+              Position of Big Table: 2
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+                      expr: _col4
+                      type: string
+                      expr: _col5
+                      type: string
+                      expr: _col8
+                      type: string
+                      expr: _col9
+                      type: string
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        a 
+          TableScan
+            alias: a
+            Reduce Output Operator
+              key expressions:
+                    expr: key1
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key1
+                    type: string
+              tag: 0
+              value expressions:
+                    expr: key1
+                    type: string
+                    expr: val1
+                    type: string
+        b 
+          TableScan
+            alias: b
+            Reduce Output Operator
+              key expressions:
+                    expr: key2
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key2
+                    type: string
+              tag: 1
+              value expressions:
+                    expr: key2
+                    type: string
+                    expr: val2
+                    type: string
+        c 
+          TableScan
+            alias: c
+            Reduce Output Operator
+              key expressions:
+                    expr: key3
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key3
+                    type: string
+              tag: 2
+              value expressions:
+                    expr: key3
+                    type: string
+                    expr: val3
+                    type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+               Inner Join 0 to 2
+          condition expressions:
+            0 {VALUE._col0} {VALUE._col1}
+            1 {VALUE._col0} {VALUE._col1}
+            2 {VALUE._col0} {VALUE._col1}
+          handleSkewJoin: false
+          outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: string
+                  expr: _col1
+                  type: string
+                  expr: _col4
+                  type: string
+                  expr: _col5
+                  type: string
+                  expr: _col8
+                  type: string
+                  expr: _col9
+                  type: string
+            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t1
+PREHOOK: Input: default@t2
+PREHOOK: Input: default@t3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t1
+POSTHOOK: Input: default@t2
+POSTHOOK: Input: default@t3
+#### A masked pattern was here ####
+2	12	2	22	2	12
