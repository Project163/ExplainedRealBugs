diff --git a/CHANGES.txt b/CHANGES.txt
index 89c6b14d48..b45826b7d5 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -127,6 +127,8 @@ Trunk -  Unreleased
     HIVE-842 Authentication Infrastructure for Hive
     (Ashutosh Chauhan via He Yongqiang)
 
+    HIVE-1853 Downrgrade JDO (Paul Yang via namit)
+
   IMPROVEMENTS
 
     HIVE-1712. Migrating metadata from derby to mysql thrown NullPointerException (Jake Farrell via pauly)
diff --git a/ivy/libraries.properties b/ivy/libraries.properties
index ef24affded..1d3773bd69 100644
--- a/ivy/libraries.properties
+++ b/ivy/libraries.properties
@@ -23,9 +23,9 @@ antlr.version=3.0.1
 antlr-runtime.version=3.0.1
 asm.version=3.1
 datanucleus-connectionpool.version=2.0.3
-datanucleus-core.version=2.1.1
-datanucleus-enhancer.version=2.1.0-release
-datanucleus-rdbms.version=2.1.1
+datanucleus-core.version=2.0.3
+datanucleus-enhancer.version=2.0.3
+datanucleus-rdbms.version=2.0.3
 checkstyle.version=5.0
 commons-cli.version=2.0-SNAPSHOT
 commons-codec.version=1.3
@@ -37,7 +37,7 @@ commons-logging-api.version=1.0.4
 commons-pool.version=1.5.4
 hbase.version=0.20.3
 hbase-test.version=0.20.3
-jdo-api.version=3.0.0
+jdo-api.version=2.3-ec
 jdom.version=1.1
 jline.version=0.9.94
 junit.version=3.8.1
diff --git a/metastore/ivy.xml b/metastore/ivy.xml
index eb65e1ba20..2d6e49e8ac 100644
--- a/metastore/ivy.xml
+++ b/metastore/ivy.xml
@@ -58,7 +58,7 @@
           <exclude org="org.apache.ant" module="ant"/>
           <exclude org="oracle" module="ojdbc14_g"/>
         </dependency>
-        <dependency org="javax.jdo" name="jdo-api" rev="${jdo-api.version}">
+        <dependency org="javax.jdo" name="jdo2-api" rev="${jdo-api.version}">
           <exclude org="javax.transaction" module="jta"/>
           <exclude org="org.apache.ant" module="ant"/>
           <exclude org="org.apache.geronimo.specs" module="geronimo-jpa_3.0_spec"/>
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
index 1fe7581574..b36c53c8ab 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
@@ -1781,45 +1781,27 @@ public List<Partition> get_partitions_ps(final String db_name,
         throws MetaException, TException {
       incrementCounter("get_partitions_ps");
       logStartPartitionFunction("get_partitions_ps", db_name, tbl_name, part_vals);
+      List<Partition> parts = null;
+      List<Partition> matchingParts = new ArrayList<Partition>();
+
+      // This gets all the partitions and then filters based on the specified
+      // criteria. An alternative approach would be to get all the partition
+      // names, do the filtering on the names, and get the partition for each
+      // of the names. that match.
 
-      Table t;
       try {
-        t = get_table(db_name, tbl_name);
+         parts = get_partitions(db_name, tbl_name, (short) -1);
       } catch (NoSuchObjectException e) {
         throw new MetaException(e.getMessage());
       }
 
-      if (part_vals.size() > t.getPartitionKeys().size()) {
-        throw new MetaException("Incorrect number of partition values");
-      }
-      // Create a map from the partition column name to the partition value
-      Map<String, String> partKeyToValues = new LinkedHashMap<String, String>();
-      int i=0;
-      for (String value : part_vals) {
-        String col = t.getPartitionKeys().get(i).getName();
-        if (value.length() > 0) {
-          partKeyToValues.put(col, value);
+      for (Partition p : parts) {
+        if (MetaStoreUtils.pvalMatches(part_vals, p.getValues())) {
+          matchingParts.add(p);
         }
-        i++;
       }
-      final String filter = MetaStoreUtils.makeFilterStringFromMap(partKeyToValues);
 
-      List<Partition> ret = null;
-      try {
-        ret = executeWithRetry(new Command<List<Partition>>() {
-          @Override
-          List<Partition> run(RawStore ms) throws Exception {
-            return ms.getPartitionsByFilter(db_name, tbl_name, filter, max_parts);
-          }
-        });
-      } catch (MetaException e) {
-        throw e;
-      } catch (Exception e) {
-        assert(e instanceof RuntimeException);
-        throw (RuntimeException)e;
-      }
-
-      return ret;
+      return matchingParts;
     }
 
     @Override
@@ -1835,37 +1817,23 @@ public List<String> get_partition_names_ps(final String db_name,
         throw new MetaException(e.getMessage());
       }
 
-      if (part_vals.size() > t.getPartitionKeys().size()) {
-        throw new MetaException("Incorrect number of partition values");
-      }
-      // Create a map from the partition column name to the partition value
-      Map<String, String> partKeyToValues = new LinkedHashMap<String, String>();
-      int i=0;
-      for (String value : part_vals) {
-        String col = t.getPartitionKeys().get(i).getName();
-        if (value.length() > 0) {
-          partKeyToValues.put(col, value);
-        }
-        i++;
-      }
-      final String filter = MetaStoreUtils.makeFilterStringFromMap(partKeyToValues);
+     List<String> partNames = get_partition_names(db_name, tbl_name, max_parts);
+     List<String> filteredPartNames = new ArrayList<String>();
 
-      List<String> ret = null;
-      try {
-        ret = executeWithRetry(new Command<List<String>>() {
-          @Override
-          List<String> run(RawStore ms) throws Exception {
-            return ms.listPartitionNamesByFilter(db_name, tbl_name, filter, max_parts);
-          }
-        });
-      } catch (MetaException e) {
-        throw e;
-      } catch (Exception e) {
-        assert(e instanceof RuntimeException);
-        throw (RuntimeException)e;
+      for(String name : partNames) {
+        LinkedHashMap<String, String> spec = Warehouse.makeSpecFromName(name);
+        List<String> vals = new ArrayList<String>();
+        // Since we are iterating through a LinkedHashMap, iteration should
+        // return the partition values in the correct order for comparison.
+        for (String val : spec.values()) {
+          vals.add(val);
+        }
+        if (MetaStoreUtils.pvalMatches(part_vals, vals)) {
+          filteredPartNames.add(name);
+        }
       }
 
-      return ret;
+      return filteredPartNames;
     }
 
     @Override
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index 300285170a..4ac023b36d 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -24,8 +24,8 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
 import java.util.Properties;
+import java.util.Map.Entry;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 
@@ -66,9 +66,9 @@
 import org.apache.hadoop.hive.metastore.model.MStorageDescriptor;
 import org.apache.hadoop.hive.metastore.model.MTable;
 import org.apache.hadoop.hive.metastore.model.MType;
-import org.apache.hadoop.hive.metastore.parser.ExpressionTree.ANTLRNoCaseStringStream;
 import org.apache.hadoop.hive.metastore.parser.FilterLexer;
 import org.apache.hadoop.hive.metastore.parser.FilterParser;
+import org.apache.hadoop.hive.metastore.parser.ExpressionTree.ANTLRNoCaseStringStream;
 import org.apache.hadoop.util.StringUtils;
 
 /**
@@ -1022,53 +1022,8 @@ private String makeParameterDeclarationString(Map<String, String> params) {
 
   private List<MPartition> listMPartitionsByFilter(String dbName, String tableName,
       String filter, short maxParts) throws MetaException, NoSuchObjectException{
-    boolean success = false;
-    List<MPartition> mparts = null;
-    try {
-      openTransaction();
-      LOG.debug("Executing listMPartitionsByFilter");
-      dbName = dbName.toLowerCase();
-      tableName = tableName.toLowerCase();
-
-      MTable mtable = getMTable(dbName, tableName);
-      if( mtable == null ) {
-        throw new NoSuchObjectException("Specified database/table does not exist : "
-            + dbName + "." + tableName);
-      }
-      Map<String, String> params = new HashMap<String, String>();
-      String queryFilterString =
-        makeQueryFilterString(mtable, filter, params);
-
-      Query query = pm.newQuery(MPartition.class,
-          queryFilterString);
-
-      if( maxParts >= 0 ) {
-        //User specified a row limit, set it on the Query
-        query.setRange(0, maxParts);
-      }
-
-      LOG.debug("Filter specified is " + filter + "," +
-             " JDOQL filter is " + queryFilterString);
-
-      params.put("t1", tableName.trim());
-      params.put("t2", dbName.trim());
-
-      String parameterDeclaration = makeParameterDeclarationString(params);
-      query.declareParameters(parameterDeclaration);
-      query.setOrdering("partitionName ascending");
-
-      mparts = (List<MPartition>) query.executeWithMap(params);
-
-      LOG.debug("Done executing query for listMPartitionsByFilter");
-      pm.retrieveAll(mparts);
-      success = commitTransaction();
-      LOG.debug("Done retrieving all objects for listMPartitionsByFilter");
-    } finally {
-      if (!success) {
-        rollbackTransaction();
-      }
-    }
-    return mparts;
+    throw new RuntimeException("listMPartitionsByFilter is not supported " +
+        "due to a JDO library downgrade");
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 25f7a06bb3..8cba80701d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -424,8 +424,8 @@ public void exceptionThrown(Exception e) {
     e.setPersistenceDelegate(GroupByDesc.Mode.class, new EnumDelegate());
     e.setPersistenceDelegate(Operator.ProgressCounter.class, new EnumDelegate());
 
-    e.setPersistenceDelegate(org.datanucleus.store.types.sco.backed.Map.class, new MapDelegate());
-    e.setPersistenceDelegate(org.datanucleus.store.types.sco.backed.List.class, new ListDelegate());
+    e.setPersistenceDelegate(org.datanucleus.sco.backed.Map.class, new MapDelegate());
+    e.setPersistenceDelegate(org.datanucleus.sco.backed.List.class, new ListDelegate());
 
     e.writeObject(plan);
     e.close();
