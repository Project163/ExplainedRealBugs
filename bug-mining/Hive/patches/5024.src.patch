diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java
index a27266f4ed..e803125d04 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java
@@ -115,11 +115,6 @@ public RecordReader<NullWritable, VectorizedRowBatch> getRecordReader(
     }
     boolean isVectorized = Utilities.getUseVectorizedInputFileFormat(job);
 
-    // validate for supported types. Until we fix HIVE-14089 we need this check.
-    if (useLlapIo) {
-      useLlapIo = Utilities.checkLlapIOSupportedTypes(job);
-    }
-
     if (!useLlapIo) {
       LlapIoImpl.LOG.warn("Not using LLAP IO for an unsupported split: " + split);
       return sourceInputFormat.getRecordReader(split, job, reporter);
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
index 2fa68a94da..8bd985c581 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
@@ -6,9 +6,7 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
+ * http://www.apache.org/licenses/LICENSE-2.0
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -18,6 +16,7 @@
 package org.apache.hadoop.hive.llap.io.decode;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.List;
 
 import org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch;
@@ -40,6 +39,8 @@
 import org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.orc.CompressionCodec;
+import org.apache.orc.impl.PositionProvider;
+import org.apache.orc.OrcProto.RowIndexEntry;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Consumer;
 import org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory;
 import org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.SettableTreeReader;
@@ -49,12 +50,19 @@
 import org.apache.orc.OrcUtils;
 import org.apache.orc.TypeDescription;
 import org.apache.orc.impl.TreeReaderFactory;
+import org.apache.orc.impl.TreeReaderFactory.StructTreeReader;
+import org.apache.orc.impl.TreeReaderFactory.TreeReader;
 import org.apache.hadoop.hive.ql.io.orc.WriterImpl;
 import org.apache.orc.OrcProto;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 
 public class OrcEncodedDataConsumer
   extends EncodedDataConsumer<OrcBatchKey, OrcEncodedColumnBatch> {
+  public static final Logger LOG = LoggerFactory.getLogger(OrcEncodedDataConsumer.class);
   private TreeReaderFactory.TreeReader[] columnReaders;
+  private int[] columnMapping; // Mapping from columnReaders (by index) to columns in file schema.
   private int previousStripeIndex = -1;
   private OrcFileMetadata fileMetadata; // We assume one request is only for one file.
   private CompressionCodec codec;
@@ -84,58 +92,6 @@ public void setStripeMetadata(OrcStripeMetadata m) {
     stripes[m.getStripeIx()] = m;
   }
 
-  private static ColumnVector createColumn(List<OrcProto.Type> types,
-      final int columnId, int batchSize) {
-    OrcProto.Type type = types.get(columnId);
-    switch (type.getKind()) {
-      case BOOLEAN:
-      case BYTE:
-      case SHORT:
-      case INT:
-      case LONG:
-      case DATE:
-        return new LongColumnVector(batchSize);
-      case FLOAT:
-      case DOUBLE:
-        return new DoubleColumnVector(batchSize);
-      case BINARY:
-      case STRING:
-      case CHAR:
-      case VARCHAR:
-        return new BytesColumnVector(batchSize);
-      case TIMESTAMP:
-        return new TimestampColumnVector(batchSize);
-      case DECIMAL:
-        return new DecimalColumnVector(batchSize, type.getPrecision(),
-            type.getScale());
-      case STRUCT: {
-        List<Integer> subtypeIdxs = type.getSubtypesList();
-        ColumnVector[] fieldVector = new ColumnVector[subtypeIdxs.size()];
-        for(int i=0; i < fieldVector.length; ++i) {
-          fieldVector[i] = createColumn(types, subtypeIdxs.get(i), batchSize);
-        }
-        return new StructColumnVector(batchSize, fieldVector);
-      }
-      case UNION: {
-        List<Integer> subtypeIdxs = type.getSubtypesList();
-        ColumnVector[] fieldVector = new ColumnVector[subtypeIdxs.size()];
-        for(int i=0; i < fieldVector.length; ++i) {
-          fieldVector[i] = createColumn(types, subtypeIdxs.get(i), batchSize);
-        }
-        return new UnionColumnVector(batchSize, fieldVector);
-      }
-      case LIST:
-        return new ListColumnVector(batchSize, createColumn(types, type.getSubtypes(0), batchSize));
-      case MAP:
-        return new MapColumnVector(batchSize,
-            createColumn(types, type.getSubtypes(0), batchSize),
-            createColumn(types, type.getSubtypes(1), batchSize));
-      default:
-        throw new IllegalArgumentException("LLAP does not support " +
-            type.getKind());
-    }
-  }
-
   @Override
   protected void decodeBatch(OrcEncodedColumnBatch batch,
       Consumer<ColumnVectorBatch> downstreamConsumer) {
@@ -157,14 +113,18 @@ protected void decodeBatch(OrcEncodedColumnBatch batch,
       }
       int maxBatchesRG = (int) ((nonNullRowCount / VectorizedRowBatch.DEFAULT_SIZE) + 1);
       int batchSize = VectorizedRowBatch.DEFAULT_SIZE;
-      int numCols = batch.getColumnIxs().length;
+      TypeDescription schema = fileMetadata.getSchema();
+
       if (columnReaders == null || !sameStripe) {
-        this.columnReaders = EncodedTreeReaderFactory.createEncodedTreeReader(numCols,
-            fileMetadata.getTypes(), stripeMetadata.getEncodings(), batch, codec, skipCorrupt,
-                stripeMetadata.getWriterTimezone());
-        positionInStreams(columnReaders, batch, numCols, stripeMetadata);
+        int[] columnMapping = new int[schema.getChildren().size()];
+        StructTreeReader treeReader = EncodedTreeReaderFactory.createRootTreeReader(
+            schema, stripeMetadata.getEncodings(), batch, codec, skipCorrupt,
+            stripeMetadata.getWriterTimezone(), columnMapping);
+        this.columnReaders = treeReader.getChildReaders();
+        this.columnMapping = Arrays.copyOf(columnMapping, columnReaders.length);
+        positionInStreams(columnReaders, batch, stripeMetadata);
       } else {
-        repositionInStreams(this.columnReaders, batch, sameStripe, numCols, stripeMetadata);
+        repositionInStreams(this.columnReaders, batch, sameStripe, stripeMetadata);
       }
       previousStripeIndex = currentStripeIndex;
 
@@ -176,18 +136,17 @@ protected void decodeBatch(OrcEncodedColumnBatch batch,
         }
 
         ColumnVectorBatch cvb = cvbPool.take();
-        assert cvb.cols.length == batch.getColumnIxs().length; // Must be constant per split.
+        // assert cvb.cols.length == batch.getColumnIxs().length; // Must be constant per split.
         cvb.size = batchSize;
-        List<OrcProto.Type> types = fileMetadata.getTypes();
-        int[] columnMapping = batch.getColumnIxs();
-        for (int idx = 0; idx < batch.getColumnIxs().length; idx++) {
+        for (int idx = 0; idx < columnReaders.length; ++idx) {
+          TreeReader reader = columnReaders[idx];
           if (cvb.cols[idx] == null) {
             // Orc store rows inside a root struct (hive writes it this way).
             // When we populate column vectors we skip over the root struct.
-            cvb.cols[idx] = createColumn(types, columnMapping[idx], batchSize);
+            cvb.cols[idx] = createColumn(schema.getChildren().get(columnMapping[idx]), batchSize);
           }
           cvb.cols[idx].ensureSize(batchSize, false);
-          columnReaders[idx].nextVector(cvb.cols[idx], null, batchSize);
+          reader.nextVector(cvb.cols[idx], null, batchSize);
         }
 
         // we are done reading a batch, send it to consumer for processing
@@ -203,36 +162,98 @@ protected void decodeBatch(OrcEncodedColumnBatch batch,
     }
   }
 
+  private ColumnVector createColumn(TypeDescription type, int batchSize) {
+    switch (type.getCategory()) {
+      case BOOLEAN:
+      case BYTE:
+      case SHORT:
+      case INT:
+      case LONG:
+      case DATE:
+        return new LongColumnVector(batchSize);
+      case FLOAT:
+      case DOUBLE:
+        return new DoubleColumnVector(batchSize);
+      case BINARY:
+      case STRING:
+      case CHAR:
+      case VARCHAR:
+        return new BytesColumnVector(batchSize);
+      case TIMESTAMP:
+        return new TimestampColumnVector(batchSize);
+      case DECIMAL:
+        return new DecimalColumnVector(batchSize, type.getPrecision(),
+            type.getScale());
+      case STRUCT: {
+        List<TypeDescription> subtypeIdxs = type.getChildren();
+        ColumnVector[] fieldVector = new ColumnVector[subtypeIdxs.size()];
+        for(int i = 0; i < fieldVector.length; ++i) {
+          fieldVector[i] = createColumn(subtypeIdxs.get(i), batchSize);
+        }
+        return new StructColumnVector(batchSize, fieldVector);
+      }
+      case UNION: {
+        List<TypeDescription> subtypeIdxs = type.getChildren();
+        ColumnVector[] fieldVector = new ColumnVector[subtypeIdxs.size()];
+        for(int i=0; i < fieldVector.length; ++i) {
+          fieldVector[i] = createColumn(subtypeIdxs.get(i), batchSize);
+        }
+        return new UnionColumnVector(batchSize, fieldVector);
+      }
+      case LIST:
+        return new ListColumnVector(batchSize, createColumn(type.getChildren().get(0), batchSize));
+      case MAP:
+        List<TypeDescription> subtypeIdxs = type.getChildren();
+        return new MapColumnVector(batchSize, createColumn(subtypeIdxs.get(0), batchSize),
+            createColumn(subtypeIdxs.get(1), batchSize));
+      default:
+        throw new IllegalArgumentException("LLAP does not support " + type.getCategory());
+    }
+  }
+
   private void positionInStreams(TreeReaderFactory.TreeReader[] columnReaders,
-      EncodedColumnBatch<OrcBatchKey> batch, int numCols,
-      OrcStripeMetadata stripeMetadata) throws IOException {
-    for (int i = 0; i < numCols; i++) {
-      int columnIndex = batch.getColumnIxs()[i];
-      int rowGroupIndex = batch.getBatchKey().rgIx;
-      OrcProto.RowIndex rowIndex = stripeMetadata.getRowIndexes()[columnIndex];
-      OrcProto.RowIndexEntry rowIndexEntry = rowIndex.getEntry(rowGroupIndex);
-      columnReaders[i].seek(new RecordReaderImpl.PositionProviderImpl(rowIndexEntry));
+      EncodedColumnBatch<OrcBatchKey> batch, OrcStripeMetadata stripeMetadata) throws IOException {
+    PositionProvider[] pps = createPositionProviders(columnReaders, batch, stripeMetadata);
+    if (pps == null) return;
+    for (int i = 0; i < columnReaders.length; i++) {
+      columnReaders[i].seek(pps);
     }
   }
 
   private void repositionInStreams(TreeReaderFactory.TreeReader[] columnReaders,
-      EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe, int numCols,
+      EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe,
       OrcStripeMetadata stripeMetadata) throws IOException {
-    for (int i = 0; i < numCols; i++) {
-      int columnIndex = batch.getColumnIxs()[i];
-      int rowGroupIndex = batch.getBatchKey().rgIx;
-      ColumnStreamData[] streamBuffers = batch.getColumnData()[i];
-      OrcProto.RowIndex rowIndex = stripeMetadata.getRowIndexes()[columnIndex];
-      OrcProto.RowIndexEntry rowIndexEntry = rowIndex.getEntry(rowGroupIndex);
-      ((SettableTreeReader)columnReaders[i]).setBuffers(streamBuffers, sameStripe);
-      // TODO: When hive moves to java8, make updateTimezone() as default method in SettableTreeReader so that we can
-      // avoid this check
-      if (columnReaders[i] instanceof EncodedTreeReaderFactory.TimestampStreamReader && !sameStripe) {
-        ((EncodedTreeReaderFactory.TimestampStreamReader) columnReaders[i])
+    PositionProvider[] pps = createPositionProviders(columnReaders, batch, stripeMetadata);
+    if (pps == null) return;
+    for (int i = 0; i < columnReaders.length; i++) {
+      TreeReader reader = columnReaders[i];
+      ((SettableTreeReader) reader).setBuffers(batch, sameStripe);
+      // TODO: When hive moves to java8, make updateTimezone() as default method in
+      // SettableTreeReader so that we can avoid this check.
+      if (reader instanceof EncodedTreeReaderFactory.TimestampStreamReader && !sameStripe) {
+        ((EncodedTreeReaderFactory.TimestampStreamReader) reader)
                 .updateTimezone(stripeMetadata.getWriterTimezone());
       }
-      columnReaders[i].seek(new RecordReaderImpl.PositionProviderImpl(rowIndexEntry));
+      reader.seek(pps);
+    }
+  }
+
+  private PositionProvider[] createPositionProviders(TreeReaderFactory.TreeReader[] columnReaders,
+      EncodedColumnBatch<OrcBatchKey> batch, OrcStripeMetadata stripeMetadata) throws IOException {
+    if (columnReaders.length == 0) return null;
+    int rowGroupIndex = batch.getBatchKey().rgIx;
+    if (rowGroupIndex == OrcEncodedColumnBatch.ALL_RGS) {
+      throw new IOException("Cannot position readers without RG information");
+    }
+    // TODO: this assumes indexes in getRowIndexes would match column IDs
+    OrcProto.RowIndex[] ris = stripeMetadata.getRowIndexes();
+    PositionProvider[] pps = new PositionProvider[ris.length];
+    for (int i = 0; i < ris.length; ++i) {
+      OrcProto.RowIndex ri = ris[i];
+      if (ri == null) continue;
+      pps[i] = new RecordReaderImpl.PositionProviderImpl(ri.getEntry(rowGroupIndex));
     }
+    return pps;
   }
 
   private long getRowCount(OrcProto.RowIndexEntry rowIndexEntry) {
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
index 72f733a544..39f4112497 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
@@ -138,7 +138,7 @@ public Pool<OrcEncodedColumnBatch> createEncodedColumnBatchPool() {
   private final BufferUsageManager bufferManager;
   private final Configuration conf;
   private final FileSplit split;
-  private List<Integer> columnIds;
+  private List<Integer> includedColumnIds;
   private final SearchArgument sarg;
   private final String[] columnNames;
   private final OrcEncodedDataConsumer consumer;
@@ -174,9 +174,9 @@ public OrcEncodedDataReader(LowLevelCache lowLevelCache, BufferUsageManager buff
     this.bufferManager = bufferManager;
     this.conf = conf;
     this.split = split;
-    this.columnIds = columnIds;
-    if (this.columnIds != null) {
-      Collections.sort(this.columnIds);
+    this.includedColumnIds = columnIds;
+    if (this.includedColumnIds != null) {
+      Collections.sort(this.includedColumnIds);
     }
     this.sarg = sarg;
     this.columnNames = columnNames;
@@ -197,7 +197,7 @@ public OrcEncodedDataReader(LowLevelCache lowLevelCache, BufferUsageManager buff
     fileKey = determineFileId(fs, split,
         HiveConf.getBoolVar(conf, ConfVars.LLAP_CACHE_ALLOW_SYNTHETIC_FILEID));
     fileMetadata = getOrReadFileMetadata();
-    globalIncludes = OrcInputFormat.genIncludedColumns(fileMetadata.getTypes(), columnIds, true);
+    globalIncludes = OrcInputFormat.genIncludedColumns(fileMetadata.getTypes(), includedColumnIds, true);
     consumer.setFileMetadata(fileMetadata);
     consumer.setIncludedColumns(globalIncludes);
   }
@@ -242,8 +242,8 @@ protected Void performDataRead() throws IOException {
         + (fileKey == null ? "" : " (" + fileKey + ")"));
     try {
       validateFileMetadata();
-      if (columnIds == null) {
-        columnIds = createColumnIds(fileMetadata);
+      if (includedColumnIds == null) {
+        includedColumnIds = getAllColumnIds(fileMetadata);
       }
 
       // 2. Determine which stripes to read based on the split.
@@ -303,13 +303,7 @@ protected Void performDataRead() throws IOException {
       return null;
     }
 
-    // 4. Get data from high-level cache.
-    //    If some cols are fully in cache, this will also give us the modified list of columns to
-    //    read for every stripe (null means read all of them - the usual path). In any case,
-    //    readState will be modified for column x rgs that were fetched from high-level cache.
-    List<Integer>[] stripeColsToRead = null;
-
-    // 5. Create encoded data reader.
+    // 4. Create encoded data reader.
     try {
       ensureOrcReader();
       // Reader creating updates HDFS counters, don't do it here.
@@ -335,17 +329,17 @@ protected Void performDataRead() throws IOException {
       }
       int stripeIx = stripeIxFrom + stripeIxMod;
       boolean[][] colRgs = null;
-      boolean[] stripeIncludes = null;
       OrcStripeMetadata stripeMetadata = null;
       StripeInformation stripe;
       try {
-        List<Integer> cols = stripeColsToRead == null ? null : stripeColsToRead[stripeIxMod];
-        if (cols != null && cols.isEmpty()) continue; // No need to read this stripe.
         stripe = fileMetadata.getStripes().get(stripeIx);
 
         LlapIoImpl.ORC_LOGGER.trace("Reading stripe {}: {}, {}", stripeIx, stripe.getOffset(),
             stripe.getLength());
         colRgs = readState[stripeIxMod];
+        if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
+          LlapIoImpl.ORC_LOGGER.trace("readState[{}]: {}", stripeIxMod, Arrays.toString(colRgs));
+        }
         // We assume that NO_RGS value is only set from SARG filter and for all columns;
         // intermediate changes for individual columns will unset values in the array.
         // Skip this case for 0-column read. We could probably special-case it just like we do
@@ -353,17 +347,6 @@ protected Void performDataRead() throws IOException {
         if (colRgs.length > 0 && colRgs[0] ==
             RecordReaderImpl.SargApplier.READ_NO_RGS) continue;
 
-        // 6.1. Determine the columns to read (usually the same as requested).
-        if (cols == null || cols.size() == colRgs.length) {
-          cols = columnIds;
-          stripeIncludes = globalIncludes;
-        } else {
-          // We are reading subset of the original columns, remove unnecessary bitmasks/etc.
-          // This will never happen w/o high-level cache.
-          stripeIncludes = OrcInputFormat.genIncludedColumns(fileMetadata.getTypes(), cols, true);
-          colRgs = genStripeColRgs(cols, colRgs);
-        }
-
         // 6.2. Ensure we have stripe metadata. We might have read it before for RG filtering.
         boolean isFoundInCache = false;
         if (stripeMetadatas != null) {
@@ -379,27 +362,27 @@ protected Void performDataRead() throws IOException {
             ensureMetadataReader();
             long startTimeHdfs = counters.startTimeCounter();
             stripeMetadata = new OrcStripeMetadata(new OrcBatchKey(fileKey, stripeIx, 0),
-                metadataReader, stripe, stripeIncludes, sargColumns);
+                metadataReader, stripe, globalIncludes, sargColumns);
             counters.incrTimeCounter(LlapIOCounters.HDFS_TIME_NS, startTimeHdfs);
             if (hasFileId && metadataCache != null) {
               stripeMetadata = metadataCache.putStripeMetadata(stripeMetadata);
               if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
                 LlapIoImpl.ORC_LOGGER.trace("Caching stripe {} metadata with includes: {}",
-                    stripeKey.stripeIx, DebugUtils.toString(stripeIncludes));
+                    stripeKey.stripeIx, DebugUtils.toString(globalIncludes));
               }
             }
           }
           consumer.setStripeMetadata(stripeMetadata);
         }
-        if (!stripeMetadata.hasAllIndexes(stripeIncludes)) {
+        if (!stripeMetadata.hasAllIndexes(globalIncludes)) {
           if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
             LlapIoImpl.ORC_LOGGER.trace("Updating indexes in stripe {} metadata for includes: {}",
-                stripeKey.stripeIx, DebugUtils.toString(stripeIncludes));
+                stripeKey.stripeIx, DebugUtils.toString(globalIncludes));
           }
           assert isFoundInCache;
           counters.incrCounter(LlapIOCounters.METADATA_CACHE_MISS);
           ensureMetadataReader();
-          updateLoadedIndexes(stripeMetadata, stripe, stripeIncludes, sargColumns);
+          updateLoadedIndexes(stripeMetadata, stripe, globalIncludes, sargColumns);
         } else if (isFoundInCache) {
           counters.incrCounter(LlapIOCounters.METADATA_CACHE_HIT);
         }
@@ -415,7 +398,7 @@ protected Void performDataRead() throws IOException {
         return null;
       }
 
-      // 6.3. Finally, hand off to the stripe reader to produce the data.
+      // 5.2. Finally, hand off to the stripe reader to produce the data.
       //      This is a sync call that will feed data to the consumer.
       try {
         // TODO: readEncodedColumns is not supposed to throw; errors should be propagated thru
@@ -423,7 +406,7 @@ protected Void performDataRead() throws IOException {
         // Also, currently readEncodedColumns is not stoppable. The consumer will discard the
         // data it receives for one stripe. We could probably interrupt it, if it checked that.
         stripeReader.readEncodedColumns(stripeIx, stripe, stripeMetadata.getRowIndexes(),
-            stripeMetadata.getEncodings(), stripeMetadata.getStreams(), stripeIncludes,
+            stripeMetadata.getEncodings(), stripeMetadata.getStreams(), globalIncludes,
             colRgs, consumer);
       } catch (Throwable t) {
         consumer.setError(t);
@@ -521,22 +504,14 @@ private static Object determineFileId(FileSystem fs, FileSplit split,
     return HdfsUtils.getFileId(fs, split.getPath(), allowSynthetic);
   }
 
-  private boolean[][] genStripeColRgs(List<Integer> stripeCols, boolean[][] globalColRgs) {
-    boolean[][] stripeColRgs = new boolean[stripeCols.size()][];
-    for (int i = 0, i2 = -1; i < globalColRgs.length; ++i) {
-      if (globalColRgs[i] == null) continue;
-      stripeColRgs[i2] = globalColRgs[i];
-      ++i2;
-    }
-    return stripeColRgs;
-  }
-
   /**
    * Puts all column indexes from metadata to make a column list to read all column.
    */
-  private static List<Integer> createColumnIds(OrcFileMetadata metadata) {
-    List<Integer> columnIds = new ArrayList<Integer>(metadata.getTypes().size());
-    for (int i = 1; i < metadata.getTypes().size(); ++i) {
+  private static List<Integer> getAllColumnIds(OrcFileMetadata metadata) {
+    int rootColumn = OrcInputFormat.getRootColumn(true);
+    List<Integer> types = metadata.getTypes().get(rootColumn).getSubtypesList();
+    List<Integer> columnIds = new ArrayList<Integer>(types.size());
+    for (int i = 0; i < types.size(); ++i) {
       columnIds.add(i);
     }
     return columnIds;
@@ -690,8 +665,9 @@ private void ensureMetadataReader() throws IOException {
 
   @Override
   public void returnData(OrcEncodedColumnBatch ecb) {
-    for (ColumnStreamData[] datas : ecb.getColumnData()) {
-      if (datas == null) continue;
+    for (int colIx = 0; colIx < ecb.getTotalColCount(); ++colIx) {
+      if (!ecb.hasData(colIx)) continue;
+      ColumnStreamData[] datas = ecb.getColumnData(colIx);
       for (ColumnStreamData data : datas) {
         if (data == null || data.decRef() != 0) continue;
         if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {
@@ -749,12 +725,17 @@ private boolean determineRgsToRead(boolean[] globalIncludes, int rowIndexStride,
         }
       }
       assert isAll || isNone || rgsToRead.length == rgCount;
-      readState[stripeIxMod] = new boolean[columnIds.size()][];
-      for (int j = 0; j < columnIds.size(); ++j) {
-        readState[stripeIxMod][j] = (isAll || isNone) ? rgsToRead :
+      int fileIncludesCount = 0;
+      // TODO: hacky for now - skip the root 0-s column.
+      //        We don't need separate readState w/o HL cache, should get rid of that instead.
+      for (int includeIx = 1; includeIx < globalIncludes.length; ++includeIx) {
+        fileIncludesCount += (globalIncludes[includeIx] ? 1 : 0);
+      }
+      readState[stripeIxMod] = new boolean[fileIncludesCount][];
+      for (int includeIx = 0; includeIx < fileIncludesCount; ++includeIx) {
+        readState[stripeIxMod][includeIx] = (isAll || isNone) ? rgsToRead :
           Arrays.copyOf(rgsToRead, rgsToRead.length);
       }
-
       adjustRgMetric(rgCount, rgsToRead, isNone, isAll);
     }
     return hasAnyData;
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcFileMetadata.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcFileMetadata.java
index c9b0a4d716..70cba05afe 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcFileMetadata.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcFileMetadata.java
@@ -32,7 +32,9 @@
 import org.apache.orc.CompressionKind;
 import org.apache.orc.FileMetadata;
 import org.apache.orc.OrcProto;
+import org.apache.orc.OrcUtils;
 import org.apache.orc.StripeInformation;
+import org.apache.orc.TypeDescription;
 import org.apache.orc.impl.ReaderImpl;
 
 /** ORC file metadata. Currently contains some duplicate info due to how different parts
@@ -222,4 +224,8 @@ public long getNumberOfRows() {
   public List<OrcProto.ColumnStatistics> getFileStats() {
     return fileStats;
   }
+
+  public TypeDescription getSchema() {
+    return OrcUtils.convertTypeFromProtobuf(this.types, 0);
+  }
 }
diff --git a/orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java b/orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java
index c3471815af..5d5f991500 100644
--- a/orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java
+++ b/orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java
@@ -292,7 +292,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
      // Pass-thru.
       convertTreeReader.seek(index);
     }
diff --git a/orc/src/java/org/apache/orc/impl/TreeReaderFactory.java b/orc/src/java/org/apache/orc/impl/TreeReaderFactory.java
index 245e3ce365..e46a0a4ceb 100644
--- a/orc/src/java/org/apache/orc/impl/TreeReaderFactory.java
+++ b/orc/src/java/org/apache/orc/impl/TreeReaderFactory.java
@@ -80,7 +80,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       }
     }
 
-    static IntegerReader createIntegerReader(OrcProto.ColumnEncoding.Kind kind,
+    protected static IntegerReader createIntegerReader(OrcProto.ColumnEncoding.Kind kind,
         InStream in,
         boolean signed, boolean skipCorrupt) throws IOException {
       switch (kind) {
@@ -115,7 +115,7 @@ void startStripe(Map<StreamName, InStream> streams,
      * @param index the indexes loaded from the file
      * @throws IOException
      */
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -201,6 +201,10 @@ public void nextVector(ColumnVector previous,
     public BitFieldReader getPresent() {
       return present;
     }
+
+    public int getColumnId() {
+      return columnId;
+    }
   }
 
   public static class NullTreeReader extends TreeReader {
@@ -262,7 +266,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -313,7 +317,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -380,7 +384,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -447,7 +451,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -515,7 +519,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -569,7 +573,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -663,7 +667,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -774,7 +778,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -898,7 +902,7 @@ protected long getBaseTimestamp(String timeZoneId) throws IOException {
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -1007,7 +1011,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -1083,7 +1087,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -1197,7 +1201,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       reader.seek(index);
     }
 
@@ -1346,7 +1350,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -1495,7 +1499,7 @@ private void readDictionaryStream(InStream in) throws IOException {
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       seek(index[columnId]);
     }
 
@@ -1701,7 +1705,7 @@ public void nextVector(ColumnVector previousVector,
     }
   }
 
-  protected static class StructTreeReader extends TreeReader {
+  public static class StructTreeReader extends TreeReader {
     protected final TreeReader[] fields;
 
     protected StructTreeReader(int columnId,
@@ -1719,8 +1723,21 @@ protected StructTreeReader(int columnId,
       }
     }
 
+    public TreeReader[] getChildReaders() {
+      return fields;
+    }
+
+    protected StructTreeReader(int columnId, InStream present,
+        OrcProto.ColumnEncoding encoding, TreeReader[] childReaders) throws IOException {
+      super(columnId, present);
+      if (encoding != null) {
+        checkEncoding(encoding);
+      }
+      this.fields = childReaders;
+    }
+
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       super.seek(index);
       for (TreeReader kid : fields) {
         if (kid != null) {
@@ -1804,8 +1821,17 @@ protected UnionTreeReader(int fileColumn,
       }
     }
 
+    protected UnionTreeReader(int columnId, InStream present,
+        OrcProto.ColumnEncoding encoding, TreeReader[] childReaders) throws IOException {
+      super(columnId, present);
+      if (encoding != null) {
+        checkEncoding(encoding);
+      }
+      this.fields = childReaders;
+    }
+
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       super.seek(index);
       tags.seek(index[columnId]);
       for (TreeReader kid : fields) {
@@ -1877,8 +1903,18 @@ protected ListTreeReader(int fileColumn,
           skipCorrupt);
     }
 
+    protected ListTreeReader(int columnId, InStream present, InStream data,
+        OrcProto.ColumnEncoding encoding, TreeReader elementReader) throws IOException {
+      super(columnId, present);
+      if (data != null && encoding != null) {
+        checkEncoding(encoding);
+        this.lengths = createIntegerReader(encoding.getKind(), data, false, false);
+      }
+      this.elementReader = elementReader;
+    }
+
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       super.seek(index);
       lengths.seek(index[columnId]);
       elementReader.seek(index);
@@ -1958,8 +1994,20 @@ protected MapTreeReader(int fileColumn,
       valueReader = createTreeReader(valueType, evolution, included, skipCorrupt);
     }
 
+    protected MapTreeReader(int columnId, InStream present, InStream data,
+        OrcProto.ColumnEncoding encoding, TreeReader keyReader, TreeReader valueReader)
+        throws IOException {
+      super(columnId, present);
+      if (data != null && encoding != null) {
+        checkEncoding(encoding);
+        this.lengths = createIntegerReader(encoding.getKind(), data, false, false);
+      }
+      this.keyReader = keyReader;
+      this.valueReader = valueReader;
+    }
+
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
       super.seek(index);
       lengths.seek(index[columnId]);
       keyReader.seek(index);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index a9dbc3e6b2..2b1d1ce96c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -3636,7 +3636,7 @@ public static StandardStructObjectInspector constructVectorizedReduceRowOI(
    * @param conf - configuration
    * @return false for types not supported by vectorization, true otherwise
    */
-  public static boolean checkLlapIOSupportedTypes(final Configuration conf) {
+  public static boolean checkVectorizerSupportedTypes(final Configuration conf) {
     final String[] readColumnNames = ColumnProjectionUtils.getReadColumnNames(conf);
     final String columnNames = conf.get(serdeConstants.LIST_COLUMNS);
     final String columnTypes = conf.get(serdeConstants.LIST_COLUMN_TYPES);
@@ -3649,7 +3649,7 @@ public static boolean checkLlapIOSupportedTypes(final Configuration conf) {
     final List<String> allColumnNames = Lists.newArrayList(columnNames.split(","));
     final List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString(columnTypes);
     final List<String> allColumnTypes = TypeInfoUtils.getTypeStringsFromTypeInfo(typeInfos);
-    return checkLlapIOSupportedTypes(Lists.newArrayList(readColumnNames), allColumnNames,
+    return checkVectorizerSupportedTypes(Lists.newArrayList(readColumnNames), allColumnNames,
         allColumnTypes);
   }
 
@@ -3660,7 +3660,7 @@ public static boolean checkLlapIOSupportedTypes(final Configuration conf) {
    * @param allColumnTypes - all column types
    * @return false for types not supported by vectorization, true otherwise
    */
-  public static boolean checkLlapIOSupportedTypes(final List<String> readColumnNames,
+  public static boolean checkVectorizerSupportedTypes(final List<String> readColumnNames,
       final List<String> allColumnNames, final List<String> allColumnTypes) {
     final String[] readColumnTypes = getReadColumnTypes(readColumnNames, allColumnNames,
         allColumnTypes);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
index ccb39da442..572953aba2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
@@ -282,7 +282,7 @@ public SerDeStats getStats() {
    * @param isOriginal is the file in the original format?
    * @return the column number for the root of row.
    */
-  static int getRootColumn(boolean isOriginal) {
+  public static int getRootColumn(boolean isOriginal) {
     return isOriginal ? 0 : (OrcRecordUpdater.ROW + 1);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReader.java
index 4d09dcd199..ea9904ad7e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReader.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReader.java
@@ -55,4 +55,4 @@ void readEncodedColumns(int stripeIx, StripeInformation stripe,
    * to just checking the constant in the first place.
    */
   void setTracing(boolean isEnabled);
-}
\ No newline at end of file
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
index a4925b994e..8f0c237cc0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.nio.ByteBuffer;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 
 import org.slf4j.Logger;
@@ -42,6 +43,7 @@
 import org.apache.orc.impl.StreamName;
 import org.apache.orc.StripeInformation;
 import org.apache.orc.impl.BufferChunk;
+import org.apache.hadoop.hive.llap.DebugUtils;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.OrcEncodedColumnBatch;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.PoolFactory;
 import org.apache.orc.OrcProto;
@@ -127,11 +129,13 @@ public EncodedReaderImpl(Object fileKey, List<OrcProto.Type> types, CompressionC
 
   /** Helper context for each column being read */
   private static final class ColumnReadContext {
+
     public ColumnReadContext(int colIx, OrcProto.ColumnEncoding encoding,
-                             OrcProto.RowIndex rowIndex) {
+                             OrcProto.RowIndex rowIndex, int colRgIx) {
       this.encoding = encoding;
       this.rowIndex = rowIndex;
       this.colIx = colIx;
+      this.includedIx = colRgIx;
       streamCount = 0;
     }
 
@@ -145,6 +149,8 @@ public ColumnReadContext(int colIx, OrcProto.ColumnEncoding encoding,
     OrcProto.RowIndex rowIndex;
     /** Column index in the file. */
     int colIx;
+    /** Column index in the included columns only (for RG masks). */
+    int includedIx;
 
     public void addStream(long offset, OrcProto.Stream stream, int indexIx) {
       streams[streamCount++] = new StreamContext(stream, offset, indexIx);
@@ -154,6 +160,7 @@ public void addStream(long offset, OrcProto.Stream stream, int indexIx) {
     public String toString() {
       StringBuilder sb = new StringBuilder();
       sb.append(" column_index: ").append(colIx);
+      sb.append(" included_index: ").append(includedIx);
       sb.append(" encoding: ").append(encoding);
       sb.append(" stream_count: ").append(streamCount);
       int i = 0;
@@ -200,6 +207,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings, List<OrcProto.Stream> streamList,
       boolean[] included, boolean[][] colRgs,
       Consumer<OrcEncodedColumnBatch> consumer) throws IOException {
+    isTracingEnabled = true;
     // Note: for now we don't have to setError here, caller will setError if we throw.
     // We are also not supposed to call setDone, since we are only part of the operation.
     long stripeOffset = stripe.getOffset();
@@ -208,18 +216,26 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
     // 1.1. Figure out which columns have a present stream
     boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);
     if (isTracingEnabled) {
-      LOG.trace("The following columns have PRESENT streams: " + arrayToString(hasNull));
+      LOG.error("The following columns have PRESENT streams: " + arrayToString(hasNull));
     }
 
     // We assume stream list is sorted by column and that non-data
     // streams do not interleave data streams for the same column.
     // 1.2. With that in mind, determine disk ranges to read/get from cache (not by stream).
-    int colRgIx = -1, lastColIx = -1;
-    ColumnReadContext[] colCtxs = new ColumnReadContext[colRgs.length];
-    boolean[] includedRgs = null;
+    ColumnReadContext[] colCtxs = new ColumnReadContext[included.length];
+    int colRgIx = -1;
+    // Don't create context for the 0-s column.
+    for (int i = 1; i < included.length; ++i) {
+      if (!included[i]) continue;
+      colCtxs[i] = new ColumnReadContext(i, encodings.get(i), indexes[i], ++colRgIx);
+      if (isTracingEnabled) {
+        LOG.error("Creating context: " + colCtxs[i].toString());
+      }
+    }
     boolean isCompressed = (codec != null);
     CreateHelper listToRead = new CreateHelper();
     boolean hasIndexOnlyCols = false;
+    boolean[] includedRgs = null; // Will always be the same for all cols at the moment.
     for (OrcProto.Stream stream : streamList) {
       long length = stream.getLength();
       int colIx = stream.getColumn();
@@ -227,28 +243,17 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       if (!included[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {
         // We have a stream for included column, but in future it might have no data streams.
         // It's more like "has at least one column included that has an index stream".
-        hasIndexOnlyCols = hasIndexOnlyCols | included[colIx];
+        hasIndexOnlyCols = hasIndexOnlyCols || included[colIx];
         if (isTracingEnabled) {
-          LOG.trace("Skipping stream: " + streamKind + " at " + offset + ", " + length);
+          LOG.trace("Skipping stream for column " + colIx + ": "
+              + streamKind + " at " + offset + ", " + length);
         }
         offset += length;
         continue;
       }
-      ColumnReadContext ctx = null;
-      if (lastColIx != colIx) {
-        ++colRgIx;
-        assert colCtxs[colRgIx] == null;
-        lastColIx = colIx;
-        includedRgs = colRgs[colRgIx];
-        ctx = colCtxs[colRgIx] = new ColumnReadContext(
-            colIx, encodings.get(colIx), indexes[colIx]);
-        if (isTracingEnabled) {
-          LOG.trace("Creating context " + colRgIx + " for column " + colIx + ":" + ctx.toString());
-        }
-      } else {
-        ctx = colCtxs[colRgIx];
-        assert ctx != null;
-      }
+      ColumnReadContext ctx = colCtxs[colIx];
+      assert ctx != null;
+      includedRgs = colRgs[ctx.includedIx];
       int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(),
           types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);
       ctx.addStream(offset, stream, indexIx);
@@ -275,7 +280,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       // TODO: there may be a bug here. Could there be partial RG filtering on index-only column?
       if (hasIndexOnlyCols && (includedRgs == null)) {
         OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();
-        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, colRgs.length);
+        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, included.length);
         consumer.consumeData(ecb);
       } else {
         LOG.warn("Nothing to read for stripe [" + stripe + "]");
@@ -309,8 +314,9 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
     // 3. For uncompressed case, we need some special processing before read.
     DiskRangeList iter = toRead.next;  // Keep "toRead" list for future use, don't extract().
     if (codec == null) {
-      for (int colIxMod = 0; colIxMod < colRgs.length; ++colIxMod) {
-        ColumnReadContext ctx = colCtxs[colIxMod];
+      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {
+        ColumnReadContext ctx = colCtxs[colIx];
+        if (ctx == null) continue; // This column is not included.
         for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {
           StreamContext sctx = ctx.streams[streamIx];
           DiskRangeList newIter = preReadUncompressedStream(
@@ -334,19 +340,27 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
       boolean isLastRg = rgIx == rgCount - 1;
       // Create the batch we will use to return data for this RG.
       OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();
-      ecb.init(fileKey, stripeIx, rgIx, colRgs.length);
+      ecb.init(fileKey, stripeIx, rgIx, included.length);
       boolean isRGSelected = true;
-      for (int colIxMod = 0; colIxMod < colRgs.length; ++colIxMod) {
-        // TODO: simplify this now that high-level cache has been removed.
-        if (colRgs[colIxMod] != null && !colRgs[colIxMod][rgIx]) {
+      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {
+        ColumnReadContext ctx = colCtxs[colIx];
+        if (ctx == null) continue; // This column is not included.
+        if (isTracingEnabled) {
+          LOG.trace("ctx: {} rgIx: {} isLastRg: {} rgCount: {}", ctx, rgIx, isLastRg, rgCount);
+        }
+        // TODO: simplify this now that high-level cache has been removed. Same RGs for all cols.
+        if (colRgs[ctx.includedIx] != null && !colRgs[ctx.includedIx][rgIx]) {
           // RG x col filtered.
           isRGSelected = false;
-          continue;
+          if (isTracingEnabled) {
+            LOG.trace("colIxMod: {} rgIx: {} colRgs[{}]: {} colRgs[{}][{}]: {}", ctx.includedIx, rgIx, ctx.includedIx,
+              Arrays.toString(colRgs[ctx.includedIx]), ctx.includedIx, rgIx, colRgs[ctx.includedIx][rgIx]);
+          }
+           continue;
         }
-        ColumnReadContext ctx = colCtxs[colIxMod];
         OrcProto.RowIndexEntry index = ctx.rowIndex.getEntry(rgIx),
             nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);
-        ecb.initColumn(colIxMod, ctx.colIx, OrcEncodedColumnBatch.MAX_DATA_STREAMS);
+        ecb.initOrcColumn(ctx.colIx);
         for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {
           StreamContext sctx = ctx.streams[streamIx];
           ColumnStreamData cb = null;
@@ -402,7 +416,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
                 sctx.bufferIter = iter = lastCached;
               }
             }
-            ecb.setStreamData(colIxMod, sctx.kind.getNumber(), cb);
+            ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);
           } catch (Exception ex) {
             DiskRangeList drl = toRead == null ? null : toRead.next;
             LOG.error("Error getting stream [" + sctx.kind + ", " + ctx.encoding + "] for"
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
index ebbdf8d0b6..d5f5f9d58a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
@@ -17,18 +17,26 @@
  */
 package org.apache.hadoop.hive.ql.io.orc.encoded;
 
+import org.apache.orc.impl.RunLengthByteReader;
+import org.apache.orc.impl.StreamName;
+
 import java.io.IOException;
 import java.util.List;
 
 import org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch;
 import org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.ColumnStreamData;
 import org.apache.orc.CompressionCodec;
+import org.apache.orc.TypeDescription;
+import org.apache.orc.TypeDescription.Category;
 import org.apache.orc.impl.PositionProvider;
 import org.apache.orc.impl.SettableUncompressedStream;
 import org.apache.orc.impl.TreeReaderFactory;
 import org.apache.orc.OrcProto;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class EncodedTreeReaderFactory extends TreeReaderFactory {
+  private static final Logger LOG = LoggerFactory.getLogger(EncodedTreeReaderFactory.class);
   /**
    * We choose to use a toy programming language, so we cannot use multiple inheritance.
    * If we could, we could have this inherit TreeReader to contain the common impl, and then
@@ -36,7 +44,7 @@ public class EncodedTreeReaderFactory extends TreeReaderFactory {
    * Instead, we have a settable interface that the caller will cast to and call setBuffers.
    */
   public interface SettableTreeReader {
-    void setBuffers(ColumnStreamData[] streamBuffers, boolean sameStripe) throws IOException;
+    void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe) throws IOException;
   }
 
   public static class TimestampStreamReader extends TimestampTreeReader
@@ -84,8 +92,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -197,6 +206,12 @@ private StringStreamReader(int columnId, SettableUncompressedStream present,
       this._dictionaryStream = dictionary;
     }
 
+    @Override
+    public void seek(PositionProvider[] index) throws IOException {
+      // This string reader should simply redirect to its own seek (what other types already do).
+      this.seek(index[columnId]);
+    }
+
     @Override
     public void seek(PositionProvider index) throws IOException {
       if (present != null) {
@@ -211,7 +226,7 @@ public void seek(PositionProvider index) throws IOException {
 
         // data stream could be empty stream or already reached end of stream before present stream.
         // This can happen if all values in stream are nulls or last row group values are all null.
-        if (_dataStream.available() > 0) {
+        if (_dataStream != null && _dataStream.available() > 0) {
           if (_isFileCompressed) {
             index.getNext();
           }
@@ -222,14 +237,14 @@ public void seek(PositionProvider index) throws IOException {
 
         // data stream could be empty stream or already reached end of stream before present stream.
         // This can happen if all values in stream are nulls or last row group values are all null.
-        if (_dataStream.available() > 0) {
+        if (_dataStream != null && _dataStream.available() > 0) {
           if (_isFileCompressed) {
             index.getNext();
           }
           ((StringDirectTreeReader) reader).getStream().seek(index);
         }
 
-        if (_lengthStream.available() > 0) {
+        if (_lengthStream != null && _lengthStream.available() > 0) {
           if (_isFileCompressed) {
             index.getNext();
           }
@@ -239,8 +254,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -371,8 +387,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -468,8 +485,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -571,8 +589,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -668,8 +687,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -758,8 +778,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -859,8 +880,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -979,8 +1001,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -1064,6 +1087,12 @@ private CharStreamReader(int columnId, int maxLength,
       this._dictionaryStream = dictionary;
     }
 
+    @Override
+    public void seek(PositionProvider[] index) throws IOException {
+      // This string reader should simply redirect to its own seek (what other types already do).
+      this.seek(index[columnId]);
+    }
+
     @Override
     public void seek(PositionProvider index) throws IOException {
       if (present != null) {
@@ -1106,8 +1135,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -1232,6 +1262,12 @@ private VarcharStreamReader(int columnId, int maxLength,
       this._dictionaryStream = dictionary;
     }
 
+    @Override
+    public void seek(PositionProvider[] index) throws IOException {
+      // This string reader should simply redirect to its own seek (what other types already do).
+      this.seek(index[columnId]);
+    }
+
     @Override
     public void seek(PositionProvider index) throws IOException {
       if (present != null) {
@@ -1274,8 +1310,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -1411,8 +1448,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -1511,8 +1549,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -1617,8 +1656,9 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     @Override
-    public void setBuffers(ColumnStreamData[] streamsData, boolean sameStripe)
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
         throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
       if (_presentStream != null) {
         _presentStream.setBuffers(StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
       }
@@ -1673,177 +1713,760 @@ public static StreamReaderBuilder builder() {
     }
   }
 
-  public static TreeReader[] createEncodedTreeReader(int numCols,
-                                                     List<OrcProto.Type> types,
-                                                     List<OrcProto.ColumnEncoding> encodings,
-                                                     EncodedColumnBatch<OrcBatchKey> batch,
-                                                     CompressionCodec codec, boolean skipCorrupt,
-                                                     String writerTimezone) throws IOException {
-    TreeReader[] treeReaders = new TreeReader[numCols];
-    for (int i = 0; i < numCols; i++) {
-      int columnIndex = batch.getColumnIxs()[i];
-      ColumnStreamData[] streamBuffers = batch.getColumnData()[i];
-      OrcProto.Type columnType = types.get(columnIndex);
-
-      // EncodedColumnBatch is already decompressed, we don't really need to pass codec.
-      // But we need to know if the original data is compressed or not. This is used to skip
-      // positions in row index properly. If the file is originally compressed,
-      // then 1st position (compressed offset) in row index should be skipped to get
-      // uncompressed offset, else 1st position should not be skipped.
-      // TODO: there should be a better way to do this, code just needs to be modified
-      OrcProto.ColumnEncoding columnEncoding = encodings.get(columnIndex);
-
-      // stream buffers are arranged in enum order of stream kind
-      ColumnStreamData present = streamBuffers[OrcProto.Stream.Kind.PRESENT_VALUE],
-        data = streamBuffers[OrcProto.Stream.Kind.DATA_VALUE],
-        dictionary = streamBuffers[OrcProto.Stream.Kind.DICTIONARY_DATA_VALUE],
-        lengths = streamBuffers[OrcProto.Stream.Kind.LENGTH_VALUE],
-        secondary = streamBuffers[OrcProto.Stream.Kind.SECONDARY_VALUE];
-
-      switch (columnType.getKind()) {
-        case BINARY:
-          treeReaders[i] = BinaryStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setLengthStream(lengths)
-              .setCompressionCodec(codec)
-              .setColumnEncoding(columnEncoding)
-              .build();
-          break;
-        case BOOLEAN:
-          treeReaders[i] = BooleanStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setCompressionCodec(codec)
-              .build();
-          break;
-        case BYTE:
-          treeReaders[i] = ByteStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setCompressionCodec(codec)
-              .build();
-          break;
-        case SHORT:
-          treeReaders[i] = ShortStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setCompressionCodec(codec)
-              .setColumnEncoding(columnEncoding)
-              .build();
-          break;
-        case INT:
-          treeReaders[i] = IntStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setCompressionCodec(codec)
-              .setColumnEncoding(columnEncoding)
-              .build();
-          break;
-        case LONG:
-          treeReaders[i] = LongStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setCompressionCodec(codec)
-              .setColumnEncoding(columnEncoding)
-              .skipCorrupt(skipCorrupt)
-              .build();
-          break;
-        case FLOAT:
-          treeReaders[i] = FloatStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setCompressionCodec(codec)
-              .build();
-          break;
-        case DOUBLE:
-          treeReaders[i] = DoubleStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setCompressionCodec(codec)
-              .build();
-          break;
-        case CHAR:
-          treeReaders[i] = CharStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setMaxLength(columnType.getMaximumLength())
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setLengthStream(lengths)
-              .setDictionaryStream(dictionary)
-              .setCompressionCodec(codec)
-              .setColumnEncoding(columnEncoding)
-              .build();
-          break;
-        case VARCHAR:
-          treeReaders[i] = VarcharStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setMaxLength(columnType.getMaximumLength())
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setLengthStream(lengths)
-              .setDictionaryStream(dictionary)
-              .setCompressionCodec(codec)
-              .setColumnEncoding(columnEncoding)
-              .build();
-          break;
-        case STRING:
-          treeReaders[i] = StringStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setDataStream(data)
-              .setLengthStream(lengths)
-              .setDictionaryStream(dictionary)
-              .setCompressionCodec(codec)
-              .setColumnEncoding(columnEncoding)
-              .build();
-          break;
-        case DECIMAL:
-          treeReaders[i] = DecimalStreamReader.builder()
-              .setColumnIndex(columnIndex)
-              .setPrecision(columnType.getPrecision())
-              .setScale(columnType.getScale())
-              .setPresentStream(present)
-              .setValueStream(data)
-              .setScaleStream(secondary)
-              .setCompressionCodec(codec)
-              .setColumnEncoding(columnEncoding)
-              .build();
-          break;
-        case TIMESTAMP:
-          treeReaders[i] = TimestampStreamReader.builder()
+  public static StructTreeReader createRootTreeReader(TypeDescription schema,
+       List<OrcProto.ColumnEncoding> encodings, EncodedColumnBatch<OrcBatchKey> batch,
+       CompressionCodec codec, boolean skipCorrupt, String tz, int[] columnMapping)
+           throws IOException {
+    if (schema.getCategory() != Category.STRUCT) {
+      throw new AssertionError("Schema is not a struct: " + schema);
+    }
+    // Some child types may be excluded. Note that this can only happen at root level.
+    List<TypeDescription> children = schema.getChildren();
+    int childCount = children.size(), includedCount = 0;
+    for (int childIx = 0; childIx < childCount; ++childIx) {
+      if (!batch.hasData(children.get(childIx).getId())) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Column at " + childIx + " " + children.get(childIx).getId()
+              + ":" + children.get(childIx).toString() + " has no data");
+        }
+        continue;
+      }
+      ++includedCount;
+    }
+    TreeReader[] childReaders = new TreeReader[includedCount];
+    for (int schemaChildIx = 0, inclChildIx = -1; schemaChildIx < childCount; ++schemaChildIx) {
+      if (!batch.hasData(children.get(schemaChildIx).getId())) continue;
+      childReaders[++inclChildIx] = createEncodedTreeReader(
+          schema.getChildren().get(schemaChildIx), encodings, batch, codec, skipCorrupt, tz);
+      columnMapping[inclChildIx] = schemaChildIx;
+    }
+    return StructStreamReader.builder()
+        .setColumnIndex(0)
+        .setCompressionCodec(codec)
+        .setColumnEncoding(encodings.get(0))
+        .setChildReaders(childReaders)
+        .build();
+  }
+
+
+  private static TreeReader createEncodedTreeReader(TypeDescription schema,
+      List<OrcProto.ColumnEncoding> encodings, EncodedColumnBatch<OrcBatchKey> batch,
+      CompressionCodec codec, boolean skipCorrupt, String tz) throws IOException {
+      int columnIndex = schema.getId();
+    ColumnStreamData[] streamBuffers = batch.getColumnData(columnIndex);
+
+    // EncodedColumnBatch is already decompressed, we don't really need to pass codec.
+    // But we need to know if the original data is compressed or not. This is used to skip
+    // positions in row index properly. If the file is originally compressed,
+    // then 1st position (compressed offset) in row index should be skipped to get
+    // uncompressed offset, else 1st position should not be skipped.
+    // TODO: there should be a better way to do this, code just needs to be modified
+    OrcProto.ColumnEncoding columnEncoding = encodings.get(columnIndex);
+
+    // stream buffers are arranged in enum order of stream kind
+    ColumnStreamData present = streamBuffers[OrcProto.Stream.Kind.PRESENT_VALUE],
+      data = streamBuffers[OrcProto.Stream.Kind.DATA_VALUE],
+      dictionary = streamBuffers[OrcProto.Stream.Kind.DICTIONARY_DATA_VALUE],
+      lengths = streamBuffers[OrcProto.Stream.Kind.LENGTH_VALUE],
+      secondary = streamBuffers[OrcProto.Stream.Kind.SECONDARY_VALUE];
+
+
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("columnIndex: {} columnType: {} streamBuffers.length: {} columnEncoding: {}" +
+          " present: {} data: {} dictionary: {} lengths: {} secondary: {} tz: {}",
+          columnIndex, schema, streamBuffers.length, columnEncoding, present != null,
+          data != null, dictionary != null, lengths != null, secondary != null, tz);
+    }
+    switch (schema.getCategory()) {
+      case BINARY:
+      case BOOLEAN:
+      case BYTE:
+      case SHORT:
+      case INT:
+      case LONG:
+      case FLOAT:
+      case DOUBLE:
+      case CHAR:
+      case VARCHAR:
+      case STRING:
+      case DECIMAL:
+      case TIMESTAMP:
+      case DATE:
+        return getPrimitiveTreeReaders(columnIndex, schema, codec, columnEncoding,
+            present, data, dictionary, lengths, secondary, skipCorrupt, tz);
+      case LIST:
+        TypeDescription elementType = schema.getChildren().get(0);
+        TreeReader elementReader = createEncodedTreeReader(
+            elementType, encodings, batch, codec, skipCorrupt, tz);
+        return ListStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setColumnEncoding(columnEncoding)
+            .setCompressionCodec(codec)
+            .setPresentStream(present)
+            .setLengthStream(lengths)
+            .setElementReader(elementReader)
+            .build();
+      case MAP:
+        TypeDescription keyType = schema.getChildren().get(0);
+        TypeDescription valueType = schema.getChildren().get(1);
+        TreeReader keyReader = createEncodedTreeReader(
+            keyType, encodings, batch, codec, skipCorrupt, tz);
+        TreeReader valueReader = createEncodedTreeReader(
+            valueType, encodings, batch, codec, skipCorrupt, tz);
+        return MapStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setColumnEncoding(columnEncoding)
+            .setCompressionCodec(codec)
+            .setPresentStream(present)
+            .setLengthStream(lengths)
+            .setKeyReader(keyReader)
+            .setValueReader(valueReader)
+            .build();
+      case STRUCT: {
+        int childCount = schema.getChildren().size();
+        TreeReader[] childReaders = new TreeReader[childCount];
+        for (int i = 0; i < childCount; i++) {
+          TypeDescription childType = schema.getChildren().get(i);
+          childReaders[i] = createEncodedTreeReader(
+              childType, encodings, batch, codec, skipCorrupt, tz);
+        }
+        return StructStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .setPresentStream(present)
+            .setChildReaders(childReaders)
+            .build();
+      }
+      case UNION: {
+        int childCount = schema.getChildren().size();
+        TreeReader[] childReaders = new TreeReader[childCount];
+        for (int i = 0; i < childCount; i++) {
+          TypeDescription childType = schema.getChildren().get(i);
+          childReaders[i] = createEncodedTreeReader(
+              childType, encodings, batch, codec, skipCorrupt, tz);
+        }
+        return UnionStreamReader.builder()
               .setColumnIndex(columnIndex)
-              .setPresentStream(present)
-              .setSecondsStream(data)
-              .setNanosStream(secondary)
               .setCompressionCodec(codec)
               .setColumnEncoding(columnEncoding)
-              .setWriterTimezone(writerTimezone)
-              .skipCorrupt(skipCorrupt)
-              .build();
-          break;
-        case DATE:
-          treeReaders[i] = DateStreamReader.builder()
-              .setColumnIndex(columnIndex)
               .setPresentStream(present)
               .setDataStream(data)
-              .setCompressionCodec(codec)
-              .setColumnEncoding(columnEncoding)
+              .setChildReaders(childReaders)
               .build();
-          break;
-        default:
-          throw new UnsupportedOperationException("Data type not supported yet! " + columnType);
+      }
+      default:
+        throw new UnsupportedOperationException("Data type not supported: " + schema);
+    }
+  }
+
+  private static TreeReader getPrimitiveTreeReaders(final int columnIndex,
+      TypeDescription columnType, CompressionCodec codec, OrcProto.ColumnEncoding columnEncoding,
+      ColumnStreamData present, ColumnStreamData data, ColumnStreamData dictionary,
+      ColumnStreamData lengths, ColumnStreamData secondary, boolean skipCorrupt, String tz)
+          throws IOException {
+    switch (columnType.getCategory()) {
+      case BINARY:
+        return BinaryStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setLengthStream(lengths)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .build();
+      case BOOLEAN:
+        return BooleanStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setCompressionCodec(codec)
+            .build();
+      case BYTE:
+        return ByteStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setCompressionCodec(codec)
+            .build();
+      case SHORT:
+        return ShortStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .build();
+      case INT:
+        return IntStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .build();
+      case LONG:
+        return LongStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .skipCorrupt(skipCorrupt)
+            .build();
+      case FLOAT:
+        return FloatStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setCompressionCodec(codec)
+            .build();
+      case DOUBLE:
+        return DoubleStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setCompressionCodec(codec)
+            .build();
+      case CHAR:
+        return CharStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setMaxLength(columnType.getMaxLength())
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setLengthStream(lengths)
+            .setDictionaryStream(dictionary)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .build();
+      case VARCHAR:
+        return VarcharStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setMaxLength(columnType.getMaxLength())
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setLengthStream(lengths)
+            .setDictionaryStream(dictionary)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .build();
+      case STRING:
+        return StringStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setLengthStream(lengths)
+            .setDictionaryStream(dictionary)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .build();
+      case DECIMAL:
+        return DecimalStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPrecision(columnType.getPrecision())
+            .setScale(columnType.getScale())
+            .setPresentStream(present)
+            .setValueStream(data)
+            .setScaleStream(secondary)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .build();
+      case TIMESTAMP:
+        return TimestampStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setSecondsStream(data)
+            .setNanosStream(secondary)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .setWriterTimezone(tz)
+            .skipCorrupt(skipCorrupt)
+            .build();
+      case DATE:
+        return DateStreamReader.builder()
+            .setColumnIndex(columnIndex)
+            .setPresentStream(present)
+            .setDataStream(data)
+            .setCompressionCodec(codec)
+            .setColumnEncoding(columnEncoding)
+            .build();
+    default:
+      throw new AssertionError("Not a primitive category: " + columnType.getCategory());
+    }
+  }
+
+  protected static class ListStreamReader extends ListTreeReader implements SettableTreeReader {
+    private boolean _isFileCompressed;
+    private SettableUncompressedStream _presentStream;
+    private SettableUncompressedStream _lengthStream;
+
+    public ListStreamReader(final int columnIndex,
+        final SettableUncompressedStream present, final SettableUncompressedStream lengthStream,
+        final OrcProto.ColumnEncoding columnEncoding, final boolean isFileCompressed,
+        final TreeReader elementReader) throws IOException {
+      super(columnIndex, present, lengthStream, columnEncoding, elementReader);
+      this._isFileCompressed = isFileCompressed;
+      this._presentStream = present;
+      this._lengthStream = lengthStream;
+    }
+
+    @Override
+    public void seek(PositionProvider[] index) throws IOException {
+      PositionProvider ownIndex = index[columnId];
+      if (present != null) {
+        if (_isFileCompressed) {
+          ownIndex.getNext();
+        }
+        present.seek(ownIndex);
+      }
+
+      // lengths stream could be empty stream or already reached end of stream before present stream.
+      // This can happen if all values in stream are nulls or last row group values are all null.
+      if (_lengthStream.available() > 0) {
+        if (_isFileCompressed) {
+          ownIndex.getNext();
+        }
+        lengths.seek(ownIndex);
+        elementReader.seek(index);
       }
     }
 
-    return treeReaders;
+    @Override
+    public void seek(PositionProvider index) throws IOException {
+      // Only our parent class can call this.
+      throw new IOException("Should never be called");
+    }
+
+    @Override
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
+        throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
+      if (_presentStream != null) {
+        _presentStream.setBuffers(
+            StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
+      }
+      if (_lengthStream != null) {
+        _lengthStream.setBuffers(
+            StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.LENGTH_VALUE]));
+      }
+
+      if (elementReader != null) {
+        ((SettableTreeReader) elementReader).setBuffers(batch, sameStripe);
+      }
+    }
+
+    public static class StreamReaderBuilder {
+      private int columnIndex;
+      private ColumnStreamData presentStream;
+      private ColumnStreamData lengthStream;
+      private CompressionCodec compressionCodec;
+      private OrcProto.ColumnEncoding columnEncoding;
+      private TreeReader elementReader;
+
+
+      public ListStreamReader.StreamReaderBuilder setColumnIndex(int columnIndex) {
+        this.columnIndex = columnIndex;
+        return this;
+      }
+
+      public ListStreamReader.StreamReaderBuilder setLengthStream(ColumnStreamData lengthStream) {
+        this.lengthStream = lengthStream;
+        return this;
+      }
+
+      public ListStreamReader.StreamReaderBuilder setPresentStream(ColumnStreamData presentStream) {
+        this.presentStream = presentStream;
+        return this;
+      }
+
+      public ListStreamReader.StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
+        this.columnEncoding = encoding;
+        return this;
+      }
+
+      public ListStreamReader.StreamReaderBuilder setCompressionCodec(CompressionCodec compressionCodec) {
+        this.compressionCodec = compressionCodec;
+        return this;
+      }
+
+      public ListStreamReader.StreamReaderBuilder setElementReader(TreeReader elementReader) {
+        this.elementReader = elementReader;
+        return this;
+      }
+
+      public ListStreamReader build() throws IOException {
+        SettableUncompressedStream present = StreamUtils
+            .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
+                presentStream);
+
+        SettableUncompressedStream length = StreamUtils
+            .createSettableUncompressedStream(OrcProto.Stream.Kind.LENGTH.name(),
+                lengthStream);
+
+        boolean isFileCompressed = compressionCodec != null;
+        return new ListStreamReader(columnIndex, present, length, columnEncoding, isFileCompressed,
+            elementReader);
+      }
+    }
+
+    public static ListStreamReader.StreamReaderBuilder builder() {
+      return new ListStreamReader.StreamReaderBuilder();
+    }
+  }
+
+  protected static class MapStreamReader extends MapTreeReader implements SettableTreeReader{
+    private boolean _isFileCompressed;
+    private SettableUncompressedStream _presentStream;
+    private SettableUncompressedStream _lengthStream;
+
+    public MapStreamReader(final int columnIndex,
+        final SettableUncompressedStream present, final SettableUncompressedStream lengthStream,
+        final OrcProto.ColumnEncoding columnEncoding, final boolean isFileCompressed,
+        final TreeReader keyReader, final TreeReader valueReader) throws IOException {
+      super(columnIndex, present, lengthStream, columnEncoding, keyReader, valueReader);
+      this._isFileCompressed = isFileCompressed;
+      this._presentStream = present;
+      this._lengthStream = lengthStream;
+    }
+
+    @Override
+    public void seek(PositionProvider[] index) throws IOException {
+      // We are not calling super.seek since we handle the present stream differently.
+      PositionProvider ownIndex = index[columnId];
+      if (present != null) {
+        if (_isFileCompressed) {
+          ownIndex.getNext();
+        }
+        present.seek(ownIndex);
+      }
+
+      // lengths stream could be empty stream or already reached end of stream before present stream.
+      // This can happen if all values in stream are nulls or last row group values are all null.
+      if (_lengthStream.available() > 0) {
+        if (_isFileCompressed) {
+          ownIndex.getNext();
+        }
+        lengths.seek(ownIndex);
+        keyReader.seek(index);
+        valueReader.seek(index);
+      }
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
+      // Only our parent class can call this.
+      throw new IOException("Should never be called");
+    }
+
+    @Override
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
+        throws IOException {
+     ColumnStreamData[] streamsData = batch.getColumnData(columnId);
+     if (_presentStream != null) {
+        _presentStream.setBuffers(
+            StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
+      }
+      if (_lengthStream != null) {
+        _lengthStream.setBuffers(
+            StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.LENGTH_VALUE]));
+      }
+
+      if (keyReader != null) {
+        ((SettableTreeReader) keyReader).setBuffers(batch, sameStripe);
+      }
+
+      if (valueReader != null) {
+        ((SettableTreeReader) valueReader).setBuffers(batch, sameStripe);
+      }
+    }
+
+    public static class StreamReaderBuilder {
+      private int columnIndex;
+      private ColumnStreamData presentStream;
+      private ColumnStreamData lengthStream;
+      private CompressionCodec compressionCodec;
+      private OrcProto.ColumnEncoding columnEncoding;
+      private TreeReader keyReader;
+      private TreeReader valueReader;
+
+
+      public MapStreamReader.StreamReaderBuilder setColumnIndex(int columnIndex) {
+        this.columnIndex = columnIndex;
+        return this;
+      }
+
+      public MapStreamReader.StreamReaderBuilder setLengthStream(ColumnStreamData lengthStream) {
+        this.lengthStream = lengthStream;
+        return this;
+      }
+
+      public MapStreamReader.StreamReaderBuilder setPresentStream(ColumnStreamData presentStream) {
+        this.presentStream = presentStream;
+        return this;
+      }
+
+      public MapStreamReader.StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
+        this.columnEncoding = encoding;
+        return this;
+      }
+
+      public MapStreamReader.StreamReaderBuilder setCompressionCodec(CompressionCodec compressionCodec) {
+        this.compressionCodec = compressionCodec;
+        return this;
+      }
+
+      public MapStreamReader.StreamReaderBuilder setKeyReader(TreeReader keyReader) {
+        this.keyReader = keyReader;
+        return this;
+      }
+
+      public MapStreamReader.StreamReaderBuilder setValueReader(TreeReader valueReader) {
+        this.valueReader = valueReader;
+        return this;
+      }
+
+      public MapStreamReader build() throws IOException {
+        SettableUncompressedStream present = StreamUtils
+            .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
+                presentStream);
+
+        SettableUncompressedStream length = StreamUtils
+            .createSettableUncompressedStream(OrcProto.Stream.Kind.LENGTH.name(),
+                lengthStream);
+
+        boolean isFileCompressed = compressionCodec != null;
+        return new MapStreamReader(columnIndex, present, length, columnEncoding, isFileCompressed,
+            keyReader, valueReader);
+      }
+    }
+
+    public static MapStreamReader.StreamReaderBuilder builder() {
+      return new MapStreamReader.StreamReaderBuilder();
+    }
+  }
+
+  protected static class StructStreamReader extends StructTreeReader
+      implements SettableTreeReader {
+    private boolean _isFileCompressed;
+    private SettableUncompressedStream _presentStream;
+
+    public StructStreamReader(final int columnIndex,
+        final SettableUncompressedStream present,
+        final OrcProto.ColumnEncoding columnEncoding, final boolean isFileCompressed,
+        final TreeReader[] childReaders) throws IOException {
+      super(columnIndex, present, columnEncoding, childReaders);
+      this._isFileCompressed = isFileCompressed;
+      this._presentStream = present;
+    }
+
+    @Override
+    public void seek(PositionProvider[] index) throws IOException {
+      PositionProvider ownIndex = index[columnId];
+      if (present != null) {
+        if (_isFileCompressed) {
+          ownIndex.getNext();
+        }
+        present.seek(ownIndex);
+      }
+      if (fields != null) {
+        for (TreeReader child : fields) {
+          child.seek(index);
+        }
+      }
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
+      // Only our parent class can call this.
+      throw new IOException("Should never be called");
+    }
+
+
+    @Override
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
+        throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
+      if (_presentStream != null) {
+        _presentStream.setBuffers(
+            StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
+      }
+      if (fields != null) {
+        for (TreeReader child : fields) {
+          ((SettableTreeReader) child).setBuffers(batch, sameStripe);
+        }
+      }
+    }
+
+    public static class StreamReaderBuilder {
+      private int columnIndex;
+      private ColumnStreamData presentStream;
+      private CompressionCodec compressionCodec;
+      private OrcProto.ColumnEncoding columnEncoding;
+      private TreeReader[] childReaders;
+
+
+      public StructStreamReader.StreamReaderBuilder setColumnIndex(int columnIndex) {
+        this.columnIndex = columnIndex;
+        return this;
+      }
+
+      public StructStreamReader.StreamReaderBuilder setPresentStream(ColumnStreamData presentStream) {
+        this.presentStream = presentStream;
+        return this;
+      }
+
+      public StructStreamReader.StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
+        this.columnEncoding = encoding;
+        return this;
+      }
+
+      public StructStreamReader.StreamReaderBuilder setCompressionCodec(CompressionCodec compressionCodec) {
+        this.compressionCodec = compressionCodec;
+        return this;
+      }
+
+      public StructStreamReader.StreamReaderBuilder setChildReaders(TreeReader[] childReaders) {
+        this.childReaders = childReaders;
+        return this;
+      }
+
+      public StructStreamReader build() throws IOException {
+        SettableUncompressedStream present = StreamUtils
+            .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
+                presentStream);
+
+        boolean isFileCompressed = compressionCodec != null;
+        return new StructStreamReader(columnIndex, present, columnEncoding, isFileCompressed,
+            childReaders);
+      }
+    }
+
+    public static StructStreamReader.StreamReaderBuilder builder() {
+      return new StructStreamReader.StreamReaderBuilder();
+    }
+  }
+
+  protected static class UnionStreamReader extends UnionTreeReader implements SettableTreeReader {
+    private boolean _isFileCompressed;
+    private SettableUncompressedStream _presentStream;
+    private SettableUncompressedStream _dataStream;
+
+    public UnionStreamReader(final int columnIndex,
+        final SettableUncompressedStream present, final SettableUncompressedStream dataStream,
+        final OrcProto.ColumnEncoding columnEncoding, final boolean isFileCompressed,
+        final TreeReader[] childReaders) throws IOException {
+      super(columnIndex, present, columnEncoding, childReaders);
+      this._isFileCompressed = isFileCompressed;
+      this._presentStream = present;
+      this._dataStream = dataStream;
+      // Note: other parent readers init everything in ctor, but union does it in startStripe.
+      this.tags = new RunLengthByteReader(dataStream);
+    }
+
+    @Override
+    public void seek(PositionProvider[] index) throws IOException {
+      PositionProvider ownIndex = index[columnId];
+      if (present != null) {
+        if (_isFileCompressed) {
+          ownIndex.getNext();
+        }
+        present.seek(ownIndex);
+      }
+
+      // lengths stream could be empty stream or already reached end of stream before present stream.
+      // This can happen if all values in stream are nulls or last row group values are all null.
+      if (_dataStream.available() > 0) {
+        if (_isFileCompressed) {
+          ownIndex.getNext();
+        }
+        tags.seek(ownIndex);
+        if (fields != null) {
+          for (TreeReader child : fields) {
+            child.seek(index);
+          }
+        }
+      }
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
+      // Only our parent class can call this.
+      throw new IOException("Should never be called");
+    }
+
+    @Override
+    public void setBuffers(EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe)
+        throws IOException {
+      ColumnStreamData[] streamsData = batch.getColumnData(columnId);
+      if (_presentStream != null) {
+        _presentStream.setBuffers(
+            StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.PRESENT_VALUE]));
+      }
+      if (_dataStream != null) {
+        _dataStream.setBuffers(
+            StreamUtils.createDiskRangeInfo(streamsData[OrcProto.Stream.Kind.DATA_VALUE]));
+      }
+      if (fields != null) {
+        for (TreeReader child : fields) {
+          ((SettableTreeReader) child).setBuffers(batch, sameStripe);
+        }
+      }
+    }
+
+    public static class StreamReaderBuilder {
+      private int columnIndex;
+      private ColumnStreamData presentStream;
+      private ColumnStreamData dataStream;
+      private CompressionCodec compressionCodec;
+      private OrcProto.ColumnEncoding columnEncoding;
+      private TreeReader[] childReaders;
+
+
+      public UnionStreamReader.StreamReaderBuilder setColumnIndex(int columnIndex) {
+        this.columnIndex = columnIndex;
+        return this;
+      }
+
+      public UnionStreamReader.StreamReaderBuilder setDataStream(ColumnStreamData dataStream) {
+        this.dataStream = dataStream;
+        return this;
+      }
+
+      public UnionStreamReader.StreamReaderBuilder setPresentStream(ColumnStreamData presentStream) {
+        this.presentStream = presentStream;
+        return this;
+      }
+
+      public UnionStreamReader.StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
+        this.columnEncoding = encoding;
+        return this;
+      }
+
+      public UnionStreamReader.StreamReaderBuilder setCompressionCodec(CompressionCodec compressionCodec) {
+        this.compressionCodec = compressionCodec;
+        return this;
+      }
+
+      public UnionStreamReader.StreamReaderBuilder setChildReaders(TreeReader[] childReaders) {
+        this.childReaders = childReaders;
+        return this;
+      }
+
+      public UnionStreamReader build() throws IOException {
+        SettableUncompressedStream present = StreamUtils.createSettableUncompressedStream(
+            OrcProto.Stream.Kind.PRESENT.name(), presentStream);
+
+        SettableUncompressedStream data = StreamUtils.createSettableUncompressedStream(
+            OrcProto.Stream.Kind.DATA.name(), dataStream);
+
+        boolean isFileCompressed = compressionCodec != null;
+        return new UnionStreamReader(columnIndex, present, data,
+            columnEncoding, isFileCompressed, childReaders);
+      }
+    }
+
+    public static UnionStreamReader.StreamReaderBuilder builder() {
+      return new UnionStreamReader.StreamReaderBuilder();
+    }
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java
index 4405232289..1c5f0e67f9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java
@@ -46,7 +46,7 @@ public static final class OrcEncodedColumnBatch extends EncodedColumnBatch<OrcBa
     public static final int ALL_RGS = -1;
     /**
      * All the previous streams are data streams, this and the next ones are index streams.
-     * We assume the sort will stay the same for backward compat.
+     * We assume the order will stay the same for backward compat.
      */
     public static final int MAX_DATA_STREAMS = OrcProto.Stream.Kind.ROW_INDEX.getNumber();
     public void init(Object fileKey, int stripeIx, int rgIx, int columnCount) {
@@ -57,6 +57,10 @@ public void init(Object fileKey, int stripeIx, int rgIx, int columnCount) {
       }
       resetColumnArrays(columnCount);
     }
+
+    public void initOrcColumn(int colIx) {
+      super.initColumn(colIx, MAX_DATA_STREAMS);
+    }
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
index 5cc36635d1..601324a1c8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
@@ -278,30 +278,35 @@ public void deriveLlap(Configuration conf, boolean isExecDriver) {
     }
 
     // check if the column types that are read are supported by LLAP IO
-    for (Map.Entry<String, Operator<? extends OperatorDesc>> entry : aliasToWork.entrySet()) {
-      if (hasLlap) {
-        final String alias = entry.getKey();
-        Operator<? extends OperatorDesc> op = entry.getValue();
-        PartitionDesc partitionDesc = aliasToPartnInfo.get(alias);
-        if (op instanceof TableScanOperator && partitionDesc != null &&
-            partitionDesc.getTableDesc() != null) {
-          final TableScanOperator tsOp = (TableScanOperator) op;
-          final List<String> readColumnNames = tsOp.getNeededColumns();
-          final Properties props = partitionDesc.getTableDesc().getProperties();
-          final List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString(
-              props.getProperty(serdeConstants.LIST_COLUMN_TYPES));
-          final List<String> allColumnTypes = TypeInfoUtils.getTypeStringsFromTypeInfo(typeInfos);
-          final List<String> allColumnNames = Utilities.getColumnNames(props);
-          hasLlap = Utilities.checkLlapIOSupportedTypes(readColumnNames, allColumnNames,
-              allColumnTypes);
-        }
-      }
+    if (hasLlap) {
+      // TODO: no need for now hasLlap = checkVectorizerSupportedTypes();
     }
 
     llapIoDesc = deriveLlapIoDescString(
         isLlapOn, canWrapAny, hasPathToPartInfo, hasLlap, hasNonLlap, hasAcid);
   }
 
+  private boolean checkVectorizerSupportedTypes(boolean hasLlap) {
+    for (Map.Entry<String, Operator<? extends OperatorDesc>> entry : aliasToWork.entrySet()) {
+      final String alias = entry.getKey();
+      Operator<? extends OperatorDesc> op = entry.getValue();
+      PartitionDesc partitionDesc = aliasToPartnInfo.get(alias);
+      if (op instanceof TableScanOperator && partitionDesc != null &&
+          partitionDesc.getTableDesc() != null) {
+        final TableScanOperator tsOp = (TableScanOperator) op;
+        final List<String> readColumnNames = tsOp.getNeededColumns();
+        final Properties props = partitionDesc.getTableDesc().getProperties();
+        final List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString(
+            props.getProperty(serdeConstants.LIST_COLUMN_TYPES));
+        final List<String> allColumnTypes = TypeInfoUtils.getTypeStringsFromTypeInfo(typeInfos);
+        final List<String> allColumnNames = Utilities.getColumnNames(props);
+        hasLlap = Utilities.checkVectorizerSupportedTypes(readColumnNames, allColumnNames,
+            allColumnTypes);
+      }
+    }
+    return hasLlap;
+  }
+
   private static String deriveLlapIoDescString(boolean isLlapOn, boolean canWrapAny,
       boolean hasPathToPartInfo, boolean hasLlap, boolean hasNonLlap, boolean hasAcid) {
     if (!isLlapOn) return null; // LLAP IO is off, don't output.
diff --git a/ql/src/test/queries/clientpositive/vector_complex_all.q b/ql/src/test/queries/clientpositive/vector_complex_all.q
index 91a736806f..b71ac62f74 100644
--- a/ql/src/test/queries/clientpositive/vector_complex_all.q
+++ b/ql/src/test/queries/clientpositive/vector_complex_all.q
@@ -1,9 +1,11 @@
 set hive.compute.query.using.stats=false;
-set hive.compute.query.using.stats=false;
+set hive.strict.checks.cartesian.product=false;
 set hive.cli.print.header=true;
 set hive.explain.user=false;
 set hive.fetch.task.conversion=none;
 SET hive.vectorized.execution.enabled=true;
+set hive.llap.io.enabled=false;
+set hive.mapred.mode=nonstrict;
 
 CREATE TABLE orc_create_staging (
   str STRING,
@@ -21,25 +23,45 @@ CREATE TABLE orc_create_complex (
   str STRING,
   mp  MAP<STRING,STRING>,
   lst ARRAY<STRING>,
-  strct STRUCT<A:STRING,B:STRING>
-) STORED AS ORC;
+  strct STRUCT<A:STRING,B:STRING>,
+  val string
+) STORED AS ORC tblproperties("orc.row.index.stride"="1000", "orc.stripe.size"="1000", "orc.compress.size"="10000");
 
-INSERT OVERWRITE TABLE orc_create_complex SELECT * FROM orc_create_staging;
+INSERT OVERWRITE TABLE orc_create_complex
+SELECT orc_create_staging.*, '0' FROM orc_create_staging;
 
--- Since complex types are not supported, this query should not vectorize.
-EXPLAIN
-SELECT * FROM orc_create_complex;
+set hive.llap.io.enabled=true;
 
 SELECT * FROM orc_create_complex;
 
--- However, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT COUNT(*) FROM orc_create_complex;
+SELECT str FROM orc_create_complex;
+
+SELECT strct, mp, lst FROM orc_create_complex;
+
+SELECT lst, str FROM orc_create_complex;
+
+SELECT mp, str FROM orc_create_complex;
+
+SELECT strct, str FROM orc_create_complex;
+
+SELECT strct.B, str FROM orc_create_complex;
+
+set hive.llap.io.enabled=false;
+
+INSERT INTO TABLE orc_create_complex
+SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2;
+
+select count(*) from orc_create_complex;
+
+set hive.llap.io.enabled=true;
+
+SELECT distinct lst, strct FROM orc_create_complex;
+
+SELECT str, count(val)  FROM orc_create_complex GROUP BY str;
+
+SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B;
+
+SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str;
 
-SELECT COUNT(*) FROM orc_create_complex;
 
--- Also, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT str FROM orc_create_complex ORDER BY str;
 
-SELECT str FROM orc_create_complex ORDER BY str;
\ No newline at end of file
diff --git a/ql/src/test/results/clientpositive/llap/vector_complex_all.q.out b/ql/src/test/results/clientpositive/llap/vector_complex_all.q.out
index 08d49bc47b..f16bb16ce2 100644
--- a/ql/src/test/results/clientpositive/llap/vector_complex_all.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_complex_all.q.out
@@ -34,8 +34,9 @@ PREHOOK: query: CREATE TABLE orc_create_complex (
   str STRING,
   mp  MAP<STRING,STRING>,
   lst ARRAY<STRING>,
-  strct STRUCT<A:STRING,B:STRING>
-) STORED AS ORC
+  strct STRUCT<A:STRING,B:STRING>,
+  val string
+) STORED AS ORC tblproperties("orc.row.index.stride"="1000", "orc.stripe.size"="1000", "orc.compress.size"="10000")
 PREHOOK: type: CREATETABLE
 PREHOOK: Output: database:default
 PREHOOK: Output: default@orc_create_complex
@@ -43,16 +44,19 @@ POSTHOOK: query: CREATE TABLE orc_create_complex (
   str STRING,
   mp  MAP<STRING,STRING>,
   lst ARRAY<STRING>,
-  strct STRUCT<A:STRING,B:STRING>
-) STORED AS ORC
+  strct STRUCT<A:STRING,B:STRING>,
+  val string
+) STORED AS ORC tblproperties("orc.row.index.stride"="1000", "orc.stripe.size"="1000", "orc.compress.size"="10000")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: database:default
 POSTHOOK: Output: default@orc_create_complex
-PREHOOK: query: INSERT OVERWRITE TABLE orc_create_complex SELECT * FROM orc_create_staging
+PREHOOK: query: INSERT OVERWRITE TABLE orc_create_complex
+SELECT orc_create_staging.*, '0' FROM orc_create_staging
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_staging
 PREHOOK: Output: default@orc_create_complex
-POSTHOOK: query: INSERT OVERWRITE TABLE orc_create_complex SELECT * FROM orc_create_staging
+POSTHOOK: query: INSERT OVERWRITE TABLE orc_create_complex
+SELECT orc_create_staging.*, '0' FROM orc_create_staging
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_staging
 POSTHOOK: Output: default@orc_create_complex
@@ -60,199 +64,166 @@ POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create
 POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
 POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]
 POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
-orc_create_staging.str	orc_create_staging.mp	orc_create_staging.lst	orc_create_staging.strct
-PREHOOK: query: -- Since complex types are not supported, this query should not vectorize.
-EXPLAIN
-SELECT * FROM orc_create_complex
+POSTHOOK: Lineage: orc_create_complex.val SIMPLE []
+orc_create_staging.str	orc_create_staging.mp	orc_create_staging.lst	orc_create_staging.strct	c1
+PREHOOK: query: SELECT * FROM orc_create_complex
 PREHOOK: type: QUERY
-POSTHOOK: query: -- Since complex types are not supported, this query should not vectorize.
-EXPLAIN
-SELECT * FROM orc_create_complex
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM orc_create_complex
 POSTHOOK: type: QUERY
-Explain
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-1
-    Tez
+POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: orc_create_complex
-                  Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-                  Select Operator
-                    expressions: str (type: string), mp (type: map<string,string>), lst (type: array<string>), strct (type: struct<a:string,b:string>)
-                    outputColumnNames: _col0, _col1, _col2, _col3
-                    Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-                    File Output Operator
-                      compressed: false
-                      Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-                      table:
-                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            Execution mode: llap
-            LLAP IO: no inputs
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-      Processor Tree:
-        ListSink
-
-PREHOOK: query: SELECT * FROM orc_create_complex
+orc_create_complex.str	orc_create_complex.mp	orc_create_complex.lst	orc_create_complex.strct	orc_create_complex.val
+line1	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]	{"a":"one","b":"two"}	0
+line2	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]	{"a":"three","b":"four"}	0
+line3	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]	{"a":"five","b":"six"}	0
+PREHOOK: query: SELECT str FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM orc_create_complex
+POSTHOOK: query: SELECT str FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+str
+line1
+line2
+line3
+PREHOOK: query: SELECT strct, mp, lst FROM orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT strct, mp, lst FROM orc_create_complex
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-orc_create_complex.str	orc_create_complex.mp	orc_create_complex.lst	orc_create_complex.strct
-line1	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]	{"a":"one","b":"two"}
-line2	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]	{"a":"three","b":"four"}
-line3	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]	{"a":"five","b":"six"}
-PREHOOK: query: -- However, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT COUNT(*) FROM orc_create_complex
+strct	mp	lst
+{"a":"one","b":"two"}	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]
+{"a":"three","b":"four"}	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]
+{"a":"five","b":"six"}	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]
+PREHOOK: query: SELECT lst, str FROM orc_create_complex
 PREHOOK: type: QUERY
-POSTHOOK: query: -- However, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT COUNT(*) FROM orc_create_complex
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT lst, str FROM orc_create_complex
 POSTHOOK: type: QUERY
-Explain
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-1
-    Tez
+POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-      Edges:
-        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+lst	str
+["a","b","c"]	line1
+["d","e","f"]	line2
+["g","h","i"]	line3
+PREHOOK: query: SELECT mp, str FROM orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: orc_create_complex
-                  Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: COMPLETE
-                  Select Operator
-                    Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: COMPLETE
-                    Group By Operator
-                      aggregations: count()
-                      mode: hash
-                      outputColumnNames: _col0
-                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                      Reduce Output Operator
-                        sort order: 
-                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                        value expressions: _col0 (type: bigint)
-            Execution mode: vectorized, llap
-            LLAP IO: all inputs
-        Reducer 2 
-            Execution mode: vectorized, llap
-            Reduce Operator Tree:
-              Group By Operator
-                aggregations: count(VALUE._col0)
-                mode: mergepartial
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                  table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-      Processor Tree:
-        ListSink
-
-PREHOOK: query: SELECT COUNT(*) FROM orc_create_complex
+POSTHOOK: query: SELECT mp, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+mp	str
+{"key13":"value13","key11":"value11","key12":"value12"}	line1
+{"key21":"value21","key22":"value22","key23":"value23"}	line2
+{"key31":"value31","key32":"value32","key33":"value33"}	line3
+PREHOOK: query: SELECT strct, str FROM orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT strct, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+strct	str
+{"a":"one","b":"two"}	line1
+{"a":"three","b":"four"}	line2
+{"a":"five","b":"six"}	line3
+PREHOOK: query: SELECT strct.B, str FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT COUNT(*) FROM orc_create_complex
+POSTHOOK: query: SELECT strct.B, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+b	str
+two	line1
+four	line2
+six	line3
+Warning: Shuffle Join MERGEJOIN[15][tables = [$hdt$_1, $hdt$_2, $hdt$_3, $hdt$_0]] in Stage 'Reducer 2' is a cross product
+PREHOOK: query: INSERT INTO TABLE orc_create_complex
+SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_staging
+PREHOOK: Input: default@src
+PREHOOK: Output: default@orc_create_complex
+POSTHOOK: query: INSERT INTO TABLE orc_create_complex
+SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_staging
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@orc_create_complex
+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]
+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
+POSTHOOK: Lineage: orc_create_complex.val SIMPLE [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
+orc_create_staging.str	orc_create_staging.mp	orc_create_staging.lst	orc_create_staging.strct	src1.key
+PREHOOK: query: select count(*) from orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: select count(*) from orc_create_complex
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
 c0
-3
-PREHOOK: query: -- Also, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT str FROM orc_create_complex ORDER BY str
+13503
+PREHOOK: query: SELECT distinct lst, strct FROM orc_create_complex
 PREHOOK: type: QUERY
-POSTHOOK: query: -- Also, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT str FROM orc_create_complex ORDER BY str
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT distinct lst, strct FROM orc_create_complex
 POSTHOOK: type: QUERY
-Explain
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-1
-    Tez
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+lst	strct
+["a","b","c"]	{"a":"one","b":"two"}
+["d","e","f"]	{"a":"three","b":"four"}
+["g","h","i"]	{"a":"five","b":"six"}
+PREHOOK: query: SELECT str, count(val)  FROM orc_create_complex GROUP BY str
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-      Edges:
-        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+POSTHOOK: query: SELECT str, count(val)  FROM orc_create_complex GROUP BY str
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: orc_create_complex
-                  Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-                  Select Operator
-                    expressions: str (type: string)
-                    outputColumnNames: _col0
-                    Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-                    Reduce Output Operator
-                      key expressions: _col0 (type: string)
-                      sort order: +
-                      Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-            Execution mode: vectorized, llap
-            LLAP IO: all inputs
-        Reducer 2 
-            Execution mode: vectorized, llap
-            Reduce Operator Tree:
-              Select Operator
-                expressions: KEY.reducesinkkey0 (type: string)
-                outputColumnNames: _col0
-                Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-      Processor Tree:
-        ListSink
-
-PREHOOK: query: SELECT str FROM orc_create_complex ORDER BY str
+str	c1
+line1	4501
+line2	4501
+line3	4501
+PREHOOK: query: SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT str FROM orc_create_complex ORDER BY str
+POSTHOOK: query: SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-str
-line1
-line2
-line3
+strct.b	_c1
+four	4501
+six	4501
+two	4501
+PREHOOK: query: SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+strct	mp	lst	str	c4
+{"a":"one","b":"two"}	{"key11":"value11","key12":"value12","key13":"value13"}	["a","b","c"]	line1	4501
+{"a":"three","b":"four"}	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]	line2	4501
+{"a":"five","b":"six"}	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]	line3	4501
diff --git a/ql/src/test/results/clientpositive/llap/vector_complex_join.q.out b/ql/src/test/results/clientpositive/llap/vector_complex_join.q.out
index 97d5642cae..133b8ef9c0 100644
--- a/ql/src/test/results/clientpositive/llap/vector_complex_join.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_complex_join.q.out
@@ -90,7 +90,7 @@ STAGE PLANS:
                         Statistics: Num rows: 1 Data size: 190 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col1 (type: map<int,string>)
             Execution mode: llap
-            LLAP IO: no inputs
+            LLAP IO: all inputs
 
   Stage: Stage-0
     Fetch Operator
@@ -211,7 +211,7 @@ STAGE PLANS:
                       Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                       value expressions: a (type: array<int>)
             Execution mode: llap
-            LLAP IO: no inputs
+            LLAP IO: all inputs
 
   Stage: Stage-0
     Fetch Operator
diff --git a/ql/src/test/results/clientpositive/vector_complex_all.q.out b/ql/src/test/results/clientpositive/vector_complex_all.q.out
index a54a371bb6..7ce707a2b0 100644
--- a/ql/src/test/results/clientpositive/vector_complex_all.q.out
+++ b/ql/src/test/results/clientpositive/vector_complex_all.q.out
@@ -34,8 +34,9 @@ PREHOOK: query: CREATE TABLE orc_create_complex (
   str STRING,
   mp  MAP<STRING,STRING>,
   lst ARRAY<STRING>,
-  strct STRUCT<A:STRING,B:STRING>
-) STORED AS ORC
+  strct STRUCT<A:STRING,B:STRING>,
+  val string
+) STORED AS ORC tblproperties("orc.row.index.stride"="1000", "orc.stripe.size"="1000", "orc.compress.size"="10000")
 PREHOOK: type: CREATETABLE
 PREHOOK: Output: database:default
 PREHOOK: Output: default@orc_create_complex
@@ -43,16 +44,19 @@ POSTHOOK: query: CREATE TABLE orc_create_complex (
   str STRING,
   mp  MAP<STRING,STRING>,
   lst ARRAY<STRING>,
-  strct STRUCT<A:STRING,B:STRING>
-) STORED AS ORC
+  strct STRUCT<A:STRING,B:STRING>,
+  val string
+) STORED AS ORC tblproperties("orc.row.index.stride"="1000", "orc.stripe.size"="1000", "orc.compress.size"="10000")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: database:default
 POSTHOOK: Output: default@orc_create_complex
-PREHOOK: query: INSERT OVERWRITE TABLE orc_create_complex SELECT * FROM orc_create_staging
+PREHOOK: query: INSERT OVERWRITE TABLE orc_create_complex
+SELECT orc_create_staging.*, '0' FROM orc_create_staging
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_staging
 PREHOOK: Output: default@orc_create_complex
-POSTHOOK: query: INSERT OVERWRITE TABLE orc_create_complex SELECT * FROM orc_create_staging
+POSTHOOK: query: INSERT OVERWRITE TABLE orc_create_complex
+SELECT orc_create_staging.*, '0' FROM orc_create_staging
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_staging
 POSTHOOK: Output: default@orc_create_complex
@@ -60,45 +64,8 @@ POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create
 POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
 POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]
 POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
-orc_create_staging.str	orc_create_staging.mp	orc_create_staging.lst	orc_create_staging.strct
-PREHOOK: query: -- Since complex types are not supported, this query should not vectorize.
-EXPLAIN
-SELECT * FROM orc_create_complex
-PREHOOK: type: QUERY
-POSTHOOK: query: -- Since complex types are not supported, this query should not vectorize.
-EXPLAIN
-SELECT * FROM orc_create_complex
-POSTHOOK: type: QUERY
-Explain
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: orc_create_complex
-            Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              expressions: str (type: string), mp (type: map<string,string>), lst (type: array<string>), strct (type: struct<a:string,b:string>)
-              outputColumnNames: _col0, _col1, _col2, _col3
-              Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-              File Output Operator
-                compressed: false
-                Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-                table:
-                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-      Processor Tree:
-        ListSink
-
+POSTHOOK: Lineage: orc_create_complex.val SIMPLE []
+orc_create_staging.str	orc_create_staging.mp	orc_create_staging.lst	orc_create_staging.strct	c1
 PREHOOK: query: SELECT * FROM orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
@@ -107,129 +74,156 @@ POSTHOOK: query: SELECT * FROM orc_create_complex
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-orc_create_complex.str	orc_create_complex.mp	orc_create_complex.lst	orc_create_complex.strct
-line1	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]	{"a":"one","b":"two"}
-line2	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]	{"a":"three","b":"four"}
-line3	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]	{"a":"five","b":"six"}
-PREHOOK: query: -- However, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT COUNT(*) FROM orc_create_complex
+orc_create_complex.str	orc_create_complex.mp	orc_create_complex.lst	orc_create_complex.strct	orc_create_complex.val
+line1	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]	{"a":"one","b":"two"}	0
+line2	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]	{"a":"three","b":"four"}	0
+line3	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]	{"a":"five","b":"six"}	0
+PREHOOK: query: SELECT str FROM orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT str FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+str
+line1
+line2
+line3
+PREHOOK: query: SELECT strct, mp, lst FROM orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT strct, mp, lst FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+strct	mp	lst
+{"a":"one","b":"two"}	{"key13":"value13","key11":"value11","key12":"value12"}	["a","b","c"]
+{"a":"three","b":"four"}	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]
+{"a":"five","b":"six"}	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]
+PREHOOK: query: SELECT lst, str FROM orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT lst, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+lst	str
+["a","b","c"]	line1
+["d","e","f"]	line2
+["g","h","i"]	line3
+PREHOOK: query: SELECT mp, str FROM orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT mp, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+mp	str
+{"key13":"value13","key11":"value11","key12":"value12"}	line1
+{"key21":"value21","key22":"value22","key23":"value23"}	line2
+{"key31":"value31","key32":"value32","key33":"value33"}	line3
+PREHOOK: query: SELECT strct, str FROM orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT strct, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+strct	str
+{"a":"one","b":"two"}	line1
+{"a":"three","b":"four"}	line2
+{"a":"five","b":"six"}	line3
+PREHOOK: query: SELECT strct.B, str FROM orc_create_complex
 PREHOOK: type: QUERY
-POSTHOOK: query: -- However, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT COUNT(*) FROM orc_create_complex
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT strct.B, str FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+b	str
+two	line1
+four	line2
+six	line3
+Warning: Shuffle Join JOIN[12][tables = [$hdt$_1, $hdt$_2, $hdt$_3, $hdt$_0]] in Stage 'Stage-1:MAPRED' is a cross product
+PREHOOK: query: INSERT INTO TABLE orc_create_complex
+SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_staging
+PREHOOK: Input: default@src
+PREHOOK: Output: default@orc_create_complex
+POSTHOOK: query: INSERT INTO TABLE orc_create_complex
+SELECT orc_create_staging.*, src1.key FROM orc_create_staging cross join src src1 cross join orc_create_staging spam1 cross join orc_create_staging spam2
 POSTHOOK: type: QUERY
-Explain
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: orc_create_complex
-            Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: COMPLETE
-            Select Operator
-              Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: COMPLETE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: bigint)
-      Execution mode: vectorized
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-      Processor Tree:
-        ListSink
-
-PREHOOK: query: SELECT COUNT(*) FROM orc_create_complex
+POSTHOOK: Input: default@orc_create_staging
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@orc_create_complex
+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]
+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]
+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]
+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]
+POSTHOOK: Lineage: orc_create_complex.val SIMPLE [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
+orc_create_staging.str	orc_create_staging.mp	orc_create_staging.lst	orc_create_staging.strct	src1.key
+PREHOOK: query: select count(*) from orc_create_complex
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT COUNT(*) FROM orc_create_complex
+POSTHOOK: query: select count(*) from orc_create_complex
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
 c0
-3
-PREHOOK: query: -- Also, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT str FROM orc_create_complex ORDER BY str
+13503
+PREHOOK: query: SELECT distinct lst, strct FROM orc_create_complex
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT distinct lst, strct FROM orc_create_complex
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+lst	strct
+["a","b","c"]	{"a":"one","b":"two"}
+["d","e","f"]	{"a":"three","b":"four"}
+["g","h","i"]	{"a":"five","b":"six"}
+PREHOOK: query: SELECT str, count(val)  FROM orc_create_complex GROUP BY str
 PREHOOK: type: QUERY
-POSTHOOK: query: -- Also, since this query is not referencing the complex fields, it should vectorize.
-EXPLAIN
-SELECT str FROM orc_create_complex ORDER BY str
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT str, count(val)  FROM orc_create_complex GROUP BY str
 POSTHOOK: type: QUERY
-Explain
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: orc_create_complex
-            Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              expressions: str (type: string)
-              outputColumnNames: _col0
-              Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col0 (type: string)
-                sort order: +
-                Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-      Execution mode: vectorized
-      Reduce Operator Tree:
-        Select Operator
-          expressions: KEY.reducesinkkey0 (type: string)
-          outputColumnNames: _col0
-          Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 3 Data size: 3177 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-      Processor Tree:
-        ListSink
-
-PREHOOK: query: SELECT str FROM orc_create_complex ORDER BY str
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+str	c1
+line1	4501
+line2	4501
+line3	4501
+PREHOOK: query: SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT str FROM orc_create_complex ORDER BY str
+POSTHOOK: query: SELECT strct.B, count(val) FROM orc_create_complex GROUP BY strct.B
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orc_create_complex
 #### A masked pattern was here ####
-str
-line1
-line2
-line3
+strct.b	_c1
+four	4501
+six	4501
+two	4501
+PREHOOK: query: SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT strct, mp, lst, str, count(val) FROM orc_create_complex GROUP BY strct, mp, lst, str
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_create_complex
+#### A masked pattern was here ####
+strct	mp	lst	str	c4
+{"a":"one","b":"two"}	{"key11":"value11","key12":"value12","key13":"value13"}	["a","b","c"]	line1	4501
+{"a":"three","b":"four"}	{"key21":"value21","key22":"value22","key23":"value23"}	["d","e","f"]	line2	4501
+{"a":"five","b":"six"}	{"key31":"value31","key32":"value32","key33":"value33"}	["g","h","i"]	line3	4501
diff --git a/storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java b/storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
index 907181eed3..13772c9e3c 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.common.io.encoded;
 
+import java.util.Arrays;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicInteger;
 
@@ -76,14 +77,17 @@ public void setIndexBaseOffset(int indexBaseOffset) {
   /** The key that is used to map this batch to source location. */
   protected BatchKey batchKey;
   /**
-   * Stream data for each stream, for each included column.
-   * For each column, streams are indexed by kind, with missing elements being null.
+   * Stream data for each column that has true in the corresponding hasData position.
+   * For each column, streams are indexed by kind (for ORC), with missing elements being null.
    */
   protected ColumnStreamData[][] columnData;
-  /** Column indexes included in the batch. Correspond to columnData elements. */
-  protected int[] columnIxs;
+  /** Indicates which columns have data. Correspond to columnData elements. */
+  protected boolean[] hasData;
 
   public void reset() {
+    if (hasData != null) {
+      Arrays.fill(hasData, false);
+    }
     if (columnData == null) return;
     for (int i = 0; i < columnData.length; ++i) {
       if (columnData[i] == null) continue;
@@ -93,37 +97,37 @@ public void reset() {
     }
   }
 
-  public void initColumn(int colIxMod, int colIx, int streamCount) {
-    columnIxs[colIxMod] = colIx;
-    if (columnData[colIxMod] == null || columnData[colIxMod].length != streamCount) {
-      columnData[colIxMod] = new ColumnStreamData[streamCount];
+  public void initColumn(int colIx, int streamCount) {
+    hasData[colIx] = true;
+    if (columnData[colIx] == null || columnData[colIx].length != streamCount) {
+      columnData[colIx] = new ColumnStreamData[streamCount];
     }
   }
 
-  public void setStreamData(int colIxMod, int streamKind, ColumnStreamData csd) {
-    columnData[colIxMod][streamKind] = csd;
-  }
-
-  public void setAllStreamsData(int colIxMod, int colIx, ColumnStreamData[] sbs) {
-    columnIxs[colIxMod] = colIx;
-    columnData[colIxMod] = sbs;
+  public void setStreamData(int colIx, int streamIx, ColumnStreamData csd) {
+    assert hasData[colIx];
+    columnData[colIx][streamIx] = csd;
   }
 
   public BatchKey getBatchKey() {
     return batchKey;
   }
 
-  public ColumnStreamData[][] getColumnData() {
-    return columnData;
+  public ColumnStreamData[] getColumnData(int colIx) {
+    if (!hasData[colIx]) throw new AssertionError("No data for column " + colIx);
+    return columnData[colIx];
   }
 
-  public int[] getColumnIxs() {
-    return columnIxs;
+  public int getTotalColCount() {
+    return columnData.length; // Includes the columns that have no data
   }
 
   protected void resetColumnArrays(int columnCount) {
-    if (columnIxs != null && columnCount == columnIxs.length) return;
-    columnIxs = new int[columnCount];
+    if (hasData != null && columnCount == hasData.length) {
+      Arrays.fill(hasData, false);
+      return;
+    }
+    hasData = new boolean[columnCount];
     ColumnStreamData[][] columnData = new ColumnStreamData[columnCount][];
     if (this.columnData != null) {
       for (int i = 0; i < Math.min(columnData.length, this.columnData.length); ++i) {
@@ -132,4 +136,8 @@ protected void resetColumnArrays(int columnCount) {
     }
     this.columnData = columnData;
   }
-}
\ No newline at end of file
+
+  public boolean hasData(int colIx) {
+    return hasData[colIx];
+  }
+}
