diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index 219d1adf25..1ec6ac8203 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -403,7 +403,6 @@ public Collection<String> getStoredStats() {
   protected transient FileSystem fs;
   protected transient Serializer serializer;
   protected final transient LongWritable row_count = new LongWritable();
-  private transient boolean isNativeTable = true;
 
   /**
    * The evaluators for the multiFile sprayer. If the table under consideration has 1000 buckets,
@@ -486,7 +485,6 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     try {
       this.hconf = hconf;
       filesCreated = false;
-      isNativeTable = !conf.getTableInfo().isNonNative();
       isTemporary = conf.isTemporary();
       multiFileSpray = conf.isMultiFileSpray();
       this.isBucketed = hconf.getInt(hive_metastoreConstants.BUCKET_COUNT, 0) > 0;
@@ -716,7 +714,7 @@ protected void createBucketFiles(FSPaths fsp) throws HiveException {
       assert filesIdx == numFiles;
 
       // in recent hadoop versions, use deleteOnExit to clean tmp files.
-      if (isNativeTable && fs != null && fsp != null && !conf.isMmTable()) {
+      if (isNativeTable() && fs != null && fsp != null && !conf.isMmTable()) {
         autoDelete = fs.deleteOnExit(fsp.outPaths[0]);
       }
     } catch (Exception e) {
@@ -730,7 +728,7 @@ protected void createBucketFiles(FSPaths fsp) throws HiveException {
   protected void createBucketForFileIdx(FSPaths fsp, int filesIdx)
       throws HiveException {
     try {
-      fsp.initializeBucketPaths(filesIdx, taskId, isNativeTable, isSkewedStoredAsSubDirectories);
+      fsp.initializeBucketPaths(filesIdx, taskId, isNativeTable(), isSkewedStoredAsSubDirectories);
       if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {
         Utilities.FILE_OP_LOGGER.trace("createBucketForFileIdx " + filesIdx + ": final path " + fsp.finalPaths[filesIdx]
           + "; out path " + fsp.outPaths[filesIdx] +" (spec path " + specPath + ", tmp path "
@@ -740,7 +738,7 @@ protected void createBucketForFileIdx(FSPaths fsp, int filesIdx)
         LOG.info("New Final Path: FS " + fsp.finalPaths[filesIdx]);
       }
 
-      if (isNativeTable && !conf.isMmTable()) {
+      if (isNativeTable() && !conf.isMmTable()) {
         // in recent hadoop versions, use deleteOnExit to clean tmp files.
         autoDelete = fs.deleteOnExit(fsp.outPaths[filesIdx]);
       }
@@ -1318,7 +1316,7 @@ public void closeOp(boolean abort) throws HiveException {
           }
         }
 
-        if (isNativeTable) {
+        if (isNativeTable()) {
           fsp.commit(fs, commitPaths);
         }
       }
@@ -1335,7 +1333,7 @@ public void closeOp(boolean abort) throws HiveException {
       // Hadoop always call close() even if an Exception was thrown in map() or
       // reduce().
       for (FSPaths fsp : valToPaths.values()) {
-        fsp.abortWriters(fs, abort, !autoDelete && isNativeTable && !conf.isMmTable());
+        fsp.abortWriters(fs, abort, !autoDelete && isNativeTable() && !conf.isMmTable());
       }
     }
     fsp = prevFsp = null;
@@ -1359,7 +1357,7 @@ static public String getOperatorName() {
   public void jobCloseOp(Configuration hconf, boolean success)
       throws HiveException {
     try {
-      if ((conf != null) && isNativeTable) {
+      if ((conf != null) && isNativeTable()) {
         Path specPath = conf.getDirName();
         String unionSuffix = null;
         DynamicPartitionCtx dpCtx = conf.getDynPartCtx();
@@ -1585,4 +1583,8 @@ private Configuration unsetNestedColumnPaths(Configuration conf) {
     }
     return conf;
   }
+
+  private boolean isNativeTable() {
+    return !conf.getTableInfo().isNonNative();
+  }
 }
