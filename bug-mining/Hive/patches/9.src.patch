diff --git a/CHANGES.txt b/CHANGES.txt
index e242238278..8e4201d3ed 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -11,6 +11,8 @@ Trunk (unreleased changes)
 
   IMPROVEMENTS
 
+    HIVE-69. genMapRedTasks uses tree walker. (Namit through zshao)
+
   OPTIMIZATIONS
 
   BUG FIXES
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index c283647d6f..40a81ea2f1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -163,4 +163,11 @@ public void process(Object row, ObjectInspector rowInspector) throws HiveExcepti
       throw new HiveException (e);
     }
   }
+
+  /**
+   * @return the name of the operator
+   */
+  public String getOperatorName() {
+    return new String("FS");
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
index 03abd4d2b5..6ac932228f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
@@ -18,19 +18,14 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
-import java.util.HashMap;
-import java.util.List;
+import java.io.Serializable;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.hive.ql.parse.OpParseContext;
-import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.filterDesc;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.parse.RowResolver;
+import org.apache.hadoop.io.LongWritable;
 
 /**
  * Filter operator implementation
@@ -78,15 +73,4 @@ public void process(Object row, ObjectInspector rowInspector) throws HiveExcepti
           conditionInspectableObject.o.getClass().getName());
     }
   }
-  
-  public List<String> mergeColListsFromChildren(List<String> colList,
-                                        HashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx) {
-    exprNodeDesc condn = conf.getPredicate();
-
-    // get list of columns used in the filter
-    List<String> cl = condn.getCols();
-
-    return Utilities.mergeUniqElems(colList, cl);
-  }
-
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
index c3bf10d163..e775354be4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
@@ -18,22 +18,20 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.util.*;
-import java.io.*;
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.parse.OpParseContext;
+import org.apache.hadoop.hive.ql.plan.explain;
 import org.apache.hadoop.hive.ql.plan.mapredWork;
-import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.mapred.OutputCollector;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.ql.plan.explain;
-import org.apache.hadoop.hive.ql.parse.RowResolver;
-import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.mapred.OutputCollector;
 
 /**
  * Base operator implementation
@@ -289,22 +287,12 @@ public void logStats () {
     }    
   }
 
-  public List<String> mergeColListsFromChildren(List<String> colList, 
-                                        HashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx) {
-    return colList;
-  }
-
-  public List<String> genColLists(HashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx) 
-    throws SemanticException {
-    List<String> colList = new ArrayList<String>();
-    if (childOperators != null)
-      for(Operator<? extends Serializable> o: childOperators)
-        colList = Utilities.mergeUniqElems(colList, o.genColLists(opParseCtx));
-
-    List<String> cols = mergeColListsFromChildren(colList, opParseCtx);
-    OpParseContext ctx = opParseCtx.get(this);
-    ctx.setColNames(cols);
-    return cols;
+  /**
+   * returns the name of the operator - specific subclasses can override the name as and when needed
+   * @return the name of the operator
+   */
+  public String getOperatorName() {
+    return new String("OP");
   }
 
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java
index adda7ac5e8..c7794288d3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java
@@ -49,6 +49,7 @@ public opTuple(Class<T> descClass, Class<? extends Operator<T>> opClass) {
     opvec.add(new opTuple<groupByDesc> (groupByDesc.class, GroupByOperator.class));
     opvec.add(new opTuple<joinDesc> (joinDesc.class, JoinOperator.class));
     opvec.add(new opTuple<limitDesc> (limitDesc.class, LimitOperator.class));
+    opvec.add(new opTuple<tableScanDesc> (tableScanDesc.class, TableScanOperator.class));
   }
               
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
index e2757833ea..f187ede1bd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
@@ -18,17 +18,16 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
+import java.io.IOException;
+import java.io.Serializable;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.HashMap;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.reduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.tableDesc;
-import org.apache.hadoop.hive.ql.io.*;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
@@ -38,9 +37,6 @@
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.hive.ql.parse.OpParseContext;
-import org.apache.hadoop.hive.ql.parse.RowResolver;
-import org.apache.hadoop.hive.ql.parse.SemanticException;
 
 /**
  * Reduce Sink Operator sends output to the reduce stage
@@ -200,61 +196,12 @@ public void process(Object row, ObjectInspector rowInspector) throws HiveExcepti
       throw new HiveException (e);
     }
   }
-  
-  public List<String> genColLists(HashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx) 
-    throws SemanticException {
-    RowResolver redSinkRR = opParseCtx.get(this).getRR();
-    List<String> childColLists = new ArrayList<String>();
-
-    for(Operator<? extends Serializable> o: childOperators)
-      childColLists = Utilities.mergeUniqElems(childColLists, o.genColLists(opParseCtx));
-
-    List<String> colLists = new ArrayList<String>();
-    ArrayList<exprNodeDesc> keys = conf.getKeyCols();
-    for (exprNodeDesc key : keys)
-      colLists = Utilities.mergeUniqElems(colLists, key.getCols());
 
-    // In case of extract child, see the columns used and propagate them
-    if ((childOperators.size() == 1) && (childOperators.get(0) instanceof ExtractOperator)) {
-      assert parentOperators.size() == 1;
-      Operator<? extends Serializable> par = parentOperators.get(0);
-      RowResolver parRR = opParseCtx.get(par).getRR();
-
-      for (String childCol : childColLists) {
-        String [] nm = redSinkRR.reverseLookup(childCol);
-        ColumnInfo cInfo = parRR.get(nm[0],nm[1]);
-        if (!colLists.contains(cInfo.getInternalName()))
-          colLists.add(cInfo.getInternalName());
-      }
-    }
-    else if ((childOperators.size() == 1) && (childOperators.get(0) instanceof JoinOperator)) {
-      assert parentOperators.size() == 1;
-      Operator<? extends Serializable> par = parentOperators.get(0);
-      RowResolver parRR = opParseCtx.get(par).getRR();
-      RowResolver childRR = opParseCtx.get(childOperators.get(0)).getRR();
-
-      for (String childCol : childColLists) {
-        String [] nm = childRR.reverseLookup(childCol);
-        ColumnInfo cInfo = redSinkRR.get(nm[0],nm[1]);
-        if (cInfo != null) {
-          cInfo = parRR.get(nm[0], nm[1]);
-          if (!colLists.contains(cInfo.getInternalName()))
-            colLists.add(cInfo.getInternalName());
-        }
-      }
-    }
-    else {
-      
-      // Reduce Sink contains the columns needed - no need to aggregate from children
-      ArrayList<exprNodeDesc> vals = conf.getValueCols();
-      for (exprNodeDesc val : vals)
-        colLists = Utilities.mergeUniqElems(colLists, val.getCols());
-    }
-
-    OpParseContext ctx = opParseCtx.get(this);
-    ctx.setColNames(colLists);
-    opParseCtx.put(this, ctx);
-    return colLists;
+  /**
+   * @return the name of the operator
+   */
+  public String getOperatorName() {
+    return new String("RS");
   }
-
+  
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
index 8e81b904b6..1c11951eb8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
@@ -18,20 +18,16 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
+import java.io.Serializable;
 import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.parse.OpParseContext;
 import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.selectDesc;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.parse.SemanticException;
 
 /**
  * Select operator implementation
@@ -91,61 +87,4 @@ public void process(Object row, ObjectInspector rowInspector)
     }
     forward(output, outputObjectInspector);
   }
-
-  private List<String> getColsFromExpr(HashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx) {
-    List<String> cols = new ArrayList<String>();
-    ArrayList<exprNodeDesc> exprList = conf.getColList();
-    for (exprNodeDesc expr : exprList)
-      cols = Utilities.mergeUniqElems(cols, expr.getCols());
-    List<Integer> listExprs = new ArrayList<Integer>();
-    for (int pos = 0; pos < exprList.size(); pos++)
-      listExprs.add(new Integer(pos));
-    OpParseContext ctx = opParseCtx.get(this);
-    ctx.setColNames(cols);
-    opParseCtx.put(this, ctx);
-    return cols;
-  }
-
-  private List<String> getColsFromExpr(List<String> colList, 
-                                       HashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx) {
-  	if (colList.isEmpty())
-  		return getColsFromExpr(opParseCtx);
-  	
-    List<String> cols = new ArrayList<String>();
-    ArrayList<exprNodeDesc> selectExprs = conf.getColList();
-    List<Integer> listExprs = new ArrayList<Integer>();
-
-    for (String col : colList) {
-      // col is the internal name i.e. position within the expression list
-      Integer pos = new Integer(col);
-      exprNodeDesc expr = selectExprs.get(pos.intValue());
-      cols = Utilities.mergeUniqElems(cols, expr.getCols());
-      listExprs.add(pos);
-    }
-
-    OpParseContext ctx = opParseCtx.get(this);
-    ctx.setColNames(cols);
-    opParseCtx.put(this, ctx);
-    return cols;
-  }
-
-  public List<String> genColLists(HashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx) 
-    throws SemanticException {
-    List<String> cols = new ArrayList<String>();
-    
-    for(Operator<? extends Serializable> o: childOperators) {
-      // if one of my children is a fileSink, return everything
-      if ((o instanceof FileSinkOperator) || (o instanceof ScriptOperator))
-        return getColsFromExpr(opParseCtx);
-
-      cols = Utilities.mergeUniqElems(cols, o.genColLists(opParseCtx));
-    }
-
-    if (conf.isSelectStar())
-      // The input to the select does not matter. Go over the expressions and return the ones which have a marked column
-      return getColsFromExpr(cols, opParseCtx);
-    
-    return getColsFromExpr(opParseCtx);
-  }
-
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
new file mode 100644
index 0000000000..d987329c37
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec;
+
+import java.io.*;
+
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.plan.tableScanDesc;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * Table Scan Operator
+ * If the data is coming from the map-reduce framework, just forward it.
+ * This will be needed as part of local work when data is not being read as part of map-reduce framework
+ **/
+public class TableScanOperator extends Operator<tableScanDesc> implements Serializable {
+  private static final long serialVersionUID = 1L;
+  public void initialize(Configuration hconf) throws HiveException {
+    super.initialize(hconf);
+    // nothing to do really ..
+  }
+
+  /**
+   * Currently, the table scan operator does not do anything special other than just forwarding the row. Since the 
+   * table data is always read as part of the map-reduce framework by the mapper. But, this assumption is not true,
+   * i.e table data is not only read by the mapper, this operator will be enhanced to read the table.
+   **/
+  @Override
+  public void process(Object row, ObjectInspector rowInspector)
+      throws HiveException {
+    forward(row, rowInspector);    
+  }
+
+  /**
+   * The operator name for this operator type. This is used to construct the rule for an operator
+   * @return the operator name
+   **/
+  public String getOperatorName() {
+    return new String("TS");
+  }
+
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
index 998bd05881..78d061dad0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
@@ -109,11 +109,17 @@ public List<Task<? extends Serializable>> getParentTasks() {
     return parentTasks;
   }
 
-  public void addDependentTask(Task<? extends Serializable> dependent) {
+  /**
+   * Add a dependent task on the current task. Return if the dependency already existed or is this a new one
+   * @return true if the task got added false if it already existed
+   */
+  public boolean addDependentTask(Task<? extends Serializable> dependent) {
+    boolean ret = false;
     if (getChildTasks() == null) {
       setChildTasks(new ArrayList<Task<? extends Serializable>>());
     }
     if (!getChildTasks().contains(dependent)) {
+      ret = true;
       getChildTasks().add(dependent);
       if (dependent.getParentTasks() == null) {
         dependent.setParentTasks(new ArrayList<Task<? extends Serializable>>());
@@ -122,6 +128,19 @@ public void addDependentTask(Task<? extends Serializable> dependent) {
         dependent.getParentTasks().add(this);
       }
     }
+    return ret;
+  }
+
+  /**
+   * remove the dependent task
+   * @param dependent the task to remove
+   */
+  public void removeDependentTask(Task<? extends Serializable> dependent) {
+    if ((getChildTasks() != null) && (getChildTasks().contains(dependent))) {
+      getChildTasks().remove(dependent);
+      if ((dependent.getParentTasks() != null) && (dependent.getParentTasks().contains(this)))
+        dependent.getParentTasks().remove(this);
+    }
   }
 
   public boolean done() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPruner.java
index 4abfea83f6..f42518715d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPruner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPruner.java
@@ -18,38 +18,64 @@
 
 package org.apache.hadoop.hive.ql.optimizer;
 
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Stack;
+
+import org.apache.hadoop.hive.ql.exec.ColumnInfo;
+import org.apache.hadoop.hive.ql.exec.ExtractOperator;
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
+import org.apache.hadoop.hive.ql.exec.FilterOperator;
+import org.apache.hadoop.hive.ql.exec.GroupByOperator;
+import org.apache.hadoop.hive.ql.exec.JoinOperator;
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.OperatorFactory;
+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
+import org.apache.hadoop.hive.ql.exec.RowSchema;
+import org.apache.hadoop.hive.ql.exec.ScriptOperator;
+import org.apache.hadoop.hive.ql.exec.SelectOperator;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.parse.DefaultDispatcher;
+import org.apache.hadoop.hive.ql.parse.Dispatcher;
+import org.apache.hadoop.hive.ql.parse.DefaultOpGraphWalker;
+import org.apache.hadoop.hive.ql.parse.OpGraphWalker;
+import org.apache.hadoop.hive.ql.parse.OpParseContext;
+import org.apache.hadoop.hive.ql.parse.OperatorProcessor;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.QB;
-import org.apache.hadoop.hive.ql.parse.OpParseContext;
-import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory;
-import org.apache.hadoop.hive.ql.parse.RowResolver;
-import org.apache.hadoop.hive.ql.exec.Operator;
-import org.apache.hadoop.hive.ql.exec.OperatorFactory;
-import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.plan.aggregationDesc;
 import org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc;
+import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.groupByDesc;
+import org.apache.hadoop.hive.ql.plan.reduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.selectDesc;
-import org.apache.hadoop.hive.ql.exec.ColumnInfo;
-import org.apache.hadoop.hive.ql.exec.RowSchema;
-import java.io.Serializable;
-import java.util.List;
-import java.util.Iterator;
-import java.util.ArrayList;
+import org.apache.hadoop.hive.ql.parse.Rule;
 
 /**
- * Implementation of one of the rule-based optimization steps. ColumnPruner gets the current operator tree. The tree is traversed to find out the columns used 
- * for all the base tables. If all the columns for a table are not used, a select is pushed on top of that table (to select only those columns). Since this 
+ * Implementation of one of the rule-based optimization steps. ColumnPruner gets the current operator tree. The \
+ * tree is traversed to find out the columns used 
+ * for all the base tables. If all the columns for a table are not used, a select is pushed on top of that table 
+ * (to select only those columns). Since this 
  * changes the row resolver, the tree is built again. This can be optimized later to patch the tree. 
  */
 public class ColumnPruner implements Transform {
-  private ParseContext pctx;
-  
+  protected ParseContext pGraphContext;
+  private HashMap<Operator<? extends Serializable>, OpParseContext> opToParseCtxMap;
+
+
   /**
    * empty constructor
    */
 	public ColumnPruner() {
-    pctx = null;
+    pGraphContext = null;
 	}
 
 	/**
@@ -59,7 +85,7 @@ public ColumnPruner() {
 	 * @return boolean
 	 */
   private boolean pushSelect(Operator<? extends Serializable> op, List<String> colNames) {
-    if (pctx.getOpParseCtx().get(op).getRR().getColumnInfos().size() == colNames.size()) return false;
+    if (pGraphContext.getOpParseCtx().get(op).getRR().getColumnInfos().size() == colNames.size()) return false;
     return true;
   }
 
@@ -72,7 +98,7 @@ private boolean pushSelect(Operator<? extends Serializable> op, List<String> col
   @SuppressWarnings("nls")
   private Operator<? extends Serializable> putOpInsertMap(Operator<? extends Serializable> op, RowResolver rr) {
     OpParseContext ctx = new OpParseContext(rr);
-    pctx.getOpParseCtx().put(op, ctx);
+    pGraphContext.getOpParseCtx().put(op, ctx);
     return op;
   }
 
@@ -87,7 +113,7 @@ private Operator<? extends Serializable> putOpInsertMap(Operator<? extends Seria
   private Operator genSelectPlan(Operator input, List<String> colNames) 
     throws SemanticException {
 
-    RowResolver inputRR  = pctx.getOpParseCtx().get(input).getRR();
+    RowResolver inputRR  = pGraphContext.getOpParseCtx().get(input).getRR();
     RowResolver outputRR = new RowResolver();
     ArrayList<exprNodeDesc> col_list = new ArrayList<exprNodeDesc>();
     
@@ -125,26 +151,33 @@ private void resetParseContext(ParseContext pctx) {
   }
 	
   /**
-   * Transform the query tree. For each table under consideration, check if all columns are needed. If not, only select the operators needed at
-   * the beginning and proceed 
+   * Transform the query tree. For each table under consideration, check if all columns are needed. If not, 
+   * only select the operators needed at the beginning and proceed 
+   * @param pactx the current parse context
    */
 	public ParseContext transform(ParseContext pactx) throws SemanticException {
-    this.pctx = pactx;
-    boolean done = true;
-    // generate useful columns for all the sources so that they can be pushed immediately after the table scan
-    for (String alias_id : pctx.getTopOps().keySet()) {
-      Operator<? extends Serializable> topOp = pctx.getTopOps().get(alias_id);
-      
-      // Scan the tree bottom-up and generate columns needed for the top operator
-      List<String> colNames = topOp.genColLists(pctx.getOpParseCtx());
+    this.pGraphContext = pactx;
+    this.opToParseCtxMap = pGraphContext.getOpParseCtx();
+
+    boolean done = true;    
+
+    // generate pruned column list for all relevant operators
+    ColumnPrunerProcessor cpp = new ColumnPrunerProcessor(opToParseCtxMap);
+    Dispatcher disp = new DefaultDispatcher(cpp);
+    OpGraphWalker ogw = new ColumnPrunerWalker(disp);
+    ogw.startWalking(pGraphContext.getTopOps().values());
 
+    // create a new select operator if any of input tables' columns can be pruned
+    for (String alias_id : pGraphContext.getTopOps().keySet()) {
+      Operator<? extends Serializable> topOp = pGraphContext.getTopOps().get(alias_id);
+
+      List<String> colNames = cpp.getPrunedColList(topOp);
       // do we need to push a SELECT - all the columns of the table are not used
       if (pushSelect(topOp, colNames)) {
         topOp.setChildOperators(null);
-
         // Generate a select and make it a child of the table scan
         Operator select = genSelectPlan(topOp, colNames);
-        pctx.getTopSelOps().put(alias_id, select);
+        pGraphContext.getTopSelOps().put(alias_id, select);
         done = false;
       }
     }
@@ -152,17 +185,203 @@ public ParseContext transform(ParseContext pactx) throws SemanticException {
     // a select was pushed on top of the table. The old plan is no longer valid. Generate the plan again.
     // The current tables and the select pushed above (after column pruning) are maintained in the parse context.
     if (!done) {
-      SemanticAnalyzer sem = (SemanticAnalyzer)SemanticAnalyzerFactory.get(pctx.getConf(), pctx.getParseTree());
-      
-      resetParseContext(pctx);
-      sem.init(pctx);
-    	QB qb = new QB(null, null, false);
-    	
-    	sem.doPhase1(pctx.getParseTree(), qb, sem.initPhase1Ctx());
-    	sem.getMetaData(qb);
-    	sem.genPlan(qb);
-      pctx = sem.getParseContext();
-   	}	
-    return pctx;
-  }
+      SemanticAnalyzer sem = (SemanticAnalyzer)SemanticAnalyzerFactory.get(pGraphContext.getConf(), pGraphContext.getParseTree());
+
+      resetParseContext(pGraphContext);
+      sem.init(pGraphContext);
+      QB qb = new QB(null, null, false);
+
+      sem.doPhase1(pGraphContext.getParseTree(), qb, sem.initPhase1Ctx());
+      sem.getMetaData(qb);
+      sem.genPlan(qb);
+      pGraphContext = sem.getParseContext();
+    }	
+    return pGraphContext;
+	}
+
+	/**
+	 * Column pruner processor
+	 **/
+	public static class ColumnPrunerProcessor implements OperatorProcessor {
+	  private  Map<Operator<? extends Serializable>,List<String>> prunedColLists = 
+	    new HashMap<Operator<? extends Serializable>, List<String>>();
+	  private HashMap<Operator<? extends Serializable>, OpParseContext> opToParseCtxMap;
+	    
+    public ColumnPrunerProcessor(HashMap<Operator<? extends Serializable>, OpParseContext> opToParseContextMap) {
+	    this.opToParseCtxMap = opToParseContextMap;
+	  }
+
+    /**
+     * @return the prunedColLists
+     */
+    public List<String> getPrunedColList(Operator<? extends Serializable> op) {
+      return prunedColLists.get(op);
+    }
+
+	  private List<String> genColLists(Operator<? extends Serializable> curOp) throws SemanticException {
+	    List<String> colList = new ArrayList<String>();
+	    if(curOp.getChildOperators() != null) {
+	      for(Operator<? extends Serializable> child: curOp.getChildOperators())
+	        colList = Utilities.mergeUniqElems(colList, prunedColLists.get(child));
+	    }
+	    return colList;
+	  }
+
+    public void process(FilterOperator op, OperatorProcessorContext ctx) throws SemanticException {
+	    exprNodeDesc condn = op.getConf().getPredicate();
+	    // get list of columns used in the filter
+	    List<String> cl = condn.getCols();
+	    // merge it with the downstream col list
+	    prunedColLists.put(op, Utilities.mergeUniqElems(genColLists(op), cl));
+	  }
+
+    public void process(GroupByOperator op, OperatorProcessorContext ctx) throws SemanticException {
+	    List<String> colLists = new ArrayList<String>();
+	    groupByDesc conf = op.getConf();
+	    ArrayList<exprNodeDesc> keys = conf.getKeys();
+	    for (exprNodeDesc key : keys)
+	      colLists = Utilities.mergeUniqElems(colLists, key.getCols());
+
+	    ArrayList<aggregationDesc> aggrs = conf.getAggregators();
+	    for (aggregationDesc aggr : aggrs) { 
+	      ArrayList<exprNodeDesc> params = aggr.getParameters();
+	      for (exprNodeDesc param : params) 
+	        colLists = Utilities.mergeUniqElems(colLists, param.getCols());
+	    }
+
+	    prunedColLists.put(op, colLists);
+	  }
+
+    public void process(Operator<? extends Serializable> op, OperatorProcessorContext ctx) throws SemanticException {
+	    prunedColLists.put(op, genColLists(op));
+	  }
+
+    public void process(ReduceSinkOperator op, OperatorProcessorContext ctx) throws SemanticException {
+	    RowResolver redSinkRR = opToParseCtxMap.get(op).getRR();
+	    reduceSinkDesc conf = op.getConf();
+	    List<Operator<? extends Serializable>> childOperators = op.getChildOperators();
+	    List<Operator<? extends Serializable>> parentOperators = op.getParentOperators();
+	    List<String> childColLists = new ArrayList<String>();
+
+	    for(Operator<? extends Serializable> child: childOperators)
+	      childColLists = Utilities.mergeUniqElems(childColLists, prunedColLists.get(child));
+
+	    List<String> colLists = new ArrayList<String>();
+	    ArrayList<exprNodeDesc> keys = conf.getKeyCols();
+	    for (exprNodeDesc key : keys)
+	      colLists = Utilities.mergeUniqElems(colLists, key.getCols());
+
+	    if ((childOperators.size() == 1) && (childOperators.get(0) instanceof JoinOperator)) {
+	      assert parentOperators.size() == 1;
+	      Operator<? extends Serializable> par = parentOperators.get(0);
+	      RowResolver parRR = opToParseCtxMap.get(par).getRR();
+	      RowResolver childRR = opToParseCtxMap.get(childOperators.get(0)).getRR();
+
+	      for (String childCol : childColLists) {
+	        String [] nm = childRR.reverseLookup(childCol);
+	        ColumnInfo cInfo = redSinkRR.get(nm[0],nm[1]);
+	        if (cInfo != null) {
+	          cInfo = parRR.get(nm[0], nm[1]);
+	          if (!colLists.contains(cInfo.getInternalName()))
+	            colLists.add(cInfo.getInternalName());
+	        }
+	      }
+	    }
+	    else {
+	      // Reduce Sink contains the columns needed - no need to aggregate from children
+	      ArrayList<exprNodeDesc> vals = conf.getValueCols();
+	      for (exprNodeDesc val : vals)
+	        colLists = Utilities.mergeUniqElems(colLists, val.getCols());
+	    }
+
+	    prunedColLists.put(op, colLists);
+	  }
+
+    public void process(SelectOperator op, OperatorProcessorContext ctx) throws SemanticException {
+	    List<String> cols = new ArrayList<String>();
+
+	    if(op.getChildOperators() != null) {
+	      for(Operator<? extends Serializable> child: op.getChildOperators()) {
+	        // If one of my children is a FileSink or Script, return all columns.
+	        // Without this break, a bug in ReduceSink to Extract edge column pruning will manifest
+	        // which should be fixed before remove this
+	        if ((child instanceof FileSinkOperator) || (child instanceof ScriptOperator)) {
+	          prunedColLists.put(op, getColsFromSelectExpr(op));
+	          return;
+	        }
+	        cols = Utilities.mergeUniqElems(cols, prunedColLists.get(child));
+	      }
+	    }
+
+	    selectDesc conf = op.getConf();
+	    if (conf.isSelectStar() && !cols.isEmpty()) {
+	      // The input to the select does not matter. Go over the expressions 
+	      // and return the ones which have a marked column
+	      prunedColLists.put(op, getSelectColsFromChildren(op, cols));
+	      return;
+	    }
+	    prunedColLists.put(op, getColsFromSelectExpr(op));
+	  }
+
+	  private List<String> getColsFromSelectExpr(SelectOperator op) {
+	    List<String> cols = new ArrayList<String>();
+	    selectDesc conf = op.getConf();
+	    ArrayList<exprNodeDesc> exprList = conf.getColList();
+	    for (exprNodeDesc expr : exprList)
+	      cols = Utilities.mergeUniqElems(cols, expr.getCols());
+	    return cols;
+	  }
+
+	  private List<String> getSelectColsFromChildren(SelectOperator op, List<String> colList) {
+	    List<String> cols = new ArrayList<String>();
+	    selectDesc conf = op.getConf();
+	    ArrayList<exprNodeDesc> selectExprs = conf.getColList();
+
+	    for (String col : colList) {
+	      // col is the internal name i.e. position within the expression list
+	      exprNodeDesc expr = selectExprs.get(Integer.parseInt(col));
+	      cols = Utilities.mergeUniqElems(cols, expr.getCols());
+	    }
+	    return cols;
+	  }
+	}
+	/**
+	 * Walks the op tree in post order fashion (skips selects with file sink or script op children)
+	 */
+	public static class ColumnPrunerWalker extends DefaultOpGraphWalker {
+
+	  public ColumnPrunerWalker(Dispatcher disp) {
+      super(disp);
+    }
+
+    /**
+	   * Walk the given operator
+	   */
+	  @Override
+	  public void walk(Operator<? extends Serializable> op) throws SemanticException {
+	    boolean walkChildren = true;
+
+	    // no need to go further down for a select op with a file sink or script child
+	    // since all cols are needed for these ops
+	    if(op instanceof SelectOperator) {
+	      for(Operator<? extends Serializable> child: op.getChildOperators()) {
+	        if ((child instanceof FileSinkOperator) || (child instanceof ScriptOperator))
+	          walkChildren = false;
+	      }
+	    }
+
+	    if((op.getChildOperators() == null) 
+	        || getDispatchedList().containsAll(op.getChildOperators()) 
+	        || !walkChildren) {
+	      // all children are done or no need to walk the children
+	      dispatch(op, null);
+	      return;
+	    }
+	    // move all the children to the front of queue
+	    getToWalk().removeAll(op.getChildOperators());
+	    getToWalk().addAll(0, op.getChildOperators());
+	    // add self to the end of the queue
+	    getToWalk().add(op);
+	  }
+	}
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
new file mode 100644
index 0000000000..bbbd5a2252
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
@@ -0,0 +1,93 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer;
+
+import java.util.List;
+import java.util.HashMap;
+import java.io.Serializable;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.parse.OperatorProcessor;
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.plan.mapredWork;
+import org.apache.hadoop.hive.ql.metadata.*;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+
+/**
+ * Processor for the rule - table scan followed by reduce sink
+ */
+public class GenMRFileSink1 implements OperatorProcessor {
+
+  public GenMRFileSink1() {
+  }
+
+  /**
+   * File Sink Operator encountered 
+   * @param op the file sink operator encountered
+   * @param opProcCtx context
+   */
+  public void process(FileSinkOperator op, OperatorProcessorContext opProcCtx) throws SemanticException {
+    GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
+    boolean ret = false;
+
+    Task<? extends Serializable> mvTask = ctx.getMvTask();
+    Task<? extends Serializable> currTask = ctx.getCurrTask();
+    Operator<? extends Serializable> currTopOp = ctx.getCurrTopOp();
+    String currAliasId = ctx.getCurrAliasId();
+    HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap = ctx.getOpTaskMap();
+    List<Operator<? extends Serializable>> seenOps = ctx.getSeenOps();
+    List<Task<? extends Serializable>>  rootTasks = ctx.getRootTasks();
+
+    // Set the move task to be dependent on the current task
+    if (mvTask != null) 
+      ret = currTask.addDependentTask(mvTask);
+    
+    // In case of multi-table insert, the path to alias mapping is needed for all the sources. Since there is no
+    // reducer, treat it as a plan with null reducer
+    // If it is a map-only job, the task needs to be processed
+    if (currTopOp != null) {
+      Task<? extends Serializable> mapTask = opTaskMap.get(null);
+      if (mapTask == null) {
+        assert (!seenOps.contains(currTopOp));
+        seenOps.add(currTopOp);
+        GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, (mapredWork) currTask.getWork(), false, ctx);
+        opTaskMap.put(null, currTask);
+        rootTasks.add(currTask);
+      }
+      else {
+        if (!seenOps.contains(currTopOp)) {
+          seenOps.add(currTopOp);
+          GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, (mapredWork) mapTask.getWork(), false, ctx);
+        }
+        if (ret)
+          currTask.removeDependentTask(mvTask);
+      }
+    }
+  }
+
+  /**
+   * @param op the operator encountered
+   * @param opProcCtx context
+   */
+  public void process(Operator<? extends Serializable> op, OperatorProcessorContext opProcCtx) throws SemanticException {
+    // should never be called
+    assert false;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java
new file mode 100644
index 0000000000..1ce16ba8fc
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java
@@ -0,0 +1,48 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer;
+
+import java.io.Serializable;
+import java.util.Map;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.parse.OperatorProcessor;
+import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;
+
+/**
+ * Processor for the rule - no specific rule fired
+ */
+public class GenMROperator implements OperatorProcessor {
+
+  public GenMROperator() {
+  }
+
+  /**
+   * Reduce Scan encountered 
+   * @param op the reduce sink operator encountered
+   * @param opProcCtx context
+   */
+  public void process(Operator<? extends Serializable> op, OperatorProcessorContext opProcCtx) throws SemanticException {
+    GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
+
+    Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = ctx.getMapCurrCtx();
+    mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(), ctx.getCurrAliasId()));
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java
new file mode 100644
index 0000000000..de1957e293
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java
@@ -0,0 +1,336 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.Set;
+import java.util.Stack;
+import java.io.Serializable;
+import java.io.File;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.JoinOperator;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.exec.OperatorFactory;
+import org.apache.hadoop.hive.ql.plan.mapredWork;
+import org.apache.hadoop.hive.ql.plan.reduceSinkDesc;
+import org.apache.hadoop.hive.ql.plan.tableDesc;
+import org.apache.hadoop.hive.ql.plan.partitionDesc;
+import org.apache.hadoop.hive.ql.plan.fileSinkDesc;
+import org.apache.hadoop.hive.ql.plan.PlanUtils;
+import org.apache.hadoop.hive.ql.metadata.*;
+import org.apache.hadoop.hive.ql.parse.ParseContext;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.fs.Path;
+
+/**
+ * Processor Context for creating map reduce task. Walk the tree in a DFS manner and process the nodes. Some state is 
+ * maintained about the current nodes visited so far.
+ */
+public class GenMRProcContext extends OperatorProcessorContext {
+
+  /** 
+   * GenMapRedCtx is used to keep track of the current state. 
+   */
+  public static class GenMapRedCtx {
+    Task<? extends Serializable>         currTask;
+    Operator<? extends Serializable>     currTopOp;
+    String                               currAliasId;
+    
+    /**
+     * @param currTask    the current task
+     * @param currTopOp   the current top operator being traversed
+     * @param currAliasId the current alias for the to operator
+     */
+    public GenMapRedCtx (Task<? extends Serializable>         currTask,
+                         Operator<? extends Serializable>     currTopOp,
+                         String                               currAliasId) {
+      this.currTask    = currTask;
+      this.currTopOp   = currTopOp;
+      this.currAliasId = currAliasId;
+    }
+
+    /**
+     * @return current task
+     */
+    public Task<? extends Serializable> getCurrTask() {
+      return currTask;
+    }
+
+    /**
+     * @return current top operator
+     */
+    public Operator<? extends Serializable> getCurrTopOp() {
+      return currTopOp;
+    }
+
+    /**
+     * @return current alias
+     */
+    public String getCurrAliasId() {
+      return currAliasId;
+    }
+  }
+  
+  private HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap;
+  private List<Operator<? extends Serializable>> seenOps;
+
+  private ParseContext                          parseCtx;
+  private Task<? extends Serializable>          mvTask;
+  private List<Task<? extends Serializable>>    rootTasks;
+  private String scratchDir;
+  private int randomid;
+  private int pathid;
+
+  private Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx; 
+  private Task<? extends Serializable>         currTask;
+  private Operator<? extends Serializable>     currTopOp;
+  private String                               currAliasId;
+  private List<Operator<? extends Serializable>> rootOps;
+
+  /**
+   * @param opTaskMap  reducer to task mapping
+   * @param seenOps    operator already visited
+   * @param parseCtx   current parse context
+   * @param rootTasks  root tasks for the plan
+   * @param mvTask     the final move task
+   * @param scratchDir directory for temp destinations   
+   * @param randomId   identifier used for temp destinations   
+   * @param pathId     identifier used for temp destinations   
+   * @param mapCurrCtx operator to task mappings
+   */
+  public GenMRProcContext (
+    HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap,
+    List<Operator<? extends Serializable>> seenOps,
+    ParseContext                           parseCtx,
+    Task<? extends Serializable>           mvTask,
+    List<Task<? extends Serializable>>     rootTasks,
+    String scratchDir, int randomid, int pathid,
+    Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx) 
+  {
+
+    this.opTaskMap  = opTaskMap;
+    this.seenOps    = seenOps;
+    this.mvTask     = mvTask;
+    this.parseCtx   = parseCtx;
+    this.rootTasks  = rootTasks;
+    this.scratchDir = scratchDir;
+    this.randomid   = randomid;
+    this.pathid     = pathid;
+    this.mapCurrCtx = mapCurrCtx;
+    currTask        = null;
+    currTopOp       = null;
+    currAliasId     = null;
+    rootOps         = new ArrayList<Operator<? extends Serializable>>();
+    rootOps.addAll(parseCtx.getTopOps().values());
+  }
+
+  /**
+   * @return  reducer to task mapping
+   */
+  public HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> getOpTaskMap() {
+    return opTaskMap;
+  }
+
+  /**
+   * @param opTaskMap  reducer to task mapping
+   */
+  public void setOpTaskMap(HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap) {
+    this.opTaskMap = opTaskMap;
+  }
+
+  /**
+   * @return  operators already visited
+   */
+  public List<Operator<? extends Serializable>> getSeenOps() {
+    return seenOps;
+  }
+
+  /**
+   * @param seenOps    operators already visited
+   */
+  public void setSeenOps(List<Operator<? extends Serializable>> seenOps) {
+    this.seenOps = seenOps;
+  }
+
+  /**
+   * @return  top operators for tasks
+   */
+  public List<Operator<? extends Serializable>> getRootOps() {
+    return rootOps;
+  }
+
+  /**
+   * @param rootOps    top operators for tasks
+   */
+  public void setRootOps(List<Operator<? extends Serializable>> rootOps) {
+    this.rootOps = rootOps;
+  }
+
+  /**
+   * @return   current parse context
+   */
+  public ParseContext getParseCtx() {
+    return parseCtx;
+  }
+
+  /**
+   * @param parseCtx   current parse context
+   */
+  public void setParseCtx(ParseContext parseCtx) {
+    this.parseCtx = parseCtx;
+  }
+
+  /**
+   * @return     the final move task
+   */
+  public Task<? extends Serializable> getMvTask() {
+    return mvTask;
+  }
+
+  /**
+   * @param mvTask     the final move task
+   */
+  public void setMvTask(Task<? extends Serializable> mvTask) {
+    this.mvTask = mvTask;
+  }
+
+  /**
+   * @return  root tasks for the plan
+   */
+  public List<Task<? extends Serializable>>  getRootTasks() {
+    return rootTasks;
+  }
+
+  /**
+   * @param rootTasks  root tasks for the plan
+   */
+  public void setRootTasks(List<Task<? extends Serializable>>  rootTasks) {
+    this.rootTasks = rootTasks;
+  }
+
+  /**
+   * @return directory for temp destinations   
+   */
+  public String getScratchDir() {
+    return scratchDir;
+  }
+
+  /**
+   * @param scratchDir directory for temp destinations   
+   */
+  public void setScratchDir(String scratchDir) {
+    this.scratchDir = scratchDir;
+  }
+
+  /**
+   * @return   identifier used for temp destinations   
+   */
+  public int getRandomId() {
+    return randomid;
+  }
+
+  /**
+   * @param randomId   identifier used for temp destinations   
+   */
+  public void setRandomId(int randomid) {
+    this.randomid = randomid;
+  }
+
+  /**
+   * @return   identifier used for temp destinations   
+   */
+  public int getPathId() {
+    return pathid;
+  }
+
+  /**
+   * @param pathid   identifier used for temp destinations   
+   */
+  public void setPathId(int pathid) {
+    this.pathid = pathid;
+  }
+
+  /**
+   * @return operator to task mappings
+   */
+  public Map<Operator<? extends Serializable>, GenMapRedCtx> getMapCurrCtx() {
+    return mapCurrCtx;
+  }
+
+  /**
+   * @param mapCurrCtx operator to task mappings
+   */
+  public void setMapCurrCtx(Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx) {
+    this.mapCurrCtx = mapCurrCtx;
+  }
+
+  /**
+   * @return current task
+   */
+  public Task<? extends Serializable>  getCurrTask() {
+    return currTask;
+  }
+
+  /**
+   * @param currTask current task
+   */
+  public void setCurrTask(Task<? extends Serializable>  currTask) {
+    this.currTask = currTask;
+  }
+
+  /**
+   * @return current top operator
+   */
+  public Operator<? extends Serializable> getCurrTopOp() {
+    return currTopOp;
+  }   
+   
+  /**
+   * @param currTopOp current top operator
+   */
+  public void setCurrTopOp(Operator<? extends Serializable> currTopOp) {
+    this.currTopOp = currTopOp;
+  }      
+
+  /**
+   * @return current top alias
+   */
+  public String  getCurrAliasId() {
+    return currAliasId;
+  }
+
+  /**
+   * @param currAliasId current top alias
+   */
+  public void setCurrAliasId(String currAliasId) {
+    this.currAliasId = currAliasId;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java
new file mode 100644
index 0000000000..0e1e1acbb2
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java
@@ -0,0 +1,111 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.Set;
+import java.util.Stack;
+import java.io.Serializable;
+import java.io.File;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.JoinOperator;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.exec.OperatorFactory;
+import org.apache.hadoop.hive.ql.plan.mapredWork;
+import org.apache.hadoop.hive.ql.plan.reduceSinkDesc;
+import org.apache.hadoop.hive.ql.plan.tableDesc;
+import org.apache.hadoop.hive.ql.plan.partitionDesc;
+import org.apache.hadoop.hive.ql.plan.fileSinkDesc;
+import org.apache.hadoop.hive.ql.plan.PlanUtils;
+import org.apache.hadoop.hive.ql.metadata.*;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.parse.OperatorProcessor;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;
+
+/**
+ * Processor for the rule - table scan followed by reduce sink
+ */
+public class GenMRRedSink1 implements OperatorProcessor {
+
+  public GenMRRedSink1() {
+  }
+
+  /**
+   * Reduce Scan encountered 
+   * @param op the reduce sink operator encountered
+   * @param opProcCtx context
+   */
+  public void process(ReduceSinkOperator op, OperatorProcessorContext opProcCtx) throws SemanticException {
+    GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
+
+    Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = ctx.getMapCurrCtx();
+    GenMapRedCtx mapredCtx = mapCurrCtx.get(op.getParentOperators().get(0));
+    Task<? extends Serializable> currTask    = mapredCtx.getCurrTask();
+    mapredWork currPlan = (mapredWork) currTask.getWork();
+    Operator<? extends Serializable> currTopOp   = mapredCtx.getCurrTopOp();
+    String currAliasId = mapredCtx.getCurrAliasId();
+    Operator<? extends Serializable> reducer = op.getChildOperators().get(0);
+    HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap = ctx.getOpTaskMap();
+    Task<? extends Serializable> opMapTask = opTaskMap.get(reducer);
+
+    ctx.setCurrTopOp(currTopOp);
+    ctx.setCurrAliasId(currAliasId);
+    ctx.setCurrTask(currTask);
+
+    // If the plan for this reducer does not exist, initialize the plan
+    if (opMapTask == null) {
+      if (currPlan.getReducer() == null) 
+        GenMapRedUtils.initPlan(op, ctx);
+      else
+        GenMapRedUtils.splitPlan(op, ctx);
+    }
+    // This will happen in case of joins. The current plan can be thrown away after being merged with the
+    // original plan
+    else {
+      GenMapRedUtils.joinPlan(opMapTask, ctx);
+      currTask = opMapTask;
+    }
+
+    mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(), ctx.getCurrAliasId()));
+  }
+
+  /**
+   * Reduce Scan encountered 
+   * @param op the reduce sink operator encountered
+   * @param opProcCtx context
+   */
+  public void process(Operator<? extends Serializable> op, OperatorProcessorContext opProcCtx) throws SemanticException {
+    // should never be called
+    assert false;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java
new file mode 100644
index 0000000000..83539563dc
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java
@@ -0,0 +1,87 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer;
+
+import java.io.Serializable;
+import java.util.Map;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
+import org.apache.hadoop.hive.ql.exec.JoinOperator;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.exec.OperatorFactory;
+import org.apache.hadoop.hive.ql.plan.mapredWork;
+import org.apache.hadoop.hive.ql.plan.reduceSinkDesc;
+import org.apache.hadoop.hive.ql.metadata.*;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.parse.OperatorProcessor;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;
+
+/**
+ * Processor for the rule - reduce sink followed by reduce sink
+ */
+public class GenMRRedSink2 implements OperatorProcessor {
+
+  public GenMRRedSink2() {
+  }
+
+  /**
+   * Reduce Scan encountered 
+   * @param op the reduce sink operator encountered
+   * @param opProcCtx context
+   */
+  public void process(ReduceSinkOperator op, OperatorProcessorContext opProcCtx) throws SemanticException {
+    GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
+
+    Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = ctx.getMapCurrCtx();
+    GenMapRedCtx mapredCtx = mapCurrCtx.get(op.getParentOperators().get(0));
+    Task<? extends Serializable> currTask      = mapredCtx.getCurrTask();
+    Operator<? extends Serializable> currTopOp = mapredCtx.getCurrTopOp();
+    String currAliasId = mapredCtx.getCurrAliasId();
+    Operator<? extends Serializable> reducer = op.getChildOperators().get(0);
+    Map<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap = ctx.getOpTaskMap();
+    Task<? extends Serializable> opMapTask = opTaskMap.get(reducer);
+
+    ctx.setCurrTopOp(currTopOp);
+    ctx.setCurrAliasId(currAliasId);
+    ctx.setCurrTask(currTask);
+
+    if (opMapTask == null)
+      GenMapRedUtils.splitPlan(op, ctx);
+    else {
+      GenMapRedUtils.joinPlan(opMapTask, ctx);
+      currTask = opMapTask;
+    }
+
+    mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(), ctx.getCurrAliasId()));
+  }
+
+  /**
+   * Reduce Scan encountered 
+   * @param op the operator encountered
+   * @param opProcCtx context
+   */
+  public void process(Operator<? extends Serializable> op, OperatorProcessorContext opProcCtx) throws SemanticException {
+    // should never be called
+    assert false;
+  }
+
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
new file mode 100644
index 0000000000..f3888c5a97
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
@@ -0,0 +1,80 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer;
+
+import java.io.Serializable;
+import java.util.Map;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.parse.OperatorProcessor;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.plan.mapredWork;
+import org.apache.hadoop.hive.ql.metadata.*;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.parse.ParseContext;
+import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;
+
+/**
+ * Processor for the rule - table scan
+ */
+public class GenMRTableScan1 implements OperatorProcessor {
+  public GenMRTableScan1() {
+  }
+
+  /**
+   * Table Sink encountered 
+   * @param op the table sink operator encountered
+   * @param opProcCtx context
+   */
+  public void process(TableScanOperator op, OperatorProcessorContext opProcCtx) throws SemanticException {
+    GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
+    ParseContext parseCtx = ctx.getParseCtx();
+    Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = ctx.getMapCurrCtx();
+
+    // create a dummy task 
+    Task<? extends Serializable> currTask  = TaskFactory.get(GenMapRedUtils.getMapRedWork(), parseCtx.getConf());
+    Operator<? extends Serializable> currTopOp = op;
+    ctx.setCurrTask(currTask);
+    ctx.setCurrTopOp(currTopOp);
+
+    for (String alias : parseCtx.getTopOps().keySet()) {
+      Operator<? extends Serializable> currOp = parseCtx.getTopOps().get(alias);
+      if (currOp == op) {
+        String currAliasId = alias;
+        ctx.setCurrAliasId(currAliasId);
+        mapCurrCtx.put(op, new GenMapRedCtx(currTask, currTopOp, currAliasId));
+        return;
+      }
+    }
+    assert false;
+  }
+
+  /**
+   * Table Sink encountered 
+   * @param op the table sink operator encountered
+   * @param opProcCtx context
+   */
+  public void process(Operator<? extends Serializable> op, OperatorProcessorContext opProcCtx) throws SemanticException {
+    // should never be called
+    assert false;
+  }
+}
+
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
new file mode 100644
index 0000000000..2ac0a3666c
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -0,0 +1,343 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.Set;
+import java.util.Stack;
+import java.io.Serializable;
+import java.io.File;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.JoinOperator;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.exec.OperatorFactory;
+import org.apache.hadoop.hive.ql.plan.mapredWork;
+import org.apache.hadoop.hive.ql.plan.reduceSinkDesc;
+import org.apache.hadoop.hive.ql.plan.tableDesc;
+import org.apache.hadoop.hive.ql.plan.partitionDesc;
+import org.apache.hadoop.hive.ql.plan.fileSinkDesc;
+import org.apache.hadoop.hive.ql.plan.PlanUtils;
+import org.apache.hadoop.hive.ql.metadata.*;
+import org.apache.hadoop.hive.ql.parse.*;
+import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * General utility common functions for the Processor to convert operator into map-reduce tasks
+ */
+public class GenMapRedUtils {
+  private static Log LOG;
+
+  static {
+    LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils");
+  }
+
+  /**
+   * Initialize the current plan by adding it to root tasks
+   * @param op the reduce sink operator encountered
+   * @param opProcCtx processing context
+   */
+  public static void initPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx) throws SemanticException {
+    Operator<? extends Serializable> reducer = op.getChildOperators().get(0);
+    Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = opProcCtx.getMapCurrCtx();
+    GenMapRedCtx mapredCtx = mapCurrCtx.get(op.getParentOperators().get(0));
+    Task<? extends Serializable> currTask    = mapredCtx.getCurrTask();
+    mapredWork plan = (mapredWork) currTask.getWork();
+    HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap = opProcCtx.getOpTaskMap();
+    Operator<? extends Serializable> currTopOp = opProcCtx.getCurrTopOp();
+
+    opTaskMap.put(reducer, currTask);
+    plan.setReducer(reducer);
+    reduceSinkDesc desc = (reduceSinkDesc)op.getConf();
+    
+    // The number of reducers may be specified in the plan in some cases, or may need to be inferred
+    if (desc != null) {
+      if (desc.getNumReducers() != -1)
+        plan.setNumReduceTasks(new Integer(desc.getNumReducers()));
+      else if (desc.getInferNumReducers() == true)
+        plan.setInferNumReducers(true);
+    }
+
+    List<Task<? extends Serializable>> rootTasks = opProcCtx.getRootTasks();
+
+    rootTasks.add(currTask);
+    if (reducer.getClass() == JoinOperator.class)
+      plan.setNeedsTagging(true);
+
+    assert currTopOp != null;
+    List<Operator<? extends Serializable>> seenOps = opProcCtx.getSeenOps();
+    String currAliasId = opProcCtx.getCurrAliasId();
+
+    seenOps.add(currTopOp);
+    setTaskPlan(currAliasId, currTopOp, plan, false, opProcCtx);
+    
+    currTopOp = null;
+    currAliasId = null;
+
+    opProcCtx.setCurrTask(currTask);
+    opProcCtx.setCurrTopOp(currTopOp);
+    opProcCtx.setCurrAliasId(currAliasId);
+  }
+
+  /**
+   * Merge the current task with the task for the current reducer
+   * @param task for the old task for the current reducer
+   * @param opProcCtx processing context
+   */
+  public static void joinPlan(Task<? extends Serializable> task, GenMRProcContext opProcCtx) throws SemanticException {
+    Task<? extends Serializable> currTask = task;
+    mapredWork plan = (mapredWork) currTask.getWork();
+    Operator<? extends Serializable> currTopOp = opProcCtx.getCurrTopOp();
+
+    if (currTopOp != null) {
+      List<Operator<? extends Serializable>> seenOps = opProcCtx.getSeenOps();
+      String                                 currAliasId = opProcCtx.getCurrAliasId();
+      
+      if (!seenOps.contains(currTopOp)) {
+        seenOps.add(currTopOp);
+        setTaskPlan(currAliasId, currTopOp, plan, false, opProcCtx);
+      }
+      currTopOp = null;
+      opProcCtx.setCurrTopOp(currTopOp);
+    }
+  }
+
+  /**
+   * Split the current plan by creating a temporary destination
+   * @param op the reduce sink operator encountered
+   * @param opProcCtx processing context
+   */
+  public static void splitPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx) {
+    // Generate a new task              
+    mapredWork cplan = getMapRedWork();
+    ParseContext parseCtx = opProcCtx.getParseCtx();
+    Task<? extends Serializable> redTask = TaskFactory.get(cplan, parseCtx.getConf());
+    Operator<? extends Serializable> reducer = op.getChildOperators().get(0);
+
+    // Add the reducer
+    cplan.setReducer(reducer);
+    reduceSinkDesc desc = (reduceSinkDesc)op.getConf();
+    
+    if (desc.getNumReducers() != -1)
+      cplan.setNumReduceTasks(new Integer(desc.getNumReducers()));
+    else
+      cplan.setInferNumReducers(desc.getInferNumReducers());
+
+    HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap = opProcCtx.getOpTaskMap();
+    opTaskMap.put(reducer, redTask);
+    
+    // generate the temporary file
+    String scratchDir = opProcCtx.getScratchDir();
+    int randomid = opProcCtx.getRandomId();
+    int pathid   = opProcCtx.getPathId();
+    
+    String taskTmpDir = scratchDir + File.separator + randomid + '.' + pathid ;
+    pathid++;
+    opProcCtx.setPathId(pathid);
+    
+    Operator<? extends Serializable> parent = op.getParentOperators().get(0);
+    tableDesc tt_desc = 
+      PlanUtils.getBinaryTableDesc(PlanUtils.getFieldSchemasFromRowSchema(parent.getSchema(), "temporarycol")); 
+    
+    // Create a file sink operator for this file name
+    Operator<? extends Serializable> fs_op = 
+      putOpInsertMap(OperatorFactory.get(new fileSinkDesc(taskTmpDir, tt_desc),parent.getSchema()), null, parseCtx);
+    
+    // replace the reduce child with this operator
+    List<Operator<? extends Serializable>> childOpList = parent.getChildOperators();
+    for (int pos = 0; pos < childOpList.size(); pos++) {
+      if (childOpList.get(pos) == op) {
+        childOpList.set(pos, fs_op);
+        break;
+      }
+    }
+    
+    List<Operator<? extends Serializable>> parentOpList = new ArrayList<Operator<? extends Serializable>>();
+    parentOpList.add(parent);
+    fs_op.setParentOperators(parentOpList);
+    
+    // Add the path to alias mapping
+    if (cplan.getPathToAliases().get(taskTmpDir) == null) {
+      cplan.getPathToAliases().put(taskTmpDir, new ArrayList<String>());
+    }
+    
+    String streamDesc;
+    if (reducer.getClass() == JoinOperator.class)
+      streamDesc = "$INTNAME";
+    else
+      streamDesc = taskTmpDir;
+    
+    cplan.getPathToAliases().get(taskTmpDir).add(streamDesc);
+    cplan.getPathToPartitionInfo().put(taskTmpDir, new partitionDesc(tt_desc, null));
+    cplan.getAliasToWork().put(streamDesc, op);
+    setKeyAndValueDesc(cplan, op);
+    
+    // Make this task dependent on the current task
+    Task<? extends Serializable> currTask    = opProcCtx.getCurrTask();
+    currTask.addDependentTask(redTask);
+    
+    // TODO: Allocate work to remove the temporary files and make that
+    // dependent on the redTask
+    if (reducer.getClass() == JoinOperator.class)
+      cplan.setNeedsTagging(true);
+    
+    Operator<? extends Serializable> currTopOp = opProcCtx.getCurrTopOp();
+    String currAliasId = opProcCtx.getCurrAliasId();
+
+    currTopOp = null;
+    currAliasId = null;
+    currTask = redTask;
+
+    opProcCtx.setCurrTask(currTask);
+    opProcCtx.setCurrTopOp(currTopOp);
+    opProcCtx.setCurrAliasId(currAliasId);
+    opProcCtx.getRootOps().add(op);
+  }
+
+  /**
+   * set the current task in the mapredWork
+   * @param alias_id current alias
+   * @param topOp    the top operator of the stack
+   * @param plan     current plan
+   * @param local    whether you need to add to map-reduce or local work
+   * @param opProcCtx processing context
+   */
+  public static void setTaskPlan(String alias_id, Operator<? extends Serializable> topOp, mapredWork plan, boolean local, GenMRProcContext opProcCtx) 
+    throws SemanticException {
+    ParseContext parseCtx = opProcCtx.getParseCtx();
+
+    if (!local) {
+      // Generate the map work for this alias_id
+      PartitionPruner pruner = parseCtx.getAliasToPruner().get(alias_id);
+      Set<Partition> parts = null;
+      try {
+        // pass both confirmed and unknown partitions through the map-reduce framework
+        PartitionPruner.PrunedPartitionList partsList = pruner.prune();
+        parts = partsList.getConfirmedPartns();
+        parts.addAll(partsList.getUnknownPartns());
+      } catch (HiveException e) {
+        // Has to use full name to make sure it does not conflict with org.apache.commons.lang.StringUtils
+        LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
+        throw new SemanticException(e.getMessage(), e);
+      }
+      SamplePruner samplePruner = parseCtx.getAliasToSamplePruner().get(alias_id);
+      
+      for (Partition part : parts) {
+        // Later the properties have to come from the partition as opposed
+        // to from the table in order to support versioning.
+        Path paths[];
+        if (samplePruner != null) {
+          paths = samplePruner.prune(part);
+        }
+        else {
+          paths = part.getPath();
+        }
+        
+        for (Path p: paths) {
+          String path = p.toString();
+          LOG.debug("Adding " + path + " of table" + alias_id);
+          // Add the path to alias mapping
+          if (plan.getPathToAliases().get(path) == null) {
+            plan.getPathToAliases().put(path, new ArrayList<String>());
+          }
+          plan.getPathToAliases().get(path).add(alias_id);
+          plan.getPathToPartitionInfo().put(path, Utilities.getPartitionDesc(part));
+          LOG.debug("Information added for path " + path);
+        }
+      }
+      plan.getAliasToWork().put(alias_id, topOp);
+      setKeyAndValueDesc(plan, topOp);
+      LOG.debug("Created Map Work for " + alias_id);
+    }
+    else {
+      FileSinkOperator fOp = (FileSinkOperator) topOp;
+      fileSinkDesc fConf = (fileSinkDesc)fOp.getConf();
+      // populate local work if needed
+    }
+  }
+
+  /**
+   * set key and value descriptor
+   * @param plan     current plan
+   * @param topOp    current top operator in the path
+   */
+  private static void setKeyAndValueDesc(mapredWork plan, Operator<? extends Serializable> topOp) {
+    if (topOp instanceof ReduceSinkOperator) {
+      ReduceSinkOperator rs = (ReduceSinkOperator)topOp;
+      plan.setKeyDesc(rs.getConf().getKeySerializeInfo());
+      int tag = Math.max(0, rs.getConf().getTag());
+      List<tableDesc> tagToSchema = plan.getTagToValueDesc();
+      while (tag + 1 > tagToSchema.size()) {
+        tagToSchema.add(null);
+      }
+      tagToSchema.set(tag, rs.getConf().getValueSerializeInfo());
+    } else {
+      List<Operator<? extends Serializable>> children = topOp.getChildOperators(); 
+      if (children != null) {
+        for(Operator<? extends Serializable> op: children) {
+          setKeyAndValueDesc(plan, op);
+        }
+      }
+    }
+  }
+
+  /**
+   * create a new plan and return
+   * @return the new plan
+   */
+  public static mapredWork getMapRedWork() {
+    mapredWork work = new mapredWork();
+    work.setPathToAliases(new LinkedHashMap<String, ArrayList<String>>());
+    work.setPathToPartitionInfo(new LinkedHashMap<String, partitionDesc>());
+    work.setAliasToWork(new HashMap<String, Operator<? extends Serializable>>());
+    work.setTagToValueDesc(new ArrayList<tableDesc>());
+    work.setReducer(null);
+    return work;
+  }
+
+  /**
+   * insert in the map for the operator to row resolver
+   * @param op operator created
+   * @param rr row resolver
+   * @param parseCtx parse context
+   */
+  @SuppressWarnings("nls")
+  private static Operator<? extends Serializable> putOpInsertMap(Operator<? extends Serializable> op, RowResolver rr, ParseContext parseCtx) 
+  {
+    OpParseContext ctx = new OpParseContext(rr);
+    parseCtx.getOpParseCtx().put(op, ctx);
+    return op;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/OperatorProcessorContext.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/OperatorProcessorContext.java
new file mode 100644
index 0000000000..5ba4b82b3d
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/OperatorProcessorContext.java
@@ -0,0 +1,25 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer;
+
+/**
+ * Operator Processor Context
+ */
+public abstract class OperatorProcessorContext {
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DefaultDispatcher.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DefaultDispatcher.java
new file mode 100644
index 0000000000..8fd25db5ad
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DefaultDispatcher.java
@@ -0,0 +1,103 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.io.Serializable;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.util.List;
+import java.util.Stack;
+import java.lang.ClassNotFoundException;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.OperatorFactory;
+import org.apache.hadoop.hive.ql.exec.OperatorFactory.opTuple;
+
+/**
+ * Dispatches calls to relevant method in processor 
+ */
+public class DefaultDispatcher implements Dispatcher {
+  
+  private OperatorProcessor opProcessor;
+  
+  /**
+   * constructor
+   * @param opp operator processor that handles actual processing of the node
+   */
+  public DefaultDispatcher(OperatorProcessor opp) {
+    this.opProcessor = opp;
+  }
+
+  /**
+   * dispatcher function
+   * @param op operator to process
+   * @param opStack the operators encountered so far
+   * @throws SemanticException
+   */
+  public void dispatch(Operator<? extends Serializable> op, Stack<Operator<? extends Serializable>> opStack) 
+    throws SemanticException {
+
+    // If the processor has registered a process method for the particular operator, invoke it.
+    // Otherwise implement the generic function, which would definitely be implemented
+    for(opTuple opt : OperatorFactory.opvec) {
+      if(opt.opClass.isInstance(op)) {
+        Method pcall;
+        try {
+          pcall = opProcessor.getClass().getMethod("process", opt.opClass, 
+                                                   Class.forName("org.apache.hadoop.hive.ql.optimizer.OperatorProcessorContext"));
+          pcall.invoke(opProcessor, op, null);
+          return;
+        } catch (SecurityException e) {
+          assert false;
+        } catch (NoSuchMethodException e) {
+          assert false;
+        } catch (IllegalArgumentException e) {
+          assert false;
+        } catch (IllegalAccessException e) {
+          assert false;
+        } catch (InvocationTargetException e) {
+          throw new SemanticException(e.getTargetException());
+        } catch (ClassNotFoundException e) {
+          assert false;
+        }
+      }
+    }
+
+    try {
+      // no method found - invoke the generic function
+      Method pcall = opProcessor.getClass().getMethod("process", Class.forName("org.apache.hadoop.hive.ql.exec.Operator"), 
+                                                      Class.forName("org.apache.hadoop.hive.ql.optimizer.OperatorProcessorContext"));
+      
+      pcall.invoke(opProcessor, ((Operator<? extends Serializable>)op), null);
+      return;
+    } catch (SecurityException e) {
+      assert false;
+    } catch (NoSuchMethodException e) {
+      assert false;
+    } catch (IllegalArgumentException e) {
+      assert false;
+    } catch (IllegalAccessException e) {
+      assert false;
+    } catch (InvocationTargetException e) {
+      throw new SemanticException(e.getTargetException());
+    } catch (ClassNotFoundException e) {
+      assert false;
+    }
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DefaultOpGraphWalker.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DefaultOpGraphWalker.java
new file mode 100644
index 0000000000..1ed1b93081
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DefaultOpGraphWalker.java
@@ -0,0 +1,92 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.Stack;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+
+/**
+ * base class for operator graph walker
+ * this class takes list of starting ops and walks them one by one. it maintains list of walked
+ * operators (dispatchedList) and a list of operators that are discovered but not yet dispatched
+ */
+public abstract class DefaultOpGraphWalker implements OpGraphWalker {
+
+  List<Operator<? extends Serializable>> toWalk = new ArrayList<Operator<? extends Serializable>>();
+  Set<Operator<? extends Serializable>> dispatchedList = new HashSet<Operator<? extends Serializable>>();
+  Dispatcher dispatcher;
+
+  /**
+   * Constructor
+   * @param ctx graph of operators to walk
+   * @param disp dispatcher to call for each op encountered
+   */
+  public DefaultOpGraphWalker(Dispatcher disp) {
+    this.dispatcher = disp;
+  }
+
+  /**
+   * @return the toWalk
+   */
+  public List<Operator<? extends Serializable>> getToWalk() {
+    return toWalk;
+  }
+
+  /**
+   * @return the doneList
+   */
+  public Set<Operator<? extends Serializable>> getDispatchedList() {
+    return dispatchedList;
+  }
+
+  /**
+   * Dispatch the current operator
+   * @param op operator being walked
+   * @param opStack stack of operators encountered
+   * @throws SemanticException
+   */
+  public void dispatch(Operator<? extends Serializable> op, Stack opStack) throws SemanticException {
+    this.dispatcher.dispatch(op, opStack);
+    this.dispatchedList.add(op);
+  }
+
+  /**
+   * starting point for walking
+   * @throws SemanticException
+   */
+  public void startWalking(Collection<Operator<? extends Serializable>> startOps) throws SemanticException {
+    toWalk.addAll(startOps);
+    while(toWalk.size() > 0)
+      walk(toWalk.remove(0));
+  }
+
+  /**
+   * walk the current operator and its descendants
+   * @param op current operator in the graph
+   * @throws SemanticException
+   */
+  public abstract void walk(Operator<? extends Serializable> op) throws SemanticException;
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DefaultRuleDispatcher.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DefaultRuleDispatcher.java
new file mode 100644
index 0000000000..ac7f168a89
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DefaultRuleDispatcher.java
@@ -0,0 +1,124 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.io.Serializable;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.lang.ClassNotFoundException;
+import java.util.List;
+import java.util.Map;
+import java.util.Stack;
+import java.lang.ClassNotFoundException;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.optimizer.OperatorProcessorContext;
+import org.apache.hadoop.hive.ql.exec.OperatorFactory;
+import org.apache.hadoop.hive.ql.exec.OperatorFactory.opTuple;
+
+/**
+ * Dispatches calls to relevant method in processor. The user registers various rules with the dispatcher, and
+ * the processor corresponding to closest matching rule is fired.
+ */
+public class DefaultRuleDispatcher implements Dispatcher {
+  
+  private Map<Rule, OperatorProcessor>  opProcRules;
+  private OperatorProcessorContext      opProcCtx;
+  
+  /**
+   * constructor
+   * @param opp operator processor that handles actual processing of the node
+   * @param opProcCtx operator processor context, which is opaque to the dispatcher
+   */
+  public DefaultRuleDispatcher(Map<Rule, OperatorProcessor> opp, OperatorProcessorContext opProcCtx) {
+    this.opProcRules = opp;
+    this.opProcCtx   = opProcCtx;
+  }
+
+  /**
+   * dispatcher function
+   * @param op operator to process
+   * @param opStack the operators encountered so far
+   * @throws SemanticException
+   */
+  public void dispatch(Operator<? extends Serializable> op, Stack<Operator<? extends Serializable>> opStack) 
+    throws SemanticException {
+
+    // find the firing rule
+    // find the rule from the stack specified
+    Rule rule = null;
+    int minCost = Integer.MAX_VALUE;
+    for (Rule r : opProcRules.keySet()) {
+      int cost = r.cost(opStack);
+      if (cost <= minCost) {
+        minCost = cost;
+        rule = r;
+      }
+    }
+
+    assert rule != null;
+    OperatorProcessor proc = opProcRules.get(rule);
+
+    // If the processor has registered a process method for the particular operator, invoke it.
+    // Otherwise implement the generic function, which would definitely be implemented
+    for(opTuple opt : OperatorFactory.opvec) {
+      if(opt.opClass.isInstance(op)) {
+        Method pcall;
+        try {
+          pcall = proc.getClass().getMethod("process", opt.opClass,
+                                            Class.forName("org.apache.hadoop.hive.ql.optimizer.OperatorProcessorContext"));
+          pcall.invoke(proc, op, opProcCtx);
+          return;
+        } catch (SecurityException e) {
+          assert false;
+        } catch (NoSuchMethodException e) {
+          assert false;
+        } catch (IllegalArgumentException e) {
+          assert false;
+        } catch (IllegalAccessException e) {
+          assert false;
+        } catch (InvocationTargetException e) {
+          throw new SemanticException(e.getTargetException());
+        } catch (ClassNotFoundException e) {
+          assert false;
+        }
+      }
+    }
+
+    try {
+      // no method found - invoke the generic function
+      Method pcall = proc.getClass().getMethod("process", Class.forName("org.apache.hadoop.hive.ql.exec.Operator"), 
+                                               Class.forName("org.apache.hadoop.hive.ql.optimizer.OperatorProcessorContext"));
+      pcall.invoke(proc, ((Operator<? extends Serializable>)op), opProcCtx);
+
+    } catch (SecurityException e) {
+      assert false;
+    } catch (NoSuchMethodException e) {
+      assert false;
+    } catch (IllegalArgumentException e) {
+      assert false;
+    } catch (IllegalAccessException e) {
+      assert false;
+    } catch (InvocationTargetException e) {
+      throw new SemanticException(e.getTargetException());
+    } catch (ClassNotFoundException e) {
+      assert false;
+    }
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/Dispatcher.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/Dispatcher.java
new file mode 100644
index 0000000000..65a8185004
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/Dispatcher.java
@@ -0,0 +1,40 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.io.Serializable;
+import java.util.Stack;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+
+/**
+ * Dispatcher interface for Operators
+ * Used in operator graph walking to dispatch process/visitor functions for operators
+ */
+public interface Dispatcher {
+
+  /**
+   * dispatcher function
+   * @param op operator to process
+   * @param Stack operator stack to process
+   * @throws SemanticException
+   */
+  public abstract void dispatch(Operator<? extends Serializable> op, Stack<Operator<? extends Serializable>> stack) 
+    throws SemanticException;
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
index df09e0085d..a695ccae4e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
@@ -63,6 +63,8 @@ public enum ErrorMsg {
   INVALID_TBL_DDL_SERDE("Either list of columns or a custom serializer should be specified"),
   TARGET_TABLE_COLUMN_MISMATCH("Cannot insert into target table because column number/types are different"),
   TABLE_ALIAS_NOT_ALLOWED("Table Alias not Allowed in Sampling Clause"),
+  CLUSTERBY_DISTRIBUTEBY_CONFLICT("Cannot have both Cluster By and Distribute By Clauses"),
+  CLUSTERBY_SORTBY_CONFLICT("Cannot have both Cluster By and Sort By Clauses"),
   NON_BUCKETED_TABLE("Sampling Expression Needed for Non-Bucketed Table");
 
   private String mesg;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/GenMapRedWalker.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenMapRedWalker.java
new file mode 100644
index 0000000000..8c000d0e48
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenMapRedWalker.java
@@ -0,0 +1,72 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.io.Serializable;
+import java.util.List;
+import java.util.Stack;
+import java.util.regex.Pattern;
+import java.util.regex.Matcher;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
+import org.apache.hadoop.hive.ql.exec.*;
+
+/**
+ * Walks the operator tree in pre order fashion
+ */
+public class GenMapRedWalker extends DefaultOpGraphWalker {
+  private Stack<Operator<? extends Serializable>> opStack;
+
+  /**
+   * constructor of the walker - the dispatcher is passed
+   * @param disp the dispatcher to be called for each node visited
+   */
+  public GenMapRedWalker(Dispatcher disp) {
+    super(disp);
+    opStack = new Stack<Operator<? extends Serializable>>();
+  }
+  
+  /**
+   * Walk the given operator
+   * @param op operator being walked
+   */
+  @Override
+  public void walk(Operator<? extends Serializable> op) throws SemanticException {
+    List<Operator<? extends Serializable>> children = op.getChildOperators();
+    
+    // maintain the stack of operators encountered
+    opStack.push(op);
+    dispatch(op, opStack);
+
+    // kids of reduce sink operator need not be traversed again
+    if ((children == null) ||
+        ((op instanceof ReduceSinkOperator) && (getDispatchedList().containsAll(children)))) {
+      opStack.pop();
+      return;
+    }
+
+    // move all the children to the front of queue
+    for (Operator<? extends Serializable> ch : children)
+      walk(ch);
+
+    // done with this operator
+    opStack.pop();
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g b/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g
index 6e4d8b7f97..6b9c864541 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g
@@ -54,6 +54,8 @@ TOK_ALIASLIST;
 TOK_GROUPBY;
 TOK_ORDERBY;
 TOK_CLUSTERBY;
+TOK_DISTRIBUTEBY;
+TOK_SORTBY;
 TOK_UNION;
 TOK_JOIN;
 TOK_LEFTOUTERJOIN;
@@ -391,16 +393,24 @@ regular_body
    whereClause?
    groupByClause?
    orderByClause?
-   clusterByClause? 
-   limitClause? -> ^(TOK_QUERY fromClause ^(TOK_INSERT insertClause selectClause whereClause? groupByClause? orderByClause? clusterByClause? limitClause?))
+   clusterByClause?
+   distributeByClause?
+   sortByClause?
+   limitClause? -> ^(TOK_QUERY fromClause ^(TOK_INSERT insertClause
+                     selectClause whereClause? groupByClause? orderByClause? clusterByClause?
+                     distributeByClause? sortByClause? limitClause?))
    |
    selectClause
    fromClause
    whereClause?
    groupByClause?
    orderByClause?
-   clusterByClause? 
-   limitClause? -> ^(TOK_QUERY fromClause ^(TOK_INSERT ^(TOK_DESTINATION ^(TOK_DIR TOK_TMP_FILE)) selectClause whereClause? groupByClause? orderByClause? clusterByClause? limitClause?))
+   clusterByClause?
+   distributeByClause?
+   sortByClause?
+   limitClause? -> ^(TOK_QUERY fromClause ^(TOK_INSERT ^(TOK_DESTINATION ^(TOK_DIR TOK_TMP_FILE))
+                     selectClause whereClause? groupByClause? orderByClause? clusterByClause?
+                     distributeByClause? sortByClause? limitClause?))
    ;
 
 
@@ -410,20 +420,28 @@ body
    selectClause
    whereClause?
    groupByClause?
-   orderByClause? 
-   clusterByClause? 
-   limitClause? -> ^(TOK_INSERT insertClause? selectClause whereClause? groupByClause? orderByClause? clusterByClause? limitClause?)
+   orderByClause?
+   clusterByClause?
+   distributeByClause?
+   sortByClause?
+   limitClause? -> ^(TOK_INSERT insertClause?
+                     selectClause whereClause? groupByClause? orderByClause? clusterByClause?
+                     distributeByClause? sortByClause? limitClause?)
    |
    selectClause
    whereClause?
    groupByClause?
-   orderByClause? 
-   clusterByClause? 
-   limitClause? -> ^(TOK_INSERT ^(TOK_DESTINATION ^(TOK_DIR TOK_TMP_FILE)) selectClause whereClause? groupByClause? orderByClause? clusterByClause? limitClause?)
+   orderByClause?
+   clusterByClause?
+   distributeByClause?
+   sortByClause?
+   limitClause? -> ^(TOK_INSERT ^(TOK_DESTINATION ^(TOK_DIR TOK_TMP_FILE))
+                     selectClause whereClause? groupByClause? orderByClause? clusterByClause?
+                     distributeByClause? sortByClause? limitClause?)
    ;
-   
+
 insertClause
-   : 
+   :
    KW_INSERT KW_OVERWRITE destination -> ^(TOK_DESTINATION destination)
    ;
 
@@ -446,12 +464,17 @@ selectClause
     KW_SELECT (KW_ALL | dist=KW_DISTINCT)?
     selectList -> {$dist == null}? ^(TOK_SELECT selectList)
                ->                  ^(TOK_SELECTDI selectList)
+    |
+    KW_SELECT KW_TRANSFORM trfmClause -> ^(TOK_SELECT ^(TOK_SELEXPR trfmClause) )
+    |
+    KW_MAP trfmClause -> ^(TOK_SELECT ^(TOK_SELEXPR trfmClause) )
+    |
+    KW_REDUCE trfmClause -> ^(TOK_SELECT ^(TOK_SELEXPR trfmClause) )
     ;
 
 selectList
     :
     selectItem ( COMMA  selectItem )* -> selectItem+
-    | trfmClause -> ^(TOK_SELEXPR trfmClause)
     ;
 
 selectItem
@@ -461,10 +484,9 @@ selectItem
     
 trfmClause
     :
-    KW_TRANSFORM
-    LPAREN expressionList RPAREN
+    ( LPAREN expressionList RPAREN | expressionList )
     KW_USING StringLiteral
-    (KW_AS LPAREN aliasList RPAREN)?
+    (KW_AS (LPAREN aliasList RPAREN | aliasList) )?
     -> ^(TOK_TRANSFORM expressionList StringLiteral aliasList?)
     ;
     
@@ -585,18 +607,25 @@ orderByExpression
 
 clusterByClause
     :
-    KW_CLUSTER KW_BY 
-    Identifier 
+    KW_CLUSTER KW_BY
+    Identifier
     ( COMMA Identifier )* -> ^(TOK_CLUSTERBY Identifier+)
     ;
 
-clusterByExpression
-    :
-    expression
+distributeByClause:
+    KW_DISTRIBUTE KW_BY
+    Identifier
+    ( COMMA Identifier )* -> ^(TOK_DISTRIBUTEBY Identifier+)
+    ;
+
+sortByClause:
+    KW_SORT KW_BY
+    columnNameOrder
+    ( COMMA columnNameOrder)* -> ^(TOK_SORTBY columnNameOrder+)
     ;
 
 // fun(par1, par2, par3)
-function 
+function
     : // LEFT and RIGHT keywords are also function names
     Identifier
     LPAREN (
@@ -822,6 +851,8 @@ KW_LOCAL: 'LOCAL';
 KW_TRANSFORM : 'TRANSFORM';
 KW_USING: 'USING';
 KW_CLUSTER: 'CLUSTER';
+KW_DISTRIBUTE: 'DISTRIBUTE';
+KW_SORT: 'SORT';
 KW_UNION: 'UNION';
 KW_LOAD: 'LOAD';
 KW_DATA: 'DATA';
@@ -849,6 +880,7 @@ KW_TIMESTAMP: 'TIMESTAMP';
 KW_STRING: 'STRING';
 KW_ARRAY: 'ARRAY';
 KW_MAP: 'MAP';
+KW_REDUCE: 'REDUCE';
 KW_PARTITIONED: 'PARTITIONED';
 KW_CLUSTERED: 'CLUSTERED';
 KW_SORTED: 'SORTED';
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/OpGraphWalker.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/OpGraphWalker.java
new file mode 100644
index 0000000000..55c5cb9331
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/OpGraphWalker.java
@@ -0,0 +1,40 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.io.Serializable;
+import java.util.Collection;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+
+/**
+ * Interface for operator graph walker.
+ */
+public interface OpGraphWalker {
+
+  /**
+   * starting point for walking.
+   * 
+   * @param startOps list of starting operators
+   * @throws SemanticException
+   */
+  public abstract void startWalking(Collection<Operator<? extends Serializable>> startOps)
+      throws SemanticException;
+
+}
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/OpParseContext.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/OpParseContext.java
index ae8af5bad5..56c082b262 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/OpParseContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/OpParseContext.java
@@ -18,20 +18,16 @@
 
 package org.apache.hadoop.hive.ql.parse;
 
-import java.util.List;
 
 /**
  * Implementation of the Operator Parse Context. It maintains the parse context
  * that may be needed by an operator. Currently, it only maintains the row
- * resolver and the list of columns used by the operator
+ * resolver.
  **/
 
 public class OpParseContext {
   private RowResolver rr;  // row resolver for the operator
 
-  // list of internal column names used
-  private List<String> colNames;
-
   /**
    * @param rr row resolver
    */
@@ -52,18 +48,4 @@ public RowResolver getRR() {
   public void setRR(RowResolver rr) {
     this.rr = rr;
   }
-
-  /**
-   * @return the column names desired
-   */
-  public List<String> getColNames() {
-    return colNames;
-  }
-
-  /**
-   * @param colNames the column names to set
-   */
-  public void setColNames(List<String> colNames) {
-    this.colNames = colNames;
-  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/OperatorProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/OperatorProcessor.java
new file mode 100644
index 0000000000..2e1cdf2c26
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/OperatorProcessor.java
@@ -0,0 +1,39 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.parse;
+
+import java.io.Serializable;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.optimizer.OperatorProcessorContext;
+
+/**
+ * Base class for processing operators which is no-op. The specific processors can register their own context with
+ * the dispatcher.
+ */
+public interface OperatorProcessor {
+  
+  /**
+   * generic process for all ops that don't have specific implementations
+   * @param op operator to process
+   * @param opProcCtx operator processor context
+   * @throws SemanticException
+   */
+  public void process(Operator<? extends Serializable> op, OperatorProcessorContext opProcCtx) 
+    throws SemanticException;
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/PrintOpTreeProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/PrintOpTreeProcessor.java
new file mode 100644
index 0000000000..798034193a
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/PrintOpTreeProcessor.java
@@ -0,0 +1,82 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.io.PrintStream;
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.Stack;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.optimizer.OperatorProcessorContext;
+
+public class PrintOpTreeProcessor implements OperatorProcessor {
+  
+  private PrintStream out;
+  private HashMap<Operator<? extends Serializable>, Integer> opMap = new HashMap<Operator<? extends Serializable>, Integer>();
+  private Integer curNum = 0;
+
+  public PrintOpTreeProcessor() {
+    out = System.out;
+  }
+  
+  public PrintOpTreeProcessor(PrintStream o) {
+    out = o;
+  }
+  
+  private String getParents(Operator<? extends Serializable> op) {
+    StringBuilder ret = new StringBuilder("[");
+    boolean first = true;
+    if(op.getParentOperators() != null) {
+      for(Operator<? extends Serializable> parent :  op.getParentOperators()) {
+        if(!first)
+          ret.append(",");
+        ret.append(opMap.get(parent));
+        first = false;
+      }
+    }
+    ret.append("]");
+    return ret.toString();
+  }
+  
+  private String getChildren(Operator<? extends Serializable> op) {
+    StringBuilder ret = new StringBuilder("[");
+    boolean first = true;
+    if(op.getChildOperators() != null) {
+      for(Operator<? extends Serializable> child :  op.getChildOperators()) {
+        if(!first)
+          ret.append(",");
+        ret.append(opMap.get(child));
+        first = false;
+      }
+    }
+    ret.append("]");
+    return ret.toString();
+  }
+  
+  public void process(Operator<? extends Serializable> op, OperatorProcessorContext ctx) throws SemanticException {
+    if (opMap.get(op) == null) {
+      opMap.put(op, curNum++);
+    }
+    out.println("[" + opMap.get(op) + "] " + op.getClass().getName() + " =p=> " + getParents(op) + " =c=> " + getChildren(op));
+    if(op.getConf() == null) {
+      return;
+    }
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java
index e0a2ca01b9..6dd1f6da7c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java
@@ -40,7 +40,20 @@ public class QBParseInfo {
   private HashMap<String, CommonTree> destToSelExpr;
   private HashMap<String, CommonTree> destToWhereExpr;
   private HashMap<String, CommonTree> destToGroupby;
+  /**
+   * ClusterBy is a short name for both DistributeBy and SortBy.  
+   */
   private HashMap<String, CommonTree> destToClusterby;
+  /**
+   * DistributeBy controls the hashcode of the row, which determines which reducer
+   * the rows will go to. 
+   */
+  private HashMap<String, CommonTree> destToDistributeby;
+  /**
+   * SortBy controls the reduce keys, which affects the order of rows 
+   * that the reducer receives. 
+   */
+  private HashMap<String, CommonTree> destToSortby;
   private HashMap<String, Integer>    destToLimit;
   private int outerQueryLimit;
 
@@ -59,6 +72,8 @@ public QBParseInfo(String alias, boolean isSubQ) {
     this.destToWhereExpr = new HashMap<String, CommonTree>();
     this.destToGroupby = new HashMap<String, CommonTree>();
     this.destToClusterby = new HashMap<String, CommonTree>();
+    this.destToDistributeby = new HashMap<String, CommonTree>();
+    this.destToSortby = new HashMap<String, CommonTree>();
     this.destToLimit = new HashMap<String, Integer>();
     
     this.destToAggregationExprs = new HashMap<String, HashMap<String, CommonTree> >();
@@ -101,10 +116,33 @@ public void setDestForClause(String clause, CommonTree ast) {
     this.nameToDest.put(clause, ast);
   }
 
+  /**
+   * Set the Cluster By AST for the clause.  
+   * @param clause the name of the clause
+   * @param ast the abstract syntax tree
+   */
   public void setClusterByExprForClause(String clause, CommonTree ast) {
     this.destToClusterby.put(clause, ast);
   }
 
+  /**
+   * Set the Distribute By AST for the clause.  
+   * @param clause the name of the clause
+   * @param ast the abstract syntax tree
+   */
+  public void setDistributeByExprForClause(String clause, CommonTree ast) {
+    this.destToDistributeby.put(clause, ast);
+  }
+
+  /**
+   * Set the Sort By AST for the clause.  
+   * @param clause the name of the clause
+   * @param ast the abstract syntax tree
+   */
+  public void setSortByExprForClause(String clause, CommonTree ast) {
+    this.destToSortby.put(clause, ast);
+  }
+
   public void setSrcForAlias(String alias, CommonTree ast) {
     this.aliasToSrc.put(alias.toLowerCase(), ast);
   }
@@ -137,10 +175,33 @@ public CommonTree getSelForClause(String clause) {
     return this.destToSelExpr.get(clause);
   }
 
+  /**
+   * Get the Cluster By AST for the clause.  
+   * @param clause the name of the clause
+   * @return the abstract syntax tree
+   */
   public CommonTree getClusterByForClause(String clause) {
     return this.destToClusterby.get(clause);
   }
 
+  /**
+   * Get the Distribute By AST for the clause.  
+   * @param clause the name of the clause
+   * @return the abstract syntax tree
+   */
+  public CommonTree getDistributeByForClause(String clause) {
+    return this.destToDistributeby.get(clause);
+  }
+
+  /**
+   * Get the Sort By AST for the clause.  
+   * @param clause the name of the clause
+   * @return the abstract syntax tree
+   */
+  public CommonTree getSortByForClause(String clause) {
+    return this.destToSortby.get(clause);
+  }
+
   public CommonTree getSrcForAlias(String alias) {
     return this.aliasToSrc.get(alias.toLowerCase());
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/Rule.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/Rule.java
new file mode 100644
index 0000000000..50f1604ea4
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/Rule.java
@@ -0,0 +1,41 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.util.Stack;
+import java.io.Serializable;
+import org.apache.hadoop.hive.ql.exec.Operator;
+
+/**
+ * Rule interface for Operators
+ * Used in operator dispatching to dispatch process/visitor functions for operators
+ */
+public interface Rule {
+
+  /**
+   * @return the cost of the rule - the lower the cost, the better the rule matches
+   * @throws SemanticException
+   */
+  public int cost(Stack<Operator<? extends Serializable>> stack) throws SemanticException;
+
+  /**
+   * @return the name of the rule - may be useful for debugging
+   */
+  public String getName();
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/RuleRegExp.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/RuleRegExp.java
new file mode 100644
index 0000000000..cef886b575
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/RuleRegExp.java
@@ -0,0 +1,88 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.util.Stack;
+import java.util.Iterator;
+import java.util.regex.Pattern;
+import java.util.regex.Matcher;
+import java.io.Serializable;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+
+/**
+ * Rule interface for Operators
+ * Used in operator dispatching to dispatch process/visitor functions for operators
+ */
+public class RuleRegExp implements Rule {
+  
+  private String    ruleName;
+  private String    regExp;
+  private Pattern   pattern;
+
+  /**
+   * 
+   * @param stack the operators encountered so far
+   * @return operators names in the stack in the form of a string
+   **/
+  private String getOpNameString(Stack<Operator<? extends Serializable>> stack) {
+    String name = new String();
+    Iterator<Operator<? extends Serializable>> iter = stack.iterator();
+    while (iter.hasNext()) {
+      Operator<? extends Serializable> op = iter.next();
+      name = name.concat(op.getOperatorName());
+    }
+
+    return name;
+  }
+
+  /**
+   * The rule specified by the regular expression. Note that, the regular expression is specified in terms of operator
+   * name. For eg: TS.*RS -> means TableScan operator followed by anything any number of times followed by ReduceSink
+   * @param ruleName name of the rule
+   * @param regExp regular expression for the rule
+   **/
+  public RuleRegExp(String ruleName, String regExp) {
+    this.ruleName = ruleName;
+    this.regExp   = regExp;
+    pattern       = Pattern.compile(regExp);
+  }
+
+  /**
+   * This function returns the cost of the rule for the specified stack. Lower the cost, the better the rule is matched
+   * @param stacl operator stack encountered so far
+   * @return cost of the function
+   * @throws SemanticException
+   */
+  public int cost(Stack<Operator<? extends Serializable>> stack) throws SemanticException {
+    String opStackName = getOpNameString(stack);
+    Matcher m = pattern.matcher(opStackName);
+    if (m.matches()) 
+      return m.group().length();
+    
+    return Integer.MAX_VALUE;
+  }
+
+  /**
+   * @return the name of the operator
+   **/
+  public String getName() {
+    return ruleName;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 67aa7ea256..6f44924209 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -37,6 +37,13 @@
 import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
 import org.apache.hadoop.hive.ql.metadata.*;
 import org.apache.hadoop.hive.ql.optimizer.Optimizer;
+import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext;
+import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;
+import org.apache.hadoop.hive.ql.optimizer.GenMROperator;
+import org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1;
+import org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1;
+import org.apache.hadoop.hive.ql.optimizer.GenMRRedSink1;
+import org.apache.hadoop.hive.ql.optimizer.GenMRRedSink2;
 import org.apache.hadoop.hive.ql.plan.*;
 import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
@@ -366,6 +373,26 @@ else if (isJoinToken(frm))
       }
         break;
 
+      case HiveParser.TOK_DISTRIBUTEBY: {
+        // Get the distribute by  aliases - these are aliased to the entries in the
+        // select list
+        qbp.setDistributeByExprForClause(ctx_1.dest, ast);
+        if (qbp.getClusterByForClause(ctx_1.dest) != null) {
+          throw new SemanticException(ErrorMsg.CLUSTERBY_DISTRIBUTEBY_CONFLICT.getMsg(ast));
+        }
+      }
+        break;
+
+      case HiveParser.TOK_SORTBY: {
+        // Get the sort by aliases - these are aliased to the entries in the
+        // select list
+        qbp.setSortByExprForClause(ctx_1.dest, ast);
+        if (qbp.getClusterByForClause(ctx_1.dest) != null) {
+          throw new SemanticException(ErrorMsg.CLUSTERBY_SORTBY_CONFLICT.getMsg(ast));
+        }
+      }
+        break;
+
       case HiveParser.TOK_GROUPBY: {
         // Get the groupby aliases - these are aliased to the entries in the
         // select list
@@ -824,7 +851,8 @@ else if (rightCondAl1.size() != 0)
   }
 
   @SuppressWarnings("nls")
-  private Operator<? extends Serializable> putOpInsertMap(Operator<? extends Serializable> op, RowResolver rr) {
+  private Operator<? extends Serializable> putOpInsertMap(Operator<? extends Serializable> op, RowResolver rr) 
+  {
     OpParseContext ctx = new OpParseContext(rr);
     opParseCtx.put(op, ctx);
     return op;
@@ -2011,37 +2039,74 @@ private Operator genLimitMapRedPlan(String dest, QB qb, Operator input, int limi
   private Operator genReduceSinkPlan(String dest, QB qb,
                                      Operator input, int numReducers) throws SemanticException {
 
-    // First generate the expression for the key
-    // The cluster by clause has the aliases for the keys
-    ArrayList<exprNodeDesc> keyCols = new ArrayList<exprNodeDesc>();
     RowResolver inputRR = opParseCtx.get(input).getRR();
-    
-    CommonTree clby = qb.getParseInfo().getClusterByForClause(dest);
-    if (clby != null) {
-      int ccount = clby.getChildCount();
+
+    // First generate the expression for the partition and sort keys
+    // The cluster by clause / distribute by clause has the aliases for partition function 
+    CommonTree partitionExprs = qb.getParseInfo().getClusterByForClause(dest);
+    if (partitionExprs == null) {
+      partitionExprs = qb.getParseInfo().getDistributeByForClause(dest);
+    }
+    ArrayList<exprNodeDesc> partitionCols = new ArrayList<exprNodeDesc>();
+    if (partitionExprs != null) {
+      int ccount = partitionExprs.getChildCount();
       for(int i=0; i<ccount; ++i) {
-        CommonTree cl = (CommonTree)clby.getChild(i);
+        CommonTree cl = (CommonTree)partitionExprs.getChild(i);
         ColumnInfo colInfo = inputRR.get(qb.getParseInfo().getAlias(),
                                          unescapeIdentifier(cl.getText()));
         if (colInfo == null) {
           throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(cl));
         }
-        
-        keyCols.add(new exprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName()));
+        partitionCols.add(new exprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName()));
       }
     }
 
-    ArrayList<exprNodeDesc> valueCols = new ArrayList<exprNodeDesc>();
+    CommonTree sortExprs = qb.getParseInfo().getClusterByForClause(dest);
+    if (sortExprs == null) {
+      sortExprs = qb.getParseInfo().getSortByForClause(dest);
+    }
+
+    ArrayList<exprNodeDesc> sortCols = new ArrayList<exprNodeDesc>();
+    StringBuilder order = new StringBuilder();
+    if (sortExprs != null) {
+      int ccount = sortExprs.getChildCount();
+      for(int i=0; i<ccount; ++i) {
+        CommonTree cl = (CommonTree)sortExprs.getChild(i);
+        
+        if (cl.getType() == HiveParser.TOK_TABSORTCOLNAMEASC) {
+          // SortBy ASC
+          order.append("+");
+          cl = (CommonTree) cl.getChild(0);
+        } else if (cl.getType() == HiveParser.TOK_TABSORTCOLNAMEDESC) {
+          // SortBy DESC
+          order.append("-");
+          cl = (CommonTree) cl.getChild(0);
+        } else {
+          // ClusterBy
+          order.append("+");
+        }
+
+        ColumnInfo colInfo = inputRR.get(qb.getParseInfo().getAlias(),
+                                         unescapeIdentifier(cl.getText()));
+        if (colInfo == null) {
+          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(cl));
+        }
+        
+        sortCols.add(new exprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName()));
+      }
+    }
 
     // For the generation of the values expression just get the inputs
     // signature and generate field expressions for those
+    ArrayList<exprNodeDesc> valueCols = new ArrayList<exprNodeDesc>();
     for(ColumnInfo colInfo: inputRR.getColumnInfos()) {
       valueCols.add(new exprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName()));
     }
 
     Operator interim = putOpInsertMap(
       OperatorFactory.getAndMakeChild(
-        PlanUtils.getReduceSinkDesc(keyCols, valueCols, -1, keyCols.size(), numReducers, false),
+        PlanUtils.getReduceSinkDesc(sortCols, valueCols, -1, partitionCols, order.toString(),
+            numReducers, false),
         new RowSchema(inputRR.getColumnInfos()),
         input), inputRR);
 
@@ -2227,8 +2292,12 @@ private void genJoinOperatorTypeCheck(Operator left, Operator[] right) throws Se
     for (int i=0; i<right.length; i++) {
       Operator oi = (i==0 && right[i] == null ? left : right[i]);
       reduceSinkDesc now = ((ReduceSinkOperator)(oi)).getConf();
-      now.setKeySerializeInfo(PlanUtils.getBinarySortableTableDesc(
-          PlanUtils.getFieldSchemasFromColumnList(now.getKeyCols(), "joinkey")));
+      now.setKeySerializeInfo(
+          PlanUtils.getBinarySortableTableDesc(
+              PlanUtils.getFieldSchemasFromColumnList(now.getKeyCols(), "joinkey"),
+              now.getOrder()
+          )
+      );
     }
   }
   
@@ -2519,14 +2588,17 @@ private Operator genBodyPlan(QB qb, Operator input)
       curr = genSelectPlan(dest, qb, curr);
       Integer limit = qbp.getDestLimit(dest);
 
+      if (qbp.getClusterByForClause(dest) != null
+          || qbp.getDistributeByForClause(dest) != null
+          || qbp.getSortByForClause(dest) != null) {
+        curr = genReduceSinkPlan(dest, qb, curr, -1);
+      }
+
       if (qbp.getIsSubQ()) {
-        if (qbp.getClusterByForClause(dest) != null)
-          curr = genReduceSinkPlan(dest, qb, curr, -1);
-        if (limit != null) 
+        if (limit != null) {
           curr = genLimitMapRedPlan(dest, qb, curr, limit.intValue(), false);
-      }
-      else
-      {
+        }
+      } else {
         curr = genConversionOps(dest, qb, curr);
         // exact limit can be taken care of by the fetch operator
         if (limit != null) {
@@ -2693,7 +2765,7 @@ private Operator genTablePlan(String alias, QB qb) throws SemanticException {
       }
       
       // Create the root of the operator tree
-      top = putOpInsertMap(OperatorFactory.get(forwardDesc.class, new RowSchema(rwsch.getColumnInfos())), rwsch);
+      top = putOpInsertMap(OperatorFactory.get(tableScanDesc.class, new RowSchema(rwsch.getColumnInfos())), rwsch);
 
       // Add this to the list of top operators - we always start from a table scan
       this.topOps.put(alias_id, top);
@@ -2930,224 +3002,50 @@ private void genMapRedTasks(QB qb) throws SemanticException {
       mvTask = TaskFactory.get(mv, this.conf);
     }
 
-    // Maintain a map from the top level left most reducer in each of these
-    // trees
-    // to a task. This tells us whether we have to allocate another
-    // root level task or we can reuse an existing one
-    HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap = 
-        new HashMap<Operator<? extends Serializable>, Task<? extends Serializable>>();
-    for (String alias_id : this.topOps.keySet()) {
-      Operator<? extends Serializable> topOp = this.topOps.get(alias_id);
-      Operator<? extends Serializable> reduceSink = getReduceSink(topOp);
-      Operator<? extends Serializable> reducer = null;
-      if (reduceSink != null) 
-        reducer = reduceSink.getChildOperators().get(0);      
-      Task<? extends Serializable> rootTask = opTaskMap.get(reducer);
-      if (rootTask == null) {
-        rootTask = TaskFactory.get(getMapRedWork(), this.conf);
-        opTaskMap.put(reducer, rootTask);
-        ((mapredWork) rootTask.getWork()).setReducer(reducer);
-        reduceSinkDesc desc = (reduceSink == null) ? null : (reduceSinkDesc)reduceSink.getConf();
-
-        // The number of reducers may be specified in the plan in some cases, or may need to be inferred
-        if (desc != null) {
-          if (desc.getNumReducers() != -1)
-            ((mapredWork) rootTask.getWork()).setNumReduceTasks(new Integer(desc.getNumReducers()));
-          else if (desc.getInferNumReducers() == true)
-            ((mapredWork) rootTask.getWork()).setInferNumReducers(true);
-        }
-        this.rootTasks.add(rootTask);
-      }
-      genTaskPlan(topOp, rootTask, opTaskMap, mvTask);
-
-      // Generate the map work for this alias_id
-      PartitionPruner pruner = this.aliasToPruner.get(alias_id);
-      Set<Partition> parts = null;
-      try {
-        // pass both confirmed and unknown partitions through the map-reduce framework
-        PartitionPruner.PrunedPartitionList partsList = pruner.prune();
-        parts = partsList.getConfirmedPartns();
-        parts.addAll(partsList.getUnknownPartns());
-      } catch (HiveException e) {
-        // Has to use full name to make sure it does not conflict with org.apache.commons.lang.StringUtils
-        LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
-        throw new SemanticException(e.getMessage(), e);
-      }
-      SamplePruner samplePruner = this.aliasToSamplePruner.get(alias_id);
-      mapredWork plan = (mapredWork) rootTask.getWork();
-      for (Partition part : parts) {
-        // Later the properties have to come from the partition as opposed
-        // to from the table in order to support versioning.
-        Path paths[];
-        if (samplePruner != null) {
-          paths = samplePruner.prune(part);
-        }
-        else {
-          paths = part.getPath();
-        }
-        for (Path p: paths) {
-          String path = p.toString();
-          LOG.debug("Adding " + path + " of table" + alias_id);
-          // Add the path to alias mapping
-          if (plan.getPathToAliases().get(path) == null) {
-            plan.getPathToAliases().put(path, new ArrayList<String>());
-          }
-          plan.getPathToAliases().get(path).add(alias_id);
-          plan.getPathToPartitionInfo().put(path, Utilities.getPartitionDesc(part));
-          LOG.debug("Information added for path " + path);
-        }
-      }
-      plan.getAliasToWork().put(alias_id, topOp);
-      setKeyAndValueDesc(plan, topOp);
-      LOG.debug("Created Map Work for " + alias_id);
-    }
-  }
+    // generate map reduce plans
+    GenMRProcContext procCtx = 
+      new GenMRProcContext(
+        new HashMap<Operator<? extends Serializable>, Task<? extends Serializable>>(),
+        new ArrayList<Operator<? extends Serializable>>(),
+        getParseContext(), mvTask, this.rootTasks, this.scratchDir, this.randomid, this.pathid,
+        new HashMap<Operator<? extends Serializable>, GenMapRedCtx>());
+
+    // create a walker which walks the tree in a DFS manner while maintaining the operator stack. The dispatcher
+    // generates the plan from the operator tree
+    Map<Rule, OperatorProcessor> opRules = new LinkedHashMap<Rule, OperatorProcessor>();
+    opRules.put(new RuleRegExp(new String("R0"), ".*"), new GenMROperator());
+    opRules.put(new RuleRegExp(new String("R1"), "TS"), new GenMRTableScan1());
+    opRules.put(new RuleRegExp(new String("R2"), "TS.*RS"), new GenMRRedSink1());
+    opRules.put(new RuleRegExp(new String("R3"), "RS.*RS"), new GenMRRedSink2());
+    opRules.put(new RuleRegExp(new String("R4"), ".*FS"), new GenMRFileSink1());
+
+    // The dispatcher fires the processor corresponding to the closest matching rule and passes the context along
+    Dispatcher disp = new DefaultRuleDispatcher(opRules, procCtx);
+    
+    OpGraphWalker ogw = new GenMapRedWalker(disp);
+    ogw.startWalking(this.topOps.values());
 
-  private void setKeyAndValueDesc(mapredWork plan, Operator<? extends Serializable> topOp) {
-    if (topOp instanceof ReduceSinkOperator) {
-      ReduceSinkOperator rs = (ReduceSinkOperator)topOp;
-      plan.setKeyDesc(rs.getConf().getKeySerializeInfo());
-      int tag = Math.max(0, rs.getConf().getTag());
-      List<tableDesc> tagToSchema = plan.getTagToValueDesc();
-      while (tag + 1 > tagToSchema.size()) {
-        tagToSchema.add(null);
-      }
-      tagToSchema.set(tag, rs.getConf().getValueSerializeInfo());
-    } else {
-      List<Operator<? extends Serializable>> children = topOp.getChildOperators(); 
-      if (children != null) {
-        for(Operator<? extends Serializable> op: children) {
-          setKeyAndValueDesc(plan, op);
-        }
-      }
-    }
+    // reduce sink does not have any kids
+    breakOperatorTree(procCtx.getRootOps());
   }
-  @SuppressWarnings("nls")
-  private void genTaskPlan(Operator<? extends Serializable> op, Task<? extends Serializable> currTask,
-      HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> redTaskMap, 
-      Task<? extends Serializable> mvTask) {
-    // Check if this is a file sink operator
-    if ((op.getClass() == FileSinkOperator.class) && (mvTask != null)) { 
-      // If this is a file sink operator then set the move task to be dependent
-      // on the current task
-      currTask.addDependentTask(mvTask);
-    }
 
-    List<Operator<? extends Serializable>> childOps = op.getChildOperators();
-
-    // If there are no children then we are done
-    if (childOps == null) {
+  private void breakOperatorTree(Collection<Operator<? extends Serializable>> topOps) {
+    if (topOps == null) 
       return;
+    Iterator<Operator<? extends Serializable>> topOpsIter = topOps.iterator();
+    while (topOpsIter.hasNext()) {
+      Operator<? extends Serializable> topOp = topOpsIter.next();
+      breakOperatorTree(topOp);
     }
+  }
 
-    // Otherwise go through the children and check for the operator following
-    // the reduce sink operator
-    mapredWork plan = (mapredWork) currTask.getWork();
-    for (int i = 0; i < childOps.size(); ++i) {
-      Operator<? extends Serializable> child = childOps.get(i);
-      
-      if (child.getClass() == ReduceSinkOperator.class) {
-        // Get the operator following the reduce sink
-        assert (child.getChildOperators().size() == 1);
-
-        Operator<? extends Serializable> reducer = child.getChildOperators().get(0);
-        assert (plan.getReducer() != null);
-        if (plan.getReducer() == reducer) {
-          if (child.getChildOperators().get(0).getClass() == JoinOperator.class)
-            plan.setNeedsTagging(true);
-
-          // Recurse on the reducer
-          genTaskPlan(reducer, currTask, redTaskMap, mvTask);
-        }
-        else if (plan.getReducer() != reducer) {
-          Task<? extends Serializable> ctask = null;
-          mapredWork cplan = null;
-
-          // First check if the reducer already has an associated task
-          ctask = redTaskMap.get(reducer);
-          if (ctask == null) {
-            // For this case we need to generate a new task
-            cplan = getMapRedWork();
-            ctask = TaskFactory.get(cplan, this.conf);
-            // Add the reducer
-            cplan.setReducer(reducer);
-            if (((reduceSinkDesc)child.getConf()).getNumReducers() != -1)
-              cplan.setNumReduceTasks(new Integer(((reduceSinkDesc)child.getConf()).getNumReducers()));
-            else
-              cplan.setInferNumReducers(((reduceSinkDesc)child.getConf()).getInferNumReducers());
-            redTaskMap.put(reducer, ctask);
-
-            // Recurse on the reducer
-            genTaskPlan(reducer, ctask, redTaskMap, mvTask);
-
-            // generate the temporary file
-            String taskTmpDir = this.scratchDir + File.separator + this.randomid + '.' + this.pathid ;
-            this.pathid++;
-
-            // Go over the row schema of the input operator and generate the
-            // column names using that
-            StringBuilder sb = new StringBuilder();
-            boolean isfirst = true;
-            for(ColumnInfo colInfo: op.getSchema().getSignature()) {
-              if (!isfirst) {
-                sb.append(",");
-              }
-              sb.append(colInfo.getInternalName());
-              isfirst = false;
-            }
-
-            tableDesc tt_desc = PlanUtils.getBinaryTableDesc(
-                PlanUtils.getFieldSchemasFromRowSchema(op.getSchema(), "temporarycol")); 
-
-            // Create a file sink operator for this file name
-            Operator<? extends Serializable> fs_op = putOpInsertMap(OperatorFactory.get(new fileSinkDesc(taskTmpDir, tt_desc),
-                                                                                        op.getSchema()), null);
-
-            // replace the reduce child with this operator
-            childOps.set(i, fs_op);
-            
-            List<Operator<? extends Serializable>> parent = new ArrayList<Operator<? extends Serializable>>();
-            parent.add(op);
-            fs_op.setParentOperators(parent);
-
-            // Add the path to alias mapping
-            if (cplan.getPathToAliases().get(taskTmpDir) == null) {
-              cplan.getPathToAliases().put(taskTmpDir, new ArrayList<String>());
-            }
-
-            String streamDesc;
-            if (child.getChildOperators().get(0).getClass() == JoinOperator.class)
-              streamDesc = "$INTNAME";
-            else
-              streamDesc = taskTmpDir;
-
-
-            cplan.getPathToAliases().get(taskTmpDir).add(streamDesc);
-
-            cplan.getPathToPartitionInfo().put(taskTmpDir,
-                                               new partitionDesc(tt_desc, null));
-
-            cplan.getAliasToWork().put(streamDesc, child);
-            setKeyAndValueDesc(cplan, child);
-
-            // Make this task dependent on the current task
-            currTask.addDependentTask(ctask);
-
-            // TODO: Allocate work to remove the temporary files and make that
-            // dependent on the cTask
-            if (child.getChildOperators().get(0).getClass() == JoinOperator.class)
-              cplan.setNeedsTagging(true);
-          }
-        }
-        child.setChildOperators(null);
-
-      } else {
-        // For any other operator just recurse
-        genTaskPlan(child, currTask, redTaskMap, mvTask);
-      }
-    }
+  private void breakOperatorTree(Operator<? extends Serializable> topOp) {
+    breakOperatorTree(topOp.getChildOperators());
+    if (topOp instanceof ReduceSinkOperator)
+      topOp.setChildOperators(null);
   }
 
+
   @SuppressWarnings("nls")
   public Phase1Ctx initPhase1Ctx() {
 
@@ -3158,23 +3056,6 @@ public Phase1Ctx initPhase1Ctx() {
     return ctx_1;
   }
 
-  private mapredWork getMapRedWork() {
-
-    mapredWork work = new mapredWork();
-    work.setPathToAliases(new LinkedHashMap<String, ArrayList<String>>());
-    work.setPathToPartitionInfo(new LinkedHashMap<String, partitionDesc>());
-    work.setAliasToWork(new HashMap<String, Operator<? extends Serializable>>());
-    work.setTagToValueDesc(new ArrayList<tableDesc>());
-    work.setReducer(null);
-
-    return work;
-  }
-
-  private boolean pushSelect(Operator<? extends Serializable> op, List<String> colNames) {
-    if (opParseCtx.get(op).getRR().getColumnInfos().size() == colNames.size()) return false;
-    return true;
-  }
-
   @Override
   @SuppressWarnings("nls")
   public void analyzeInternal(CommonTree ast, Context ctx) throws SemanticException {
@@ -3203,7 +3084,7 @@ public void analyzeInternal(CommonTree ast, Context ctx) throws SemanticExceptio
     pCtx = optm.optimize();
     init(pCtx);
     qb = pCtx.getQB();
-
+    
     // Do any partition pruning
     genPartitionPruners(qb);
     LOG.info("Completed partition pruning");
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TopoWalker.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TopoWalker.java
new file mode 100644
index 0000000000..ed4c29fd48
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TopoWalker.java
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.parse;
+
+import java.io.Serializable;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+
+/**
+ * walks the tree in toplogically sorted order 
+ */
+public class TopoWalker extends DefaultOpGraphWalker {
+
+  public TopoWalker(Dispatcher disp) {
+    super(disp);
+  }
+  
+  @Override
+  public void walk(Operator<? extends Serializable> op) throws SemanticException {
+    // the dispatcher does not care about the stack - so dont maintin it
+    dispatch(op, null);
+    if(op.getChildOperators() != null) {
+      for(Operator<? extends Serializable> child : op.getChildOperators()) {
+        if(getDispatchedList().containsAll(child.getParentOperators())) {
+          walk(child);
+        }
+      }
+    }
+  }
+
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
index 89be464e4e..d87c13b6a1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
@@ -102,7 +102,7 @@ public static tableDesc getDefaultTableDesc(String separatorCode) {
   /** 
    * Generate the table descriptor of DynamicSerDe and TBinarySortableProtocol.
    */
-  public static tableDesc getBinarySortableTableDesc(List<FieldSchema> fieldSchemas) {
+  public static tableDesc getBinarySortableTableDesc(List<FieldSchema> fieldSchemas, String order) {
     String structName = "binary_sortable_table";
     return new tableDesc(
         DynamicSerDe.class,
@@ -110,10 +110,13 @@ public static tableDesc getBinarySortableTableDesc(List<FieldSchema> fieldSchema
         SequenceFileOutputFormat.class,
         Utilities.makeProperties(
             "name", structName,        
-            org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT, TBinarySortableProtocol.class.getName(),
+            org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT,
+              TBinarySortableProtocol.class.getName(),
             org.apache.hadoop.hive.serde.Constants.SERIALIZATION_DDL, 
-              MetaStoreUtils.getDDLFromFieldSchema(structName, fieldSchemas)
-        ));    
+              MetaStoreUtils.getDDLFromFieldSchema(structName, fieldSchemas),
+            org.apache.hadoop.hive.serde.Constants.SERIALIZATION_SORT_ORDER, 
+              order
+        ));
   }
 
   /** 
@@ -182,11 +185,12 @@ public static List<FieldSchema> getFieldSchemasFromColumnInfo(Vector<ColumnInfo>
   public static reduceSinkDesc getReduceSinkDesc(ArrayList<exprNodeDesc> keyCols, 
                                                  ArrayList<exprNodeDesc> valueCols, 
                                                  int tag, 
-                                                 ArrayList<exprNodeDesc> partitionCols, 
+                                                 ArrayList<exprNodeDesc> partitionCols,
+                                                 String order,
                                                  int numReducers, boolean inferNumReducers) {
     
     return new reduceSinkDesc(keyCols, valueCols, tag, partitionCols, numReducers, inferNumReducers,
-        getBinarySortableTableDesc(getFieldSchemasFromColumnList(keyCols, "reducesinkkey")),
+        getBinarySortableTableDesc(getFieldSchemasFromColumnList(keyCols, "reducesinkkey"), order),
         getBinaryTableDesc(getFieldSchemasFromColumnList(valueCols, "reducesinkvalue")));
   }
 
@@ -221,7 +225,12 @@ public static reduceSinkDesc getReduceSinkDesc(ArrayList<exprNodeDesc> keyCols,
       partitionCols.add(SemanticAnalyzer.getFuncExprNodeDesc("rand"));
     }
     
-    return getReduceSinkDesc(keyCols, valueCols, tag, partitionCols, numReducers, inferNumReducers);
+    StringBuilder order = new StringBuilder();
+    for (int i=0; i<keyCols.size(); i++) {
+      order.append("+");
+    }
+    return getReduceSinkDesc(keyCols, valueCols, tag, partitionCols, order.toString(),
+        numReducers, inferNumReducers);
   }
   
   
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/groupByDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/groupByDesc.java
index 9793c48026..983d000f3a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/groupByDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/groupByDesc.java
@@ -70,7 +70,7 @@ public void setKeys(final java.util.ArrayList<exprNodeDesc> keys) {
     this.keys = keys;
   }
   
-  @explain(displayName="")
+  @explain(displayName="aggregations")
   public java.util.ArrayList<org.apache.hadoop.hive.ql.plan.aggregationDesc> getAggregators() {
     return this.aggregators;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/reduceSinkDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/reduceSinkDesc.java
index 40f8fbfd2e..34786df8a7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/reduceSinkDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/reduceSinkDesc.java
@@ -139,5 +139,17 @@ public tableDesc getValueSerializeInfo() {
   public void setValueSerializeInfo(tableDesc valueSerializeInfo) {
     this.valueSerializeInfo = valueSerializeInfo;
   }
+
+  /**
+   * Returns the sort order of the key columns.
+   * @return null, which means ascending order for all key columns,
+   *   or a String of the same length as key columns, that consists of only 
+   *   "+" (ascending order) and "-" (descending order). 
+   */
+  @explain(displayName="sort order")
+  public String getOrder() {
+    return keySerializeInfo.getProperties().getProperty(
+        org.apache.hadoop.hive.serde.Constants.SERIALIZATION_SORT_ORDER);
+  }
   
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/tableScanDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/tableScanDesc.java
new file mode 100644
index 0000000000..2a187c125b
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/tableScanDesc.java
@@ -0,0 +1,35 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.plan;
+
+import java.io.Serializable;
+
+/**
+ * Table Scan Descriptor
+ * Currently, data is only read from a base source as part of map-reduce framework. So, nothing is stored in the 
+ * descriptor. But, more things will be added here as table scan is invoked as part of local work.
+ **/
+@explain(displayName="TableScan")
+public class tableScanDesc implements Serializable {
+  private static final long serialVersionUID = 1L;
+  @SuppressWarnings("nls")
+  public tableScanDesc() {
+    throw new RuntimeException("This class does not need to be instantiated"); 
+  }
+}
diff --git a/ql/src/test/queries/clientpositive/union2.q b/ql/src/test/queries/clientpositive/union2.q
new file mode 100644
index 0000000000..857862fdaf
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/union2.q
@@ -0,0 +1,6 @@
+explain 
+  select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL  
+                        select s2.key as key, s2.value as value from src s2) unionsrc;
+
+select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL  
+                      select s2.key as key, s2.value as value from src s2) unionsrc;
diff --git a/ql/src/test/results/clientpositive/groupby1.q.out b/ql/src/test/results/clientpositive/groupby1.q.out
index 27d09f148a..bce833a102 100644
--- a/ql/src/test/results/clientpositive/groupby1.q.out
+++ b/ql/src/test/results/clientpositive/groupby1.q.out
@@ -15,6 +15,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: rand()
                     type: double
@@ -24,8 +25,8 @@ STAGE PLANS:
                     type: string
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(UDFToDouble(VALUE.0))
+          aggregations:
+                expr: sum(UDFToDouble(VALUE.0))
           keys:
                 expr: KEY.0
                 type: string
@@ -39,11 +40,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/250776234/130709293.10001 
+        /tmp/hive-njain/7857726/161687564.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -53,8 +55,8 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(VALUE.0)
+          aggregations:
+                expr: sum(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
diff --git a/ql/src/test/results/clientpositive/groupby1_limit.q.out b/ql/src/test/results/clientpositive/groupby1_limit.q.out
index 96f14ad643..8c007bdb1a 100644
--- a/ql/src/test/results/clientpositive/groupby1_limit.q.out
+++ b/ql/src/test/results/clientpositive/groupby1_limit.q.out
@@ -16,6 +16,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: rand()
                     type: double
@@ -25,8 +26,8 @@ STAGE PLANS:
                     type: string
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(UDFToDouble(VALUE.0))
+          aggregations:
+                expr: sum(UDFToDouble(VALUE.0))
           keys:
                 expr: KEY.0
                 type: string
@@ -40,11 +41,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-athusoo/28951539/40025562.10002 
+        /tmp/hive-njain/324923626/1284785232.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -54,8 +56,8 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(VALUE.0)
+          aggregations:
+                expr: sum(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
@@ -76,8 +78,9 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-athusoo/28951539/40025562.10001 
+        /tmp/hive-njain/324923626/1284785232.10002 
           Reduce Output Operator
+            sort order: 
             tag: -1
             value expressions:
                   expr: 0
diff --git a/ql/src/test/results/clientpositive/groupby1_map.q.out b/ql/src/test/results/clientpositive/groupby1_map.q.out
index d90d2d982d..474665da75 100644
--- a/ql/src/test/results/clientpositive/groupby1_map.q.out
+++ b/ql/src/test/results/clientpositive/groupby1_map.q.out
@@ -12,8 +12,8 @@ STAGE PLANS:
       Alias -> Map Operator Tree:
         src 
             Group By Operator
-            
-                  expr: sum(UDFToDouble(substr(value, 4)))
+              aggregations:
+                    expr: sum(UDFToDouble(substr(value, 4)))
               keys:
                     expr: key
                     type: string
@@ -22,6 +22,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: rand()
                       type: double
@@ -31,8 +32,8 @@ STAGE PLANS:
                       type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(VALUE.0)
+          aggregations:
+                expr: sum(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
@@ -46,11 +47,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/126291708/5299613.10001 
+        /tmp/hive-njain/871552050/153342031.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -60,8 +62,8 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(VALUE.0)
+          aggregations:
+                expr: sum(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
diff --git a/ql/src/test/results/clientpositive/groupby2.q.out b/ql/src/test/results/clientpositive/groupby2.q.out
index 462a5d7cce..27292d97dc 100644
--- a/ql/src/test/results/clientpositive/groupby2.q.out
+++ b/ql/src/test/results/clientpositive/groupby2.q.out
@@ -17,6 +17,7 @@ STAGE PLANS:
                     type: string
                     expr: substr(value, 4)
                     type: string
+              sort order: ++
               Map-reduce partition columns:
                     expr: substr(key, 0, 1)
                     type: string
@@ -25,9 +26,9 @@ STAGE PLANS:
               tag: -1
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(DISTINCT KEY.1)
-              expr: sum(UDFToDouble(KEY.1))
+          aggregations:
+                expr: count(DISTINCT KEY.1)
+                expr: sum(UDFToDouble(KEY.1))
           keys:
                 expr: KEY.0
                 type: string
@@ -41,11 +42,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/282197599/271187968.10001 
+        /tmp/hive-njain/5285021/59406042.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -57,9 +59,9 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(VALUE.0)
-              expr: sum(VALUE.1)
+          aggregations:
+                expr: count(VALUE.0)
+                expr: sum(VALUE.1)
           keys:
                 expr: KEY.0
                 type: string
diff --git a/ql/src/test/results/clientpositive/groupby2_limit.q.out b/ql/src/test/results/clientpositive/groupby2_limit.q.out
index 191877c374..653bfbb767 100644
--- a/ql/src/test/results/clientpositive/groupby2_limit.q.out
+++ b/ql/src/test/results/clientpositive/groupby2_limit.q.out
@@ -15,6 +15,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: rand()
                     type: double
@@ -24,8 +25,8 @@ STAGE PLANS:
                     type: string
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(UDFToDouble(VALUE.0))
+          aggregations:
+                expr: sum(UDFToDouble(VALUE.0))
           keys:
                 expr: KEY.0
                 type: string
@@ -39,11 +40,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/4007501/112626006.10002 
+        /tmp/hive-njain/635379378/60676453.10002 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -53,8 +55,8 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(VALUE.0)
+          aggregations:
+                expr: sum(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
diff --git a/ql/src/test/results/clientpositive/groupby2_map.q.out b/ql/src/test/results/clientpositive/groupby2_map.q.out
index 8d952defd7..af8694159a 100644
--- a/ql/src/test/results/clientpositive/groupby2_map.q.out
+++ b/ql/src/test/results/clientpositive/groupby2_map.q.out
@@ -12,9 +12,9 @@ STAGE PLANS:
       Alias -> Map Operator Tree:
         src 
             Group By Operator
-            
-                  expr: count(DISTINCT substr(value, 4))
-                  expr: sum(UDFToDouble(substr(value, 4)))
+              aggregations:
+                    expr: count(DISTINCT substr(value, 4))
+                    expr: sum(UDFToDouble(substr(value, 4)))
               keys:
                     expr: substr(key, 0, 1)
                     type: string
@@ -27,6 +27,7 @@ STAGE PLANS:
                       type: string
                       expr: 1
                       type: string
+                sort order: ++
                 Map-reduce partition columns:
                       expr: 0
                       type: string
@@ -40,9 +41,9 @@ STAGE PLANS:
                       type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(DISTINCT KEY.1)
-              expr: sum(VALUE.1)
+          aggregations:
+                expr: count(DISTINCT KEY.1)
+                expr: sum(VALUE.1)
           keys:
                 expr: KEY.0
                 type: string
@@ -56,11 +57,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/781819455/901894899.10001 
+        /tmp/hive-njain/951471939/220829541.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -72,9 +74,9 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(VALUE.0)
-              expr: sum(VALUE.1)
+          aggregations:
+                expr: count(VALUE.0)
+                expr: sum(VALUE.1)
           keys:
                 expr: KEY.0
                 type: string
diff --git a/ql/src/test/results/clientpositive/groupby3.q.out b/ql/src/test/results/clientpositive/groupby3.q.out
index 389f828b42..14d94d1cb0 100644
--- a/ql/src/test/results/clientpositive/groupby3.q.out
+++ b/ql/src/test/results/clientpositive/groupby3.q.out
@@ -19,18 +19,19 @@ STAGE PLANS:
                 key expressions:
                       expr: substr(0, 4)
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: substr(0, 4)
                       type: string
                 tag: -1
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: avg(DISTINCT UDFToDouble(KEY.0))
-              expr: sum(UDFToDouble(KEY.0))
-              expr: avg(UDFToDouble(KEY.0))
-              expr: min(UDFToDouble(KEY.0))
-              expr: max(UDFToDouble(KEY.0))
+          aggregations:
+                expr: avg(DISTINCT UDFToDouble(KEY.0))
+                expr: sum(UDFToDouble(KEY.0))
+                expr: avg(UDFToDouble(KEY.0))
+                expr: min(UDFToDouble(KEY.0))
+                expr: max(UDFToDouble(KEY.0))
           mode: partial1
           File Output Operator
             table:
@@ -41,8 +42,9 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/67781830/202058716.10001 
+        /tmp/hive-njain/1428098224/715983841.10001 
           Reduce Output Operator
+            sort order: 
             tag: -1
             value expressions:
                   expr: 0
@@ -57,12 +59,12 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: avg(VALUE.0)
-              expr: sum(VALUE.1)
-              expr: avg(VALUE.2)
-              expr: min(VALUE.3)
-              expr: max(VALUE.4)
+          aggregations:
+                expr: avg(VALUE.0)
+                expr: sum(VALUE.1)
+                expr: avg(VALUE.2)
+                expr: min(VALUE.3)
+                expr: max(VALUE.4)
           mode: unknown
           Select Operator
             expressions:
diff --git a/ql/src/test/results/clientpositive/groupby3_map.q.out b/ql/src/test/results/clientpositive/groupby3_map.q.out
index 01a04ffc12..2bbcd64556 100644
--- a/ql/src/test/results/clientpositive/groupby3_map.q.out
+++ b/ql/src/test/results/clientpositive/groupby3_map.q.out
@@ -16,12 +16,12 @@ STAGE PLANS:
                     expr: value
                     type: string
               Group By Operator
-              
-                    expr: avg(DISTINCT UDFToDouble(substr(0, 4)))
-                    expr: sum(UDFToDouble(substr(0, 4)))
-                    expr: avg(UDFToDouble(substr(0, 4)))
-                    expr: min(UDFToDouble(substr(0, 4)))
-                    expr: max(UDFToDouble(substr(0, 4)))
+                aggregations:
+                      expr: avg(DISTINCT UDFToDouble(substr(0, 4)))
+                      expr: sum(UDFToDouble(substr(0, 4)))
+                      expr: avg(UDFToDouble(substr(0, 4)))
+                      expr: min(UDFToDouble(substr(0, 4)))
+                      expr: max(UDFToDouble(substr(0, 4)))
                 keys:
                       expr: substr(0, 4)
                       type: string
@@ -30,6 +30,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -47,12 +48,12 @@ STAGE PLANS:
                         type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: avg(DISTINCT UDFToDouble(KEY.0))
-              expr: sum(VALUE.1)
-              expr: avg(VALUE.2)
-              expr: min(VALUE.3)
-              expr: max(VALUE.4)
+          aggregations:
+                expr: avg(DISTINCT UDFToDouble(KEY.0))
+                expr: sum(VALUE.1)
+                expr: avg(VALUE.2)
+                expr: min(VALUE.3)
+                expr: max(VALUE.4)
           mode: partial2
           File Output Operator
             table:
@@ -63,8 +64,9 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/130240402/83496104.10001 
+        /tmp/hive-njain/82630066/2591068.10001 
           Reduce Output Operator
+            sort order: 
             tag: -1
             value expressions:
                   expr: 0
@@ -79,12 +81,12 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: avg(VALUE.0)
-              expr: sum(VALUE.1)
-              expr: avg(VALUE.2)
-              expr: min(VALUE.3)
-              expr: max(VALUE.4)
+          aggregations:
+                expr: avg(VALUE.0)
+                expr: sum(VALUE.1)
+                expr: avg(VALUE.2)
+                expr: min(VALUE.3)
+                expr: max(VALUE.4)
           mode: unknown
           Select Operator
             expressions:
diff --git a/ql/src/test/results/clientpositive/groupby4.q.out b/ql/src/test/results/clientpositive/groupby4.q.out
index e21b35162e..484154d8d1 100644
--- a/ql/src/test/results/clientpositive/groupby4.q.out
+++ b/ql/src/test/results/clientpositive/groupby4.q.out
@@ -19,6 +19,7 @@ STAGE PLANS:
                 key expressions:
                       expr: substr(0, 0, 1)
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: rand()
                       type: double
@@ -38,11 +39,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/213545057/773995409.10001 
+        /tmp/hive-njain/142162733/220682119.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
diff --git a/ql/src/test/results/clientpositive/groupby4_map.q.out b/ql/src/test/results/clientpositive/groupby4_map.q.out
index 2c702abb26..7927f4864a 100644
--- a/ql/src/test/results/clientpositive/groupby4_map.q.out
+++ b/ql/src/test/results/clientpositive/groupby4_map.q.out
@@ -12,10 +12,11 @@ STAGE PLANS:
         src 
             Select Operator
               Group By Operator
-              
-                    expr: count(1)
+                aggregations:
+                      expr: count(1)
                 mode: hash
                 Reduce Output Operator
+                  sort order: 
                   Map-reduce partition columns:
                         expr: rand()
                         type: double
@@ -25,8 +26,8 @@ STAGE PLANS:
                         type: bigint
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(VALUE.0)
+          aggregations:
+                expr: count(VALUE.0)
           mode: unknown
           Select Operator
             expressions:
diff --git a/ql/src/test/results/clientpositive/groupby5.q.out b/ql/src/test/results/clientpositive/groupby5.q.out
index 660e99584c..6797ae972c 100644
--- a/ql/src/test/results/clientpositive/groupby5.q.out
+++ b/ql/src/test/results/clientpositive/groupby5.q.out
@@ -15,6 +15,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: rand()
                     type: double
@@ -24,8 +25,8 @@ STAGE PLANS:
                     type: string
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(UDFToDouble(VALUE.0))
+          aggregations:
+                expr: sum(UDFToDouble(VALUE.0))
           keys:
                 expr: KEY.0
                 type: string
@@ -39,11 +40,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/485297652/61546074.10001 
+        /tmp/hive-njain/450793281/288129670.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -53,8 +55,8 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(VALUE.0)
+          aggregations:
+                expr: sum(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
diff --git a/ql/src/test/results/clientpositive/groupby5_map.q.out b/ql/src/test/results/clientpositive/groupby5_map.q.out
index 3b14055253..ab486ccf0a 100644
--- a/ql/src/test/results/clientpositive/groupby5_map.q.out
+++ b/ql/src/test/results/clientpositive/groupby5_map.q.out
@@ -15,10 +15,11 @@ STAGE PLANS:
                     expr: key
                     type: string
               Group By Operator
-              
-                    expr: sum(UDFToDouble(0))
+                aggregations:
+                      expr: sum(UDFToDouble(0))
                 mode: hash
                 Reduce Output Operator
+                  sort order: 
                   Map-reduce partition columns:
                         expr: rand()
                         type: double
@@ -28,8 +29,8 @@ STAGE PLANS:
                         type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: sum(VALUE.0)
+          aggregations:
+                expr: sum(VALUE.0)
           mode: unknown
           Select Operator
             expressions:
diff --git a/ql/src/test/results/clientpositive/groupby6.q.out b/ql/src/test/results/clientpositive/groupby6.q.out
index 64098e7c38..df8f707b33 100644
--- a/ql/src/test/results/clientpositive/groupby6.q.out
+++ b/ql/src/test/results/clientpositive/groupby6.q.out
@@ -19,6 +19,7 @@ STAGE PLANS:
                 key expressions:
                       expr: substr(0, 4, 1)
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: rand()
                       type: double
@@ -38,11 +39,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/911936039/66288606.10001 
+        /tmp/hive-njain/53979816/107916670.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
diff --git a/ql/src/test/results/clientpositive/input11_limit.q.out b/ql/src/test/results/clientpositive/input11_limit.q.out
index 42c2ca42ca..e5675558e9 100644
--- a/ql/src/test/results/clientpositive/input11_limit.q.out
+++ b/ql/src/test/results/clientpositive/input11_limit.q.out
@@ -22,6 +22,7 @@ STAGE PLANS:
                       type: string
                 Limit
                   Reduce Output Operator
+                    sort order: 
                     tag: -1
                     value expressions:
                           expr: 0
diff --git a/ql/src/test/results/clientpositive/input14.q.out b/ql/src/test/results/clientpositive/input14.q.out
index 8ad19d19dc..540dcbd55d 100644
--- a/ql/src/test/results/clientpositive/input14.q.out
+++ b/ql/src/test/results/clientpositive/input14.q.out
@@ -25,6 +25,7 @@ STAGE PLANS:
                   key expressions:
                         expr: tkey
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: tkey
                         type: string
diff --git a/ql/src/test/results/clientpositive/input14_limit.q.out b/ql/src/test/results/clientpositive/input14_limit.q.out
index 7a000ec4a5..ee3e4165ca 100644
--- a/ql/src/test/results/clientpositive/input14_limit.q.out
+++ b/ql/src/test/results/clientpositive/input14_limit.q.out
@@ -26,6 +26,7 @@ STAGE PLANS:
                   key expressions:
                         expr: tkey
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: tkey
                         type: string
@@ -47,11 +48,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/195632265/461794176.10001 
+        /tmp/hive-njain/458204227/247333025.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
diff --git a/ql/src/test/results/clientpositive/input17.q.out b/ql/src/test/results/clientpositive/input17.q.out
index edede9537f..f3177f37f1 100644
--- a/ql/src/test/results/clientpositive/input17.q.out
+++ b/ql/src/test/results/clientpositive/input17.q.out
@@ -33,6 +33,7 @@ STAGE PLANS:
                     key expressions:
                           expr: tkey
                           type: string
+                    sort order: +
                     Map-reduce partition columns:
                           expr: tkey
                           type: string
diff --git a/ql/src/test/results/clientpositive/input18.q.out b/ql/src/test/results/clientpositive/input18.q.out
index 3857de6309..fb6415c06e 100644
--- a/ql/src/test/results/clientpositive/input18.q.out
+++ b/ql/src/test/results/clientpositive/input18.q.out
@@ -29,6 +29,7 @@ STAGE PLANS:
                   key expressions:
                         expr: key
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: key
                         type: string
diff --git a/ql/src/test/results/clientpositive/input1_limit.q.out b/ql/src/test/results/clientpositive/input1_limit.q.out
index 6b083c12fd..ea3bdbe779 100644
--- a/ql/src/test/results/clientpositive/input1_limit.q.out
+++ b/ql/src/test/results/clientpositive/input1_limit.q.out
@@ -24,6 +24,7 @@ STAGE PLANS:
                       type: string
                 Limit
                   Reduce Output Operator
+                    sort order: 
                     tag: -1
                     value expressions:
                           expr: 0
@@ -76,8 +77,9 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-athusoo/8552631/401018009.10002 
+        /tmp/hive-njain/842103907/482854493.10002 
           Reduce Output Operator
+            sort order: 
             tag: -1
             value expressions:
                   expr: 0
diff --git a/ql/src/test/results/clientpositive/input3_limit.q.out b/ql/src/test/results/clientpositive/input3_limit.q.out
index 3136f856da..7a56db18bf 100644
--- a/ql/src/test/results/clientpositive/input3_limit.q.out
+++ b/ql/src/test/results/clientpositive/input3_limit.q.out
@@ -18,6 +18,7 @@ STAGE PLANS:
                     type: string
               Limit
                 Reduce Output Operator
+                  sort order: 
                   tag: -1
                   value expressions:
                         expr: 0
diff --git a/ql/src/test/results/clientpositive/input5.q.out b/ql/src/test/results/clientpositive/input5.q.out
index 2957521d97..4bc103f8ce 100644
--- a/ql/src/test/results/clientpositive/input5.q.out
+++ b/ql/src/test/results/clientpositive/input5.q.out
@@ -31,6 +31,7 @@ STAGE PLANS:
                     key expressions:
                           expr: tkey
                           type: string
+                    sort order: +
                     Map-reduce partition columns:
                           expr: tkey
                           type: string
diff --git a/ql/src/test/results/clientpositive/join1.q.out b/ql/src/test/results/clientpositive/join1.q.out
index 8c6e355824..1a374501a5 100644
--- a/ql/src/test/results/clientpositive/join1.q.out
+++ b/ql/src/test/results/clientpositive/join1.q.out
@@ -14,6 +14,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: key
                     type: string
@@ -32,6 +33,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
diff --git a/ql/src/test/results/clientpositive/join10.q.out b/ql/src/test/results/clientpositive/join10.q.out
index 448b7972fa..89475ea1ab 100644
--- a/ql/src/test/results/clientpositive/join10.q.out
+++ b/ql/src/test/results/clientpositive/join10.q.out
@@ -22,6 +22,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -40,6 +41,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
diff --git a/ql/src/test/results/clientpositive/join11.q.out b/ql/src/test/results/clientpositive/join11.q.out
index c56f93f7db..6b0eaa180b 100644
--- a/ql/src/test/results/clientpositive/join11.q.out
+++ b/ql/src/test/results/clientpositive/join11.q.out
@@ -20,6 +20,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
@@ -44,6 +45,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
diff --git a/ql/src/test/results/clientpositive/join12.q.out b/ql/src/test/results/clientpositive/join12.q.out
index c54abcf51c..fdb9d20d58 100644
--- a/ql/src/test/results/clientpositive/join12.q.out
+++ b/ql/src/test/results/clientpositive/join12.q.out
@@ -20,6 +20,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
@@ -44,6 +45,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -68,6 +70,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
diff --git a/ql/src/test/results/clientpositive/join13.q.out b/ql/src/test/results/clientpositive/join13.q.out
index 844ac1b453..c731415ebe 100644
--- a/ql/src/test/results/clientpositive/join13.q.out
+++ b/ql/src/test/results/clientpositive/join13.q.out
@@ -21,6 +21,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
@@ -45,6 +46,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -75,6 +77,7 @@ STAGE PLANS:
             key expressions:
                   expr: (UDFToDouble(0) + UDFToDouble(2))
                   type: double
+            sort order: +
             Map-reduce partition columns:
                   expr: (UDFToDouble(0) + UDFToDouble(2))
                   type: double
@@ -103,6 +106,7 @@ STAGE PLANS:
                   key expressions:
                         expr: UDFToDouble(0)
                         type: double
+                  sort order: +
                   Map-reduce partition columns:
                         expr: UDFToDouble(0)
                         type: double
diff --git a/ql/src/test/results/clientpositive/join14.q.out b/ql/src/test/results/clientpositive/join14.q.out
index 87cd9f4a74..e7389c359f 100644
--- a/ql/src/test/results/clientpositive/join14.q.out
+++ b/ql/src/test/results/clientpositive/join14.q.out
@@ -26,6 +26,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -50,6 +51,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
diff --git a/ql/src/test/results/clientpositive/join15.q.out b/ql/src/test/results/clientpositive/join15.q.out
index 7d3d338cb8..bb9df32471 100644
--- a/ql/src/test/results/clientpositive/join15.q.out
+++ b/ql/src/test/results/clientpositive/join15.q.out
@@ -14,6 +14,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: key
                     type: string
@@ -28,6 +29,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: key
                     type: string
diff --git a/ql/src/test/results/clientpositive/join16.q.out b/ql/src/test/results/clientpositive/join16.q.out
index efb3b186a4..127fd24b4a 100644
--- a/ql/src/test/results/clientpositive/join16.q.out
+++ b/ql/src/test/results/clientpositive/join16.q.out
@@ -30,6 +30,7 @@ STAGE PLANS:
                           type: string
                           expr: 1
                           type: string
+                    sort order: ++
                     Map-reduce partition columns:
                           expr: 0
                           type: string
@@ -48,6 +49,7 @@ STAGE PLANS:
                     type: string
                     expr: value
                     type: string
+              sort order: ++
               Map-reduce partition columns:
                     expr: key
                     type: string
diff --git a/ql/src/test/results/clientpositive/join17.q.out b/ql/src/test/results/clientpositive/join17.q.out
index 1b12c07180..422c21500d 100644
--- a/ql/src/test/results/clientpositive/join17.q.out
+++ b/ql/src/test/results/clientpositive/join17.q.out
@@ -14,6 +14,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: key
                     type: string
@@ -28,6 +29,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: key
                     type: string
@@ -39,9 +41,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging:
       Path -> Alias:
-        file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/src 
+        file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src 
       Path -> Partition:
-        file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/src 
+        file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src 
           Partition
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -55,7 +57,7 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/src
+                location file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src
               serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
               name: src
       Reduce Operator Tree:
@@ -76,7 +78,7 @@ STAGE PLANS:
                   expr: 3
                   type: string
             File Output Operator
-              directory: /tmp/hive-athusoo/156101421/211911021.10000.insclause-0
+              directory: /tmp/hive-njain/562245695/1983405.10000.insclause-0
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
@@ -89,7 +91,7 @@ STAGE PLANS:
                     serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                     file.inputformat org.apache.hadoop.mapred.TextInputFormat
                     file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                    location file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/dest1
+                    location file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1
                   serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                   name: dest1
 
@@ -97,7 +99,7 @@ STAGE PLANS:
     Move Operator
       tables:
             replace:
-            source: /tmp/hive-athusoo/156101421/211911021.10000.insclause-0
+            source: /tmp/hive-njain/562245695/1983405.10000.insclause-0
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
@@ -110,7 +112,7 @@ STAGE PLANS:
                   serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/dest1
+                  location file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1
                 serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                 name: dest1
 
diff --git a/ql/src/test/results/clientpositive/join2.q.out b/ql/src/test/results/clientpositive/join2.q.out
index fa323d2b44..a637b5bd19 100644
--- a/ql/src/test/results/clientpositive/join2.q.out
+++ b/ql/src/test/results/clientpositive/join2.q.out
@@ -4,8 +4,7 @@ ABSTRACT SYNTAX TREE:
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
   Stage-2 depends on stages: Stage-1
-  Stage-0 depends on stages: Stage-2, Stage-1
-  Stage-0 depends on stages: Stage-2, Stage-1
+  Stage-0 depends on stages: Stage-2
 
 STAGE PLANS:
   Stage: Stage-1
@@ -20,6 +19,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
@@ -36,6 +36,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
@@ -64,6 +65,7 @@ STAGE PLANS:
               key expressions:
                     expr: UDFToDouble(key)
                     type: double
+              sort order: +
               Map-reduce partition columns:
                     expr: UDFToDouble(key)
                     type: double
@@ -78,6 +80,7 @@ STAGE PLANS:
             key expressions:
                   expr: (UDFToDouble(0) + UDFToDouble(1))
                   type: double
+            sort order: +
             Map-reduce partition columns:
                   expr: (UDFToDouble(0) + UDFToDouble(1))
                   type: double
diff --git a/ql/src/test/results/clientpositive/join3.q.out b/ql/src/test/results/clientpositive/join3.q.out
index 9d6bec0699..019bdcce9b 100644
--- a/ql/src/test/results/clientpositive/join3.q.out
+++ b/ql/src/test/results/clientpositive/join3.q.out
@@ -18,6 +18,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
@@ -30,6 +31,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: key
                     type: string
@@ -48,6 +50,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
diff --git a/ql/src/test/results/clientpositive/join4.q.out b/ql/src/test/results/clientpositive/join4.q.out
index c2f94c71ee..856a1e034a 100644
--- a/ql/src/test/results/clientpositive/join4.q.out
+++ b/ql/src/test/results/clientpositive/join4.q.out
@@ -24,6 +24,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -48,6 +49,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
diff --git a/ql/src/test/results/clientpositive/join5.q.out b/ql/src/test/results/clientpositive/join5.q.out
index 078e570729..f09e1a3850 100644
--- a/ql/src/test/results/clientpositive/join5.q.out
+++ b/ql/src/test/results/clientpositive/join5.q.out
@@ -24,6 +24,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -48,6 +49,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
diff --git a/ql/src/test/results/clientpositive/join6.q.out b/ql/src/test/results/clientpositive/join6.q.out
index 2330baac3d..af90f0be96 100644
--- a/ql/src/test/results/clientpositive/join6.q.out
+++ b/ql/src/test/results/clientpositive/join6.q.out
@@ -24,6 +24,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -48,6 +49,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
diff --git a/ql/src/test/results/clientpositive/join7.q.out b/ql/src/test/results/clientpositive/join7.q.out
index 1cf6539e4c..06c2d1dc20 100644
--- a/ql/src/test/results/clientpositive/join7.q.out
+++ b/ql/src/test/results/clientpositive/join7.q.out
@@ -24,6 +24,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -48,6 +49,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -72,6 +74,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
diff --git a/ql/src/test/results/clientpositive/join8.q.out b/ql/src/test/results/clientpositive/join8.q.out
index c6ad73c80b..0cdcde6718 100644
--- a/ql/src/test/results/clientpositive/join8.q.out
+++ b/ql/src/test/results/clientpositive/join8.q.out
@@ -24,6 +24,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
@@ -48,6 +49,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: 0
                         type: string
diff --git a/ql/src/test/results/clientpositive/join9.q.out b/ql/src/test/results/clientpositive/join9.q.out
index df0d85e5dd..fa4caa96a1 100644
--- a/ql/src/test/results/clientpositive/join9.q.out
+++ b/ql/src/test/results/clientpositive/join9.q.out
@@ -14,6 +14,7 @@ STAGE PLANS:
               key expressions:
                     expr: key
                     type: string
+              sort order: +
               Map-reduce partition columns:
                     expr: key
                     type: string
@@ -36,6 +37,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: 0
                       type: string
@@ -49,10 +51,10 @@ STAGE PLANS:
                       type: string
       Needs Tagging:
       Path -> Alias:
-        file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src 
-        file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src 
+        file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
       Path -> Partition:
-        file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src 
+        file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src 
           Partition
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -66,10 +68,10 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                location file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src
+                location file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src
               serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
               name: src
-        file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
           Partition
             partition values:
               ds 2008-04-08
@@ -87,7 +89,7 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                location file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcpart
+                location file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
               name: srcpart
       Reduce Operator Tree:
@@ -108,7 +110,7 @@ STAGE PLANS:
                     expr: 4
                     type: string
               File Output Operator
-                directory: /tmp/hive-njain/653311979.10000.insclause-0
+                directory: /tmp/hive-njain/292054552.10000.insclause-0
                 table:
                     input format: org.apache.hadoop.mapred.TextInputFormat
                     output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
@@ -121,7 +123,7 @@ STAGE PLANS:
                       serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                       file.inputformat org.apache.hadoop.mapred.TextInputFormat
                       file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                      location file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1
+                      location file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1
                     serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                     name: dest1
 
@@ -129,7 +131,7 @@ STAGE PLANS:
     Move Operator
       tables:
             replace:
-            source: /tmp/hive-njain/653311979.10000.insclause-0
+            source: /tmp/hive-njain/292054552.10000.insclause-0
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
@@ -142,7 +144,7 @@ STAGE PLANS:
                   serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                  location file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1
+                  location file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1
                 serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                 name: dest1
 
diff --git a/ql/src/test/results/clientpositive/notable_alias1.q.out b/ql/src/test/results/clientpositive/notable_alias1.q.out
index cf7c57066f..de5d2b6175 100644
--- a/ql/src/test/results/clientpositive/notable_alias1.q.out
+++ b/ql/src/test/results/clientpositive/notable_alias1.q.out
@@ -23,6 +23,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: rand()
                         type: double
@@ -32,8 +33,8 @@ STAGE PLANS:
                         type: int
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(VALUE.0)
+          aggregations:
+                expr: count(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
@@ -47,11 +48,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/252553932/208217017.10001 
+        /tmp/hive-njain/323467472/558965085.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -61,8 +63,8 @@ STAGE PLANS:
                   type: bigint
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(VALUE.0)
+          aggregations:
+                expr: count(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
diff --git a/ql/src/test/results/clientpositive/notable_alias2.q.out b/ql/src/test/results/clientpositive/notable_alias2.q.out
index c88aef8cba..87272761dd 100644
--- a/ql/src/test/results/clientpositive/notable_alias2.q.out
+++ b/ql/src/test/results/clientpositive/notable_alias2.q.out
@@ -23,6 +23,7 @@ STAGE PLANS:
                   key expressions:
                         expr: 0
                         type: string
+                  sort order: +
                   Map-reduce partition columns:
                         expr: rand()
                         type: double
@@ -32,8 +33,8 @@ STAGE PLANS:
                         type: int
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(VALUE.0)
+          aggregations:
+                expr: count(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
@@ -47,11 +48,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/140889204/772631826.10001 
+        /tmp/hive-njain/202657451/1070275771.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -61,8 +63,8 @@ STAGE PLANS:
                   type: bigint
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(VALUE.0)
+          aggregations:
+                expr: count(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
diff --git a/ql/src/test/results/clientpositive/subq2.q.out b/ql/src/test/results/clientpositive/subq2.q.out
index ea0d60b5a5..c0b8e44393 100644
--- a/ql/src/test/results/clientpositive/subq2.q.out
+++ b/ql/src/test/results/clientpositive/subq2.q.out
@@ -19,6 +19,7 @@ STAGE PLANS:
                 key expressions:
                       expr: 0
                       type: string
+                sort order: +
                 Map-reduce partition columns:
                       expr: rand()
                       type: double
@@ -28,8 +29,8 @@ STAGE PLANS:
                       type: int
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(VALUE.0)
+          aggregations:
+                expr: count(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
@@ -43,11 +44,12 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/158235739/1374397779.10002 
+        /tmp/hive-njain/611234651/29133673.10002 
           Reduce Output Operator
             key expressions:
                   expr: 0
                   type: string
+            sort order: +
             Map-reduce partition columns:
                   expr: 0
                   type: string
@@ -57,8 +59,8 @@ STAGE PLANS:
                   type: bigint
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: count(VALUE.0)
+          aggregations:
+                expr: count(VALUE.0)
           keys:
                 expr: KEY.0
                 type: string
diff --git a/ql/src/test/results/clientpositive/udf3.q.out b/ql/src/test/results/clientpositive/udf3.q.out
index 5e4d5f3f2f..e2c861b798 100644
--- a/ql/src/test/results/clientpositive/udf3.q.out
+++ b/ql/src/test/results/clientpositive/udf3.q.out
@@ -13,6 +13,7 @@ STAGE PLANS:
         src 
             Select Operator
               Reduce Output Operator
+                sort order: 
                 Map-reduce partition columns:
                       expr: rand()
                       type: double
@@ -22,12 +23,12 @@ STAGE PLANS:
                       type: int
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: max(UDFToDouble(VALUE.0))
-              expr: avg(UDFToDouble(VALUE.0))
-              expr: count(VALUE.0)
-              expr: sum(UDFToDouble(VALUE.0))
-              expr: min(UDFToDouble(VALUE.0))
+          aggregations:
+                expr: max(UDFToDouble(VALUE.0))
+                expr: avg(UDFToDouble(VALUE.0))
+                expr: count(VALUE.0)
+                expr: sum(UDFToDouble(VALUE.0))
+                expr: min(UDFToDouble(VALUE.0))
           mode: partial1
           File Output Operator
             table:
@@ -38,8 +39,9 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-njain/20731779/1245261687.10001 
+        /tmp/hive-njain/650928980/125435985.10001 
           Reduce Output Operator
+            sort order: 
             tag: -1
             value expressions:
                   expr: 0
@@ -54,12 +56,12 @@ STAGE PLANS:
                   type: double
       Reduce Operator Tree:
         Group By Operator
-        
-              expr: max(VALUE.0)
-              expr: avg(VALUE.1)
-              expr: count(VALUE.2)
-              expr: sum(VALUE.3)
-              expr: min(VALUE.4)
+          aggregations:
+                expr: max(VALUE.0)
+                expr: avg(VALUE.1)
+                expr: count(VALUE.2)
+                expr: sum(VALUE.3)
+                expr: min(VALUE.4)
           mode: unknown
           Select Operator
             expressions:
diff --git a/ql/src/test/results/clientpositive/union2.q.out b/ql/src/test/results/clientpositive/union2.q.out
new file mode 100644
index 0000000000..52c942e1c3
--- /dev/null
+++ b/ql/src/test/results/clientpositive/union2.q.out
@@ -0,0 +1,85 @@
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_UNION (TOK_QUERY (TOK_FROM (TOK_TABREF src s1)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF s1 key) key) (TOK_SELEXPR (TOK_COLREF s1 value) value)))) (TOK_QUERY (TOK_FROM (TOK_TABREF src s2)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF s2 key) key) (TOK_SELEXPR (TOK_COLREF s2 value) value))))) unionsrc)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION count 1)))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        null-subquery1:unionsrc-subquery1:s1 
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+                Reduce Output Operator
+                  sort order: 
+                  Map-reduce partition columns:
+                        expr: rand()
+                        type: double
+                  tag: -1
+                  value expressions:
+                        expr: 1
+                        type: int
+        null-subquery2:unionsrc-subquery2:s2 
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+                Reduce Output Operator
+                  sort order: 
+                  Map-reduce partition columns:
+                        expr: rand()
+                        type: double
+                  tag: -1
+                  value expressions:
+                        expr: 1
+                        type: int
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE.0)
+          mode: partial1
+          File Output Operator
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.mapred.SequenceFileOutputFormat
+                name: binary_table
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        /tmp/hive-njain/351706607/217456705.10002 
+          Reduce Output Operator
+            sort order: 
+            tag: -1
+            value expressions:
+                  expr: 0
+                  type: bigint
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE.0)
+          mode: unknown
+          Select Operator
+            expressions:
+                  expr: 0
+                  type: bigint
+            File Output Operator
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+1000
diff --git a/ql/src/test/results/compiler/plan/case_sensitivity.q.xml b/ql/src/test/results/compiler/plan/case_sensitivity.q.xml
index 85b9646a44..b61dbb5b32 100644
--- a/ql/src/test/results/compiler/plan/case_sensitivity.q.xml
+++ b/ql/src/test/results/compiler/plan/case_sensitivity.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/14809281.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/1375367896.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src_thrift</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -128,7 +128,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-njain/14809281.10000.insclause-0</string> 
+                           <string>/tmp/hive-njain/1375367896.10000.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object idref="tableDesc0"/> 
@@ -425,7 +425,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -526,7 +526,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src_thrift</string> 
@@ -538,7 +538,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -594,7 +594,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/cast1.q.xml b/ql/src/test/results/compiler/plan/cast1.q.xml
index 1d8f63b735..d80dd5700f 100644
--- a/ql/src/test/results/compiler/plan/cast1.q.xml
+++ b/ql/src/test/results/compiler/plan/cast1.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -30,7 +30,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-njain/168127484.10001.insclause-0</string> 
+                           <string>/tmp/hive-njain/802827123.10001.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -666,7 +666,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -716,7 +716,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -728,7 +728,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -780,7 +780,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby1.q.xml b/ql/src/test/results/compiler/plan/groupby1.q.xml
index 797d5f12ef..e9031aade7 100755
--- a/ql/src/test/results/compiler/plan/groupby1.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby1.q.xml
@@ -35,7 +35,7 @@
                  <boolean>true</boolean> 
                 </void> 
                 <void property="sourceDir"> 
-                 <string>/tmp/hive-njain/255951081/1610738340.10000.insclause-0</string> 
+                 <string>/tmp/hive-njain/1734876980/99648103.10000.insclause-0</string> 
                 </void> 
                 <void property="table"> 
                  <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -84,7 +84,7 @@
                     </void> 
                     <void method="put"> 
                      <string>location</string> 
-                     <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                     <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                     </void> 
                    </object> 
                   </void> 
@@ -118,7 +118,7 @@
         <void property="aliasToWork"> 
          <object class="java.util.HashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/255951081/1610738340.10001</string> 
+           <string>/tmp/hive-njain/1734876980/99648103.10001</string> 
            <object id="ReduceSinkOperator0" class="org.apache.hadoop.hive.ql.exec.ReduceSinkOperator"> 
             <void property="conf"> 
              <object class="org.apache.hadoop.hive.ql.plan.reduceSinkDesc"> 
@@ -160,6 +160,10 @@
                    <string>name</string> 
                    <string>binary_sortable_table</string> 
                   </void> 
+                  <void method="put"> 
+                   <string>serialization.sort.order</string> 
+                   <string>+</string> 
+                  </void> 
                   <void method="put"> 
                    <string>serialization.ddl</string> 
                    <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -241,7 +245,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/255951081/1610738340.10001</string> 
+                       <string>/tmp/hive-njain/1734876980/99648103.10001</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object id="tableDesc3" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -422,6 +426,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -552,7 +560,7 @@
                     <void property="parentOperators"> 
                      <object class="java.util.ArrayList"> 
                       <void method="add"> 
-                       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
                         <void property="childOperators"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
@@ -672,10 +680,10 @@
         <void property="pathToAliases"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/255951081/1610738340.10001</string> 
+           <string>/tmp/hive-njain/1734876980/99648103.10001</string> 
            <object class="java.util.ArrayList"> 
             <void method="add"> 
-             <string>/tmp/hive-njain/255951081/1610738340.10001</string> 
+             <string>/tmp/hive-njain/1734876980/99648103.10001</string> 
             </void> 
            </object> 
           </void> 
@@ -684,7 +692,7 @@
         <void property="pathToPartitionInfo"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/255951081/1610738340.10001</string> 
+           <string>/tmp/hive-njain/1734876980/99648103.10001</string> 
            <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
             <void property="tableDesc"> 
              <object idref="tableDesc3"/> 
@@ -706,7 +714,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/255951081/1610738340.10000.insclause-0</string> 
+                     <string>/tmp/hive-njain/1734876980/99648103.10000.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object idref="tableDesc0"/> 
@@ -905,7 +913,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object idref="ForwardOperator0"/> 
+       <object idref="TableScanOperator0"/> 
       </void> 
      </object> 
     </void> 
@@ -915,7 +923,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -927,7 +935,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -979,7 +987,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby2.q.xml b/ql/src/test/results/compiler/plan/groupby2.q.xml
index 6793be80e7..d26597a160 100755
--- a/ql/src/test/results/compiler/plan/groupby2.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby2.q.xml
@@ -20,7 +20,7 @@
         <void property="aliasToWork"> 
          <object class="java.util.HashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/16836316/818176241.10002</string> 
+           <string>/tmp/hive-njain/568944209/29322851.10002</string> 
            <object id="ReduceSinkOperator0" class="org.apache.hadoop.hive.ql.exec.ReduceSinkOperator"> 
             <void property="conf"> 
              <object class="org.apache.hadoop.hive.ql.plan.reduceSinkDesc"> 
@@ -62,6 +62,10 @@
                    <string>name</string> 
                    <string>binary_sortable_table</string> 
                   </void> 
+                  <void method="put"> 
+                   <string>serialization.sort.order</string> 
+                   <string>+</string> 
+                  </void> 
                   <void method="put"> 
                    <string>serialization.ddl</string> 
                    <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -157,7 +161,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/16836316/818176241.10002</string> 
+                       <string>/tmp/hive-njain/568944209/29322851.10002</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object id="tableDesc2" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -473,6 +477,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>++</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string reducesinkkey0, string reducesinkkey1}</string> 
@@ -531,7 +539,7 @@
                     <void property="parentOperators"> 
                      <object class="java.util.ArrayList"> 
                       <void method="add"> 
-                       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
                         <void property="childOperators"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
@@ -661,10 +669,10 @@
         <void property="pathToAliases"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/16836316/818176241.10002</string> 
+           <string>/tmp/hive-njain/568944209/29322851.10002</string> 
            <object class="java.util.ArrayList"> 
             <void method="add"> 
-             <string>/tmp/hive-njain/16836316/818176241.10002</string> 
+             <string>/tmp/hive-njain/568944209/29322851.10002</string> 
             </void> 
            </object> 
           </void> 
@@ -673,7 +681,7 @@
         <void property="pathToPartitionInfo"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/16836316/818176241.10002</string> 
+           <string>/tmp/hive-njain/568944209/29322851.10002</string> 
            <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
             <void property="tableDesc"> 
              <object idref="tableDesc2"/> 
@@ -695,7 +703,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/16836316/818176241.10001.insclause-0</string> 
+                     <string>/tmp/hive-njain/568944209/29322851.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1028,7 +1036,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object idref="ForwardOperator0"/> 
+       <object idref="TableScanOperator0"/> 
       </void> 
      </object> 
     </void> 
@@ -1038,7 +1046,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -1050,7 +1058,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -1102,7 +1110,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby3.q.xml b/ql/src/test/results/compiler/plan/groupby3.q.xml
index 9f0e4e2e4a..4b3f68859d 100644
--- a/ql/src/test/results/compiler/plan/groupby3.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby3.q.xml
@@ -20,7 +20,7 @@
         <void property="aliasToWork"> 
          <object class="java.util.HashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/6206243/1572612461.10002</string> 
+           <string>/tmp/hive-njain/332856862/73535516.10002</string> 
            <object id="ReduceSinkOperator0" class="org.apache.hadoop.hive.ql.exec.ReduceSinkOperator"> 
             <void property="conf"> 
              <object class="org.apache.hadoop.hive.ql.plan.reduceSinkDesc"> 
@@ -47,6 +47,10 @@
                    <string>name</string> 
                    <string>binary_sortable_table</string> 
                   </void> 
+                  <void method="put"> 
+                   <string>serialization.sort.order</string> 
+                   <string></string> 
+                  </void> 
                   <void method="put"> 
                    <string>serialization.ddl</string> 
                    <string>struct binary_sortable_table { }</string> 
@@ -172,7 +176,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/6206243/1572612461.10002</string> 
+                       <string>/tmp/hive-njain/332856862/73535516.10002</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object id="tableDesc2" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -596,6 +600,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -683,7 +691,7 @@
                         <void property="parentOperators"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                           <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
                             <void property="childOperators"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -845,10 +853,10 @@
         <void property="pathToAliases"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/6206243/1572612461.10002</string> 
+           <string>/tmp/hive-njain/332856862/73535516.10002</string> 
            <object class="java.util.ArrayList"> 
             <void method="add"> 
-             <string>/tmp/hive-njain/6206243/1572612461.10002</string> 
+             <string>/tmp/hive-njain/332856862/73535516.10002</string> 
             </void> 
            </object> 
           </void> 
@@ -857,7 +865,7 @@
         <void property="pathToPartitionInfo"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/6206243/1572612461.10002</string> 
+           <string>/tmp/hive-njain/332856862/73535516.10002</string> 
            <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
             <void property="tableDesc"> 
              <object idref="tableDesc2"/> 
@@ -879,7 +887,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/453676475.10001.insclause-0</string> 
+                     <string>/tmp/hive-njain/229394155.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1263,7 +1271,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object idref="ForwardOperator0"/> 
+       <object idref="TableScanOperator0"/> 
       </void> 
      </object> 
     </void> 
@@ -1273,7 +1281,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -1285,7 +1293,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -1337,7 +1345,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby4.q.xml b/ql/src/test/results/compiler/plan/groupby4.q.xml
index 561ef2e351..0658907e88 100644
--- a/ql/src/test/results/compiler/plan/groupby4.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby4.q.xml
@@ -20,7 +20,7 @@
         <void property="aliasToWork"> 
          <object class="java.util.HashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/887282678/418909568.10002</string> 
+           <string>/tmp/hive-njain/270959803/34104799.10002</string> 
            <object id="ReduceSinkOperator0" class="org.apache.hadoop.hive.ql.exec.ReduceSinkOperator"> 
             <void property="conf"> 
              <object class="org.apache.hadoop.hive.ql.plan.reduceSinkDesc"> 
@@ -62,6 +62,10 @@
                    <string>name</string> 
                    <string>binary_sortable_table</string> 
                   </void> 
+                  <void method="put"> 
+                   <string>serialization.sort.order</string> 
+                   <string>+</string> 
+                  </void> 
                   <void method="put"> 
                    <string>serialization.ddl</string> 
                    <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -128,7 +132,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/887282678/418909568.10002</string> 
+                       <string>/tmp/hive-njain/270959803/34104799.10002</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object id="tableDesc2" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -307,6 +311,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -418,7 +426,7 @@
                         <void property="parentOperators"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                           <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
                             <void property="childOperators"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -540,10 +548,10 @@
         <void property="pathToAliases"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/887282678/418909568.10002</string> 
+           <string>/tmp/hive-njain/270959803/34104799.10002</string> 
            <object class="java.util.ArrayList"> 
             <void method="add"> 
-             <string>/tmp/hive-njain/887282678/418909568.10002</string> 
+             <string>/tmp/hive-njain/270959803/34104799.10002</string> 
             </void> 
            </object> 
           </void> 
@@ -552,7 +560,7 @@
         <void property="pathToPartitionInfo"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/887282678/418909568.10002</string> 
+           <string>/tmp/hive-njain/270959803/34104799.10002</string> 
            <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
             <void property="tableDesc"> 
              <object idref="tableDesc2"/> 
@@ -574,7 +582,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/261034330.10001.insclause-0</string> 
+                     <string>/tmp/hive-njain/648245175.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -743,7 +751,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object idref="ForwardOperator0"/> 
+       <object idref="TableScanOperator0"/> 
       </void> 
      </object> 
     </void> 
@@ -753,7 +761,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -765,7 +773,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -817,7 +825,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby5.q.xml b/ql/src/test/results/compiler/plan/groupby5.q.xml
index 7d2a992d56..aee72caced 100644
--- a/ql/src/test/results/compiler/plan/groupby5.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby5.q.xml
@@ -20,7 +20,7 @@
         <void property="aliasToWork"> 
          <object class="java.util.HashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/261145182/229076034.10002</string> 
+           <string>/tmp/hive-njain/25539710/241360638.10002</string> 
            <object id="ReduceSinkOperator0" class="org.apache.hadoop.hive.ql.exec.ReduceSinkOperator"> 
             <void property="conf"> 
              <object class="org.apache.hadoop.hive.ql.plan.reduceSinkDesc"> 
@@ -62,6 +62,10 @@
                    <string>name</string> 
                    <string>binary_sortable_table</string> 
                   </void> 
+                  <void method="put"> 
+                   <string>serialization.sort.order</string> 
+                   <string>+</string> 
+                  </void> 
                   <void method="put"> 
                    <string>serialization.ddl</string> 
                    <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -143,7 +147,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/261145182/229076034.10002</string> 
+                       <string>/tmp/hive-njain/25539710/241360638.10002</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object id="tableDesc2" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -324,6 +328,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -454,7 +462,7 @@
                     <void property="parentOperators"> 
                      <object class="java.util.ArrayList"> 
                       <void method="add"> 
-                       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
                         <void property="childOperators"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
@@ -574,10 +582,10 @@
         <void property="pathToAliases"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/261145182/229076034.10002</string> 
+           <string>/tmp/hive-njain/25539710/241360638.10002</string> 
            <object class="java.util.ArrayList"> 
             <void method="add"> 
-             <string>/tmp/hive-njain/261145182/229076034.10002</string> 
+             <string>/tmp/hive-njain/25539710/241360638.10002</string> 
             </void> 
            </object> 
           </void> 
@@ -586,7 +594,7 @@
         <void property="pathToPartitionInfo"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/261145182/229076034.10002</string> 
+           <string>/tmp/hive-njain/25539710/241360638.10002</string> 
            <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
             <void property="tableDesc"> 
              <object idref="tableDesc2"/> 
@@ -608,7 +616,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/261145182/229076034.10001.insclause-0</string> 
+                     <string>/tmp/hive-njain/25539710/241360638.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -829,7 +837,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object idref="ForwardOperator0"/> 
+       <object idref="TableScanOperator0"/> 
       </void> 
      </object> 
     </void> 
@@ -839,7 +847,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -851,7 +859,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -903,7 +911,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/groupby6.q.xml b/ql/src/test/results/compiler/plan/groupby6.q.xml
index e341618f9e..11b0f764e3 100644
--- a/ql/src/test/results/compiler/plan/groupby6.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby6.q.xml
@@ -20,7 +20,7 @@
         <void property="aliasToWork"> 
          <object class="java.util.HashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/14395907/27060646.10002</string> 
+           <string>/tmp/hive-njain/1062373643/636855904.10002</string> 
            <object id="ReduceSinkOperator0" class="org.apache.hadoop.hive.ql.exec.ReduceSinkOperator"> 
             <void property="conf"> 
              <object class="org.apache.hadoop.hive.ql.plan.reduceSinkDesc"> 
@@ -62,6 +62,10 @@
                    <string>name</string> 
                    <string>binary_sortable_table</string> 
                   </void> 
+                  <void method="put"> 
+                   <string>serialization.sort.order</string> 
+                   <string>+</string> 
+                  </void> 
                   <void method="put"> 
                    <string>serialization.ddl</string> 
                    <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -128,7 +132,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/14395907/27060646.10002</string> 
+                       <string>/tmp/hive-njain/1062373643/636855904.10002</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object id="tableDesc2" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -307,6 +311,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -418,7 +426,7 @@
                         <void property="parentOperators"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                           <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
                             <void property="childOperators"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -540,10 +548,10 @@
         <void property="pathToAliases"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/14395907/27060646.10002</string> 
+           <string>/tmp/hive-njain/1062373643/636855904.10002</string> 
            <object class="java.util.ArrayList"> 
             <void method="add"> 
-             <string>/tmp/hive-njain/14395907/27060646.10002</string> 
+             <string>/tmp/hive-njain/1062373643/636855904.10002</string> 
             </void> 
            </object> 
           </void> 
@@ -552,7 +560,7 @@
         <void property="pathToPartitionInfo"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/14395907/27060646.10002</string> 
+           <string>/tmp/hive-njain/1062373643/636855904.10002</string> 
            <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
             <void property="tableDesc"> 
              <object idref="tableDesc2"/> 
@@ -574,7 +582,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/786616721.10001.insclause-0</string> 
+                     <string>/tmp/hive-njain/786081691.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -743,7 +751,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object idref="ForwardOperator0"/> 
+       <object idref="TableScanOperator0"/> 
       </void> 
      </object> 
     </void> 
@@ -753,7 +761,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -765,7 +773,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -817,7 +825,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input1.q.xml b/ql/src/test/results/compiler/plan/input1.q.xml
index d74ae81a85..5778b255dc 100755
--- a/ql/src/test/results/compiler/plan/input1.q.xml
+++ b/ql/src/test/results/compiler/plan/input1.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/1266815017/75925705.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/202548334/480112542.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -124,7 +124,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/1266815017/75925705.10000.insclause-0</string> 
+                       <string>/tmp/hive-njain/202548334/480112542.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -282,7 +282,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -332,7 +332,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -344,7 +344,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -396,7 +396,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input2.q.xml b/ql/src/test/results/compiler/plan/input2.q.xml
index 4e92bdaa36..f882be3e9b 100755
--- a/ql/src/test/results/compiler/plan/input2.q.xml
+++ b/ql/src/test/results/compiler/plan/input2.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/96064841/341873759.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/104100889/1367774772.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -100,7 +100,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/96064841/341873759.10001.insclause-1</string> 
+             <string>/tmp/hive-njain/104100889/1367774772.10001.insclause-1</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc1" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -149,7 +149,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest2</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest2</string> 
                 </void> 
                </object> 
               </void> 
@@ -178,7 +178,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/96064841/341873759.10002.insclause-2</string> 
+             <string>/tmp/hive-njain/104100889/1367774772.10002.insclause-2</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc2" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -231,7 +231,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest3</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest3</string> 
                 </void> 
                </object> 
               </void> 
@@ -259,7 +259,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -275,7 +275,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/96064841/341873759.10000.insclause-0</string> 
+                       <string>/tmp/hive-njain/104100889/1367774772.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -436,7 +436,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -483,7 +483,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/96064841/341873759.10001.insclause-1</string> 
+                       <string>/tmp/hive-njain/104100889/1367774772.10001.insclause-1</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc1"/> 
@@ -703,7 +703,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -729,7 +729,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/96064841/341873759.10002.insclause-2</string> 
+                       <string>/tmp/hive-njain/104100889/1367774772.10002.insclause-2</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc2"/> 
@@ -875,7 +875,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -904,7 +904,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -916,7 +916,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -968,7 +968,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input3.q.xml b/ql/src/test/results/compiler/plan/input3.q.xml
index 1dd935f03f..aaa32628d4 100755
--- a/ql/src/test/results/compiler/plan/input3.q.xml
+++ b/ql/src/test/results/compiler/plan/input3.q.xml
@@ -28,7 +28,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/346624347/1822685820.10003.insclause-3</string> 
+             <string>/tmp/hive-njain/44210915/741228445.10003.insclause-3</string> 
             </void> 
             <void property="targetDir"> 
              <string>../../../../build/contrib/hive/ql/test/data/warehouse/dest4.out</string> 
@@ -48,7 +48,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/346624347/1822685820.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/44210915/741228445.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -97,7 +97,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -117,7 +117,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/346624347/1822685820.10001.insclause-1</string> 
+             <string>/tmp/hive-njain/44210915/741228445.10001.insclause-1</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc1" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -166,7 +166,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest2</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest2</string> 
                 </void> 
                </object> 
               </void> 
@@ -195,7 +195,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/346624347/1822685820.10002.insclause-2</string> 
+             <string>/tmp/hive-njain/44210915/741228445.10002.insclause-2</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc2" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -248,7 +248,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest3</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest3</string> 
                 </void> 
                </object> 
               </void> 
@@ -276,7 +276,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -292,7 +292,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/346624347/1822685820.10000.insclause-0</string> 
+                       <string>/tmp/hive-njain/44210915/741228445.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -453,7 +453,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -500,7 +500,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/346624347/1822685820.10001.insclause-1</string> 
+                       <string>/tmp/hive-njain/44210915/741228445.10001.insclause-1</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc1"/> 
@@ -720,7 +720,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -746,7 +746,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/346624347/1822685820.10002.insclause-2</string> 
+                       <string>/tmp/hive-njain/44210915/741228445.10002.insclause-2</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc2"/> 
@@ -966,7 +966,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -992,7 +992,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/346624347/1822685820.10003.insclause-3</string> 
+                       <string>/tmp/hive-njain/44210915/741228445.10003.insclause-3</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1140,7 +1140,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -1169,7 +1169,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -1181,7 +1181,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -1233,7 +1233,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input4.q.xml b/ql/src/test/results/compiler/plan/input4.q.xml
index 963fd36b2b..7a082a42ad 100755
--- a/ql/src/test/results/compiler/plan/input4.q.xml
+++ b/ql/src/test/results/compiler/plan/input4.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/1463239211/50419269.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/89591678/324016511.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>tmap:src</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -124,7 +124,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.reduceSinkDesc"> 
                       <void property="keyCols"> 
-                       <object id="ArrayList0" class="java.util.ArrayList"> 
+                       <object class="java.util.ArrayList"> 
                         <void method="add"> 
                          <object class="org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc"> 
                           <void property="column"> 
@@ -158,6 +158,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -174,7 +178,18 @@
                        <int>-1</int> 
                       </void> 
                       <void property="partitionCols"> 
-                       <object idref="ArrayList0"/> 
+                       <object class="java.util.ArrayList"> 
+                        <void method="add"> 
+                         <object class="org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc"> 
+                          <void property="column"> 
+                           <string>tkey</string> 
+                          </void> 
+                          <void property="typeInfo"> 
+                           <object idref="PrimitiveTypeInfo0"/> 
+                          </void> 
+                         </object> 
+                        </void> 
+                       </object> 
                       </void> 
                       <void property="tag"> 
                        <int>-1</int> 
@@ -379,7 +394,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -453,7 +468,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>tmap:src</string> 
@@ -465,7 +480,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -517,7 +532,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -547,7 +562,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/1463239211/50419269.10000.insclause-0</string> 
+                     <string>/tmp/hive-njain/89591678/324016511.10000.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object idref="tableDesc0"/> 
diff --git a/ql/src/test/results/compiler/plan/input5.q.xml b/ql/src/test/results/compiler/plan/input5.q.xml
index 05a3a3de38..fac6a23d4f 100644
--- a/ql/src/test/results/compiler/plan/input5.q.xml
+++ b/ql/src/test/results/compiler/plan/input5.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/37361661.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/113180245.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>tmap:src_thrift</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -128,7 +128,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.reduceSinkDesc"> 
                           <void property="keyCols"> 
-                           <object id="ArrayList0" class="java.util.ArrayList"> 
+                           <object class="java.util.ArrayList"> 
                             <void method="add"> 
                              <object class="org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc"> 
                               <void property="column"> 
@@ -162,6 +162,10 @@
                                <string>name</string> 
                                <string>binary_sortable_table</string> 
                               </void> 
+                              <void method="put"> 
+                               <string>serialization.sort.order</string> 
+                               <string>+</string> 
+                              </void> 
                               <void method="put"> 
                                <string>serialization.ddl</string> 
                                <string>struct binary_sortable_table { string reducesinkkey0}</string> 
@@ -178,7 +182,18 @@
                            <int>-1</int> 
                           </void> 
                           <void property="partitionCols"> 
-                           <object idref="ArrayList0"/> 
+                           <object class="java.util.ArrayList"> 
+                            <void method="add"> 
+                             <object class="org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc"> 
+                              <void property="column"> 
+                               <string>tkey</string> 
+                              </void> 
+                              <void property="typeInfo"> 
+                               <object idref="PrimitiveTypeInfo0"/> 
+                              </void> 
+                             </object> 
+                            </void> 
+                           </object> 
                           </void> 
                           <void property="tag"> 
                            <int>-1</int> 
@@ -462,7 +477,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -587,7 +602,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>tmap:src_thrift</string> 
@@ -599,7 +614,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -655,7 +670,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
             </void> 
            </object> 
           </void> 
@@ -681,7 +696,7 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>/tmp/hive-njain/37361661.10000.insclause-0</string> 
+                 <string>/tmp/hive-njain/113180245.10000.insclause-0</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object idref="tableDesc0"/> 
diff --git a/ql/src/test/results/compiler/plan/input6.q.xml b/ql/src/test/results/compiler/plan/input6.q.xml
index 70d9d9552f..120cf18de3 100644
--- a/ql/src/test/results/compiler/plan/input6.q.xml
+++ b/ql/src/test/results/compiler/plan/input6.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/958229575/254397981.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/545048490/63016941.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src1</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -124,7 +124,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/958229575/254397981.10000.insclause-0</string> 
+                       <string>/tmp/hive-njain/545048490/63016941.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -265,7 +265,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -315,7 +315,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src1</string> 
@@ -327,7 +327,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -379,7 +379,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input7.q.xml b/ql/src/test/results/compiler/plan/input7.q.xml
index 6b4cff79af..c40ca7a947 100644
--- a/ql/src/test/results/compiler/plan/input7.q.xml
+++ b/ql/src/test/results/compiler/plan/input7.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/505144482.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/1794861784.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src1</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -124,7 +124,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/505144482.10000.insclause-0</string> 
+                       <string>/tmp/hive-njain/1794861784.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -242,7 +242,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -303,7 +303,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src1</string> 
@@ -315,7 +315,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -367,7 +367,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input8.q.xml b/ql/src/test/results/compiler/plan/input8.q.xml
index 81531b65c7..35b8a9e107 100644
--- a/ql/src/test/results/compiler/plan/input8.q.xml
+++ b/ql/src/test/results/compiler/plan/input8.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src1</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -26,7 +26,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/568957411.10001.insclause-0</string> 
+                       <string>/tmp/hive-njain/590666906.10001.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -320,7 +320,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -381,7 +381,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src1</string> 
@@ -393,7 +393,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -445,7 +445,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input9.q.xml b/ql/src/test/results/compiler/plan/input9.q.xml
index 23298459cc..851265c4b1 100644
--- a/ql/src/test/results/compiler/plan/input9.q.xml
+++ b/ql/src/test/results/compiler/plan/input9.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/510164179.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/483179349.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src1</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -128,7 +128,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-njain/510164179.10000.insclause-0</string> 
+                           <string>/tmp/hive-njain/483179349.10000.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object idref="tableDesc0"/> 
@@ -324,7 +324,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -374,7 +374,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src1</string> 
@@ -386,7 +386,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -438,7 +438,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src1</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src1</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input_part1.q.xml b/ql/src/test/results/compiler/plan/input_part1.q.xml
index 8f2b3815c3..bb1fd1681b 100644
--- a/ql/src/test/results/compiler/plan/input_part1.q.xml
+++ b/ql/src/test/results/compiler/plan/input_part1.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>srcpart</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -26,7 +26,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/133993466/535789358.10001.insclause-0</string> 
+                       <string>/tmp/hive-njain/381790525/715994016.10001.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -394,7 +394,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -464,7 +464,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>srcpart</string> 
@@ -476,7 +476,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"> 
@@ -541,7 +541,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcpart</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input_testsequencefile.q.xml b/ql/src/test/results/compiler/plan/input_testsequencefile.q.xml
index 2064424d60..7fa8158217 100644
--- a/ql/src/test/results/compiler/plan/input_testsequencefile.q.xml
+++ b/ql/src/test/results/compiler/plan/input_testsequencefile.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/317501800/36487063.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/380325162/471311562.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest4_sequencefile</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest4_sequencefile</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -120,7 +120,7 @@
                 <void property="conf"> 
                  <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                   <void property="dirName"> 
-                   <string>/tmp/hive-njain/317501800/36487063.10000.insclause-0</string> 
+                   <string>/tmp/hive-njain/380325162/471311562.10000.insclause-0</string> 
                   </void> 
                   <void property="tableInfo"> 
                    <object idref="tableDesc0"/> 
@@ -201,7 +201,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -251,7 +251,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -263,7 +263,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -315,7 +315,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input_testxpath.q.xml b/ql/src/test/results/compiler/plan/input_testxpath.q.xml
index 637349860b..b4d9781cbc 100644
--- a/ql/src/test/results/compiler/plan/input_testxpath.q.xml
+++ b/ql/src/test/results/compiler/plan/input_testxpath.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src_thrift</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -26,7 +26,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/493664790.10001.insclause-0</string> 
+                       <string>/tmp/hive-njain/1163598770.10001.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -288,7 +288,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -413,7 +413,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src_thrift</string> 
@@ -425,7 +425,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -481,7 +481,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input_testxpath2.q.xml b/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
index 26d765a660..7bc82dbc07 100644
--- a/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
+++ b/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src_thrift</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -30,7 +30,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-njain/709347576.10001.insclause-0</string> 
+                           <string>/tmp/hive-njain/610717789.10001.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -476,7 +476,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -570,7 +570,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src_thrift</string> 
@@ -582,7 +582,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -638,7 +638,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src_thrift</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src_thrift</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/join1.q.xml b/ql/src/test/results/compiler/plan/join1.q.xml
index 6973e0387d..22f85d180f 100644
--- a/ql/src/test/results/compiler/plan/join1.q.xml
+++ b/ql/src/test/results/compiler/plan/join1.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/810786628.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/605771515.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src2</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -150,6 +150,10 @@
                    <string>name</string> 
                    <string>binary_sortable_table</string> 
                   </void> 
+                  <void method="put"> 
+                   <string>serialization.sort.order</string> 
+                   <string>+</string> 
+                  </void> 
                   <void method="put"> 
                    <string>serialization.ddl</string> 
                    <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -229,7 +233,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -297,7 +301,7 @@
       </void> 
       <void method="put"> 
        <string>src1</string> 
-       <object id="ForwardOperator1" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator1" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -339,6 +343,10 @@
                        <string>name</string> 
                        <string>binary_sortable_table</string> 
                       </void> 
+                      <void method="put"> 
+                       <string>serialization.sort.order</string> 
+                       <string>+</string> 
+                      </void> 
                       <void method="put"> 
                        <string>serialization.ddl</string> 
                        <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -452,7 +460,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator1"/> 
+               <object idref="TableScanOperator1"/> 
               </void> 
              </object> 
             </void> 
@@ -519,7 +527,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src2</string> 
@@ -534,7 +542,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -586,7 +594,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -612,7 +620,7 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>/tmp/hive-njain/810786628.10000.insclause-0</string> 
+                 <string>/tmp/hive-njain/605771515.10000.insclause-0</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object idref="tableDesc0"/> 
diff --git a/ql/src/test/results/compiler/plan/join2.q.xml b/ql/src/test/results/compiler/plan/join2.q.xml
index 6ee9c91f62..89365d048b 100644
--- a/ql/src/test/results/compiler/plan/join2.q.xml
+++ b/ql/src/test/results/compiler/plan/join2.q.xml
@@ -8,7 +8,7 @@
       <void property="childTasks"> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
-         <object id="MoveTask0" class="org.apache.hadoop.hive.ql.exec.MoveTask"> 
+         <object class="org.apache.hadoop.hive.ql.exec.MoveTask"> 
           <void property="id"> 
            <string>Stage-1</string> 
           </void> 
@@ -17,9 +17,6 @@
             <void method="add"> 
              <object idref="MapRedTask1"/> 
             </void> 
-            <void method="add"> 
-             <object idref="MapRedTask0"/> 
-            </void> 
            </object> 
           </void> 
           <void property="work"> 
@@ -38,7 +35,7 @@
                  <boolean>true</boolean> 
                 </void> 
                 <void property="sourceDir"> 
-                 <string>/tmp/hive-njain/303286777.10000.insclause-0</string> 
+                 <string>/tmp/hive-njain/441145354.10000.insclause-0</string> 
                 </void> 
                 <void property="table"> 
                  <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -87,7 +84,7 @@
                     </void> 
                     <void method="put"> 
                      <string>location</string> 
-                     <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                     <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                     </void> 
                    </object> 
                   </void> 
@@ -122,7 +119,7 @@
          <object class="java.util.HashMap"> 
           <void method="put"> 
            <string>src3</string> 
-           <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+           <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
             <void property="childOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
@@ -192,6 +189,10 @@
                        <string>name</string> 
                        <string>binary_sortable_table</string> 
                       </void> 
+                      <void method="put"> 
+                       <string>serialization.sort.order</string> 
+                       <string>+</string> 
+                      </void> 
                       <void method="put"> 
                        <string>serialization.ddl</string> 
                        <string>struct binary_sortable_table { double joinkey0}</string> 
@@ -271,7 +272,7 @@
                 <void property="parentOperators"> 
                  <object class="java.util.ArrayList"> 
                   <void method="add"> 
-                   <object idref="ForwardOperator0"/> 
+                   <object idref="TableScanOperator0"/> 
                   </void> 
                  </object> 
                 </void> 
@@ -458,6 +459,10 @@
                    <string>name</string> 
                    <string>binary_sortable_table</string> 
                   </void> 
+                  <void method="put"> 
+                   <string>serialization.sort.order</string> 
+                   <string>+</string> 
+                  </void> 
                   <void method="put"> 
                    <string>serialization.ddl</string> 
                    <string>struct binary_sortable_table { double joinkey0}</string> 
@@ -542,7 +547,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/547407703/58605415.10001</string> 
+                       <string>/tmp/hive-njain/709768097/233295454.10001</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object id="tableDesc4" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -700,6 +705,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -795,7 +804,7 @@
                         <void property="parentOperators"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object id="ForwardOperator1" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                           <object id="TableScanOperator1" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
                             <void property="childOperators"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -912,6 +921,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -1010,7 +1023,7 @@
                         <void property="parentOperators"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object id="ForwardOperator2" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                           <object id="TableScanOperator2" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
                             <void property="childOperators"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -1142,7 +1155,7 @@
         <void property="pathToAliases"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/547407703/58605415.10001</string> 
+           <string>/tmp/hive-njain/709768097/233295454.10001</string> 
            <object class="java.util.ArrayList"> 
             <void method="add"> 
              <string>$INTNAME</string> 
@@ -1150,7 +1163,7 @@
            </object> 
           </void> 
           <void method="put"> 
-           <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+           <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
            <object class="java.util.ArrayList"> 
             <void method="add"> 
              <string>src3</string> 
@@ -1162,7 +1175,7 @@
         <void property="pathToPartitionInfo"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/547407703/58605415.10001</string> 
+           <string>/tmp/hive-njain/709768097/233295454.10001</string> 
            <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
             <void property="tableDesc"> 
              <object idref="tableDesc4"/> 
@@ -1170,7 +1183,7 @@
            </object> 
           </void> 
           <void method="put"> 
-           <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+           <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
            <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
             <void property="partSpec"> 
              <object class="java.util.LinkedHashMap"/> 
@@ -1222,7 +1235,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
                 </void> 
                </object> 
               </void> 
@@ -1248,7 +1261,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/303286777.10000.insclause-0</string> 
+                     <string>/tmp/hive-njain/441145354.10000.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object idref="tableDesc0"/> 
@@ -1483,9 +1496,6 @@
       </void> 
      </object> 
     </void> 
-    <void method="add"> 
-     <object idref="MoveTask0"/> 
-    </void> 
    </object> 
   </void> 
   <void property="id"> 
@@ -1497,11 +1507,11 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src2</string> 
-       <object idref="ForwardOperator2"/> 
+       <object idref="TableScanOperator2"/> 
       </void> 
       <void method="put"> 
        <string>src1</string> 
-       <object idref="ForwardOperator1"/> 
+       <object idref="TableScanOperator1"/> 
       </void> 
      </object> 
     </void> 
@@ -1514,7 +1524,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src2</string> 
@@ -1529,7 +1539,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -1581,7 +1591,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/join3.q.xml b/ql/src/test/results/compiler/plan/join3.q.xml
index 4bec2a3a74..9934aa6de4 100644
--- a/ql/src/test/results/compiler/plan/join3.q.xml
+++ b/ql/src/test/results/compiler/plan/join3.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/1136932164.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/268045919.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src2</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -154,6 +154,10 @@
                        <string>name</string> 
                        <string>binary_sortable_table</string> 
                       </void> 
+                      <void method="put"> 
+                       <string>serialization.sort.order</string> 
+                       <string>+</string> 
+                      </void> 
                       <void method="put"> 
                        <string>serialization.ddl</string> 
                        <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -270,7 +274,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -328,7 +332,7 @@
       </void> 
       <void method="put"> 
        <string>src3</string> 
-       <object id="ForwardOperator1" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator1" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -366,6 +370,10 @@
                    <string>name</string> 
                    <string>binary_sortable_table</string> 
                   </void> 
+                  <void method="put"> 
+                   <string>serialization.sort.order</string> 
+                   <string>+</string> 
+                  </void> 
                   <void method="put"> 
                    <string>serialization.ddl</string> 
                    <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -445,7 +453,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator1"/> 
+               <object idref="TableScanOperator1"/> 
               </void> 
              </object> 
             </void> 
@@ -513,7 +521,7 @@
       </void> 
       <void method="put"> 
        <string>src1</string> 
-       <object id="ForwardOperator2" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator2" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -555,6 +563,10 @@
                        <string>name</string> 
                        <string>binary_sortable_table</string> 
                       </void> 
+                      <void method="put"> 
+                       <string>serialization.sort.order</string> 
+                       <string>+</string> 
+                      </void> 
                       <void method="put"> 
                        <string>serialization.ddl</string> 
                        <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -668,7 +680,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator2"/> 
+               <object idref="TableScanOperator2"/> 
               </void> 
              </object> 
             </void> 
@@ -735,7 +747,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src2</string> 
@@ -753,7 +765,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -805,7 +817,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -831,7 +843,7 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>/tmp/hive-njain/1136932164.10000.insclause-0</string> 
+                 <string>/tmp/hive-njain/268045919.10000.insclause-0</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object idref="tableDesc0"/> 
diff --git a/ql/src/test/results/compiler/plan/join4.q.xml b/ql/src/test/results/compiler/plan/join4.q.xml
index 87de237924..e95df793ee 100644
--- a/ql/src/test/results/compiler/plan/join4.q.xml
+++ b/ql/src/test/results/compiler/plan/join4.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>c:b:src2</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -60,6 +60,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -378,7 +382,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -425,7 +429,7 @@
       </void> 
       <void method="put"> 
        <string>c:a:src1</string> 
-       <object id="ForwardOperator1" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator1" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -471,6 +475,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -778,7 +786,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator1"/> 
+               <object idref="TableScanOperator1"/> 
               </void> 
              </object> 
             </void> 
@@ -834,7 +842,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -849,7 +857,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -901,7 +909,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -931,7 +939,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/170014316/124791502.10001.insclause-0</string> 
+                     <string>/tmp/hive-njain/291874807/86598252.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
diff --git a/ql/src/test/results/compiler/plan/join5.q.xml b/ql/src/test/results/compiler/plan/join5.q.xml
index 42e6d30a0d..68a13e17e1 100644
--- a/ql/src/test/results/compiler/plan/join5.q.xml
+++ b/ql/src/test/results/compiler/plan/join5.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>c:b:src2</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -60,6 +60,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -378,7 +382,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -425,7 +429,7 @@
       </void> 
       <void method="put"> 
        <string>c:a:src1</string> 
-       <object id="ForwardOperator1" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator1" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -471,6 +475,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -778,7 +786,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator1"/> 
+               <object idref="TableScanOperator1"/> 
               </void> 
              </object> 
             </void> 
@@ -834,7 +842,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -849,7 +857,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -901,7 +909,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -931,7 +939,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/198286603/204778341.10001.insclause-0</string> 
+                     <string>/tmp/hive-njain/1109739262/425333867.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
diff --git a/ql/src/test/results/compiler/plan/join6.q.xml b/ql/src/test/results/compiler/plan/join6.q.xml
index 512ca9cd7e..56a681b25a 100644
--- a/ql/src/test/results/compiler/plan/join6.q.xml
+++ b/ql/src/test/results/compiler/plan/join6.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>c:b:src2</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -60,6 +60,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -378,7 +382,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -425,7 +429,7 @@
       </void> 
       <void method="put"> 
        <string>c:a:src1</string> 
-       <object id="ForwardOperator1" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator1" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -471,6 +475,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -778,7 +786,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator1"/> 
+               <object idref="TableScanOperator1"/> 
               </void> 
              </object> 
             </void> 
@@ -834,7 +842,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -849,7 +857,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -901,7 +909,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -931,7 +939,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/113474329/1108575385.10001.insclause-0</string> 
+                     <string>/tmp/hive-njain/166521027/637281771.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
diff --git a/ql/src/test/results/compiler/plan/join7.q.xml b/ql/src/test/results/compiler/plan/join7.q.xml
index 4affacfd6d..ecc9be7ef5 100644
--- a/ql/src/test/results/compiler/plan/join7.q.xml
+++ b/ql/src/test/results/compiler/plan/join7.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>c:b:src2</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -60,6 +60,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -378,7 +382,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -425,7 +429,7 @@
       </void> 
       <void method="put"> 
        <string>c:a:src1</string> 
-       <object id="ForwardOperator1" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator1" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -471,6 +475,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -778,7 +786,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator1"/> 
+               <object idref="TableScanOperator1"/> 
               </void> 
              </object> 
             </void> 
@@ -825,7 +833,7 @@
       </void> 
       <void method="put"> 
        <string>c:c:src3</string> 
-       <object id="ForwardOperator2" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator2" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -871,6 +879,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -1181,7 +1193,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator2"/> 
+               <object idref="TableScanOperator2"/> 
               </void> 
              </object> 
             </void> 
@@ -1237,7 +1249,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -1255,7 +1267,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -1307,7 +1319,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -1337,7 +1349,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/604172747/190204720.10001.insclause-0</string> 
+                     <string>/tmp/hive-njain/759592261/286929524.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
diff --git a/ql/src/test/results/compiler/plan/join8.q.xml b/ql/src/test/results/compiler/plan/join8.q.xml
index a01d9b2ce7..486420a4d6 100644
--- a/ql/src/test/results/compiler/plan/join8.q.xml
+++ b/ql/src/test/results/compiler/plan/join8.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>c:b:src2</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -60,6 +60,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -378,7 +382,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -425,7 +429,7 @@
       </void> 
       <void method="put"> 
        <string>c:a:src1</string> 
-       <object id="ForwardOperator1" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator1" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -471,6 +475,10 @@
                            <string>name</string> 
                            <string>binary_sortable_table</string> 
                           </void> 
+                          <void method="put"> 
+                           <string>serialization.sort.order</string> 
+                           <string>+</string> 
+                          </void> 
                           <void method="put"> 
                            <string>serialization.ddl</string> 
                            <string>struct binary_sortable_table { string joinkey0}</string> 
@@ -778,7 +786,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator1"/> 
+               <object idref="TableScanOperator1"/> 
               </void> 
              </object> 
             </void> 
@@ -834,7 +842,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -849,7 +857,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -901,7 +909,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -935,7 +943,7 @@
                       <void property="conf"> 
                        <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                         <void property="dirName"> 
-                         <string>/tmp/hive-njain/378159414/850135187.10001.insclause-0</string> 
+                         <string>/tmp/hive-njain/19854159/32645100.10001.insclause-0</string> 
                         </void> 
                         <void property="tableInfo"> 
                          <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
diff --git a/ql/src/test/results/compiler/plan/sample1.q.xml b/ql/src/test/results/compiler/plan/sample1.q.xml
index 086c73b87f..9661aef3ad 100644
--- a/ql/src/test/results/compiler/plan/sample1.q.xml
+++ b/ql/src/test/results/compiler/plan/sample1.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>s</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -30,7 +30,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-njain/57674480/1824861482.10001.insclause-0</string> 
+                           <string>/tmp/hive-njain/425404034/89912765.10001.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -548,7 +548,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -570,7 +570,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -582,7 +582,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"> 
@@ -647,7 +647,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcpart</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample2.q.xml b/ql/src/test/results/compiler/plan/sample2.q.xml
index 641c843c7a..4027fbca27 100644
--- a/ql/src/test/results/compiler/plan/sample2.q.xml
+++ b/ql/src/test/results/compiler/plan/sample2.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/186273508/11889374.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/20107464/1379933163.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>s</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -120,7 +120,7 @@
                 <void property="conf"> 
                  <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                   <void property="dirName"> 
-                   <string>/tmp/hive-njain/186273508/11889374.10000.insclause-0</string> 
+                   <string>/tmp/hive-njain/20107464/1379933163.10000.insclause-0</string> 
                   </void> 
                   <void property="tableInfo"> 
                    <object idref="tableDesc0"/> 
@@ -204,7 +204,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -254,7 +254,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -266,7 +266,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -322,7 +322,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample3.q.xml b/ql/src/test/results/compiler/plan/sample3.q.xml
index 422d63356a..1072988419 100644
--- a/ql/src/test/results/compiler/plan/sample3.q.xml
+++ b/ql/src/test/results/compiler/plan/sample3.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/164819642/256272558.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/1789469168/381631340.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>s</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -124,7 +124,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/164819642/256272558.10000.insclause-0</string> 
+                       <string>/tmp/hive-njain/1789469168/381631340.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -396,7 +396,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -439,7 +439,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -451,7 +451,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -507,7 +507,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample4.q.xml b/ql/src/test/results/compiler/plan/sample4.q.xml
index 7bd393e6e7..ff8923490e 100644
--- a/ql/src/test/results/compiler/plan/sample4.q.xml
+++ b/ql/src/test/results/compiler/plan/sample4.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/85259061/94539507.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/1853542808/45744403.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>s</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -120,7 +120,7 @@
                 <void property="conf"> 
                  <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                   <void property="dirName"> 
-                   <string>/tmp/hive-njain/85259061/94539507.10000.insclause-0</string> 
+                   <string>/tmp/hive-njain/1853542808/45744403.10000.insclause-0</string> 
                   </void> 
                   <void property="tableInfo"> 
                    <object idref="tableDesc0"/> 
@@ -204,7 +204,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -254,7 +254,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -266,7 +266,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -322,7 +322,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample5.q.xml b/ql/src/test/results/compiler/plan/sample5.q.xml
index 3fd993aa3b..9e6c776047 100644
--- a/ql/src/test/results/compiler/plan/sample5.q.xml
+++ b/ql/src/test/results/compiler/plan/sample5.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/423510659/355253855.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/824806517/591440948.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>s</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -124,7 +124,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/423510659/355253855.10000.insclause-0</string> 
+                       <string>/tmp/hive-njain/824806517/591440948.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -383,7 +383,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -426,7 +426,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -438,7 +438,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -494,7 +494,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample6.q.xml b/ql/src/test/results/compiler/plan/sample6.q.xml
index f3a8ae5386..ae4b318974 100644
--- a/ql/src/test/results/compiler/plan/sample6.q.xml
+++ b/ql/src/test/results/compiler/plan/sample6.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/404713103/151871838.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/814020855/624762724.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>s</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -124,7 +124,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/404713103/151871838.10000.insclause-0</string> 
+                       <string>/tmp/hive-njain/814020855/624762724.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -383,7 +383,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -426,7 +426,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -438,7 +438,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -494,7 +494,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample7.q.xml b/ql/src/test/results/compiler/plan/sample7.q.xml
index 01e35b1520..f3c771d7fa 100644
--- a/ql/src/test/results/compiler/plan/sample7.q.xml
+++ b/ql/src/test/results/compiler/plan/sample7.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/1242963668/223671734.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/475551879/391349136.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -108,7 +108,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>s</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -128,7 +128,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-njain/1242963668/223671734.10000.insclause-0</string> 
+                           <string>/tmp/hive-njain/475551879/391349136.10000.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object idref="tableDesc0"/> 
@@ -477,7 +477,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -499,7 +499,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -511,7 +511,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -567,7 +567,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/subq.q.xml b/ql/src/test/results/compiler/plan/subq.q.xml
index bbca811d51..5a560ae306 100644
--- a/ql/src/test/results/compiler/plan/subq.q.xml
+++ b/ql/src/test/results/compiler/plan/subq.q.xml
@@ -28,7 +28,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-athusoo/853032200/122389140.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/276583968/36258042.10000.insclause-0</string> 
             </void> 
             <void property="targetDir"> 
              <string>../build/ql/test/data/warehouse/union.out</string> 
@@ -55,7 +55,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>unioninput:src</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -75,7 +75,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-athusoo/853032200/122389140.10000.insclause-0</string> 
+                           <string>/tmp/hive-njain/276583968/36258042.10000.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -328,7 +328,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -378,7 +378,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>unioninput:src</string> 
@@ -390,7 +390,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -442,7 +442,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/udf1.q.xml b/ql/src/test/results/compiler/plan/udf1.q.xml
index 87959c2d2a..448b4d0ed7 100644
--- a/ql/src/test/results/compiler/plan/udf1.q.xml
+++ b/ql/src/test/results/compiler/plan/udf1.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>src</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -30,7 +30,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-njain/530183626.10001.insclause-0</string> 
+                           <string>/tmp/hive-njain/487094769.10001.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1238,7 +1238,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -1288,7 +1288,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -1300,7 +1300,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -1352,7 +1352,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/udf4.q.xml b/ql/src/test/results/compiler/plan/udf4.q.xml
index 7f6e9398e1..b43d05c55d 100644
--- a/ql/src/test/results/compiler/plan/udf4.q.xml
+++ b/ql/src/test/results/compiler/plan/udf4.q.xml
@@ -10,7 +10,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>dest1</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -26,7 +26,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/126141838.10001.insclause-0</string> 
+                       <string>/tmp/hive-njain/1170725444.10001.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -948,7 +948,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -998,7 +998,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>dest1</string> 
@@ -1010,7 +1010,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -1062,7 +1062,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/dest1</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/union.q.xml b/ql/src/test/results/compiler/plan/union.q.xml
index 5c9ffe1c17..af0b1d87c6 100644
--- a/ql/src/test/results/compiler/plan/union.q.xml
+++ b/ql/src/test/results/compiler/plan/union.q.xml
@@ -28,7 +28,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-athusoo/37064530/676639755.10000.insclause-0</string> 
+             <string>/tmp/hive-njain/1238482884/612877942.10000.insclause-0</string> 
             </void> 
             <void property="targetDir"> 
              <string>../build/ql/test/data/warehouse/union.out</string> 
@@ -55,7 +55,7 @@
      <object class="java.util.HashMap"> 
       <void method="put"> 
        <string>null-subquery1:unioninput-subquery1:src</string> 
-       <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+       <object id="TableScanOperator0" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
         <void property="childOperators"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -67,7 +67,7 @@
                 <void property="childOperators"> 
                  <object id="ArrayList0" class="java.util.ArrayList"> 
                   <void method="add"> 
-                   <object id="ForwardOperator1" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                   <object id="ForwardOperator0" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
                     <void property="childOperators"> 
                      <object class="java.util.ArrayList"> 
                       <void method="add"> 
@@ -79,7 +79,7 @@
                             <void property="conf"> 
                              <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                               <void property="dirName"> 
-                               <string>/tmp/hive-athusoo/37064530/676639755.10000.insclause-0</string> 
+                               <string>/tmp/hive-njain/1238482884/612877942.10000.insclause-0</string> 
                               </void> 
                               <void property="tableInfo"> 
                                <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -185,7 +185,7 @@
                         <void property="parentOperators"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object idref="ForwardOperator1"/> 
+                           <object idref="ForwardOperator0"/> 
                           </void> 
                          </object> 
                         </void> 
@@ -314,7 +314,7 @@
                             <void property="parentOperators"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
-                               <object id="ForwardOperator2" class="org.apache.hadoop.hive.ql.exec.ForwardOperator"> 
+                               <object id="TableScanOperator1" class="org.apache.hadoop.hive.ql.exec.TableScanOperator"> 
                                 <void property="childOperators"> 
                                  <object class="java.util.ArrayList"> 
                                   <void method="add"> 
@@ -536,7 +536,7 @@
             <void property="parentOperators"> 
              <object class="java.util.ArrayList"> 
               <void method="add"> 
-               <object idref="ForwardOperator0"/> 
+               <object idref="TableScanOperator0"/> 
               </void> 
              </object> 
             </void> 
@@ -583,14 +583,14 @@
       </void> 
       <void method="put"> 
        <string>null-subquery2:unioninput-subquery2:src</string> 
-       <object idref="ForwardOperator2"/> 
+       <object idref="TableScanOperator1"/> 
       </void> 
      </object> 
     </void> 
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>null-subquery1:unioninput-subquery1:src</string> 
@@ -605,7 +605,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -657,7 +657,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/data/users/athusoo/apacheprojects/hive_local_ws1/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
