diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index e635c8361d..602626f769 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -127,7 +127,7 @@ public class Driver implements CommandProcessor {
 
   // A limit on the number of threads that can be launched
   private int maxthreads;
-  private final int sleeptime = 2000;
+  private static final int SLEEP_TIME = 2000;
   protected int tryCount = Integer.MAX_VALUE;
 
   private boolean checkLockManager() {
@@ -1287,7 +1287,7 @@ public TaskResult pollTasks(Set<TaskResult> results) {
       // In this loop, nothing was found
       // Sleep 10 seconds and restart
       try {
-        Thread.sleep(sleeptime);
+        Thread.sleep(SLEEP_TIME);
       } catch (InterruptedException ie) {
         // Do Nothing
         ;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java
index 7ac5b7a9a0..9391acd3af 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java
@@ -20,6 +20,8 @@
 
 import java.io.Serializable;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
@@ -38,6 +40,8 @@ public class CopyTask extends Task<CopyWork> implements Serializable {
 
   private static final long serialVersionUID = 1L;
 
+  private static transient final Log LOG = LogFactory.getLog(CopyTask.class);
+
   public CopyTask() {
     super();
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
index 506d0791e1..e17de01d5e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
@@ -99,12 +99,13 @@ public class ExecDriver extends Task<MapredWork> implements Serializable, Hadoop
   public static MemoryMXBean memoryMXBean;
   protected HadoopJobExecHelper jobExecHelper;
 
+  protected static transient final Log LOG = LogFactory.getLog(ExecDriver.class);
+
   /**
    * Constructor when invoked from QL.
    */
   public ExecDriver() {
     super();
-    LOG = LogFactory.getLog(this.getClass().getName());
     console = new LogHelper(LOG);
     this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);
   }
@@ -178,7 +179,6 @@ public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext driverC
   public ExecDriver(MapredWork plan, JobConf job, boolean isSilent) throws HiveException {
     setWork(plan);
     this.job = job;
-    LOG = LogFactory.getLog(this.getClass().getName());
     console = new LogHelper(LOG, isSilent);
     this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
index 5c71270728..f8373a37d6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
@@ -23,6 +23,8 @@
 import java.util.ArrayList;
 import java.util.Properties;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.CommandNeedRetryException;
@@ -52,6 +54,7 @@ public class FetchTask extends Task<FetchWork> implements Serializable {
   private FetchOperator ftOp;
   private SerDe mSerde;
   private int totalRows;
+  private static transient final Log LOG = LogFactory.getLog(FetchTask.class);
 
   public FetchTask() {
     super();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
index 16a207e567..13584c1f94 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
@@ -20,7 +20,6 @@
 
 import java.lang.reflect.Method;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedHashMap;
@@ -122,7 +121,6 @@
 import org.apache.hadoop.hive.ql.udf.UDFWeekOfYear;
 import org.apache.hadoop.hive.ql.udf.UDFYear;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage;
-import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams;
@@ -130,6 +128,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovarianceSample;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;
@@ -147,13 +146,13 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFArray;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFArrayContains;
-import org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapAnd;
-import org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapOr;
-import org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapEmpty;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFCase;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFCoalesce;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapAnd;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapEmpty;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapOr;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFElt;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFField;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash;
@@ -210,7 +209,7 @@
  */
 public final class FunctionRegistry {
 
-  private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.exec.FunctionRegistry");
+  private static Log LOG = LogFactory.getLog(FunctionRegistry.class);
 
   /**
    * The mapping from expression function names to expression classes.
@@ -707,7 +706,7 @@ public static <T> Method getMethodInternal(Class<? extends T> udfClass,
 
     List<Method> mlist = new ArrayList<Method>();
 
-    for (Method m : Arrays.asList(udfClass.getMethods())) {
+    for (Method m : udfClass.getMethods()) {
       if (m.getName().equals(methodName)) {
         mlist.add(m);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java
index 225c5c7a9b..5c6ea07e9d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java
@@ -22,8 +22,8 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.DriverContext;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.DriverContext;
 import org.apache.hadoop.hive.ql.QueryPlan;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.CreateFunctionDesc;
@@ -42,7 +42,7 @@
  */
 public class FunctionTask extends Task<FunctionWork> {
   private static final long serialVersionUID = 1L;
-  private static final Log LOG = LogFactory.getLog("hive.ql.exec.FunctionTask");
+  private static transient final Log LOG = LogFactory.getLog(FunctionTask.class);
 
   transient HiveConf conf;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java
index 691f0389af..3d5e95d495 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java
@@ -64,11 +64,12 @@ public class MapredLocalTask extends Task<MapredLocalWork> implements Serializab
 
   private Map<String, FetchOperator> fetchOperators;
   private JobConf job;
-  public static final Log l4j = LogFactory.getLog("MapredLocalTask");
+  public static transient final Log l4j = LogFactory.getLog(MapredLocalTask.class);
   static final String HADOOP_MEM_KEY = "HADOOP_HEAPSIZE";
   static final String HADOOP_OPTS_KEY = "HADOOP_OPTS";
   static final String[] HIVE_SYS_PROP = {"build.dir", "build.dir.hive"};
   public static MemoryMXBean memoryMXBean;
+  private static final Log LOG = LogFactory.getLog(MapredLocalTask.class);
 
   // not sure we need this exec context; but all the operators in the work
   // will pass this context throught
@@ -81,7 +82,6 @@ public MapredLocalTask() {
   public MapredLocalTask(MapredLocalWork plan, JobConf job, boolean isSilent) throws HiveException {
     setWork(plan);
     this.job = job;
-    LOG = LogFactory.getLog(this.getClass().getName());
     console = new LogHelper(LOG, isSilent);
   }
 
@@ -97,6 +97,7 @@ public static String now() {
     return sdf.format(cal.getTime());
   }
 
+  @Override
   public boolean requireLock() {
     return true;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index 1afb3d3639..326bb61a02 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -28,6 +28,8 @@
 import java.util.List;
 import java.util.Map;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.LocalFileSystem;
@@ -35,8 +37,8 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.DriverContext;
-import org.apache.hadoop.hive.ql.hooks.LineageInfo.DataContainer;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
+import org.apache.hadoop.hive.ql.hooks.LineageInfo.DataContainer;
 import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
@@ -56,6 +58,7 @@
 public class MoveTask extends Task<MoveWork> implements Serializable {
 
   private static final long serialVersionUID = 1L;
+  private static transient final Log LOG = LogFactory.getLog(MoveTask.class);
 
   public MoveTask() {
     super();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
index a6bce8183e..d4d0af89cc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
@@ -26,6 +26,8 @@
 import java.util.List;
 import java.util.Map;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -54,6 +56,7 @@
 public class StatsTask extends Task<StatsWork> implements Serializable {
 
   private static final long serialVersionUID = 1L;
+  private static transient final Log LOG = LogFactory.getLog(StatsTask.class);
 
   private Table table;
   private List<LinkedHashMap<String, String>> dpPartSpecs;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
index b0a02b14ee..8868679be9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
@@ -53,7 +53,6 @@ public abstract class Task<T extends Serializable> implements Serializable, Node
   protected transient boolean queued;
   protected transient HiveConf conf;
   protected transient Hive db;
-  protected static transient Log LOG;
   protected transient LogHelper console;
   protected transient QueryPlan queryPlan;
   protected transient TaskHandle taskHandle;
@@ -62,6 +61,7 @@ public abstract class Task<T extends Serializable> implements Serializable, Node
   protected transient boolean clonedConf = false;
   protected Task<? extends Serializable> backupTask;
   protected List<Task<? extends Serializable>> backupChildrenTasks = new ArrayList<Task<? extends Serializable>>();
+  protected static transient Log LOG = LogFactory.getLog(Task.class);
   protected int taskTag;
   private boolean isLocalMode =false;
   private boolean retryCmdWhenFail = false;
@@ -91,7 +91,6 @@ public Task() {
     started = false;
     initialized = false;
     queued = false;
-    LOG = LogFactory.getLog(this.getClass().getName());
     this.taskCounters = new HashMap<String, Long>();
     taskTag = Task.NO_TAG;
   }
@@ -474,7 +473,7 @@ public boolean isLocalMode() {
   public void setLocalMode(boolean isLocalMode) {
     this.isLocalMode = isLocalMode;
   }
-  
+
   public boolean requireLock() {
     return false;
   }
@@ -486,11 +485,11 @@ public boolean ifRetryCmdWhenFail() {
   public void setRetryCmdWhenFail(boolean retryCmdWhenFail) {
     this.retryCmdWhenFail = retryCmdWhenFail;
   }
-  
+
   public QueryPlan getQueryPlan() {
     return queryPlan;
   }
-  
+
   public void setQueryPlan(QueryPlan queryPlan) {
     this.queryPlan = queryPlan;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 337a8eb9a8..3054f76435 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -26,7 +26,6 @@
 
 import java.io.Serializable;
 import java.net.URI;
-import java.net.URISyntaxException;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -121,7 +120,7 @@
  *
  */
 public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
-  private static final Log LOG = LogFactory.getLog("hive.ql.parse.DDLSemanticAnalyzer");
+  private static final Log LOG = LogFactory.getLog(DDLSemanticAnalyzer.class);
   private static final Map<Integer, String> TokenToTypeName = new HashMap<Integer, String>();
 
   private final Set<String> reservedPartitionValues;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
index e90da626e0..f2b70187f3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
@@ -36,7 +36,7 @@
  */
 public class FunctionSemanticAnalyzer extends BaseSemanticAnalyzer {
   private static final Log LOG = LogFactory
-      .getLog("hive.ql.parse.FunctionSemanticAnalyzer");
+      .getLog(FunctionSemanticAnalyzer.class);
 
   public FunctionSemanticAnalyzer(HiveConf conf) throws SemanticException {
     super(conf);
