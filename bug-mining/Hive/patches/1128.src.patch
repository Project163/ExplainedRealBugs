diff --git a/cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java b/cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java
index 70fd1a9562..0af27b27ae 100644
--- a/cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java
+++ b/cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java
@@ -76,6 +76,7 @@ public int run(String[] args) throws Exception {
     boolean verbose = false;
     boolean columnSizes = false;
     boolean pretty = false;
+    boolean fileSizes = false;
 
     //get options from arguments
     if (args.length < 1 || args.length > 3) {
@@ -95,6 +96,8 @@ public int run(String[] args) throws Exception {
       } else if (arg.equals("--column-sizes-pretty")) {
         columnSizes = true;
         pretty = true;
+      } else if (arg.equals("--file-sizes")){
+        fileSizes = true;
       } else if (fileName == null){
         fileName = new Path(arg);
       } else {
@@ -119,10 +122,14 @@ public int run(String[] args) throws Exception {
     FileSplit split = new FileSplit(fileName,start, length, new JobConf(conf));
     RCFileRecordReader recordReader = new RCFileRecordReader(conf, split);
 
-    if (columnSizes) {
+    if (columnSizes || fileSizes) {
       // Print out the un/compressed sizes of each column
       long[] compressedColumnSizes = null;
       long[] uncompressedColumnSizes = null;
+      // un/compressed sizes of file and no. of rows
+      long rowNo = 0;
+      long uncompressedFileSize = 0;
+      long compressedFileSize = 0;
       // Skip from block to block since we only need the header
       while (recordReader.nextBlock()) {
         // Get the sizes from the key buffer and aggregate
@@ -137,9 +144,10 @@ public int run(String[] args) throws Exception {
           uncompressedColumnSizes[i] += keyBuffer.getEachColumnUncompressedValueLen()[i];
           compressedColumnSizes[i] += keyBuffer.getEachColumnValueLen()[i];
         }
+        rowNo += keyBuffer.getNumberRows();
       }
 
-      if (uncompressedColumnSizes != null && compressedColumnSizes != null) {
+      if (columnSizes && uncompressedColumnSizes != null && compressedColumnSizes != null) {
         // Print out the sizes, if pretty is set, print it out in a human friendly format,
         // otherwise print it out as if it were a row
         for (int i = 0; i < uncompressedColumnSizes.length; i++) {
@@ -153,6 +161,18 @@ public int run(String[] args) throws Exception {
         }
       }
 
+      if (fileSizes) {
+        if (uncompressedColumnSizes != null && compressedColumnSizes != null) {
+          for (int i = 0; i < uncompressedColumnSizes.length; i++) {
+            uncompressedFileSize += uncompressedColumnSizes[i];
+            compressedFileSize += compressedColumnSizes[i];
+          }
+        }
+        System.out.print("File size (uncompressed): " + uncompressedFileSize
+            + ". File size (compressed): " + compressedFileSize + ". Number of rows: " + rowNo
+            + "." + NEWLINE);
+      }
+
       System.out.flush();
       return 0;
     }
@@ -219,7 +239,7 @@ public void setConf(Configuration conf) {
   }
 
   private static String Usage = "RCFileCat [--start=start_offet] [--length=len] [--verbose] " +
-      "[--column-sizes | --column-sizes-pretty] fileName";
+      "[--column-sizes | --column-sizes-pretty] [--file-sizes] fileName";
 
   public static void main(String[] args) {
     try {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java b/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
index 8adea1ecd2..1afafdf56f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
@@ -325,6 +325,13 @@ public int[] getEachColumnUncompressedValueLen() {
     public int[] getEachColumnValueLen() {
       return eachColumnValueLen;
     }
+
+    /**
+     * @return the numberRows
+     */
+    public int getNumberRows() {
+      return numberRows;
+    }
   }
 
   /**
