diff --git a/hcatalog/conf/proto-hive-site.xml b/hcatalog/conf/proto-hive-site.xml
index 7437484815..dbfc955ac2 100644
--- a/hcatalog/conf/proto-hive-site.xml
+++ b/hcatalog/conf/proto-hive-site.xml
@@ -114,7 +114,7 @@
 
 <property>
   <name>hive.security.authorization.manager</name>
-  <value>org.apache.hive.hcatalog.security.StorageDelegationAuthorizationProvider</value>
+  <value>org.apache.hcatalog.security.StorageDelegationAuthorizationProvider</value>
   <description>the hive client authorization manager class name.
   The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.
   HCatalog uses a model, where authorization checks are delegated to the storage layer (hdfs, hbase, ...).
diff --git a/hcatalog/core/src/main/java/org/apache/hcatalog/security/HdfsAuthorizationProvider.java b/hcatalog/core/src/main/java/org/apache/hcatalog/security/HdfsAuthorizationProvider.java
index c939f109ee..50734cb78d 100644
--- a/hcatalog/core/src/main/java/org/apache/hcatalog/security/HdfsAuthorizationProvider.java
+++ b/hcatalog/core/src/main/java/org/apache/hcatalog/security/HdfsAuthorizationProvider.java
@@ -56,7 +56,7 @@
  * An AuthorizationProvider, which checks against the data access level permissions on HDFS.
  * It makes sense to eventually move this class to Hive, so that all hive users can
  * use this authorization model. 
- * @deprecated Use/modify {@link org.apache.hive.hcatalog.security.HdfsAuthorizationProvider} instead
+ * @deprecated use {@link org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider}
  */
 public class HdfsAuthorizationProvider extends HiveAuthorizationProviderBase {
 
diff --git a/hcatalog/core/src/main/java/org/apache/hcatalog/security/StorageDelegationAuthorizationProvider.java b/hcatalog/core/src/main/java/org/apache/hcatalog/security/StorageDelegationAuthorizationProvider.java
index 711c471b07..9129fd8475 100644
--- a/hcatalog/core/src/main/java/org/apache/hcatalog/security/StorageDelegationAuthorizationProvider.java
+++ b/hcatalog/core/src/main/java/org/apache/hcatalog/security/StorageDelegationAuthorizationProvider.java
@@ -42,7 +42,7 @@
 /**
  * A HiveAuthorizationProvider which delegates the authorization requests to 
  * the underlying AuthorizationProviders obtained from the StorageHandler.
- * @deprecated Use/modify {@link org.apache.hive.hcatalog.security.StorageDelegationAuthorizationProvider} instead
+ * @deprecated 
  */
 public class StorageDelegationAuthorizationProvider extends HiveAuthorizationProviderBase {
 
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/security/HdfsAuthorizationProvider.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/security/HdfsAuthorizationProvider.java
deleted file mode 100644
index 3233a451f9..0000000000
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/security/HdfsAuthorizationProvider.java
+++ /dev/null
@@ -1,337 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hive.hcatalog.security;
-
-import static org.apache.hadoop.hive.metastore.MetaStoreUtils.DEFAULT_DATABASE_NAME;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.EnumSet;
-import java.util.List;
-
-import javax.security.auth.login.LoginException;
-
-import org.apache.commons.lang.ArrayUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsAction;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.Warehouse;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.ql.metadata.AuthorizationException;
-import org.apache.hadoop.hive.ql.metadata.Hive;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.metadata.Partition;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
-import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase;
-import org.apache.hadoop.hive.ql.security.authorization.Privilege;
-import org.apache.hadoop.hive.shims.HadoopShims;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.security.AccessControlException;
-import org.apache.hadoop.security.UserGroupInformation;
-
-/**
- * An AuthorizationProvider, which checks against the data access level permissions on HDFS.
- * It makes sense to eventually move this class to Hive, so that all hive users can
- * use this authorization model. 
- */
-public class HdfsAuthorizationProvider extends HiveAuthorizationProviderBase {
-
-  protected Warehouse wh;
-
-  //Config variables : create an enum to store them if we have more
-  private static final String PROXY_USER_NAME = "proxy.user.name";
-
-  public HdfsAuthorizationProvider() {
-    super();
-  }
-
-  public HdfsAuthorizationProvider(Configuration conf) {
-    super();
-    setConf(conf);
-  }
-
-  @Override
-  public void init(Configuration conf) throws HiveException {
-    hive_db = new HiveProxy(Hive.get(new HiveConf(conf, HiveAuthorizationProvider.class)));
-  }
-
-  @Override
-  public void setConf(Configuration conf) {
-    super.setConf(conf);
-    try {
-      this.wh = new Warehouse(conf);
-    } catch (MetaException ex) {
-      throw new RuntimeException(ex);
-    }
-  }
-
-  protected FsAction getFsAction(Privilege priv, Path path) {
-
-    switch (priv.getPriv()) {
-    case ALL:
-      throw new AuthorizationException("no matching Action for Privilege.All");
-    case ALTER_DATA:
-      return FsAction.WRITE;
-    case ALTER_METADATA:
-      return FsAction.WRITE;
-    case CREATE:
-      return FsAction.WRITE;
-    case DROP:
-      return FsAction.WRITE;
-    case INDEX:
-      return FsAction.WRITE;
-    case LOCK:
-      return FsAction.WRITE;
-    case SELECT:
-      return FsAction.READ;
-    case SHOW_DATABASE:
-      return FsAction.READ;
-    case UNKNOWN:
-    default:
-      throw new AuthorizationException("Unknown privilege");
-    }
-  }
-
-  protected EnumSet<FsAction> getFsActions(Privilege[] privs, Path path) {
-    EnumSet<FsAction> actions = EnumSet.noneOf(FsAction.class);
-
-    if (privs == null) {
-      return actions;
-    }
-
-    for (Privilege priv : privs) {
-      actions.add(getFsAction(priv, path));
-    }
-
-    return actions;
-  }
-
-  private static final String DATABASE_WAREHOUSE_SUFFIX = ".db";
-
-  private Path getDefaultDatabasePath(String dbName) throws MetaException {
-    if (dbName.equalsIgnoreCase(DEFAULT_DATABASE_NAME)) {
-      return wh.getWhRoot();
-    }
-    return new Path(wh.getWhRoot(), dbName.toLowerCase() + DATABASE_WAREHOUSE_SUFFIX);
-  }
-
-  protected Path getDbLocation(Database db) throws HiveException {
-    try {
-      String location = db.getLocationUri();
-      if (location == null) {
-        return getDefaultDatabasePath(db.getName());
-      } else {
-        return wh.getDnsPath(wh.getDatabasePath(db));
-      }
-    } catch (MetaException ex) {
-      throw new HiveException(ex.getMessage());
-    }
-  }
-
-  @Override
-  public void authorize(Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-    throws HiveException, AuthorizationException {
-    //Authorize for global level permissions at the warehouse dir
-    Path root;
-    try {
-      root = wh.getWhRoot();
-      authorize(root, readRequiredPriv, writeRequiredPriv);
-    } catch (MetaException ex) {
-      throw new HiveException(ex);
-    }
-  }
-
-  @Override
-  public void authorize(Database db, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-    throws HiveException, AuthorizationException {
-    if (db == null) {
-      return;
-    }
-
-    Path path = getDbLocation(db);
-
-    authorize(path, readRequiredPriv, writeRequiredPriv);
-  }
-
-  @Override
-  public void authorize(Table table, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-    throws HiveException, AuthorizationException {
-    if (table == null) {
-      return;
-    }
-
-    //unlike Hive's model, this can be called at CREATE TABLE as well, since we should authorize
-    //against the table's declared location
-    Path path = null;
-    try {
-      if (table.getTTable().getSd().getLocation() == null
-        || table.getTTable().getSd().getLocation().isEmpty()) {
-        path = wh.getTablePath(hive_db.getDatabase(table.getDbName()), table.getTableName());
-      } else {
-        path = table.getPath();
-      }
-    } catch (MetaException ex) {
-      throw new HiveException(ex);
-    }
-
-    authorize(path, readRequiredPriv, writeRequiredPriv);
-  }
-
-  //TODO: HiveAuthorizationProvider should expose this interface instead of #authorize(Partition, Privilege[], Privilege[])
-  public void authorize(Table table, Partition part, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-    throws HiveException, AuthorizationException {
-
-    if (part == null || part.getLocation() == null) {
-      authorize(table, readRequiredPriv, writeRequiredPriv);
-    } else {
-      authorize(part.getPartitionPath(), readRequiredPriv, writeRequiredPriv);
-    }
-  }
-
-  @Override
-  public void authorize(Partition part, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-    throws HiveException, AuthorizationException {
-    if (part == null) {
-      return;
-    }
-    authorize(part.getTable(), part, readRequiredPriv, writeRequiredPriv);
-  }
-
-  @Override
-  public void authorize(Table table, Partition part, List<String> columns,
-              Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) throws HiveException,
-    AuthorizationException {
-    //columns cannot live in different files, just check for partition level permissions
-    authorize(table, part, readRequiredPriv, writeRequiredPriv);
-  }
-
-  /**
-   * Authorization privileges against a path.
-   * @param path a filesystem path
-   * @param readRequiredPriv a list of privileges needed for inputs.
-   * @param writeRequiredPriv a list of privileges needed for outputs.
-   */
-  public void authorize(Path path, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-    throws HiveException, AuthorizationException {
-    try {
-      EnumSet<FsAction> actions = getFsActions(readRequiredPriv, path);
-      actions.addAll(getFsActions(writeRequiredPriv, path));
-      if (actions.isEmpty()) {
-        return;
-      }
-
-      checkPermissions(getConf(), path, actions);
-
-    } catch (AccessControlException ex) {
-      throw new AuthorizationException(ex);
-    } catch (LoginException ex) {
-      throw new AuthorizationException(ex);
-    } catch (IOException ex) {
-      throw new HiveException(ex);
-    }
-  }
-
-  /**
-   * Checks the permissions for the given path and current user on Hadoop FS. If the given path
-   * does not exists, it checks for it's parent folder.
-   */
-  protected static void checkPermissions(final Configuration conf, final Path path,
-                       final EnumSet<FsAction> actions) throws IOException, LoginException {
-
-    if (path == null) {
-      throw new IllegalArgumentException("path is null");
-    }
-
-    HadoopShims shims = ShimLoader.getHadoopShims();
-    final UserGroupInformation ugi;
-    if (conf.get(PROXY_USER_NAME) != null) {
-      ugi = UserGroupInformation.createRemoteUser(conf.get(PROXY_USER_NAME));
-    } else {
-      ugi = shims.getUGIForConf(conf);
-    }
-    final String user = shims.getShortUserName(ugi);
-
-    final FileSystem fs = path.getFileSystem(conf);
-
-    if (fs.exists(path)) {
-      checkPermissions(fs, path, actions, user, ugi.getGroupNames());
-    } else if (path.getParent() != null) {
-      // find the ancestor which exists to check it's permissions
-      Path par = path.getParent();
-      while (par != null) {
-        if (fs.exists(par)) {
-          break;
-        }
-        par = par.getParent();
-      }
-
-      checkPermissions(fs, par, actions, user, ugi.getGroupNames());
-    }
-  }
-
-  /**
-   * Checks the permissions for the given path and current user on Hadoop FS. If the given path
-   * does not exists, it returns.
-   */
-  @SuppressWarnings("deprecation")
-  protected static void checkPermissions(final FileSystem fs, final Path path,
-                       final EnumSet<FsAction> actions, String user, String[] groups) throws IOException,
-    AccessControlException {
-
-    final FileStatus stat;
-
-    try {
-      stat = fs.getFileStatus(path);
-    } catch (FileNotFoundException fnfe) {
-      // File named by path doesn't exist; nothing to validate.
-      return;
-    } catch (org.apache.hadoop.fs.permission.AccessControlException ace) {
-      // Older hadoop version will throw this @deprecated Exception.
-      throw new AccessControlException(ace.getMessage());
-    }
-
-    final FsPermission dirPerms = stat.getPermission();
-    final String grp = stat.getGroup();
-
-    for (FsAction action : actions) {
-      if (user.equals(stat.getOwner())) {
-        if (dirPerms.getUserAction().implies(action)) {
-          continue;
-        }
-      }
-      if (ArrayUtils.contains(groups, grp)) {
-        if (dirPerms.getGroupAction().implies(action)) {
-          continue;
-        }
-      }
-      if (dirPerms.getOtherAction().implies(action)) {
-        continue;
-      }
-      throw new AccessControlException("action " + action + " not permitted on path "
-        + path + " for user " + user);
-    }
-  }
-}
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/security/StorageDelegationAuthorizationProvider.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/security/StorageDelegationAuthorizationProvider.java
deleted file mode 100644
index 48681a19a5..0000000000
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/security/StorageDelegationAuthorizationProvider.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hive.hcatalog.security;
-
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.ql.metadata.AuthorizationException;
-import org.apache.hadoop.hive.ql.metadata.Hive;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
-import org.apache.hadoop.hive.ql.metadata.Partition;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
-import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
-import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase;
-import org.apache.hadoop.hive.ql.security.authorization.Privilege;
-import org.apache.hadoop.util.ReflectionUtils;
-
-/**
- * A HiveAuthorizationProvider which delegates the authorization requests to 
- * the underlying AuthorizationProviders obtained from the StorageHandler.
- */
-public class StorageDelegationAuthorizationProvider extends HiveAuthorizationProviderBase {
-
-  protected HiveAuthorizationProvider hdfsAuthorizer = new HdfsAuthorizationProvider();
-
-  protected static Map<String, String> authProviders = new HashMap<String, String>();
-
-  @Override
-  public void setConf(Configuration conf) {
-    super.setConf(conf);
-    hdfsAuthorizer.setConf(conf);
-  }
-
-  @Override
-  public void init(Configuration conf) throws HiveException {
-    hive_db = new HiveProxy(Hive.get(new HiveConf(conf, HiveAuthorizationProvider.class)));
-  }
-
-  @Override
-  public void setAuthenticator(HiveAuthenticationProvider authenticator) {
-    super.setAuthenticator(authenticator);
-    hdfsAuthorizer.setAuthenticator(authenticator);
-  }
-
-  static {
-    registerAuthProvider("org.apache.hadoop.hive.hbase.HBaseStorageHandler",
-      "org.apache.hive.hcatalog.hbase.HBaseAuthorizationProvider");
-    registerAuthProvider("org.apache.hive.hcatalog.hbase.HBaseHCatStorageHandler",
-      "org.apache.hive.hcatalog.hbase.HBaseAuthorizationProvider");
-  }
-
-  //workaround until Hive adds StorageHandler.getAuthorizationProvider(). Remove these parts afterwards
-  public static void registerAuthProvider(String storageHandlerClass,
-                      String authProviderClass) {
-    authProviders.put(storageHandlerClass, authProviderClass);
-  }
-
-  /** Returns the StorageHandler of the Table obtained from the HCatStorageHandler */
-  protected HiveAuthorizationProvider getDelegate(Table table) throws HiveException {
-    HiveStorageHandler handler = table.getStorageHandler();
-
-    if (handler != null) {
-      if (handler instanceof HiveStorageHandler) {
-        return ((HiveStorageHandler) handler).getAuthorizationProvider();
-      } else {
-        String authProviderClass = authProviders.get(handler.getClass().getCanonicalName());
-
-        if (authProviderClass != null) {
-          try {
-            ReflectionUtils.newInstance(getConf().getClassByName(authProviderClass), getConf());
-          } catch (ClassNotFoundException ex) {
-            throw new HiveException("Cannot instantiate delegation AuthotizationProvider");
-          }
-        }
-
-        //else we do not have anything to delegate to
-        throw new HiveException(String.format("Storage Handler for table:%s is not an instance " +
-          "of HCatStorageHandler", table.getTableName()));
-      }
-    } else {
-      //return an authorizer for HDFS
-      return hdfsAuthorizer;
-    }
-  }
-
-  @Override
-  public void authorize(Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-    throws HiveException, AuthorizationException {
-    //global authorizations against warehouse hdfs directory
-    hdfsAuthorizer.authorize(readRequiredPriv, writeRequiredPriv);
-  }
-
-  @Override
-  public void authorize(Database db, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-    throws HiveException, AuthorizationException {
-    //db's are tied to a hdfs location
-    hdfsAuthorizer.authorize(db, readRequiredPriv, writeRequiredPriv);
-  }
-
-  @Override
-  public void authorize(Table table, Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv)
-    throws HiveException, AuthorizationException {
-    getDelegate(table).authorize(table, readRequiredPriv, writeRequiredPriv);
-  }
-
-  @Override
-  public void authorize(Partition part, Privilege[] readRequiredPriv,
-              Privilege[] writeRequiredPriv) throws HiveException, AuthorizationException {
-    getDelegate(part.getTable()).authorize(part, readRequiredPriv, writeRequiredPriv);
-  }
-
-  @Override
-  public void authorize(Table table, Partition part, List<String> columns,
-              Privilege[] readRequiredPriv, Privilege[] writeRequiredPriv) throws HiveException,
-    AuthorizationException {
-    getDelegate(table).authorize(table, part, columns, readRequiredPriv, writeRequiredPriv);
-  }
-}
diff --git a/hcatalog/core/src/test/java/org/apache/hcatalog/security/TestHdfsAuthorizationProvider.java b/hcatalog/core/src/test/java/org/apache/hcatalog/security/TestHdfsAuthorizationProvider.java
index 3cc75ad61e..e1b37adfcd 100644
--- a/hcatalog/core/src/test/java/org/apache/hcatalog/security/TestHdfsAuthorizationProvider.java
+++ b/hcatalog/core/src/test/java/org/apache/hcatalog/security/TestHdfsAuthorizationProvider.java
@@ -53,7 +53,7 @@
 import org.junit.Test;
 
 /**
- * @deprecated Use/modify {@link org.apache.hive.hcatalog.security.TestHdfsAuthorizationProvider} instead
+ * @deprecated 
  */
 public class TestHdfsAuthorizationProvider {
 
@@ -78,7 +78,7 @@ public void setUp() throws Exception {
     conf.set(ConfVars.SEMANTIC_ANALYZER_HOOK.varname, HCatSemanticAnalyzer.class.getName());
     conf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED, true);
     conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
-        StorageDelegationAuthorizationProvider.class.getCanonicalName());
+        StorageDelegationAuthorizationProvider.class.getName());
     conf.set("fs.pfile.impl", "org.apache.hadoop.fs.ProxyLocalFileSystem");
 
     whDir = System.getProperty("test.warehouse.dir", "/tmp/testhdfsauthorization_wh");
diff --git a/hcatalog/core/src/test/java/org/apache/hive/hcatalog/security/TestHdfsAuthorizationProvider.java b/hcatalog/core/src/test/java/org/apache/hive/hcatalog/security/TestHdfsAuthorizationProvider.java
deleted file mode 100644
index bf6f19ae7d..0000000000
--- a/hcatalog/core/src/test/java/org/apache/hive/hcatalog/security/TestHdfsAuthorizationProvider.java
+++ /dev/null
@@ -1,583 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.hive.hcatalog.security;
-
-import static org.apache.hive.hcatalog.HcatTestUtils.perm300;
-import static org.apache.hive.hcatalog.HcatTestUtils.perm500;
-import static org.apache.hive.hcatalog.HcatTestUtils.perm555;
-import static org.apache.hive.hcatalog.HcatTestUtils.perm700;
-import static org.apache.hive.hcatalog.HcatTestUtils.perm755;
-
-import java.io.IOException;
-import java.util.Random;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.cli.CliSessionState;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
-import org.apache.hadoop.hive.metastore.Warehouse;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.ql.metadata.Hive;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hive.hcatalog.HcatTestUtils;
-import org.apache.hive.hcatalog.cli.HCatDriver;
-import org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestHdfsAuthorizationProvider {
-
-  protected HCatDriver hcatDriver;
-  protected HiveMetaStoreClient msc;
-  protected HiveConf conf;
-  protected String whDir;
-  protected Path whPath;
-  protected FileSystem whFs;
-  protected Warehouse wh;
-  protected Hive hive;
-
-  @Before
-  public void setUp() throws Exception {
-
-    conf = new HiveConf(this.getClass());
-    conf.set(ConfVars.PREEXECHOOKS.varname, "");
-    conf.set(ConfVars.POSTEXECHOOKS.varname, "");
-    conf.set(ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
-
-    conf.set("hive.metastore.local", "true");
-    conf.set(ConfVars.SEMANTIC_ANALYZER_HOOK.varname, HCatSemanticAnalyzer.class.getName());
-    conf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED, true);
-    conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
-        StorageDelegationAuthorizationProvider.class.getCanonicalName());
-    conf.set("fs.pfile.impl", "org.apache.hadoop.fs.ProxyLocalFileSystem");
-
-    whDir = System.getProperty("test.warehouse.dir", "/tmp/testhdfsauthorization_wh");
-    conf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, whDir);
-
-    UserGroupInformation ugi = ShimLoader.getHadoopShims().getUGIForConf(conf);
-    String username = ShimLoader.getHadoopShims().getShortUserName(ugi);
-
-    whPath = new Path(whDir);
-    whFs = whPath.getFileSystem(conf);
-
-    wh = new Warehouse(conf);
-    hive = Hive.get(conf);
-
-    //clean up mess in HMS
-    HcatTestUtils.cleanupHMS(hive, wh, perm700);
-
-    whFs.delete(whPath, true);
-    whFs.mkdirs(whPath, perm755);
-
-    SessionState.start(new CliSessionState(conf));
-    hcatDriver = new HCatDriver();
-  }
-
-  @After
-  public void tearDown() throws IOException {
-    whFs.close();
-    hcatDriver.close();
-    Hive.closeCurrent();
-  }
-
-  public Path getDbPath(String dbName) throws MetaException, HiveException {
-    return HcatTestUtils.getDbPath(hive, wh, dbName);
-  }
-
-  public Path getTablePath(String dbName, String tableName) throws HiveException {
-    Table table = hive.getTable(dbName, tableName);
-    return table.getPath();
-  }
-
-  public Path getPartPath(String partName, String dbName, String tableName) throws HiveException {
-    return new Path(getTablePath(dbName, tableName), partName);
-  }
-
-  /** Execute the query expecting success*/
-  public void exec(String format, Object... args) throws Exception {
-    String command = String.format(format, args);
-    CommandProcessorResponse resp = hcatDriver.run(command);
-    Assert.assertEquals(resp.getErrorMessage(), 0, resp.getResponseCode());
-    Assert.assertEquals(resp.getErrorMessage(), null, resp.getErrorMessage());
-  }
-
-  /** Execute the query expecting it to fail with AuthorizationException */
-  public void execFail(String format, Object... args) throws Exception {
-    String command = String.format(format, args);
-    CommandProcessorResponse resp = hcatDriver.run(command);
-    Assert.assertNotSame(resp.getErrorMessage(), 0, resp.getResponseCode());
-    Assert.assertTrue((resp.getResponseCode() == 40000) || (resp.getResponseCode() == 403));
-    if (resp.getErrorMessage() != null) {
-      Assert.assertTrue(resp.getErrorMessage().contains("org.apache.hadoop.security.AccessControlException"));
-    }
-  }
-
-
-  /**
-   * Tests whether the warehouse directory is writable by the current user (as defined by Hadoop)
-   */
-  @Test
-  public void testWarehouseIsWritable() throws Exception {
-    Path top = new Path(whPath, "_foobarbaz12_");
-    try {
-      whFs.mkdirs(top);
-    } finally {
-      whFs.delete(top, true);
-    }
-  }
-
-  @Test
-  public void testShowDatabases() throws Exception {
-    exec("CREATE DATABASE doo");
-    exec("SHOW DATABASES");
-
-    whFs.setPermission(whPath, perm300); //revoke r
-    execFail("SHOW DATABASES");
-  }
-
-  @Test
-  public void testDatabaseOps() throws Exception {
-    exec("SHOW TABLES");
-    exec("SHOW TABLE EXTENDED LIKE foo1");
-
-    whFs.setPermission(whPath, perm700);
-    exec("CREATE DATABASE doo");
-    exec("DESCRIBE DATABASE doo");
-    exec("USE doo");
-    exec("SHOW TABLES");
-    exec("SHOW TABLE EXTENDED LIKE foo1");
-    exec("DROP DATABASE doo");
-
-    //custom location
-    Path dbPath = new Path(whPath, new Random().nextInt() + "/mydb");
-    whFs.mkdirs(dbPath, perm700);
-    exec("CREATE DATABASE doo2 LOCATION '%s'", dbPath.toUri());
-    exec("DESCRIBE DATABASE doo2", dbPath.toUri());
-    exec("USE doo2");
-    exec("SHOW TABLES");
-    exec("SHOW TABLE EXTENDED LIKE foo1");
-    exec("DROP DATABASE doo2", dbPath.toUri());
-
-    //custom non-existing location
-    exec("CREATE DATABASE doo3 LOCATION '%s/subpath'", dbPath.toUri());
-  }
-
-  @Test
-  public void testCreateDatabaseFail1() throws Exception {
-    whFs.setPermission(whPath, perm500);
-    execFail("CREATE DATABASE doo"); //in the default location
-
-    whFs.setPermission(whPath, perm555);
-    execFail("CREATE DATABASE doo2");
-  }
-
-  @Test
-  public void testCreateDatabaseFail2() throws Exception {
-    //custom location
-    Path dbPath = new Path(whPath, new Random().nextInt() + "/mydb");
-
-    whFs.mkdirs(dbPath, perm700);
-    whFs.setPermission(dbPath, perm500);
-    execFail("CREATE DATABASE doo2 LOCATION '%s'", dbPath.toUri());
-  }
-
-  @Test
-  public void testDropDatabaseFail1() throws Exception {
-    whFs.setPermission(whPath, perm700);
-    exec("CREATE DATABASE doo"); //in the default location
-
-    whFs.setPermission(getDbPath("doo"), perm500); //revoke write
-    execFail("DROP DATABASE doo");
-  }
-
-  @Test
-  public void testDropDatabaseFail2() throws Exception {
-    //custom location
-    Path dbPath = new Path(whPath, new Random().nextInt() + "/mydb");
-
-    whFs.mkdirs(dbPath, perm700);
-    exec("CREATE DATABASE doo2 LOCATION '%s'", dbPath.toUri());
-
-    whFs.setPermission(dbPath, perm500);
-    execFail("DROP DATABASE doo2");
-  }
-
-  @Test
-  public void testDescSwitchDatabaseFail() throws Exception {
-    whFs.setPermission(whPath, perm700);
-    exec("CREATE DATABASE doo");
-    whFs.setPermission(getDbPath("doo"), perm300); //revoke read
-    execFail("DESCRIBE DATABASE doo");
-    execFail("USE doo");
-
-    //custom location
-    Path dbPath = new Path(whPath, new Random().nextInt() + "/mydb");
-    whFs.mkdirs(dbPath, perm700);
-    exec("CREATE DATABASE doo2 LOCATION '%s'", dbPath.toUri());
-    whFs.mkdirs(dbPath, perm300); //revoke read
-    execFail("DESCRIBE DATABASE doo2", dbPath.toUri());
-    execFail("USE doo2");
-  }
-
-  @Test
-  public void testShowTablesFail() throws Exception {
-    whFs.setPermission(whPath, perm700);
-    exec("CREATE DATABASE doo");
-    exec("USE doo");
-    whFs.setPermission(getDbPath("doo"), perm300); //revoke read
-    execFail("SHOW TABLES");
-    execFail("SHOW TABLE EXTENDED LIKE foo1");
-  }
-
-  @Test
-  public void testTableOps() throws Exception {
-    //default db
-    exec("CREATE TABLE foo1 (foo INT) STORED AS RCFILE");
-    exec("DESCRIBE foo1");
-    exec("DROP TABLE foo1");
-
-    //default db custom location
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    whFs.mkdirs(tablePath, perm700);
-    exec("CREATE EXTERNAL TABLE foo2 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-    exec("DESCRIBE foo2");
-    exec("DROP TABLE foo2");
-
-    //default db custom non existing location
-    exec("CREATE EXTERNAL TABLE foo3 (foo INT) STORED AS RCFILE LOCATION '%s/subpath'", tablePath);
-    exec("DESCRIBE foo3");
-    exec("DROP TABLE foo3");
-
-    //non default db
-    exec("CREATE DATABASE doo");
-    exec("USE doo");
-
-    exec("CREATE TABLE foo4 (foo INT) STORED AS RCFILE");
-    exec("DESCRIBE foo4");
-    exec("DROP TABLE foo4");
-
-    //non-default db custom location
-    tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    whFs.mkdirs(tablePath, perm700);
-    exec("CREATE EXTERNAL TABLE foo5 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-    exec("DESCRIBE foo5");
-    exec("DROP TABLE foo5");
-
-    //non-default db custom non existing location
-    exec("CREATE EXTERNAL TABLE foo6 (foo INT) STORED AS RCFILE LOCATION '%s/subpath'", tablePath);
-    exec("DESCRIBE foo6");
-    exec("DROP TABLE foo6");
-
-    exec("DROP TABLE IF EXISTS foo_non_exists");
-
-    exec("CREATE TABLE foo1 (foo INT) STORED AS RCFILE");
-    exec("DESCRIBE EXTENDED foo1");
-    exec("DESCRIBE FORMATTED foo1");
-    exec("DESCRIBE foo1.foo");
-
-    //deep non-existing path for the table
-    tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    whFs.mkdirs(tablePath, perm700);
-    exec("CREATE EXTERNAL TABLE foo2 (foo INT) STORED AS RCFILE LOCATION '%s/a/a/a/'", tablePath);
-  }
-
-  @Test
-  public void testCreateTableFail1() throws Exception {
-    //default db
-    whFs.mkdirs(whPath, perm500); //revoke w
-    execFail("CREATE TABLE foo1 (foo INT) STORED AS RCFILE");
-  }
-
-  @Test
-  public void testCreateTableFail2() throws Exception {
-    //default db custom location
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    whFs.mkdirs(tablePath, perm500);
-    execFail("CREATE EXTERNAL TABLE foo2 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-
-    //default db custom non existing location
-    execFail("CREATE EXTERNAL TABLE foo3 (foo INT) STORED AS RCFILE LOCATION '%s/subpath'", tablePath);
-  }
-
-  @Test
-  public void testCreateTableFail3() throws Exception {
-    //non default db
-    exec("CREATE DATABASE doo");
-    whFs.setPermission(getDbPath("doo"), perm500);
-
-    execFail("CREATE TABLE doo.foo4 (foo INT) STORED AS RCFILE");
-
-    //non-default db custom location, permission to write to tablePath, but not on db path
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    whFs.mkdirs(tablePath, perm700);
-    exec("USE doo");
-    execFail("CREATE EXTERNAL TABLE foo5 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-  }
-
-  @Test
-  public void testCreateTableFail4() throws Exception {
-    //non default db
-    exec("CREATE DATABASE doo");
-
-    //non-default db custom location
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    whFs.mkdirs(tablePath, perm500);
-    execFail("CREATE EXTERNAL TABLE doo.foo5 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-
-    //non-default db custom non existing location
-    execFail("CREATE EXTERNAL TABLE doo.foo6 (foo INT) STORED AS RCFILE LOCATION '%s/a/a/a/'", tablePath);
-  }
-
-  @Test
-  public void testDropTableFail1() throws Exception {
-    //default db
-    exec("CREATE TABLE foo1 (foo INT) STORED AS RCFILE");
-    whFs.mkdirs(getTablePath("default", "foo1"), perm500); //revoke w
-    execFail("DROP TABLE foo1");
-  }
-
-  @Test
-  public void testDropTableFail2() throws Exception {
-    //default db custom location
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    exec("CREATE EXTERNAL TABLE foo2 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-    whFs.mkdirs(tablePath, perm500);
-    execFail("DROP TABLE foo2");
-  }
-
-  @Test
-  public void testDropTableFail4() throws Exception {
-    //non default db
-    exec("CREATE DATABASE doo");
-
-    //non-default db custom location
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-
-    exec("CREATE EXTERNAL TABLE doo.foo5 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-    whFs.mkdirs(tablePath, perm500);
-    exec("USE doo"); //There is no DROP TABLE doo.foo5 support in Hive
-    execFail("DROP TABLE foo5");
-  }
-
-  @Test
-  public void testDescTableFail() throws Exception {
-    //default db
-    exec("CREATE TABLE foo1 (foo INT) STORED AS RCFILE");
-    whFs.mkdirs(getTablePath("default", "foo1"), perm300); //revoke read
-    execFail("DESCRIBE foo1");
-
-    //default db custom location
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    whFs.mkdirs(tablePath, perm700);
-    exec("CREATE EXTERNAL TABLE foo2 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-    whFs.mkdirs(tablePath, perm300); //revoke read
-    execFail("DESCRIBE foo2");
-  }
-
-  @Test
-  public void testAlterTableRename() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) STORED AS RCFILE");
-    exec("ALTER TABLE foo1 RENAME TO foo2");
-
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    exec("CREATE EXTERNAL TABLE foo3 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-    exec("ALTER TABLE foo3 RENAME TO foo4");
-  }
-
-  @Test
-  public void testAlterTableRenameFail() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) STORED AS RCFILE");
-    whFs.mkdirs(getTablePath("default", "foo1"), perm500); //revoke write
-    execFail("ALTER TABLE foo1 RENAME TO foo2");
-
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    exec("CREATE EXTERNAL TABLE foo3 (foo INT) STORED AS RCFILE LOCATION '%s'", tablePath);
-    whFs.mkdirs(tablePath, perm500); //revoke write
-    execFail("ALTER TABLE foo3 RENAME TO foo4");
-  }
-
-  @Test
-  public void testAlterTableRelocate() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) STORED AS RCFILE");
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    exec("ALTER TABLE foo1 SET LOCATION '%s'", tablePath.makeQualified(whFs));
-
-    tablePath = new Path(whPath, new Random().nextInt() + "/mytable2");
-    exec("CREATE EXTERNAL TABLE foo3 (foo INT) STORED AS RCFILE LOCATION '%s'",
-        tablePath.makeQualified(whFs));
-    tablePath = new Path(whPath, new Random().nextInt() + "/mytable2");
-    exec("ALTER TABLE foo3 SET LOCATION '%s'", tablePath.makeQualified(whFs));
-  }
-
-  @Test
-  public void testAlterTableRelocateFail() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) STORED AS RCFILE");
-    Path tablePath = new Path(whPath, new Random().nextInt() + "/mytable");
-    whFs.mkdirs(tablePath, perm500); //revoke write
-    execFail("ALTER TABLE foo1 SET LOCATION '%s'", tablePath.makeQualified(whFs));
-
-    //dont have access to new table loc
-    tablePath = new Path(whPath, new Random().nextInt() + "/mytable2");
-    exec("CREATE EXTERNAL TABLE foo3 (foo INT) STORED AS RCFILE LOCATION '%s'",
-        tablePath.makeQualified(whFs));
-    tablePath = new Path(whPath, new Random().nextInt() + "/mytable2");
-    whFs.mkdirs(tablePath, perm500); //revoke write
-    execFail("ALTER TABLE foo3 SET LOCATION '%s'", tablePath.makeQualified(whFs));
-
-    //have access to new table loc, but not old table loc
-    tablePath = new Path(whPath, new Random().nextInt() + "/mytable3");
-    exec("CREATE EXTERNAL TABLE foo4 (foo INT) STORED AS RCFILE LOCATION '%s'",
-        tablePath.makeQualified(whFs));
-    whFs.mkdirs(tablePath, perm500); //revoke write
-    tablePath = new Path(whPath, new Random().nextInt() + "/mytable3");
-    execFail("ALTER TABLE foo4 SET LOCATION '%s'", tablePath.makeQualified(whFs));
-  }
-
-  @Test
-  public void testAlterTable() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS TEXTFILE");
-    exec("ALTER TABLE foo1 SET TBLPROPERTIES ('foo'='bar')");
-    exec("ALTER TABLE foo1 SET SERDEPROPERTIES ('foo'='bar')");
-    exec("ALTER TABLE foo1 ADD COLUMNS (foo2 INT)");
-  }
-
-  @Test
-  public void testAddDropPartition() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS TEXTFILE");
-    exec("ALTER TABLE foo1 ADD PARTITION (b='2010-10-10')");
-    exec("ALTER TABLE foo1 ADD IF NOT EXISTS PARTITION (b='2010-10-10')");
-    String relPath = new Random().nextInt() + "/mypart";
-    exec("ALTER TABLE foo1 ADD PARTITION (b='2010-10-11') LOCATION '%s'", relPath);
-
-    exec("ALTER TABLE foo1 PARTITION (b='2010-10-10') SET FILEFORMAT RCFILE");
-
-    exec("ALTER TABLE foo1 PARTITION (b='2010-10-10') SET FILEFORMAT INPUTFORMAT "
-        + "'org.apache.hadoop.hive.ql.io.RCFileInputFormat' OUTPUTFORMAT "
-        + "'org.apache.hadoop.hive.ql.io.RCFileOutputFormat' inputdriver "
-        + "'mydriver' outputdriver 'yourdriver'");
-
-    exec("ALTER TABLE foo1 DROP PARTITION (b='2010-10-10')");
-    exec("ALTER TABLE foo1 DROP PARTITION (b='2010-10-11')");
-  }
-
-  @Test
-  public void testAddPartitionFail1() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS TEXTFILE");
-    whFs.mkdirs(getTablePath("default", "foo1"), perm500);
-    execFail("ALTER TABLE foo1 ADD PARTITION (b='2010-10-10')");
-  }
-
-  @Test
-  public void testAddPartitionFail2() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS TEXTFILE");
-    String relPath = new Random().nextInt() + "/mypart";
-    Path partPath = new Path(getTablePath("default", "foo1"), relPath);
-    whFs.mkdirs(partPath, perm500);
-    exec("ALTER TABLE foo1 ADD PARTITION (b='2010-10-10') LOCATION '%s'", partPath);
-  }
-
-  @Test
-  public void testDropPartitionFail1() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS TEXTFILE");
-    exec("ALTER TABLE foo1 ADD PARTITION (b='2010-10-10')");
-    whFs.mkdirs(getPartPath("b=2010-10-10", "default", "foo1"), perm500);
-    execFail("ALTER TABLE foo1 DROP PARTITION (b='2010-10-10')");
-  }
-
-  @Test
-  public void testDropPartitionFail2() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS TEXTFILE");
-    String relPath = new Random().nextInt() + "/mypart";
-    Path partPath = new Path(getTablePath("default", "foo1"), relPath);
-    whFs.mkdirs(partPath, perm700);
-    exec("ALTER TABLE foo1 ADD PARTITION (b='2010-10-10') LOCATION '%s'", partPath);
-    whFs.mkdirs(partPath, perm500); //revoke write
-    execFail("ALTER TABLE foo1 DROP PARTITION (b='2010-10-10')");
-  }
-
-  @Test
-  public void testAlterTableFail() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (boo STRING) STORED AS TEXTFILE");
-    whFs.mkdirs(getTablePath("default", "foo1"), perm500); //revoke write
-    execFail("ALTER TABLE foo1 SET TBLPROPERTIES ('foo'='bar')");
-    execFail("ALTER TABLE foo1 SET SERDEPROPERTIES ('foo'='bar')");
-    execFail("ALTER TABLE foo1 ADD COLUMNS (foo2 INT)");
-  }
-
-  @Test
-  public void testShowTables() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (boo STRING) STORED AS TEXTFILE");
-    exec("SHOW PARTITIONS foo1");
-
-    whFs.mkdirs(getTablePath("default", "foo1"), perm300); //revoke read
-    execFail("SHOW PARTITIONS foo1");
-  }
-
-  @Test
-  public void testAlterTablePartRename() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS RCFILE");
-    Path loc = new Path(whPath, new Random().nextInt() + "/mypart");
-    exec("ALTER TABLE foo1 ADD PARTITION (b='2010-10-16') LOCATION '%s'", loc);
-    exec("ALTER TABLE foo1 PARTITION (b='2010-10-16') RENAME TO PARTITION (b='2010-10-17')");
-  }
-
-  @Test
-  public void testAlterTablePartRenameFail() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS RCFILE");
-    Path loc = new Path(whPath, new Random().nextInt() + "/mypart");
-    exec("ALTER TABLE foo1 ADD PARTITION (b='2010-10-16') LOCATION '%s'", loc);
-    whFs.setPermission(loc, perm500); //revoke w
-    execFail("ALTER TABLE foo1 PARTITION (b='2010-10-16') RENAME TO PARTITION (b='2010-10-17')");
-  }
-
-  @Test
-  public void testAlterTablePartRelocate() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS RCFILE");
-    exec("ALTER TABLE foo1 ADD PARTITION (b='2010-10-16')");
-    Path partPath = new Path(whPath, new Random().nextInt() + "/mypart");
-    exec("ALTER TABLE foo1 PARTITION (b='2010-10-16') SET LOCATION '%s'", partPath.makeQualified(whFs));
-  }
-
-  @Test
-  public void testAlterTablePartRelocateFail() throws Exception {
-    exec("CREATE TABLE foo1 (foo INT) PARTITIONED BY (b STRING) STORED AS RCFILE");
-
-    Path oldLoc = new Path(whPath, new Random().nextInt() + "/mypart");
-    Path newLoc = new Path(whPath, new Random().nextInt() + "/mypart2");
-
-    exec("ALTER TABLE foo1 ADD PARTITION (b='2010-10-16') LOCATION '%s'", oldLoc);
-    whFs.mkdirs(oldLoc, perm500);
-    execFail("ALTER TABLE foo1 PARTITION (b='2010-10-16') SET LOCATION '%s'", newLoc.makeQualified(whFs));
-    whFs.mkdirs(oldLoc, perm700);
-    whFs.mkdirs(newLoc, perm500);
-    execFail("ALTER TABLE foo1 PARTITION (b='2010-10-16') SET LOCATION '%s'", newLoc.makeQualified(whFs));
-  }
-
-}
diff --git a/hcatalog/src/test/e2e/hcatalog/tests/hadoop.conf b/hcatalog/src/test/e2e/hcatalog/tests/hadoop.conf
index 18020e2fa3..de868f4654 100644
--- a/hcatalog/src/test/e2e/hcatalog/tests/hadoop.conf
+++ b/hcatalog/src/test/e2e/hcatalog/tests/hadoop.conf
@@ -225,9 +225,9 @@ jar :FUNCPATH:/testudf.jar org.apache.hive.hcatalog.utils.WriteRC -libjars :HCAT
                                 {
                                  'num' => 1
                                 ,'hcat_prep'=>q\drop table if exists hadoop_hbase_1;
-create table hadoop_hbase_1(key string, gpa string) STORED BY 'org.apache.hive.hcatalog.hbase.HBaseHCatStorageHandler' TBLPROPERTIES ('hbase.columns.mapping'=':key,info:gpa');\
+create table hadoop_hbase_1(key string, gpa string) STORED BY 'org.apache.hcatalog.hbase.HBaseHCatStorageHandler' TBLPROPERTIES ('hbase.columns.mapping'=':key,info:gpa');\
                                 ,'hadoop' => q\
-jar :FUNCPATH:/testudf.jar org.apache.hive.hcatalog.utils.HBaseReadWrite -libjars :HCAT_JAR: :THRIFTSERVER: :INPATH:/studenttab10k hadoop_hbase_1 :OUTPATH:
+jar :FUNCPATH:/testudf.jar org.apache.hcatalog.utils.HBaseReadWrite -libjars :HCAT_JAR: :THRIFTSERVER: :INPATH:/studenttab10k hadoop_hbase_1 :OUTPATH:
 \,
                                 ,'sql' => q\select name, sum(gpa) from studenttab10k group by name;\
                                 ,'floatpostprocess' => 1
diff --git a/hcatalog/src/test/e2e/hcatalog/tests/pig.conf b/hcatalog/src/test/e2e/hcatalog/tests/pig.conf
index 4c03fe481b..8fbe2a8415 100644
--- a/hcatalog/src/test/e2e/hcatalog/tests/pig.conf
+++ b/hcatalog/src/test/e2e/hcatalog/tests/pig.conf
@@ -319,15 +319,15 @@ store c into ':OUTPATH:';\
                                 {
                                  'num' => 1
                                 ,'hcat_prep'=>q\drop table if exists pig_hbase_1;
-create table pig_hbase_1(key string, age string, gpa string) STORED BY 'org.apache.hive.hcatalog.hbase.HBaseHCatStorageHandler' TBLPROPERTIES ('hbase.columns.mapping'=':key,info:age,info:gpa');\
+create table pig_hbase_1(key string, age string, gpa string) STORED BY 'org.apache.hcatalog.hbase.HBaseHCatStorageHandler' TBLPROPERTIES ('hbase.columns.mapping'=':key,info:age,info:gpa');\
                                 ,'pig' => q\set hcat.hbase.output.bulkMode 'false'
 a = load ':INPATH:/studenttab10k' as (name:chararray, age:int, gpa:float);
 b = group a by name;
 c = foreach b generate group as name, AVG(a.age) as age, AVG(a.gpa) as gpa;
 d = foreach c generate name as key, (chararray)age, (chararray)gpa as gpa;
-store d into 'pig_hbase_1' using org.apache.hive.hcatalog.pig.HCatStorer();
+store d into 'pig_hbase_1' using org.apache.hcatalog.pig.HCatStorer();
 exec
-e = load 'pig_hbase_1' using org.apache.hive.hcatalog.pig.HCatLoader();
+e = load 'pig_hbase_1' using org.apache.hcatalog.pig.HCatLoader();
 store e into ':OUTPATH:';\,
                                 ,'result_table' => ['pig_hbase_1','?']
 				,'sql'   => [ 'select name, avg(cast(age as decimal(10,5))), avg(gpa) from studenttab10k group by name;', 'select name, avg(cast(age as decimal(10,5))), avg(gpa) from studenttab10k group by name;' ]
@@ -338,17 +338,17 @@ store e into ':OUTPATH:';\,
                                  # multiquery
                                  'num' => 2
                                 ,'hcat_prep'=>q\drop table if exists pig_hbase_2_1;
-create table pig_hbase_2_1(key string, age string, gpa string) STORED BY 'org.apache.hive.hcatalog.hbase.HBaseHCatStorageHandler' TBLPROPERTIES ('hbase.columns.mapping'=':key,info:age,info:gpa');
+create table pig_hbase_2_1(key string, age string, gpa string) STORED BY 'org.apache.hcatalog.hbase.HBaseHCatStorageHandler' TBLPROPERTIES ('hbase.columns.mapping'=':key,info:age,info:gpa');
 drop table if exists pig_hbase_2_2;
-create table pig_hbase_2_2(key string, age string, gpa string) STORED BY 'org.apache.hive.hcatalog.hbase.HBaseHCatStorageHandler' TBLPROPERTIES ('hbase.columns.mapping'=':key,info:age,info:gpa');
+create table pig_hbase_2_2(key string, age string, gpa string) STORED BY 'org.apache.hcatalog.hbase.HBaseHCatStorageHandler' TBLPROPERTIES ('hbase.columns.mapping'=':key,info:age,info:gpa');
 \
                                 ,'pig' => q\set hcat.hbase.output.bulkMode 'false'
 a = load ':INPATH:/studenttab10k' as (name:chararray, age:int, gpa:float);
 b = group a by name;
 c = foreach b generate group as name, AVG(a.age) as age, AVG(a.gpa) as gpa;
 d = foreach c generate name as key, (chararray)age, (chararray)gpa as gpa;
-store d into 'pig_hbase_2_1' using org.apache.hive.hcatalog.pig.HCatStorer();
-store d into 'pig_hbase_2_2' using org.apache.hive.hcatalog.pig.HCatStorer();\,
+store d into 'pig_hbase_2_1' using org.apache.hcatalog.pig.HCatStorer();
+store d into 'pig_hbase_2_2' using org.apache.hcatalog.pig.HCatStorer();\,
                                 ,'result_table' => ['pig_hbase_2_1','pig_hbase_2_2']
 				,'sql'   => [ 'select name, avg(cast(age as decimal(10,5))), avg(gpa) from studenttab10k group by name;', 'select name, avg(cast(age as decimal(10,5))), avg(gpa) from studenttab10k group by name;']
                                 ,'floatpostprocess' => 1
diff --git a/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/HBaseReadWrite.java b/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java
similarity index 93%
rename from hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/HBaseReadWrite.java
rename to hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java
index f0d7221bf1..18e0ea2db5 100644
--- a/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/HBaseReadWrite.java
+++ b/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hcatalog/utils/HBaseReadWrite.java
@@ -17,7 +17,7 @@
  * under the License.
  */
 
-package org.apache.hive.hcatalog.utils;
+package org.apache.hcatalog.utils;
 
 import java.io.IOException;
 
@@ -36,13 +36,13 @@
 import org.apache.hadoop.util.GenericOptionsParser;
 import org.apache.hadoop.util.Tool;
 import org.apache.hadoop.util.ToolRunner;
-import org.apache.hive.hcatalog.common.HCatConstants;
-import org.apache.hive.hcatalog.data.DefaultHCatRecord;
-import org.apache.hive.hcatalog.data.HCatRecord;
-import org.apache.hive.hcatalog.mapreduce.HCatInputFormat;
-import org.apache.hive.hcatalog.mapreduce.HCatOutputFormat;
-import org.apache.hive.hcatalog.mapreduce.InputJobInfo;
-import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;
+import org.apache.hcatalog.common.HCatConstants;
+import org.apache.hcatalog.data.DefaultHCatRecord;
+import org.apache.hcatalog.data.HCatRecord;
+import org.apache.hcatalog.mapreduce.HCatInputFormat;
+import org.apache.hcatalog.mapreduce.HCatOutputFormat;
+import org.apache.hcatalog.mapreduce.InputJobInfo;
+import org.apache.hcatalog.mapreduce.OutputJobInfo;
 
 /**
  * This is a map reduce test for testing hcat which goes against the "numbers"
diff --git a/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java b/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java
index 8fd66076a9..1414cbce7f 100644
--- a/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java
+++ b/hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/WriteTextPartitioned.java
@@ -48,7 +48,7 @@
  * other columns. This is to simulate a typical operation in a map reduce
  * program to test that hcat hands the right data to the map reduce program
  *
- * Usage: hadoop jar org.apache.hive.hcatalog.utils.HBaseReadWrite -libjars
+ * Usage: hadoop jar org.apache.hcatalog.utils.HBaseReadWrite -libjars
  * &lt;hcat_jar&gt; * &lt;serveruri&gt; &lt;input_tablename&gt; &lt;output_tablename&gt; [filter]
  * If filter is given it will be provided as the partition to write to.
  */
diff --git a/hcatalog/src/test/e2e/templeton/README.txt b/hcatalog/src/test/e2e/templeton/README.txt
index e92bb7bf0f..2b5125e22f 100644
--- a/hcatalog/src/test/e2e/templeton/README.txt
+++ b/hcatalog/src/test/e2e/templeton/README.txt
@@ -69,6 +69,7 @@ Setup
 2. Install perl and following perl modules  (cpan -i <MODULE_NAME>)
 * IPC::Run
 * JSON
+* JSON::Path
 * Data::Dump
 * Number::Compare
 * Text::Glob
diff --git a/hcatalog/storage-handlers/hbase/src/java/org/apache/hcatalog/hbase/HBaseAuthorizationProvider.java b/hcatalog/storage-handlers/hbase/src/java/org/apache/hcatalog/hbase/HBaseAuthorizationProvider.java
index 97ffcff593..dc7824c70c 100644
--- a/hcatalog/storage-handlers/hbase/src/java/org/apache/hcatalog/hbase/HBaseAuthorizationProvider.java
+++ b/hcatalog/storage-handlers/hbase/src/java/org/apache/hcatalog/hbase/HBaseAuthorizationProvider.java
@@ -22,6 +22,8 @@
 import java.util.List;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.classification.InterfaceAudience;
+import org.apache.hadoop.hive.common.classification.InterfaceStability;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.ql.metadata.AuthorizationException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -34,8 +36,11 @@
 /**
  * This class is an implementation of HiveAuthorizationProvider to provide
  * authorization functionality for HBase tables.
+ * @deprecated 
  */
-class HBaseAuthorizationProvider implements HiveAuthorizationProvider {
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class HBaseAuthorizationProvider implements HiveAuthorizationProvider {
 
   @Override
   public Configuration getConf() {
