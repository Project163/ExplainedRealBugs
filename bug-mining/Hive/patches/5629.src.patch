diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidGroupByQueryRecordReader.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidGroupByQueryRecordReader.java
index f0bdb9ee3f..fddabf7fe1 100644
--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidGroupByQueryRecordReader.java
+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidGroupByQueryRecordReader.java
@@ -24,8 +24,10 @@
 import org.apache.calcite.adapter.druid.DruidTable;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.druid.DruidStorageHandlerUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.mapreduce.InputSplit;
+import org.joda.time.format.ISODateTimeFormat;
 
 import com.fasterxml.jackson.core.type.TypeReference;
 
@@ -45,6 +47,10 @@ public class DruidGroupByQueryRecordReader
 
   private int[] indexes = new int[0];
 
+  // Grouping dimensions can have different types if we are grouping using an
+  // extraction function
+  private PrimitiveTypeInfo[] dimensionTypes;
+
   // Row objects returned by GroupByQuery have different access paths depending on
   // whether the result for the metric is a Float or a Long, thus we keep track
   // using these converters
@@ -53,6 +59,7 @@ public class DruidGroupByQueryRecordReader
   @Override
   public void initialize(InputSplit split, Configuration conf) throws IOException {
     super.initialize(split, conf);
+    initDimensionTypes();
     initExtractors();
   }
 
@@ -69,6 +76,13 @@ protected List<Row> createResultsList(InputStream content) throws IOException {
     );
   }
 
+  private void initDimensionTypes() throws IOException {
+    dimensionTypes = new PrimitiveTypeInfo[query.getDimensions().size()];
+    for (int i = 0; i < query.getDimensions().size(); i++) {
+      dimensionTypes[i] = DruidSerDeUtils.extractTypeFromDimension(query.getDimensions().get(i));
+    }
+  }
+
   private void initExtractors() throws IOException {
     extractors = new Extract[query.getAggregatorSpecs().size() + query.getPostAggregatorSpecs()
             .size()];
@@ -137,7 +151,20 @@ public DruidWritable getCurrentValue() throws IOException, InterruptedException
         value.getValue().put(ds.getOutputName(), null);
       } else {
         int pos = dims.size() - indexes[i] - 1;
-        value.getValue().put(ds.getOutputName(), dims.get(pos));
+        Object val;
+        switch (dimensionTypes[i].getPrimitiveCategory()) {
+          case TIMESTAMP:
+            // FLOOR extraction function
+            val = ISODateTimeFormat.dateTimeParser().parseMillis((String) dims.get(pos));
+            break;
+          case INT:
+            // EXTRACT extraction function
+            val = Integer.valueOf((String) dims.get(pos));
+            break;
+          default:
+            val = dims.get(pos);
+        }
+        value.getValue().put(ds.getOutputName(), val);
       }
     }
     int counter = 0;
@@ -176,7 +203,20 @@ public boolean next(NullWritable key, DruidWritable value) {
           value.getValue().put(ds.getOutputName(), null);
         } else {
           int pos = dims.size() - indexes[i] - 1;
-          value.getValue().put(ds.getOutputName(), dims.get(pos));
+          Object val;
+          switch (dimensionTypes[i].getPrimitiveCategory()) {
+            case TIMESTAMP:
+              // FLOOR extraction function
+              val = ISODateTimeFormat.dateTimeParser().parseMillis((String) dims.get(pos));
+              break;
+            case INT:
+              // EXTRACT extraction function
+              val = Integer.valueOf((String) dims.get(pos));
+              break;
+            default:
+              val = dims.get(pos);
+          }
+          value.getValue().put(ds.getOutputName(), val);
         }
       }
       int counter = 0;
diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java
index 2d11e4b96f..ee9dcb344b 100644
--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java
+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java
@@ -68,6 +68,7 @@
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.util.StringUtils;
 import org.joda.time.Period;
+import org.joda.time.format.ISODateTimeFormat;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -76,6 +77,7 @@
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.metamx.common.lifecycle.Lifecycle;
+import com.metamx.common.parsers.ParseException;
 import com.metamx.http.client.HttpClient;
 import com.metamx.http.client.HttpClientConfig;
 import com.metamx.http.client.HttpClientInit;
@@ -86,6 +88,8 @@
 import io.druid.query.aggregation.AggregatorFactory;
 import io.druid.query.aggregation.PostAggregator;
 import io.druid.query.dimension.DimensionSpec;
+import io.druid.query.dimension.ExtractionDimensionSpec;
+import io.druid.query.extraction.TimeFormatExtractionFn;
 import io.druid.query.groupby.GroupByQuery;
 import io.druid.query.metadata.metadata.ColumnAnalysis;
 import io.druid.query.metadata.metadata.SegmentAnalysis;
@@ -410,7 +414,7 @@ private void inferSchema(GroupByQuery query,
     // Dimension columns
     for (DimensionSpec ds : query.getDimensions()) {
       columnNames.add(ds.getOutputName());
-      columnTypes.add(TypeInfoFactory.stringTypeInfo);
+      columnTypes.add(DruidSerDeUtils.extractTypeFromDimension(ds));
     }
     // Aggregator columns
     for (AggregatorFactory af : query.getAggregatorSpecs()) {
diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDeUtils.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDeUtils.java
index 64a19f63d9..c8a63abd68 100644
--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDeUtils.java
+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDeUtils.java
@@ -23,6 +23,10 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import io.druid.query.dimension.DimensionSpec;
+import io.druid.query.dimension.ExtractionDimensionSpec;
+import io.druid.query.extraction.TimeFormatExtractionFn;
+
 /**
  * Utils class for Druid SerDe.
  */
@@ -30,10 +34,10 @@ public final class DruidSerDeUtils {
 
   private static final Logger LOG = LoggerFactory.getLogger(DruidSerDeUtils.class);
 
-  protected static final String FLOAT_TYPE = "FLOAT";
+  protected static final String ISO_TIME_FORMAT = "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'";
 
+  protected static final String FLOAT_TYPE = "FLOAT";
   protected static final String LONG_TYPE = "LONG";
-
   protected static final String STRING_TYPE = "STRING";
 
   /* This method converts from the String representation of Druid type
@@ -82,4 +86,22 @@ public static String convertDruidToHiveTypeString(String typeName) {
     }
   }
 
+  /* Extract type from dimension spec. It returns TIMESTAMP if it is a FLOOR,
+   * INTEGER if it is a EXTRACT, or STRING otherwise. */
+  public static PrimitiveTypeInfo extractTypeFromDimension(DimensionSpec ds) {
+    if (ds instanceof ExtractionDimensionSpec) {
+      ExtractionDimensionSpec eds = (ExtractionDimensionSpec) ds;
+      TimeFormatExtractionFn tfe = (TimeFormatExtractionFn) eds.getExtractionFn();
+      if (tfe.getFormat() == null || tfe.getFormat().equals(ISO_TIME_FORMAT)) {
+        // Timestamp (null or default used by FLOOR)
+        return TypeInfoFactory.timestampTypeInfo;
+      } else {
+        // EXTRACT from timestamp
+        return TypeInfoFactory.intTypeInfo;
+      }
+    }
+    // Default
+    return TypeInfoFactory.stringTypeInfo;
+  }
+
 }
diff --git a/druid-handler/src/test/org/apache/hadoop/hive/druid/TestDruidSerDe.java b/druid-handler/src/test/org/apache/hadoop/hive/druid/TestDruidSerDe.java
index 695e0dd7c4..62b2d6b250 100644
--- a/druid-handler/src/test/org/apache/hadoop/hive/druid/TestDruidSerDe.java
+++ b/druid-handler/src/test/org/apache/hadoop/hive/druid/TestDruidSerDe.java
@@ -72,7 +72,6 @@
 import com.google.common.collect.Lists;
 
 import io.druid.data.input.Row;
-import io.druid.jackson.DefaultObjectMapper;
 import io.druid.query.Query;
 import io.druid.query.Result;
 import io.druid.query.groupby.GroupByQuery;
@@ -699,9 +698,12 @@ private static void deserializeQueryResults(DruidSerDe serDe, String queryType,
     field1.setAccessible(true);
     field1.set(reader, query);
     if (reader instanceof DruidGroupByQueryRecordReader) {
-      Method method1 = DruidGroupByQueryRecordReader.class.getDeclaredMethod("initExtractors");
+      Method method1 = DruidGroupByQueryRecordReader.class.getDeclaredMethod("initDimensionTypes");
       method1.setAccessible(true);
       method1.invoke(reader);
+      Method method2 = DruidGroupByQueryRecordReader.class.getDeclaredMethod("initExtractors");
+      method2.setAccessible(true);
+      method2.invoke(reader);
     }
     Field field2 = DruidQueryRecordReader.class.getDeclaredField("results");
     field2.setAccessible(true);
