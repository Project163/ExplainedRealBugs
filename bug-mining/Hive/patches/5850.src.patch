diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidOutputFormat.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidOutputFormat.java
index 8156231cfd..da7642c61a 100644
--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidOutputFormat.java
+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidOutputFormat.java
@@ -51,6 +51,7 @@
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
@@ -127,9 +128,10 @@ public FileSinkOperator.RecordWriter getHiveRecordWriter(
     final List<DimensionSchema> dimensions = new ArrayList<>();
     ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();
     for (int i = 0; i < columnTypes.size(); i++) {
-      PrimitiveTypeInfo f = (PrimitiveTypeInfo) columnTypes.get(i);
+      final PrimitiveObjectInspector.PrimitiveCategory primitiveCategory = ((PrimitiveTypeInfo) columnTypes
+              .get(i)).getPrimitiveCategory();
       AggregatorFactory af;
-      switch (f.getPrimitiveCategory()) {
+      switch (primitiveCategory) {
         case BYTE:
         case SHORT:
         case INT:
@@ -146,16 +148,17 @@ public FileSinkOperator.RecordWriter getHiveRecordWriter(
           if (!tColumnName.equals(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN) && !tColumnName
                   .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {
             throw new IOException("Dimension " + tColumnName + " does not have STRING type: " +
-                    f.getPrimitiveCategory());
+                    primitiveCategory);
           }
           continue;
         default:
           // Dimension
           String dColumnName = columnNames.get(i);
-          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(f.getPrimitiveCategory()) !=
-                  PrimitiveGrouping.STRING_GROUP) {
+          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(primitiveCategory) !=
+                  PrimitiveGrouping.STRING_GROUP
+                  && primitiveCategory != PrimitiveObjectInspector.PrimitiveCategory.BOOLEAN) {
             throw new IOException("Dimension " + dColumnName + " does not have STRING type: " +
-                    f.getPrimitiveCategory());
+                    primitiveCategory);
           }
           dimensions.add(new StringDimensionSchema(dColumnName));
           continue;
diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java
index e6e01d1659..a43f62ed82 100644
--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java
+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java
@@ -52,6 +52,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
@@ -68,6 +69,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
+import org.apache.hadoop.io.BooleanWritable;
 import org.apache.hadoop.io.FloatWritable;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
@@ -494,6 +496,10 @@ public Writable serialize(Object o, ObjectInspector objectInspector) throws SerD
           res = ((StringObjectInspector) fields.get(i).getFieldObjectInspector())
                   .getPrimitiveJavaObject(values.get(i));
           break;
+        case BOOLEAN:
+          res = ((BooleanObjectInspector) fields.get(i).getFieldObjectInspector())
+                  .get(values.get(i));
+          break;
         default:
           throw new SerDeException("Unknown type: " + types[i].getPrimitiveCategory());
       }
@@ -564,6 +570,9 @@ public Object deserialize(Writable writable) throws SerDeException {
         case STRING:
           output.add(new Text(value.toString()));
           break;
+        case BOOLEAN:
+          output.add(new BooleanWritable(Boolean.valueOf(value.toString())));
+          break;
         default:
           throw new SerDeException("Unknown type: " + types[i].getPrimitiveCategory());
       }
