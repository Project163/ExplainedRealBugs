diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 23e8c9f8c8..bc9d630976 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -813,6 +813,8 @@ public static enum ConfVars {
 
     // Whether to show the unquoted partition names in query results.
     HIVE_DECODE_PARTITION_NAME("hive.decode.partition.name", false),
+
+    HIVE_TYPE_CHECK_ON_INSERT("hive.typecheck.on.insert", true),
     ;
 
     public final String varname;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
index 393ef57294..d22009a3b2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
@@ -362,6 +362,7 @@ public enum ErrorMsg {
   UNSUPPORTED_ALTER_TBL_OP(10245, "{0} alter table options is not supported"),
   INVALID_BIGTABLE_MAPJOIN(10246, "{0} table chosen for streaming is not valid", true),
   MISSING_OVER_CLAUSE(10247, "Missing over clause for function : "),
+  PARTITION_SPEC_TYPE_MISMATCH(10248, "Cannot add partition column {0} of type {1} as it cannot be converted to type {2}", true),
 
   SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),
   SCRIPT_IO_ERROR(20001, "An error occurred while reading or writing to your custom script. "
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index a704462ffa..46d1fac521 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -29,6 +29,7 @@
 import java.util.LinkedHashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Map.Entry;
 
 import org.antlr.runtime.tree.CommonTree;
 import org.antlr.runtime.tree.Tree;
@@ -40,6 +41,7 @@
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryProperties;
+import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory;
 import org.apache.hadoop.hive.ql.exec.FetchTask;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.Utilities;
@@ -59,10 +61,15 @@
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ListBucketingCtx;
 import org.apache.hadoop.hive.ql.plan.PlanUtils;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
 import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
 import org.apache.hadoop.mapred.TextInputFormat;
@@ -304,6 +311,13 @@ protected void reset() {
     rootTasks = new ArrayList<Task<? extends Serializable>>();
   }
 
+  public static String stripIdentifierQuotes(String val) {
+    if ((val.charAt(0) == '`' && val.charAt(val.length() - 1) == '`')) {
+      val = val.substring(1, val.length() - 1);
+    }
+    return val;
+  }
+
   public static String stripQuotes(String val) {
     return PlanUtils.stripQuotes(val);
   }
@@ -580,7 +594,7 @@ public static List<FieldSchema> getColumns(ASTNode ast, boolean lowerCase) throw
         // child 2 is the optional comment of the column
         if (child.getChildCount() == 3) {
           col.setComment(unescapeSQLString(child.getChild(2).getText()));
-        }        
+        }
       }
       colList.add(col);
     }
@@ -748,7 +762,7 @@ public tableSpec(Hive db, HiveConf conf, ASTNode ast,
         }
 
         // check if the columns specified in the partition() clause are actually partition columns
-        Utilities.validatePartSpec(tableHandle, partSpec);
+        validatePartSpec(tableHandle, partSpec, ast, conf);
 
         // check if the partition spec is valid
         if (numDynParts > 0) {
@@ -1115,4 +1129,79 @@ protected boolean analyzeStoredAdDirs(ASTNode child) {
     return storedAsDirs;
   }
 
+  private static void getPartExprNodeDesc(ASTNode astNode,
+      Map<ASTNode, ExprNodeDesc> astExprNodeMap)
+          throws SemanticException, HiveException {
+
+    if ((astNode == null) || (astNode.getChildren() == null) ||
+        (astNode.getChildren().size() <= 1)) {
+      return;
+    }
+
+    TypeCheckCtx typeCheckCtx = new TypeCheckCtx(null);
+    for (Node childNode : astNode.getChildren()) {
+      ASTNode childASTNode = (ASTNode)childNode;
+
+      if (childASTNode.getType() != HiveParser.TOK_PARTVAL) {
+        getPartExprNodeDesc(childASTNode, astExprNodeMap);
+      } else {
+        if (childASTNode.getChildren().size() <= 1) {
+          throw new HiveException("This is dynamic partitioning");
+        }
+
+        ASTNode partValASTChild = (ASTNode)childASTNode.getChildren().get(1);
+        astExprNodeMap.put((ASTNode)childASTNode.getChildren().get(0),
+            TypeCheckProcFactory.genExprNode(partValASTChild, typeCheckCtx).get(partValASTChild));
+      }
+    }
+  }
+
+  public static void validatePartSpec(Table tbl,
+      Map<String, String> partSpec, ASTNode astNode, HiveConf conf) throws SemanticException {
+
+    Map<ASTNode, ExprNodeDesc> astExprNodeMap = new HashMap<ASTNode, ExprNodeDesc>();
+
+    Utilities.validatePartSpec(tbl, partSpec);
+
+    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_TYPE_CHECK_ON_INSERT)) {
+      try {
+        getPartExprNodeDesc(astNode, astExprNodeMap);
+      } catch (HiveException e) {
+        return;
+      }
+      List<FieldSchema> parts = tbl.getPartitionKeys();
+      Map<String, String> partCols = new HashMap<String, String>(parts.size());
+      for (FieldSchema col : parts) {
+        partCols.put(col.getName(), col.getType().toLowerCase());
+      }
+      for (Entry<ASTNode, ExprNodeDesc> astExprNodePair : astExprNodeMap.entrySet()) {
+
+        String astKeyName = astExprNodePair.getKey().toString().toLowerCase();
+        if (astExprNodePair.getKey().getType() == HiveParser.Identifier) {
+          astKeyName = stripIdentifierQuotes(astKeyName);
+        }
+        String colType = partCols.get(astKeyName);
+        ObjectInspector inputOI = astExprNodePair.getValue().getWritableObjectInspector();
+
+        TypeInfo expectedType =
+            TypeInfoUtils.getTypeInfoFromTypeString(colType);
+        ObjectInspector outputOI =
+            TypeInfoUtils.getStandardWritableObjectInspectorFromTypeInfo(expectedType);
+        Object value = null;
+        try {
+          value =
+              ExprNodeEvaluatorFactory.get(astExprNodePair.getValue()).
+              evaluate(partSpec.get(astKeyName));
+        } catch (HiveException e) {
+          throw new SemanticException(e);
+        }
+        Object convertedValue =
+          ObjectInspectorConverters.getConverter(inputOI, outputOI).convert(value);
+        if (convertedValue == null) {
+          throw new SemanticException(ErrorMsg.PARTITION_SPEC_TYPE_MISMATCH.format(astKeyName,
+              inputOI.getTypeName(), outputOI.getTypeName()));
+        }
+      }
+    }
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 0c01749ca5..36034d6858 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -2613,6 +2613,7 @@ private void analyzeAlterTableAddParts(CommonTree ast, boolean expectView)
           currentLocation = null;
         }
         currentPart = getPartSpec(child);
+        validatePartSpec(tab, currentPart, (ASTNode)child, conf);
         break;
       case HiveParser.TOK_PARTITIONLOCATION:
         // if location specified, set in partition
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
index 767f545826..abb77a1b80 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
@@ -110,9 +110,14 @@ public static ExprNodeDesc processGByExpr(Node nd, Object procCtx)
     // build the exprNodeFuncDesc with recursively built children.
     ASTNode expr = (ASTNode) nd;
     TypeCheckCtx ctx = (TypeCheckCtx) procCtx;
+
     RowResolver input = ctx.getInputRR();
     ExprNodeDesc desc = null;
 
+    if ((ctx == null) || (input == null)) {
+      return null;
+    }
+
     // If the current subExpression is pre-calculated, as in Group-By etc.
     ColumnInfo colInfo = input.getExpression(expr);
     if (colInfo != null) {
diff --git a/ql/src/test/queries/clientnegative/illegal_partition_type.q b/ql/src/test/queries/clientnegative/illegal_partition_type.q
new file mode 100644
index 0000000000..1cdaffd1f3
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/illegal_partition_type.q
@@ -0,0 +1,7 @@
+-- begin part(string, int) pass(string, string)
+CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;
+LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='second');
+
+select * from tab1;
+drop table tab1;
+
diff --git a/ql/src/test/queries/clientnegative/illegal_partition_type2.q b/ql/src/test/queries/clientnegative/illegal_partition_type2.q
new file mode 100644
index 0000000000..2438288209
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/illegal_partition_type2.q
@@ -0,0 +1,3 @@
+create table tab1 (id1 int, id2 string) PARTITIONED BY(month string,day int) row format delimited fields terminated by ',';
+alter table tab1 add partition (month='June', day='second');
+drop table tab1;
diff --git a/ql/src/test/queries/clientpositive/alter_partition_coltype.q b/ql/src/test/queries/clientpositive/alter_partition_coltype.q
index 4d2e02f3cb..5479afbbd5 100644
--- a/ql/src/test/queries/clientpositive/alter_partition_coltype.q
+++ b/ql/src/test/queries/clientpositive/alter_partition_coltype.q
@@ -24,6 +24,8 @@ select count(*) from alter_coltype where dt = 100;
 -- alter partition key column data type for ts column.
 alter table alter_coltype partition column (ts double);
 
+alter table alter_coltype partition column (dt string);
+
 -- load a new partition using new data type.
 insert overwrite table alter_coltype partition(dt='100x', ts=3.0) select * from src1;
 
diff --git a/ql/src/test/queries/clientpositive/partition_type_check.q b/ql/src/test/queries/clientpositive/partition_type_check.q
new file mode 100644
index 0000000000..7f1accadac
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/partition_type_check.q
@@ -0,0 +1,24 @@
+set hive.typecheck.on.insert = true;
+
+-- begin part(string, string) pass(string, int)
+CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day string) stored as textfile;
+LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day=2);
+
+select * from tab1;
+drop table tab1;
+
+-- begin part(string, int) pass(string, string)
+CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day int) stored as textfile;
+LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='2');
+
+select * from tab1;
+drop table tab1;
+
+-- begin part(string, date) pass(string, date)
+create table tab1 (id1 int, id2 string) PARTITIONED BY(month string,day date) stored as textfile;
+alter table tab1 add partition (month='June', day='2008-01-01');
+LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='2008-01-01');
+
+select id1, id2, day from tab1 where day='2008-01-01';
+drop table tab1;
+
diff --git a/ql/src/test/results/clientnegative/alter_table_add_partition.q.out b/ql/src/test/results/clientnegative/alter_table_add_partition.q.out
index bd9c148df1..557ac4d76f 100644
--- a/ql/src/test/results/clientnegative/alter_table_add_partition.q.out
+++ b/ql/src/test/results/clientnegative/alter_table_add_partition.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table mp (a int) partitioned by (b int)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@mp
-FAILED: SemanticException [Error 10214]: Invalid partition spec specified table is partitioned but partition spec is not specified or does not fully match table partitioning: {b=1, c=1}
+FAILED: SemanticException [Error 10098]: Non-Partition column appears in the partition specification:  c
diff --git a/ql/src/test/results/clientnegative/alter_view_failure5.q.out b/ql/src/test/results/clientnegative/alter_view_failure5.q.out
index 4edb82c790..83511ee9a2 100644
--- a/ql/src/test/results/clientnegative/alter_view_failure5.q.out
+++ b/ql/src/test/results/clientnegative/alter_view_failure5.q.out
@@ -13,4 +13,4 @@ AS
 SELECT * FROM src
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@xxx6
-FAILED: SemanticException [Error 10214]: Invalid partition spec specified value not found in table's partition spec: {v=val_86}
+FAILED: SemanticException [Error 10098]: Non-Partition column appears in the partition specification:  v
diff --git a/ql/src/test/results/clientnegative/illegal_partition_type.q.out b/ql/src/test/results/clientnegative/illegal_partition_type.q.out
new file mode 100644
index 0000000000..e1f943d7cb
--- /dev/null
+++ b/ql/src/test/results/clientnegative/illegal_partition_type.q.out
@@ -0,0 +1,8 @@
+PREHOOK: query: -- begin part(string, int) pass(string, string)
+CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- begin part(string, int) pass(string, string)
+CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tab1
+FAILED: SemanticException [Error 10248]: Cannot add partition column day of type string as it cannot be converted to type int
diff --git a/ql/src/test/results/clientnegative/illegal_partition_type2.q.out b/ql/src/test/results/clientnegative/illegal_partition_type2.q.out
new file mode 100644
index 0000000000..1fb634c6a3
--- /dev/null
+++ b/ql/src/test/results/clientnegative/illegal_partition_type2.q.out
@@ -0,0 +1,6 @@
+PREHOOK: query: create table tab1 (id1 int, id2 string) PARTITIONED BY(month string,day int) row format delimited fields terminated by ','
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table tab1 (id1 int, id2 string) PARTITIONED BY(month string,day int) row format delimited fields terminated by ','
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tab1
+FAILED: SemanticException [Error 10248]: Cannot add partition column day of type string as it cannot be converted to type int
diff --git a/ql/src/test/results/clientpositive/alter_partition_coltype.q.out b/ql/src/test/results/clientpositive/alter_partition_coltype.q.out
index fe16fdda1d..3bb311fb9f 100644
--- a/ql/src/test/results/clientpositive/alter_partition_coltype.q.out
+++ b/ql/src/test/results/clientpositive/alter_partition_coltype.q.out
@@ -242,6 +242,17 @@ POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.
 POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: alter table alter_coltype partition column (dt string)
+PREHOOK: type: null
+PREHOOK: Input: default@alter_coltype
+POSTHOOK: query: alter table alter_coltype partition column (dt string)
+POSTHOOK: type: null
+POSTHOOK: Input: default@alter_coltype
+POSTHOOK: Output: default@alter_coltype
+POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: -- load a new partition using new data type.
 insert overwrite table alter_coltype partition(dt='100x', ts=3.0) select * from src1
 PREHOOK: type: QUERY
@@ -1239,13 +1250,13 @@ POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)
 POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 key                 	string              	None                
 value               	string              	None                
-dt                  	int                 	None                
+dt                  	string              	None                
 ts                  	double              	None                
 	 	 
 # Partition Information	 	 
 # col_name            	data_type           	comment             
 	 	 
-dt                  	int                 	None                
+dt                  	string              	None                
 ts                  	double              	None                
 PREHOOK: query: desc alter_coltype partition (dt='100x', ts='6:30pm')
 PREHOOK: type: DESCTABLE
@@ -1259,13 +1270,13 @@ POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)
 POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 key                 	string              	None                
 value               	string              	None                
-dt                  	int                 	None                
+dt                  	string              	None                
 ts                  	double              	None                
 	 	 
 # Partition Information	 	 
 # col_name            	data_type           	comment             
 	 	 
-dt                  	int                 	None                
+dt                  	string              	None                
 ts                  	double              	None                
 PREHOOK: query: desc alter_coltype partition (dt='100x', ts=3.0)
 PREHOOK: type: DESCTABLE
@@ -1279,13 +1290,13 @@ POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)
 POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 key                 	string              	None                
 value               	string              	None                
-dt                  	int                 	None                
+dt                  	string              	None                
 ts                  	double              	None                
 	 	 
 # Partition Information	 	 
 # col_name            	data_type           	comment             
 	 	 
-dt                  	int                 	None                
+dt                  	string              	None                
 ts                  	double              	None                
 PREHOOK: query: desc alter_coltype partition (dt=10, ts=3.0)
 PREHOOK: type: DESCTABLE
@@ -1299,13 +1310,13 @@ POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)
 POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 key                 	string              	None                
 value               	string              	None                
-dt                  	int                 	None                
+dt                  	string              	None                
 ts                  	double              	None                
 	 	 
 # Partition Information	 	 
 # col_name            	data_type           	comment             
 	 	 
-dt                  	int                 	None                
+dt                  	string              	None                
 ts                  	double              	None                
 PREHOOK: query: drop table alter_coltype
 PREHOOK: type: DROPTABLE
diff --git a/ql/src/test/results/clientpositive/partition_type_check.q.out b/ql/src/test/results/clientpositive/partition_type_check.q.out
new file mode 100644
index 0000000000..8aed925514
--- /dev/null
+++ b/ql/src/test/results/clientpositive/partition_type_check.q.out
@@ -0,0 +1,120 @@
+PREHOOK: query: -- begin part(string, string) pass(string, int)
+CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day string) stored as textfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- begin part(string, string) pass(string, int)
+CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day string) stored as textfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tab1
+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day=2)
+PREHOOK: type: LOAD
+PREHOOK: Output: default@tab1
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day=2)
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@tab1
+POSTHOOK: Output: default@tab1@month=June/day=2
+PREHOOK: query: select * from tab1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tab1
+PREHOOK: Input: default@tab1@month=June/day=2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from tab1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tab1
+POSTHOOK: Input: default@tab1@month=June/day=2
+#### A masked pattern was here ####
+1	11	June	2
+2	12	June	2
+3	13	June	2
+7	17	June	2
+8	18	June	2
+8	28	June	2
+PREHOOK: query: drop table tab1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@tab1
+PREHOOK: Output: default@tab1
+POSTHOOK: query: drop table tab1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@tab1
+POSTHOOK: Output: default@tab1
+PREHOOK: query: -- begin part(string, int) pass(string, string)
+CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day int) stored as textfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- begin part(string, int) pass(string, string)
+CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day int) stored as textfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tab1
+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='2')
+PREHOOK: type: LOAD
+PREHOOK: Output: default@tab1
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='2')
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@tab1
+POSTHOOK: Output: default@tab1@month=June/day=2
+PREHOOK: query: select * from tab1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tab1
+PREHOOK: Input: default@tab1@month=June/day=2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from tab1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tab1
+POSTHOOK: Input: default@tab1@month=June/day=2
+#### A masked pattern was here ####
+1	11	June	2
+2	12	June	2
+3	13	June	2
+7	17	June	2
+8	18	June	2
+8	28	June	2
+PREHOOK: query: drop table tab1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@tab1
+PREHOOK: Output: default@tab1
+POSTHOOK: query: drop table tab1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@tab1
+POSTHOOK: Output: default@tab1
+PREHOOK: query: -- begin part(string, date) pass(string, date)
+create table tab1 (id1 int, id2 string) PARTITIONED BY(month string,day date) stored as textfile
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- begin part(string, date) pass(string, date)
+create table tab1 (id1 int, id2 string) PARTITIONED BY(month string,day date) stored as textfile
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tab1
+PREHOOK: query: alter table tab1 add partition (month='June', day='2008-01-01')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: default@tab1
+POSTHOOK: query: alter table tab1 add partition (month='June', day='2008-01-01')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: default@tab1
+POSTHOOK: Output: default@tab1@month=June/day=2008-01-01
+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='2008-01-01')
+PREHOOK: type: LOAD
+PREHOOK: Output: default@tab1@month=June/day=2008-01-01
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='2008-01-01')
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@tab1@month=June/day=2008-01-01
+PREHOOK: query: select id1, id2, day from tab1 where day='2008-01-01'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tab1
+PREHOOK: Input: default@tab1@month=June/day=2008-01-01
+#### A masked pattern was here ####
+POSTHOOK: query: select id1, id2, day from tab1 where day='2008-01-01'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tab1
+POSTHOOK: Input: default@tab1@month=June/day=2008-01-01
+#### A masked pattern was here ####
+1	11	2008-01-01
+2	12	2008-01-01
+3	13	2008-01-01
+7	17	2008-01-01
+8	18	2008-01-01
+8	28	2008-01-01
+PREHOOK: query: drop table tab1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@tab1
+PREHOOK: Output: default@tab1
+POSTHOOK: query: drop table tab1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@tab1
+POSTHOOK: Output: default@tab1
