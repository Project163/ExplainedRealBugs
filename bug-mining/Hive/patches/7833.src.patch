diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rewrite/AlterMaterializedViewRewriteOperation.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rewrite/AlterMaterializedViewRewriteOperation.java
index 426daa6db6..4f2b6cccc6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rewrite/AlterMaterializedViewRewriteOperation.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rewrite/AlterMaterializedViewRewriteOperation.java
@@ -56,7 +56,7 @@ public int execute() throws HiveException {
         planner.initCtx(ctx);
         planner.init(false);
 
-        RelNode plan = planner.genLogicalPlan(ParseUtils.parse(newMV.getViewExpandedText()));
+        RelNode plan = planner.genLogicalPlan(ParseUtils.parse(newMV.getViewExpandedText(), ctx));
         if (plan == null) {
           String msg = "Cannot enable automatic rewriting for materialized view.";
           if (ctx.getCboInfo() != null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java
index 148d1d6b49..195ca090fd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java
@@ -79,9 +79,6 @@
 public final class ParseUtils {
   /** Parses the Hive query. */
   private static final Logger LOG = LoggerFactory.getLogger(ParseUtils.class);
-  public static ASTNode parse(String command) throws ParseException {
-    return parse(command, null);
-  }
 
   /** Parses the Hive query. */
   public static ASTNode parse(String command, Context ctx) throws ParseException {
@@ -105,7 +102,7 @@ public static ASTNode parse(
     }
     ASTNode tree = parseResult.getTree();
     tree = findRootNonNullToken(tree);
-    handleSetColRefs(tree);
+    handleSetColRefs(tree, ctx);
     return tree;
   }
 
@@ -355,13 +352,13 @@ public static boolean sameTree(ASTNode node, ASTNode otherNode) {
     }
 
 
-    private static void handleSetColRefs(ASTNode tree) {
+    private static void handleSetColRefs(ASTNode tree, Context ctx) {
       CalcitePlanner.ASTSearcher astSearcher = new CalcitePlanner.ASTSearcher();
       while (true) {
         astSearcher.reset();
         ASTNode setCols = astSearcher.depthFirstSearch(tree, HiveParser.TOK_SETCOLREF);
         if (setCols == null) break;
-        processSetColsNode(setCols, astSearcher);
+        processSetColsNode(setCols, astSearcher, ctx);
       }
     }
 
@@ -374,7 +371,7 @@ private static void handleSetColRefs(ASTNode tree) {
      * @param setCols TOK_SETCOLREF ASTNode.
      * @param searcher AST searcher to reuse.
      */
-    private static void processSetColsNode(ASTNode setCols, ASTSearcher searcher) {
+    private static void processSetColsNode(ASTNode setCols, ASTSearcher searcher, Context ctx) {
       searcher.reset();
       CommonTree rootNode = setCols;
       while (rootNode != null && rootNode.getType() != HiveParser.TOK_INSERT) {
@@ -460,8 +457,8 @@ private static void processSetColsNode(ASTNode setCols, ASTSearcher searcher) {
         switch (child.getType()) {
         case HiveParser.TOK_SETCOLREF:
           // We have a nested setcolref. Process that and start from scratch TODO: use stack?
-          processSetColsNode((ASTNode)child, searcher);
-          processSetColsNode(setCols, searcher);
+          processSetColsNode((ASTNode)child, searcher, ctx);
+          processSetColsNode(setCols, searcher, ctx);
           return;
         case HiveParser.TOK_ALLCOLREF:
           // We should find an alias of this insert and do (alias).*. This however won't fix e.g.
@@ -472,13 +469,13 @@ private static void processSetColsNode(ASTNode setCols, ASTSearcher searcher) {
         case HiveParser.TOK_TABLE_OR_COL:
           Tree idChild = child.getChild(0);
           assert idChild.getType() == HiveParser.Identifier : idChild;
-          if (!createChildColumnRef(idChild, alias, newChildren, aliases)) {
+          if (!createChildColumnRef(idChild, alias, newChildren, aliases, ctx)) {
             setCols.token.setType(HiveParser.TOK_ALLCOLREF);
             return;
           }
           break;
         case HiveParser.Identifier:
-          if (!createChildColumnRef(child, alias, newChildren, aliases)) {
+          if (!createChildColumnRef(child, alias, newChildren, aliases, ctx)) {
             setCols.token.setType(HiveParser.TOK_ALLCOLREF);
             return;
           }
@@ -486,7 +483,7 @@ private static void processSetColsNode(ASTNode setCols, ASTSearcher searcher) {
         case HiveParser.DOT: {
           Tree colChild = child.getChild(child.getChildCount() - 1);
           assert colChild.getType() == HiveParser.Identifier : colChild;
-          if (!createChildColumnRef(colChild, alias, newChildren, aliases)) {
+          if (!createChildColumnRef(colChild, alias, newChildren, aliases, ctx)) {
             setCols.token.setType(HiveParser.TOK_ALLCOLREF);
             return;
           }
@@ -514,8 +511,12 @@ private static void processSetColsNode(ASTNode setCols, ASTSearcher searcher) {
     }
 
     private static boolean createChildColumnRef(Tree child, String alias,
-        List<ASTNode> newChildren, HashSet<String> aliases) {
+        List<ASTNode> newChildren, HashSet<String> aliases, Context ctx) {
       String colAlias = child.getText();
+      if (SemanticAnalyzer.isRegex(colAlias, (HiveConf)ctx.getConf())) {
+        LOG.debug("Skip creating child column reference because of regexp used as alias: " + colAlias);
+        return false;
+      }
       if (!aliases.add(colAlias)) {
         // TODO: if a side of the union has 2 columns with the same name, noone on the higher
         //       level can refer to them. We could change the alias in the original node.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 8b22303808..fb460692d0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -82,7 +82,6 @@
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.conf.HiveConf.ResultFileFormat;
 import org.apache.hadoop.hive.conf.HiveConf.StrictChecks;
-import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.TableType;
 import org.apache.hadoop.hive.metastore.TransactionalValidationListener;
 import org.apache.hadoop.hive.metastore.Warehouse;
@@ -4453,7 +4452,7 @@ static String[] getColAlias(ASTNode selExpr, String defaultName,
    * Returns whether the pattern is a regex expression (instead of a normal
    * string). Normal string is a string with all alphabets/digits and "_".
    */
-  boolean isRegex(String pattern, HiveConf conf) {
+  static boolean isRegex(String pattern, HiveConf conf) {
     String qIdSupport = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_QUOTEDID_SUPPORT);
     if (!"none".equals(qIdSupport)) {
       return false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/tools/LineageInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/tools/LineageInfo.java
index 02c75c1ec0..9a44ffb450 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/tools/LineageInfo.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/tools/LineageInfo.java
@@ -27,6 +27,8 @@
 import java.util.Stack;
 import java.util.TreeSet;
 
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.lib.DefaultGraphWalker;
 import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
 import org.apache.hadoop.hive.ql.lib.SemanticDispatcher;
@@ -41,6 +43,8 @@
 import org.apache.hadoop.hive.ql.parse.ParseException;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 
+import com.google.common.annotations.VisibleForTesting;
+
 /**
  *
  * This class prints out the lineage info. It takes sql as input and prints
@@ -102,15 +106,12 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
    * parses given query and gets the lineage info.
    *
    * @param query
-   * @throws ParseException
    */
-  public void getLineageInfo(String query) throws ParseException,
-      SemanticException {
-
+  public void getLineageInfo(String query, Context ctx) throws Exception {
     /*
      * Get the AST tree
      */
-    ASTNode tree = ParseUtils.parse(query, null);
+    ASTNode tree = ParseUtils.parse(query, ctx );
 
     while ((tree.getToken() == null) && (tree.getChildCount() > 0)) {
       tree = (ASTNode) tree.getChild(0);
@@ -138,14 +139,14 @@ public void getLineageInfo(String query) throws ParseException,
     ogw.startWalking(topNodes, null);
   }
 
-  public static void main(String[] args) throws IOException, ParseException,
-      SemanticException {
+  public static void main(String[] args) throws Exception {
 
     String query = args[0];
 
     LineageInfo lep = new LineageInfo();
 
-    lep.getLineageInfo(query);
+    Context ctx=new Context(new HiveConf());
+    lep.getLineageInfo(query, ctx);
 
     for (String tab : lep.getInputTableList()) {
       System.out.println("InputTable=" + tab);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/ddl/table/partition/show/TestShowPartitionAnalyzer.java b/ql/src/test/org/apache/hadoop/hive/ql/ddl/table/partition/show/TestShowPartitionAnalyzer.java
index 6e07d5d3af..685124edb9 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/ddl/table/partition/show/TestShowPartitionAnalyzer.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/ddl/table/partition/show/TestShowPartitionAnalyzer.java
@@ -18,9 +18,15 @@
 
 package org.apache.hadoop.hive.ql.ddl.table.partition.show;
 
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.TableType;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.QueryState;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
@@ -36,18 +42,29 @@
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.junit.After;
 import org.junit.Assert;
+import org.junit.Before;
 import org.junit.Test;
 
-import java.util.ArrayList;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-
 public class TestShowPartitionAnalyzer {
 
+  private HiveConf conf;
+
+  @Before
+  public void before() throws Exception {
+    conf = new HiveConf();
+    SessionState.start(conf);
+  }
+
+  @After
+  public void after() throws Exception {
+    SessionState.get().close();
+  }
+
   @Test
   public void testGetShowPartitionsFilter() throws Exception {
 
@@ -67,7 +84,7 @@ public void testGetShowPartitionsFilter() throws Exception {
     String showPart1 = "show partitions databaseFoo.tableBar " +
         "where ds > '2010-03-03' and hr = '__HIVE_DEFAULT_PARTITION__' and "
         + "rs <= 421021";
-    ASTNode command = ParseUtils.parse(showPart1);
+    ASTNode command = ParseUtils.parse(showPart1, new Context(conf));
     ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc)genExprNodeByDefault(tcCtx, command);
     // the hr op '__HIVE_DEFAULT_PARTITION__' converts to null
     Assert.assertEquals(new ExprNodeConstantDesc(TypeInfoFactory.booleanTypeInfo,
@@ -112,7 +129,7 @@ public void testGetShowPartitionsFilter() throws Exception {
     // invalid input
     String showPart2 = "show partitions databaseFoo.tableBar " +
         "where hr > 'a123' and hr <= '2346b'";
-    command = ParseUtils.parse(showPart2);
+    command = ParseUtils.parse(showPart2, new Context(conf));
     try {
       analyzer.getShowPartitionsFilter(table, command);
       Assert.fail("show throw semantic exception");
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/parse/TestMacroSemanticAnalyzer.java b/ql/src/test/org/apache/hadoop/hive/ql/parse/TestMacroSemanticAnalyzer.java
index 8b5ae71350..6f61c33c03 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/parse/TestMacroSemanticAnalyzer.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/parse/TestMacroSemanticAnalyzer.java
@@ -49,7 +49,7 @@ public void setup() throws Exception {
   }
 
   private ASTNode parse(String command) throws Exception {
-    return ParseUtils.parse(command);
+    return ParseUtils.parse(command, context);
   }
 
   private void analyze(ASTNode ast) throws Exception {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/parse/TestParseUtils.java b/ql/src/test/org/apache/hadoop/hive/ql/parse/TestParseUtils.java
index 69cb96d1a3..7827f0ff1f 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/parse/TestParseUtils.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/parse/TestParseUtils.java
@@ -21,14 +21,19 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.TxnType;
+import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
-
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.junit.After;
 import org.junit.Assert;
+import org.junit.Before;
+import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
 import org.junit.runners.Parameterized.Parameters;
 
+import java.io.IOException;
 import java.util.Arrays;
 import java.util.Collection;
 
@@ -48,6 +53,16 @@ public TestParseUtils(String query, TxnType txnType) {
     this.conf = new HiveConf();
   }
 
+  @Before
+  public void before() {
+    SessionState.start((HiveConf) conf);
+  }
+
+  @After
+  public void after() throws Exception {
+    SessionState.get().close();
+  }
+
   @Parameters
   public static Collection<Object[]> data() {
     return Arrays.asList(
@@ -97,15 +112,15 @@ public static Collection<Object[]> data() {
   }
 
   @Test
-  public void testTxnTypeWithEnabledReadOnlyFeature() throws ParseException {
+  public void testTxnTypeWithEnabledReadOnlyFeature() throws Exception {
     enableReadOnlyTxnFeature(true);
-    Assert.assertEquals(AcidUtils.getTxnType(conf, ParseUtils.parse(query)), txnType);
+    Assert.assertEquals(AcidUtils.getTxnType(conf, ParseUtils.parse(query,new Context(conf))), txnType);
   }
 
   @Test
-  public void testTxnTypeWithDisabledReadOnlyFeature() throws ParseException {
+  public void testTxnTypeWithDisabledReadOnlyFeature() throws Exception {
     enableReadOnlyTxnFeature(false);
-    Assert.assertEquals(AcidUtils.getTxnType(conf, ParseUtils.parse(query)),
+    Assert.assertEquals(AcidUtils.getTxnType(conf, ParseUtils.parse(query,new Context(conf))),
         txnType == TxnType.READ_ONLY ? TxnType.DEFAULT : txnType);
   }
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/AuthorizationTestUtil.java b/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/AuthorizationTestUtil.java
index 99eead3871..3ab9cb9417 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/AuthorizationTestUtil.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/AuthorizationTestUtil.java
@@ -44,12 +44,12 @@ public static DDLWork analyze(ASTNode ast, QueryState queryState, Hive db) throw
     return (DDLWork) inList(rootTasks).ofSize(1).get(0).getWork();
   }
 
-  public static DDLWork analyze(String command, QueryState queryState, Hive db) throws Exception {
-    return analyze(parse(command), queryState, db);
+  public static DDLWork analyze(String command, QueryState queryState, Hive db, Context ctx) throws Exception {
+    return analyze(parse(command, ctx), queryState, db);
   }
 
-  private static ASTNode parse(String command) throws Exception {
-    return ParseUtils.parse(command);
+  private static ASTNode parse(String command, Context ctx) throws Exception {
+    return ParseUtils.parse(command, ctx);
   }
 
   /**
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/PrivilegesTestBase.java b/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/PrivilegesTestBase.java
index 6fd07a00e3..77d2454f49 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/PrivilegesTestBase.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/PrivilegesTestBase.java
@@ -17,7 +17,9 @@
  */
 package org.apache.hadoop.hive.ql.parse.authorization;
 
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.PrincipalType;
+import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.QueryState;
 import org.apache.hadoop.hive.ql.ddl.DDLWork;
 import org.apache.hadoop.hive.ql.ddl.privilege.PrincipalDesc;
@@ -35,8 +37,9 @@ public class PrivilegesTestBase {
 
   public static void grantUserTable(String privStr, PrivilegeType privType, QueryState queryState, Hive db)
       throws Exception {
+    Context ctx=new Context(new HiveConf());
     DDLWork work = AuthorizationTestUtil.analyze(
-        "GRANT " + privStr + " ON TABLE " + TABLE + " TO USER " + USER, queryState, db);
+        "GRANT " + privStr + " ON TABLE " + TABLE + " TO USER " + USER, queryState, db, ctx);
     GrantDesc grantDesc = (GrantDesc)work.getDDLDesc();
     Assert.assertNotNull("Grant should not be null", grantDesc);
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java b/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java
index 54ce8f3553..d170986e52 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.api.PrincipalType;
+import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.QueryState;
 import org.apache.hadoop.hive.ql.ddl.DDLWork;
 import org.apache.hadoop.hive.ql.ddl.privilege.PrincipalDesc;
@@ -457,7 +458,7 @@ public void testGrantServer() throws Exception {
   }
 
   private DDLWork analyze(String command) throws Exception {
-    return AuthorizationTestUtil.analyze(command, queryState, db);
+    return AuthorizationTestUtil.analyze(command, queryState, db, new Context(queryState.getConf()));
   }
 
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/tool/TestLineageInfo.java b/ql/src/test/org/apache/hadoop/hive/ql/tool/TestLineageInfo.java
index 5b17e8addb..cde89cfe76 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/tool/TestLineageInfo.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/tool/TestLineageInfo.java
@@ -18,12 +18,16 @@
 
 package org.apache.hadoop.hive.ql.tool;
 
-import java.util.TreeSet;
-
+import static org.junit.Assert.fail;
 
+import java.util.TreeSet;
 
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.tools.LineageInfo;
-import static org.junit.Assert.fail;
+import org.junit.After;
+import org.junit.Before;
 import org.junit.Test;
 
 /**
@@ -32,6 +36,20 @@
  */
 public class TestLineageInfo {
 
+  private Context ctx;
+
+  @Before
+  public void before() throws Exception {
+    HiveConf conf = new HiveConf();
+    SessionState.start(conf);
+    ctx = new Context(conf);
+  }
+
+  @After
+  public void after() throws Exception {
+    SessionState.get().close();
+  }
+
   /**
    * Checks whether the test outputs match the expected outputs.
    * 
@@ -58,7 +76,7 @@ public void testSimpleQuery() {
     try {
       lep.getLineageInfo("INSERT OVERWRITE TABLE dest1 partition (ds = '111')  " 
           + "SELECT s.* FROM srcpart TABLESAMPLE (BUCKET 1 OUT OF 1) s " 
-          + "WHERE s.ds='2008-04-08' and s.hr='11'");
+          + "WHERE s.ds='2008-04-08' and s.hr='11'", ctx);
       TreeSet<String> i = new TreeSet<String>();
       TreeSet<String> o = new TreeSet<String>();
       i.add("srcpart");
@@ -71,47 +89,37 @@ public void testSimpleQuery() {
   }
 
   @Test
-  public void testSimpleQuery2() {
+  public void testSimpleQuery2() throws Exception {
     LineageInfo lep = new LineageInfo();
-    try {
-      lep.getLineageInfo("FROM (FROM src select src.key, src.value " 
-          + "WHERE src.key < 10 UNION ALL FROM src SELECT src.* WHERE src.key > 10 ) unioninput " 
-          + "INSERT OVERWRITE DIRECTORY '../../../../build/contrib/hive/ql/test/data/warehouse/union.out' " 
-          + "SELECT unioninput.*");
-      TreeSet<String> i = new TreeSet<String>();
-      TreeSet<String> o = new TreeSet<String>();
-      i.add("src");
-      checkOutput(lep, i, o);
-    } catch (Exception e) {
-      e.printStackTrace();
-      fail("Failed");
-    }
+    lep.getLineageInfo("FROM (FROM src select src.key, src.value "
+        + "WHERE src.key < 10 UNION ALL FROM src SELECT src.* WHERE src.key > 10 ) unioninput "
+        + "INSERT OVERWRITE DIRECTORY '../../../../build/contrib/hive/ql/test/data/warehouse/union.out' "
+        + "SELECT unioninput.*", ctx);
+    TreeSet<String> i = new TreeSet<String>();
+    TreeSet<String> o = new TreeSet<String>();
+    i.add("src");
+    checkOutput(lep, i, o);
   }
 
   @Test
-  public void testSimpleQuery3() {
+  public void testSimpleQuery3() throws Exception {
     LineageInfo lep = new LineageInfo();
-    try {
-      lep.getLineageInfo("FROM (FROM src select src.key, src.value " 
-          + "WHERE src.key < 10 UNION ALL FROM src1 SELECT src1.* WHERE src1.key > 10 ) unioninput " 
-          + "INSERT OVERWRITE DIRECTORY '../../../../build/contrib/hive/ql/test/data/warehouse/union.out' " 
-          + "SELECT unioninput.*");
-      TreeSet<String> i = new TreeSet<String>();
-      TreeSet<String> o = new TreeSet<String>();
-      i.add("src");
-      i.add("src1");
-      checkOutput(lep, i, o);
-    } catch (Exception e) {
-      e.printStackTrace();
-      fail("Failed");
-    }
+    lep.getLineageInfo("FROM (FROM src select src.key, src.value "
+        + "WHERE src.key < 10 UNION ALL FROM src1 SELECT src1.* WHERE src1.key > 10 ) unioninput "
+        + "INSERT OVERWRITE DIRECTORY '../../../../build/contrib/hive/ql/test/data/warehouse/union.out' "
+        + "SELECT unioninput.*", ctx);
+    TreeSet<String> i = new TreeSet<String>();
+    TreeSet<String> o = new TreeSet<String>();
+    i.add("src");
+    i.add("src1");
+    checkOutput(lep, i, o);
   }
 
   @Test
   public void testSimpleQuery4() {
     LineageInfo lep = new LineageInfo();
     try {
-      lep.getLineageInfo("FROM ( FROM ( FROM src1 src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20) a RIGHT OUTER JOIN ( FROM src2 src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25) b ON (a.c1 = b.c3) SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4) c SELECT c.c1, c.c2, c.c3, c.c4");
+      lep.getLineageInfo("FROM ( FROM ( FROM src1 src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20) a RIGHT OUTER JOIN ( FROM src2 src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25) b ON (a.c1 = b.c3) SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4) c SELECT c.c1, c.c2, c.c3, c.c4", ctx);
       TreeSet<String> i = new TreeSet<String>();
       TreeSet<String> o = new TreeSet<String>();
       i.add("src1");
@@ -128,7 +136,7 @@ public void testSimpleQuery5() {
     LineageInfo lep = new LineageInfo();
     try {
       lep.getLineageInfo("insert overwrite table x select a.y, b.y " 
-          + "from a a full outer join b b on (a.x = b.y)");
+          + "from a a full outer join b b on (a.x = b.y)", ctx);
       TreeSet<String> i = new TreeSet<String>();
       TreeSet<String> o = new TreeSet<String>();
       i.add("a");
diff --git a/ql/src/test/queries/clientpositive/union_regex.q b/ql/src/test/queries/clientpositive/union_regex.q
new file mode 100644
index 0000000000..7d2f9d0e9a
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/union_regex.q
@@ -0,0 +1,17 @@
+CREATE TABLE t (a1 INT, a2 INT, a1a int, a2a int);
+
+insert into t values (1,2,10,20);
+
+SET hive.support.quoted.identifiers=none;
+
+SELECT `(a1)?+.+` FROM t
+UNION
+SELECT `(a2)?+.+` FROM t;
+
+SELECT `(a1)?+.+` FROM t
+UNION DISTINCT
+SELECT `(a2)?+.+` FROM t;
+
+SELECT `(a1)?+.+` FROM t
+UNION ALL
+SELECT `(a2)?+.+` FROM t;
diff --git a/ql/src/test/results/clientpositive/llap/union_regex.q.out b/ql/src/test/results/clientpositive/llap/union_regex.q.out
new file mode 100644
index 0000000000..057a7ae84f
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/union_regex.q.out
@@ -0,0 +1,62 @@
+PREHOOK: query: CREATE TABLE t (a1 INT, a2 INT, a1a int, a2a int)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@t
+POSTHOOK: query: CREATE TABLE t (a1 INT, a2 INT, a1a int, a2a int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@t
+PREHOOK: query: insert into t values (1,2,10,20)
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@t
+POSTHOOK: query: insert into t values (1,2,10,20)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@t
+POSTHOOK: Lineage: t.a1 SCRIPT []
+POSTHOOK: Lineage: t.a1a SCRIPT []
+POSTHOOK: Lineage: t.a2 SCRIPT []
+POSTHOOK: Lineage: t.a2a SCRIPT []
+PREHOOK: query: SELECT `(a1)?+.+` FROM t
+UNION
+SELECT `(a2)?+.+` FROM t
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT `(a1)?+.+` FROM t
+UNION
+SELECT `(a2)?+.+` FROM t
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t
+#### A masked pattern was here ####
+1	10	20
+2	10	20
+PREHOOK: query: SELECT `(a1)?+.+` FROM t
+UNION DISTINCT
+SELECT `(a2)?+.+` FROM t
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT `(a1)?+.+` FROM t
+UNION DISTINCT
+SELECT `(a2)?+.+` FROM t
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t
+#### A masked pattern was here ####
+1	10	20
+2	10	20
+PREHOOK: query: SELECT `(a1)?+.+` FROM t
+UNION ALL
+SELECT `(a2)?+.+` FROM t
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT `(a1)?+.+` FROM t
+UNION ALL
+SELECT `(a2)?+.+` FROM t
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t
+#### A masked pattern was here ####
+2	10	20
+1	10	20
