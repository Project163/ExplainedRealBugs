diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index 0aadee3e3d..59128e23b1 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -418,6 +418,7 @@ minillap.query.files=acid_bucket_pruning.q,\
   intersect_all.q,\
   intersect_distinct.q,\
   intersect_merge.q,\
+  llap_smb.q,\
   llap_udf.q,\
   llapdecider.q,\
   reduce_deduplicate.q,\
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java
index c22d446320..22ca025e90 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java
@@ -32,9 +32,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.llap.counters.QueryFragmentCounters;
 import org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer;
-import org.apache.hadoop.hive.llap.io.decode.ReadPipeline;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.RowSchema;
@@ -46,7 +44,6 @@
 import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.AvoidSplitCombination;
 import org.apache.hadoop.hive.ql.io.LlapAwareSplit;
 import org.apache.hadoop.hive.ql.io.SelfDescribingInputFormatInterface;
-import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
 import org.apache.hadoop.hive.ql.plan.MapWork;
@@ -63,13 +60,6 @@
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hive.common.util.HiveStringUtils;
-import org.apache.orc.OrcUtils;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.impl.SchemaEvolution;
-import org.apache.tez.common.counters.TezCounters;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.slf4j.MDC;
 
 public class LlapInputFormat implements InputFormat<NullWritable, VectorizedRowBatch>,
     VectorizedInputFormatInterface, SelfDescribingInputFormatInterface,
@@ -77,16 +67,14 @@ public class LlapInputFormat implements InputFormat<NullWritable, VectorizedRowB
   private static final String NONVECTOR_SETTING_MESSAGE = "disable "
       + ConfVars.LLAP_IO_NONVECTOR_WRAPPER_ENABLED.varname + " to work around this error";
 
-  @SuppressWarnings("rawtypes")
-  private final InputFormat sourceInputFormat;
+  private final InputFormat<NullWritable, VectorizedRowBatch> sourceInputFormat;
   private final AvoidSplitCombination sourceASC;
-  @SuppressWarnings("deprecation")
   private final Deserializer sourceSerDe;
   final ColumnVectorProducer cvp;
   final ExecutorService executor;
   private final String hostName;
 
-  @SuppressWarnings("rawtypes")
+  @SuppressWarnings({ "rawtypes", "unchecked" })
   LlapInputFormat(InputFormat sourceInputFormat, Deserializer sourceSerDe,
       ColumnVectorProducer cvp, ExecutorService executor) {
     this.executor = executor;
@@ -101,42 +89,49 @@ public class LlapInputFormat implements InputFormat<NullWritable, VectorizedRowB
   @Override
   public RecordReader<NullWritable, VectorizedRowBatch> getRecordReader(
       InputSplit split, JobConf job, Reporter reporter) throws IOException {
-    RecordReader<NullWritable, VectorizedRowBatch> noLlap = checkLlapSplit(split, job, reporter);
+    // Check LLAP-aware split (e.g. OrcSplit) to make sure it's compatible.
+    RecordReader<NullWritable, VectorizedRowBatch> noLlap = checkLlapSplit(
+        split, job, reporter);
     if (noLlap != null) return noLlap;
 
-    boolean isVectorized = Utilities.getUseVectorizedInputFileFormat(job);
-
     FileSplit fileSplit = (FileSplit) split;
     reporter.setStatus(fileSplit.toString());
     try {
       List<Integer> includedCols = ColumnProjectionUtils.isReadAllColumns(job)
           ? null : ColumnProjectionUtils.getReadColumnIDs(job);
-      LlapRecordReader rr = new LlapRecordReader(job, fileSplit, includedCols, hostName, cvp,
-          executor, sourceInputFormat, sourceSerDe, reporter);
-      if (!rr.init()) {
+      LlapRecordReader rr = LlapRecordReader.create(job, fileSplit, includedCols, hostName,
+          cvp, executor, sourceInputFormat, sourceSerDe, reporter);
+      if (rr == null) {
+        // Reader-specific incompatibility like SMB or schema evolution.
         return sourceInputFormat.getRecordReader(split, job, reporter);
       }
-
-      return wrapLlapReader(isVectorized, includedCols, rr, split, job, reporter);
+      // For non-vectorized operator case, wrap the reader if possible.
+      RecordReader<NullWritable, VectorizedRowBatch> result = rr;
+      if (!Utilities.getUseVectorizedInputFileFormat(job)) {
+        result = wrapLlapReader(includedCols, rr, split);
+        if (result == null) {
+          // Cannot wrap a reader for non-vectorized pipeline.
+          return sourceInputFormat.getRecordReader(split, job, reporter);
+        }
+      }
+      // This starts the reader in the background.
+      rr.start();
+      return result;
     } catch (Exception ex) {
       throw new IOException(ex);
     }
   }
 
-  public RecordReader<NullWritable, VectorizedRowBatch> wrapLlapReader(
-      boolean isVectorized, List<Integer> includedCols, LlapRecordReader rr,
-      InputSplit split, JobConf job, Reporter reporter) throws IOException {
+  private RecordReader<NullWritable, VectorizedRowBatch> wrapLlapReader(
+      List<Integer> includedCols, LlapRecordReader rr, InputSplit split) throws IOException {
     // vectorized row batch reader
-    if (isVectorized) {
-      return rr;
-    } else if (sourceInputFormat instanceof BatchToRowInputFormat) {
+    if (sourceInputFormat instanceof BatchToRowInputFormat) {
       LlapIoImpl.LOG.info("Using batch-to-row converter for split: " + split);
       return bogusCast(((BatchToRowInputFormat) sourceInputFormat).getWrapper(
           rr, rr.getVectorizedRowBatchCtx(), includedCols));
-    } else {
-      LlapIoImpl.LOG.warn("Not using LLAP IO for an unsupported split: " + split);
-      return sourceInputFormat.getRecordReader(split, job, reporter);
     }
+    LlapIoImpl.LOG.warn("Not using LLAP IO for an unsupported split: " + split);
+    return null;
   }
 
   public RecordReader<NullWritable, VectorizedRowBatch> checkLlapSplit(
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
index d4e14a88c2..bbfe856843 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
@@ -25,8 +25,7 @@
 import java.util.List;
 import java.util.concurrent.ExecutorService;
 
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.llap.ConsumerFeedback;
@@ -38,6 +37,7 @@
 import org.apache.hadoop.hive.llap.io.decode.ReadPipeline;
 import org.apache.hadoop.hive.llap.tezplugins.LlapTezUtils;
 import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.exec.tez.DagUtils;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
@@ -45,8 +45,8 @@
 import org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.plan.BaseWork;
 import org.apache.hadoop.hive.ql.plan.MapWork;
-import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.io.NullWritable;
@@ -62,12 +62,14 @@
 import org.slf4j.LoggerFactory;
 import org.slf4j.MDC;
 
+import com.google.common.collect.Lists;
+
 class LlapRecordReader
     implements RecordReader<NullWritable, VectorizedRowBatch>, Consumer<ColumnVectorBatch> {
   private static final Logger LOG = LoggerFactory.getLogger(LlapRecordReader.class);
 
   private final FileSplit split;
-  private final List<Integer> columnIds;
+  private List<Integer> columnIds;
   private final SearchArgument sarg;
   private final String[] columnNames;
   private final VectorizedRowBatchCtx rbCtx;
@@ -86,19 +88,34 @@ class LlapRecordReader
   private long firstReturnTime;
 
   private final JobConf jobConf;
-  private final boolean[] includedColumns;
   private final ReadPipeline rp;
   private final ExecutorService executor;
   private final int columnCount;
-
-  private SchemaEvolution evolution;
-
   private final boolean isAcidScan;
 
-  public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols,
+  /**
+   * Creates the record reader and checks the input-specific compatibility.
+   * @return The reader if the split can be read, null otherwise.
+   */
+  public static LlapRecordReader create(JobConf job, FileSplit split, List<Integer> includedCols,
       String hostName, ColumnVectorProducer cvp, ExecutorService executor,
       InputFormat<?, ?> sourceInputFormat, Deserializer sourceSerDe, Reporter reporter)
           throws IOException, HiveException {
+    MapWork mapWork = findMapWork(job);
+    if (mapWork == null) return null; // No compatible MapWork.
+    LlapRecordReader rr = new LlapRecordReader(mapWork, job, split, includedCols, hostName,
+        cvp, executor, sourceInputFormat, sourceSerDe, reporter);
+    if (!rr.checkOrcSchemaEvolution()) {
+      rr.close();
+      return null;
+    }
+    return rr;
+  }
+
+  private LlapRecordReader(MapWork mapWork, JobConf job, FileSplit split,
+      List<Integer> includedCols, String hostName, ColumnVectorProducer cvp,
+      ExecutorService executor, InputFormat<?, ?> sourceInputFormat, Deserializer sourceSerDe,
+      Reporter reporter) throws IOException, HiveException {
     this.executor = executor;
     this.jobConf = job;
     this.split = split;
@@ -120,7 +137,12 @@ public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols
     this.counters = new QueryFragmentCounters(job, taskCounters);
     this.counters.setDesc(QueryFragmentCounters.Desc.MACHINE, hostName);
 
-    MapWork mapWork = Utilities.getMapWork(job);
+    isAcidScan = HiveConf.getBoolVar(jobConf, ConfVars.HIVE_TRANSACTIONAL_TABLE_SCAN);
+    TypeDescription schema = OrcInputFormat.getDesiredRowTypeDescr(
+        job, isAcidScan, Integer.MAX_VALUE);
+    this.columnIds = includedCols;
+    this.columnCount = columnIds.size();
+
     VectorizedRowBatchCtx ctx = mapWork.getVectorizedRowBatchCtx();
     rbCtx = ctx != null ? ctx : LlapInputFormat.createFakeVrbCtx(mapWork);
     if (includedCols == null) {
@@ -130,35 +152,56 @@ public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols
         includedCols.add(i);
       }
     }
-    this.columnIds = includedCols;
-    this.columnCount = columnIds.size();
 
     int partitionColumnCount = rbCtx.getPartitionColumnCount();
     if (partitionColumnCount > 0) {
       partitionValues = new Object[partitionColumnCount];
-      VectorizedRowBatchCtx.getPartitionValues(rbCtx, job, split, partitionValues);
+      VectorizedRowBatchCtx.getPartitionValues(rbCtx, mapWork, split, partitionValues);
     } else {
       partitionValues = null;
     }
 
-    isAcidScan = HiveConf.getBoolVar(jobConf, ConfVars.HIVE_TRANSACTIONAL_TABLE_SCAN);
-    TypeDescription schema = OrcInputFormat.getDesiredRowTypeDescr(
-        job, isAcidScan, Integer.MAX_VALUE);
-
     // Create the consumer of encoded data; it will coordinate decoding to CVBs.
     feedback = rp = cvp.createReadPipeline(this, split, columnIds, sarg, columnNames,
         counters, schema, sourceInputFormat, sourceSerDe, reporter, job,
         mapWork.getPathToPartitionInfo());
-    evolution = rp.getSchemaEvolution();
-    includedColumns = rp.getIncludedColumns();
+  }
+
+  private static MapWork findMapWork(JobConf job) throws HiveException {
+    String inputName = job.get(Utilities.INPUT_NAME, null);
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Initializing for input " + inputName);
+    }
+    String prefixes = job.get(DagUtils.TEZ_MERGE_WORK_FILE_PREFIXES);
+    if (prefixes != null && !StringUtils.isBlank(prefixes)) {
+      // Currently SMB is broken, so we cannot check if it's  compatible with IO elevator.
+      // So, we don't use the below code that would get the correct MapWork. See HIVE-16985.
+      return null;
+    }
+
+    BaseWork work = null;
+    // HIVE-16985: try to find the fake merge work for SMB join, that is really another MapWork.
+    /*
+    if (inputName != null) {
+      if (prefixes == null ||
+          !Lists.newArrayList(prefixes.split(",")).contains(inputName)) {
+        inputName = null;
+      }
+    }
+    if (inputName != null) {
+      work = Utilities.getMergeWork(job, inputName);
+    }
+    */
+    if (work == null || !(work instanceof MapWork)) {
+      work = Utilities.getMapWork(job);
+    }
+    return (MapWork) work;
   }
 
   /**
    * Starts the data read pipeline
    */
-  public boolean init() {
-    if (!checkOrcSchemaEvolution()) return false;
-
+  public void start() {
     // perform the data read asynchronously
     if (executor instanceof StatsRecordingThreadPool) {
       // Every thread created by this thread pool will use the same handler
@@ -166,10 +209,10 @@ public boolean init() {
           new IOUncaughtExceptionHandler());
     }
     executor.submit(rp.getReadCallable());
-    return true;
   }
 
   private boolean checkOrcSchemaEvolution() {
+    SchemaEvolution evolution = rp.getSchemaEvolution();
     for (int i = 0; i < columnCount; ++i) {
       int projectedColId = columnIds == null ? i : columnIds.get(i);
       // Adjust file column index for ORC struct.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java
index e546a658c7..3c12e04a6c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java
@@ -39,6 +39,7 @@
 import org.apache.hadoop.hive.ql.io.IOPrepareCache;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.Explain;
+import org.apache.hadoop.hive.ql.plan.MapWork;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.ql.plan.Explain.Level;
 import org.apache.hadoop.hive.ql.plan.Explain.Vectorization;
@@ -145,16 +146,21 @@ public void init(StructObjectInspector structObjectInspector, String[] scratchCo
 
   public static void getPartitionValues(VectorizedRowBatchCtx vrbCtx, Configuration hiveConf,
       FileSplit split, Object[] partitionValues) throws IOException {
+    // TODO: this is invalid for SMB. Keep this for now for legacy reasons. See the other overload.
+    MapWork mapWork = Utilities.getMapWork(hiveConf);
+    getPartitionValues(vrbCtx, mapWork, split, partitionValues);
+  }
 
-    Map<Path, PartitionDesc> pathToPartitionInfo = Utilities
-        .getMapWork(hiveConf).getPathToPartitionInfo();
+  public static void getPartitionValues(VectorizedRowBatchCtx vrbCtx,
+      MapWork mapWork, FileSplit split, Object[] partitionValues)
+      throws IOException {
+    Map<Path, PartitionDesc> pathToPartitionInfo = mapWork.getPathToPartitionInfo();
 
     PartitionDesc partDesc = HiveFileFormatUtils
         .getPartitionDescFromPathRecursively(pathToPartitionInfo,
             split.getPath(), IOPrepareCache.get().getPartitionDescMap());
 
     getPartitionValues(vrbCtx, partDesc, partitionValues);
-
   }
 
   public static void getPartitionValues(VectorizedRowBatchCtx vrbCtx, PartitionDesc partDesc,
diff --git a/ql/src/test/queries/clientpositive/llap_smb.q b/ql/src/test/queries/clientpositive/llap_smb.q
new file mode 100644
index 0000000000..83681ea75e
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/llap_smb.q
@@ -0,0 +1,53 @@
+set hive.mapred.mode=nonstrict;
+SET hive.vectorized.execution.enabled=true;
+
+SET hive.llap.io.enabled=false;
+SET hive.exec.orc.default.buffer.size=32768;
+SET hive.exec.orc.default.row.index.stride=1000;
+SET hive.optimize.index.filter=true;
+set hive.fetch.task.conversion=none;
+
+set hive.exec.dynamic.partition.mode=nonstrict;
+
+DROP TABLE orc_a;
+DROP TABLE orc_b;
+
+CREATE TABLE orc_a (id bigint, cdouble double) partitioned by (y int, q smallint)
+  CLUSTERED BY (id) SORTED BY (id) INTO 2 BUCKETS stored as orc;
+CREATE TABLE orc_b (id bigint, cfloat float)
+  CLUSTERED BY (id) SORTED BY (id) INTO 2 BUCKETS stored as orc;
+
+insert into table orc_a partition (y=2000, q)
+select cbigint, cdouble, csmallint % 10 from alltypesorc
+  where cbigint is not null and csmallint > 0 order by cbigint asc;
+insert into table orc_a partition (y=2001, q)
+select cbigint, cdouble, csmallint % 10 from alltypesorc
+  where cbigint is not null and csmallint > 0 order by cbigint asc;
+
+insert into table orc_b 
+select cbigint, cfloat from alltypesorc
+  where cbigint is not null and csmallint > 0 order by cbigint asc limit 200;
+
+set hive.cbo.enable=false;
+
+select y,q,count(*) from orc_a a join orc_b b on a.id=b.id group by y,q;
+
+
+
+SET hive.llap.io.enabled=false;
+set hive.enforce.sortmergebucketmapjoin=false;
+set hive.optimize.bucketmapjoin=true;
+set hive.optimize.bucketmapjoin.sortedmerge=true;
+set hive.auto.convert.sortmerge.join=true;
+set hive.auto.convert.join=true;
+set hive.auto.convert.join.noconditionaltask.size=10;
+
+explain
+select y,q,count(*) from orc_a a join orc_b b on a.id=b.id group by y,q;
+
+-- The results are currently incorrect. See HIVE-16985/HIVE-16965
+
+select y,q,count(*) from orc_a a join orc_b b on a.id=b.id group by y,q;
+
+DROP TABLE orc_a;
+DROP TABLE orc_b;
diff --git a/ql/src/test/results/clientpositive/llap/llap_smb.q.out b/ql/src/test/results/clientpositive/llap/llap_smb.q.out
new file mode 100644
index 0000000000..4cb620a662
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/llap_smb.q.out
@@ -0,0 +1,341 @@
+PREHOOK: query: DROP TABLE orc_a
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE orc_a
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: DROP TABLE orc_b
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE orc_b
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE orc_a (id bigint, cdouble double) partitioned by (y int, q smallint)
+  CLUSTERED BY (id) SORTED BY (id) INTO 2 BUCKETS stored as orc
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@orc_a
+POSTHOOK: query: CREATE TABLE orc_a (id bigint, cdouble double) partitioned by (y int, q smallint)
+  CLUSTERED BY (id) SORTED BY (id) INTO 2 BUCKETS stored as orc
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@orc_a
+PREHOOK: query: CREATE TABLE orc_b (id bigint, cfloat float)
+  CLUSTERED BY (id) SORTED BY (id) INTO 2 BUCKETS stored as orc
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@orc_b
+POSTHOOK: query: CREATE TABLE orc_b (id bigint, cfloat float)
+  CLUSTERED BY (id) SORTED BY (id) INTO 2 BUCKETS stored as orc
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@orc_b
+PREHOOK: query: insert into table orc_a partition (y=2000, q)
+select cbigint, cdouble, csmallint % 10 from alltypesorc
+  where cbigint is not null and csmallint > 0 order by cbigint asc
+PREHOOK: type: QUERY
+PREHOOK: Input: default@alltypesorc
+PREHOOK: Output: default@orc_a@y=2000
+POSTHOOK: query: insert into table orc_a partition (y=2000, q)
+select cbigint, cdouble, csmallint % 10 from alltypesorc
+  where cbigint is not null and csmallint > 0 order by cbigint asc
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@alltypesorc
+POSTHOOK: Output: default@orc_a@y=2000/q=0
+POSTHOOK: Output: default@orc_a@y=2000/q=1
+POSTHOOK: Output: default@orc_a@y=2000/q=2
+POSTHOOK: Output: default@orc_a@y=2000/q=3
+POSTHOOK: Output: default@orc_a@y=2000/q=4
+POSTHOOK: Output: default@orc_a@y=2000/q=5
+POSTHOOK: Output: default@orc_a@y=2000/q=6
+POSTHOOK: Output: default@orc_a@y=2000/q=7
+POSTHOOK: Output: default@orc_a@y=2000/q=8
+POSTHOOK: Output: default@orc_a@y=2000/q=9
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=0).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=0).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=1).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=1).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=2).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=2).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=3).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=3).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=4).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=4).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=5).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=5).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=6).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=6).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=7).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=7).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=8).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=8).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=9).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2000,q=9).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+PREHOOK: query: insert into table orc_a partition (y=2001, q)
+select cbigint, cdouble, csmallint % 10 from alltypesorc
+  where cbigint is not null and csmallint > 0 order by cbigint asc
+PREHOOK: type: QUERY
+PREHOOK: Input: default@alltypesorc
+PREHOOK: Output: default@orc_a@y=2001
+POSTHOOK: query: insert into table orc_a partition (y=2001, q)
+select cbigint, cdouble, csmallint % 10 from alltypesorc
+  where cbigint is not null and csmallint > 0 order by cbigint asc
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@alltypesorc
+POSTHOOK: Output: default@orc_a@y=2001/q=0
+POSTHOOK: Output: default@orc_a@y=2001/q=1
+POSTHOOK: Output: default@orc_a@y=2001/q=2
+POSTHOOK: Output: default@orc_a@y=2001/q=3
+POSTHOOK: Output: default@orc_a@y=2001/q=4
+POSTHOOK: Output: default@orc_a@y=2001/q=5
+POSTHOOK: Output: default@orc_a@y=2001/q=6
+POSTHOOK: Output: default@orc_a@y=2001/q=7
+POSTHOOK: Output: default@orc_a@y=2001/q=8
+POSTHOOK: Output: default@orc_a@y=2001/q=9
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=0).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=0).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=1).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=1).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=2).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=2).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=3).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=3).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=4).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=4).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=5).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=5).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=6).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=6).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=7).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=7).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=8).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=8).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=9).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
+POSTHOOK: Lineage: orc_a PARTITION(y=2001,q=9).id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+PREHOOK: query: insert into table orc_b 
+select cbigint, cfloat from alltypesorc
+  where cbigint is not null and csmallint > 0 order by cbigint asc limit 200
+PREHOOK: type: QUERY
+PREHOOK: Input: default@alltypesorc
+PREHOOK: Output: default@orc_b
+POSTHOOK: query: insert into table orc_b 
+select cbigint, cfloat from alltypesorc
+  where cbigint is not null and csmallint > 0 order by cbigint asc limit 200
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@alltypesorc
+POSTHOOK: Output: default@orc_b
+POSTHOOK: Lineage: orc_b.cfloat SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cfloat, type:float, comment:null), ]
+POSTHOOK: Lineage: orc_b.id SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
+PREHOOK: query: select y,q,count(*) from orc_a a join orc_b b on a.id=b.id group by y,q
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_a
+PREHOOK: Input: default@orc_a@y=2000/q=0
+PREHOOK: Input: default@orc_a@y=2000/q=1
+PREHOOK: Input: default@orc_a@y=2000/q=2
+PREHOOK: Input: default@orc_a@y=2000/q=3
+PREHOOK: Input: default@orc_a@y=2000/q=4
+PREHOOK: Input: default@orc_a@y=2000/q=5
+PREHOOK: Input: default@orc_a@y=2000/q=6
+PREHOOK: Input: default@orc_a@y=2000/q=7
+PREHOOK: Input: default@orc_a@y=2000/q=8
+PREHOOK: Input: default@orc_a@y=2000/q=9
+PREHOOK: Input: default@orc_a@y=2001/q=0
+PREHOOK: Input: default@orc_a@y=2001/q=1
+PREHOOK: Input: default@orc_a@y=2001/q=2
+PREHOOK: Input: default@orc_a@y=2001/q=3
+PREHOOK: Input: default@orc_a@y=2001/q=4
+PREHOOK: Input: default@orc_a@y=2001/q=5
+PREHOOK: Input: default@orc_a@y=2001/q=6
+PREHOOK: Input: default@orc_a@y=2001/q=7
+PREHOOK: Input: default@orc_a@y=2001/q=8
+PREHOOK: Input: default@orc_a@y=2001/q=9
+PREHOOK: Input: default@orc_b
+#### A masked pattern was here ####
+POSTHOOK: query: select y,q,count(*) from orc_a a join orc_b b on a.id=b.id group by y,q
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_a
+POSTHOOK: Input: default@orc_a@y=2000/q=0
+POSTHOOK: Input: default@orc_a@y=2000/q=1
+POSTHOOK: Input: default@orc_a@y=2000/q=2
+POSTHOOK: Input: default@orc_a@y=2000/q=3
+POSTHOOK: Input: default@orc_a@y=2000/q=4
+POSTHOOK: Input: default@orc_a@y=2000/q=5
+POSTHOOK: Input: default@orc_a@y=2000/q=6
+POSTHOOK: Input: default@orc_a@y=2000/q=7
+POSTHOOK: Input: default@orc_a@y=2000/q=8
+POSTHOOK: Input: default@orc_a@y=2000/q=9
+POSTHOOK: Input: default@orc_a@y=2001/q=0
+POSTHOOK: Input: default@orc_a@y=2001/q=1
+POSTHOOK: Input: default@orc_a@y=2001/q=2
+POSTHOOK: Input: default@orc_a@y=2001/q=3
+POSTHOOK: Input: default@orc_a@y=2001/q=4
+POSTHOOK: Input: default@orc_a@y=2001/q=5
+POSTHOOK: Input: default@orc_a@y=2001/q=6
+POSTHOOK: Input: default@orc_a@y=2001/q=7
+POSTHOOK: Input: default@orc_a@y=2001/q=8
+POSTHOOK: Input: default@orc_a@y=2001/q=9
+POSTHOOK: Input: default@orc_b
+#### A masked pattern was here ####
+2000	2	6578
+2001	8	9438
+2000	3	6149
+2000	5	5720
+2000	9	8151
+2001	0	6721
+2001	1	7493
+2001	2	6578
+2001	4	7865
+2001	9	8151
+2000	1	7493
+2000	7	6149
+2000	8	9438
+2001	6	5577
+2001	7	6149
+2000	0	6721
+2000	4	7865
+2000	6	5577
+2001	3	6149
+2001	5	5720
+PREHOOK: query: explain
+select y,q,count(*) from orc_a a join orc_b b on a.id=b.id group by y,q
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select y,q,count(*) from orc_a a join orc_b b on a.id=b.id group by y,q
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  filterExpr: id is not null (type: boolean)
+                  Statistics: Num rows: 200 Data size: 1828 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: id is not null (type: boolean)
+                    Statistics: Num rows: 200 Data size: 1828 Basic stats: COMPLETE Column stats: NONE
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  filterExpr: id is not null (type: boolean)
+                  Statistics: Num rows: 5000 Data size: 120000 Basic stats: COMPLETE Column stats: PARTIAL
+                  Filter Operator
+                    predicate: id is not null (type: boolean)
+                    Statistics: Num rows: 5000 Data size: 40000 Basic stats: COMPLETE Column stats: PARTIAL
+                    Merge Join Operator
+                      condition map:
+                           Inner Join 0 to 1
+                      keys:
+                        0 id (type: bigint)
+                        1 id (type: bigint)
+                      outputColumnNames: _col2, _col3
+                      Statistics: Num rows: 5500 Data size: 44000 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: count()
+                        keys: _col2 (type: int), _col3 (type: smallint)
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2
+                        Statistics: Num rows: 5500 Data size: 44000 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          key expressions: _col0 (type: int), _col1 (type: smallint)
+                          sort order: ++
+                          Map-reduce partition columns: _col0 (type: int), _col1 (type: smallint)
+                          Statistics: Num rows: 5500 Data size: 44000 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col2 (type: bigint)
+            Execution mode: llap
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                keys: KEY._col0 (type: int), KEY._col1 (type: smallint)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 2750 Data size: 22000 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 2750 Data size: 22000 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select y,q,count(*) from orc_a a join orc_b b on a.id=b.id group by y,q
+PREHOOK: type: QUERY
+PREHOOK: Input: default@orc_a
+PREHOOK: Input: default@orc_a@y=2000/q=0
+PREHOOK: Input: default@orc_a@y=2000/q=1
+PREHOOK: Input: default@orc_a@y=2000/q=2
+PREHOOK: Input: default@orc_a@y=2000/q=3
+PREHOOK: Input: default@orc_a@y=2000/q=4
+PREHOOK: Input: default@orc_a@y=2000/q=5
+PREHOOK: Input: default@orc_a@y=2000/q=6
+PREHOOK: Input: default@orc_a@y=2000/q=7
+PREHOOK: Input: default@orc_a@y=2000/q=8
+PREHOOK: Input: default@orc_a@y=2000/q=9
+PREHOOK: Input: default@orc_a@y=2001/q=0
+PREHOOK: Input: default@orc_a@y=2001/q=1
+PREHOOK: Input: default@orc_a@y=2001/q=2
+PREHOOK: Input: default@orc_a@y=2001/q=3
+PREHOOK: Input: default@orc_a@y=2001/q=4
+PREHOOK: Input: default@orc_a@y=2001/q=5
+PREHOOK: Input: default@orc_a@y=2001/q=6
+PREHOOK: Input: default@orc_a@y=2001/q=7
+PREHOOK: Input: default@orc_a@y=2001/q=8
+PREHOOK: Input: default@orc_a@y=2001/q=9
+PREHOOK: Input: default@orc_b
+#### A masked pattern was here ####
+POSTHOOK: query: select y,q,count(*) from orc_a a join orc_b b on a.id=b.id group by y,q
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@orc_a
+POSTHOOK: Input: default@orc_a@y=2000/q=0
+POSTHOOK: Input: default@orc_a@y=2000/q=1
+POSTHOOK: Input: default@orc_a@y=2000/q=2
+POSTHOOK: Input: default@orc_a@y=2000/q=3
+POSTHOOK: Input: default@orc_a@y=2000/q=4
+POSTHOOK: Input: default@orc_a@y=2000/q=5
+POSTHOOK: Input: default@orc_a@y=2000/q=6
+POSTHOOK: Input: default@orc_a@y=2000/q=7
+POSTHOOK: Input: default@orc_a@y=2000/q=8
+POSTHOOK: Input: default@orc_a@y=2000/q=9
+POSTHOOK: Input: default@orc_a@y=2001/q=0
+POSTHOOK: Input: default@orc_a@y=2001/q=1
+POSTHOOK: Input: default@orc_a@y=2001/q=2
+POSTHOOK: Input: default@orc_a@y=2001/q=3
+POSTHOOK: Input: default@orc_a@y=2001/q=4
+POSTHOOK: Input: default@orc_a@y=2001/q=5
+POSTHOOK: Input: default@orc_a@y=2001/q=6
+POSTHOOK: Input: default@orc_a@y=2001/q=7
+POSTHOOK: Input: default@orc_a@y=2001/q=8
+POSTHOOK: Input: default@orc_a@y=2001/q=9
+POSTHOOK: Input: default@orc_b
+#### A masked pattern was here ####
+2001	4	139630
+2001	6	52
+PREHOOK: query: DROP TABLE orc_a
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@orc_a
+PREHOOK: Output: default@orc_a
+POSTHOOK: query: DROP TABLE orc_a
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@orc_a
+POSTHOOK: Output: default@orc_a
+PREHOOK: query: DROP TABLE orc_b
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@orc_b
+PREHOOK: Output: default@orc_b
+POSTHOOK: query: DROP TABLE orc_b
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@orc_b
+POSTHOOK: Output: default@orc_b
