diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
index 420b160147..351b35be7a 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
@@ -76,6 +76,7 @@
 import org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork;
 import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;
 import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.metadata.StringAppender;
 import org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData;
 import org.apache.hadoop.hive.ql.parse.repl.load.EventDumpDirComparator;
 import org.apache.hadoop.hive.ql.parse.repl.load.metric.BootstrapLoadMetricCollector;
@@ -115,6 +116,8 @@
 import java.nio.charset.StandardCharsets;
 import java.util.concurrent.TimeUnit;
 import java.util.Base64;
+import org.apache.logging.log4j.Level;
+import org.apache.logging.log4j.LogManager;
 
 import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;
 import static org.apache.hadoop.hive.metastore.Warehouse.DEFAULT_CATALOG_NAME;
@@ -1069,7 +1072,7 @@ public void testMultipleDbMetadataOnlyDump() throws IOException {
     String bootstrapDb = dbName + "_boot";
 
     //insert data in the additional db
-    String[] unptn_data = new String[]{ "eleven" , "twelve" };
+    String[] unptn_data = new String[]{"eleven", "twelve"};
     String unptn_locn = new Path(TEST_PATH, name + "_unptn").toUri().getPath();
     createTestDataFile(unptn_locn, unptn_data);
     run("CREATE DATABASE " + bootstrapDb, driver);
@@ -1088,7 +1091,7 @@ public void testMultipleDbMetadataOnlyDump() throws IOException {
 
     //create new database and dump all databases again
     String incDbName1 = dbName + "_inc1";
-    run("CREATE DATABASE " + incDbName1 , driver);
+    run("CREATE DATABASE " + incDbName1, driver);
     run("CREATE TABLE " + incDbName1 + ".unptned(a string) STORED AS TEXTFILE", driver);
     run("LOAD DATA LOCAL INPATH '" + unptn_locn + "' OVERWRITE INTO TABLE " + incDbName1 + ".unptned", true, driver);
     verifySetup("SELECT * from " + incDbName1 + ".unptned", unptn_data, driver);
@@ -1102,13 +1105,77 @@ public void testMultipleDbMetadataOnlyDump() throws IOException {
 
     //create new database and dump all databases again
     String incDbName2 = dbName + "_inc2";
-    run("CREATE DATABASE " + incDbName2 , driver);
+    run("CREATE DATABASE " + incDbName2, driver);
     run("CREATE TABLE " + incDbName2 + ".unptned(a string) STORED AS TEXTFILE", driver);
     run("LOAD DATA LOCAL INPATH '" + unptn_locn + "' OVERWRITE INTO TABLE " + incDbName2 + ".unptned", true, driver);
     verifySetup("SELECT * from " + incDbName2 + ".unptned", unptn_data, driver);
     replDumpAllDbs(metadataOnlyClause);
   }
 
+  @Test
+  public void testIncrementalLogs() throws IOException {
+    verifySetupSteps = true;
+    String name = testName.getMethodName();
+    org.apache.logging.log4j.Logger logger = LogManager.getLogger("hive.ql.metadata.HIVE");
+    StringAppender appender = StringAppender.createStringAppender("%m");
+    appender.addToLogger(logger.getName(), Level.INFO);
+    appender.start();
+
+    String dbName = createDB(name, driver);
+    String replDbName = dbName + "_dupe";
+
+    run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);
+    run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);
+
+    Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
+
+    String[] unptn_data = new String[]{ "eleven" , "twelve" };
+    String[] ptn_data_1 = new String[]{ "thirteen", "fourteen", "fifteen"};
+    String[] ptn_data_2 = new String[]{ "fifteen", "sixteen", "seventeen"};
+    String[] empty = new String[]{};
+
+    String unptn_locn = new Path(TEST_PATH, name + "_unptn").toUri().getPath();
+    String ptn_locn_1 = new Path(TEST_PATH, name + "_ptn1").toUri().getPath();
+    String ptn_locn_2 = new Path(TEST_PATH, name + "_ptn2").toUri().getPath();
+
+    createTestDataFile(unptn_locn, unptn_data);
+    createTestDataFile(ptn_locn_1, ptn_data_1);
+    createTestDataFile(ptn_locn_2, ptn_data_2);
+
+
+    run("LOAD DATA LOCAL INPATH '" + unptn_locn + "' OVERWRITE INTO TABLE " + dbName + ".unptned", true, driver);
+    verifySetup("SELECT * from " + dbName + ".unptned", unptn_data, driver);
+
+    run("LOAD DATA LOCAL INPATH '" + ptn_locn_1 + "' OVERWRITE INTO TABLE " + dbName + ".ptned PARTITION(b=1)", true, driver);
+    verifySetup("SELECT a from " + dbName + ".ptned WHERE b=1", ptn_data_1, driver);
+    run("LOAD DATA LOCAL INPATH '" + ptn_locn_2 + "' OVERWRITE INTO TABLE " + dbName + ".ptned PARTITION(b=2)", true, driver);
+    verifySetup("SELECT a from " + dbName + ".ptned WHERE b=2", ptn_data_2, driver);
+
+    run("CREATE TABLE " + dbName + ".ptned_late(a string) PARTITIONED BY (b int) STORED AS TEXTFILE", driver);
+    run("LOAD DATA LOCAL INPATH '" + ptn_locn_1 + "' OVERWRITE INTO TABLE " + dbName + ".ptned_late PARTITION(b=1)", true, driver);
+    verifySetup("SELECT a from " + dbName + ".ptned_late WHERE b=1",ptn_data_1, driver);
+    run("LOAD DATA LOCAL INPATH '" + ptn_locn_2 + "' OVERWRITE INTO TABLE " + dbName + ".ptned_late PARTITION(b=2)", true, driver);
+    verifySetup("SELECT a from " + dbName + ".ptned_late WHERE b=2", ptn_data_2, driver);
+
+    // Perform REPL-DUMP/LOAD
+    // Set approx load tasks to a low value to trigger REPL_LOAD execution multiple times
+    List<String> replApproxTasksClause = Arrays.asList("'" + HiveConf.ConfVars.REPL_APPROX_MAX_LOAD_TASKS.varname + "'='1'");
+    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDbName, replApproxTasksClause);
+    FileSystem fs = new Path(bootstrapDump.dumpLocation).getFileSystem(hconf);
+    Path dumpPath = new Path(incrementalDump.dumpLocation, ReplUtils.REPL_HIVE_BASE_DIR);
+    assertTrue(fs.exists(new Path(dumpPath, DUMP_ACKNOWLEDGEMENT.toString())));
+    assertTrue(fs.exists(new Path(dumpPath, LOAD_ACKNOWLEDGEMENT.toString())));
+    verifyIncrementalLogs(appender);
+
+    verifyRun("SELECT * from " + replDbName + ".unptned", unptn_data, driverMirror);
+    verifyRun("SELECT a from " + replDbName + ".ptned WHERE b=1", ptn_data_1, driverMirror);
+    verifyRun("SELECT a from " + replDbName + ".ptned WHERE b=2", ptn_data_2, driverMirror);
+    verifyRun("SELECT a from " + replDbName + ".ptned_late WHERE b=1", ptn_data_1, driverMirror);
+    verifyRun("SELECT a from " + replDbName + ".ptned_late WHERE b=2", ptn_data_2, driverMirror);
+    appender.removeFromLogger(logger.getName());
+    verifySetupSteps = false;
+  }
+
   @Test
   public void testIncrementalLoadWithVariableLengthEventId() throws IOException, TException {
     String testName = "incrementalLoadWithVariableLengthEventId";
@@ -4792,6 +4859,22 @@ public static Path getNonRecoverablePath(Path dumpDir, String dbName, HiveConf c
     return null;
   }
 
+  private void verifyIncrementalLogs(StringAppender appender) {
+    String logStr = appender.getOutput();
+    String eventStr = "REPL::EVENT_LOAD:";
+    String eventDurationStr = "eventDuration";
+    String incLoadStageStr = "REPL_INCREMENTAL_LOAD";
+    String incLoadTaskDurationStr = "REPL_INCREMENTAL_LOAD stage duration";
+    String incTaskBuilderDurationStr = "REPL_INCREMENTAL_LOAD task-builder";
+    assertTrue(logStr, logStr.contains(eventStr));
+    //verify for each loaded event, there is the event duration log
+    assertEquals(StringUtils.countMatches(logStr, eventStr), StringUtils.countMatches(logStr, eventDurationStr));
+    //verify for each repl-load stage, there is one log for entire stage-duration and one log for DAG-duration(builder)
+    assertTrue(StringUtils.countMatches(logStr, incLoadStageStr) > 3);
+    assertEquals(StringUtils.countMatches(logStr, incLoadStageStr) / 3, StringUtils.countMatches(logStr, incTaskBuilderDurationStr));
+    assertEquals(StringUtils.countMatches(logStr, incLoadStageStr) / 3, StringUtils.countMatches(logStr, incLoadTaskDurationStr));
+  }
+
   private void deleteNewMetadataFields(Tuple dump) throws SemanticException {
     Path dumpHiveDir = new Path(dump.dumpLocation, ReplUtils.REPL_HIVE_BASE_DIR);
     DumpMetaData dmd = new DumpMetaData(dumpHiveDir, hconf);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
index 4ffd148322..a070930e1d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
@@ -20,6 +20,7 @@
 import org.apache.hadoop.hive.common.repl.ReplConst;
 import org.apache.hadoop.fs.Options;
 import org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils;
+import org.apache.hadoop.hive.ql.parse.repl.load.log.IncrementalLoadLogger;
 import org.apache.thrift.TException;
 import com.google.common.collect.Collections2;
 import org.apache.commons.lang3.StringUtils;
@@ -120,6 +121,7 @@ private static class Scope {
   @Override
   public int execute() {
     try {
+      long loadTaskStartTime = System.currentTimeMillis();
       SecurityUtils.reloginExpiringKeytabUser();
       Task<?> rootTask = work.getRootTask();
       if (rootTask != null) {
@@ -138,7 +140,7 @@ public int execute() {
       }
       LOG.info("Data copy at load enabled : {}", conf.getBoolVar(HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET));
       if (work.isIncrementalLoad()) {
-        return executeIncrementalLoad();
+        return executeIncrementalLoad(loadTaskStartTime);
       } else {
         return executeBootStrapLoad();
       }
@@ -623,7 +625,7 @@ private void createBuilderTask(List<Task<?>> rootTasks) {
     DAGTraversal.traverse(rootTasks, new AddDependencyToLeaves(loadTask));
   }
 
-  private int executeIncrementalLoad() throws Exception {
+  private int executeIncrementalLoad(long loadStartTime) throws Exception {
     // If replication policy is changed between previous and current repl load, then drop the tables
     // that are excluded in the new replication policy.
     if (work.replScopeModified) {
@@ -703,6 +705,11 @@ private int executeIncrementalLoad() throws Exception {
       cleanupSnapshots(new Path(work.getDumpDirectory()).getParent().getParent().getParent(),
           work.getSourceDbName().toLowerCase(), conf, null, true);
     }
+
+    //pass the current time at the end of repl-load stage as the starting time of the first event.
+    long currentTimestamp = System.currentTimeMillis();
+    ((IncrementalLoadLogger)work.incrementalLoadTasksBuilder().getReplLogger()).initiateEventTimestamp(currentTimestamp);
+    LOG.info("REPL_INCREMENTAL_LOAD stage duration : {} ms", currentTimestamp - loadStartTime);
     return 0;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java
index f8f062a97b..2451639e06 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java
@@ -103,6 +103,7 @@ public IncrementalLoadTasksBuilder(String dbName, String loadPath,
 
   public Task<?> build(Context context, Hive hive, Logger log,
                                             TaskTracker tracker) throws Exception {
+    long builderStartTime = System.currentTimeMillis();
     Task<?> evTaskRoot = TaskFactory.get(new DependencyCollectionWork());
     Task<?> taskChainTail = evTaskRoot;
     Long lastReplayedEvent = null;
@@ -178,6 +179,8 @@ public Task<?> build(Context context, Hive hive, Logger log,
               taskChainTail.getClass(), taskChainTail.getId(),
               barrierTask.getClass(), barrierTask.getId());
     }
+    this.log.info("REPL_INCREMENTAL_LOAD task-builder iteration #{}, duration : {} ms",
+            numIteration, System.currentTimeMillis() - builderStartTime);
     return evTaskRoot;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/IncrementalLoadLogger.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/IncrementalLoadLogger.java
index 9e3e2d9292..5612c2dd93 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/IncrementalLoadLogger.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/IncrementalLoadLogger.java
@@ -33,12 +33,20 @@ public class IncrementalLoadLogger extends ReplLogger<String> {
   private String dumpDir;
   private long numEvents;
   private long eventSeqNo;
+  private long currentEventTimestamp;
 
   public IncrementalLoadLogger(String dbName, String dumpDir, int numEvents) {
     this.dbName = dbName;
     this.dumpDir = dumpDir;
     this.numEvents = numEvents;
     this.eventSeqNo = 0;
+    this.currentEventTimestamp = 0;
+  }
+
+  public void initiateEventTimestamp(long timestamp) {
+    if (this.currentEventTimestamp == 0) {
+      this.currentEventTimestamp = timestamp;
+    }
   }
 
   @Override
@@ -49,8 +57,11 @@ public void startLog() {
   @Override
   public void eventLog(String eventId, String eventType) {
     eventSeqNo++;
-    (new IncrementalLoadEvent(dbName, eventId, eventType, eventSeqNo, numEvents))
-            .log(LogTag.EVENT_LOAD);
+    long previousEventTimestamp = this.currentEventTimestamp;
+    IncrementalLoadEvent incEvent = new IncrementalLoadEvent(dbName,
+            eventId, eventType, eventSeqNo, numEvents, previousEventTimestamp);
+    incEvent.log(LogTag.EVENT_LOAD);
+    this.currentEventTimestamp = incEvent.getLoadTimeMillis();
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/state/IncrementalLoadEvent.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/state/IncrementalLoadEvent.java
index c8c93f9d34..cdeca81a09 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/state/IncrementalLoadEvent.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/state/IncrementalLoadEvent.java
@@ -36,16 +36,28 @@ public class IncrementalLoadEvent extends ReplState {
   @JsonProperty
   private Long loadTime;
 
+  @JsonProperty
+  private String eventDuration;
+
+  private long loadTimeMillis;
+
   public IncrementalLoadEvent(String dbName,
                               String eventId,
                               String eventType,
                               long eventSeqNo,
-                              long numEvents) {
+                              long numEvents,
+                              long previousTimestamp) {
     this.dbName = dbName;
     this.eventId = eventId;
     this.eventType = eventType;
     this.eventsLoadProgress = new String(new StringBuilder()
                                             .append(eventSeqNo).append("/").append(numEvents));
-    this.loadTime = System.currentTimeMillis() / 1000;
+    this.loadTimeMillis = System.currentTimeMillis();
+    this.loadTime = loadTimeMillis / 1000;
+    this.eventDuration = (this.loadTimeMillis - previousTimestamp) + " ms";
+  }
+
+  public long getLoadTimeMillis() {
+    return this.loadTimeMillis;
   }
 }
