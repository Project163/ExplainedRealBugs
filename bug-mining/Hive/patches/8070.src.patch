diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/ReplicationTestUtils.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/ReplicationTestUtils.java
index cbf19eaae0..0859218573 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/ReplicationTestUtils.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/ReplicationTestUtils.java
@@ -320,7 +320,7 @@ public static void insertIntoDB(WarehouseInstance primary, String dbName, String
             .run(txnStrCommit);
   }
 
-  private static void insertIntoDB(WarehouseInstance primary, String dbName, String tableName,
+  public static void insertIntoDB(WarehouseInstance primary, String dbName, String tableName,
                                    String tableProperty, String storageType, String[] resultArray)
           throws Throwable {
     insertIntoDB(primary, dbName, tableName, tableProperty, storageType, resultArray, false);
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java
index 575ffc1725..05c7d2c71a 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java
@@ -17,11 +17,15 @@
  */
 package org.apache.hadoop.hive.ql.parse;
 
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClientWithLocalCache;
 import org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder;
+import org.apache.hadoop.hive.ql.parse.repl.CopyUtils;
 import org.apache.hadoop.hive.shims.Utils;
 import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;
 import org.junit.rules.TestName;
@@ -34,6 +38,7 @@
 import org.junit.Test;
 import org.junit.BeforeClass;
 import org.junit.AfterClass;
+
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -243,6 +248,64 @@ public void testReplCM() throws Throwable {
             Lists.newArrayList(result, result));
   }
 
+  @Test
+  public void testReplCommitTransactionOnSourceDeleteORC() throws Throwable {
+    // Run test with ORC format & with transactional true.
+    testReplCommitTransactionOnSourceDelete("STORED AS ORC", "'transactional'='true'");
+  }
+
+  @Test
+  public void testReplCommitTransactionOnSourceDeleteText() throws Throwable {
+    // Run test with TEXT format & with transactional false.
+    testReplCommitTransactionOnSourceDelete("STORED AS TEXTFILE", "'transactional'='false'");
+  }
+
+  public void testReplCommitTransactionOnSourceDelete(String tableStorage, String tableProperty) throws Throwable {
+    String tableName = "testReplCommitTransactionOnSourceDelete";
+    String[] result = new String[] { "5" };
+
+    // Do a bootstrap dump.
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
+    replica.load(replicatedDbName, primaryDbName).run("REPL STATUS " + replicatedDbName)
+        .verifyResult(bootStrapDump.lastReplicationId);
+
+    // Add some data to the table & do a incremental dump.
+    ReplicationTestUtils.insertIntoDB(primary, primaryDbName, tableName, tableProperty, tableStorage,
+        new String[] { "1", "2", "3", "4", "5" });
+    WarehouseInstance.Tuple incrementalDump = primary.dump(primaryDbName);
+
+    // Keep a copy of the data, before we drop the table, so that we can copy it back to the location, in order to
+    // trigger source delete at the time of checksum verification.
+    Path tablePath = new Path(primary.getTable(primaryDbName, tableName).getSd().getLocation());
+    Path tablePath_dupe = new Path(primary.getTable(primaryDbName, tableName).getSd().getLocation() + "_dupe");
+    FileSystem fs = tablePath.getFileSystem(conf);
+    FileUtils.copy(fs, tablePath, fs, tablePath_dupe, false, false, conf);
+
+    // Drop the table.
+    primary.run("drop table " + primaryDbName + "." + tableName);
+
+    // Copy back the data to original location, so that copy happens from original location, not the CM location.
+    FileUtils.copy(fs, tablePath_dupe, fs, tablePath, false, false, conf);
+
+    // Add a util to delete the original source at the time of source checksum verification.
+    CopyUtils.testCallable = () -> {
+      try {
+        fs.delete(tablePath, true);
+      } catch (Throwable throwable) {
+        throwable.printStackTrace();
+      }
+      return null;
+    };
+
+    // Do an incremental load & verify if things are good.
+    replica.loadWithoutExplain(replicatedDbName, primaryDbName)
+        .run("REPL STATUS " + replicatedDbName).verifyResult(incrementalDump.lastReplicationId);
+    verifyResultsInReplicaInt(Lists.newArrayList("select count(*) from " + tableName,
+        "select count(*) from " + tableName + "_nopart"),
+        Lists.newArrayList(result, result));
+
+  }
+
   private void verifyResultsInReplicaInt(List<String> selectStmtList, List<String[]> expectedValues) throws Throwable  {
     ReplicationTestUtils.verifyResultsInReplica(replica, replicatedDbName, selectStmtList, expectedValues);
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/CopyUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/CopyUtils.java
index 2f9c0716b9..43e4ecc5d4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/CopyUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/CopyUtils.java
@@ -66,6 +66,8 @@ public class CopyUtils {
   private final String copyAsUser;
   private FileSystem destinationFs;
   private final int maxParallelCopyTask;
+  @VisibleForTesting
+  public static Callable<Boolean> testCallable;
 
   private List<Class<? extends Exception>> failOnParentExceptionList = Arrays.asList(org.apache.hadoop.fs.PathIOException.class,
           org.apache.hadoop.fs.UnsupportedFileSystemException.class,
@@ -294,12 +296,12 @@ private List<Path> getFilesToRetry(FileSystem sourceFs, List<ReplChangeManager.F
         continue;
       }
       Path srcPath = srcFile.getEffectivePath();
-      //Path destPath = new Path(destination, srcPath.getName());
+      Path destPath = new Path(destination, srcPath.getName());
       if (exists(destinationFs, destination)) {
         // If destination file is present and checksum of source mismatch, then retry copy.
         if (isSourceFileMismatch(sourceFs, srcFile)) {
           // Delete the incorrectly copied file and retry with CM path
-          delete(destinationFs, destination, true);
+          delete(destinationFs, destPath, true);
           srcFile.setIsUseSourcePath(false);
         } else {
           // If the retry logic is reached after copy error, then include the copied file as well.
@@ -377,6 +379,7 @@ public void renameFileCopiedFromCmPath(Path toPath, FileSystem dstFs, List<ReplC
 
   // Check if the source file unmodified even after copy to see if we copied the right file
   private boolean isSourceFileMismatch(FileSystem sourceFs, ReplChangeManager.FileInfo srcFile) throws IOException {
+    runTestOnlyExecutions();
     // If source is already CM path, the checksum will be always matching
     if (srcFile.isUseSourcePath()) {
       String sourceChecksumString = srcFile.getCheckSum();
@@ -401,6 +404,18 @@ private boolean isSourceFileMismatch(FileSystem sourceFs, ReplChangeManager.File
     return false;
   }
 
+  @VisibleForTesting
+  private void runTestOnlyExecutions() throws IOException {
+    if (testCallable != null) {
+      // testCallable will be not-null only in cases of execution through test code.
+      try {
+        testCallable.call();
+      } catch (Exception e) {
+        throw new IOException(e);
+      }
+    }
+  }
+
   private UserGroupInformation getProxyUser() throws IOException {
     if (copyAsUser == null) {
       return null;
