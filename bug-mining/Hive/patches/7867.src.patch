diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 0610053e83..d95ab68de7 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -2861,6 +2861,9 @@ public static enum ConfVars {
         "this is set to false, however unless MAPREDUCE-7086 fix is present (hadoop 3.1.1+),\n" +
         "queries that read non-orc MM tables with original files will fail. The default in\n" +
         "Hive 3.0 is false."),
+    HIVE_LOCK_FILE_MOVE_MODE("hive.lock.file.move.protect", "all", new StringSet("none", "dp", "all"),
+        "During file move operations acqueires a SEMI_SHARED lock at the table level."
+            + "none:never; dp: only in case of dynamic partitioning operations; all: all table operations"),
 
     // Zookeeper related configs
     HIVE_ZOOKEEPER_USE_KERBEROS("hive.zookeeper.kerberos.enabled", true,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index 207c8e4b1a..49f5dca66e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -41,7 +41,9 @@
 import org.apache.hadoop.hive.ql.io.merge.MergeFileTask;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLock;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockManager;
+import org.apache.hadoop.hive.ql.lockmgr.HiveLockMode;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockObj;
+import org.apache.hadoop.hive.ql.lockmgr.HiveLockObject;
 import org.apache.hadoop.hive.ql.lockmgr.LockException;
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.metadata.Hive;
@@ -59,6 +61,7 @@
 import org.apache.hadoop.hive.ql.plan.MapWork;
 import org.apache.hadoop.hive.ql.plan.MapredWork;
 import org.apache.hadoop.hive.ql.plan.MoveWork;
+import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.util.DirectionUtils;
@@ -66,6 +69,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.io.Closeable;
 import java.io.IOException;
 import java.io.Serializable;
 import java.util.ArrayList;
@@ -75,6 +79,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.Optional;
 
 /**
  * MoveTask implementation.
@@ -307,10 +312,11 @@ public int execute() {
         + work.getLoadMultiFilesWork());
     }
 
-    try {
-      if (context.getExplainAnalyze() == AnalyzeState.RUNNING) {
-        return 0;
-      }
+    if (context.getExplainAnalyze() == AnalyzeState.RUNNING) {
+      return 0;
+    }
+
+    try (LocalTableLock lock = acquireLockForFileMove(work.getLoadTableWork())) {
       Hive db = getHive();
 
       // Do any hive related operations like moving tables and files
@@ -565,7 +571,7 @@ private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,
           tbd.getMoveTaskId(), work.getLoadTableWork().getWriteType(), tbd.getSourcePath());
       LOG.debug("The statementId used when loading the dynamic partitions is " + statementId);
     }
-    
+
     // load the list of DP partitions and return the list of partition specs
     // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions
     // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.
@@ -774,9 +780,100 @@ WriteEntity.WriteType getWriteType(LoadTableDesc tbd, AcidUtils.Operation operat
     }
   }
 
+  class LocalTableLock  implements Closeable{
+
+    private Optional<HiveLockObject> lock;
+    private HiveLock lockObj;
+
+    public LocalTableLock(HiveLockObject lock) throws LockException {
+      this.lock = Optional.of(lock);
+      LOG.debug("LocalTableLock; locking: " + lock);
+      HiveLockManager lockMgr = context.getHiveTxnManager().getLockManager();
+      lockObj = lockMgr.lock(lock, HiveLockMode.SEMI_SHARED, true);
+      LOG.debug("LocalTableLock; locked: " + lock);
+    }
+
+    public LocalTableLock() {
+      lock = Optional.empty();
+    }
+
+    @Override
+    public void close() throws IOException {
+      if(!lock.isPresent()) {
+        return;
+      }
+      LOG.debug("LocalTableLock; unlocking: " + lock);
+      HiveLockManager lockMgr;
+      try {
+        lockMgr = context.getHiveTxnManager().getLockManager();
+        lockMgr.unlock(lockObj);
+      } catch (LockException e1) {
+        throw new IOException(e1);
+      }
+      LOG.debug("LocalTableLock; unlocked: " + lock);
+    }
+
+  }
+
+  static enum LockFileMoveMode {
+    NONE, DP, ALL;
+
+    public static LockFileMoveMode fromConf(HiveConf conf) {
+      if (!conf.getBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY)) {
+        return NONE;
+      }
+      String lockFileMoveMode = conf.getVar(HiveConf.ConfVars.HIVE_LOCK_FILE_MOVE_MODE).toUpperCase();
+      return valueOf(lockFileMoveMode);
+    }
+  }
+
+  private LocalTableLock acquireLockForFileMove(LoadTableDesc loadTableWork) throws HiveException {
+    LockFileMoveMode mode = LockFileMoveMode.fromConf(conf);
+
+    if (mode == LockFileMoveMode.NONE) {
+      return new LocalTableLock();
+    }
+    if (mode == LockFileMoveMode.DP && loadTableWork.getDPCtx() == null) {
+      return new LocalTableLock();
+    }
+
+    WriteEntity output = context.getLoadTableOutputMap().get(loadTableWork);
+    List<HiveLockObj> lockObjects = context.getOutputLockObjects().get(output);
+    if (lockObjects == null) {
+      return new LocalTableLock();
+    }
+    TableDesc table = loadTableWork.getTable();
+    if (table == null) {
+      return new LocalTableLock();
+    }
+
+    Hive db = getHive();
+    Table baseTable = db.getTable(loadTableWork.getTable().getTableName());
+
+    HiveLockObject.HiveLockObjectData lockData =
+        new HiveLockObject.HiveLockObjectData(queryPlan.getQueryId(),
+                               String.valueOf(System.currentTimeMillis()),
+                               "IMPLICIT",
+                               queryPlan.getQueryStr(),
+                               conf);
+
+    HiveLockObject lock = new HiveLockObject(baseTable, lockData);
+
+    for (HiveLockObj hiveLockObj : lockObjects) {
+      if (Arrays.equals(hiveLockObj.getObj().getPaths(), lock.getPaths())) {
+        HiveLockMode l = hiveLockObj.getMode();
+        if (l == HiveLockMode.EXCLUSIVE || l == HiveLockMode.SEMI_SHARED) {
+          // no need to lock ; already owns a more powerful one
+          return new LocalTableLock();
+        }
+      }
+    }
+
+    return new LocalTableLock(lock);
+  }
+
   private boolean isSkewedStoredAsDirs(LoadTableDesc tbd) {
-    return (tbd.getLbCtx() == null) ? false : tbd.getLbCtx()
-        .isSkewedStoredAsDir();
+    return tbd.getLbCtx() != null && tbd.getLbCtx().isSkewedStoredAsDir();
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLock.java b/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLock.java
index fe81e0400a..ddf0f53dba 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLock.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLock.java
@@ -72,7 +72,7 @@ public boolean equals(Object o) {
       obj.equals(zLock.getHiveLockObject()) &&
       mode == zLock.getHiveLockMode();
   }
-  
+
   @Override
   public int hashCode() {
     HashCodeBuilder builder = new HashCodeBuilder();
@@ -93,4 +93,9 @@ public int hashCode() {
     }
     return builder.toHashCode();
   }
+
+  @Override
+  public String toString() {
+    return String.format("ZKHL[%s;%s;%s]", path, obj, mode);
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java b/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
index 0610758444..eec9097039 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
@@ -419,6 +419,7 @@ private ZooKeeperHiveLock lockPrimitive(HiveLockObject key,
 
     String exLock = getLockName(lastName, HiveLockMode.EXCLUSIVE);
     String shLock = getLockName(lastName, HiveLockMode.SHARED);
+    String semiLock = getLockName(lastName, HiveLockMode.SEMI_SHARED);
 
     for (String child : children) {
       child = lastName + "/" + child;
@@ -432,6 +433,9 @@ private ZooKeeperHiveLock lockPrimitive(HiveLockObject key,
       if ((mode == HiveLockMode.EXCLUSIVE) && child.startsWith(shLock)) {
         childSeq = getSequenceNumber(child, shLock);
       }
+      if ((mode == HiveLockMode.SEMI_SHARED || mode == HiveLockMode.EXCLUSIVE) && child.startsWith(semiLock)) {
+        childSeq = getSequenceNumber(child, semiLock);
+      }
 
       if ((childSeq >= 0) && (childSeq < seqNo)) {
         try {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestConcurrentDppInserts.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestConcurrentDppInserts.java
new file mode 100644
index 0000000000..1288c10d32
--- /dev/null
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestConcurrentDppInserts.java
@@ -0,0 +1,174 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.exec;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.concurrent.CyclicBarrier;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.ql.DriverFactory;
+import org.apache.hadoop.hive.ql.IDriver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hive.testutils.HiveTestEnvSetup;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TestRule;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.base.Joiner;
+
+public class TestConcurrentDppInserts {
+
+  static final private Logger LOG = LoggerFactory.getLogger(TestConcurrentDppInserts.class.getName());
+
+  @ClassRule
+  public static HiveTestEnvSetup env_setup = new HiveTestEnvSetup();
+
+  @Rule
+  public TestRule methodRule = env_setup.getMethodRule();
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    IDriver driver = createDriver(false);
+    dropTables(driver);
+    String[] cmds = {
+        // @formatter:off
+        "create table tu(i int) partitioned by (k string, p string)",
+        // @formatter:on
+    };
+    for (String cmd : cmds) {
+      driver.run(cmd);
+    }
+  }
+
+  @AfterClass
+  public static void afterClass() throws Exception {
+    IDriver driver = createDriver(false);
+    dropTables(driver);
+  }
+
+  public static void dropTables(IDriver driver) throws Exception {
+    String[] tables = new String[] { "tu" };
+    for (String t : tables) {
+      driver.run("drop table if exists " + t);
+    }
+  }
+
+  int N = 3; // num parallel threads
+  int M = 3; // num partitions a thread inserts into at a time
+  int K = 3; // num tests to repeat
+
+  CyclicBarrier barrier = new CyclicBarrier(N);
+  Semaphore finished = new Semaphore(0);
+
+  LinkedList<Exception> exceptions = new LinkedList<>();
+
+  class InserterThread extends Thread {
+
+    @Override
+    public void run() {
+      try {
+        IDriver driver = createDriver(true);
+        for (int i = 0; i < K; i++) {
+          doTest(driver, i);
+        }
+      } catch (Throwable t) {
+        System.out.println(t);
+      } finally {
+        finished.release();
+      }
+
+    }
+
+    private void doTest(IDriver driver, int pIdx) {
+      try {
+        barrier.await(30, TimeUnit.SECONDS);
+        List<String> parts = new ArrayList<>();
+        for (int i = 0; i < M; i++) {
+          parts.add(String.format("select %d as i,%d as p", M * pIdx + i, M * pIdx + i));
+        }
+        driver.run("insert into tu partition(k=1,p) (" + Joiner.on(" union all ").join(parts) + ")");
+
+      } catch (Exception e) {
+        LOG.info("Exception in InserterThread:", e);
+        exceptions.add(e);
+      }
+    }
+
+  }
+
+  @Test(timeout = 600000)
+  public void testConcurrentCreationOfSamePartition() throws Exception {
+    List<Object> threads = new ArrayList<>();
+    for (int i = 0; i < N; i++) {
+      InserterThread e = new InserterThread();
+      e.start();
+      threads.add(e);
+    }
+    finished.acquire(N);
+
+    IDriver driver = createDriver(true);
+    driver.run("select p,count(i) as c from tu group by p");
+    ArrayList<String> res = new ArrayList<>();
+    assertEquals(0, exceptions.size(), " there were exceptions: " + getExceptionMessages());
+    driver.getResults(res);
+    assertEquals(K * M, res.size());
+    for (String row : res) {
+      String[] parts = row.split("\t");
+      assertEquals(Integer.toString(N), parts[1], row);
+    }
+  }
+
+  private String getExceptionMessages() {
+    StringBuilder sb = new StringBuilder();
+    for (Exception exception : exceptions) {
+      if (sb.length() > 0) {
+        sb.append(", ");
+      }
+      sb.append(exception.getClass().getName() + ":" + exception.getMessage());
+    }
+    return sb.toString();
+  }
+
+  private static IDriver createDriver(boolean custom) {
+    HiveConf conf = new HiveConf(env_setup.getTestCtx().hiveConf);
+
+    if (custom) {
+      conf.setVar(ConfVars.HIVE_LOCK_FILE_MOVE_MODE, "all");
+      conf.setBoolVar(ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
+      conf.setTimeVar(HiveConf.ConfVars.HIVE_LOCK_SLEEP_BETWEEN_RETRIES, 100, TimeUnit.MILLISECONDS);
+    }
+
+    SessionState.start(conf);
+
+    IDriver driver = DriverFactory.newDriver(conf);
+    return driver;
+  }
+
+
+}
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/zookeeper/TestZookeeperLockManager.java b/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/zookeeper/TestZookeeperLockManager.java
index 4482f86dc0..3f0a27b2ce 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/zookeeper/TestZookeeperLockManager.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/zookeeper/TestZookeeperLockManager.java
@@ -18,21 +18,25 @@
 
 package org.apache.hadoop.hive.ql.lockmgr.zookeeper;
 
-import java.io.File;
-import java.nio.file.Files;
-import java.nio.file.Paths;
-
 import org.apache.hadoop.hive.common.metrics.MetricsTestUtils;
 import org.apache.hadoop.hive.common.metrics.common.MetricsConstant;
 import org.apache.hadoop.hive.common.metrics.common.MetricsFactory;
 import org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics;
 import org.apache.hadoop.hive.common.metrics.metrics2.MetricsReporting;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockManagerCtx;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockMode;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockObject;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData;
+import org.apache.hadoop.hive.ql.lockmgr.LockException;
 import org.apache.zookeeper.KeeperException;
+
+import static org.junit.Assert.assertTrue;
+
+import java.util.concurrent.BrokenBarrierException;
+import java.util.concurrent.CyclicBarrier;
+import java.util.concurrent.Semaphore;
 import org.apache.curator.framework.CuratorFramework;
 import org.apache.curator.framework.CuratorFrameworkFactory;
 import org.apache.curator.retry.RetryOneTime;
@@ -42,9 +46,6 @@
 import org.junit.After;
 import org.junit.Test;
 
-import com.fasterxml.jackson.databind.JsonNode;
-import com.fasterxml.jackson.databind.ObjectMapper;
-
 public class TestZookeeperLockManager {
 
   private HiveConf conf;
@@ -61,6 +62,7 @@ public class TestZookeeperLockManager {
   @Before
   public void setup() {
     conf = new HiveConf();
+    conf.setVar(ConfVars.HIVE_LOCK_SLEEP_BETWEEN_RETRIES, "100ms");
     lockObjData = new HiveLockObjectData("1", "10", "SHARED", "show tables", conf);
     hiveLock = new HiveLockObject(TABLE, lockObjData);
     zLock = new ZooKeeperHiveLock(TABLE_LOCK_PATH, hiveLock, HiveLockMode.SHARED);
@@ -145,8 +147,64 @@ public void testMetrics() throws Exception{
     zMgr.unlock(curLock);
     json = metrics.dumpJson();
     MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.COUNTER, MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS, 0);
+
+    testLockModeTiming(zMgr, HiveLockMode.SHARED, HiveLockMode.SHARED, 1000, 2000);
+    testLockModeTiming(zMgr, HiveLockMode.SEMI_SHARED, HiveLockMode.SEMI_SHARED, 2000, 3000);
+    testLockModeTiming(zMgr, HiveLockMode.EXCLUSIVE, HiveLockMode.SHARED, 2000, 3000);
+    testLockModeTiming(zMgr, HiveLockMode.EXCLUSIVE, HiveLockMode.SEMI_SHARED, 2000, 3000);
+
     zMgr.close();
   }
 
+  static class LockTesterThread extends Thread {
+    private CyclicBarrier barrier;
+    private ZooKeeperHiveLockManager zMgr;
+    private HiveLockObject hiveLock;
+    private HiveLockMode lockMode;
+    private Semaphore semaphore;
+
+    public LockTesterThread(CyclicBarrier barrier, Semaphore semaphore, ZooKeeperHiveLockManager zMgr,
+        HiveLockObject hiveLock,
+        HiveLockMode semiShared) {
+      this.barrier = barrier;
+      this.semaphore = semaphore;
+      this.zMgr = zMgr;
+      this.hiveLock = hiveLock;
+      this.lockMode = semiShared;
+
+    }
+
+    public void run() {
+      try {
+        barrier.await();
+        ZooKeeperHiveLock l = zMgr.lock(hiveLock, lockMode, false);
+        Thread.sleep(1000);
+        zMgr.unlock(l);
+        semaphore.release();
+      } catch (LockException | InterruptedException | BrokenBarrierException e) {
+        e.printStackTrace();
+        throw new RuntimeException(e);
+      }
+    }
+  }
+
+  private void testLockModeTiming(ZooKeeperHiveLockManager zMgr, HiveLockMode lock1mode, HiveLockMode lock2mode,
+      int minT, int maxT)
+      throws Exception, LockException, InterruptedException, BrokenBarrierException {
+
+    CyclicBarrier barrier = new CyclicBarrier(3);
+    Semaphore semaphore = new Semaphore(0);
+
+    new LockTesterThread(barrier, semaphore, zMgr, hiveLock, lock1mode).start();
+    new LockTesterThread(barrier, semaphore, zMgr, hiveLock, lock2mode).start();
+    barrier.await();
+    long t0 = System.currentTimeMillis();
+    semaphore.acquire(2);
+    long t1 = System.currentTimeMillis();
+    long dt = t1 - t0;
+    assertTrue(String.format("%d>%d", dt, minT), dt > minT);
+    assertTrue(String.format("%d<%d", dt, maxT), dt < maxT);
+  }
+
 }
 
diff --git a/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java b/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
index 3c6ad23e03..f04a5c9da6 100644
--- a/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
+++ b/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
@@ -1384,7 +1384,7 @@ public enum ConfVars {
         "hive.metastore.custom.database.product.classname", "none",
           "Hook for external RDBMS. This class will be instantiated only when " +
           "metastore.use.custom.database.product is set to true."),
-        
+
     // Deprecated Hive values that we are keeping for backwards compatibility.
     @Deprecated
     HIVE_CODAHALE_METRICS_REPORTER_CLASSES("hive.service.metrics.codahale.reporter.classes",
@@ -1408,7 +1408,8 @@ public enum ConfVars {
             + "e.g. javax.net.ssl.trustStore=/tmp/truststore,javax.net.ssl.trustStorePassword=pwd.\n " +
             "If both this and the metastore.dbaccess.ssl.* properties are set, then the latter properties \n" +
             "will overwrite what was set in the deprecated property."),
-
+    METASTORE_NUM_STRIPED_TABLE_LOCKS("metastore.num.striped.table.locks", "hive.metastore.num.striped.table.locks", 32,
+        "Number of striped locks available to provide exclusive operation support for critical table operations like add_partitions."),
     COLSTATS_RETAIN_ON_COLUMN_REMOVAL("metastore.colstats.retain.on.column.removal",
         "hive.metastore.colstats.retain.on.column.removal", true,
         "Whether to retain column statistics during column removals in partitioned tables - disabling this "
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
index 401ffc70e4..cbd1d4abc8 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
@@ -211,6 +211,7 @@
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Preconditions;
 import com.google.common.base.Splitter;
+import com.google.common.util.concurrent.Striped;
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
 
 /**
@@ -278,6 +279,7 @@ public static class HMSHandler extends FacebookBase implements IHMSHandler {
     static AtomicInteger databaseCount, tableCount, partCount;
 
     private Warehouse wh; // hdfs warehouse
+    private static Striped<Lock> tablelocks;
     private static final ThreadLocal<RawStore> threadLocalMS =
         new ThreadLocal<RawStore>() {
           @Override
@@ -480,10 +482,13 @@ public HMSHandler(String name, Configuration conf, boolean init) throws MetaExce
       isInTest = MetastoreConf.getBoolVar(this.conf, ConfVars.HIVE_IN_TEST);
       if (threadPool == null) {
         synchronized (HMSHandler.class) {
-          int numThreads = MetastoreConf.getIntVar(conf, ConfVars.FS_HANDLER_THREADS_COUNT);
-          threadPool = Executors.newFixedThreadPool(numThreads,
-              new ThreadFactoryBuilder().setDaemon(true)
-                  .setNameFormat("HMSHandler #%d").build());
+          if (threadPool == null) {
+            int numThreads = MetastoreConf.getIntVar(conf, ConfVars.FS_HANDLER_THREADS_COUNT);
+            threadPool = Executors.newFixedThreadPool(numThreads,
+                new ThreadFactoryBuilder().setDaemon(true).setNameFormat("HMSHandler #%d").build());
+            int numTableLocks = MetastoreConf.getIntVar(conf, ConfVars.METASTORE_NUM_STRIPED_TABLE_LOCKS);
+            tablelocks = Striped.lock(numTableLocks);
+          }
         }
       }
       if (init) {
@@ -633,8 +638,9 @@ public void init() throws MetaException {
         Constructor<?> constructor;
         try {
           constructor = clazz.getConstructor(IHMSHandler.class);
-          if (Modifier.isPrivate(constructor.getModifiers()))
+          if (Modifier.isPrivate(constructor.getModifiers())) {
             throw new IllegalArgumentException("Illegal implementation for metadata transformer. Constructor is private");
+          }
           transformer = (IMetaStoreMetadataTransformer) constructor.newInstance(this);
         } catch (NoSuchMethodException | InstantiationException | IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {
           LOG.error("Unable to create instance of class " + className, e);
@@ -1467,8 +1473,9 @@ private void create_database_core(RawStore ms, final Database db)
         //reinstate location uri for metastore db.
         if (skipAuthorization == true){
           db.setLocationUri(dbExtPath.toString());
-          if (dbMgdPath != null)
+          if (dbMgdPath != null) {
             db.setManagedLocationUri(dbMgdPath.toString());
+          }
         }
         if (db.getCatalogName() != null && !db.getCatalogName().
             equals(Warehouse.DEFAULT_CATALOG_NAME)) {
@@ -1640,8 +1647,9 @@ public Database get_database(final String name)
       GetDatabaseRequest request = new GetDatabaseRequest();
       String[] parsedDbName = parseDbName(name, conf);
       request.setName(parsedDbName[DB_NAME]);
-      if (parsedDbName[CAT_NAME] != null)
-          request.setCatalogName(parsedDbName[CAT_NAME]);
+      if (parsedDbName[CAT_NAME] != null) {
+        request.setCatalogName(parsedDbName[CAT_NAME]);
+      }
         return get_database_req(request);
     }
 
@@ -2126,20 +2134,27 @@ private void create_table_core(final RawStore ms, final Table tbl,
         throws AlreadyExistsException, MetaException,
         InvalidObjectException, NoSuchObjectException, InvalidInputException {
       CreateTableRequest req = new CreateTableRequest(tbl);
-      if (envContext != null)
+      if (envContext != null) {
         req.setEnvContext(envContext);
-      if (primaryKeys != null)
+      }
+      if (primaryKeys != null) {
         req.setPrimaryKeys(primaryKeys);
-      if (foreignKeys != null)
+      }
+      if (foreignKeys != null) {
         req.setForeignKeys(foreignKeys);
-      if (uniqueConstraints != null)
+      }
+      if (uniqueConstraints != null) {
         req.setUniqueConstraints(uniqueConstraints);
-      if (notNullConstraints != null)
+      }
+      if (notNullConstraints != null) {
         req.setNotNullConstraints(notNullConstraints);
-      if (defaultConstraints != null)
+      }
+      if (defaultConstraints != null) {
         req.setDefaultConstraints(defaultConstraints);
-      if (checkConstraints != null)
+      }
+      if (checkConstraints != null) {
         req.setCheckConstraints(checkConstraints);
+      }
       if (processorCapabilities != null) {
         req.setProcessorCapabilities(processorCapabilities);
         req.setProcessorIdentifier(processorIdentifier);
@@ -3398,8 +3413,9 @@ public List<ExtendedTableInfo> get_tables_ext(final GetTablesExtRequest req) thr
     private ExtendedTableInfo convertTableToExtendedTable (Table table,
           List<String> processorCapabilities, int mask) {
       ExtendedTableInfo extTable = new ExtendedTableInfo(table.getTableName());
-      if ((mask & GetTablesExtRequestFields.ACCESS_TYPE.getValue()) == GetTablesExtRequestFields.ACCESS_TYPE.getValue())
+      if ((mask & GetTablesExtRequestFields.ACCESS_TYPE.getValue()) == GetTablesExtRequestFields.ACCESS_TYPE.getValue()) {
         extTable.setAccessType(table.getAccessType());
+      }
 
       if ((mask & GetTablesExtRequestFields.PROCESSOR_CAPABILITIES.getValue())
              == GetTablesExtRequestFields.PROCESSOR_CAPABILITIES.getValue()) {
@@ -3458,7 +3474,7 @@ private Table getTableInternal(String catName, String dbname, String name,
               LOG.warn("Unexpected resultset size:" + ret.size());
               throw new MetaException("Unexpected result from metadata transformer:return list size is " + ret.size());
             }
-            t = (Table)(ret.keySet().iterator().next());
+            t = ret.keySet().iterator().next();
           }
         }
 
@@ -3926,6 +3942,8 @@ private List<Partition> add_partitions_core(final RawStore ms, String catName,
       List<ColumnStatistics> partsColStats = new ArrayList<>(parts.size());
       List<Long> partsWriteIds = new ArrayList<>(parts.size());
 
+      Lock tableLock = getTableLockFor(dbName, tblName);
+      tableLock.lock();
       try {
         ms.openTransaction();
         tbl = ms.getTable(catName, dbName, tblName, null);
@@ -4020,22 +4038,30 @@ private List<Partition> add_partitions_core(final RawStore ms, String catName,
 
         success = ms.commitTransaction();
       } finally {
+        try {
         if (!success) {
-          ms.rollbackTransaction();
-          cleanupPartitionFolders(addedPartitions, db);
+            ms.rollbackTransaction();
+            cleanupPartitionFolders(addedPartitions, db);
 
-          if (!listeners.isEmpty()) {
-            MetaStoreListenerNotifier.notifyEvent(listeners,
-                                                  EventType.ADD_PARTITION,
-                                                  new AddPartitionEvent(tbl, parts, false, this),
-                                                  null, null, ms);
+            if (!listeners.isEmpty()) {
+              MetaStoreListenerNotifier.notifyEvent(listeners,
+                                                    EventType.ADD_PARTITION,
+                                                    new AddPartitionEvent(tbl, parts, false, this),
+                                                    null, null, ms);
+            }
           }
+        } finally {
+          tableLock.unlock();
         }
       }
 
       return newParts;
     }
 
+    private Lock getTableLockFor(String dbName, String tblName) {
+      return tablelocks.get(dbName + "." + tblName);
+    }
+
     /**
      * Remove the newly created partition folders. The values in the addedPartitions map indicates
      * whether or not the location of the partition was newly created. If the value is false, the
@@ -4313,6 +4339,8 @@ private int add_partitions_pspec_core(RawStore ms, String catName, String dbName
       Table tbl = null;
       Map<String, String> transactionalListenerResponses = Collections.emptyMap();
       Database db = null;
+      Lock tableLock = getTableLockFor(dbName, tblName);
+      tableLock.lock();
       try {
         ms.openTransaction();
         tbl = ms.getTable(catName, dbName, tblName, null);
@@ -4352,17 +4380,20 @@ private int add_partitions_pspec_core(RawStore ms, String catName, String dbName
         success = ms.commitTransaction();
         return addedPartitions.size();
       } finally {
-        if (!success) {
-          ms.rollbackTransaction();
-          cleanupPartitionFolders(addedPartitions, db);
-        }
-
-        if (!listeners.isEmpty()) {
-          MetaStoreListenerNotifier.notifyEvent(listeners,
-                                                EventType.ADD_PARTITION,
-                                                new AddPartitionEvent(tbl, partitionSpecProxy, true, this),
-                                                null,
-                                                transactionalListenerResponses, ms);
+        try {
+          if (!success) {
+            ms.rollbackTransaction();
+            cleanupPartitionFolders(addedPartitions, db);
+          }
+          if (!listeners.isEmpty()) {
+            MetaStoreListenerNotifier.notifyEvent(listeners,
+                                                  EventType.ADD_PARTITION,
+                                                  new AddPartitionEvent(tbl, partitionSpecProxy, true, this),
+                                                  null,
+                                                  transactionalListenerResponses, ms);
+          }
+        } finally {
+          tableLock.unlock();
         }
       }
     }
@@ -5627,6 +5658,8 @@ private void alter_partitions_with_environment_context(String catName, String db
       // all prehooks are fired together followed by all post hooks
       List<Partition> oldParts = null;
       Exception ex = null;
+      Lock tableLock = getTableLockFor(db_name, tbl_name);
+      tableLock.lock();
       try {
         for (Partition tmpPart : new_parts) {
           // Make sure the catalog name is set in the new partition
@@ -5673,6 +5706,7 @@ private void alter_partitions_with_environment_context(String catName, String db
         ex = e;
         throw newMetaException(e);
       } finally {
+        tableLock.unlock();
         endFunction("alter_partition", oldParts != null, ex, tbl_name);
       }
     }
@@ -6467,7 +6501,9 @@ public Map<String, String> partition_name_to_spec(String part_name) throws TExce
     }
 
     private String lowerCaseConvertPartName(String partName) throws MetaException {
-      if (partName == null) return partName;
+      if (partName == null) {
+        return partName;
+      }
       boolean isFirst = true;
       Map<String, String> partSpec = Warehouse.makeEscSpecFromName(partName);
       String convertedPartName = new String();
@@ -6607,7 +6643,9 @@ public PartitionsStatsResult get_partitions_statistics_req(PartitionsStatsReques
             //       is currently only done on metastore size (see set_aggr...).
             //       For some optimizations we might make use of incorrect stats that are "better than
             //       nothing", so this may change in future.
-            if (stat.isSetIsStatsCompliant() && !stat.isIsStatsCompliant()) continue;
+            if (stat.isSetIsStatsCompliant() && !stat.isIsStatsCompliant()) {
+              continue;
+            }
             map.put(stat.getStatsDesc().getPartName(), stat.getStatsObj());
           }
         }
@@ -10859,7 +10897,9 @@ public void run() {
   protected static void startStatsUpdater(Configuration conf) throws Exception {
     StatsUpdateMode mode = StatsUpdateMode.valueOf(
         MetastoreConf.getVar(conf, ConfVars.STATS_AUTO_UPDATE).toUpperCase());
-    if (mode == StatsUpdateMode.NONE) return;
+    if (mode == StatsUpdateMode.NONE) {
+      return;
+    }
     MetaStoreThread t = instantiateThread("org.apache.hadoop.hive.ql.stats.StatsUpdaterThread");
     initializeAndStartThread(t, conf);
   }
