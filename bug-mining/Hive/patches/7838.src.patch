diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/arrow/Serializer.java b/ql/src/java/org/apache/hadoop/hive/ql/io/arrow/Serializer.java
index 5a79641ea8..3cb305c8d5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/arrow/Serializer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/arrow/Serializer.java
@@ -177,17 +177,7 @@ public Serializer(Configuration conf, String attemptId, List<TypeInfo> typeInfos
 
   //Construct an emptyBatch which contains schema-only info
   public ArrowWrapperWritable emptyBatch() {
-    rootVector.setValueCount(0);
-    for (int fieldIndex = 0; fieldIndex < fieldTypeInfos.size(); fieldIndex++) {
-      final TypeInfo fieldTypeInfo = fieldTypeInfos.get(fieldIndex);
-      final String fieldName = fieldNames.get(fieldIndex);
-      final FieldType fieldType = toFieldType(fieldTypeInfo);
-      final FieldVector arrowVector = rootVector.addOrGet(fieldName, fieldType, FieldVector.class);
-      arrowVector.setInitialCapacity(0);
-      arrowVector.allocateNew();
-    }
-    VectorSchemaRoot vectorSchemaRoot = new VectorSchemaRoot(rootVector);
-    return new ArrowWrapperWritable(vectorSchemaRoot, allocator, rootVector);
+    return serializeBatch(new VectorizedRowBatch(fieldTypeInfos.size()), false);
   }
 
   //Used for both:
@@ -314,7 +304,7 @@ private void write(FieldVector arrowVector, ColumnVector hiveVector, TypeInfo ty
   private void writeMap(ListVector arrowVector, MapColumnVector hiveVector, MapTypeInfo typeInfo,
       int size, VectorizedRowBatch vectorizedRowBatch, boolean isNative) {
     final ListTypeInfo structListTypeInfo = toStructListTypeInfo(typeInfo);
-    final ListColumnVector structListVector = toStructListVector(hiveVector);
+    final ListColumnVector structListVector = hiveVector == null ? null : toStructListVector(hiveVector);
 
     write(arrowVector, structListVector, structListTypeInfo, size, vectorizedRowBatch, isNative);
 
@@ -349,12 +339,12 @@ private void writeStruct(NonNullableStructVector arrowVector, StructColumnVector
       StructTypeInfo typeInfo, int size, VectorizedRowBatch vectorizedRowBatch, boolean isNative) {
     final List<String> fieldNames = typeInfo.getAllStructFieldNames();
     final List<TypeInfo> fieldTypeInfos = typeInfo.getAllStructFieldTypeInfos();
-    final ColumnVector[] hiveFieldVectors = hiveVector.fields;
+    final ColumnVector[] hiveFieldVectors = hiveVector == null ? null : hiveVector.fields;
     final int fieldSize = fieldTypeInfos.size();
 
     for (int fieldIndex = 0; fieldIndex < fieldSize; fieldIndex++) {
       final TypeInfo fieldTypeInfo = fieldTypeInfos.get(fieldIndex);
-      final ColumnVector hiveFieldVector = hiveFieldVectors[fieldIndex];
+      final ColumnVector hiveFieldVector = hiveVector == null ? null : hiveFieldVectors[fieldIndex];
       final String fieldName = fieldNames.get(fieldIndex);
       final FieldVector arrowFieldVector =
           arrowVector.addOrGet(fieldName,
@@ -365,7 +355,7 @@ private void writeStruct(NonNullableStructVector arrowVector, StructColumnVector
     }
 
     for (int rowIndex = 0; rowIndex < size; rowIndex++) {
-      if (hiveVector.isNull[rowIndex]) {
+      if (hiveVector == null || hiveVector.isNull[rowIndex]) {
         BitVectorHelper.setValidityBit(arrowVector.getValidityBuffer(), rowIndex, 0);
       } else {
         BitVectorHelper.setValidityBitToOne(arrowVector.getValidityBuffer(), rowIndex);
@@ -414,12 +404,12 @@ private void writeList(ListVector arrowVector, ListColumnVector hiveVector, List
                          VectorizedRowBatch vectorizedRowBatch, boolean isNative) {
     final int OFFSET_WIDTH = 4;
     final TypeInfo elementTypeInfo = typeInfo.getListElementTypeInfo();
-    final ColumnVector hiveElementVector = hiveVector.child;
+    final ColumnVector hiveElementVector = hiveVector == null ? null : hiveVector.child;
     final FieldVector arrowElementVector =
             (FieldVector) arrowVector.addOrGetVector(toFieldType(elementTypeInfo)).getVector();
 
     VectorizedRowBatch correctedVrb = vectorizedRowBatch;
-    int correctedSize = hiveVector.childCount;
+    int correctedSize = hiveVector == null ? 0 : hiveVector.childCount;
     if (vectorizedRowBatch.selectedInUse) {
       correctedVrb = correctSelectedAndSize(vectorizedRowBatch, hiveVector);
       correctedSize = correctedVrb.size;
@@ -436,7 +426,7 @@ private void writeList(ListVector arrowVector, ListColumnVector hiveVector, List
       if (vectorizedRowBatch.selectedInUse) {
         selectedIndex = vectorizedRowBatch.selected[rowIndex];
       }
-      if (hiveVector.isNull[selectedIndex]) {
+      if (hiveVector == null || hiveVector.isNull[selectedIndex]) {
         arrowVector.getOffsetBuffer().setInt(rowIndex * OFFSET_WIDTH, nextOffset);
       } else {
         arrowVector.getOffsetBuffer().setInt(rowIndex * OFFSET_WIDTH, nextOffset);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/arrow/TestSerializer.java b/ql/src/test/org/apache/hadoop/hive/ql/io/arrow/TestSerializer.java
new file mode 100644
index 0000000000..aed958b59d
--- /dev/null
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/arrow/TestSerializer.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.arrow;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.util.Arrays;
+import java.util.List;
+
+public class TestSerializer {
+  @Test
+  public void testEmptyList() {
+    List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString("array<tinyint>");
+    List<String> fieldNames = Arrays.asList(new String[]{"a"});
+    Serializer converter = new Serializer(new HiveConf(), "attemptId", typeInfos, fieldNames);
+    ArrowWrapperWritable writable = converter.emptyBatch();
+    Assert.assertEquals("Schema<a: List<$data$: Int(8, true)>>",
+        writable.getVectorSchemaRoot().getSchema().toString());
+  }
+
+  @Test
+  public void testEmptyStruct() {
+    List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString("struct<b:int,c:string>");
+    List<String> fieldNames = Arrays.asList(new String[] { "a" });
+    Serializer converter = new Serializer(new HiveConf(), "attemptId", typeInfos, fieldNames);
+    ArrowWrapperWritable writable = converter.emptyBatch();
+    Assert.assertEquals("Schema<a: Struct<b: Int(32, true), c: Utf8>>",
+        writable.getVectorSchemaRoot().getSchema().toString());
+  }
+
+  @Test
+  public void testEmptyMap() {
+    List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString("map<string,string>");
+    List<String> fieldNames = Arrays.asList(new String[] { "a" });
+    Serializer converter = new Serializer(new HiveConf(), "attemptId", typeInfos, fieldNames);
+    ArrowWrapperWritable writable = converter.emptyBatch();
+    Assert.assertEquals("Schema<a: List<$data$: Struct<keys: Utf8, values: Utf8>>>",
+        writable.getVectorSchemaRoot().getSchema().toString());
+  }
+
+  @Test
+  public void testEmptyComplexStruct() {
+    List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString(
+        "struct<b:array<tinyint>,c:map<string,string>,d:struct<e:array<tinyint>,f:map<string,string>>>");
+    List<String> fieldNames = Arrays.asList(new String[] { "a" });
+    Serializer converter = new Serializer(new HiveConf(), "attemptId", typeInfos, fieldNames);
+    ArrowWrapperWritable writable = converter.emptyBatch();
+    Assert.assertEquals(
+        "Schema<a: Struct<b: List<$data$: Int(8, true)>, c: List<$data$: Struct<keys: Utf8, values: Utf8>>, " +
+            "d: Struct<e: List<$data$: Int(8, true)>, f: List<$data$: Struct<keys: Utf8, values: Utf8>>>>>",
+        writable.getVectorSchemaRoot().getSchema().toString());
+  }
+
+  @Test
+  public void testEmptyComplexMap() {
+    List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString(
+        "map<array<tinyint>,struct<b:array<tinyint>,c:map<string,string>>>");
+    List<String> fieldNames = Arrays.asList(new String[] { "a" });
+    Serializer converter = new Serializer(new HiveConf(), "attemptId", typeInfos, fieldNames);
+    ArrowWrapperWritable writable = converter.emptyBatch();
+    Assert.assertEquals(
+        "Schema<a: List<$data$: Struct<keys: List<$data$: Int(8, true)>, values: " +
+            "Struct<b: List<$data$: Int(8, true)>, c: List<$data$: Struct<keys: Utf8, values: Utf8>>>>>>",
+        writable.getVectorSchemaRoot().getSchema().toString());
+  }
+
+  @Test
+  public void testEmptyComplexList() {
+    List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString("struct<b:array<array<tinyint>>," +
+        "c:array<map<string,string>>,d:array<struct<e:array<tinyint>,f:map<string,string>>>>");
+    List<String> fieldNames = Arrays.asList(new String[] { "a" });
+    Serializer converter = new Serializer(new HiveConf(), "attemptId", typeInfos, fieldNames);
+    ArrowWrapperWritable writable = converter.emptyBatch();
+    Assert.assertEquals(
+        "Schema<a: Struct<b: List<$data$: List<$data$: Int(8, true)>>, c: List<$data$: List<$data$: " +
+            "Struct<keys: Utf8, values: Utf8>>>, d: List<$data$: Struct<e: List<$data$: Int(8, true)>, " +
+            "f: List<$data$: Struct<keys: Utf8, values: Utf8>>>>>>",
+        writable.getVectorSchemaRoot().getSchema().toString());
+  }
+}
