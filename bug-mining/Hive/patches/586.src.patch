diff --git a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
index df1bc03252..7bcad61fcf 100644
--- a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
+++ b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
@@ -32,6 +32,7 @@
 import org.apache.hadoop.io.BooleanWritable;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.MapWritable;
@@ -229,12 +230,18 @@ public void writeNull() throws IOException {
   }
 
   public void writeWritable(Writable w) throws IOException {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    DataOutputStream dos = new DataOutputStream(baos);
-    WritableUtils.writeString(dos, w.getClass().getName());
-    w.write(dos);
-    dos.close();
-    out.writeBytes(baos.toByteArray(), Type.WRITABLE.code);
+    DataOutputStream dos = null;
+    try {
+      ByteArrayOutputStream baos = new ByteArrayOutputStream();
+      dos = new DataOutputStream(baos);
+      WritableUtils.writeString(dos, w.getClass().getName());
+      w.write(dos);
+      out.writeBytes(baos.toByteArray(), Type.WRITABLE.code);
+      dos.close();
+      dos = null;
+    } finally {
+      IOUtils.closeStream(dos);
+    }
   }
 
   public void writeEndOfRecord() throws IOException {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
index 9e0d48e93e..fe95cb18b4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
@@ -38,6 +38,7 @@
 import org.apache.hadoop.hive.ql.plan.Explain;
 import org.apache.hadoop.hive.ql.plan.ExplainWork;
 import org.apache.hadoop.hive.ql.plan.api.StageType;
+import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.util.StringUtils;
 
 
@@ -55,10 +56,11 @@ public ExplainTask() {
   @Override
   public int execute(DriverContext driverContext) {
 
+    PrintStream out = null;
     try {
       Path resFile = new Path(work.getResFile());
       OutputStream outS = resFile.getFileSystem(conf).create(resFile);
-      PrintStream out = new PrintStream(outS);
+      out = new PrintStream(outS);
 
       // Print out the parse AST
       outputAST(work.getAstStringTree(), out, 0);
@@ -70,12 +72,15 @@ public int execute(DriverContext driverContext) {
       // Go over all the tasks and dump out the plans
       outputStagePlans(out, work.getRootTasks(), 0);
       out.close();
+      out = null;
 
       return (0);
     } catch (Exception e) {
       console.printError("Failed with exception " + e.getMessage(), "\n"
           + StringUtils.stringifyException(e));
       return (1);
+    } finally {
+      IOUtils.closeStream(out);
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Throttle.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Throttle.java
index 3012557775..b240125962 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Throttle.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Throttle.java
@@ -24,6 +24,7 @@
 import java.util.regex.Pattern;
 
 import org.apache.commons.logging.Log;
+import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.mapred.JobConf;
 
 /**
@@ -62,9 +63,15 @@ public static void checkJobTracker(JobConf conf, Log LOG) {
         // read in the first 1K characters from the URL
         URL url = new URL(tracker);
         LOG.debug("Throttle: URL " + tracker);
-        InputStream in = url.openStream();
-        in.read(buffer);
-        in.close();
+        InputStream in = null;
+        try {
+          in = url.openStream();
+          in.read(buffer);
+          in.close();
+          in = null;
+        } finally {
+          IOUtils.closeStream(in);
+        }
         String fetchString = new String(buffer);
 
         // fetch the xml tag <dogc>xxx</dogc>
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 061f9e7b8e..14a6eddab2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -115,6 +115,7 @@
 import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.SequenceFile.CompressionType;
@@ -457,10 +458,16 @@ public void exceptionThrown(Exception e) {
    * Deserialize the whole query plan.
    */
   public static QueryPlan deserializeQueryPlan(InputStream in, Configuration conf) {
-    XMLDecoder d = new XMLDecoder(in, null, null, conf.getClassLoader());
-    QueryPlan ret = (QueryPlan) d.readObject();
-    d.close();
-    return (ret);
+    XMLDecoder d = null;
+    try {
+      d = new XMLDecoder(in, null, null, conf.getClassLoader());
+      QueryPlan ret = (QueryPlan) d.readObject();
+      return (ret);
+    } finally {
+      if (null != d) {
+        d.close();
+      }
+    }
   }
 
   /**
@@ -468,19 +475,32 @@ public static QueryPlan deserializeQueryPlan(InputStream in, Configuration conf)
    * output since it closes the output stream. DO USE mapredWork.toXML() instead.
    */
   public static void serializeMapRedWork(MapredWork w, OutputStream out) {
-    XMLEncoder e = new XMLEncoder(out);
-    // workaround for java 1.5
-    e.setPersistenceDelegate(ExpressionTypes.class, new EnumDelegate());
-    e.setPersistenceDelegate(GroupByDesc.Mode.class, new EnumDelegate());
-    e.writeObject(w);
-    e.close();
+    XMLEncoder e = null;
+    try {
+      e = new XMLEncoder(out);
+      // workaround for java 1.5
+      e.setPersistenceDelegate(ExpressionTypes.class, new EnumDelegate());
+      e.setPersistenceDelegate(GroupByDesc.Mode.class, new EnumDelegate());
+      e.writeObject(w);
+    } finally {
+      if (null != e) {
+        e.close();
+      }
+    }
+
   }
 
   public static MapredWork deserializeMapRedWork(InputStream in, Configuration conf) {
-    XMLDecoder d = new XMLDecoder(in, null, null, conf.getClassLoader());
-    MapredWork ret = (MapredWork) d.readObject();
-    d.close();
-    return (ret);
+    XMLDecoder d = null;
+    try {
+      d = new XMLDecoder(in, null, null, conf.getClassLoader());
+      MapredWork ret = (MapredWork) d.readObject();
+      return (ret);
+    } finally {
+      if (null != d) {
+        d.close();
+      }
+    }
   }
 
   /**
@@ -488,19 +508,31 @@ public static MapredWork deserializeMapRedWork(InputStream in, Configuration con
    * output since it closes the output stream. DO USE mapredWork.toXML() instead.
    */
   public static void serializeMapRedLocalWork(MapredLocalWork w, OutputStream out) {
-    XMLEncoder e = new XMLEncoder(out);
-    // workaround for java 1.5
-    e.setPersistenceDelegate(ExpressionTypes.class, new EnumDelegate());
-    e.setPersistenceDelegate(GroupByDesc.Mode.class, new EnumDelegate());
-    e.writeObject(w);
-    e.close();
+    XMLEncoder e = null;
+    try {
+      e = new XMLEncoder(out);
+      // workaround for java 1.5
+      e.setPersistenceDelegate(ExpressionTypes.class, new EnumDelegate());
+      e.setPersistenceDelegate(GroupByDesc.Mode.class, new EnumDelegate());
+      e.writeObject(w);
+    } finally {
+      if (null != e) {
+        e.close();
+      }
+    }
   }
 
   public static MapredLocalWork deserializeMapRedLocalWork(InputStream in, Configuration conf) {
-    XMLDecoder d = new XMLDecoder(in, null, null, conf.getClassLoader());
-    MapredLocalWork ret = (MapredLocalWork) d.readObject();
-    d.close();
-    return (ret);
+    XMLDecoder d = null;
+    try {
+      d = new XMLDecoder(in, null, null, conf.getClassLoader());
+      MapredLocalWork ret = (MapredLocalWork) d.readObject();
+      return (ret);
+    } finally {
+      if (null != d) {
+        d.close();
+      }
+    }
   }
 
   /**
@@ -610,9 +642,10 @@ public StreamPrinter(InputStream is, String type, PrintStream os) {
 
     @Override
     public void run() {
+      BufferedReader br = null;
       try {
         InputStreamReader isr = new InputStreamReader(is);
-        BufferedReader br = new BufferedReader(isr);
+        br = new BufferedReader(isr);
         String line = null;
         if (type != null) {
           while ((line = br.readLine()) != null) {
@@ -623,8 +656,12 @@ public void run() {
             os.println(line);
           }
         }
+        br.close();
+        br=null;
       } catch (IOException ioe) {
         ioe.printStackTrace();
+      }finally{
+        IOUtils.closeStream(br);
       }
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/SequenceFileInputFormatChecker.java b/ql/src/java/org/apache/hadoop/hive/ql/io/SequenceFileInputFormatChecker.java
index a24262d71e..e2666d7cfc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/SequenceFileInputFormatChecker.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/SequenceFileInputFormatChecker.java
@@ -24,6 +24,7 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.SequenceFile;
 
 /**
@@ -39,12 +40,16 @@ public boolean validateInput(FileSystem fs, HiveConf conf,
       return false;
     }
     for (int fileId = 0; fileId < files.size(); fileId++) {
+      SequenceFile.Reader reader = null;
       try {
-        SequenceFile.Reader reader = new SequenceFile.Reader(fs, files.get(
+        reader = new SequenceFile.Reader(fs, files.get(
             fileId).getPath(), conf);
         reader.close();
+        reader = null;
       } catch (IOException e) {
         return false;
+      }finally{
+        IOUtils.closeStream(reader);
       }
     }
     return true;
