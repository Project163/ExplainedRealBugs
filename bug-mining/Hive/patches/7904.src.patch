diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/NonBlockingOpDeDupProc.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/NonBlockingOpDeDupProc.java
index a5972d0ce2..cfa85679e6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/NonBlockingOpDeDupProc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/NonBlockingOpDeDupProc.java
@@ -27,6 +27,8 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.Stack;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
 
 import org.apache.hadoop.hive.ql.exec.FilterOperator;
 import org.apache.hadoop.hive.ql.exec.JoinOperator;
@@ -73,6 +75,10 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {
     return pctx;
   }
 
+  public static <K, V> Map<K, V> zipToMap(List<K> keys, List<V> values) {
+    return IntStream.range(0, keys.size()).boxed().collect(Collectors.toMap(keys::get, values::get));
+  }
+
   private class SelectDedup implements SemanticNodeProcessor {
 
     private ParseContext pctx;
@@ -110,6 +116,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
           // we do not need to update the ColumnExprMap in the parent SelectOperator.
           pSEL.getConf().setColList(ExprNodeDescUtils.backtrack(cSELColList, cSEL, pSEL, true));
           pSEL.getConf().setOutputColumnNames(cSELOutputColumnNames);
+
+          pSEL.setColumnExprMap(zipToMap(cSELOutputColumnNames, pSEL.getConf().getColList()));
         } else {
           // If the child SelectOperator has the ColumnExprMap,
           // we need to update the ColumnExprMap in the parent SelectOperator.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationUtilities.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationUtilities.java
index d2cf78bee5..ea7777dc79 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationUtilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationUtilities.java
@@ -43,7 +43,6 @@
 import org.apache.hadoop.hive.ql.exec.SelectOperator;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.Utilities;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.ReduceSinkDeduplicateProcCtx;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
@@ -384,7 +383,21 @@ public static List<Operator<? extends OperatorDesc>> findSiblingOperators(
   protected static SelectOperator replaceReduceSinkWithSelectOperator(ReduceSinkOperator childRS,
       ParseContext context, AbstractCorrelationProcCtx procCtx) throws SemanticException {
     RowSchema inputRS = childRS.getSchema();
-    SelectDesc select = new SelectDesc(childRS.getConf().getValueCols(), childRS.getConf().getOutputValueColumnNames());
+
+    List<String> columnNames = new ArrayList<String>();
+
+    for (String colName : childRS.getConf().getOutputValueColumnNames()) {
+      columnNames.add("VALUE." + colName);
+    }
+    for (String colName : childRS.getConf().getOutputKeyColumnNames()) {
+      columnNames.add("KEY." + colName);
+    }
+
+    List<ExprNodeDesc> colExprs = new ArrayList<ExprNodeDesc>();
+    colExprs.addAll(childRS.getConf().getKeyCols());
+    colExprs.addAll(childRS.getConf().getValueCols());
+
+    SelectDesc select = new SelectDesc(colExprs, columnNames);
 
     Operator<?> parent = getSingleParent(childRS);
     parent.removeChild(childRS);
