diff --git a/CHANGES.txt b/CHANGES.txt
index a172b635ac..632e2aa6dd 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -397,6 +397,9 @@ Trunk - Unreleased
     HIVE-654. renaming thrift serde
     (Zheng Shao via namit)
 
+    HIVE-666. Propagate errors in FetchTask to client.
+    (Zheng Shao via rmurthy)
+
 Release 0.3.1 - Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java b/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
index bbedd1f8dc..4c0de13aac 100644
--- a/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
+++ b/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
@@ -126,18 +126,26 @@ public int processCmd(String cmd) {
           }
         
           Vector<String> res = new Vector<String>();
-          while (qp.getResults(res)) {
-            for (String r:res) {
-              out.println(r);
-            }
-            res.clear();
-            if (out.checkError()) {
-              break;
+          try {
+            while (qp.getResults(res)) {
+              for (String r:res) {
+                out.println(r);
+              }
+              res.clear();
+              if (out.checkError()) {
+                break;
+              }
             }
+          } catch (IOException e) {
+            console.printError("Failed with exception " + e.getClass().getName() + ":" +   e.getMessage(),
+                "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
+            ret = 1;
           }
-      
+            
           int cret = qp.close();
-          ret = cret;
+          if (ret == 0) {
+            ret = cret;
+          }
 
           long end = System.currentTimeMillis();
           if (end > start) {
diff --git a/data/files/kv1_broken.seq b/data/files/kv1_broken.seq
new file mode 100644
index 0000000000..49bddeb805
Binary files /dev/null and b/data/files/kv1_broken.seq differ
diff --git a/hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionItem.java b/hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionItem.java
index cefba51d35..8a0bbbf5d0 100644
--- a/hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionItem.java
+++ b/hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionItem.java
@@ -298,14 +298,19 @@ public void runQuery() {
 
 		queryRet = qp.run(this.query);
 		Vector<String> res = new Vector<String>();
-		while (qp.getResults(res)) {
-			for (String row : res) {
-				if (ss.out != null) {
-					ss.out.println(row);
-				}
-			}
-			res.clear();
-		}
+    try {
+  		while (qp.getResults(res)) {
+  			for (String row : res) {
+  				if (ss.out != null) {
+  					ss.out.println(row);
+  				}
+  			}
+  			res.clear();
+  		}
+    } catch (IOException ex) {
+      l4j.error(this.getSessionName() + " getting results "
+          + this.getResultFile() + " caused exception.", ex);
+    }
 		try {
 			if (fos != null) {
 				fos.close();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index e75daad53e..90d5773c55 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -384,7 +384,7 @@ public int execute() {
     return (0);
   }
 
-  public boolean getResults(Vector<String> res) {
+  public boolean getResults(Vector<String> res) throws IOException {
     if (plan != null && plan.getPlan().getFetchTask() != null) {
       BaseSemanticAnalyzer sem = plan.getPlan();
       if (!sem.getFetchTaskInit()) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
index cc2c2f9307..dc620ab537 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
@@ -245,7 +245,7 @@ private RecordReader<WritableComparable, Writable> getRecordReader()
    * @param ctx
    *          fetch context
    **/
-  public InspectableObject getNextRow() {
+  public InspectableObject getNextRow() throws IOException {
     try {
       if (currRecReader == null) {
         currRecReader = getRecordReader();
@@ -272,7 +272,7 @@ public InspectableObject getNextRow() {
           return getNextRow();
       }
     } catch (Exception e) {
-      return null;
+      throw new IOException(e);
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
index cdd422c7e0..0e6a00f968 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
@@ -118,7 +118,7 @@ public void setMaxRows(int maxRows) {
     this.maxRows = maxRows;
   }
 	
-  public boolean fetch(Vector<String> res) {
+  public boolean fetch(Vector<String> res) throws IOException {
     try {
       int numRows = 0;
       int rowsRet = maxRows;
@@ -144,9 +144,11 @@ public boolean fetch(Vector<String> res) {
       totalRows += numRows;
       return true;
     }
+    catch (IOException e) {
+      throw e;
+    }
     catch (Exception e) {
-      console.printError("Failed with exception " + e.getClass().getName() + ":" +   e.getMessage(), "\n" + StringUtils.stringifyException(e));
-      return false;
+      throw new IOException(e);
     }
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
index cd93933ad9..377bb1b0c6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
@@ -80,7 +80,7 @@ public void initialize (HiveConf conf) {
   public abstract int execute();
   
   // dummy method - FetchTask overwrites this
-  public boolean fetch(Vector<String> res) { 
+  public boolean fetch(Vector<String> res) throws IOException { 
     assert false;
   	return false;
   }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
index f03767be83..4cc4d6c7a7 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
@@ -264,7 +264,8 @@ public void cleanUp() throws Exception {
     for(String s: new String [] {"src", "src1", "src_json", "src_thrift", "src_sequencefile", 
                                  "srcpart", "srcbucket", "dest1", "dest2", 
                                  "dest3", "dest4", "dest4_sequencefile",
-                                 "dest_j1", "dest_j2", "dest_g1", "dest_g2"}) {
+                                 "dest_j1", "dest_j2", "dest_g1", "dest_g2",
+                                 "fetchtask_ioexception"}) {
       db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, s);
     }
     for(String s: new String [] {"dest4.out", "union.out"}) {
diff --git a/ql/src/test/queries/clientnegative/fetchtask_ioexception.q b/ql/src/test/queries/clientnegative/fetchtask_ioexception.q
new file mode 100644
index 0000000000..9f44f225e9
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/fetchtask_ioexception.q
@@ -0,0 +1,7 @@
+CREATE TABLE fetchtask_ioexception (
+  KEY STRING,
+  VALUE STRING) STORED AS SEQUENCEFILE;
+
+LOAD DATA LOCAL INPATH '../data/files/kv1_broken.seq' OVERWRITE INTO TABLE fetchtask_ioexception;
+
+SELECT * FROM fetchtask_ioexception;
diff --git a/ql/src/test/results/clientnegative/fetchtask_ioexception.q.out b/ql/src/test/results/clientnegative/fetchtask_ioexception.q.out
new file mode 100644
index 0000000000..1ddd663fa8
--- /dev/null
+++ b/ql/src/test/results/clientnegative/fetchtask_ioexception.q.out
@@ -0,0 +1,8 @@
+query: CREATE TABLE fetchtask_ioexception (
+  KEY STRING,
+  VALUE STRING) STORED AS SEQUENCEFILE
+query: LOAD DATA LOCAL INPATH '../data/files/kv1_broken.seq' OVERWRITE INTO TABLE fetchtask_ioexception
+query: SELECT * FROM fetchtask_ioexception
+Input: default/fetchtask_ioexception
+Output: file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/tmp/1939498833/10000
+Failed with exception java.io.IOException:java.io.EOFException
diff --git a/service/src/java/org/apache/hadoop/hive/service/HiveServer.java b/service/src/java/org/apache/hadoop/hive/service/HiveServer.java
index fb00cce191..3361aee363 100644
--- a/service/src/java/org/apache/hadoop/hive/service/HiveServer.java
+++ b/service/src/java/org/apache/hadoop/hive/service/HiveServer.java
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.service;
 
+import java.io.IOException;
 import java.util.List;
 import java.util.Vector;
 
@@ -143,13 +144,17 @@ public Schema getSchema() throws HiveServerException, TException {
     public String fetchOne() throws HiveServerException, TException {
       driver.setMaxRows(1);
       Vector<String> result = new Vector<String>();
-      if (driver.getResults(result)) {
-        return result.get(0);
+      try {
+        if (driver.getResults(result)) {
+          return result.get(0);
+        }
+        // TODO: Cannot return null here because thrift cannot handle nulls
+        // TODO: Returning empty string for now. Need to figure out how to
+        // TODO: return null in some other way
+        return "";
+      } catch (IOException e) {
+        throw new HiveServerException(e.getMessage());
       }
-      // TODO: Cannot return null here because thrift cannot handle nulls
-      // TODO: Returning empty string for now. Need to figure out how to
-      // TODO: return null in some other way
-      return "";
     }
 
     /**
@@ -168,7 +173,11 @@ public List<String> fetchN(int numRows) throws HiveServerException, TException {
       } 
       Vector<String> result = new Vector<String>();
       driver.setMaxRows(numRows);
-      driver.getResults(result);
+      try {
+        driver.getResults(result);
+      } catch (IOException e) {
+        throw new HiveServerException(e.getMessage());
+      }
       return result;
     }
 
@@ -183,9 +192,13 @@ public List<String> fetchN(int numRows) throws HiveServerException, TException {
     public List<String> fetchAll() throws HiveServerException, TException {
       Vector<String> rows = new Vector<String>();
       Vector<String> result = new Vector<String>();
-      while (driver.getResults(result)) {
-        rows.addAll(result);
-        result.clear();
+      try {
+        while (driver.getResults(result)) {
+          rows.addAll(result);
+          result.clear();
+        }
+      } catch (IOException e) {
+        throw new HiveServerException(e.getMessage());
       }
       return rows;
     }
