diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
index 6f24e93cb5..08bc654c03 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
@@ -436,6 +436,15 @@ public enum ErrorMsg {
   HIVE_GROUPING_SETS_AGGR_NOMAPAGGR_MULTIGBY(10315,
       "Grouping sets aggregations (with rollups or cubes) are not allowed when " +
       "HIVEMULTIGROUPBYSINGLEREDUCER is turned on. Set hive.multigroupby.singlereducer=false if you want to use grouping sets"),
+  CANNOT_RETRIEVE_TABLE_METADATA(10316, "Error while retrieving table metadata"),
+  CANNOT_DROP_INDEX(10317, "Error while dropping index"),
+  INVALID_AST_TREE(10318, "Internal error : Invalid AST"),
+  ERROR_SERIALIZE_METASTORE(10319, "Error while serializing the metastore objects"),
+  IO_ERROR(10320, "Error while peforming IO operation "),
+  ERROR_SERIALIZE_METADATA(10321, "Error while serializing the metadata"),
+  INVALID_LOAD_TABLE_FILE_WORK(10322, "Invalid Load Table Work or Load File Work"),
+  CLASSPATH_ERROR(10323, "Classpath error"),
+  IMPORT_SEMANTIC_ERROR(10324, "Import Semantic Analyzer Error"),
   //========================== 20000 range starts here ========================//
   SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),
   SCRIPT_IO_ERROR(20001, "An error occurred while reading or writing to your custom script. "
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index 8a9411a26b..af1ee20a86 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -781,7 +781,7 @@ public TableSpec(Hive db, HiveConf conf, ASTNode ast, boolean allowDynamicPartit
         throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(ast
             .getChild(0)), ite);
       } catch (HiveException e) {
-        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg(ast
+        throw new SemanticException(ErrorMsg.CANNOT_RETRIEVE_TABLE_METADATA.getMsg(ast
             .getChild(childIndex), e.getMessage()), e);
       }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 141c2e6069..a14955ae7a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -1182,7 +1182,7 @@ private void analyzeDropIndex(ASTNode ast) throws SemanticException {
       Index idx = db.getIndex(tableName, indexName);
     } catch (HiveException e) {
       if (!(e.getCause() instanceof NoSuchObjectException)) {
-        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg("dropping index"), e);
+        throw new SemanticException(ErrorMsg.CANNOT_DROP_INDEX.getMsg("dropping index"), e);
       }
       if (throwException) {
         throw new SemanticException(ErrorMsg.INVALID_INDEX.getMsg(indexName));
@@ -2092,7 +2092,7 @@ private void analyzeShowTables(ASTNode ast) throws SemanticException {
     String tableNames = null;
 
     if (ast.getChildCount() > 3) {
-      throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
+      throw new SemanticException(ErrorMsg.INVALID_AST_TREE.getMsg(ast.toStringTree()));
     }
 
     switch (ast.getChildCount()) {
@@ -2148,7 +2148,7 @@ private void analyzeShowTableStatus(ASTNode ast) throws SemanticException {
     HashMap<String, String> partSpec = null;
     if (children >= 2) {
       if (children > 3) {
-        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
+        throw new SemanticException(ErrorMsg.INVALID_AST_TREE.getMsg());
       }
       for (int i = 1; i < children; i++) {
         ASTNode child = (ASTNode) ast.getChild(i);
@@ -2157,7 +2157,8 @@ private void analyzeShowTableStatus(ASTNode ast) throws SemanticException {
         } else if (child.getToken().getType() == HiveParser.TOK_PARTSPEC) {
           partSpec = getValidatedPartSpec(getTable(tableNames), child, conf, false);
         } else {
-          throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
+          throw new SemanticException(ErrorMsg.INVALID_AST_TREE.getMsg(child.toStringTree() +
+            " , Invalid token " + child.getToken().getType()));
         }
       }
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java
index 179f9c2ff0..a3fcaa0a88 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java
@@ -122,7 +122,7 @@ static URI getValidatedURI(HiveConf conf, String dcPath) throws SemanticExceptio
         throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(), e);
       }
     } catch (IOException e) {
-      throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg(), e);
+      throw new SemanticException(ErrorMsg.IO_ERROR.getMsg(), e);
     }
   }
 
@@ -239,8 +239,8 @@ public static void createExportDump(FileSystem fs, Path metadataPath,
         jgen.writeEndArray();
       } catch (TException e) {
         throw new SemanticException(
-            ErrorMsg.GENERIC_ERROR
-                .getMsg("Exception while serializing the metastore objects"), e);
+            ErrorMsg.ERROR_SERIALIZE_METASTORE
+                .getMsg(), e);
       }
     }
     jgen.writeEndObject();
@@ -318,9 +318,9 @@ public static ReadMetaData readMetaData(FileSystem fs, Path metadataPath)
 
       return new ReadMetaData(table, partitionsList,readReplicationSpec(jsonContainer));
     } catch (JSONException e) {
-      throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg("Error in serializing metadata"), e);
+      throw new SemanticException(ErrorMsg.ERROR_SERIALIZE_METADATA.getMsg(), e);
     } catch (TException e) {
-      throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg("Error in serializing metadata"), e);
+      throw new SemanticException(ErrorMsg.ERROR_SERIALIZE_METADATA.getMsg(), e);
     } finally {
       if (mdstream != null) {
         mdstream.close();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
index ff385d0312..fe8147af0b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
@@ -166,7 +166,7 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
           + " and then copied to " + toURI.toString());
     } catch (Exception e) {
       throw new SemanticException(
-          ErrorMsg.GENERIC_ERROR
+          ErrorMsg.IO_ERROR
               .getMsg("Exception while writing out the local file"), e);
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
index 88c4b95d91..549d24f7bf 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
@@ -237,7 +237,7 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
     } catch (SemanticException e) {
       throw e;
     } catch (Exception e) {
-      throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg(), e);
+      throw new SemanticException(ErrorMsg.IMPORT_SEMANTIC_ERROR.getMsg(), e);
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java
index a8f9f50644..89897d7e58 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java
@@ -108,7 +108,7 @@ public void compile(final ParseContext pCtx, final List<Task<? extends Serializa
      */
     if (pCtx.getQueryProperties().isQuery() && !isCStats) {
       if ((!loadTableWork.isEmpty()) || (loadFileWork.size() != 1)) {
-        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
+        throw new SemanticException(ErrorMsg.INVALID_LOAD_TABLE_FILE_WORK.getMsg());
       }
 
       LoadFileDesc loadFileDesc = loadFileWork.get(0);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
index 3f38f74ffd..fd2bade99f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
@@ -430,7 +430,7 @@ public void validate(HiveConf conf)
             .getMsg());
         }
       } catch (ClassNotFoundException e) {
-        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg(), e);
+        throw new SemanticException(ErrorMsg.CLASSPATH_ERROR.getMsg(), e);
       }
     }
 
