diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
index f6531e81a3..221c99e259 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
@@ -53,7 +53,7 @@
 import org.apache.hadoop.hive.llap.io.decode.GenericColumnVectorProducer.SerDeStripeMetadata;
 import org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer;
 import org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.CacheWriter;
-import org.apache.hadoop.hive.llap.io.encoded.VertorDeserializeOrcWriter.AsyncCallback;
+import org.apache.hadoop.hive.llap.io.encoded.VectorDeserializeOrcWriter.AsyncCallback;
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.io.HdfsUtils;
@@ -176,7 +176,7 @@ public DiskRangeList createCacheChunk(MemoryBuffer buffer, long offset, long end
    * the consumer, at which point the consumer is responsible for it.
    */
   private FileData cachedData;
-  private List<VertorDeserializeOrcWriter> asyncWriters = new ArrayList<>();
+  private List<VectorDeserializeOrcWriter> asyncWriters = new ArrayList<>();
 
   public SerDeEncodedDataReader(SerDeLowLevelCacheImpl cache,
       BufferUsageManager bufferManager, Configuration daemonConf, FileSplit split,
@@ -1375,15 +1375,15 @@ public void startReadSplitFromFile(
       List<Integer> splitColumnIds = OrcInputFormat.genIncludedColumnsReverse(
           schema, splitIncludes, false);
       // fileread writes to the writer, which writes to orcWriter, which writes to cacheWriter
-      EncodingWriter writer = VertorDeserializeOrcWriter.create(
+      EncodingWriter writer = VectorDeserializeOrcWriter.create(
           sourceInputFormat, sourceSerDe, parts, daemonConf, jobConf, split.getPath(), originalOi,
           splitColumnIds, splitIncludes, allocSize);
       // TODO: move this into ctor? EW would need to create CacheWriter then
       List<Integer> cwColIds = writer.isOnlyWritingIncludedColumns() ? splitColumnIds : columnIds;
       writer.init(new CacheWriter(bufferManager, cwColIds, splitIncludes,
           writer.isOnlyWritingIncludedColumns()), daemonConf, split.getPath());
-      if (writer instanceof VertorDeserializeOrcWriter) {
-        VertorDeserializeOrcWriter asyncWriter = (VertorDeserializeOrcWriter)writer;
+      if (writer instanceof VectorDeserializeOrcWriter) {
+        VectorDeserializeOrcWriter asyncWriter = (VectorDeserializeOrcWriter)writer;
         asyncWriter.startAsync(new AsyncCacheDataCallback());
         this.asyncWriters.add(asyncWriter);
       }
@@ -1403,7 +1403,7 @@ public void startReadSplitFromFile(
 
   private class AsyncCacheDataCallback implements AsyncCallback {
     @Override
-    public void onComplete(VertorDeserializeOrcWriter writer) {
+    public void onComplete(VectorDeserializeOrcWriter writer) {
       CacheWriter cacheWriter = null;
       try {
         cacheWriter = writer.getCacheWriter();
@@ -1596,7 +1596,7 @@ private boolean sendEcbToConsumer(OrcEncodedColumnBatch ecb,
   private void cleanup(boolean isError) {
     cleanUpCurrentRead();
     if (!isError) return;
-    for (VertorDeserializeOrcWriter asyncWriter : asyncWriters) {
+    for (VectorDeserializeOrcWriter asyncWriter : asyncWriters) {
       try {
         asyncWriter.interrupt();
       } catch (Exception ex) {
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VertorDeserializeOrcWriter.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VectorDeserializeOrcWriter.java
similarity index 98%
rename from llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VertorDeserializeOrcWriter.java
rename to llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VectorDeserializeOrcWriter.java
index 86d9ecc183..c9df7d94a0 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VertorDeserializeOrcWriter.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VectorDeserializeOrcWriter.java
@@ -60,7 +60,7 @@
 import org.apache.hadoop.mapred.TextInputFormat;
 
 /** The class that writes rows from a text reader to an ORC writer using VectorDeserializeRow. */
-class VertorDeserializeOrcWriter extends EncodingWriter implements Runnable {
+class VectorDeserializeOrcWriter extends EncodingWriter implements Runnable {
   private final VectorizedRowBatchCtx vrbCtx;
   private Writer orcWriter;
   private final LazySimpleDeserializeRead deserializeRead;
@@ -113,11 +113,11 @@ public static EncodingWriter create(InputFormat<?, ?> sourceIf, Deserializer ser
       }
     }
     LlapIoImpl.LOG.info("Creating VertorDeserializeOrcWriter for " + path);
-    return new VertorDeserializeOrcWriter(
+    return new VectorDeserializeOrcWriter(
         daemonConf, tblProps, sourceOi, sourceIncludes, cacheIncludes, allocSize);
   }
 
-  private VertorDeserializeOrcWriter(Configuration conf, Properties tblProps,
+  private VectorDeserializeOrcWriter(Configuration conf, Properties tblProps,
       StructObjectInspector sourceOi, List<Integer> sourceIncludes, boolean[] cacheIncludes,
       int allocSize) throws IOException {
     super(sourceOi, allocSize);
@@ -219,7 +219,7 @@ public void init(CacheWriter cacheWriter, Configuration conf, Path path) throws
   }
 
   public interface AsyncCallback {
-    void onComplete(VertorDeserializeOrcWriter writer);
+    void onComplete(VectorDeserializeOrcWriter writer);
   }
 
   @Override
