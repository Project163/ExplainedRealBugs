diff --git a/CHANGES.txt b/CHANGES.txt
index 59e2acc6ae..ac6dd02d0e 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -309,6 +309,9 @@ Trunk -  Unreleased
     HIVE-1671 multithreading on Context.pathToCS
     (Bennie Schut via namit)
 
+    HIVE-1670 MapJoin throws an error if no column from the mapjoined table is selected
+    (Ning Zhang via namit)
+
   TESTS
 
     HIVE-1464. improve  test query performance
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectValue.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectValue.java
index ed6a0d7211..dbb7a03fe2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectValue.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectValue.java
@@ -24,6 +24,8 @@
 import java.io.ObjectOutput;
 import java.util.ArrayList;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.MapJoinOperator;
@@ -33,7 +35,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.io.Writable;
-
 /**
  * Map Join Object used for both key and value.
  */
@@ -43,6 +44,7 @@ public class MapJoinObjectValue implements Externalizable {
   protected transient RowContainer obj;
   protected transient Configuration conf;
   protected int bucketSize; // bucket size for RowContainer
+  protected Log LOG = LogFactory.getLog(this.getClass().getName());
 
   public MapJoinObjectValue() {
     bucketSize = 100; // default bucket size
@@ -71,7 +73,6 @@ public boolean equals(Object o) {
         }
       }
     }
-
     return false;
   }
 
@@ -90,19 +91,25 @@ public void readExternal(ObjectInput in) throws IOException,
       MapJoinObjectCtx ctx = MapJoinOperator.getMapMetadata().get(
           Integer.valueOf(metadataTag));
       int sz = in.readInt();
+
       RowContainer res = new RowContainer(bucketSize, ctx.getConf());
       res.setSerDe(ctx.getSerDe(), ctx.getStandardOI());
       res.setTableDesc(ctx.getTblDesc());
-      for (int pos = 0; pos < sz; pos++) {
-        Writable val = ctx.getSerDe().getSerializedClass().newInstance();
-        val.readFields(in);
-
-        ArrayList<Object> memObj = (ArrayList<Object>) ObjectInspectorUtils
-            .copyToStandardObject(ctx.getSerDe().deserialize(val), ctx
-            .getSerDe().getObjectInspector(),
-            ObjectInspectorCopyOption.WRITABLE);
-
-        res.add(memObj);
+      if (sz > 0) {
+        int numCols = in.readInt();
+        if (numCols > 0) {
+          for (int pos = 0; pos < sz; pos++) {
+            Writable val = ctx.getSerDe().getSerializedClass().newInstance();
+            val.readFields(in);
+
+            ArrayList<Object> memObj = (ArrayList<Object>) ObjectInspectorUtils
+              .copyToStandardObject(ctx.getSerDe().deserialize(val), ctx
+              .getSerDe().getObjectInspector(),
+               ObjectInspectorCopyOption.WRITABLE);
+
+            res.add(memObj);
+          }
+        }
       }
       obj = res;
     } catch (Exception e) {
@@ -123,10 +130,16 @@ public void writeExternal(ObjectOutput out) throws IOException {
       // Different processing for key and value
       RowContainer<ArrayList<Object>> v = obj;
       out.writeInt(v.size());
-
-      for (ArrayList<Object> row = v.first(); row != null; row = v.next()) {
-        Writable outVal = ctx.getSerDe().serialize(row, ctx.getStandardOI());
-        outVal.write(out);
+      if (v.size() > 0) {
+        ArrayList<Object> row = v.first();
+        out.writeInt(row.size());
+
+        if (row.size() > 0) {
+          for (; row != null; row = v.next()) {
+            Writable outVal = ctx.getSerDe().serialize(row, ctx.getStandardOI());
+            outVal.write(out);
+          }
+        }
       }
     } catch (SerDeException e) {
       throw new IOException(e);
diff --git a/ql/src/test/queries/clientpositive/mapjoin1.q b/ql/src/test/queries/clientpositive/mapjoin1.q
new file mode 100644
index 0000000000..03b06d48ca
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/mapjoin1.q
@@ -0,0 +1,5 @@
+set hive.mapjoin.cache.numrows=100;
+
+SELECT  /*+ MAPJOIN(b) */ sum(a.key) as sum_a
+	FROM srcpart a
+	JOIN src b ON a.key = b.key where a.ds is not null;
diff --git a/ql/src/test/results/clientpositive/mapjoin1.q.out b/ql/src/test/results/clientpositive/mapjoin1.q.out
new file mode 100644
index 0000000000..e027e518a3
--- /dev/null
+++ b/ql/src/test/results/clientpositive/mapjoin1.q.out
@@ -0,0 +1,21 @@
+PREHOOK: query: SELECT  /*+ MAPJOIN(b) */ sum(a.key) as sum_a
+	FROM srcpart a
+	JOIN src b ON a.key = b.key where a.ds is not null
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
+PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
+PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
+PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
+PREHOOK: Output: file:/tmp/nzhang/hive_2010-09-24_21-14-16_226_3903968223903966652/-mr-10000
+POSTHOOK: query: SELECT  /*+ MAPJOIN(b) */ sum(a.key) as sum_a
+	FROM srcpart a
+	JOIN src b ON a.key = b.key where a.ds is not null
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
+POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
+POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
+POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
+POSTHOOK: Output: file:/tmp/nzhang/hive_2010-09-24_21-14-16_226_3903968223903966652/-mr-10000
+76260.0
