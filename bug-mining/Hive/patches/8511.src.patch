diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java
index 82dc791b64..67437aed36 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java
@@ -84,6 +84,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.ArrayListMultimap;
 import com.google.common.collect.HashMultimap;
 import com.google.common.collect.ImmutableList;
@@ -217,48 +218,7 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {
     }
 
     if (pctx.getConf().getBoolVar(ConfVars.HIVE_SHARED_WORK_REUSE_MAPJOIN_CACHE)) {
-      // Try to reuse cache for broadcast side in mapjoin operators that
-      // share same input.
-      // First we group together all the mapjoin operators that share same
-      // reduce sink operator.
-      final Multimap<Operator<?>, MapJoinOperator> parentToMapJoinOperators =
-          ArrayListMultimap.create();
-      for (Set<Operator<?>> workOperators : optimizerCache.getWorkGroups()) {
-        for (Operator<?> op : workOperators) {
-          if (op instanceof MapJoinOperator) {
-            MapJoinOperator mapJoinOp = (MapJoinOperator) op;
-            // Only allowed for mapjoin operator
-            if (!mapJoinOp.getConf().isBucketMapJoin() &&
-                !mapJoinOp.getConf().isDynamicPartitionHashJoin()) {
-              parentToMapJoinOperators.put(
-                  obtainBroadcastInput(mapJoinOp).getParentOperators().get(0), mapJoinOp);
-            }
-          }
-        }
-      }
-      // For each group, set the cache key accordingly if there is more than one operator
-      // and input RS operator are equal
-      for (Collection<MapJoinOperator> c : parentToMapJoinOperators.asMap().values()) {
-        Map<ReduceSinkOperator, String> rsOpToCacheKey = new HashMap<>();
-        for (MapJoinOperator mapJoinOp : c) {
-          ReduceSinkOperator rsOp = obtainBroadcastInput(mapJoinOp);
-          String cacheKey = null;
-          for (Entry<ReduceSinkOperator, String> e: rsOpToCacheKey.entrySet()) {
-            if (compareOperator(pctx, rsOp, e.getKey())) {
-              cacheKey = e.getValue();
-              break;
-            }
-          }
-          if (cacheKey == null) {
-            // Either it is the first map join operator or there was no equivalent RS,
-            // hence generate cache key
-            cacheKey = MapJoinDesc.generateCacheKey(mapJoinOp.getOperatorId());
-            rsOpToCacheKey.put(rsOp, cacheKey);
-          }
-          // Set in the conf of the map join operator
-          mapJoinOp.getConf().setCacheKey(cacheKey);
-        }
-      }
+      runMapJoinCacheReuseOptimization(pctx, optimizerCache);
     }
 
     // If we are running tests, we are going to verify that the contents of the cache
@@ -983,14 +943,91 @@ private static void sharedWorkExtendedOptimization(ParseContext pctx, SharedWork
   }
 
   /**
-   * Obtain the RS input for a mapjoin operator.
+   * Try to reuse cache for broadcast side in mapjoin operators that share the same input.
    */
-  private static ReduceSinkOperator obtainBroadcastInput(MapJoinOperator mapJoinOp) {
+  @VisibleForTesting
+  public void runMapJoinCacheReuseOptimization(
+      ParseContext pctx, SharedWorkOptimizerCache optimizerCache) throws SemanticException {
+    // First we group together all the mapjoin operators whose first broadcast input is the same.
+    final Multimap<Operator<?>, MapJoinOperator> parentToMapJoinOperators = ArrayListMultimap.create();
+    for (Set<Operator<?>> workOperators : optimizerCache.getWorkGroups()) {
+      for (Operator<?> op : workOperators) {
+        if (op instanceof MapJoinOperator) {
+          MapJoinOperator mapJoinOp = (MapJoinOperator) op;
+          // Only allowed for mapjoin operator
+          if (!mapJoinOp.getConf().isBucketMapJoin() &&
+              !mapJoinOp.getConf().isDynamicPartitionHashJoin()) {
+            parentToMapJoinOperators.put(
+                obtainFirstBroadcastInput(mapJoinOp).getParentOperators().get(0), mapJoinOp);
+          }
+        }
+      }
+    }
+
+    // For each group, set the cache key accordingly if there is more than one operator
+    // and input RS operator are equal
+    for (Collection<MapJoinOperator> c : parentToMapJoinOperators.asMap().values()) {
+      Map<MapJoinOperator, String> mapJoinOpToCacheKey = new HashMap<>();
+      for (MapJoinOperator mapJoinOp : c) {
+        String cacheKey = null;
+        for (Entry<MapJoinOperator, String> e: mapJoinOpToCacheKey.entrySet()) {
+          if (canShareBroadcastInputs(pctx, mapJoinOp, e.getKey())) {
+            cacheKey = e.getValue();
+            break;
+          }
+        }
+
+        if (cacheKey == null) {
+          // Either it is the first map join operator or there was no equivalent broadcast input,
+          // hence generate cache key
+          cacheKey = MapJoinDesc.generateCacheKey(mapJoinOp.getOperatorId());
+          mapJoinOpToCacheKey.put(mapJoinOp, cacheKey);
+        }
+
+        mapJoinOp.getConf().setCacheKey(cacheKey);
+      }
+    }
+  }
+
+  private ReduceSinkOperator obtainFirstBroadcastInput(MapJoinOperator mapJoinOp) {
     return mapJoinOp.getParentOperators().get(0) instanceof ReduceSinkOperator ?
         (ReduceSinkOperator) mapJoinOp.getParentOperators().get(0) :
         (ReduceSinkOperator) mapJoinOp.getParentOperators().get(1);
   }
 
+  private boolean canShareBroadcastInputs(
+      ParseContext pctx, MapJoinOperator mapJoinOp1, MapJoinOperator mapJoinOp2) throws SemanticException {
+    if (mapJoinOp1.getNumParent() != mapJoinOp2.getNumParent()) {
+      return false;
+    }
+
+    if (mapJoinOp1.getConf().getPosBigTable() != mapJoinOp2.getConf().getPosBigTable()) {
+      return false;
+    }
+
+    for (int i = 0; i < mapJoinOp1.getNumParent(); i ++) {
+      if (i == mapJoinOp1.getConf().getPosBigTable()) {
+        continue;
+      }
+
+      ReduceSinkOperator parentRS1 = (ReduceSinkOperator) mapJoinOp1.getParentOperators().get(i);
+      ReduceSinkOperator parentRS2 = (ReduceSinkOperator) mapJoinOp2.getParentOperators().get(i);
+
+      if (!compareOperator(pctx, parentRS1, parentRS2)) {
+        return false;
+      }
+
+      Operator<?> grandParent1 = parentRS1.getParentOperators().get(0);
+      Operator<?> grandParent2 = parentRS2.getParentOperators().get(0);
+
+      if (grandParent1 != grandParent2) {
+        return false;
+      }
+    }
+
+    return true;
+  }
+
   /**
    * This method gathers the TS operators with DPP from the context and
    * stores them into the input optimization cache.
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/TestSharedWorkOptimizer.java b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/TestSharedWorkOptimizer.java
index c4f77fd1d9..a2d241688e 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/TestSharedWorkOptimizer.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/TestSharedWorkOptimizer.java
@@ -18,39 +18,48 @@
 
 package org.apache.hadoop.hive.ql.optimizer;
 
+import com.google.common.collect.Lists;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
-
+import org.apache.hadoop.hive.ql.exec.MapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.OperatorFactory;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.SharedWorkOptimizerCache;
 import org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.TSComparator;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.plan.FilterDesc;
+import org.apache.hadoop.hive.ql.plan.MapJoinDesc;
+import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.Statistics;
+import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.TableScanDesc;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.SharedWorkOptimizerCache;
-import org.apache.hadoop.hive.ql.plan.FilterDesc;
-import org.apache.hadoop.hive.ql.plan.OperatorDesc;
-import java.util.ArrayList;
-import java.util.List;
-
 import org.junit.Test;
 
-import com.google.common.collect.Lists;
-
+import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.EnumSet;
+import java.util.List;
+import java.util.Properties;
+
 import static org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.ReducerTraits.AUTOPARALLEL;
 import static org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.ReducerTraits.FIXED;
 import static org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.ReducerTraits.UNIFORM;
 import static org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.ReducerTraits.UNSET;
-import static org.junit.Assert.*;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotEquals;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
 public class TestSharedWorkOptimizer {
 
@@ -266,4 +275,87 @@ public void testSharedWorkOptimizerCache() {
 
   }
 
-}
\ No newline at end of file
+  private ReduceSinkDesc getReduceSinkDesc() {
+    TableDesc dummyKeySerializeInfo = new TableDesc();
+    dummyKeySerializeInfo.setProperties(new Properties());
+
+    ReduceSinkDesc conf = new ReduceSinkDesc(new ArrayList<>(), 0, new ArrayList<>(), new ArrayList<>(),
+        new ArrayList<>(), new ArrayList<>(), 0, new ArrayList<>(), 0, null, null, null);
+    conf.setKeySerializeInfo(dummyKeySerializeInfo);
+    return conf;
+  }
+
+  private MapJoinDesc getMapJoinDesc(int posBigTable) {
+    MapJoinDesc conf = new MapJoinDesc();
+    conf.setPosBigTable(posBigTable);
+    return conf;
+  }
+
+  private void runMapJoinCacheReuseOptimization(Operator<?> op1, Operator<?> op2) {
+    SharedWorkOptimizer sharedWorkOptimizer = new SharedWorkOptimizer();
+    SharedWorkOptimizerCache optimizerCache = new SharedWorkOptimizerCache();
+
+    optimizerCache.addWorkGroup(Arrays.asList(op1, op2));
+    try {
+      sharedWorkOptimizer.runMapJoinCacheReuseOptimization(null, optimizerCache);
+    } catch (SemanticException se) {
+      fail();
+    }
+  }
+
+  @Test
+  public void testMapJoinCacheReuse() {
+    // Big tables
+    TableScanOperator ts1 = getTsOp();
+    TableScanOperator ts2 = getTsOp();
+
+    // Small tables
+    Operator<?> smallTableA = getFilterOp(0);
+    Operator<?> smallTableB = getFilterOp(0);
+    Operator<?> smallTableC = getFilterOp(0);
+
+    Operator<?> rsA1 = OperatorFactory.getAndMakeChild(getReduceSinkDesc(), smallTableA);
+    Operator<?> rsA2 = OperatorFactory.getAndMakeChild(getReduceSinkDesc(), smallTableA);
+
+    Operator<?> rsB1 = OperatorFactory.getAndMakeChild(getReduceSinkDesc(), smallTableB);
+    Operator<?> rsB2 = OperatorFactory.getAndMakeChild(getReduceSinkDesc(), smallTableB);
+
+    Operator<?> rsC2 = OperatorFactory.getAndMakeChild(getReduceSinkDesc(), smallTableC);
+
+    // case 1. MapJoin1: (big, A, B), MapJoin2: (big, A, B)
+    List<Operator<?>> joinSource1 = Arrays.asList(ts1, rsA1, rsB1);
+    List<Operator<?>> joinSource2 = Arrays.asList(ts2, rsA2, rsB2);
+
+    MapJoinOperator mapJoin1 = (MapJoinOperator) OperatorFactory.getAndMakeChild(
+        cCtx, getMapJoinDesc(0), joinSource1);
+    MapJoinOperator mapJoin2 = (MapJoinOperator) OperatorFactory.getAndMakeChild(
+        cCtx, getMapJoinDesc(0), joinSource2);
+
+    runMapJoinCacheReuseOptimization(mapJoin1, mapJoin2);
+    assertEquals(mapJoin1.getConf().getCacheKey(), mapJoin2.getConf().getCacheKey());
+
+    // case 2. MapJoin3: (big, A, B), MapJoin4: (big, A, C)
+    List<Operator<?>> joinSource3 = Arrays.asList(ts1, rsA1, rsB1);
+    List<Operator<?>> joinSource4 = Arrays.asList(ts2, rsA2, rsC2);
+
+    MapJoinOperator mapJoin3 = (MapJoinOperator) OperatorFactory.getAndMakeChild(
+        cCtx, getMapJoinDesc(0), joinSource3);
+    MapJoinOperator mapJoin4 = (MapJoinOperator) OperatorFactory.getAndMakeChild(
+        cCtx, getMapJoinDesc(0), joinSource4);
+
+    runMapJoinCacheReuseOptimization(mapJoin3, mapJoin4);
+    assertNotEquals(mapJoin3.getConf().getCacheKey(), mapJoin4.getConf().getCacheKey());
+
+    // case 3. MapJoin5: (big, A, B), MapJoin6: (A, big, B)
+    List<Operator<?>> joinSource5 = Arrays.asList(ts1, rsA1, rsB1);
+    List<Operator<?>> joinSource6 = Arrays.asList(rsA2, ts2, rsB2);
+
+    MapJoinOperator mapJoin5 = (MapJoinOperator) OperatorFactory.getAndMakeChild(
+        cCtx, getMapJoinDesc(0), joinSource5);
+    MapJoinOperator mapJoin6 = (MapJoinOperator) OperatorFactory.getAndMakeChild(
+        cCtx, getMapJoinDesc(1), joinSource6);
+
+    runMapJoinCacheReuseOptimization(mapJoin5, mapJoin6);
+    assertNotEquals(mapJoin5.getConf().getCacheKey(), mapJoin6.getConf().getCacheKey());
+  }
+}
