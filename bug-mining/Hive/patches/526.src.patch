diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java.orig b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java.orig
deleted file mode 100644
index 06f903afbb..0000000000
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java.orig
+++ /dev/null
@@ -1,2754 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec;
-
-import static org.apache.commons.lang.StringUtils.join;
-import static org.apache.hadoop.util.StringUtils.stringifyException;
-
-import java.io.BufferedWriter;
-import java.io.DataOutput;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.OutputStreamWriter;
-import java.io.Serializable;
-import java.io.Writer;
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Set;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FsShell;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.metastore.ProtectMode;
-import org.apache.hadoop.hive.metastore.TableType;
-import org.apache.hadoop.hive.metastore.Warehouse;
-import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Index;
-import org.apache.hadoop.hive.metastore.api.InvalidOperationException;
-import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
-import org.apache.hadoop.hive.metastore.api.Order;
-import org.apache.hadoop.hive.ql.Context;
-import org.apache.hadoop.hive.ql.DriverContext;
-import org.apache.hadoop.hive.ql.QueryPlan;
-import org.apache.hadoop.hive.ql.hooks.ReadEntity;
-import org.apache.hadoop.hive.ql.hooks.WriteEntity;
-import org.apache.hadoop.hive.ql.lockmgr.HiveLock;
-import org.apache.hadoop.hive.ql.lockmgr.HiveLockManager;
-import org.apache.hadoop.hive.ql.lockmgr.HiveLockMode;
-import org.apache.hadoop.hive.ql.lockmgr.HiveLockObject;
-import org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData;
-import org.apache.hadoop.hive.ql.metadata.CheckResult;
-import org.apache.hadoop.hive.ql.metadata.Hive;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker;
-import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
-import org.apache.hadoop.hive.ql.metadata.InvalidTableException;
-import org.apache.hadoop.hive.ql.metadata.MetaDataFormatUtils;
-import org.apache.hadoop.hive.ql.metadata.Partition;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.plan.AddPartitionDesc;
-import org.apache.hadoop.hive.ql.plan.AlterIndexDesc;
-import org.apache.hadoop.hive.ql.plan.AlterTableDesc;
-import org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableTypes;
-import org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc;
-import org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc;
-import org.apache.hadoop.hive.ql.plan.CreateIndexDesc;
-import org.apache.hadoop.hive.ql.plan.CreateTableDesc;
-import org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc;
-import org.apache.hadoop.hive.ql.plan.CreateViewDesc;
-import org.apache.hadoop.hive.ql.plan.DDLWork;
-import org.apache.hadoop.hive.ql.plan.DescDatabaseDesc;
-import org.apache.hadoop.hive.ql.plan.DescFunctionDesc;
-import org.apache.hadoop.hive.ql.plan.DescTableDesc;
-import org.apache.hadoop.hive.ql.plan.DropDatabaseDesc;
-import org.apache.hadoop.hive.ql.plan.DropIndexDesc;
-import org.apache.hadoop.hive.ql.plan.DropTableDesc;
-import org.apache.hadoop.hive.ql.plan.LockTableDesc;
-import org.apache.hadoop.hive.ql.plan.MsckDesc;
-import org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc;
-import org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc;
-import org.apache.hadoop.hive.ql.plan.ShowIndexesDesc;
-import org.apache.hadoop.hive.ql.plan.ShowLocksDesc;
-import org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc;
-import org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc;
-import org.apache.hadoop.hive.ql.plan.ShowTablesDesc;
-import org.apache.hadoop.hive.ql.plan.SwitchDatabaseDesc;
-import org.apache.hadoop.hive.ql.plan.UnlockTableDesc;
-import org.apache.hadoop.hive.ql.plan.api.StageType;
-import org.apache.hadoop.hive.serde.Constants;
-import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
-import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.SerDeUtils;
-import org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe;
-import org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe;
-import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
-import org.apache.hadoop.hive.shims.HadoopShims;
-import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.util.ToolRunner;
-/**
- * DDLTask implementation.
- *
- **/
-public class DDLTask extends Task<DDLWork> implements Serializable {
-  private static final long serialVersionUID = 1L;
-  private static final Log LOG = LogFactory.getLog("hive.ql.exec.DDLTask");
-
-  transient HiveConf conf;
-  private static final int separator = Utilities.tabCode;
-  private static final int terminator = Utilities.newLineCode;
-
-  // These are suffixes attached to intermediate directory names used in the
-  // archiving / un-archiving process.
-  private static String INTERMEDIATE_ARCHIVED_DIR_SUFFIX;
-  private static String INTERMEDIATE_ORIGINAL_DIR_SUFFIX;
-  private static String INTERMEDIATE_EXTRACTED_DIR_SUFFIX;
-
-  public DDLTask() {
-    super();
-  }
-
-  @Override
-  public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext ctx) {
-    super.initialize(conf, queryPlan, ctx);
-    this.conf = conf;
-
-    INTERMEDIATE_ARCHIVED_DIR_SUFFIX =
-      HiveConf.getVar(conf, ConfVars.METASTORE_INT_ARCHIVED);
-    INTERMEDIATE_ORIGINAL_DIR_SUFFIX =
-      HiveConf.getVar(conf, ConfVars.METASTORE_INT_ORIGINAL);
-    INTERMEDIATE_EXTRACTED_DIR_SUFFIX =
-      HiveConf.getVar(conf, ConfVars.METASTORE_INT_EXTRACTED);
-  }
-
-  @Override
-  public int execute(DriverContext driverContext) {
-
-    // Create the db
-    Hive db;
-    try {
-      db = Hive.get(conf);
-
-      CreateDatabaseDesc createDatabaseDesc = work.getCreateDatabaseDesc();
-      if (null != createDatabaseDesc) {
-        return createDatabase(db, createDatabaseDesc);
-      }
-
-      DropDatabaseDesc dropDatabaseDesc = work.getDropDatabaseDesc();
-      if (dropDatabaseDesc != null) {
-        return dropDatabase(db, dropDatabaseDesc);
-      }
-
-      SwitchDatabaseDesc switchDatabaseDesc = work.getSwitchDatabaseDesc();
-      if (switchDatabaseDesc != null) {
-        return switchDatabase(db, switchDatabaseDesc);
-      }
-
-      DescDatabaseDesc descDatabaseDesc = work.getDescDatabaseDesc();
-      if (descDatabaseDesc != null) {
-        return descDatabase(descDatabaseDesc);
-      }
-
-      CreateTableDesc crtTbl = work.getCreateTblDesc();
-      if (crtTbl != null) {
-        return createTable(db, crtTbl);
-      }
-
-      CreateIndexDesc crtIndex = work.getCreateIndexDesc();
-      if (crtIndex != null) {
-        return createIndex(db, crtIndex);
-      }
-
-      AlterIndexDesc alterIndex = work.getAlterIndexDesc();
-      if (alterIndex != null) {
-        return alterIndex(db, alterIndex);
-      }
-
-      DropIndexDesc dropIdx = work.getDropIdxDesc();
-      if (dropIdx != null) {
-        return dropIndex(db, dropIdx);
-      }
-
-      CreateTableLikeDesc crtTblLike = work.getCreateTblLikeDesc();
-      if (crtTblLike != null) {
-        return createTableLike(db, crtTblLike);
-      }
-
-      DropTableDesc dropTbl = work.getDropTblDesc();
-      if (dropTbl != null) {
-        return dropTable(db, dropTbl);
-      }
-
-      AlterTableDesc alterTbl = work.getAlterTblDesc();
-      if (alterTbl != null) {
-        return alterTable(db, alterTbl);
-      }
-
-      CreateViewDesc crtView = work.getCreateViewDesc();
-      if (crtView != null) {
-        return createView(db, crtView);
-      }
-
-      AddPartitionDesc addPartitionDesc = work.getAddPartitionDesc();
-      if (addPartitionDesc != null) {
-        return addPartition(db, addPartitionDesc);
-      }
-
-      AlterTableSimpleDesc simpleDesc = work.getAlterTblSimpleDesc();
-      if (simpleDesc != null) {
-        if (simpleDesc.getType() == AlterTableTypes.TOUCH) {
-          return touch(db, simpleDesc);
-        } else if (simpleDesc.getType() == AlterTableTypes.ARCHIVE) {
-          return archive(db, simpleDesc, driverContext);
-        } else if (simpleDesc.getType() == AlterTableTypes.UNARCHIVE) {
-          return unarchive(db, simpleDesc);
-        }
-      }
-
-      MsckDesc msckDesc = work.getMsckDesc();
-      if (msckDesc != null) {
-        return msck(db, msckDesc);
-      }
-
-      DescTableDesc descTbl = work.getDescTblDesc();
-      if (descTbl != null) {
-        return describeTable(db, descTbl);
-      }
-
-      DescFunctionDesc descFunc = work.getDescFunctionDesc();
-      if (descFunc != null) {
-        return describeFunction(descFunc);
-      }
-
-      ShowDatabasesDesc showDatabases = work.getShowDatabasesDesc();
-      if (showDatabases != null) {
-        return showDatabases(db, showDatabases);
-      }
-
-      ShowTablesDesc showTbls = work.getShowTblsDesc();
-      if (showTbls != null) {
-        return showTables(db, showTbls);
-      }
-
-      ShowTableStatusDesc showTblStatus = work.getShowTblStatusDesc();
-      if (showTblStatus != null) {
-        return showTableStatus(db, showTblStatus);
-      }
-
-      ShowFunctionsDesc showFuncs = work.getShowFuncsDesc();
-      if (showFuncs != null) {
-        return showFunctions(showFuncs);
-      }
-
-      ShowLocksDesc showLocks = work.getShowLocksDesc();
-      if (showLocks != null) {
-        return showLocks(showLocks);
-      }
-
-      LockTableDesc lockTbl = work.getLockTblDesc();
-      if (lockTbl != null) {
-        return lockTable(lockTbl);
-      }
-
-      UnlockTableDesc unlockTbl = work.getUnlockTblDesc();
-      if (unlockTbl != null) {
-        return unlockTable(unlockTbl);
-      }
-
-      ShowPartitionsDesc showParts = work.getShowPartsDesc();
-      if (showParts != null) {
-        return showPartitions(db, showParts);
-      }
-
-      ShowIndexesDesc showIndexes = work.getShowIndexesDesc();
-      if (showIndexes != null) {
-        return showIndexes(db, showIndexes);
-      }
-
-    } catch (InvalidTableException e) {
-      console.printError("Table " + e.getTableName() + " does not exist");
-      LOG.debug(stringifyException(e));
-      return 1;
-    } catch (HiveException e) {
-      console.printError("FAILED: Error in metadata: " + e.getMessage(), "\n"
-          + stringifyException(e));
-      LOG.debug(stringifyException(e));
-      return 1;
-    } catch (Exception e) {
-      console.printError("Failed with exception " + e.getMessage(), "\n"
-          + stringifyException(e));
-      return (1);
-    }
-    assert false;
-    return 0;
-  }
-
-  private int dropIndex(Hive db, DropIndexDesc dropIdx) throws HiveException {
-    db.dropIndex(db.getCurrentDatabase(), dropIdx.getTableName(),
-        dropIdx.getIndexName(), true);
-    return 0;
-  }
-
-  private int createIndex(Hive db, CreateIndexDesc crtIndex) throws HiveException {
-
-    if( crtIndex.getSerde() != null) {
-      validateSerDe(crtIndex.getSerde());
-    }
-
-    db
-        .createIndex(
-        crtIndex.getTableName(), crtIndex.getIndexName(), crtIndex.getIndexTypeHandlerClass(),
-        crtIndex.getIndexedCols(), crtIndex.getIndexTableName(), crtIndex.getDeferredRebuild(),
-        crtIndex.getInputFormat(), crtIndex.getOutputFormat(), crtIndex.getSerde(),
-        crtIndex.getStorageHandler(), crtIndex.getLocation(), crtIndex.getIdxProps(), crtIndex.getTblProps(),
-        crtIndex.getSerdeProps(), crtIndex.getCollItemDelim(), crtIndex.getFieldDelim(), crtIndex.getFieldEscape(),
-        crtIndex.getLineDelim(), crtIndex.getMapKeyDelim(), crtIndex.getIndexComment()
-        );
-    return 0;
-  }
-
-  private int alterIndex(Hive db, AlterIndexDesc alterIndex) throws HiveException {
-    String dbName = alterIndex.getDbName();
-    String baseTableName = alterIndex.getBaseTableName();
-    String indexName = alterIndex.getIndexName();
-    Index idx = db.getIndex(dbName, baseTableName, indexName);
-
-    if (alterIndex.getOp() == AlterIndexDesc.AlterIndexTypes.ADDPROPS) {
-      idx.getParameters().putAll(alterIndex.getProps());
-    } else {
-      console.printError("Unsupported Alter commnad");
-      return 1;
-    }
-
-    // set last modified by properties
-    if (!updateModifiedParameters(idx.getParameters(), conf)) {
-      return 1;
-    }
-
-    try {
-      db.alterIndex(dbName, baseTableName, indexName, idx);
-    } catch (InvalidOperationException e) {
-      console.printError("Invalid alter operation: " + e.getMessage());
-      LOG.info("alter index: " + stringifyException(e));
-      return 1;
-    } catch (HiveException e) {
-      console.printError("Invalid alter operation: " + e.getMessage());
-      return 1;
-    }
-    return 0;
-  }
-
-  /**
-   * Add a partition to a table.
-   *
-   * @param db
-   *          Database to add the partition to.
-   * @param addPartitionDesc
-   *          Add this partition.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   */
-  private int addPartition(Hive db, AddPartitionDesc addPartitionDesc) throws HiveException {
-
-    Table tbl = db.getTable(addPartitionDesc.getDbName(), addPartitionDesc.getTableName());
-
-    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.ADDPARTITION);
-
-    // If the add partition was created with IF NOT EXISTS, then we should
-    // not throw an error if the specified part does exist.
-    Partition checkPart = db.getPartition(tbl, addPartitionDesc.getPartSpec(), false);
-    if (checkPart != null && addPartitionDesc.getIfNotExists()) {
-      return 0;
-    }
-
-    if (addPartitionDesc.getLocation() == null) {
-      db.createPartition(tbl, addPartitionDesc.getPartSpec());
-    } else {
-      // set partition path relative to table
-      db.createPartition(tbl, addPartitionDesc.getPartSpec(), new Path(tbl
-          .getPath(), addPartitionDesc.getLocation()));
-    }
-
-    Partition part = db
-        .getPartition(tbl, addPartitionDesc.getPartSpec(), false);
-    work.getOutputs().add(new WriteEntity(part));
-
-    return 0;
-  }
-
-  /**
-   * Rewrite the partition's metadata and force the pre/post execute hooks to
-   * be fired.
-   *
-   * @param db
-   * @param touchDesc
-   * @return
-   * @throws HiveException
-   */
-  private int touch(Hive db, AlterTableSimpleDesc touchDesc)
-      throws HiveException {
-
-    String dbName = touchDesc.getDbName();
-    String tblName = touchDesc.getTableName();
-
-    Table tbl = db.getTable(dbName, tblName);
-
-    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.TOUCH);
-
-    if (touchDesc.getPartSpec() == null) {
-      try {
-        db.alterTable(tblName, tbl);
-      } catch (InvalidOperationException e) {
-        throw new HiveException("Uable to update table");
-      }
-      work.getInputs().add(new ReadEntity(tbl));
-      work.getOutputs().add(new WriteEntity(tbl));
-    } else {
-      Partition part = db.getPartition(tbl, touchDesc.getPartSpec(), false);
-      if (part == null) {
-        throw new HiveException("Specified partition does not exist");
-      }
-      try {
-        db.alterPartition(tblName, part);
-      } catch (InvalidOperationException e) {
-        throw new HiveException(e);
-      }
-      work.getInputs().add(new ReadEntity(part));
-      work.getOutputs().add(new WriteEntity(part));
-    }
-    return 0;
-  }
-  /**
-   * Determines whether a partition has been archived
-   *
-   * @param p
-   * @return
-   */
-
-  private boolean isArchived(Partition p) {
-    Map<String, String> params = p.getParameters();
-    if ("true".equalsIgnoreCase(params.get(
-        org.apache.hadoop.hive.metastore.api.Constants.IS_ARCHIVED))) {
-      return true;
-    } else {
-      return false;
-    }
-  }
-
-  private void setIsArchived(Partition p, boolean state) {
-    Map<String, String> params = p.getParameters();
-    if (state) {
-      params.put(org.apache.hadoop.hive.metastore.api.Constants.IS_ARCHIVED,
-          "true");
-    } else {
-      params.remove(org.apache.hadoop.hive.metastore.api.Constants.IS_ARCHIVED);
-    }
-  }
-
-  private String getOriginalLocation(Partition p) {
-    Map<String, String> params = p.getParameters();
-    return params.get(
-        org.apache.hadoop.hive.metastore.api.Constants.ORIGINAL_LOCATION);
-  }
-
-  private void setOriginalLocation(Partition p, String loc) {
-    Map<String, String> params = p.getParameters();
-    if (loc == null) {
-      params.remove(org.apache.hadoop.hive.metastore.api.Constants.ORIGINAL_LOCATION);
-    } else {
-      params.put(org.apache.hadoop.hive.metastore.api.Constants.ORIGINAL_LOCATION, loc);
-    }
-  }
-
-  // Returns only the path component of the URI
-  private String getArchiveDirOnly(Path parentDir, String archiveName) {
-    URI parentUri = parentDir.toUri();
-    Path harDir = new Path(parentUri.getPath(), archiveName);
-    return harDir.toString();
-  }
-
-  /**
-   * Sets the appropriate attributes in the supplied Partition object to mark
-   * it as archived. Note that the metastore is not touched - a separate
-   * call to alter_partition is needed.
-   *
-   * @param p - the partition object to modify
-   * @param parentDir - the parent directory of the archive, which is the
-   * original directory that the partition's files resided in
-   * @param dirInArchive - the directory within the archive file that contains
-   * the partitions files
-   * @param archiveName - the name of the archive
-   * @throws URISyntaxException
-   */
-  private void setArchived(Partition p, Path parentDir, String dirInArchive, String archiveName)
-      throws URISyntaxException {
-    assert(isArchived(p) == false);
-    Map<String, String> params = p.getParameters();
-
-    URI parentUri = parentDir.toUri();
-    String parentHost = parentUri.getHost();
-    String harHost = null;
-    if (parentHost == null) {
-     harHost = "";
-    } else {
-      harHost = parentUri.getScheme() + "-" + parentHost;
-    }
-
-    // harUri is used to access the partition's files, which are in the archive
-    // The format of the RI is something like:
-    // har://underlyingfsscheme-host:port/archivepath
-    URI harUri = null;
-    if (dirInArchive.length() == 0) {
-      harUri = new URI("har", parentUri.getUserInfo(), harHost, parentUri.getPort(),
-          getArchiveDirOnly(parentDir, archiveName),
-          parentUri.getQuery(), parentUri.getFragment());
-    } else {
-      harUri = new URI("har", parentUri.getUserInfo(), harHost, parentUri.getPort(),
-          new Path(getArchiveDirOnly(parentDir, archiveName), dirInArchive).toUri().getPath(),
-          parentUri.getQuery(), parentUri.getFragment());
-    }
-    setIsArchived(p, true);
-    setOriginalLocation(p, parentDir.toString());
-    p.setLocation(harUri.toString());
-  }
-
-  /**
-   * Sets the appropriate attributes in the supplied Partition object to mark
-   * it as not archived. Note that the metastore is not touched - a separate
-   * call to alter_partition is needed.
-   *
-   * @param p - the partition to modify
-   */
-  private void setUnArchived(Partition p) {
-    assert(isArchived(p) == true);
-    String parentDir = getOriginalLocation(p);
-    setIsArchived(p, false);
-    setOriginalLocation(p, null);
-    assert(parentDir != null);
-    p.setLocation(parentDir);
-  }
-
-  private boolean pathExists(Path p) throws HiveException {
-    try {
-      FileSystem fs = p.getFileSystem(conf);
-      return fs.exists(p);
-    } catch (IOException e) {
-      throw new HiveException(e);
-    }
-  }
-
-  private void moveDir(FileSystem fs, Path from, Path to) throws HiveException {
-    try {
-      if (!fs.rename(from, to)) {
-        throw new HiveException("Moving " + from + " to " + to + " failed!");
-      }
-    } catch (IOException e) {
-      throw new HiveException(e);
-    }
-  }
-
-  private void deleteDir(Path dir) throws HiveException {
-    try {
-      Warehouse wh = new Warehouse(conf);
-      wh.deleteDir(dir, true);
-    } catch (MetaException e) {
-      throw new HiveException(e);
-    }
-  }
-
-  private int archive(Hive db, AlterTableSimpleDesc simpleDesc, DriverContext driverContext)
-      throws HiveException {
-    String dbName = simpleDesc.getDbName();
-    String tblName = simpleDesc.getTableName();
-
-    Table tbl = db.getTable(dbName, tblName);
-    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.ARCHIVE);
-
-    Map<String, String> partSpec = simpleDesc.getPartSpec();
-    Partition p = db.getPartition(tbl, partSpec, false);
-
-    if (tbl.getTableType() != TableType.MANAGED_TABLE) {
-      throw new HiveException("ARCHIVE can only be performed on managed tables");
-    }
-
-    if (p == null) {
-      throw new HiveException("Specified partition does not exist");
-    }
-
-    if (isArchived(p)) {
-      // If there were a failure right after the metadata was updated in an
-      // archiving operation, it's possible that the original, unarchived files
-      // weren't deleted.
-      Path originalDir = new Path(getOriginalLocation(p));
-      Path leftOverIntermediateOriginal = new Path(originalDir.getParent(),
-          originalDir.getName() + INTERMEDIATE_ORIGINAL_DIR_SUFFIX);
-
-      if (pathExists(leftOverIntermediateOriginal)) {
-        console.printInfo("Deleting " + leftOverIntermediateOriginal +
-        " left over from a previous archiving operation");
-        deleteDir(leftOverIntermediateOriginal);
-      }
-
-      throw new HiveException("Specified partition is already archived");
-    }
-
-    Path originalDir = p.getPartitionPath();
-    Path intermediateArchivedDir = new Path(originalDir.getParent(),
-        originalDir.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);
-    Path intermediateOriginalDir = new Path(originalDir.getParent(),
-        originalDir.getName() + INTERMEDIATE_ORIGINAL_DIR_SUFFIX);
-    String archiveName = "data.har";
-    FileSystem fs = null;
-    try {
-      fs = originalDir.getFileSystem(conf);
-    } catch (IOException e) {
-      throw new HiveException(e);
-    }
-
-    // The following steps seem roundabout, but they are meant to aid in
-    // recovery if a failure occurs and to keep a consistent state in the FS
-
-    // Steps:
-    // 1. Create the archive in a temporary folder
-    // 2. Move the archive dir to an intermediate dir that is in at the same
-    //    dir as the original partition dir. Call the new dir
-    //    intermediate-archive.
-    // 3. Rename the original partition dir to an intermediate dir. Call the
-    //    renamed dir intermediate-original
-    // 4. Rename intermediate-archive to the original partition dir
-    // 5. Change the metadata
-    // 6. Delete the original partition files in intermediate-original
-
-    // The original partition files are deleted after the metadata change
-    // because the presence of those files are used to indicate whether
-    // the original partition directory contains archived or unarchived files.
-
-    // Create an archived version of the partition in a directory ending in
-    // ARCHIVE_INTERMEDIATE_DIR_SUFFIX that's the same level as the partition,
-    // if it does not already exist. If it does exist, we assume the dir is good
-    // to use as the move operation that created it is atomic.
-    if (!pathExists(intermediateArchivedDir) &&
-        !pathExists(intermediateOriginalDir)) {
-
-      // First create the archive in a tmp dir so that if the job fails, the
-      // bad files don't pollute the filesystem
-      Path tmpDir = new Path(driverContext.getCtx().getExternalTmpFileURI(originalDir.toUri()), "partlevel");
-
-      console.printInfo("Creating " + archiveName + " for " + originalDir.toString());
-      console.printInfo("in " + tmpDir);
-      console.printInfo("Please wait... (this may take a while)");
-
-      // Create the Hadoop archive
-      HadoopShims shim = ShimLoader.getHadoopShims();
-      int ret=0;
-      try {
-        ret = shim.createHadoopArchive(conf, originalDir, tmpDir, archiveName);
-      } catch (Exception e) {
-        throw new HiveException(e);
-      }
-      if (ret != 0) {
-        throw new HiveException("Error while creating HAR");
-      }
-      // Move from the tmp dir to an intermediate directory, in the same level as
-      // the partition directory. e.g. .../hr=12-intermediate-archived
-      try {
-        console.printInfo("Moving " + tmpDir + " to " + intermediateArchivedDir);
-        if (pathExists(intermediateArchivedDir)) {
-          throw new HiveException("The intermediate archive directory already exists.");
-        }
-        fs.rename(tmpDir, intermediateArchivedDir);
-      } catch (IOException e) {
-        throw new HiveException("Error while moving tmp directory");
-      }
-    } else {
-      if (pathExists(intermediateArchivedDir)) {
-        console.printInfo("Intermediate archive directory " + intermediateArchivedDir +
-        " already exists. Assuming it contains an archived version of the partition");
-      }
-    }
-
-    // If we get to here, we know that we've archived the partition files, but
-    // they may be in the original partition location, or in the intermediate
-    // original dir.
-
-    // Move the original parent directory to the intermediate original directory
-    // if the move hasn't been made already
-    if (!pathExists(intermediateOriginalDir)) {
-      console.printInfo("Moving " + originalDir + " to " +
-          intermediateOriginalDir);
-      moveDir(fs, originalDir, intermediateOriginalDir);
-    } else {
-      console.printInfo(intermediateOriginalDir + " already exists. " +
-          "Assuming it contains the original files in the partition");
-    }
-
-    // If there's a failure from here to when the metadata is updated,
-    // there will be no data in the partition, or an error while trying to read
-    // the partition (if the archive files have been moved to the original
-    // partition directory.) But re-running the archive command will allow
-    // recovery
-
-    // Move the intermediate archived directory to the original parent directory
-    if (!pathExists(originalDir)) {
-      console.printInfo("Moving " + intermediateArchivedDir + " to " +
-          originalDir);
-      moveDir(fs, intermediateArchivedDir, originalDir);
-    } else {
-      console.printInfo(originalDir + " already exists. " +
-          "Assuming it contains the archived version of the partition");
-    }
-
-    // Record this change in the metastore
-    try {
-      boolean parentSettable =
-        conf.getBoolVar(HiveConf.ConfVars.HIVEHARPARENTDIRSETTABLE);
-
-      // dirInArchive is the directory within the archive that has all the files
-      // for this partition. With older versions of Hadoop, archiving a
-      // a directory would produce the same directory structure
-      // in the archive. So if you created myArchive.har of /tmp/myDir, the
-      // files in /tmp/myDir would be located under myArchive.har/tmp/myDir/*
-      // In this case, dirInArchive should be tmp/myDir
-
-      // With newer versions of Hadoop, the parent directory could be specified.
-      // Assuming the parent directory was set to /tmp/myDir when creating the
-      // archive, the files can be found under myArchive.har/*
-      // In this case, dirInArchive should be empty
-
-      String dirInArchive = "";
-      if (!parentSettable) {
-        dirInArchive = originalDir.toUri().getPath();
-        if(dirInArchive.length() > 1 && dirInArchive.charAt(0)=='/') {
-          dirInArchive = dirInArchive.substring(1);
-        }
-      }
-      setArchived(p, originalDir, dirInArchive, archiveName);
-      db.alterPartition(tblName, p);
-    } catch (Exception e) {
-      throw new HiveException("Unable to change the partition info for HAR", e);
-    }
-
-    // If a failure occurs here, the directory containing the original files
-    // will not be deleted. The user will run ARCHIVE again to clear this up
-    deleteDir(intermediateOriginalDir);
-
-
-    return 0;
-  }
-
-  private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)
-      throws HiveException {
-    String dbName = simpleDesc.getDbName();
-    String tblName = simpleDesc.getTableName();
-
-    Table tbl = db.getTable(dbName, tblName);
-    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.UNARCHIVE);
-
-    // Means user specified a table, not a partition
-    if (simpleDesc.getPartSpec() == null) {
-      throw new HiveException("ARCHIVE is for partitions only");
-    }
-
-    Map<String, String> partSpec = simpleDesc.getPartSpec();
-    Partition p = db.getPartition(tbl, partSpec, false);
-
-    if (tbl.getTableType() != TableType.MANAGED_TABLE) {
-      throw new HiveException("UNARCHIVE can only be performed on managed tables");
-    }
-
-    if (p == null) {
-      throw new HiveException("Specified partition does not exist");
-    }
-
-    if (!isArchived(p)) {
-      Path location = new Path(p.getLocation());
-      Path leftOverArchiveDir = new Path(location.getParent(),
-          location.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);
-
-      if (pathExists(leftOverArchiveDir)) {
-        console.printInfo("Deleting " + leftOverArchiveDir + " left over " +
-        "from a previous unarchiving operation");
-        deleteDir(leftOverArchiveDir);
-      }
-
-      throw new HiveException("Specified partition is not archived");
-    }
-
-    Path originalLocation = new Path(getOriginalLocation(p));
-    Path sourceDir = new Path(p.getLocation());
-    Path intermediateArchiveDir = new Path(originalLocation.getParent(),
-        originalLocation.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);
-    Path intermediateExtractedDir = new Path(originalLocation.getParent(),
-        originalLocation.getName() + INTERMEDIATE_EXTRACTED_DIR_SUFFIX);
-
-    Path tmpDir = new Path(driverContext
-          .getCtx()
-          .getExternalTmpFileURI(originalLocation.toUri()));
-
-    FileSystem fs = null;
-    try {
-      fs = tmpDir.getFileSystem(conf);
-      // Verify that there are no files in the tmp dir, because if there are, it
-      // would be copied to the partition
-      FileStatus [] filesInTmpDir = fs.listStatus(tmpDir);
-      if (filesInTmpDir != null && filesInTmpDir.length != 0) {
-        for (FileStatus file : filesInTmpDir) {
-          console.printInfo(file.getPath().toString());
-        }
-        throw new HiveException("Temporary directory " + tmpDir + " is not empty");
-      }
-
-    } catch (IOException e) {
-      throw new HiveException(e);
-    }
-
-    // Some sanity checks
-    if (originalLocation == null) {
-      throw new HiveException("Missing archive data in the partition");
-    }
-    if (!"har".equals(sourceDir.toUri().getScheme())) {
-      throw new HiveException("Location should refer to a HAR");
-    }
-
-    // Clarification of terms:
-    // - The originalLocation directory represents the original directory of the
-    //   partition's files. They now contain an archived version of those files
-    //   eg. hdfs:/warehouse/myTable/ds=1/
-    // - The source directory is the directory containing all the files that
-    //   should be in the partition. e.g. har:/warehouse/myTable/ds=1/myTable.har/
-    //   Note the har:/ scheme
-
-    // Steps:
-    // 1. Extract the archive in a temporary folder
-    // 2. Move the archive dir to an intermediate dir that is in at the same
-    //    dir as originalLocation. Call the new dir intermediate-extracted.
-    // 3. Rename the original partition dir to an intermediate dir. Call the
-    //    renamed dir intermediate-archive
-    // 4. Rename intermediate-extracted to the original partition dir
-    // 5. Change the metadata
-    // 6. Delete the archived partition files in intermediate-archive
-
-    if (!pathExists(intermediateExtractedDir) &&
-        !pathExists(intermediateArchiveDir)) {
-      try {
-
-        // Copy the files out of the archive into the temporary directory
-        String copySource = (new Path(sourceDir, "*")).toString();
-        String copyDest = tmpDir.toString();
-        List<String> args = new ArrayList<String>();
-        args.add("-cp");
-        args.add(copySource);
-        args.add(copyDest);
-
-        console.printInfo("Copying " + copySource + " to " + copyDest);
-        FsShell fss = new FsShell(conf);
-        int ret = 0;
-        try {
-          ret = ToolRunner.run(fss, args.toArray(new String[0]));
-        } catch (Exception e) {
-          throw new HiveException(e);
-        }
-        if (ret != 0) {
-          throw new HiveException("Error while copying files from archive");
-        }
-
-        console.printInfo("Moving " + tmpDir + " to " + intermediateExtractedDir);
-        if (fs.exists(intermediateExtractedDir)) {
-          throw new HiveException("Invalid state: the intermediate extracted " +
-              "directory already exists.");
-        }
-        fs.rename(tmpDir, intermediateExtractedDir);
-      } catch (Exception e) {
-        throw new HiveException(e);
-      }
-    }
-
-    // At this point, we know that the extracted files are in the intermediate
-    // extracted dir, or in the the original directory.
-
-    if (!pathExists(intermediateArchiveDir)) {
-      try {
-        console.printInfo("Moving " + originalLocation + " to " + intermediateArchiveDir);
-        fs.rename(originalLocation, intermediateArchiveDir);
-      } catch (IOException e) {
-        throw new HiveException(e);
-      }
-    } else {
-      console.printInfo(intermediateArchiveDir + " already exists. " +
-      "Assuming it contains the archived version of the partition");
-    }
-
-    // If there is a failure from here to until when the metadata is changed,
-    // the partition will be empty or throw errors on read.
-
-    // If the original location exists here, then it must be the extracted files
-    // because in the previous step, we moved the previous original location
-    // (containing the archived version of the files) to intermediateArchiveDir
-    if (!pathExists(originalLocation)) {
-      try {
-        console.printInfo("Moving " + intermediateExtractedDir + " to " + originalLocation);
-        fs.rename(intermediateExtractedDir, originalLocation);
-      } catch (IOException e) {
-        throw new HiveException(e);
-      }
-    } else {
-      console.printInfo(originalLocation + " already exists. " +
-      "Assuming it contains the extracted files in the partition");
-    }
-
-    setUnArchived(p);
-    try {
-      db.alterPartition(tblName, p);
-    } catch (InvalidOperationException e) {
-      throw new HiveException(e);
-    }
-    // If a failure happens here, the intermediate archive files won't be
-    // deleted. The user will need to call unarchive again to clear those up.
-    deleteDir(intermediateArchiveDir);
-
-    return 0;
-  }
-
-  private void validateAlterTableType(
-    Table tbl, AlterTableDesc.AlterTableTypes alterType)  throws HiveException {
-
-    if (tbl.isView()) {
-      switch (alterType) {
-      case ADDPROPS:
-        // allow this form
-        break;
-      default:
-        throw new HiveException(
-          "Cannot use this form of ALTER TABLE on a view");
-      }
-    }
-
-    if (tbl.isNonNative()) {
-      throw new HiveException("Cannot use ALTER TABLE on a non-native table");
-    }
-  }
-
-  /**
-   * MetastoreCheck, see if the data in the metastore matches what is on the
-   * dfs. Current version checks for tables and partitions that are either
-   * missing on disk on in the metastore.
-   *
-   * @param db
-   *          The database in question.
-   * @param msckDesc
-   *          Information about the tables and partitions we want to check for.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   */
-  private int msck(Hive db, MsckDesc msckDesc) {
-    CheckResult result = new CheckResult();
-    List<String> repairOutput = new ArrayList<String>();
-    try {
-      HiveMetaStoreChecker checker = new HiveMetaStoreChecker(db);
-      checker.checkMetastore(db.getCurrentDatabase(), msckDesc
-          .getTableName(), msckDesc.getPartSpecs(), result);
-      if (msckDesc.isRepairPartitions()) {
-        Table table = db.getTable(msckDesc.getTableName());
-        for (CheckResult.PartitionResult part : result.getPartitionsNotInMs()) {
-          try {
-            db.createPartition(table, Warehouse.makeSpecFromName(part
-                .getPartitionName()));
-            repairOutput.add("Repair: Added partition to metastore "
-                + msckDesc.getTableName() + ':' + part.getPartitionName());
-          } catch (Exception e) {
-            LOG.warn("Repair error, could not add partition to metastore: ", e);
-          }
-        }
-      }
-    } catch (HiveException e) {
-      LOG.warn("Failed to run metacheck: ", e);
-      return 1;
-    } catch (IOException e) {
-      LOG.warn("Failed to run metacheck: ", e);
-      return 1;
-    } finally {
-      BufferedWriter resultOut = null;
-      try {
-        Path resFile = new Path(msckDesc.getResFile());
-        FileSystem fs = resFile.getFileSystem(conf);
-        resultOut = new BufferedWriter(new OutputStreamWriter(fs
-            .create(resFile)));
-
-        boolean firstWritten = false;
-        firstWritten |= writeMsckResult(result.getTablesNotInMs(),
-            "Tables not in metastore:", resultOut, firstWritten);
-        firstWritten |= writeMsckResult(result.getTablesNotOnFs(),
-            "Tables missing on filesystem:", resultOut, firstWritten);
-        firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),
-            "Partitions not in metastore:", resultOut, firstWritten);
-        firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),
-            "Partitions missing from filesystem:", resultOut, firstWritten);
-        for (String rout : repairOutput) {
-          if (firstWritten) {
-            resultOut.write(terminator);
-          } else {
-            firstWritten = true;
-          }
-          resultOut.write(rout);
-        }
-      } catch (IOException e) {
-        LOG.warn("Failed to save metacheck output: ", e);
-        return 1;
-      } finally {
-        if (resultOut != null) {
-          try {
-            resultOut.close();
-          } catch (IOException e) {
-            LOG.warn("Failed to close output file: ", e);
-            return 1;
-          }
-        }
-      }
-    }
-
-    return 0;
-  }
-
-  /**
-   * Write the result of msck to a writer.
-   *
-   * @param result
-   *          The result we're going to write
-   * @param msg
-   *          Message to write.
-   * @param out
-   *          Writer to write to
-   * @param wrote
-   *          if any previous call wrote data
-   * @return true if something was written
-   * @throws IOException
-   *           In case the writing fails
-   */
-  private boolean writeMsckResult(List<? extends Object> result, String msg,
-      Writer out, boolean wrote) throws IOException {
-
-    if (!result.isEmpty()) {
-      if (wrote) {
-        out.write(terminator);
-      }
-
-      out.write(msg);
-      for (Object entry : result) {
-        out.write(separator);
-        out.write(entry.toString());
-      }
-      return true;
-    }
-
-    return false;
-  }
-
-  /**
-   * Write a list of partitions to a file.
-   *
-   * @param db
-   *          The database in question.
-   * @param showParts
-   *          These are the partitions we're interested in.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int showPartitions(Hive db, ShowPartitionsDesc showParts) throws HiveException {
-    // get the partitions for the table and populate the output
-    String tabName = showParts.getTabName();
-    Table tbl = null;
-    List<String> parts = null;
-
-    tbl = db.getTable(tabName);
-
-    if (!tbl.isPartitioned()) {
-      console.printError("Table " + tabName + " is not a partitioned table");
-      return 1;
-    }
-    if (showParts.getPartSpec() != null) {
-      parts = db.getPartitionNames(db.getCurrentDatabase(),
-          tbl.getTableName(), showParts.getPartSpec(), (short) -1);
-    } else {
-      parts = db.getPartitionNames(db.getCurrentDatabase(), tbl.getTableName(), (short) -1);
-    }
-
-    // write the results in the file
-    try {
-      Path resFile = new Path(showParts.getResFile());
-      FileSystem fs = resFile.getFileSystem(conf);
-      DataOutput outStream = fs.create(resFile);
-      Iterator<String> iterParts = parts.iterator();
-
-      while (iterParts.hasNext()) {
-        // create a row per partition name
-        outStream.writeBytes(iterParts.next());
-        outStream.write(terminator);
-      }
-      ((FSDataOutputStream) outStream).close();
-    } catch (FileNotFoundException e) {
-      LOG.info("show partitions: " + stringifyException(e));
-      throw new HiveException(e.toString());
-    } catch (IOException e) {
-      LOG.info("show partitions: " + stringifyException(e));
-      throw new HiveException(e.toString());
-    } catch (Exception e) {
-      throw new HiveException(e.toString());
-    }
-
-    return 0;
-  }
-
-  /**
-   * Write a list of indexes to a file.
-   *
-   * @param db
-   *          The database in question.
-   * @param showIndexes
-   *          These are the indexes we're interested in.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int showIndexes(Hive db, ShowIndexesDesc showIndexes) throws HiveException {
-    // get the indexes for the table and populate the output
-    String tableName = showIndexes.getTableName();
-    Table tbl = null;
-    List<Index> indexes = null;
-
-    tbl = db.getTable(tableName);
-
-    indexes = db.getIndexes(db.getCurrentDatabase(), tbl.getTableName(), (short) -1);
-
-    // write the results in the file
-    try {
-      Path resFile = new Path(showIndexes.getResFile());
-      FileSystem fs = resFile.getFileSystem(conf);
-      DataOutput outStream = fs.create(resFile);
-
-      if (showIndexes.isFormatted()) {
-        // column headers
-        outStream.writeBytes(MetaDataFormatUtils.getIndexColumnsHeader());
-        outStream.write(terminator);
-        outStream.write(terminator);
-      }
-
-      for (Index index : indexes)
-      {
-        outStream.writeBytes(MetaDataFormatUtils.getAllColumnsInformation(index));
-      }
-
-      ((FSDataOutputStream) outStream).close();
-
-    } catch (FileNotFoundException e) {
-      LOG.info("show indexes: " + stringifyException(e));
-      throw new HiveException(e.toString());
-    } catch (IOException e) {
-      LOG.info("show indexes: " + stringifyException(e));
-      throw new HiveException(e.toString());
-    } catch (Exception e) {
-      throw new HiveException(e.toString());
-    }
-
-    return 0;
-  }
-
-  /**
-   * Write a list of the available databases to a file.
-   *
-   * @param showDatabases
-   *          These are the databases we're interested in.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int showDatabases(Hive db, ShowDatabasesDesc showDatabasesDesc) throws HiveException {
-    // get the databases for the desired pattern - populate the output stream
-    List<String> databases = null;
-    if (showDatabasesDesc.getPattern() != null) {
-      LOG.info("pattern: " + showDatabasesDesc.getPattern());
-      databases = db.getDatabasesByPattern(showDatabasesDesc.getPattern());
-    } else {
-      databases = db.getAllDatabases();
-    }
-    LOG.info("results : " + databases.size());
-
-    // write the results in the file
-    try {
-      Path resFile = new Path(showDatabasesDesc.getResFile());
-      FileSystem fs = resFile.getFileSystem(conf);
-      DataOutput outStream = fs.create(resFile);
-
-      for (String database : databases) {
-        // create a row per database name
-        outStream.writeBytes(database);
-        outStream.write(terminator);
-      }
-      ((FSDataOutputStream) outStream).close();
-    } catch (FileNotFoundException e) {
-      LOG.warn("show databases: " + stringifyException(e));
-      return 1;
-    } catch (IOException e) {
-      LOG.warn("show databases: " + stringifyException(e));
-      return 1;
-    } catch (Exception e) {
-      throw new HiveException(e.toString());
-    }
-    return 0;
-  }
-
-  /**
-   * Write a list of the tables in the database to a file.
-   *
-   * @param db
-   *          The database in question.
-   * @param showTbls
-   *          These are the tables we're interested in.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int showTables(Hive db, ShowTablesDesc showTbls) throws HiveException {
-    // get the tables for the desired pattenn - populate the output stream
-    List<String> tbls = null;
-    if (showTbls.getPattern() != null) {
-      LOG.info("pattern: " + showTbls.getPattern());
-      tbls = db.getTablesByPattern(showTbls.getPattern());
-      LOG.info("results : " + tbls.size());
-    } else {
-      tbls = db.getAllTables();
-    }
-
-    // write the results in the file
-    try {
-      Path resFile = new Path(showTbls.getResFile());
-      FileSystem fs = resFile.getFileSystem(conf);
-      DataOutput outStream = fs.create(resFile);
-      SortedSet<String> sortedTbls = new TreeSet<String>(tbls);
-      Iterator<String> iterTbls = sortedTbls.iterator();
-
-      while (iterTbls.hasNext()) {
-        // create a row per table name
-        outStream.writeBytes(iterTbls.next());
-        outStream.write(terminator);
-      }
-      ((FSDataOutputStream) outStream).close();
-    } catch (FileNotFoundException e) {
-      LOG.warn("show table: " + stringifyException(e));
-      return 1;
-    } catch (IOException e) {
-      LOG.warn("show table: " + stringifyException(e));
-      return 1;
-    } catch (Exception e) {
-      throw new HiveException(e.toString());
-    }
-    return 0;
-  }
-
-  /**
-   * Write a list of the user defined functions to a file.
-   *
-   * @param showFuncs
-   *          are the functions we're interested in.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int showFunctions(ShowFunctionsDesc showFuncs) throws HiveException {
-    // get the tables for the desired pattenn - populate the output stream
-    Set<String> funcs = null;
-    if (showFuncs.getPattern() != null) {
-      LOG.info("pattern: " + showFuncs.getPattern());
-      funcs = FunctionRegistry.getFunctionNames(showFuncs.getPattern());
-      LOG.info("results : " + funcs.size());
-    } else {
-      funcs = FunctionRegistry.getFunctionNames();
-    }
-
-    // write the results in the file
-    try {
-      Path resFile = new Path(showFuncs.getResFile());
-      FileSystem fs = resFile.getFileSystem(conf);
-      DataOutput outStream = fs.create(resFile);
-      SortedSet<String> sortedFuncs = new TreeSet<String>(funcs);
-      Iterator<String> iterFuncs = sortedFuncs.iterator();
-
-      while (iterFuncs.hasNext()) {
-        // create a row per table name
-        outStream.writeBytes(iterFuncs.next());
-        outStream.write(terminator);
-      }
-      ((FSDataOutputStream) outStream).close();
-    } catch (FileNotFoundException e) {
-      LOG.warn("show function: " + stringifyException(e));
-      return 1;
-    } catch (IOException e) {
-      LOG.warn("show function: " + stringifyException(e));
-      return 1;
-    } catch (Exception e) {
-      throw new HiveException(e.toString());
-    }
-    return 0;
-  }
-
-  /**
-   * Write a list of the current locks to a file.
-   *
-   * @param showLocks
-   *          the locks we're interested in.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int showLocks(ShowLocksDesc showLocks) throws HiveException {
-    Context ctx = driverContext.getCtx();
-    HiveLockManager lockMgr = ctx.getHiveLockMgr();
-    boolean isExt = showLocks.isExt();
-    if (lockMgr == null) {
-      throw new HiveException("show Locks LockManager not specified");
-    }
-
-    // write the results in the file
-    try {
-      Path resFile = new Path(showLocks.getResFile());
-      FileSystem fs = resFile.getFileSystem(conf);
-      DataOutput outStream = fs.create(resFile);
-      List<HiveLock> locks = null;
-
-      if (showLocks.getTableName() == null) {
-        locks = lockMgr.getLocks(false, isExt);
-      }
-      else {
-        locks = lockMgr.getLocks(getHiveObject(showLocks.getTableName(),
-                                               showLocks.getPartSpec()),
-                                 true, isExt);
-      }
-
-      Collections.sort(locks, new Comparator<HiveLock>() {
-
-          @Override
-            public int compare(HiveLock o1, HiveLock o2) {
-            int cmp = o1.getHiveLockObject().getName().compareTo(o2.getHiveLockObject().getName());
-            if (cmp == 0) {
-              if (o1.getHiveLockMode() == o2.getHiveLockMode()) {
-                return cmp;
-              }
-              // EXCLUSIVE locks occur before SHARED locks
-              if (o1.getHiveLockMode() == HiveLockMode.EXCLUSIVE) {
-                return -1;
-              }
-              return +1;
-            }
-            return cmp;
-          }
-
-        });
-
-      Iterator<HiveLock> locksIter = locks.iterator();
-
-      while (locksIter.hasNext()) {
-        HiveLock lock = locksIter.next();
-        outStream.writeBytes(lock.getHiveLockObject().getName());
-        outStream.write(separator);
-        outStream.writeBytes(lock.getHiveLockMode().toString());
-        if (isExt) {
-          outStream.write(terminator);
-          HiveLockObjectData lockData = lock.getHiveLockObject().getData();
-          if (lockData != null) {
-            outStream.writeBytes("LOCK_QUERYID:" + lockData.getQueryId() + " ");
-            outStream.writeBytes("LOCK_TIME:" + lockData.getLockTime() + " ");
-            outStream.writeBytes("LOCK_MODE:" + lockData.getLockMode() + " ");
-          }
-        }
-        outStream.write(terminator);
-      }
-      ((FSDataOutputStream) outStream).close();
-    } catch (FileNotFoundException e) {
-      LOG.warn("show function: " + stringifyException(e));
-      return 1;
-    } catch (IOException e) {
-      LOG.warn("show function: " + stringifyException(e));
-      return 1;
-    } catch (Exception e) {
-      throw new HiveException(e.toString());
-    }
-    return 0;
-  }
-
-  /**
-   * Lock the table/partition specified
-   *
-   * @param lockTbl
-   *          the table/partition to be locked along with the mode
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int lockTable(LockTableDesc lockTbl) throws HiveException {
-    Context ctx = driverContext.getCtx();
-    HiveLockManager lockMgr = ctx.getHiveLockMgr();
-    if (lockMgr == null) {
-      throw new HiveException("lock Table LockManager not specified");
-    }
-
-    HiveLockMode mode = HiveLockMode.valueOf(lockTbl.getMode());
-    String tabName = lockTbl.getTableName();
-    Table  tbl = db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, tabName);
-    if (tbl == null) {
-      throw new HiveException("Table " + tabName + " does not exist ");
-    }
-
-    Map<String, String> partSpec = lockTbl.getPartSpec();
-    HiveLockObjectData lockData =
-      new HiveLockObjectData(lockTbl.getQueryId(),
-                             String.valueOf(System.currentTimeMillis()),
-                             "EXPLICIT");
-
-    if (partSpec == null) {
-      HiveLock lck = lockMgr.lock(new HiveLockObject(tbl, lockData), mode, true, 0, 0);
-      if (lck == null) {
-        return 1;
-      }
-      return 0;
-    }
-
-    Partition par = db.getPartition(tbl, partSpec, false);
-    if (par == null) {
-      throw new HiveException("Partition " + partSpec + " for table " + tabName + " does not exist");
-    }
-    HiveLock lck = lockMgr.lock(new HiveLockObject(par, lockData), mode, true, 0, 0);
-    if (lck == null) {
-      return 1;
-    }
-    return 0;
-  }
-
-  private HiveLockObject getHiveObject(String tabName,
-                                       Map<String, String> partSpec) throws HiveException {
-    Table  tbl = db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, tabName);
-    if (tbl == null) {
-      throw new HiveException("Table " + tabName + " does not exist ");
-    }
-
-    HiveLockObject obj = null;
-
-    if  (partSpec == null) {
-      obj = new HiveLockObject(tbl, null);
-    }
-    else {
-      Partition par = db.getPartition(tbl, partSpec, false);
-      if (par == null) {
-        throw new HiveException("Partition " + partSpec + " for table " + tabName + " does not exist");
-      }
-      obj = new HiveLockObject(par, null);
-    }
-    return obj;
-  }
-
-  /**
-   * Unlock the table/partition specified
-   *
-   * @param unlockTbl
-   *          the table/partition to be unlocked
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int unlockTable(UnlockTableDesc unlockTbl) throws HiveException {
-    Context ctx = driverContext.getCtx();
-    HiveLockManager lockMgr = ctx.getHiveLockMgr();
-    if (lockMgr == null) {
-      throw new HiveException("unlock Table LockManager not specified");
-    }
-
-    String tabName = unlockTbl.getTableName();
-    HiveLockObject obj = getHiveObject(tabName, unlockTbl.getPartSpec());
-
-    List<HiveLock> locks = lockMgr.getLocks(obj, false, false);
-    if ((locks == null) || (locks.isEmpty())) {
-      throw new HiveException("Table " + tabName + " is not locked ");
-    }
-    Iterator<HiveLock> locksIter = locks.iterator();
-    while (locksIter.hasNext()) {
-      HiveLock lock = locksIter.next();
-      lockMgr.unlock(lock);
-    }
-
-    return 0;
-  }
-
-  /**
-   * Shows a description of a function.
-   *
-   * @param descFunc
-   *          is the function we are describing
-   * @throws HiveException
-   */
-  private int describeFunction(DescFunctionDesc descFunc) throws HiveException {
-    String funcName = descFunc.getName();
-
-    // write the results in the file
-    try {
-      Path resFile = new Path(descFunc.getResFile());
-      FileSystem fs = resFile.getFileSystem(conf);
-      DataOutput outStream = fs.create(resFile);
-
-      // get the function documentation
-      Description desc = null;
-      Class<?> funcClass = null;
-      FunctionInfo functionInfo = FunctionRegistry.getFunctionInfo(funcName);
-      if (functionInfo != null) {
-        funcClass = functionInfo.getFunctionClass();
-      }
-      if (funcClass != null) {
-        desc = funcClass.getAnnotation(Description.class);
-      }
-      if (desc != null) {
-        outStream.writeBytes(desc.value().replace("_FUNC_", funcName));
-        if (descFunc.isExtended()) {
-          Set<String> synonyms = FunctionRegistry.getFunctionSynonyms(funcName);
-          if (synonyms.size() > 0) {
-            outStream.writeBytes("\nSynonyms: " + join(synonyms, ", "));
-          }
-          if (desc.extended().length() > 0) {
-            outStream.writeBytes("\n"
-                + desc.extended().replace("_FUNC_", funcName));
-          }
-        }
-      } else {
-        if (funcClass != null) {
-          outStream.writeBytes("There is no documentation for function '"
-              + funcName + "'");
-        } else {
-          outStream.writeBytes("Function '" + funcName + "' does not exist.");
-        }
-      }
-
-      outStream.write(terminator);
-
-      ((FSDataOutputStream) outStream).close();
-    } catch (FileNotFoundException e) {
-      LOG.warn("describe function: " + stringifyException(e));
-      return 1;
-    } catch (IOException e) {
-      LOG.warn("describe function: " + stringifyException(e));
-      return 1;
-    } catch (Exception e) {
-      throw new HiveException(e.toString());
-    }
-    return 0;
-  }
-
-  private int descDatabase(DescDatabaseDesc descDatabase) throws HiveException {
-    try {
-      Path resFile = new Path(descDatabase.getResFile());
-      FileSystem fs = resFile.getFileSystem(conf);
-      DataOutput outStream = fs.create(resFile);
-
-      Database database = db.getDatabase(descDatabase.getDatabaseName());
-
-      if (database != null) {
-        outStream.writeBytes(database.getName());
-        outStream.write(separator);
-        if (database.getDescription() != null) {
-          outStream.writeBytes(database.getDescription());
-        }
-        outStream.write(separator);
-        if (database.getLocationUri() != null) {
-          outStream.writeBytes(database.getLocationUri());
-        }
-
-        outStream.write(separator);
-        if (descDatabase.isExt() && database.getParametersSize() > 0) {
-          Map<String, String> params = database.getParameters();
-          outStream.writeBytes(params.toString());
-        }
-
-    	} else {
-    	  outStream.writeBytes("No such database: " + descDatabase.getDatabaseName());
-    	}
-
-      outStream.write(terminator);
-
-      ((FSDataOutputStream) outStream).close();
-
-    } catch (FileNotFoundException e) {
-      LOG.warn("describe database: " + stringifyException(e));
-      return 1;
-    } catch (IOException e) {
-      LOG.warn("describe database: " + stringifyException(e));
-      return 1;
-    } catch (Exception e) {
-      throw new HiveException(e.toString());
-    }
-    return 0;
-  }
-
-  /**
-   * Write the status of tables to a file.
-   *
-   * @param db
-   *          The database in question.
-   * @param showTblStatus
-   *          tables we are interested in
-   * @return Return 0 when execution succeeds and above 0 if it fails.
-   */
-  private int showTableStatus(Hive db, ShowTableStatusDesc showTblStatus) throws HiveException {
-    // get the tables for the desired pattenn - populate the output stream
-    List<Table> tbls = new ArrayList<Table>();
-    Map<String, String> part = showTblStatus.getPartSpec();
-    Partition par = null;
-    if (part != null) {
-      Table tbl = db.getTable(showTblStatus.getDbName(), showTblStatus.getPattern());
-      par = db.getPartition(tbl, part, false);
-      if (par == null) {
-        throw new HiveException("Partition " + part + " for table "
-            + showTblStatus.getPattern() + " does not exist.");
-      }
-      tbls.add(tbl);
-    } else {
-      LOG.info("pattern: " + showTblStatus.getPattern());
-      List<String> tblStr = db.getTablesForDb(showTblStatus.getDbName(),
-          showTblStatus.getPattern());
-      SortedSet<String> sortedTbls = new TreeSet<String>(tblStr);
-      Iterator<String> iterTbls = sortedTbls.iterator();
-      while (iterTbls.hasNext()) {
-        // create a row per table name
-        String tblName = iterTbls.next();
-        Table tbl = db.getTable(showTblStatus.getDbName(), tblName);
-        tbls.add(tbl);
-      }
-      LOG.info("results : " + tblStr.size());
-    }
-
-    // write the results in the file
-    try {
-      Path resFile = new Path(showTblStatus.getResFile());
-      FileSystem fs = resFile.getFileSystem(conf);
-      DataOutput outStream = fs.create(resFile);
-
-      Iterator<Table> iterTables = tbls.iterator();
-      while (iterTables.hasNext()) {
-        // create a row per table name
-        Table tbl = iterTables.next();
-        String tableName = tbl.getTableName();
-        String tblLoc = null;
-        String inputFormattCls = null;
-        String outputFormattCls = null;
-        if (part != null) {
-          if (par != null) {
-            tblLoc = par.getDataLocation().toString();
-            inputFormattCls = par.getInputFormatClass().getName();
-            outputFormattCls = par.getOutputFormatClass().getName();
-          }
-        } else {
-          tblLoc = tbl.getDataLocation().toString();
-          inputFormattCls = tbl.getInputFormatClass().getName();
-          outputFormattCls = tbl.getOutputFormatClass().getName();
-        }
-
-        String owner = tbl.getOwner();
-        List<FieldSchema> cols = tbl.getCols();
-        String ddlCols = MetaStoreUtils.getDDLFromFieldSchema("columns", cols);
-        boolean isPartitioned = tbl.isPartitioned();
-        String partitionCols = "";
-        if (isPartitioned) {
-          partitionCols = MetaStoreUtils.getDDLFromFieldSchema(
-              "partition_columns", tbl.getPartCols());
-        }
-
-        outStream.writeBytes("tableName:" + tableName);
-        outStream.write(terminator);
-        outStream.writeBytes("owner:" + owner);
-        outStream.write(terminator);
-        outStream.writeBytes("location:" + tblLoc);
-        outStream.write(terminator);
-        outStream.writeBytes("inputformat:" + inputFormattCls);
-        outStream.write(terminator);
-        outStream.writeBytes("outputformat:" + outputFormattCls);
-        outStream.write(terminator);
-        outStream.writeBytes("columns:" + ddlCols);
-        outStream.write(terminator);
-        outStream.writeBytes("partitioned:" + isPartitioned);
-        outStream.write(terminator);
-        outStream.writeBytes("partitionColumns:" + partitionCols);
-        outStream.write(terminator);
-        // output file system information
-        Path tablLoc = tbl.getPath();
-        List<Path> locations = new ArrayList<Path>();
-        if (isPartitioned) {
-          if (par == null) {
-            for (Partition curPart : db.getPartitions(tbl)) {
-              locations.add(new Path(curPart.getTPartition().getSd()
-                  .getLocation()));
-            }
-          } else {
-            locations.add(new Path(par.getTPartition().getSd().getLocation()));
-          }
-        } else {
-          locations.add(tablLoc);
-        }
-        writeFileSystemStats(outStream, locations, tablLoc, false, 0);
-
-        outStream.write(terminator);
-      }
-      ((FSDataOutputStream) outStream).close();
-    } catch (FileNotFoundException e) {
-      LOG.info("show table status: " + stringifyException(e));
-      return 1;
-    } catch (IOException e) {
-      LOG.info("show table status: " + stringifyException(e));
-      return 1;
-    } catch (Exception e) {
-      throw new HiveException(e);
-    }
-    return 0;
-  }
-
-  /**
-   * Write the description of a table to a file.
-   *
-   * @param db
-   *          The database in question.
-   * @param descTbl
-   *          This is the table we're interested in.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int describeTable(Hive db, DescTableDesc descTbl) throws HiveException {
-    String colPath = descTbl.getTableName();
-    String tableName = colPath.substring(0,
-        colPath.indexOf('.') == -1 ? colPath.length() : colPath.indexOf('.'));
-
-    // describe the table - populate the output stream
-    Table tbl = db.getTable(db.getCurrentDatabase(), tableName, false);
-    Partition part = null;
-    try {
-      Path resFile = new Path(descTbl.getResFile());
-      if (tbl == null) {
-        FileSystem fs = resFile.getFileSystem(conf);
-        DataOutput outStream = (DataOutput) fs.open(resFile);
-        String errMsg = "Table " + tableName + " does not exist";
-        outStream.write(errMsg.getBytes("UTF-8"));
-        ((FSDataOutputStream) outStream).close();
-        return 0;
-      }
-      if (descTbl.getPartSpec() != null) {
-        part = db.getPartition(tbl, descTbl.getPartSpec(), false);
-        if (part == null) {
-          FileSystem fs = resFile.getFileSystem(conf);
-          DataOutput outStream = (DataOutput) fs.open(resFile);
-          String errMsg = "Partition " + descTbl.getPartSpec() + " for table "
-              + tableName + " does not exist";
-          outStream.write(errMsg.getBytes("UTF-8"));
-          ((FSDataOutputStream) outStream).close();
-          return 0;
-        }
-        tbl = part.getTable();
-      }
-    } catch (FileNotFoundException e) {
-      LOG.info("describe table: " + stringifyException(e));
-      return 1;
-    } catch (IOException e) {
-      LOG.info("describe table: " + stringifyException(e));
-      return 1;
-    }
-
-    try {
-
-      LOG.info("DDLTask: got data for " + tbl.getTableName());
-
-			Path resFile = new Path(descTbl.getResFile());
-			FileSystem fs = resFile.getFileSystem(conf);
-			DataOutput outStream = fs.create(resFile);
-
-      if (colPath.equals(tableName)) {
-        if (!descTbl.isFormatted()) {
-          List<FieldSchema> cols = tbl.getCols();
-          if (tableName.equals(colPath)) {
-            cols.addAll(tbl.getPartCols());
-          }
-          outStream.writeBytes(MetaDataFormatUtils.displayColsUnformatted(cols));
-        } else {
-          outStream.writeBytes(MetaDataFormatUtils.getAllColumnsInformation(tbl));
-        }
-      } else {
-        List<FieldSchema> cols = Hive.getFieldsFromDeserializer(colPath, tbl.getDeserializer());
-        if (descTbl.isFormatted()) {
-          outStream.writeBytes(MetaDataFormatUtils.getAllColumnsInformation(cols));
-        } else {
-          outStream.writeBytes(MetaDataFormatUtils.displayColsUnformatted(cols));
-        }
-      }
-
-      if (tableName.equals(colPath)) {
-
-        if (descTbl.isFormatted()) {
-          if (part != null) {
-            outStream.writeBytes(MetaDataFormatUtils.getPartitionInformation(part));
-          } else {
-            outStream.writeBytes(MetaDataFormatUtils.getTableInformation(tbl));
-          }
-        }
-
-        // if extended desc table then show the complete details of the table
-        if (descTbl.isExt()) {
-          // add empty line
-          outStream.write(terminator);
-          if (part != null) {
-            // show partition information
-            outStream.writeBytes("Detailed Partition Information");
-            outStream.write(separator);
-            outStream.writeBytes(part.getTPartition().toString());
-            outStream.write(separator);
-            // comment column is empty
-            outStream.write(terminator);
-          } else {
-            // show table information
-            outStream.writeBytes("Detailed Table Information");
-            outStream.write(separator);
-            outStream.writeBytes(tbl.getTTable().toString());
-            outStream.write(separator);
-            outStream.write(terminator);
-          }
-        }
-      }
-
-      LOG.info("DDLTask: written data for " + tbl.getTableName());
-      ((FSDataOutputStream) outStream).close();
-
-    } catch (FileNotFoundException e) {
-      LOG.info("describe table: " + stringifyException(e));
-      return 1;
-    } catch (IOException e) {
-      LOG.info("describe table: " + stringifyException(e));
-      return 1;
-    } catch (Exception e) {
-      throw new HiveException(e);
-    }
-
-    return 0;
-  }
-
-  private void writeFileSystemStats(DataOutput outStream, List<Path> locations,
-      Path tabLoc, boolean partSpecified, int indent) throws IOException {
-    long totalFileSize = 0;
-    long maxFileSize = 0;
-    long minFileSize = Long.MAX_VALUE;
-    long lastAccessTime = 0;
-    long lastUpdateTime = 0;
-    int numOfFiles = 0;
-
-    boolean unknown = false;
-    FileSystem fs = tabLoc.getFileSystem(conf);
-    // in case all files in locations do not exist
-    try {
-      FileStatus tmpStatus = fs.getFileStatus(tabLoc);
-      lastAccessTime = ShimLoader.getHadoopShims().getAccessTime(tmpStatus);
-      lastUpdateTime = tmpStatus.getModificationTime();
-      if (partSpecified) {
-        // check whether the part exists or not in fs
-        tmpStatus = fs.getFileStatus(locations.get(0));
-      }
-    } catch (IOException e) {
-      LOG.warn(
-          "Cannot access File System. File System status will be unknown: ", e);
-      unknown = true;
-    }
-
-    if (!unknown) {
-      for (Path loc : locations) {
-        try {
-          FileStatus status = fs.getFileStatus(tabLoc);
-          FileStatus[] files = fs.listStatus(loc);
-          long accessTime = ShimLoader.getHadoopShims().getAccessTime(status);
-          long updateTime = status.getModificationTime();
-          // no matter loc is the table location or part location, it must be a
-          // directory.
-          if (!status.isDir()) {
-            continue;
-          }
-          if (accessTime > lastAccessTime) {
-            lastAccessTime = accessTime;
-          }
-          if (updateTime > lastUpdateTime) {
-            lastUpdateTime = updateTime;
-          }
-          for (FileStatus currentStatus : files) {
-            if (currentStatus.isDir()) {
-              continue;
-            }
-            numOfFiles++;
-            long fileLen = currentStatus.getLen();
-            totalFileSize += fileLen;
-            if (fileLen > maxFileSize) {
-              maxFileSize = fileLen;
-            }
-            if (fileLen < minFileSize) {
-              minFileSize = fileLen;
-            }
-            accessTime = ShimLoader.getHadoopShims().getAccessTime(
-                currentStatus);
-            updateTime = currentStatus.getModificationTime();
-            if (accessTime > lastAccessTime) {
-              lastAccessTime = accessTime;
-            }
-            if (updateTime > lastUpdateTime) {
-              lastUpdateTime = updateTime;
-            }
-          }
-        } catch (IOException e) {
-          // ignore
-        }
-      }
-    }
-    String unknownString = "unknown";
-
-    for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
-    }
-    outStream.writeBytes("totalNumberFiles:");
-    outStream.writeBytes(unknown ? unknownString : "" + numOfFiles);
-    outStream.write(terminator);
-
-    for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
-    }
-    outStream.writeBytes("totalFileSize:");
-    outStream.writeBytes(unknown ? unknownString : "" + totalFileSize);
-    outStream.write(terminator);
-
-    for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
-    }
-    outStream.writeBytes("maxFileSize:");
-    outStream.writeBytes(unknown ? unknownString : "" + maxFileSize);
-    outStream.write(terminator);
-
-    for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
-    }
-    outStream.writeBytes("minFileSize:");
-    if (numOfFiles > 0) {
-      outStream.writeBytes(unknown ? unknownString : "" + minFileSize);
-    } else {
-      outStream.writeBytes(unknown ? unknownString : "" + 0);
-    }
-    outStream.write(terminator);
-
-    for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
-    }
-    outStream.writeBytes("lastAccessTime:");
-    outStream.writeBytes((unknown || lastAccessTime < 0) ? unknownString : ""
-        + lastAccessTime);
-    outStream.write(terminator);
-
-    for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
-    }
-    outStream.writeBytes("lastUpdateTime:");
-    outStream.writeBytes(unknown ? unknownString : "" + lastUpdateTime);
-    outStream.write(terminator);
-  }
-
-  /**
-   * Alter a given table.
-   *
-   * @param db
-   *          The database in question.
-   * @param alterTbl
-   *          This is the table we're altering.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {
-    // alter the table
-    Table tbl = db.getTable(alterTbl.getOldName());
-
-    Partition part = null;
-    if(alterTbl.getPartSpec() != null) {
-      part = db.getPartition(tbl, alterTbl.getPartSpec(), false);
-      if(part == null) {
-        console.printError("Partition : " + alterTbl.getPartSpec().toString()
-            + " does not exist.");
-        return 1;
-      }
-    }
-
-    validateAlterTableType(tbl, alterTbl.getOp());
-
-    if (tbl.isView()) {
-      if (!alterTbl.getExpectView()) {
-        throw new HiveException("Cannot alter a view with ALTER TABLE");
-      }
-    } else {
-      if (alterTbl.getExpectView()) {
-        throw new HiveException("Cannot alter a base table with ALTER VIEW");
-      }
-    }
-
-    Table oldTbl = tbl.copy();
-
-    if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAME) {
-      tbl.setTableName(alterTbl.getNewName());
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDCOLS) {
-      List<FieldSchema> newCols = alterTbl.getNewCols();
-      List<FieldSchema> oldCols = tbl.getCols();
-      if (tbl.getSerializationLib().equals(
-          "org.apache.hadoop.hive.serde.thrift.columnsetSerDe")) {
-        console
-            .printInfo("Replacing columns for columnsetSerDe and changing to LazySimpleSerDe");
-        tbl.setSerializationLib(LazySimpleSerDe.class.getName());
-        tbl.getTTable().getSd().setCols(newCols);
-      } else {
-        // make sure the columns does not already exist
-        Iterator<FieldSchema> iterNewCols = newCols.iterator();
-        while (iterNewCols.hasNext()) {
-          FieldSchema newCol = iterNewCols.next();
-          String newColName = newCol.getName();
-          Iterator<FieldSchema> iterOldCols = oldCols.iterator();
-          while (iterOldCols.hasNext()) {
-            String oldColName = iterOldCols.next().getName();
-            if (oldColName.equalsIgnoreCase(newColName)) {
-              console.printError("Column '" + newColName + "' exists");
-              return 1;
-            }
-          }
-          oldCols.add(newCol);
-        }
-        tbl.getTTable().getSd().setCols(oldCols);
-      }
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAMECOLUMN) {
-      List<FieldSchema> oldCols = tbl.getCols();
-      List<FieldSchema> newCols = new ArrayList<FieldSchema>();
-      Iterator<FieldSchema> iterOldCols = oldCols.iterator();
-      String oldName = alterTbl.getOldColName();
-      String newName = alterTbl.getNewColName();
-      String type = alterTbl.getNewColType();
-      String comment = alterTbl.getNewColComment();
-      boolean first = alterTbl.getFirst();
-      String afterCol = alterTbl.getAfterCol();
-      FieldSchema column = null;
-
-      boolean found = false;
-      int position = -1;
-      if (first) {
-        position = 0;
-      }
-
-      int i = 1;
-      while (iterOldCols.hasNext()) {
-        FieldSchema col = iterOldCols.next();
-        String oldColName = col.getName();
-        if (oldColName.equalsIgnoreCase(newName)
-            && !oldColName.equalsIgnoreCase(oldName)) {
-          console.printError("Column '" + newName + "' exists");
-          return 1;
-        } else if (oldColName.equalsIgnoreCase(oldName)) {
-          col.setName(newName);
-          if (type != null && !type.trim().equals("")) {
-            col.setType(type);
-          }
-          if (comment != null) {
-            col.setComment(comment);
-          }
-          found = true;
-          if (first || (afterCol != null && !afterCol.trim().equals(""))) {
-            column = col;
-            continue;
-          }
-        }
-
-        if (afterCol != null && !afterCol.trim().equals("")
-            && oldColName.equalsIgnoreCase(afterCol)) {
-          position = i;
-        }
-
-        i++;
-        newCols.add(col);
-      }
-
-      // did not find the column
-      if (!found) {
-        console.printError("Column '" + oldName + "' does not exist");
-        return 1;
-      }
-      // after column is not null, but we did not find it.
-      if ((afterCol != null && !afterCol.trim().equals("")) && position < 0) {
-        console.printError("Column '" + afterCol + "' does not exist");
-        return 1;
-      }
-
-      if (position >= 0) {
-        newCols.add(position, column);
-      }
-
-      tbl.getTTable().getSd().setCols(newCols);
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.REPLACECOLS) {
-      // change SerDe to LazySimpleSerDe if it is columnsetSerDe
-      if (tbl.getSerializationLib().equals(
-          "org.apache.hadoop.hive.serde.thrift.columnsetSerDe")) {
-        console
-            .printInfo("Replacing columns for columnsetSerDe and changing to LazySimpleSerDe");
-        tbl.setSerializationLib(LazySimpleSerDe.class.getName());
-      } else if (!tbl.getSerializationLib().equals(
-          MetadataTypedColumnsetSerDe.class.getName())
-          && !tbl.getSerializationLib().equals(LazySimpleSerDe.class.getName())
-          && !tbl.getSerializationLib().equals(ColumnarSerDe.class.getName())
-          && !tbl.getSerializationLib().equals(DynamicSerDe.class.getName())) {
-        console.printError("Replace columns is not supported for this table. "
-            + "SerDe may be incompatible.");
-        return 1;
-      }
-      tbl.getTTable().getSd().setCols(alterTbl.getNewCols());
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDPROPS) {
-      tbl.getTTable().getParameters().putAll(alterTbl.getProps());
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDSERDEPROPS) {
-      tbl.getTTable().getSd().getSerdeInfo().getParameters().putAll(
-          alterTbl.getProps());
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDSERDE) {
-      tbl.setSerializationLib(alterTbl.getSerdeName());
-      if ((alterTbl.getProps() != null) && (alterTbl.getProps().size() > 0)) {
-        tbl.getTTable().getSd().getSerdeInfo().getParameters().putAll(
-            alterTbl.getProps());
-      }
-      tbl.setFields(Hive.getFieldsFromDeserializer(tbl.getTableName(), tbl
-          .getDeserializer()));
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDFILEFORMAT) {
-      if(part != null) {
-        part.getTPartition().getSd().setInputFormat(alterTbl.getInputFormat());
-        part.getTPartition().getSd().setOutputFormat(alterTbl.getOutputFormat());
-        if (alterTbl.getSerdeName() != null) {
-          part.getTPartition().getSd().getSerdeInfo().setSerializationLib(
-              alterTbl.getSerdeName());
-        }
-      } else {
-        tbl.getTTable().getSd().setInputFormat(alterTbl.getInputFormat());
-        tbl.getTTable().getSd().setOutputFormat(alterTbl.getOutputFormat());
-        if (alterTbl.getSerdeName() != null) {
-          tbl.setSerializationLib(alterTbl.getSerdeName());
-        }
-      }
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ALTERPROTECTMODE) {
-      boolean protectModeEnable = alterTbl.isProtectModeEnable();
-      AlterTableDesc.ProtectModeType protectMode = alterTbl.getProtectModeType();
-
-      ProtectMode mode = null;
-      if(part != null) {
-        mode = part.getProtectMode();
-      } else {
-        mode = tbl.getProtectMode();
-      }
-
-      if (protectModeEnable
-          && protectMode == AlterTableDesc.ProtectModeType.OFFLINE) {
-        mode.offline = true;
-      } else if (protectModeEnable
-          && protectMode == AlterTableDesc.ProtectModeType.NO_DROP) {
-        mode.noDrop = true;
-      } else if (!protectModeEnable
-          && protectMode == AlterTableDesc.ProtectModeType.OFFLINE) {
-        mode.offline = false;
-      } else if (!protectModeEnable
-          && protectMode == AlterTableDesc.ProtectModeType.NO_DROP) {
-        mode.noDrop = false;
-      }
-
-      if (part != null) {
-        part.setProtectMode(mode);
-      } else {
-        tbl.setProtectMode(mode);
-      }
-
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDCLUSTERSORTCOLUMN) {
-      // validate sort columns and bucket columns
-      List<String> columns = Utilities.getColumnNamesFromFieldSchema(tbl
-          .getCols());
-      Utilities.validateColumnNames(columns, alterTbl.getBucketColumns());
-      if (alterTbl.getSortColumns() != null) {
-        Utilities.validateColumnNames(columns, Utilities
-            .getColumnNamesFromSortCols(alterTbl.getSortColumns()));
-      }
-
-      int numBuckets = -1;
-      ArrayList<String> bucketCols = null;
-      ArrayList<Order> sortCols = null;
-
-      // -1 buckets means to turn off bucketing
-      if (alterTbl.getNumberBuckets() == -1) {
-        bucketCols = new ArrayList<String>();
-        sortCols = new ArrayList<Order>();
-        numBuckets = -1;
-      } else {
-        bucketCols = alterTbl.getBucketColumns();
-        sortCols = alterTbl.getSortColumns();
-        numBuckets = alterTbl.getNumberBuckets();
-      }
-      tbl.getTTable().getSd().setBucketCols(bucketCols);
-      tbl.getTTable().getSd().setNumBuckets(numBuckets);
-      tbl.getTTable().getSd().setSortCols(sortCols);
-    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ALTERLOCATION) {
-      String newLocation = alterTbl.getNewLocation();
-      try {
-        URI locURI = new URI(newLocation);
-        if (!locURI.isAbsolute() || locURI.getScheme() == null
-            || locURI.getScheme().trim().equals("")) {
-          throw new HiveException(
-              newLocation
-                  + " is not absolute or has no scheme information. "
-                  + "Please specify a complete absolute uri with scheme information.");
-        }
-        if (part != null) {
-          part.setLocation(newLocation);
-        } else {
-          tbl.setDataLocation(locURI);
-        }
-      } catch (URISyntaxException e) {
-        throw new HiveException(e);
-      }
-    } else {
-      console.printError("Unsupported Alter commnad");
-      return 1;
-    }
-
-    if(part == null) {
-      if (!updateModifiedParameters(tbl.getTTable().getParameters(), conf)) {
-        return 1;
-      }
-      try {
-        tbl.checkValidity();
-      } catch (HiveException e) {
-        console.printError("Invalid table columns : " + e.getMessage(),
-            stringifyException(e));
-        return 1;
-      }
-    } else {
-      if (!updateModifiedParameters(part.getParameters(), conf)) {
-        return 1;
-      }
-    }
-
-    try {
-      if (part == null) {
-        db.alterTable(alterTbl.getOldName(), tbl);
-      } else {
-        db.alterPartition(tbl.getTableName(), part);
-      }
-    } catch (InvalidOperationException e) {
-      console.printError("Invalid alter operation: " + e.getMessage());
-      LOG.info("alter table: " + stringifyException(e));
-      return 1;
-    } catch (HiveException e) {
-      return 1;
-    }
-
-    // This is kind of hacky - the read entity contains the old table, whereas
-    // the write entity
-    // contains the new table. This is needed for rename - both the old and the
-    // new table names are
-    // passed
-    if(part != null) {
-      work.getInputs().add(new ReadEntity(part));
-      work.getOutputs().add(new WriteEntity(part));
-    } else {
-      work.getInputs().add(new ReadEntity(oldTbl));
-      work.getOutputs().add(new WriteEntity(tbl));
-    }
-    return 0;
-  }
-
-  /**
-   * Drop a given table.
-   *
-   * @param db
-   *          The database in question.
-   * @param dropTbl
-   *          This is the table we're dropping.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int dropTable(Hive db, DropTableDesc dropTbl) throws HiveException {
-    // We need to fetch the table before it is dropped so that it can be passed
-    // to
-    // post-execution hook
-    Table tbl = null;
-    try {
-      tbl = db.getTable(dropTbl.getTableName());
-    } catch (InvalidTableException e) {
-      // drop table is idempotent
-    }
-
-    if (tbl != null) {
-      if (tbl.isView()) {
-        if (!dropTbl.getExpectView()) {
-          throw new HiveException("Cannot drop a view with DROP TABLE");
-        }
-      } else {
-        if (dropTbl.getExpectView()) {
-          throw new HiveException("Cannot drop a base table with DROP VIEW");
-        }
-      }
-    }
-
-    if (dropTbl.getPartSpecs() == null) {
-      if (tbl != null && !tbl.canDrop()) {
-        throw new HiveException("Table " + tbl.getTableName() +
-            " is protected from being dropped");
-      }
-
-      // We should check that all the partitions of the table can be dropped
-      if (tbl != null && tbl.isPartitioned()) {
-        List<Partition> listPartitions = db.getPartitions(tbl);
-        for (Partition p: listPartitions) {
-            if (!p.canDrop()) {
-              throw new HiveException("Table " + tbl.getTableName() +
-                  " Partition" + p.getName() +
-                  " is protected from being dropped");
-            }
-        }
-      }
-
-      // drop the table
-      db.dropTable(db.getCurrentDatabase(), dropTbl.getTableName());
-      if (tbl != null) {
-        work.getOutputs().add(new WriteEntity(tbl));
-      }
-    } else {
-      // get all partitions of the table
-      List<String> partitionNames =
-        db.getPartitionNames(db.getCurrentDatabase(), dropTbl.getTableName(), (short) -1);
-      Set<Map<String, String>> partitions = new HashSet<Map<String, String>>();
-      for (String partitionName : partitionNames) {
-        try {
-          partitions.add(Warehouse.makeSpecFromName(partitionName));
-        } catch (MetaException e) {
-          LOG.warn("Unrecognized partition name from metastore: " + partitionName);
-        }
-      }
-      // drop partitions in the list
-      List<Partition> partsToDelete = new ArrayList<Partition>();
-      for (Map<String, String> partSpec : dropTbl.getPartSpecs()) {
-        Iterator<Map<String, String>> it = partitions.iterator();
-        while (it.hasNext()) {
-          Map<String, String> part = it.next();
-          // test if partSpec matches part
-          boolean match = true;
-          for (Map.Entry<String, String> item : partSpec.entrySet()) {
-            if (!item.getValue().equals(part.get(item.getKey()))) {
-              match = false;
-              break;
-            }
-          }
-          if (match) {
-            Partition p = db.getPartition(tbl, part, false);
-            if (!p.canDrop()) {
-              throw new HiveException("Table " + tbl.getTableName() +
-                  " Partition " + p.getName() +
-                  " is protected from being dropped");
-            }
-
-            partsToDelete.add(p);
-            it.remove();
-          }
-        }
-      }
-
-      // drop all existing partitions from the list
-      for (Partition partition : partsToDelete) {
-        console.printInfo("Dropping the partition " + partition.getName());
-        db.dropPartition(db.getCurrentDatabase(), dropTbl.getTableName(),
-            partition.getValues(), true); // drop data for the
-        // partition
-        work.getOutputs().add(new WriteEntity(partition));
-      }
-    }
-
-    return 0;
-  }
-
-  /**
-   * Update last_modified_by and last_modified_time parameters in parameter map.
-   *
-   * @param params
-   *          Parameters.
-   * @param user
-   *          user that is doing the updating.
-   */
-  private boolean updateModifiedParameters(Map<String, String> params, HiveConf conf) {
-    String user = null;
-    try {
-      user = conf.getUser();
-    } catch (IOException e) {
-      console.printError("Unable to get current user: " + e.getMessage(),
-          stringifyException(e));
-      return false;
-    }
-
-    params.put("last_modified_by", user);
-    params.put("last_modified_time", Long.toString(System.currentTimeMillis() / 1000));
-    return true;
-  }
-
-  /**
-   * Check if the given serde is valid.
-   */
-  private void validateSerDe(String serdeName) throws HiveException {
-    try {
-      Deserializer d = SerDeUtils.lookupDeserializer(serdeName);
-      if (d != null) {
-        LOG.debug("Found class for " + serdeName);
-      }
-    } catch (SerDeException e) {
-      throw new HiveException("Cannot validate serde: " + serdeName, e);
-    }
-  }
-
-  /**
-   * Create a Database
-   * @param db
-   * @param crtDb
-   * @return Always returns 0
-   * @throws HiveException
-   * @throws AlreadyExistsException
-   */
-  private int createDatabase(Hive db, CreateDatabaseDesc crtDb)
-      throws HiveException, AlreadyExistsException {
-    Database database = new Database();
-    database.setName(crtDb.getName());
-    database.setDescription(crtDb.getComment());
-    database.setLocationUri(crtDb.getLocationUri());
-    database.setParameters(crtDb.getDatabaseProperties());
-
-    db.createDatabase(database, crtDb.getIfNotExists());
-    return 0;
-  }
-
-  /**
-   * Drop a Database
-   * @param db
-   * @param dropDb
-   * @return Always returns 0
-   * @throws HiveException
-   * @throws NoSuchObjectException
-   */
-  private int dropDatabase(Hive db, DropDatabaseDesc dropDb)
-      throws HiveException, NoSuchObjectException {
-    db.dropDatabase(dropDb.getDatabaseName(), true, dropDb.getIfExists());
-    return 0;
-  }
-
-  /**
-   * Switch to a different Database
-   * @param db
-   * @param switchDb
-   * @return Always returns 0
-   * @throws HiveException
-   */
-  private int switchDatabase(Hive db, SwitchDatabaseDesc switchDb)
-      throws HiveException {
-    String dbName = switchDb.getDatabaseName();
-    if (!db.databaseExists(dbName)) {
-      throw new HiveException("ERROR: The database " + dbName + " does not exist.");
-    }
-    db.setCurrentDatabase(dbName);
-    return 0;
-  }
-
-
-  /**
-   * Create a new table.
-   *
-   * @param db
-   *          The database in question.
-   * @param crtTbl
-   *          This is the table we're creating.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int createTable(Hive db, CreateTableDesc crtTbl) throws HiveException {
-    // create the table
-    Table tbl = new Table(db.getCurrentDatabase(), crtTbl.getTableName());
-
-    if (crtTbl.getTblProps() != null) {
-      tbl.getTTable().getParameters().putAll(crtTbl.getTblProps());
-    }
-
-    if (crtTbl.getPartCols() != null) {
-      tbl.setPartCols(crtTbl.getPartCols());
-    }
-    if (crtTbl.getNumBuckets() != -1) {
-      tbl.setNumBuckets(crtTbl.getNumBuckets());
-    }
-
-    if (crtTbl.getStorageHandler() != null) {
-      tbl.setProperty(
-        org.apache.hadoop.hive.metastore.api.Constants.META_TABLE_STORAGE,
-        crtTbl.getStorageHandler());
-    }
-    HiveStorageHandler storageHandler = tbl.getStorageHandler();
-
-    /*
-     * We use LazySimpleSerDe by default.
-     *
-     * If the user didn't specify a SerDe, and any of the columns are not simple
-     * types, we will have to use DynamicSerDe instead.
-     */
-    if (crtTbl.getSerName() == null) {
-      if (storageHandler == null) {
-        LOG.info("Default to LazySimpleSerDe for table " + crtTbl.getTableName());
-        tbl.setSerializationLib(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
-      } else {
-        String serDeClassName = storageHandler.getSerDeClass().getName();
-        LOG.info("Use StorageHandler-supplied " + serDeClassName
-          + " for table " + crtTbl.getTableName());
-        tbl.setSerializationLib(serDeClassName);
-      }
-    } else {
-      // let's validate that the serde exists
-      validateSerDe(crtTbl.getSerName());
-      tbl.setSerializationLib(crtTbl.getSerName());
-    }
-
-    if (crtTbl.getFieldDelim() != null) {
-      tbl.setSerdeParam(Constants.FIELD_DELIM, crtTbl.getFieldDelim());
-      tbl.setSerdeParam(Constants.SERIALIZATION_FORMAT, crtTbl.getFieldDelim());
-    }
-    if (crtTbl.getFieldEscape() != null) {
-      tbl.setSerdeParam(Constants.ESCAPE_CHAR, crtTbl.getFieldEscape());
-    }
-
-    if (crtTbl.getCollItemDelim() != null) {
-      tbl.setSerdeParam(Constants.COLLECTION_DELIM, crtTbl.getCollItemDelim());
-    }
-    if (crtTbl.getMapKeyDelim() != null) {
-      tbl.setSerdeParam(Constants.MAPKEY_DELIM, crtTbl.getMapKeyDelim());
-    }
-    if (crtTbl.getLineDelim() != null) {
-      tbl.setSerdeParam(Constants.LINE_DELIM, crtTbl.getLineDelim());
-    }
-
-    if (crtTbl.getSerdeProps() != null) {
-      Iterator<Entry<String, String>> iter = crtTbl.getSerdeProps().entrySet()
-        .iterator();
-      while (iter.hasNext()) {
-        Entry<String, String> m = iter.next();
-        tbl.setSerdeParam(m.getKey(), m.getValue());
-      }
-    }
-
-    if (crtTbl.getCols() != null) {
-      tbl.setFields(crtTbl.getCols());
-    }
-    if (crtTbl.getBucketCols() != null) {
-      tbl.setBucketCols(crtTbl.getBucketCols());
-    }
-    if (crtTbl.getSortCols() != null) {
-      tbl.setSortCols(crtTbl.getSortCols());
-    }
-    if (crtTbl.getComment() != null) {
-      tbl.setProperty("comment", crtTbl.getComment());
-    }
-    if (crtTbl.getLocation() != null) {
-      tbl.setDataLocation(new Path(crtTbl.getLocation()).toUri());
-    }
-
-    tbl.setInputFormatClass(crtTbl.getInputFormat());
-    tbl.setOutputFormatClass(crtTbl.getOutputFormat());
-
-    tbl.getTTable().getSd().setInputFormat(
-      tbl.getInputFormatClass().getName());
-    tbl.getTTable().getSd().setOutputFormat(
-      tbl.getOutputFormatClass().getName());
-
-    if (crtTbl.isExternal()) {
-      tbl.setProperty("EXTERNAL", "TRUE");
-      tbl.setTableType(TableType.EXTERNAL_TABLE);
-    }
-
-    // If the sorted columns is a superset of bucketed columns, store this fact.
-    // It can be later used to
-    // optimize some group-by queries. Note that, the order does not matter as
-    // long as it in the first
-    // 'n' columns where 'n' is the length of the bucketed columns.
-    if ((tbl.getBucketCols() != null) && (tbl.getSortCols() != null)) {
-      List<String> bucketCols = tbl.getBucketCols();
-      List<Order> sortCols = tbl.getSortCols();
-
-      if ((sortCols.size() > 0) && (sortCols.size() >= bucketCols.size())) {
-        boolean found = true;
-
-        Iterator<String> iterBucketCols = bucketCols.iterator();
-        while (iterBucketCols.hasNext()) {
-          String bucketCol = iterBucketCols.next();
-          boolean colFound = false;
-          for (int i = 0; i < bucketCols.size(); i++) {
-            if (bucketCol.equals(sortCols.get(i).getCol())) {
-              colFound = true;
-              break;
-            }
-          }
-          if (colFound == false) {
-            found = false;
-            break;
-          }
-        }
-        if (found) {
-          tbl.setProperty("SORTBUCKETCOLSPREFIX", "TRUE");
-        }
-      }
-    }
-
-    int rc = setGenericTableAttributes(tbl);
-    if (rc != 0) {
-      return rc;
-    }
-
-    // create the table
-    db.createTable(tbl, crtTbl.getIfNotExists());
-    work.getOutputs().add(new WriteEntity(tbl));
-    return 0;
-  }
-
-  /**
-   * Create a new table like an existing table.
-   *
-   * @param db
-   *          The database in question.
-   * @param crtTbl
-   *          This is the table we're creating.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int createTableLike(Hive db, CreateTableLikeDesc crtTbl) throws HiveException {
-    // Get the existing table
-    Table tbl = db.getTable(crtTbl.getLikeTableName());
-
-    tbl.setTableName(crtTbl.getTableName());
-
-    if (crtTbl.isExternal()) {
-      tbl.setProperty("EXTERNAL", "TRUE");
-    } else {
-      tbl.setProperty("EXTERNAL", "FALSE");
-    }
-
-    if (crtTbl.getLocation() != null) {
-      tbl.setDataLocation(new Path(crtTbl.getLocation()).toUri());
-    } else {
-      tbl.unsetDataLocation();
-    }
-
-    // we should reset table specific parameters including (stats, lastDDLTime etc.)
-    Map<String, String> params = tbl.getParameters();
-    params.clear();
-
-    // create the table
-    db.createTable(tbl, crtTbl.getIfNotExists());
-    work.getOutputs().add(new WriteEntity(tbl));
-    return 0;
-  }
-
-  /**
-   * Create a new view.
-   *
-   * @param db
-   *          The database in question.
-   * @param crtView
-   *          This is the view we're creating.
-   * @return Returns 0 when execution succeeds and above 0 if it fails.
-   * @throws HiveException
-   *           Throws this exception if an unexpected error occurs.
-   */
-  private int createView(Hive db, CreateViewDesc crtView) throws HiveException {
-    Table tbl = new Table(db.getCurrentDatabase(), crtView.getViewName());
-    tbl.setTableType(TableType.VIRTUAL_VIEW);
-    tbl.setSerializationLib(null);
-    tbl.clearSerDeInfo();
-    tbl.setViewOriginalText(crtView.getViewOriginalText());
-    tbl.setViewExpandedText(crtView.getViewExpandedText());
-    tbl.setFields(crtView.getSchema());
-    if (crtView.getComment() != null) {
-      tbl.setProperty("comment", crtView.getComment());
-    }
-    if (crtView.getTblProps() != null) {
-      tbl.getTTable().getParameters().putAll(crtView.getTblProps());
-    }
-
-    int rc = setGenericTableAttributes(tbl);
-    if (rc != 0) {
-      return rc;
-    }
-
-    db.createTable(tbl, crtView.getIfNotExists());
-    work.getOutputs().add(new WriteEntity(tbl));
-    return 0;
-  }
-
-  private int setGenericTableAttributes(Table tbl) {
-    try {
-      tbl.setOwner(conf.getUser());
-    } catch (IOException e) {
-      console.printError("Unable to get current user: " + e.getMessage(),
-          stringifyException(e));
-      return 1;
-    }
-    // set create time
-    tbl.setCreateTime((int) (System.currentTimeMillis() / 1000));
-    return 0;
-  }
-
-  @Override
-  public int getType() {
-    return StageType.DDL;
-  }
-
-  @Override
-  public String getName() {
-    return "DDL";
-  }
-
-  @Override
-  protected void localizeMRTmpFilesImpl(Context ctx) {
-    // no-op
-  }
-}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java.orig b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java.orig
deleted file mode 100644
index fa1eb35f07..0000000000
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java.orig
+++ /dev/null
@@ -1,1061 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec;
-
-import java.io.Serializable;
-import java.lang.management.ManagementFactory;
-import java.lang.management.MemoryMXBean;
-import java.lang.reflect.Field;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.parse.OpParseContext;
-import org.apache.hadoop.hive.ql.plan.AggregationDesc;
-import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
-import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
-import org.apache.hadoop.hive.ql.plan.GroupByDesc;
-import org.apache.hadoop.hive.ql.plan.api.OperatorType;
-import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
-import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AggregationBuffer;
-import org.apache.hadoop.hive.serde2.lazy.LazyPrimitive;
-import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
-import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StructField;
-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.UnionObject;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
-import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.apache.hadoop.io.Text;
-
-/**
- * GroupBy operator implementation.
- */
-public class GroupByOperator extends Operator<GroupByDesc> implements
-    Serializable {
-
-  private static final Log LOG = LogFactory.getLog(GroupByOperator.class
-      .getName());
-
-  private static final long serialVersionUID = 1L;
-  private static final int NUMROWSESTIMATESIZE = 1000;
-
-  protected transient ExprNodeEvaluator[] keyFields;
-  protected transient ObjectInspector[] keyObjectInspectors;
-
-  protected transient ExprNodeEvaluator[][] aggregationParameterFields;
-  protected transient ObjectInspector[][] aggregationParameterObjectInspectors;
-  protected transient ObjectInspector[][] aggregationParameterStandardObjectInspectors;
-  protected transient Object[][] aggregationParameterObjects;
-  // In the future, we may allow both count(DISTINCT a) and sum(DISTINCT a) in
-  // the same SQL clause,
-  // so aggregationIsDistinct is a boolean array instead of a single number.
-  protected transient boolean[] aggregationIsDistinct;
-  // Map from integer tag to distinct aggrs
-  transient protected Map<Integer, Set<Integer>> distinctKeyAggrs =
-    new HashMap<Integer, Set<Integer>>();
-  // Map from integer tag to non-distinct aggrs with key parameters.
-  transient protected Map<Integer, Set<Integer>> nonDistinctKeyAggrs =
-    new HashMap<Integer, Set<Integer>>();
-  // List of non-distinct aggrs.
-  transient protected List<Integer> nonDistinctAggrs = new ArrayList<Integer>();
-  // Union expr for distinct keys
-  transient ExprNodeEvaluator unionExprEval = null;
-
-  transient GenericUDAFEvaluator[] aggregationEvaluators;
-
-  protected transient ArrayList<ObjectInspector> objectInspectors;
-  transient ArrayList<String> fieldNames;
-
-  transient KeyWrapperFactory keyWrapperFactory;
-  // Used by sort-based GroupBy: Mode = COMPLETE, PARTIAL1, PARTIAL2,
-  // MERGEPARTIAL
-  protected transient KeyWrapper currentKeys;
-  protected transient KeyWrapper newKeys;
-  protected transient AggregationBuffer[] aggregations;
-  protected transient Object[][] aggregationsParametersLastInvoke;
-
-  // Used by hash-based GroupBy: Mode = HASH, PARTIALS
-  protected transient HashMap<KeyWrapper, AggregationBuffer[]> hashAggregations;
-
-  // Used by hash distinct aggregations when hashGrpKeyNotRedKey is true
-  protected transient HashSet<KeyWrapper> keysCurrentGroup;
-
-  transient boolean bucketGroup;
-
-  transient boolean firstRow;
-  transient long totalMemory;
-  transient boolean hashAggr;
-  // The reduction is happening on the reducer, and the grouping key and
-  // reduction keys are different.
-  // For example: select a, count(distinct b) from T group by a
-  // The data is sprayed by 'b' and the reducer is grouping it by 'a'
-  transient boolean groupKeyIsNotReduceKey;
-  transient boolean firstRowInGroup;
-  transient long numRowsInput;
-  transient long numRowsHashTbl;
-  transient int groupbyMapAggrInterval;
-  transient long numRowsCompareHashAggr;
-  transient float minReductionHashAggr;
-
-  // current Key ObjectInspectors are standard ObjectInspectors
-  protected transient ObjectInspector[] currentKeyObjectInspectors;
-  // new Key ObjectInspectors are objectInspectors from the parent
-  transient StructObjectInspector newKeyObjectInspector;
-  transient StructObjectInspector currentKeyObjectInspector;
-  public static MemoryMXBean memoryMXBean;
-  private long maxMemory;
-  private float memoryThreshold;
-
-  /**
-   * This is used to store the position and field names for variable length
-   * fields.
-   **/
-  class varLenFields {
-    int aggrPos;
-    List<Field> fields;
-
-    varLenFields(int aggrPos, List<Field> fields) {
-      this.aggrPos = aggrPos;
-      this.fields = fields;
-    }
-
-    int getAggrPos() {
-      return aggrPos;
-    }
-
-    List<Field> getFields() {
-      return fields;
-    }
-  };
-
-  // for these positions, some variable primitive type (String) is used, so size
-  // cannot be estimated. sample it at runtime.
-  transient List<Integer> keyPositionsSize;
-
-  // for these positions, some variable primitive type (String) is used for the
-  // aggregation classes
-  transient List<varLenFields> aggrPositions;
-
-  transient int fixedRowSize;
-  transient long maxHashTblMemory;
-  transient int totalVariableSize;
-  transient int numEntriesVarSize;
-  transient int numEntriesHashTable;
-  transient int countAfterReport;
-  transient int heartbeatInterval;
-
-  @Override
-  protected void initializeOp(Configuration hconf) throws HiveException {
-    totalMemory = Runtime.getRuntime().totalMemory();
-    numRowsInput = 0;
-    numRowsHashTbl = 0;
-
-    heartbeatInterval = HiveConf.getIntVar(hconf,
-        HiveConf.ConfVars.HIVESENDHEARTBEAT);
-    countAfterReport = 0;
-
-    ObjectInspector rowInspector = inputObjInspectors[0];
-
-    // init keyFields
-    keyFields = new ExprNodeEvaluator[conf.getKeys().size()];
-    keyObjectInspectors = new ObjectInspector[conf.getKeys().size()];
-    currentKeyObjectInspectors = new ObjectInspector[conf.getKeys().size()];
-    for (int i = 0; i < keyFields.length; i++) {
-      keyFields[i] = ExprNodeEvaluatorFactory.get(conf.getKeys().get(i));
-      keyObjectInspectors[i] = keyFields[i].initialize(rowInspector);
-      currentKeyObjectInspectors[i] = ObjectInspectorUtils
-          .getStandardObjectInspector(keyObjectInspectors[i],
-          ObjectInspectorCopyOption.WRITABLE);
-    }
-
-    // initialize unionExpr for reduce-side
-    // reduce KEY has union field as the last field if there are distinct
-    // aggregates in group-by.
-    List<? extends StructField> sfs =
-      ((StandardStructObjectInspector) rowInspector).getAllStructFieldRefs();
-    if (sfs.size() > 0) {
-      StructField keyField = sfs.get(0);
-      if (keyField.getFieldName().toUpperCase().equals(
-          Utilities.ReduceField.KEY.name())) {
-        ObjectInspector keyObjInspector = keyField.getFieldObjectInspector();
-        if (keyObjInspector instanceof StandardStructObjectInspector) {
-          List<? extends StructField> keysfs =
-            ((StandardStructObjectInspector) keyObjInspector).getAllStructFieldRefs();
-          if (keysfs.size() > 0) {
-            // the last field is the union field, if any
-            StructField sf = keysfs.get(keysfs.size() - 1);
-            if (sf.getFieldObjectInspector().getCategory().equals(
-                ObjectInspector.Category.UNION)) {
-              unionExprEval = ExprNodeEvaluatorFactory.get(
-                new ExprNodeColumnDesc(TypeInfoUtils.getTypeInfoFromObjectInspector(
-                sf.getFieldObjectInspector()),
-                keyField.getFieldName() + "." + sf.getFieldName(), null,
-                false));
-              unionExprEval.initialize(rowInspector);
-            }
-          }
-        }
-      }
-    }
-    // init aggregationParameterFields
-    ArrayList<AggregationDesc> aggrs = conf.getAggregators();
-    aggregationParameterFields = new ExprNodeEvaluator[aggrs.size()][];
-    aggregationParameterObjectInspectors = new ObjectInspector[aggrs.size()][];
-    aggregationParameterStandardObjectInspectors = new ObjectInspector[aggrs.size()][];
-    aggregationParameterObjects = new Object[aggrs.size()][];
-    aggregationIsDistinct = new boolean[aggrs.size()];
-    for (int i = 0; i < aggrs.size(); i++) {
-      AggregationDesc aggr = aggrs.get(i);
-      ArrayList<ExprNodeDesc> parameters = aggr.getParameters();
-      aggregationParameterFields[i] = new ExprNodeEvaluator[parameters.size()];
-      aggregationParameterObjectInspectors[i] = new ObjectInspector[parameters
-          .size()];
-      aggregationParameterStandardObjectInspectors[i] = new ObjectInspector[parameters
-          .size()];
-      aggregationParameterObjects[i] = new Object[parameters.size()];
-      for (int j = 0; j < parameters.size(); j++) {
-        aggregationParameterFields[i][j] = ExprNodeEvaluatorFactory
-            .get(parameters.get(j));
-        aggregationParameterObjectInspectors[i][j] = aggregationParameterFields[i][j]
-            .initialize(rowInspector);
-        if (unionExprEval != null) {
-          String[] names = parameters.get(j).getExprString().split("\\.");
-          // parameters of the form : KEY.colx:t.coly
-          if (Utilities.ReduceField.KEY.name().equals(names[0])) {
-            String name = names[names.length - 2];
-            int tag = Integer.parseInt(name.split("\\:")[1]);
-            if (aggr.getDistinct()) {
-              // is distinct
-              Set<Integer> set = distinctKeyAggrs.get(tag);
-              if (null == set) {
-                set = new HashSet<Integer>();
-                distinctKeyAggrs.put(tag, set);
-              }
-              if (!set.contains(i)) {
-                set.add(i);
-              }
-            } else {
-              Set<Integer> set = nonDistinctKeyAggrs.get(tag);
-              if (null == set) {
-                set = new HashSet<Integer>();
-                nonDistinctKeyAggrs.put(tag, set);
-              }
-              if (!set.contains(i)) {
-                set.add(i);
-              }
-            }
-          } else {
-            // will be VALUE._COLx
-            if (!nonDistinctAggrs.contains(i)) {
-              nonDistinctAggrs.add(i);
-            }
-          }
-        } else {
-          if (aggr.getDistinct()) {
-            aggregationIsDistinct[i] = true;
-          }
-        }
-        aggregationParameterStandardObjectInspectors[i][j] = ObjectInspectorUtils
-            .getStandardObjectInspector(
-            aggregationParameterObjectInspectors[i][j],
-            ObjectInspectorCopyOption.WRITABLE);
-        aggregationParameterObjects[i][j] = null;
-      }
-      if (parameters.size() == 0) {
-        // for ex: count(*)
-        if (!nonDistinctAggrs.contains(i)) {
-          nonDistinctAggrs.add(i);
-        }
-      }
-    }
-
-    // init aggregationClasses
-    aggregationEvaluators = new GenericUDAFEvaluator[conf.getAggregators()
-        .size()];
-    for (int i = 0; i < aggregationEvaluators.length; i++) {
-      AggregationDesc agg = conf.getAggregators().get(i);
-      aggregationEvaluators[i] = agg.getGenericUDAFEvaluator();
-    }
-
-    // init objectInspectors
-    int totalFields = keyFields.length + aggregationEvaluators.length;
-    objectInspectors = new ArrayList<ObjectInspector>(totalFields);
-    for (ExprNodeEvaluator keyField : keyFields) {
-      objectInspectors.add(null);
-    }
-    for (int i = 0; i < aggregationEvaluators.length; i++) {
-      ObjectInspector roi = aggregationEvaluators[i].init(conf.getAggregators()
-          .get(i).getMode(), aggregationParameterObjectInspectors[i]);
-      objectInspectors.add(roi);
-    }
-
-    bucketGroup = conf.getBucketGroup();
-    aggregationsParametersLastInvoke = new Object[conf.getAggregators().size()][];
-    if (conf.getMode() != GroupByDesc.Mode.HASH || bucketGroup) {
-      aggregations = newAggregations();
-      hashAggr = false;
-    } else {
-      hashAggregations = new HashMap<KeyWrapper, AggregationBuffer[]>(256);
-      aggregations = newAggregations();
-      hashAggr = true;
-      keyPositionsSize = new ArrayList<Integer>();
-      aggrPositions = new ArrayList<varLenFields>();
-      groupbyMapAggrInterval = HiveConf.getIntVar(hconf,
-          HiveConf.ConfVars.HIVEGROUPBYMAPINTERVAL);
-
-      // compare every groupbyMapAggrInterval rows
-      numRowsCompareHashAggr = groupbyMapAggrInterval;
-      minReductionHashAggr = HiveConf.getFloatVar(hconf,
-          HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION);
-      groupKeyIsNotReduceKey = conf.getGroupKeyNotReductionKey();
-      if (groupKeyIsNotReduceKey) {
-        keysCurrentGroup = new HashSet<KeyWrapper>();
-      }
-    }
-
-    fieldNames = conf.getOutputColumnNames();
-
-    for (int i = 0; i < keyFields.length; i++) {
-      objectInspectors.set(i, currentKeyObjectInspectors[i]);
-    }
-
-    // Generate key names
-    ArrayList<String> keyNames = new ArrayList<String>(keyFields.length);
-    for (int i = 0; i < keyFields.length; i++) {
-      keyNames.add(fieldNames.get(i));
-    }
-    newKeyObjectInspector = ObjectInspectorFactory
-        .getStandardStructObjectInspector(keyNames, Arrays
-        .asList(keyObjectInspectors));
-    currentKeyObjectInspector = ObjectInspectorFactory
-        .getStandardStructObjectInspector(keyNames, Arrays
-        .asList(currentKeyObjectInspectors));
-
-    outputObjInspector = ObjectInspectorFactory
-        .getStandardStructObjectInspector(fieldNames, objectInspectors);
-
-    keyWrapperFactory = new KeyWrapperFactory(keyFields, keyObjectInspectors, currentKeyObjectInspectors);
-
-    newKeys = keyWrapperFactory.getKeyWrapper();
-
-    firstRow = true;
-    // estimate the number of hash table entries based on the size of each
-    // entry. Since the size of a entry
-    // is not known, estimate that based on the number of entries
-    if (hashAggr) {
-      computeMaxEntriesHashAggr(hconf);
-    }
-    memoryMXBean = ManagementFactory.getMemoryMXBean();
-    maxMemory = memoryMXBean.getHeapMemoryUsage().getMax();
-    memoryThreshold = this.getConf().getMemoryThreshold();
-    initializeChildren(hconf);
-  }
-
-  /**
-   * Estimate the number of entries in map-side hash table. The user can specify
-   * the total amount of memory to be used by the map-side hash. By default, all
-   * available memory is used. The size of each row is estimated, rather
-   * crudely, and the number of entries are figure out based on that.
-   *
-   * @return number of entries that can fit in hash table - useful for map-side
-   *         aggregation only
-   **/
-  private void computeMaxEntriesHashAggr(Configuration hconf) throws HiveException {
-    float memoryPercentage = this.getConf().getGroupByMemoryUsage();
-    maxHashTblMemory = (long) (memoryPercentage * Runtime.getRuntime().maxMemory());
-    estimateRowSize();
-  }
-
-  private static final int javaObjectOverHead = 64;
-  private static final int javaHashEntryOverHead = 64;
-  private static final int javaSizePrimitiveType = 16;
-  private static final int javaSizeUnknownType = 256;
-
-  /**
-   * The size of the element at position 'pos' is returned, if possible. If the
-   * datatype is of variable length, STRING, a list of such key positions is
-   * maintained, and the size for such positions is then actually calculated at
-   * runtime.
-   *
-   * @param pos
-   *          the position of the key
-   * @param c
-   *          the type of the key
-   * @return the size of this datatype
-   **/
-  private int getSize(int pos, PrimitiveCategory category) {
-    switch (category) {
-    case VOID:
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-    case FLOAT:
-    case DOUBLE:
-      return javaSizePrimitiveType;
-    case STRING:
-      keyPositionsSize.add(new Integer(pos));
-      return javaObjectOverHead;
-    default:
-      return javaSizeUnknownType;
-    }
-  }
-
-  /**
-   * The size of the element at position 'pos' is returned, if possible. If the
-   * field is of variable length, STRING, a list of such field names for the
-   * field position is maintained, and the size for such positions is then
-   * actually calculated at runtime.
-   *
-   * @param pos
-   *          the position of the key
-   * @param c
-   *          the type of the key
-   * @param f
-   *          the field to be added
-   * @return the size of this datatype
-   **/
-  private int getSize(int pos, Class<?> c, Field f) {
-    if (c.isPrimitive()
-        || c.isInstance(new Boolean(true))
-        || c.isInstance(new Byte((byte) 0))
-        || c.isInstance(new Short((short) 0))
-        || c.isInstance(new Integer(0))
-        || c.isInstance(new Long(0))
-        || c.isInstance(new Float(0))
-        || c.isInstance(new Double(0))) {
-      return javaSizePrimitiveType;
-    }
-
-    if (c.isInstance(new String())) {
-      int idx = 0;
-      varLenFields v = null;
-      for (idx = 0; idx < aggrPositions.size(); idx++) {
-        v = aggrPositions.get(idx);
-        if (v.getAggrPos() == pos) {
-          break;
-        }
-      }
-
-      if (idx == aggrPositions.size()) {
-        v = new varLenFields(pos, new ArrayList<Field>());
-        aggrPositions.add(v);
-      }
-
-      v.getFields().add(f);
-      return javaObjectOverHead;
-    }
-
-    return javaSizeUnknownType;
-  }
-
-  /**
-   * @param pos
-   *          position of the key
-   * @param typeinfo
-   *          type of the input
-   * @return the size of this datatype
-   **/
-  private int getSize(int pos, TypeInfo typeInfo) {
-    if (typeInfo instanceof PrimitiveTypeInfo) {
-      return getSize(pos, ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory());
-    }
-    return javaSizeUnknownType;
-  }
-
-  /**
-   * @return the size of each row
-   **/
-  private void estimateRowSize() throws HiveException {
-    // estimate the size of each entry -
-    // a datatype with unknown size (String/Struct etc. - is assumed to be 256
-    // bytes for now).
-    // 64 bytes is the overhead for a reference
-    fixedRowSize = javaHashEntryOverHead;
-
-    ArrayList<ExprNodeDesc> keys = conf.getKeys();
-
-    // Go over all the keys and get the size of the fields of fixed length. Keep
-    // track of the variable length keys
-    for (int pos = 0; pos < keys.size(); pos++) {
-      fixedRowSize += getSize(pos, keys.get(pos).getTypeInfo());
-    }
-
-    // Go over all the aggregation classes and and get the size of the fields of
-    // fixed length. Keep track of the variable length
-    // fields in these aggregation classes.
-    for (int i = 0; i < aggregationEvaluators.length; i++) {
-
-      fixedRowSize += javaObjectOverHead;
-      Class<? extends AggregationBuffer> agg = aggregationEvaluators[i]
-          .getNewAggregationBuffer().getClass();
-      Field[] fArr = ObjectInspectorUtils.getDeclaredNonStaticFields(agg);
-      for (Field f : fArr) {
-        fixedRowSize += getSize(i, f.getType(), f);
-      }
-    }
-  }
-
-  protected AggregationBuffer[] newAggregations() throws HiveException {
-    AggregationBuffer[] aggs = new AggregationBuffer[aggregationEvaluators.length];
-    for (int i = 0; i < aggregationEvaluators.length; i++) {
-      aggs[i] = aggregationEvaluators[i].getNewAggregationBuffer();
-      // aggregationClasses[i].reset(aggs[i]);
-    }
-    return aggs;
-  }
-
-  protected void resetAggregations(AggregationBuffer[] aggs) throws HiveException {
-    for (int i = 0; i < aggs.length; i++) {
-      aggregationEvaluators[i].reset(aggs[i]);
-    }
-  }
-
-  /*
-   * Update aggregations. If the aggregation is for distinct, in case of hash
-   * aggregation, the client tells us whether it is a new entry. For sort-based
-   * aggregations, the last row is compared with the current one to figure out
-   * whether it has changed. As a cleanup, the lastInvoke logic can be pushed in
-   * the caller, and this function can be independent of that. The client should
-   * always notify whether it is a different row or not.
-   *
-   * @param aggs the aggregations to be evaluated
-   *
-   * @param row the row being processed
-   *
-   * @param rowInspector the inspector for the row
-   *
-   * @param hashAggr whether hash aggregation is being performed or not
-   *
-   * @param newEntryForHashAggr only valid if it is a hash aggregation, whether
-   * it is a new entry or not
-   */
-  protected void updateAggregations(AggregationBuffer[] aggs, Object row,
-      ObjectInspector rowInspector, boolean hashAggr,
-      boolean newEntryForHashAggr, Object[][] lastInvoke) throws HiveException {
-    if (unionExprEval == null) {
-      for (int ai = 0; ai < aggs.length; ai++) {
-        // Calculate the parameters
-        Object[] o = new Object[aggregationParameterFields[ai].length];
-        for (int pi = 0; pi < aggregationParameterFields[ai].length; pi++) {
-          o[pi] = aggregationParameterFields[ai][pi].evaluate(row);
-        }
-
-        // Update the aggregations.
-        if (aggregationIsDistinct[ai]) {
-          if (hashAggr) {
-            if (newEntryForHashAggr) {
-              aggregationEvaluators[ai].aggregate(aggs[ai], o);
-            }
-          } else {
-            if (lastInvoke[ai] == null) {
-              lastInvoke[ai] = new Object[o.length];
-            }
-            if (ObjectInspectorUtils.compare(o,
-                aggregationParameterObjectInspectors[ai], lastInvoke[ai],
-                aggregationParameterStandardObjectInspectors[ai]) != 0) {
-              aggregationEvaluators[ai].aggregate(aggs[ai], o);
-              for (int pi = 0; pi < o.length; pi++) {
-                lastInvoke[ai][pi] = ObjectInspectorUtils.copyToStandardObject(
-                    o[pi], aggregationParameterObjectInspectors[ai][pi],
-                    ObjectInspectorCopyOption.WRITABLE);
-              }
-            }
-          }
-        } else {
-          aggregationEvaluators[ai].aggregate(aggs[ai], o);
-        }
-      }
-      return;
-    }
-
-    if (distinctKeyAggrs.size() > 0) {
-      // evaluate union object
-      UnionObject uo = (UnionObject) (unionExprEval.evaluate(row));
-      int unionTag = uo.getTag();
-
-      // update non-distinct key aggregations : "KEY._colx:t._coly"
-      if (nonDistinctKeyAggrs.get(unionTag) != null) {
-        for (int pos : nonDistinctKeyAggrs.get(unionTag)) {
-          Object[] o = new Object[aggregationParameterFields[pos].length];
-          for (int pi = 0; pi < aggregationParameterFields[pos].length; pi++) {
-            o[pi] = aggregationParameterFields[pos][pi].evaluate(row);
-          }
-          aggregationEvaluators[pos].aggregate(aggs[pos], o);
-        }
-      }
-      // there may be multi distinct clauses for one column
-      // update them all.
-      if (distinctKeyAggrs.get(unionTag) != null) {
-        for (int i : distinctKeyAggrs.get(unionTag)) {
-          Object[] o = new Object[aggregationParameterFields[i].length];
-          for (int pi = 0; pi < aggregationParameterFields[i].length; pi++) {
-            o[pi] = aggregationParameterFields[i][pi].evaluate(row);
-          }
-
-          if (hashAggr) {
-            if (newEntryForHashAggr) {
-              aggregationEvaluators[i].aggregate(aggs[i], o);
-            }
-          } else {
-            if (lastInvoke[i] == null) {
-              lastInvoke[i] = new Object[o.length];
-            }
-            if (ObjectInspectorUtils.compare(o,
-                aggregationParameterObjectInspectors[i],
-                lastInvoke[i],
-                aggregationParameterStandardObjectInspectors[i]) != 0) {
-              aggregationEvaluators[i].aggregate(aggs[i], o);
-              for (int pi = 0; pi < o.length; pi++) {
-                lastInvoke[i][pi] = ObjectInspectorUtils.copyToStandardObject(
-                    o[pi], aggregationParameterObjectInspectors[i][pi],
-                    ObjectInspectorCopyOption.WRITABLE);
-              }
-            }
-          }
-        }
-      }
-
-      // update non-distinct value aggregations: 'VALUE._colx'
-      // these aggregations should be updated only once.
-      if (unionTag == 0) {
-        for (int pos : nonDistinctAggrs) {
-          Object[] o = new Object[aggregationParameterFields[pos].length];
-          for (int pi = 0; pi < aggregationParameterFields[pos].length; pi++) {
-            o[pi] = aggregationParameterFields[pos][pi].evaluate(row);
-          }
-          aggregationEvaluators[pos].aggregate(aggs[pos], o);
-        }
-      }
-    } else {
-      for (int ai = 0; ai < aggs.length; ai++) {
-        // there is no distinct aggregation,
-        // update all aggregations
-        Object[] o = new Object[aggregationParameterFields[ai].length];
-        for (int pi = 0; pi < aggregationParameterFields[ai].length; pi++) {
-          o[pi] = aggregationParameterFields[ai][pi].evaluate(row);
-        }
-        aggregationEvaluators[ai].aggregate(aggs[ai], o);
-      }
-    }
-  }
-
-  @Override
-  public void startGroup() throws HiveException {
-    firstRowInGroup = true;
-  }
-
-  @Override
-  public void endGroup() throws HiveException {
-    if (groupKeyIsNotReduceKey) {
-      keysCurrentGroup.clear();
-    }
-  }
-
-  @Override
-  public void processOp(Object row, int tag) throws HiveException {
-    firstRow = false;
-    ObjectInspector rowInspector = inputObjInspectors[tag];
-    // Total number of input rows is needed for hash aggregation only
-    if (hashAggr && !groupKeyIsNotReduceKey) {
-      numRowsInput++;
-      // if hash aggregation is not behaving properly, disable it
-      if (numRowsInput == numRowsCompareHashAggr) {
-        numRowsCompareHashAggr += groupbyMapAggrInterval;
-        // map-side aggregation should reduce the entries by at-least half
-        if (numRowsHashTbl > numRowsInput * minReductionHashAggr) {
-          LOG.warn("Disable Hash Aggr: #hash table = " + numRowsHashTbl
-              + " #total = " + numRowsInput + " reduction = " + 1.0
-              * (numRowsHashTbl / numRowsInput) + " minReduction = "
-              + minReductionHashAggr);
-          flush(true);
-          hashAggr = false;
-        } else {
-          LOG.trace("Hash Aggr Enabled: #hash table = " + numRowsHashTbl
-              + " #total = " + numRowsInput + " reduction = " + 1.0
-              * (numRowsHashTbl / numRowsInput) + " minReduction = "
-              + minReductionHashAggr);
-        }
-      }
-    }
-
-    try {
-      countAfterReport++;
-
-      newKeys.getNewKey(row, rowInspector);
-      if (hashAggr) {
-        newKeys.setHashKey();
-        processHashAggr(row, rowInspector, newKeys);
-      } else {
-        processAggr(row, rowInspector, newKeys);
-      }
-
-      firstRowInGroup = false;
-
-      if (countAfterReport != 0 && (countAfterReport % heartbeatInterval) == 0
-          && (reporter != null)) {
-        reporter.progress();
-        countAfterReport = 0;
-      }
-    } catch (HiveException e) {
-      throw e;
-    } catch (Exception e) {
-      throw new HiveException(e);
-    }
-  }
-
-  private void processHashAggr(Object row, ObjectInspector rowInspector,
-      KeyWrapper newKeys) throws HiveException {
-    // Prepare aggs for updating
-    AggregationBuffer[] aggs = null;
-    boolean newEntryForHashAggr = false;
-
-    // hash-based aggregations
-    aggs = hashAggregations.get(newKeys);
-    if (aggs == null) {
-      KeyWrapper newKeyProber = newKeys.copyKey();
-      aggs = newAggregations();
-      hashAggregations.put(newKeyProber, aggs);
-      newEntryForHashAggr = true;
-      numRowsHashTbl++; // new entry in the hash table
-    }
-
-    // If the grouping key and the reduction key are different, a set of
-    // grouping keys for the current reduction key are maintained in
-    // keysCurrentGroup
-    // Peek into the set to find out if a new grouping key is seen for the given
-    // reduction key
-    if (groupKeyIsNotReduceKey) {
-      newEntryForHashAggr = keysCurrentGroup.add(newKeys.copyKey());
-    }
-
-    // Update the aggs
-    updateAggregations(aggs, row, rowInspector, true, newEntryForHashAggr, null);
-
-    // We can only flush after the updateAggregations is done, or the
-    // potentially new entry "aggs"
-    // can be flushed out of the hash table.
-
-    // Based on user-specified parameters, check if the hash table needs to be
-    // flushed.
-    // If the grouping key is not the same as reduction key, flushing can only
-    // happen at boundaries
-    if ((!groupKeyIsNotReduceKey || firstRowInGroup)
-        && shouldBeFlushed(newKeys)) {
-      flush(false);
-    }
-  }
-
-  // Non-hash aggregation
-  private void processAggr(Object row, ObjectInspector rowInspector,
-      KeyWrapper newKeys) throws HiveException {
-    // Prepare aggs for updating
-    AggregationBuffer[] aggs = null;
-    Object[][] lastInvoke = null;
-    //boolean keysAreEqual = (currentKeys != null && newKeys != null)?
-    //  newKeyStructEqualComparer.areEqual(currentKeys, newKeys) : false;
-
-    boolean keysAreEqual = (currentKeys != null && newKeys != null)?
-        newKeys.equals(currentKeys) : false;
-
-
-    // Forward the current keys if needed for sort-based aggregation
-    if (currentKeys != null && !keysAreEqual) {
-      forward(currentKeys.getKeyArray(), aggregations);
-      countAfterReport = 0;
-    }
-
-    // Need to update the keys?
-    if (currentKeys == null || !keysAreEqual) {
-      if (currentKeys == null) {
-        currentKeys = newKeys.copyKey();
-      } else {
-        currentKeys.copyKey(newKeys);
-      }
-
-      // Reset the aggregations
-      resetAggregations(aggregations);
-
-      // clear parameters in last-invoke
-      for (int i = 0; i < aggregationsParametersLastInvoke.length; i++) {
-        aggregationsParametersLastInvoke[i] = null;
-      }
-    }
-
-    aggs = aggregations;
-
-    lastInvoke = aggregationsParametersLastInvoke;
-    // Update the aggs
-
-    updateAggregations(aggs, row, rowInspector, false, false, lastInvoke);
-  }
-
-  /**
-   * Based on user-parameters, should the hash table be flushed.
-   *
-   * @param newKeys
-   *          keys for the row under consideration
-   **/
-  private boolean shouldBeFlushed(KeyWrapper newKeys) {
-    int numEntries = hashAggregations.size();
-    long usedMemory;
-    float rate;
-
-    // The fixed size for the aggregation class is already known. Get the
-    // variable portion of the size every NUMROWSESTIMATESIZE rows.
-    if ((numEntriesHashTable == 0) || ((numEntries % NUMROWSESTIMATESIZE) == 0)) {
-      //check how much memory left memory
-      usedMemory = memoryMXBean.getHeapMemoryUsage().getUsed();
-      rate = (float) usedMemory / (float) maxMemory;
-      if(rate > memoryThreshold){
-        return true;
-      }
-      for (Integer pos : keyPositionsSize) {
-        Object key = newKeys.getKeyArray()[pos.intValue()];
-        // Ignore nulls
-        if (key != null) {
-          if (key instanceof LazyPrimitive) {
-              totalVariableSize +=
-                  ((LazyPrimitive<LazyStringObjectInspector, Text>) key).
-                      getWritableObject().getLength();
-          } else if (key instanceof String) {
-            totalVariableSize += ((String) key).length();
-          } else if (key instanceof Text) {
-            totalVariableSize += ((Text) key).getLength();
-          }
-        }
-      }
-
-      AggregationBuffer[] aggs = null;
-      if (aggrPositions.size() > 0) {
-        KeyWrapper newKeyProber = newKeys.copyKey();
-        aggs = hashAggregations.get(newKeyProber);
-      }
-
-      for (varLenFields v : aggrPositions) {
-        int aggrPos = v.getAggrPos();
-        List<Field> fieldsVarLen = v.getFields();
-        AggregationBuffer agg = aggs[aggrPos];
-
-        try {
-          for (Field f : fieldsVarLen) {
-            totalVariableSize += ((String) f.get(agg)).length();
-          }
-        } catch (IllegalAccessException e) {
-          assert false;
-        }
-      }
-
-      numEntriesVarSize++;
-
-      // Update the number of entries that can fit in the hash table
-      numEntriesHashTable =
-          (int) (maxHashTblMemory / (fixedRowSize + (totalVariableSize / numEntriesVarSize)));
-      LOG.trace("Hash Aggr: #hash table = " + numEntries
-          + " #max in hash table = " + numEntriesHashTable);
-    }
-
-    // flush if necessary
-    if (numEntries >= numEntriesHashTable) {
-      return true;
-    }
-    return false;
-  }
-
-  private void flush(boolean complete) throws HiveException {
-
-    countAfterReport = 0;
-
-    // Currently, the algorithm flushes 10% of the entries - this can be
-    // changed in the future
-
-    if (complete) {
-      Iterator<Map.Entry<KeyWrapper, AggregationBuffer[]>> iter = hashAggregations
-          .entrySet().iterator();
-      while (iter.hasNext()) {
-        Map.Entry<KeyWrapper, AggregationBuffer[]> m = iter.next();
-        forward(m.getKey().getKeyArray(), m.getValue());
-      }
-      hashAggregations.clear();
-      hashAggregations = null;
-      LOG.warn("Hash Table completed flushed");
-      return;
-    }
-
-    int oldSize = hashAggregations.size();
-    LOG.warn("Hash Tbl flush: #hash table = " + oldSize);
-    Iterator<Map.Entry<KeyWrapper, AggregationBuffer[]>> iter = hashAggregations
-        .entrySet().iterator();
-    int numDel = 0;
-    while (iter.hasNext()) {
-      Map.Entry<KeyWrapper, AggregationBuffer[]> m = iter.next();
-      forward(m.getKey().getKeyArray(), m.getValue());
-      iter.remove();
-      numDel++;
-      if (numDel * 10 >= oldSize) {
-        LOG.warn("Hash Table flushed: new size = " + hashAggregations.size());
-        return;
-      }
-    }
-  }
-
-  transient Object[] forwardCache;
-
-  /**
-   * Forward a record of keys and aggregation results.
-   *
-   * @param keys
-   *          The keys in the record
-   * @throws HiveException
-   */
-  protected void forward(Object[] keys, AggregationBuffer[] aggs)
-      throws HiveException {
-    int totalFields = keys.length+ aggs.length;
-    if (forwardCache == null) {
-      forwardCache = new Object[totalFields];
-    }
-    for (int i = 0; i < keys.length; i++) {
-      forwardCache[i] = keys[i];
-    }
-    for (int i = 0; i < aggs.length; i++) {
-      forwardCache[keys.length + i] = aggregationEvaluators[i]
-          .evaluate(aggs[i]);
-    }
-
-    forward(forwardCache, outputObjInspector);
-  }
-
-  /**
-   * We need to forward all the aggregations to children.
-   *
-   */
-  @Override
-  public void closeOp(boolean abort) throws HiveException {
-    if (!abort) {
-      try {
-        // If there is no grouping key and no row came to this operator
-        if (firstRow && (keyFields.length == 0)) {
-          firstRow = false;
-
-          // There is no grouping key - simulate a null row
-          // This is based on the assumption that a null row is ignored by
-          // aggregation functions
-          for (int ai = 0; ai < aggregations.length; ai++) {
-
-            // o is set to NULL in order to distinguish no rows at all
-            Object[] o;
-            if (aggregationParameterFields[ai].length > 0) {
-              o = new Object[aggregationParameterFields[ai].length];
-            } else {
-              o = null;
-            }
-
-            // Calculate the parameters
-            for (int pi = 0; pi < aggregationParameterFields[ai].length; pi++) {
-              o[pi] = null;
-            }
-            aggregationEvaluators[ai].aggregate(aggregations[ai], o);
-          }
-
-          // create dummy keys - size 0
-          forward(new Object[0], aggregations);
-        } else {
-          if (hashAggregations != null) {
-            LOG.warn("Begin Hash Table flush at close: size = "
-                + hashAggregations.size());
-            Iterator iter = hashAggregations.entrySet().iterator();
-            while (iter.hasNext()) {
-              Map.Entry<KeyWrapper, AggregationBuffer[]> m = (Map.Entry) iter
-                  .next();
-
-              forward(m.getKey().getKeyArray(), m.getValue());
-              iter.remove();
-            }
-            hashAggregations.clear();
-          } else if (aggregations != null) {
-            // sort-based aggregations
-            if (currentKeys != null) {
-              forward(currentKeys.getKeyArray(), aggregations);
-            }
-            currentKeys = null;
-          } else {
-            // The GroupByOperator is not initialized, which means there is no
-            // data
-            // (since we initialize the operators when we see the first record).
-            // Just do nothing here.
-          }
-        }
-      } catch (Exception e) {
-        e.printStackTrace();
-        throw new HiveException(e);
-      }
-    }
-  }
-
-  // Group by contains the columns needed - no need to aggregate from children
-  public List<String> genColLists(
-      HashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx) {
-    List<String> colLists = new ArrayList<String>();
-    ArrayList<ExprNodeDesc> keys = conf.getKeys();
-    for (ExprNodeDesc key : keys) {
-      colLists = Utilities.mergeUniqElems(colLists, key.getCols());
-    }
-
-    ArrayList<AggregationDesc> aggrs = conf.getAggregators();
-    for (AggregationDesc aggr : aggrs) {
-      ArrayList<ExprNodeDesc> params = aggr.getParameters();
-      for (ExprNodeDesc param : params) {
-        colLists = Utilities.mergeUniqElems(colLists, param.getCols());
-      }
-    }
-
-    return colLists;
-  }
-
-  /**
-   * @return the name of the operator
-   */
-  @Override
-  public String getName() {
-    return new String("GBY");
-  }
-
-  @Override
-  public int getType() {
-    return OperatorType.GROUPBY;
-  }
-}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java.orig b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java.orig
deleted file mode 100644
index 13892b5079..0000000000
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java.orig
+++ /dev/null
@@ -1,449 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.exec;
-
-import java.io.File;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.persistence.AbstractMapJoinKey;
-import org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper;
-import org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectValue;
-import org.apache.hadoop.hive.ql.exec.persistence.MapJoinRowContainer;
-import org.apache.hadoop.hive.ql.exec.persistence.RowContainer;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.plan.HashTableSinkDesc;
-import org.apache.hadoop.hive.ql.plan.TableDesc;
-import org.apache.hadoop.hive.ql.plan.api.OperatorType;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-import org.apache.hadoop.hive.serde2.SerDe;
-import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
-import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.util.ReflectionUtils;
-
-
-public class HashTableSinkOperator extends TerminalOperator<HashTableSinkDesc> implements
-    Serializable {
-  private static final long serialVersionUID = 1L;
-  private static final Log LOG = LogFactory.getLog(HashTableSinkOperator.class.getName());
-
-  // from abstract map join operator
-  /**
-   * The expressions for join inputs's join keys.
-   */
-  protected transient Map<Byte, List<ExprNodeEvaluator>> joinKeys;
-  /**
-   * The ObjectInspectors for the join inputs's join keys.
-   */
-  protected transient Map<Byte, List<ObjectInspector>> joinKeysObjectInspectors;
-  /**
-   * The standard ObjectInspectors for the join inputs's join keys.
-   */
-  protected transient Map<Byte, List<ObjectInspector>> joinKeysStandardObjectInspectors;
-
-  protected transient int posBigTableTag = -1; // one of the tables that is not in memory
-  protected transient int posBigTableAlias = -1; // one of the tables that is not in memory
-  transient int mapJoinRowsKey; // rows for a given key
-
-  protected transient RowContainer<ArrayList<Object>> emptyList = null;
-
-  transient int numMapRowsRead;
-  protected transient int totalSz; // total size of the composite object
-  transient boolean firstRow;
-  /**
-   * The filters for join
-   */
-  protected transient Map<Byte, List<ExprNodeEvaluator>> joinFilters;
-
-  protected transient int numAliases; // number of aliases
-  /**
-   * The expressions for join outputs.
-   */
-  protected transient Map<Byte, List<ExprNodeEvaluator>> joinValues;
-  /**
-   * The ObjectInspectors for the join inputs.
-   */
-  protected transient Map<Byte, List<ObjectInspector>> joinValuesObjectInspectors;
-  /**
-   * The ObjectInspectors for join filters.
-   */
-  protected transient Map<Byte, List<ObjectInspector>> joinFilterObjectInspectors;
-  /**
-   * The standard ObjectInspectors for the join inputs.
-   */
-  protected transient Map<Byte, List<ObjectInspector>> joinValuesStandardObjectInspectors;
-
-  protected transient Map<Byte, List<ObjectInspector>> rowContainerStandardObjectInspectors;
-
-  protected transient Byte[] order; // order in which the results should
-  Configuration hconf;
-  protected transient Byte alias;
-  protected transient Map<Byte, TableDesc> spillTableDesc; // spill tables are
-
-  protected transient Map<Byte, HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue>> mapJoinTables;
-  protected transient boolean noOuterJoin;
-
-  private long rowNumber = 0;
-  protected transient LogHelper console;
-  private long hashTableScale;
-  private boolean isAbort = false;
-
-  public static class HashTableSinkObjectCtx {
-    ObjectInspector standardOI;
-    SerDe serde;
-    TableDesc tblDesc;
-    Configuration conf;
-
-    /**
-     * @param standardOI
-     * @param serde
-     */
-    public HashTableSinkObjectCtx(ObjectInspector standardOI, SerDe serde, TableDesc tblDesc,
-        Configuration conf) {
-      this.standardOI = standardOI;
-      this.serde = serde;
-      this.tblDesc = tblDesc;
-      this.conf = conf;
-    }
-
-    /**
-     * @return the standardOI
-     */
-    public ObjectInspector getStandardOI() {
-      return standardOI;
-    }
-
-    /**
-     * @return the serde
-     */
-    public SerDe getSerDe() {
-      return serde;
-    }
-
-    public TableDesc getTblDesc() {
-      return tblDesc;
-    }
-
-    public Configuration getConf() {
-      return conf;
-    }
-
-  }
-
-  private static final transient String[] FATAL_ERR_MSG = {
-      null, // counter value 0 means no error
-      "Mapside join size exceeds hive.mapjoin.maxsize. "
-          + "Please increase that or remove the mapjoin hint."};
-  private final int metadataKeyTag = -1;
-  transient int[] metadataValueTag;
-  transient int maxMapJoinSize;
-
-
-  public HashTableSinkOperator() {
-  }
-
-  public HashTableSinkOperator(MapJoinOperator mjop) {
-    this.conf = new HashTableSinkDesc(mjop.getConf());
-  }
-
-
-  @Override
-  protected void initializeOp(Configuration hconf) throws HiveException {
-    boolean isSilent = HiveConf.getBoolVar(hconf, HiveConf.ConfVars.HIVESESSIONSILENT);
-    console = new LogHelper(LOG, isSilent);
-    maxMapJoinSize = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVEMAXMAPJOINSIZE);
-
-    numMapRowsRead = 0;
-    firstRow = true;
-
-    // for small tables only; so get the big table position first
-    posBigTableTag = conf.getPosBigTable();
-
-    order = conf.getTagOrder();
-
-    posBigTableAlias = order[posBigTableTag];
-
-    // initialize some variables, which used to be initialized in CommonJoinOperator
-    numAliases = conf.getExprs().size();
-    this.hconf = hconf;
-    totalSz = 0;
-
-    noOuterJoin = conf.isNoOuterJoin();
-
-    // process join keys
-    joinKeys = new HashMap<Byte, List<ExprNodeEvaluator>>();
-    JoinUtil.populateJoinKeyValue(joinKeys, conf.getKeys(), order, posBigTableAlias);
-    joinKeysObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinKeys,
-        inputObjInspectors, posBigTableAlias);
-    joinKeysStandardObjectInspectors = JoinUtil.getStandardObjectInspectors(
-        joinKeysObjectInspectors, posBigTableAlias);
-
-    // process join values
-    joinValues = new HashMap<Byte, List<ExprNodeEvaluator>>();
-    JoinUtil.populateJoinKeyValue(joinValues, conf.getExprs(), order, posBigTableAlias);
-    joinValuesObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinValues,
-        inputObjInspectors, posBigTableAlias);
-    joinValuesStandardObjectInspectors = JoinUtil.getStandardObjectInspectors(
-        joinValuesObjectInspectors, posBigTableAlias);
-
-    // process join filters
-    joinFilters = new HashMap<Byte, List<ExprNodeEvaluator>>();
-    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(), order, posBigTableAlias);
-    joinFilterObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinFilters,
-        inputObjInspectors, posBigTableAlias);
-
-    if (noOuterJoin) {
-      rowContainerStandardObjectInspectors = joinValuesStandardObjectInspectors;
-    } else {
-      Map<Byte, List<ObjectInspector>> rowContainerObjectInspectors = new HashMap<Byte, List<ObjectInspector>>();
-      for (Byte alias : order) {
-        if (alias == posBigTableAlias) {
-          continue;
-        }
-        ArrayList<ObjectInspector> rcOIs = new ArrayList<ObjectInspector>();
-        rcOIs.addAll(joinValuesObjectInspectors.get(alias));
-        // for each alias, add object inspector for boolean as the last element
-        rcOIs.add(PrimitiveObjectInspectorFactory.writableBooleanObjectInspector);
-        rowContainerObjectInspectors.put(alias, rcOIs);
-      }
-      rowContainerStandardObjectInspectors = getStandardObjectInspectors(rowContainerObjectInspectors);
-    }
-
-    metadataValueTag = new int[numAliases];
-    for (int pos = 0; pos < numAliases; pos++) {
-      metadataValueTag[pos] = -1;
-    }
-    mapJoinTables = new HashMap<Byte, HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue>>();
-
-    int hashTableThreshold = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVEHASHTABLETHRESHOLD);
-    float hashTableLoadFactor = HiveConf.getFloatVar(hconf,
-        HiveConf.ConfVars.HIVEHASHTABLELOADFACTOR);
-    float hashTableMaxMemoryUsage = this.getConf().getHashtableMemoryUsage();
-
-    hashTableScale = HiveConf.getLongVar(hconf, HiveConf.ConfVars.HIVEHASHTABLESCALE);
-    if (hashTableScale <= 0) {
-      hashTableScale = 1;
-    }
-
-    // initialize the hash tables for other tables
-    for (Byte pos : order) {
-      if (pos == posBigTableTag) {
-        continue;
-      }
-
-      HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = new HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue>(
-          hashTableThreshold, hashTableLoadFactor, hashTableMaxMemoryUsage);
-
-      mapJoinTables.put(pos, hashTable);
-    }
-  }
-
-
-
-  protected static HashMap<Byte, List<ObjectInspector>> getStandardObjectInspectors(
-      Map<Byte, List<ObjectInspector>> aliasToObjectInspectors) {
-    HashMap<Byte, List<ObjectInspector>> result = new HashMap<Byte, List<ObjectInspector>>();
-    for (Entry<Byte, List<ObjectInspector>> oiEntry : aliasToObjectInspectors.entrySet()) {
-      Byte alias = oiEntry.getKey();
-      List<ObjectInspector> oiList = oiEntry.getValue();
-      ArrayList<ObjectInspector> fieldOIList = new ArrayList<ObjectInspector>(oiList.size());
-      for (int i = 0; i < oiList.size(); i++) {
-        fieldOIList.add(ObjectInspectorUtils.getStandardObjectInspector(oiList.get(i),
-            ObjectInspectorCopyOption.WRITABLE));
-      }
-      result.put(alias, fieldOIList);
-    }
-    return result;
-
-  }
-
-  private void setKeyMetaData() throws SerDeException {
-    TableDesc keyTableDesc = conf.getKeyTblDesc();
-    SerDe keySerializer = (SerDe) ReflectionUtils.newInstance(keyTableDesc.getDeserializerClass(),
-        null);
-    keySerializer.initialize(null, keyTableDesc.getProperties());
-
-    MapJoinMetaData.clear();
-    MapJoinMetaData.put(Integer.valueOf(metadataKeyTag), new HashTableSinkObjectCtx(
-        ObjectInspectorUtils.getStandardObjectInspector(keySerializer.getObjectInspector(),
-            ObjectInspectorCopyOption.WRITABLE), keySerializer, keyTableDesc, hconf));
-  }
-
-  /*
-   * This operator only process small tables Read the key/value pairs Load them into hashtable
-   */
-  @Override
-  public void processOp(Object row, int tag) throws HiveException {
-    // let the mapJoinOp process these small tables
-    try {
-      if (firstRow) {
-        // generate the map metadata
-        setKeyMetaData();
-        firstRow = false;
-      }
-      alias = order[tag];
-      // alias = (byte)tag;
-
-      // compute keys and values as StandardObjects
-      AbstractMapJoinKey keyMap = JoinUtil.computeMapJoinKeys(row, joinKeys.get(alias),
-          joinKeysObjectInspectors.get(alias));
-
-      Object[] value = JoinUtil.computeMapJoinValues(row, joinValues.get(alias),
-          joinValuesObjectInspectors.get(alias), joinFilters.get(alias), joinFilterObjectInspectors
-              .get(alias), noOuterJoin);
-
-
-      HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = mapJoinTables
-          .get((byte) tag);
-
-      MapJoinObjectValue o = hashTable.get(keyMap);
-      MapJoinRowContainer<Object[]> res = null;
-
-      boolean needNewKey = true;
-      if (o == null) {
-        res = new MapJoinRowContainer<Object[]>();
-        res.add(value);
-
-        if (metadataValueTag[tag] == -1) {
-          metadataValueTag[tag] = order[tag];
-          setValueMetaData(tag);
-        }
-
-        // Construct externalizable objects for key and value
-        if (needNewKey) {
-          MapJoinObjectValue valueObj = new MapJoinObjectValue(metadataValueTag[tag], res);
-
-          rowNumber++;
-          if (rowNumber > hashTableScale && rowNumber % hashTableScale == 0) {
-            isAbort = hashTable.isAbort(rowNumber, console);
-            if (isAbort) {
-              throw new HiveException("RunOutOfMeomoryUsage");
-            }
-          }
-          hashTable.put(keyMap, valueObj);
-        }
-
-      } else {
-        res = o.getObj();
-        res.add(value);
-      }
-
-
-    } catch (SerDeException e) {
-      throw new HiveException(e);
-    }
-
-  }
-
-  private void setValueMetaData(int tag) throws SerDeException {
-    TableDesc valueTableDesc = conf.getValueTblFilteredDescs().get(tag);
-    SerDe valueSerDe = (SerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(),
-        null);
-
-    valueSerDe.initialize(null, valueTableDesc.getProperties());
-
-    List<ObjectInspector> newFields = rowContainerStandardObjectInspectors.get((Byte) alias);
-    int length = newFields.size();
-    List<String> newNames = new ArrayList<String>(length);
-    for (int i = 0; i < length; i++) {
-      String tmp = new String("tmp_" + i);
-      newNames.add(tmp);
-    }
-    StandardStructObjectInspector standardOI = ObjectInspectorFactory
-        .getStandardStructObjectInspector(newNames, newFields);
-
-    MapJoinMetaData.put(Integer.valueOf(metadataValueTag[tag]), new HashTableSinkObjectCtx(
-        standardOI, valueSerDe, valueTableDesc, hconf));
-  }
-
-  @Override
-  public void closeOp(boolean abort) throws HiveException {
-    try {
-      if (mapJoinTables != null) {
-        // get tmp file URI
-        String tmpURI = this.getExecContext().getLocalWork().getTmpFileURI();
-        LOG.info("Get TMP URI: " + tmpURI);
-        long fileLength;
-        for (Map.Entry<Byte, HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue>> hashTables : mapJoinTables
-            .entrySet()) {
-          // get the key and value
-          Byte tag = hashTables.getKey();
-          HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = hashTables.getValue();
-
-          // get current input file name
-          String bigBucketFileName = this.getExecContext().getCurrentBigBucketFile();
-          if (bigBucketFileName == null || bigBucketFileName.length() == 0) {
-            bigBucketFileName = "-";
-          }
-          // get the tmp URI path; it will be a hdfs path if not local mode
-          String tmpURIPath = Utilities.generatePath(tmpURI, tag, bigBucketFileName);
-          hashTable.isAbort(rowNumber, console);
-          console.printInfo(Utilities.now() + "\tDump the hashtable into file: " + tmpURIPath);
-          // get the hashtable file and path
-          Path path = new Path(tmpURIPath);
-          FileSystem fs = path.getFileSystem(hconf);
-          File file = new File(path.toUri().getPath());
-          fs.create(path);
-          fileLength = hashTable.flushMemoryCacheToPersistent(file);
-          console.printInfo(Utilities.now() + "\tUpload 1 File to: " + tmpURIPath + " File size: "
-              + fileLength);
-
-          hashTable.close();
-        }
-      }
-
-      super.closeOp(abort);
-    } catch (Exception e) {
-      LOG.error("Generate Hashtable error");
-      e.printStackTrace();
-    }
-  }
-
-  /**
-   * Implements the getName function for the Node Interface.
-   *
-   * @return the name of the operator
-   */
-  @Override
-  public String getName() {
-    return "HASHTABLESINK";
-  }
-
-  @Override
-  public int getType() {
-    return OperatorType.HASHTABLESINK;
-  }
-
-
-
-}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index c075cd2542..19c25d8113 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -171,8 +171,7 @@ public int execute(DriverContext driverContext) {
         if (tbd.getPartitionSpec().size() == 0) {
           dc = new DataContainer(table.getTTable());
           db.loadTable(new Path(tbd.getSourceDir()), tbd.getTable()
-              .getTableName(), tbd.getReplace(), new Path(tbd.getTmpDir()),
-              tbd.getHoldDDLTime());
+              .getTableName(), tbd.getReplace(), tbd.getHoldDDLTime());
           if (work.getOutputs() != null) {
             work.getOutputs().add(new WriteEntity(table));
           }
@@ -202,7 +201,6 @@ public int execute(DriverContext driverContext) {
                   tbd.getTable().getTableName(),
                 	tbd.getPartitionSpec(),
                 	tbd.getReplace(),
-                	new Path(tbd.getTmpDir()),
                 	dpCtx.getNumDPCols(),
                 	tbd.getHoldDDLTime());
 
@@ -243,8 +241,7 @@ public int execute(DriverContext driverContext) {
             dc = null; // reset data container to prevent it being added again.
           } else { // static partitions
             db.loadPartition(new Path(tbd.getSourceDir()), tbd.getTable().getTableName(),
-                tbd.getPartitionSpec(), tbd.getReplace(), new Path(tbd.getTmpDir()),
-                tbd.getHoldDDLTime());
+                tbd.getPartitionSpec(), tbd.getReplace(), tbd.getHoldDDLTime());
           	Partition partn = db.getPartition(table, tbd.getPartitionSpec(), false);
           	dc = new DataContainer(table.getTTable(), partn.getTPartition());
           	// add this partition to post-execution hook
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 8cba80701d..07517ab578 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -171,7 +171,7 @@ public static MapredWork getMapRedWork(Configuration job) {
       if (gWork == null) {
         String jtConf = HiveConf.getVar(job, HiveConf.ConfVars.HADOOPJT);
         String path;
-        if (jtConf == "local") {
+        if (jtConf.equals("local")) {
           String planPath = HiveConf.getVar(job, HiveConf.ConfVars.PLAN);
           path = new Path(planPath).toUri().getPath();
         } else {
