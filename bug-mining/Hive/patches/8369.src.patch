diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
index 61ed0ce37d..c1f5d8ca32 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
@@ -26,7 +26,6 @@
 import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import org.apache.hadoop.hive.common.type.DataTypePhysicalVariation;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.VectorPartitionConversion;
@@ -87,7 +86,6 @@ public final class VectorDeserializeRow<T extends DeserializeRead> {
   private T deserializeRead;
 
   private TypeInfo[] sourceTypeInfos;
-  protected DataTypePhysicalVariation[] dataTypePhysicalVariations;
 
   private byte[] inputBytes;
 
@@ -99,7 +97,6 @@ public VectorDeserializeRow(T deserializeRead) {
     this();
     this.deserializeRead = deserializeRead;
     sourceTypeInfos = deserializeRead.typeInfos();
-    dataTypePhysicalVariations = deserializeRead.getDataTypePhysicalVariations();
   }
 
   // Not public since we must have the deserialize read object.
@@ -115,8 +112,6 @@ private static class Field {
     private PrimitiveCategory primitiveCategory;
                   //The data type primitive category of the column being deserialized.
 
-    private DataTypePhysicalVariation dataTypePhysicalVariation;
-
     private int maxLength;
                   // For the CHAR and VARCHAR data types, the maximum character length of
                   // the column.  Otherwise, 0.
@@ -139,12 +134,11 @@ private static class Field {
 
     private VectorBatchDeserializer deserializer;
 
-    public Field(PrimitiveCategory primitiveCategory, DataTypePhysicalVariation dataTypePhysicalVariation,
+    public Field(PrimitiveCategory primitiveCategory,
         int maxLength, VectorBatchDeserializer deserializer) {
       isPrimitive = true;
       this.category = Category.PRIMITIVE;
       this.primitiveCategory = primitiveCategory;
-      this.dataTypePhysicalVariation = dataTypePhysicalVariation;
       this.maxLength = maxLength;
       this.isConvert = false;
       this.conversionWritable = null;
@@ -160,7 +154,6 @@ public Field(Category category, ComplexTypeHelper complexTypeHelper, TypeInfo ty
       this.category = category;
       this.objectInspector = TypeInfoUtils.getStandardWritableObjectInspectorFromTypeInfo(typeInfo);
       this.primitiveCategory = null;
-      this.dataTypePhysicalVariation = null;
       this.maxLength = 0;
       this.isConvert = false;
       this.conversionWritable = null;
@@ -180,10 +173,6 @@ public PrimitiveCategory getPrimitiveCategory() {
       return primitiveCategory;
     }
 
-    public DataTypePhysicalVariation getDataTypePhysicalVariation() {
-      return dataTypePhysicalVariation;
-    }
-
     public void setMaxLength(int maxLength) {
       this.maxLength = maxLength;
     }
@@ -253,8 +242,7 @@ private void allocateArrays(int count) {
     topLevelFields = new Field[count];
   }
 
-  private Field allocatePrimitiveField(TypeInfo sourceTypeInfo,
-      DataTypePhysicalVariation dataTypePhysicalVariation) {
+  private Field allocatePrimitiveField(TypeInfo sourceTypeInfo) {
     final PrimitiveTypeInfo sourcePrimitiveTypeInfo = (PrimitiveTypeInfo) sourceTypeInfo;
     final PrimitiveCategory sourcePrimitiveCategory = sourcePrimitiveTypeInfo.getPrimitiveCategory();
     int maxLength = 0;
@@ -318,7 +306,7 @@ private Field allocatePrimitiveField(TypeInfo sourceTypeInfo,
         throw new RuntimeException("Primitive category " + sourcePrimitiveCategory +
                 " not supported");
     }
-    return new Field(sourcePrimitiveCategory, dataTypePhysicalVariation, maxLength, deserializer);
+    return new Field(sourcePrimitiveCategory, maxLength, deserializer);
   }
 
   private Field allocateComplexField(TypeInfo sourceTypeInfo) {
@@ -329,7 +317,7 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
         final ListTypeInfo listTypeInfo = (ListTypeInfo) sourceTypeInfo;
         final ListComplexTypeHelper listHelper =
             new ListComplexTypeHelper(
-                allocateField(listTypeInfo.getListElementTypeInfo(), DataTypePhysicalVariation.NONE));
+                allocateField(listTypeInfo.getListElementTypeInfo()));
         return new Field(category, listHelper, sourceTypeInfo, new VectorListDeserializer());
       }
     case MAP:
@@ -337,8 +325,8 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
         final MapTypeInfo mapTypeInfo = (MapTypeInfo) sourceTypeInfo;
         final MapComplexTypeHelper mapHelper =
             new MapComplexTypeHelper(
-                allocateField(mapTypeInfo.getMapKeyTypeInfo(), DataTypePhysicalVariation.NONE),
-                allocateField(mapTypeInfo.getMapValueTypeInfo(), DataTypePhysicalVariation.NONE));
+                allocateField(mapTypeInfo.getMapKeyTypeInfo()),
+                allocateField(mapTypeInfo.getMapValueTypeInfo()));
         return new Field(category, mapHelper, sourceTypeInfo, new VectorMapDeserializer());
       }
     case STRUCT:
@@ -348,7 +336,7 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
         final int count = fieldTypeInfoList.size();
         final Field[] fields = new Field[count];
         for (int i = 0; i < count; i++) {
-          fields[i] = allocateField(fieldTypeInfoList.get(i), DataTypePhysicalVariation.NONE);
+          fields[i] = allocateField(fieldTypeInfoList.get(i));
         }
         final StructComplexTypeHelper structHelper =
             new StructComplexTypeHelper(fields);
@@ -361,7 +349,7 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
         final int count = fieldTypeInfoList.size();
         final Field[] fields = new Field[count];
         for (int i = 0; i < count; i++) {
-          fields[i] = allocateField(fieldTypeInfoList.get(i), DataTypePhysicalVariation.NONE);
+          fields[i] = allocateField(fieldTypeInfoList.get(i));
         }
         final UnionComplexTypeHelper unionHelper =
             new UnionComplexTypeHelper(fields);
@@ -372,10 +360,10 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
     }
   }
 
-  private Field allocateField(TypeInfo sourceTypeInfo, DataTypePhysicalVariation dataTypePhysicalVariation) {
+  private Field allocateField(TypeInfo sourceTypeInfo) {
     switch (sourceTypeInfo.getCategory()) {
     case PRIMITIVE:
-      return allocatePrimitiveField(sourceTypeInfo, dataTypePhysicalVariation);
+      return allocatePrimitiveField(sourceTypeInfo);
     case LIST:
     case MAP:
     case STRUCT:
@@ -390,11 +378,11 @@ private Field allocateField(TypeInfo sourceTypeInfo, DataTypePhysicalVariation d
    * Initialize one column's source deserializtion information.
    */
   private void initTopLevelField(int logicalColumnIndex, int projectionColumnNum,
-      TypeInfo sourceTypeInfo, DataTypePhysicalVariation dataTypePhysicalVariation) {
+      TypeInfo sourceTypeInfo) {
 
     projectionColumnNums[logicalColumnIndex] = projectionColumnNum;
 
-    topLevelFields[logicalColumnIndex] = allocateField(sourceTypeInfo, dataTypePhysicalVariation);
+    topLevelFields[logicalColumnIndex] = allocateField(sourceTypeInfo);
   }
 
   /*
@@ -434,7 +422,7 @@ public void init(int[] outputColumns) throws HiveException {
 
     for (int i = 0; i < count; i++) {
       int outputColumn = outputColumns[i];
-      initTopLevelField(i, outputColumn, sourceTypeInfos[i], dataTypePhysicalVariations[i]);
+      initTopLevelField(i, outputColumn, sourceTypeInfos[i]);
     }
   }
 
@@ -448,7 +436,7 @@ public void init(List<Integer> outputColumns) throws HiveException {
 
     for (int i = 0; i < count; i++) {
       int outputColumn = outputColumns.get(i);
-      initTopLevelField(i, outputColumn, sourceTypeInfos[i], dataTypePhysicalVariations[i]);
+      initTopLevelField(i, outputColumn, sourceTypeInfos[i]);
     }
   }
 
@@ -462,7 +450,7 @@ public void init(int startColumn) throws HiveException {
 
     for (int i = 0; i < count; i++) {
       int outputColumn = startColumn + i;
-      initTopLevelField(i, outputColumn, sourceTypeInfos[i], dataTypePhysicalVariations[i]);
+      initTopLevelField(i, outputColumn, sourceTypeInfos[i]);
     }
   }
 
@@ -488,7 +476,7 @@ public void init(boolean[] columnsToIncludeTruncated) throws HiveException {
 
       } else {
 
-        initTopLevelField(i, i, sourceTypeInfos[i], dataTypePhysicalVariations[i]);
+        initTopLevelField(i, i, sourceTypeInfos[i]);
         includedIndices[includedCount++] = i;
       }
     }
@@ -520,7 +508,7 @@ public void init(int[] outputColumns, boolean[] columnsToInclude) throws HiveExc
 
       } else {
 
-        initTopLevelField(i, outputColumns[i], sourceTypeInfos[i], dataTypePhysicalVariations[i]);
+        initTopLevelField(i, outputColumns[i], sourceTypeInfos[i]);
         includedIndices[includedCount++] = i;
       }
     }
@@ -579,12 +567,12 @@ public void initConversion(TypeInfo[] targetTypeInfos,
           if (VectorPartitionConversion.isImplicitVectorColumnConversion(sourceTypeInfo, targetTypeInfo)) {
 
             // Do implicit conversion from source type to target type.
-            initTopLevelField(i, i, sourceTypeInfo, dataTypePhysicalVariations[i]);
+            initTopLevelField(i, i, sourceTypeInfo);
 
           } else {
 
             // Do formal conversion...
-            initTopLevelField(i, i, sourceTypeInfo, dataTypePhysicalVariations[i]);
+            initTopLevelField(i, i, sourceTypeInfo);
 
             // UNDONE: No for List and Map; Yes for Struct and Union when field count different...
             addTopLevelConversion(i, targetTypeInfo);
@@ -594,7 +582,7 @@ public void initConversion(TypeInfo[] targetTypeInfos,
         } else {
 
           // No conversion.
-          initTopLevelField(i, i, sourceTypeInfo, dataTypePhysicalVariations[i]);
+          initTopLevelField(i, i, sourceTypeInfo);
 
         }
 
@@ -1061,13 +1049,8 @@ class VectorDecimalDeserializer extends VectorBatchDeserializer {
     @Override
     void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
             throws IOException {
-      if (field.getDataTypePhysicalVariation() == DataTypePhysicalVariation.DECIMAL_64) {
-        ((Decimal64ColumnVector) colVector).vector[batchIndex] = deserializeRead.currentDecimal64;
-      } else {
-        // The DecimalColumnVector set method will quickly copy the deserialized decimal writable fields.
-        ((DecimalColumnVector) colVector).set(
-                batchIndex, deserializeRead.currentHiveDecimalWritable);
-      }
+      Preconditions.checkState(colVector instanceof IDecimalColumnVector);
+      ((IDecimalColumnVector) colVector).set(batchIndex, deserializeRead.currentHiveDecimalWritable);
     }
 
     @Override
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorSerDeRow.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorSerDeRow.java
index 3af75ee2a0..dd4021d1ba 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorSerDeRow.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorSerDeRow.java
@@ -49,10 +49,12 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.UnionObject;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.hive.serde2.fast.SerializeWrite;
 import org.apache.hadoop.hive.serde2.io.HiveCharWritable;
 import org.apache.hadoop.hive.serde2.io.HiveVarcharWritable;
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.hive.common.type.DataTypePhysicalVariation;
 
 
 import static org.junit.Assert.fail;
@@ -388,19 +390,20 @@ private LazySerDeParameters getSerDeParams(
   }
 
   void testVectorDeserializeRow(
-      Random r, SerializationType serializationType,
+      Random r, SerializationType serializationType, boolean testDecimal64,
       boolean alternate1, boolean alternate2, boolean useExternalBuffer)
       throws HiveException, IOException, SerDeException {
 
     for (int i = 0; i < 20; i++) {
       innerTestVectorDeserializeRow(
-          r, i,serializationType, alternate1, alternate2, useExternalBuffer);
+          r, i,serializationType, testDecimal64, alternate1, alternate2, useExternalBuffer);
     }
   }
 
   void innerTestVectorDeserializeRow(
       Random r, int iteration,
       SerializationType serializationType,
+      boolean testDecimal64,
       boolean alternate1, boolean alternate2, boolean useExternalBuffer)
       throws HiveException, IOException, SerDeException {
 
@@ -419,6 +422,22 @@ void innerTestVectorDeserializeRow(
     batchContext.init(source.rowStructObjectInspector(), emptyScratchTypeNames);
     VectorizedRowBatch batch = batchContext.createVectorizedRowBatch();
 
+    // For Decimal64 tests, we manually change the VectorizedRowBatch to contain
+    // a Decimal64ColumnVector.  While this may seem hacky, this is the way that it
+    // is done within the VectorPTFOperator.
+    if (testDecimal64) {
+      for (int i = 0; i < batch.cols.length; ++i) {
+        if (batch.cols[i] instanceof DecimalColumnVector) {
+          DecimalColumnVector dcv = (DecimalColumnVector) batch.cols[i];
+          if (dcv.precision <= 18) {
+            String newType = String.format("decimal(%d,%d)", dcv.precision, dcv.scale);
+            TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(newType);
+            batch.cols[i] = VectorizedBatchUtil.createColumnVector(typeInfo, DataTypePhysicalVariation.DECIMAL_64);
+          }
+        }
+      }
+    }
+
     // junk the destination for the 1st pass
     for (ColumnVector cv : batch.cols) {
       Arrays.fill(cv.isNull, true);
@@ -581,48 +600,56 @@ public void testVectorBinarySortableDeserializeRow() throws Throwable {
     Random r = new Random(8732);
     testVectorDeserializeRow(r,
         SerializationType.BINARY_SORTABLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useColumnSortOrderIsDesc */ false,
         /* alternate2 = useBinarySortableCharsNeedingEscape */ false,
         /* useExternalBuffer */ false);
 
     testVectorDeserializeRow(r,
         SerializationType.BINARY_SORTABLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useColumnSortOrderIsDesc */ true,
         /* alternate2 = useBinarySortableCharsNeedingEscape */ false,
         /* useExternalBuffer */ false);
 
     testVectorDeserializeRow(r,
         SerializationType.BINARY_SORTABLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useColumnSortOrderIsDesc */ false,
         /* alternate2 = useBinarySortableCharsNeedingEscape */ false,
         /* useExternalBuffer */ true);
 
     testVectorDeserializeRow(r,
         SerializationType.BINARY_SORTABLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useColumnSortOrderIsDesc */ true,
         /* alternate2 = useBinarySortableCharsNeedingEscape */ false,
         /* useExternalBuffer */ true);
 
     testVectorDeserializeRow(r,
         SerializationType.BINARY_SORTABLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useColumnSortOrderIsDesc */ false,
         /* alternate2 = useBinarySortableCharsNeedingEscape */ true,
         /* useExternalBuffer */ false);
 
     testVectorDeserializeRow(r,
         SerializationType.BINARY_SORTABLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useColumnSortOrderIsDesc */ true,
         /* alternate2 = useBinarySortableCharsNeedingEscape */ true,
         /* useExternalBuffer */ false);
 
     testVectorDeserializeRow(r,
         SerializationType.BINARY_SORTABLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useColumnSortOrderIsDesc */ false,
         /* alternate2 = useBinarySortableCharsNeedingEscape */ true,
         /* useExternalBuffer */ true);
 
     testVectorDeserializeRow(r,
         SerializationType.BINARY_SORTABLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useColumnSortOrderIsDesc */ true,
         /* alternate2 = useBinarySortableCharsNeedingEscape */ true,
         /* useExternalBuffer */ true);
@@ -633,15 +660,24 @@ public void testVectorLazyBinaryDeserializeRow() throws Throwable {
     Random r = new Random(8732);
     testVectorDeserializeRow(r,
         SerializationType.LAZY_BINARY,
+        /* testDecimal64 */ false,
         /* alternate1 = unused */ false,
         /* alternate2 = unused */ false,
         /* useExternalBuffer */ false);
 
     testVectorDeserializeRow(r,
         SerializationType.LAZY_BINARY,
+        /* testDecimal64 */ false,
         /* alternate1 = unused */ false,
         /* alternate2 = unused */ false,
         /* useExternalBuffer */ true);
+
+    testVectorDeserializeRow(r,
+        SerializationType.LAZY_BINARY,
+        /* testDecimal64 */ true,
+        /* alternate1 = unused */ false,
+        /* alternate2 = unused */ false,
+        /* useExternalBuffer */ false);
   }
 
   @Test
@@ -649,26 +685,38 @@ public void testVectorLazySimpleDeserializeRow() throws Throwable {
     Random r = new Random(8732);
     testVectorDeserializeRow(r,
         SerializationType.LAZY_SIMPLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useLazySimpleEscapes */ false,
         /* alternate2 = unused */ false,
         /* useExternalBuffer */ false);
 
     testVectorDeserializeRow(r,
         SerializationType.LAZY_SIMPLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useLazySimpleEscapes */ false,
         /* alternate2 = unused */ false,
         /* useExternalBuffer */ true);
 
     testVectorDeserializeRow(r,
         SerializationType.LAZY_SIMPLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useLazySimpleEscapes */ true,
         /* alternate2 = unused */ false,
         /* useExternalBuffer */ false);
 
     testVectorDeserializeRow(r,
         SerializationType.LAZY_SIMPLE,
+        /* testDecimal64 */ false,
         /* alternate1 = useLazySimpleEscapes */ true,
         /* alternate2 = unused */ false,
         /* useExternalBuffer */ true);
+
+    testVectorDeserializeRow(r,
+        SerializationType.LAZY_SIMPLE,
+        /* testDecimal64 */ true,
+        /* alternate1 = useLazySimpleEscapes */ false,
+        /* alternate2 = unused */ false,
+        /* useExternalBuffer */ false);
+
   }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/VectorRandomRowSource.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/VectorRandomRowSource.java
index 1b6dbe48d8..c61896532c 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/VectorRandomRowSource.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/VectorRandomRowSource.java
@@ -377,7 +377,8 @@ public void initGenerationSpecSchema(Random r, List<GenerationSpec> generationSp
       "timestamp",
       "interval_year_month",
       "interval_day_time",
-      "decimal"
+      "decimal",
+      "decimal64"
   };
 
   private static String[] possibleHiveComplexTypeNames = {
@@ -434,6 +435,8 @@ public static String getDecoratedTypeName(Random random, String typeName,
               "decimal(%d,%d)",
               HiveDecimal.SYSTEM_DEFAULT_PRECISION,
               HiveDecimal.SYSTEM_DEFAULT_SCALE);
+    } else if (typeName.equals("decimal64")) {
+      typeName = "decimal(18,6)";
     } else if (typeName.equals("array")) {
       String elementTypeName = getRandomTypeName(random, supportedTypes, allowedTypeNameSet);
       elementTypeName =
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java
index ac710578ee..34e5a4a9ba 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java
@@ -855,9 +855,6 @@ private boolean doReadField(Field field) {
 
               decimalIsNull = !currentHiveDecimalWritable.mutateEnforcePrecisionScale(precision, scale);
               if (!decimalIsNull) {
-                if (HiveDecimalWritable.isPrecisionDecimal64(precision)) {
-                  currentDecimal64 = currentHiveDecimalWritable.serialize64(scale);
-                }
                 return true;
               }
             }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java
index 53af41be65..8a737a4c68 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java
@@ -397,17 +397,11 @@ private boolean readPrimitive(Field field) throws IOException {
 
           decimalIsNull = !currentHiveDecimalWritable.mutateEnforcePrecisionScale(precision, scale);
           if (!decimalIsNull) {
-            if (HiveDecimalWritable.isPrecisionDecimal64(precision)) {
-              currentDecimal64 = currentHiveDecimalWritable.serialize64(scale);
-            }
             return true;
           }
         }
-        if (decimalIsNull) {
-          return false;
-        }
+        return false;
       }
-      break;
     default:
       throw new Error("Unexpected primitive category " + primitiveCategory.name());
     }
diff --git a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/Decimal64ColumnVector.java b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/Decimal64ColumnVector.java
index 615eb6fcf7..7f47b905f2 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/Decimal64ColumnVector.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/Decimal64ColumnVector.java
@@ -23,7 +23,7 @@
 /**
 
  */
-public class Decimal64ColumnVector extends LongColumnVector {
+public class Decimal64ColumnVector extends LongColumnVector implements IDecimalColumnVector {
 
   public short scale;
   public short precision;
diff --git a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java
index e074fb9173..5defd27623 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java
@@ -24,7 +24,7 @@
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 
-public class DecimalColumnVector extends ColumnVector {
+public class DecimalColumnVector extends ColumnVector implements IDecimalColumnVector {
 
   /**
    * A vector of HiveDecimalWritable objects.
diff --git a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/IDecimalColumnVector.java b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/IDecimalColumnVector.java
new file mode 100644
index 0000000000..eedc0ee62f
--- /dev/null
+++ b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/IDecimalColumnVector.java
@@ -0,0 +1,25 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector;
+
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+
+public interface IDecimalColumnVector {
+  void set(int elementNum, HiveDecimalWritable writable);
+}
