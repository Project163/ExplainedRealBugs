diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 54e2b1861c..31aeba9f72 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -581,6 +581,11 @@ public static enum ConfVars {
     HIVEJAR("hive.jar.path", "", ""),
     HIVEAUXJARS("hive.aux.jars.path", "", ""),
 
+    // reloadable jars
+    HIVERELOADABLEJARS("hive.reloadable.aux.jars.path", "",
+        "Jars can be renewed by executing reload command. And these jars can be "
+            + "used as the auxiliary classes like creating a UDF or SerDe."),
+
     // hive added files and jars
     HIVEADDEDFILES("hive.added.files.path", "", ""),
     HIVEADDEDJARS("hive.added.jars.path", "", ""),
@@ -1613,7 +1618,7 @@ public static enum ConfVars {
     HIVE_SERVER2_SSL_KEYSTORE_PATH("hive.server2.keystore.path", "", ""),
     HIVE_SERVER2_SSL_KEYSTORE_PASSWORD("hive.server2.keystore.password", "", ""),
 
-    HIVE_SECURITY_COMMAND_WHITELIST("hive.security.command.whitelist", "set,reset,dfs,add,list,delete,compile",
+    HIVE_SECURITY_COMMAND_WHITELIST("hive.security.command.whitelist", "set,reset,dfs,add,list,delete,reload,compile",
         "Comma separated list of non-SQL Hive commands users are authorized to execute"),
 
     HIVE_SERVER2_SESSION_CHECK_INTERVAL("hive.server2.session.check.interval", "0ms",
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java
index 93a03adeab..4fdb5c9851 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java
@@ -44,6 +44,7 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
 import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
 import org.apache.hadoop.hive.ql.metadata.Partition;
@@ -425,7 +426,7 @@ public static HiveStorageHandler getStorageHandler(Configuration conf,
     try {
       Class<? extends HiveStorageHandler> handlerClass =
         (Class<? extends HiveStorageHandler>) Class
-          .forName(storageHandler, true, JavaUtils.getClassLoader());
+          .forName(storageHandler, true, Utilities.getSessionSpecifiedClassLoader());
       return (HiveStorageHandler) ReflectionUtils.newInstance(
         handlerClass, conf);
     } catch (ClassNotFoundException e) {
diff --git a/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatClient.java b/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatClient.java
index f25039dcf5..ccad819a84 100644
--- a/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatClient.java
+++ b/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatClient.java
@@ -22,8 +22,8 @@
 import java.util.Map;
 
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.metastore.api.PartitionEventType;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hive.hcatalog.common.HCatException;
 import org.apache.hive.hcatalog.data.schema.HCatFieldSchema;
 
@@ -49,7 +49,7 @@ public static HCatClient create(Configuration conf) throws HCatException {
       HCatClientHMSImpl.class.getName());
     try {
       Class<? extends HCatClient> clientClass = Class.forName(className,
-        true, JavaUtils.getClassLoader()).asSubclass(
+        true, Utilities.getSessionSpecifiedClassLoader()).asSubclass(
           HCatClient.class);
       client = (HCatClient) clientClass.newInstance();
     } catch (ClassNotFoundException e) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultFetchFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultFetchFormatter.java
index 5924bcf1f5..928be4269b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultFetchFormatter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultFetchFormatter.java
@@ -53,7 +53,7 @@ public void initialize(Configuration hconf, Properties props) throws HiveExcepti
   private SerDe initializeSerde(Configuration conf, Properties props) throws Exception {
     String serdeName = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEFETCHOUTPUTSERDE);
     Class<? extends SerDe> serdeClass = Class.forName(serdeName, true,
-        JavaUtils.getClassLoader()).asSubclass(SerDe.class);
+        Utilities.getSessionSpecifiedClassLoader()).asSubclass(SerDe.class);
     // cast only needed for Hadoop 0.17 compatibility
     SerDe serde = ReflectionUtils.newInstance(serdeClass, null);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
index 0c6a3d44ef..dbb8eaf826 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
@@ -39,7 +39,6 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.Function;
@@ -562,7 +561,7 @@ private static FunctionInfo getFunctionInfoFromMetastore(String functionName) {
           return null;
         }
 
-        Class<?> udfClass = Class.forName(func.getClassName(), true, JavaUtils.getClassLoader());
+        Class<?> udfClass = Class.forName(func.getClassName(), true, Utilities.getSessionSpecifiedClassLoader());
         if (registerTemporaryFunction(functionName, udfClass)) {
           ret = mFunctions.get(functionName);
         } else {
@@ -610,7 +609,7 @@ private static void checkFunctionClass(CommonFunctionInfo cfi) throws ClassNotFo
     // Even if we have a reference to the class (which will be the case for GenericUDFs),
     // the classloader may not be able to resolve the class, which would mean reflection-based
     // methods would fail such as for plan deserialization. Make sure this works too.
-    Class.forName(udfClass.getName(), true, JavaUtils.getClassLoader());
+    Class.forName(udfClass.getName(), true, Utilities.getSessionSpecifiedClassLoader());
   }
 
   private static void loadFunctionResourcesIfNecessary(String functionName, CommonFunctionInfo cfi) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java
index bd45df1a40..569c12585d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java
@@ -21,7 +21,6 @@
 import static org.apache.hadoop.util.StringUtils.stringifyException;
 
 import java.io.IOException;
-import java.net.URI;
 import java.util.List;
 
 import org.apache.commons.logging.Log;
@@ -33,10 +32,8 @@
 import org.apache.hadoop.hive.metastore.api.PrincipalType;
 import org.apache.hadoop.hive.metastore.api.ResourceType;
 import org.apache.hadoop.hive.metastore.api.ResourceUri;
-import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.DriverContext;
 import org.apache.hadoop.hive.ql.QueryPlan;
-import org.apache.hadoop.hive.ql.exec.FunctionUtils.FunctionType;
 import org.apache.hadoop.hive.ql.exec.FunctionUtils.UDFClassType;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -47,10 +44,6 @@
 import org.apache.hadoop.hive.ql.plan.FunctionWork;
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver;
-import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
-import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
-import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 
 /**
@@ -308,9 +301,10 @@ public static void addFunctionResources(List<ResourceUri> resources) throws Hive
     }
   }
 
-  @SuppressWarnings("unchecked")
   private Class<?> getUdfClass(CreateFunctionDesc desc) throws ClassNotFoundException {
-    return Class.forName(desc.getClassName(), true, JavaUtils.getClassLoader());
+    // get the session specified class loader from SessionState
+    ClassLoader classLoader = Utilities.getSessionSpecifiedClassLoader();
+    return Class.forName(desc.getClassName(), true, classLoader);
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ListSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ListSinkOperator.java
index dcc19f7064..25797c633f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ListSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ListSinkOperator.java
@@ -57,7 +57,7 @@ private FetchFormatter initializeFetcher(Configuration conf) throws Exception {
     FetchFormatter fetcher;
     if (formatterName != null && !formatterName.isEmpty()) {
       Class<? extends FetchFormatter> fetcherClass = Class.forName(formatterName, true,
-          JavaUtils.getClassLoader()).asSubclass(FetchFormatter.class);
+          Utilities.getSessionSpecifiedClassLoader()).asSubclass(FetchFormatter.class);
       fetcher = ReflectionUtils.newInstance(fetcherClass, null);
     } else {
       fetcher = new DefaultFetchFormatter();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 2d9b9c34cf..5bbf3f695c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -1974,6 +1974,26 @@ public static String getResourceFiles(Configuration conf, SessionState.ResourceT
     }
   }
 
+  /**
+   * get session specified class loader and get current class loader if fall
+   *
+   * @return
+   */
+  public static ClassLoader getSessionSpecifiedClassLoader() {
+    SessionState state = SessionState.get();
+    if (state == null || state.getConf() == null) {
+      LOG.debug("Hive Conf not found or Session not initiated, use thread based class loader instead");
+      return JavaUtils.getClassLoader();
+    }
+    ClassLoader sessionCL = state.getConf().getClassLoader();
+    if (sessionCL != null){
+      LOG.debug("Use session specified class loader");
+      return sessionCL;
+    }
+    LOG.debug("Session specified class loader not found, use thread based class loader");
+    return JavaUtils.getClassLoader();
+  }
+
   /**
    * Create a URL from a string representing a path to a local file.
    * The path string can be just a path, or can start with file:/, file:///
@@ -1994,6 +2014,33 @@ private static URL urlFromPathString(String onestr) {
     return oneurl;
   }
 
+    /**
+     * get the jar files from specified directory or get jar files by several jar names sperated by comma
+     * @param path
+     * @return
+     */
+    public static Set<String> getJarFilesByPath(String path){
+        Set<String> result = new HashSet<String>();
+        if (path == null || path.isEmpty()) {
+            return result;
+        }
+
+        File paths = new File(path);
+        if (paths.exists() && paths.isDirectory()) {
+            // add all jar files under the reloadable auxiliary jar paths
+            Set<File> jarFiles = new HashSet<File>();
+            jarFiles.addAll(org.apache.commons.io.FileUtils.listFiles(
+                    paths, new String[]{"jar"}, true));
+            for (File f : jarFiles) {
+                result.add(f.getAbsolutePath());
+            }
+        } else {
+            String[] files = path.split(",");
+            Collections.addAll(result, files);
+        }
+        return result;
+    }
+
   /**
    * Add new elements to the classpath.
    *
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java
index df8640c60b..cda06293cb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java
@@ -635,7 +635,7 @@ private List<ClientStatsPublisher> getClientStatPublishers() {
     for (String clientStatsPublisherClass : clientStatsPublisherClasses) {
       try {
         clientStatsPublishers.add((ClientStatsPublisher) Class.forName(
-            clientStatsPublisherClass.trim(), true, JavaUtils.getClassLoader()).newInstance());
+            clientStatsPublisherClass.trim(), true, Utilities.getSessionSpecifiedClassLoader()).newInstance());
       } catch (Exception e) {
         LOG.warn(e.getClass().getName() + " occured when trying to create class: "
             + clientStatsPublisherClass.trim() + " implementing ClientStatsPublisher interface");
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookUtils.java
index 3f474f846c..390ffd9965 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookUtils.java
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 
 public class HookUtils {
   /**
@@ -57,7 +58,7 @@ public static <T extends Hook> List<T> getHooks(HiveConf conf,
     String[] hookClasses = csHooks.split(",");
     for (String hookClass : hookClasses) {
         T hook = (T) Class.forName(hookClass.trim(), true,
-            JavaUtils.getClassLoader()).newInstance();
+                Utilities.getSessionSpecifiedClassLoader()).newInstance();
         hooks.add(hook);
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/HivePassThroughOutputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/HivePassThroughOutputFormat.java
index 0962cadce0..04eff93090 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/HivePassThroughOutputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/HivePassThroughOutputFormat.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.JavaUtils;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.mapred.JobConf;
@@ -65,7 +66,7 @@ private void createActualOF() throws IOException {
        {
         cls =
            (Class<? extends OutputFormat>) Class.forName(actualOutputFormatClass, true,
-                JavaUtils.getClassLoader());
+                Utilities.getSessionSpecifiedClassLoader());
       } else {
         throw new RuntimeException("Null pointer detected in actualOutputFormatClass");
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java
index 9051ba6d80..c4633f603b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.index.HiveIndexHandler;
 import org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
@@ -307,7 +308,7 @@ public static HiveStorageHandler getStorageHandler(
     try {
       Class<? extends HiveStorageHandler> handlerClass =
         (Class<? extends HiveStorageHandler>)
-        Class.forName(className, true, JavaUtils.getClassLoader());
+        Class.forName(className, true, Utilities.getSessionSpecifiedClassLoader());
       HiveStorageHandler storageHandler = ReflectionUtils.newInstance(handlerClass, conf);
       return storageHandler;
     } catch (ClassNotFoundException e) {
@@ -329,7 +330,7 @@ public static HiveIndexHandler getIndexHandler(HiveConf conf,
     try {
       Class<? extends HiveIndexHandler> handlerClass =
         (Class<? extends HiveIndexHandler>)
-        Class.forName(indexHandlerClass, true, JavaUtils.getClassLoader());
+        Class.forName(indexHandlerClass, true, Utilities.getSessionSpecifiedClassLoader());
       HiveIndexHandler indexHandler = ReflectionUtils.newInstance(handlerClass, conf);
       return indexHandler;
     } catch (ClassNotFoundException e) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
index edec1b734f..13277a971b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
@@ -302,7 +302,7 @@ final public Class<? extends InputFormat> getInputFormatClass()
       }
       try {
         inputFormatClass = ((Class<? extends InputFormat>) Class.forName(clsName, true,
-            JavaUtils.getClassLoader()));
+            Utilities.getSessionSpecifiedClassLoader()));
       } catch (ClassNotFoundException e) {
         throw new HiveException("Class not found: " + clsName, e);
       }
@@ -322,7 +322,7 @@ final public Class<? extends HiveOutputFormat> getOutputFormatClass()
       }
       try {
         Class<?> c = (Class.forName(clsName, true,
-            JavaUtils.getClassLoader()));
+            Utilities.getSessionSpecifiedClassLoader()));
         // Replace FileOutputFormat for backward compatibility
         if (!HiveOutputFormat.class.isAssignableFrom(c)) {
           outputFormatClass = HiveFileFormatUtils.getOutputFormatSubstitute(c,false);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
index 2f13ac2e30..4acafba601 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.metastore.api.SkewedInfo;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 import org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat;
@@ -293,7 +294,7 @@ final public Class<? extends InputFormat> getInputFormatClass() {
           inputFormatClass = getStorageHandler().getInputFormatClass();
         } else {
           inputFormatClass = (Class<? extends InputFormat>)
-            Class.forName(className, true, JavaUtils.getClassLoader());
+            Class.forName(className, true, Utilities.getSessionSpecifiedClassLoader());
         }
       } catch (ClassNotFoundException e) {
         throw new RuntimeException(e);
@@ -329,7 +330,7 @@ final public Class<? extends HiveOutputFormat> getOutputFormatClass() {
             }
             else {
               c = Class.forName(className, true,
-                  JavaUtils.getClassLoader());
+                  Utilities.getSessionSpecifiedClassLoader());
             }
         }
         if (!HiveOutputFormat.class.isAssignableFrom(c)) {
@@ -677,7 +678,7 @@ public void setInputFormatClass(String name) throws HiveException {
     }
     try {
       setInputFormatClass((Class<? extends InputFormat<WritableComparable, Writable>>) Class
-          .forName(name, true, JavaUtils.getClassLoader()));
+          .forName(name, true, Utilities.getSessionSpecifiedClassLoader()));
     } catch (ClassNotFoundException e) {
       throw new HiveException("Class not found: " + name, e);
     }
@@ -690,7 +691,7 @@ public void setOutputFormatClass(String name) throws HiveException {
       return;
     }
     try {
-      Class<?> origin = Class.forName(name, true, JavaUtils.getClassLoader());
+      Class<?> origin = Class.forName(name, true, Utilities.getSessionSpecifiedClassLoader());
       setOutputFormatClass(HiveFileFormatUtils
           .getOutputFormatSubstitute(origin,false));
     } catch (ClassNotFoundException e) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java
index f7b8bd682f..790a92e7d1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java
@@ -29,7 +29,6 @@
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
 import org.apache.hadoop.hive.ql.exec.FilterOperator;
@@ -41,6 +40,7 @@
 import org.apache.hadoop.hive.ql.exec.SelectOperator;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.UDF;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
@@ -275,7 +275,7 @@ private static boolean isDeterministicUdf(GenericUDF udf) {
       String udfClassName = bridge.getUdfClassName();
       try {
         UDF udfInternal =
-            (UDF) Class.forName(bridge.getUdfClassName(), true, JavaUtils.getClassLoader())
+            (UDF) Class.forName(bridge.getUdfClassName(), true, Utilities.getSessionSpecifiedClassLoader())
                 .newInstance();
         files = udfInternal.getRequiredFiles();
         jars = udf.getRequiredJars();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
index d86df453cd..de4025bd8e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
@@ -445,7 +445,7 @@ private static void checkTable(Table table, CreateTableDesc tableDesc)
        * substitute OutputFormat name based on HiveFileFormatUtils.outputFormatSubstituteMap
        */
       try {
-        Class<?> origin = Class.forName(importedofc, true, JavaUtils.getClassLoader());
+        Class<?> origin = Class.forName(importedofc, true, Utilities.getSessionSpecifiedClassLoader());
         Class<? extends HiveOutputFormat> replaced = HiveFileFormatUtils
             .getOutputFormatSubstitute(origin,false);
         if (replaced == null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java
index 0a1c660b4b..396553af11 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.ErrorMsg;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
@@ -221,7 +222,7 @@ public static String ensureClassExists(String className)
       return null;
     }
     try {
-      Class.forName(className, true, JavaUtils.getClassLoader());
+      Class.forName(className, true, Utilities.getSessionSpecifiedClassLoader());
     } catch (ClassNotFoundException e) {
       throw new SemanticException("Cannot find class '" + className + "'", e);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index e4a30a2082..97fa52c8d6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -2534,7 +2534,7 @@ private TableDesc getTableDescFromSerDe(ASTNode child, String cols,
 
       try {
         serdeClass = (Class<? extends Deserializer>) Class.forName(serdeName,
-            true, JavaUtils.getClassLoader());
+            true, Utilities.getSessionSpecifiedClassLoader());
       } catch (ClassNotFoundException e) {
         throw new SemanticException(e);
       }
@@ -2723,7 +2723,7 @@ private Operator genScriptPlan(ASTNode trfm, QB qb, Operator input)
 
     try {
       serde = (Class<? extends Deserializer>) Class.forName(defaultSerdeName,
-          true, JavaUtils.getClassLoader());
+          true, Utilities.getSessionSpecifiedClassLoader());
     } catch (ClassNotFoundException e) {
       throw new SemanticException(e);
     }
@@ -2790,7 +2790,7 @@ private Class<? extends RecordReader> getRecordReader(ASTNode node)
 
     try {
       return (Class<? extends RecordReader>) Class.forName(name, true,
-          JavaUtils.getClassLoader());
+          Utilities.getSessionSpecifiedClassLoader());
     } catch (ClassNotFoundException e) {
       throw new SemanticException(e);
     }
@@ -2804,7 +2804,7 @@ private Class<? extends RecordReader> getDefaultRecordReader()
 
     try {
       return (Class<? extends RecordReader>) Class.forName(name, true,
-          JavaUtils.getClassLoader());
+          Utilities.getSessionSpecifiedClassLoader());
     } catch (ClassNotFoundException e) {
       throw new SemanticException(e);
     }
@@ -2822,7 +2822,7 @@ private Class<? extends RecordWriter> getRecordWriter(ASTNode node)
 
     try {
       return (Class<? extends RecordWriter>) Class.forName(name, true,
-          JavaUtils.getClassLoader());
+          Utilities.getSessionSpecifiedClassLoader());
     } catch (ClassNotFoundException e) {
       throw new SemanticException(e);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/AggregationDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/AggregationDesc.java
index 17eeae1a34..1a0cdf8b1f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/AggregationDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/AggregationDesc.java
@@ -23,6 +23,7 @@
 
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.ql.exec.PTFUtils;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
 import org.apache.hadoop.util.ReflectionUtils;
 
@@ -93,7 +94,7 @@ public GenericUDAFEvaluator getGenericUDAFEvaluator() {
     try {
       return genericUDAFEvaluator =
           ReflectionUtils.newInstance(Class.forName(genericUDAFEvaluatorClassName, true,
-          JavaUtils.getClassLoader()).asSubclass(GenericUDAFEvaluator.class), null);
+          Utilities.getSessionSpecifiedClassLoader()).asSubclass(GenericUDAFEvaluator.class), null);
     } catch (ClassNotFoundException e) {
       throw new RuntimeException(e);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
index 930acbc98e..deba19839e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
@@ -419,7 +419,7 @@ public void validate(HiveConf conf)
     if (this.getStorageHandler() == null) {
       try {
         Class<?> origin = Class.forName(this.getOutputFormat(), true,
-          JavaUtils.getClassLoader());
+          Utilities.getSessionSpecifiedClassLoader());
         Class<? extends HiveOutputFormat> replaced = HiveFileFormatUtils
           .getOutputFormatSubstitute(origin,false);
         if (replaced == null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java
index 39f1793aaa..78d4d1fac0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java
@@ -26,6 +26,7 @@
 
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 import org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat;
@@ -65,7 +66,7 @@ public TableDesc(
   public Class<? extends Deserializer> getDeserializerClass() {
     try {
       return (Class<? extends Deserializer>) Class.forName(
-          getSerdeClassName(), true, JavaUtils.getClassLoader());
+          getSerdeClassName(), true, Utilities.getSessionSpecifiedClassLoader());
     } catch (ClassNotFoundException e) {
       throw new RuntimeException(e);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory.java
index 0d237f01a2..727f61fb1e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory.java
@@ -80,6 +80,8 @@ public static CommandProcessor getForHiveCommand(String[] cmd, HiveConf conf)
         return new DeleteResourceProcessor();
       case COMPILE:
         return new CompileProcessor();
+      case RELOAD:
+        return new ReloadProcessor();
       default:
         throw new AssertionError("Unknown HiveCommand " + hiveCommand);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/processors/HiveCommand.java b/ql/src/java/org/apache/hadoop/hive/ql/processors/HiveCommand.java
index f5bc427a58..27d8325829 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/processors/HiveCommand.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/processors/HiveCommand.java
@@ -31,6 +31,7 @@ public enum HiveCommand {
   DFS(),
   ADD(),
   LIST(),
+  RELOAD(),
   DELETE(),
   COMPILE();
   private static final Set<String> COMMANDS = new HashSet<String>();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/processors/ReloadProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/processors/ReloadProcessor.java
new file mode 100644
index 0000000000..b84c9dd7df
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/processors/ReloadProcessor.java
@@ -0,0 +1,49 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.processors;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.CommandNeedRetryException;
+import org.apache.hadoop.hive.ql.session.SessionState;
+
+/**
+ * used for reload auxiliary and jars without restarting hive server2
+ */
+public class ReloadProcessor implements CommandProcessor{
+  private static final Log LOG = LogFactory.getLog(ReloadProcessor.class);
+
+  @Override
+  public void init() {
+  }
+
+  @Override
+  public CommandProcessorResponse run(String command) throws CommandNeedRetryException {
+    SessionState ss = SessionState.get();
+    try {
+      ss.reloadAuxJars();
+    } catch (IOException e) {
+      LOG.error("fail to reload auxiliary jar files", e);
+      return CommandProcessorResponse.create(e);
+    }
+    return new CommandProcessorResponse(0);
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index c409ef53e6..47fe508f6f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -24,14 +24,8 @@
 import java.io.InputStream;
 import java.io.PrintStream;
 import java.net.URI;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.UUID;
+import java.net.URLClassLoader;
+import java.util.*;
 
 import org.apache.commons.io.FileUtils;
 import org.apache.commons.lang.StringUtils;
@@ -236,6 +230,11 @@ public enum AuthorizationMode{V1, V2};
    */
   private boolean txnAutoCommit = true;
 
+  /**
+   * store the jars loaded last time
+   */
+  private final Set<String> preReloadableAuxJars = new HashSet<String>();
+
   /**
    * Get the lineage state stored in this session.
    *
@@ -830,7 +829,6 @@ static void validateFiles(List<String> newFiles) throws IllegalArgumentException
     SessionState ss = SessionState.get();
     Configuration conf = (ss == null) ? new Configuration() : ss.getConf();
 
-    LogHelper console = getConsole();
     for (String newFile : newFiles) {
       try {
         if (Utilities.realFile(newFile, conf) == null) {
@@ -844,6 +842,52 @@ static void validateFiles(List<String> newFiles) throws IllegalArgumentException
     }
   }
 
+  // reloading the jars under the path specified in hive.reloadable.aux.jars.path property
+  public void reloadAuxJars() throws IOException {
+    final Set<String> reloadedAuxJars = new HashSet<String>();
+
+    final String renewableJarPath = conf.getVar(ConfVars.HIVERELOADABLEJARS);
+    // do nothing if this property is not specified or empty
+    if (renewableJarPath == null || renewableJarPath.isEmpty()) {
+      return;
+    }
+
+    Set<String> jarPaths = Utilities.getJarFilesByPath(renewableJarPath);
+
+    // load jars under the hive.reloadable.aux.jars.path
+    if(!jarPaths.isEmpty()){
+      reloadedAuxJars.addAll(jarPaths);
+    }
+
+    // remove the previous renewable jars
+    try {
+      if (preReloadableAuxJars != null && !preReloadableAuxJars.isEmpty()) {
+        Utilities.removeFromClassPath(preReloadableAuxJars.toArray(new String[0]));
+      }
+    } catch (Exception e) {
+      String msg = "Fail to remove the reloaded jars loaded last time: " + e;
+      throw new IOException(msg, e);
+    }
+
+    try {
+      if (reloadedAuxJars != null && !reloadedAuxJars.isEmpty()) {
+        URLClassLoader currentCLoader =
+            (URLClassLoader) SessionState.get().getConf().getClassLoader();
+        currentCLoader =
+            (URLClassLoader) Utilities.addToClassPath(currentCLoader,
+                reloadedAuxJars.toArray(new String[0]));
+        conf.setClassLoader(currentCLoader);
+        Thread.currentThread().setContextClassLoader(currentCLoader);
+      }
+      preReloadableAuxJars.clear();
+      preReloadableAuxJars.addAll(reloadedAuxJars);
+    } catch (Exception e) {
+      String msg =
+          "Fail to add jars from the path specified in hive.reloadable.aux.jars.path property: " + e;
+      throw new IOException(msg, e);
+    }
+  }
+
   static void registerJars(List<String> newJars) throws IllegalArgumentException {
     LogHelper console = getConsole();
     try {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsFactory.java
index e247184b7d..b9878a3fb8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsFactory.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.common.StatsSetupConst.StatDB;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.util.ReflectionUtils;
 
 import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVESTATSDBCLASS;
@@ -87,7 +88,7 @@ private StatsFactory(Configuration conf) {
   }
 
   private boolean initialize(String type) {
-    ClassLoader classLoader = JavaUtils.getClassLoader();
+    ClassLoader classLoader = Utilities.getSessionSpecifiedClassLoader();
     try {
       StatDB statDB = type.startsWith("jdbc") ? StatDB.jdbc : StatDB.valueOf(type);
       publisherImplementation = (Class<? extends Serializable>)
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java
index 959007a54b..e471285a1a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.UDF;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ConversionHelper;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
@@ -128,7 +129,7 @@ public void setOperator(boolean isOperator) {
 
   public Class<? extends UDF> getUdfClass() {
     try {
-      return (Class<? extends UDF>) Class.forName(udfClassName, true, JavaUtils.getClassLoader());
+      return (Class<? extends UDF>) Class.forName(udfClassName, true, Utilities.getSessionSpecifiedClassLoader());
     } catch (ClassNotFoundException e) {
       throw new RuntimeException(e);
     }
@@ -138,7 +139,7 @@ public Class<? extends UDF> getUdfClass() {
   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
 
     try {
-      udf = (UDF) Class.forName(udfClassName, true, JavaUtils.getClassLoader()).newInstance();
+      udf = (UDF) Class.forName(udfClassName, true, Utilities.getSessionSpecifiedClassLoader()).newInstance();
     } catch (Exception e) {
       throw new UDFArgumentException(
           "Unable to instantiate UDF implementation class " + udfClassName + ": " + e);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
index 390a4a63c8..69f88893f2 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
@@ -20,12 +20,22 @@
 
 import static org.apache.hadoop.hive.ql.exec.Utilities.getFileExtension;
 
+import java.io.File;
+import java.io.IOException;
 import java.sql.Timestamp;
 import java.util.ArrayList;
+import java.util.HashSet;
 import java.util.List;
+import java.util.Set;
 
+import com.google.common.collect.Sets;
+import com.google.common.io.Files;
+import junit.framework.Assert;
 import junit.framework.TestCase;
 
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -38,6 +48,7 @@
 import org.apache.hadoop.mapred.JobConf;
 
 public class TestUtilities extends TestCase {
+  public static final Log LOG = LogFactory.getLog(TestUtilities.class);
 
   public void testGetFileExtension() {
     JobConf jc = new JobConf();
@@ -105,4 +116,28 @@ public void testgetDbTableName() throws HiveException{
       assertEquals("Invalid table name " + tablename, ex.getMessage());
     }
   }
+
+  public void testGetJarFilesByPath() {
+    File f = Files.createTempDir();
+    String jarFileName1 = f.getAbsolutePath() + File.separator + "a.jar";
+    String jarFileName2 = f.getAbsolutePath() + File.separator + "b.jar";
+    File jarFile = new File(jarFileName1);
+    try {
+      FileUtils.touch(jarFile);
+      HashSet<String> jars = (HashSet) Utilities.getJarFilesByPath(f.getAbsolutePath());
+      Assert.assertEquals(Sets.newHashSet(jarFile.getAbsolutePath()),jars);
+
+      File jarFile2 = new File(jarFileName2);
+      FileUtils.touch(jarFile2);
+      String newPath = "file://" + jarFileName1 + "," + "file://" + jarFileName2;
+      jars = (HashSet) Utilities.getJarFilesByPath(newPath);
+
+      Assert.assertEquals(Sets.newHashSet("file://" + jarFileName1, "file://" + jarFileName2), jars);
+    } catch (IOException e) {
+      LOG.error("failed to copy file to reloading folder", e);
+      Assert.fail(e.getMessage());
+    } finally {
+      FileUtils.deleteQuietly(f);
+    }
+  }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java b/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java
index ef0052f576..45ba07e59d 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java
@@ -20,18 +20,29 @@
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNull;
 
+import java.io.File;
+import java.io.IOException;
+import java.lang.reflect.Method;
 import java.util.Arrays;
 import java.util.Collection;
 
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hive.common.util.HiveTestUtils;
+import org.junit.After;
+import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
 import org.junit.runners.Parameterized.Parameters;
 
+import com.google.common.io.Files;
+
 /**
  * Test SessionState
  */
@@ -39,6 +50,14 @@
 public class TestSessionState {
 
   private final boolean prewarm;
+  private final static String clazzDistFileName = "SessionStateTest.jar.v1";
+  private final static String clazzV2FileName = "SessionStateTest.jar.v2";
+  private final static String reloadClazzFileName = "reloadingClazz.jar";
+  private final static String reloadClazzName = "org.apache.test.RefreshedJarClass";
+  private final static String versionMethodName = "version";
+  private static String hiveReloadPath;
+  private File reloadFolder;
+  public static final Log LOG = LogFactory.getLog(TestSessionState.class);
 
   public TestSessionState(Boolean mode) {
     this.prewarm = mode.booleanValue();
@@ -50,8 +69,20 @@ public static Collection<Boolean[]> data() {
   }
 
   @Before
-  public void setup() {
+  public void setUp() {
     HiveConf conf = new HiveConf();
+    String tmp = System.getProperty("java.io.tmpdir");
+    File tmpDir = new File(tmp);
+    if (!tmpDir.exists()) {
+      tmpDir.mkdir();
+    }
+    hiveReloadPath = Files.createTempDir().getAbsolutePath();
+    // create the reloading folder to place jar files if not exist
+    reloadFolder = new File(hiveReloadPath);
+    if (!reloadFolder.exists()) {
+      reloadFolder.mkdir();
+    }
+
     if (prewarm) {
       HiveConf.setBoolVar(conf, ConfVars.HIVE_PREWARM_ENABLED, true);
       HiveConf.setIntVar(conf, ConfVars.HIVE_PREWARM_NUM_CONTAINERS, 1);
@@ -59,6 +90,11 @@ public void setup() {
     SessionState.start(conf);
   }
 
+  @After
+  public void tearDown(){
+    FileUtils.deleteQuietly(reloadFolder);
+  }
+
   /**
    * test set and get db
    */
@@ -129,4 +165,81 @@ public void testClassLoaderEquality() throws Exception {
     assertEquals("Other thread loader and current thread loader",
         otherThread.loader, Thread.currentThread().getContextClassLoader());
   }
+
+  private String getReloadedClazzVersion(ClassLoader cl) throws Exception {
+    Class addedClazz = Class.forName(reloadClazzName, true, cl);
+    Method versionMethod = addedClazz.getMethod(versionMethodName);
+    return (String) versionMethod.invoke(addedClazz.newInstance());
+  }
+
+  @Test
+  public void testReloadAuxJars2() {
+    HiveConf conf = new HiveConf();
+    HiveConf.setVar(conf, ConfVars.HIVERELOADABLEJARS, hiveReloadPath);
+    SessionState ss = new SessionState(conf);
+    SessionState.start(ss);
+
+    ss = SessionState.get();
+    File dist = null;
+    try {
+      dist = new File(reloadFolder.getAbsolutePath() + File.separator + reloadClazzFileName);
+      Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzDistFileName)), dist);
+      ss.reloadAuxJars();
+      Assert.assertEquals("version1", getReloadedClazzVersion(ss.getConf().getClassLoader()));
+    } catch (Exception e) {
+      LOG.error("Reload auxiliary jar test fail with message: ", e);
+      Assert.fail(e.getMessage());
+    } finally {
+      FileUtils.deleteQuietly(dist);
+      try {
+        ss.close();
+      } catch (IOException ioException) {
+        Assert.fail(ioException.getMessage());
+        LOG.error("Fail to close the created session: ", ioException);
+      }
+    }
+  }
+
+  @Test
+  public void testReloadExistingAuxJars2() {
+    HiveConf conf = new HiveConf();
+    HiveConf.setVar(conf, ConfVars.HIVERELOADABLEJARS, hiveReloadPath);
+
+    SessionState ss = new SessionState(conf);
+    SessionState.start(ss);
+    File dist = null;
+
+    try {
+      ss = SessionState.get();
+
+      LOG.info("copy jar file 1");
+      dist = new File(reloadFolder.getAbsolutePath() + File.separator + reloadClazzFileName);
+
+      Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzDistFileName)), dist);
+      ss.reloadAuxJars();
+
+      Assert.assertEquals("version1", getReloadedClazzVersion(ss.getConf().getClassLoader()));
+
+      LOG.info("copy jar file 2");
+      FileUtils.deleteQuietly(dist);
+      Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzV2FileName)), dist);
+
+      ss.reloadAuxJars();
+      Assert.assertEquals("version2", getReloadedClazzVersion(ss.getConf().getClassLoader()));
+
+      FileUtils.deleteQuietly(dist);
+      ss.reloadAuxJars();
+    } catch (Exception e) {
+      LOG.error("refresh existing jar file case failed with message: ", e);
+      Assert.fail(e.getMessage());
+    } finally {
+      FileUtils.deleteQuietly(dist);
+      try {
+        ss.close();
+      } catch (IOException ioException) {
+        Assert.fail(ioException.getMessage());
+        LOG.error("Fail to close the created session: ", ioException);
+      }
+    }
+  }
 }
diff --git a/ql/src/test/resources/SessionStateTest.jar.v1 b/ql/src/test/resources/SessionStateTest.jar.v1
new file mode 100644
index 0000000000..47bceb8597
Binary files /dev/null and b/ql/src/test/resources/SessionStateTest.jar.v1 differ
diff --git a/ql/src/test/resources/SessionStateTest.jar.v2 b/ql/src/test/resources/SessionStateTest.jar.v2
new file mode 100644
index 0000000000..df0da41cf9
Binary files /dev/null and b/ql/src/test/resources/SessionStateTest.jar.v2 differ
diff --git a/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java b/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
index 0b5ef12bfa..b0bb8bef9a 100644
--- a/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
+++ b/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
@@ -120,6 +120,15 @@ public HiveSessionImpl(TProtocolVersion protocol, String username, String passwo
   public void initialize(Map<String, String> sessionConfMap) throws Exception {
     // Process global init file: .hiverc
     processGlobalInitFile();
+    try {
+      sessionState.reloadAuxJars();
+    } catch (IOException e) {
+      String msg = "fail to load reloadable jar file path" + e;
+      LOG.error(msg, e);
+      throw new Exception(msg, e);
+    }
+    SessionState.setCurrentSessionState(sessionState);
+
     // Set conf properties specified by user from client side
     if (sessionConfMap != null) {
       configureSession(sessionConfMap);
