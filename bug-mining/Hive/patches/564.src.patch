diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
index f3f009629c..d2e5e5f176 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRMapJoinCtx;
+import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
@@ -277,6 +278,9 @@ private void createMergeJob(FileSinkOperator fsOp, GenMRProcContext ctx, String
       createMap4Merge(fsOp, ctx, finalName);
       LOG.info("use CombineHiveInputformat for the merge job");
     } else {
+      if (fsOp.getConf().getDynPartCtx() != null) {
+        throw new SemanticException(ErrorMsg.DYNAMIC_PARTITION_MERGE.getMsg());
+      }
       createMapReduce4Merge(fsOp, ctx, finalName);
       LOG.info("use HiveInputFormat for the merge job");
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
index 01eef6943c..c5f9ab0f1a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
@@ -143,9 +143,9 @@ public enum ErrorMsg {
       + "hive.exec.dynamic.partition=true or specify partition column values"),
   DYNAMIC_PARTITION_STRICT_MODE("Dynamic partition strict mode requires at least one "
       + "static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict"),
-  DYNAMIC_PARTITION_MERGE("Dynamic partition does not support merging mapfiles/mapredfiles yet."
-      + "Please set hive.merge.mapfiles and hive.merge.mapredfiles to false or use static "
-      +	"partitions"),
+  DYNAMIC_PARTITION_MERGE("Dynamic partition does not support merging using non-CombineHiveInputFormat."
+      + "Please check your hive.input.format setting and make sure your Hadoop version support "
+      + "CombineFileInputFormat."),
   NONEXISTPARTCOL("Non-Partition column appears in the partition specification: "),
   UNSUPPORTED_TYPE("DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use "
       + "STRING instead."),
diff --git a/ql/src/test/queries/clientnegative/dyn_part_merge.q b/ql/src/test/queries/clientnegative/dyn_part_merge.q
new file mode 100644
index 0000000000..5636bf8cbf
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/dyn_part_merge.q
@@ -0,0 +1,9 @@
+set hive.exec.dynamic.partition=true;
+set hive.exec.dynamic.partition.mode=nonstrict;
+set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
+set hive.mergejob.maponly=false;
+set hive.merge.mapfiles=true;
+
+create table dyn_merge(key string, value string) partitioned by (ds string);
+
+insert overwrite table dyn_merge partition(ds) select key, value, ds from srcpart where ds is not null;
diff --git a/ql/src/test/results/clientnegative/dyn_part_merge.q.out b/ql/src/test/results/clientnegative/dyn_part_merge.q.out
new file mode 100644
index 0000000000..cab97f79ca
--- /dev/null
+++ b/ql/src/test/results/clientnegative/dyn_part_merge.q.out
@@ -0,0 +1,6 @@
+PREHOOK: query: create table dyn_merge(key string, value string) partitioned by (ds string)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table dyn_merge(key string, value string) partitioned by (ds string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@dyn_merge
+FAILED: Error in semantic analysis: Dynamic partition does not support merging using non-CombineHiveInputFormat.Please check your hive.input.format setting and make sure your Hadoop version support CombineFileInputFormat.
