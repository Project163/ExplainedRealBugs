diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
index e0a1d3cab5..300542e26a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.ql.plan.ExplainWork;
 import org.apache.hadoop.hive.ql.processors.CommandProcessor;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.stats.StatsAggregator;
 import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
@@ -103,11 +104,15 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
         // runCtx and ctx share the configuration
         runCtx.setExplainConfig(config);
         Driver driver = new Driver(conf, runCtx);
-        driver.run(query);
+        CommandProcessorResponse ret = driver.run(query);
+        if(ret.getResponseCode() == 0) {
           // Note that we need to call getResults for simple fetch optimization.
           // However, we need to skip all the results.
           while (driver.getResults(new ArrayList<String>())) {
           }
+        } else {
+          throw new SemanticException(ret.getErrorMessage(), ret.getException());
+        }
         config.setOpIdToRuntimeNumRows(aggregateStats(config.getExplainRootPath()));
       } catch (IOException e1) {
         throw new SemanticException(e1);
diff --git a/ql/src/test/queries/clientpositive/explainanalyze_3.q b/ql/src/test/queries/clientpositive/explainanalyze_3.q
index 69f82e5765..61f68a1a86 100644
--- a/ql/src/test/queries/clientpositive/explainanalyze_3.q
+++ b/ql/src/test/queries/clientpositive/explainanalyze_3.q
@@ -40,9 +40,13 @@ use default;
 
 drop database newDB;
 
-explain analyze analyze table src compute statistics;
+drop table src_stats;
 
-explain analyze analyze table src compute statistics for columns;
+create table src_stats as select * from src;
+
+explain analyze analyze table src_stats compute statistics;
+
+explain analyze analyze table src_stats compute statistics for columns;
 
 explain analyze
 CREATE TEMPORARY MACRO SIGMOID (x DOUBLE) 1.0 / (1.0 + EXP(-x));
diff --git a/ql/src/test/queries/clientpositive/explainanalyze_5.q b/ql/src/test/queries/clientpositive/explainanalyze_5.q
index bb23e45705..577b7666bb 100644
--- a/ql/src/test/queries/clientpositive/explainanalyze_5.q
+++ b/ql/src/test/queries/clientpositive/explainanalyze_5.q
@@ -1,8 +1,12 @@
 set hive.stats.column.autogather=true;
 
-explain analyze analyze table src compute statistics;
+drop table src_stats;
 
-explain analyze analyze table src compute statistics for columns;
+create table src_stats as select * from src;
+
+explain analyze analyze table src_stats compute statistics;
+
+explain analyze analyze table src_stats compute statistics for columns;
 
 drop table src_multi2;
 
diff --git a/ql/src/test/results/clientpositive/tez/explainanalyze_3.q.out b/ql/src/test/results/clientpositive/tez/explainanalyze_3.q.out
index c86e1439fb..57dc950732 100644
--- a/ql/src/test/results/clientpositive/tez/explainanalyze_3.q.out
+++ b/ql/src/test/results/clientpositive/tez/explainanalyze_3.q.out
@@ -196,36 +196,52 @@ POSTHOOK: query: drop database newDB
 POSTHOOK: type: DROPDATABASE
 POSTHOOK: Input: database:newdb
 POSTHOOK: Output: database:newdb
-PREHOOK: query: analyze table src compute statistics
-PREHOOK: type: QUERY
+PREHOOK: query: drop table src_stats
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table src_stats
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table src_stats as select * from src
+PREHOOK: type: CREATETABLE_AS_SELECT
 PREHOOK: Input: default@src
-PREHOOK: Output: default@src
-FAILED: Hive Internal Error: java.lang.RuntimeException(Cannot overwrite read-only table: src)
-java.lang.RuntimeException: Cannot overwrite read-only table: src
-#### A masked pattern was here ####
-
-PREHOOK: query: explain analyze analyze table src compute statistics
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_stats
+POSTHOOK: query: create table src_stats as select * from src
+POSTHOOK: type: CREATETABLE_AS_SELECT
+POSTHOOK: Input: default@src
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_stats
+POSTHOOK: Lineage: src_stats.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: src_stats.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: analyze table src_stats compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src_stats
+PREHOOK: Output: default@src_stats
+POSTHOOK: query: analyze table src_stats compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src_stats
+POSTHOOK: Output: default@src_stats
+PREHOOK: query: explain analyze analyze table src_stats compute statistics
 PREHOOK: type: QUERY
-POSTHOOK: query: explain analyze analyze table src compute statistics
+POSTHOOK: query: explain analyze analyze table src_stats compute statistics
 POSTHOOK: type: QUERY
 Stage-2
   Stats-Aggr Operator
     Stage-0
       Map 1
-      TableScan [TS_0] (rows=500/0 width=10)
-        default@src,src,Tbl:COMPLETE,Col:COMPLETE
+      TableScan [TS_0] (rows=500/500 width=10)
+        default@src_stats,src_stats,Tbl:COMPLETE,Col:COMPLETE
 
-PREHOOK: query: analyze table src compute statistics for columns
+PREHOOK: query: analyze table src_stats compute statistics for columns
 PREHOOK: type: QUERY
-PREHOOK: Input: default@src
+PREHOOK: Input: default@src_stats
 #### A masked pattern was here ####
-POSTHOOK: query: analyze table src compute statistics for columns
+POSTHOOK: query: analyze table src_stats compute statistics for columns
 POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
+POSTHOOK: Input: default@src_stats
 #### A masked pattern was here ####
-PREHOOK: query: explain analyze analyze table src compute statistics for columns
+PREHOOK: query: explain analyze analyze table src_stats compute statistics for columns
 PREHOOK: type: QUERY
-POSTHOOK: query: explain analyze analyze table src compute statistics for columns
+POSTHOOK: query: explain analyze analyze table src_stats compute statistics for columns
 POSTHOOK: type: QUERY
 Vertex dependency in root stage
 Reducer 2 <- Map 1 (SIMPLE_EDGE)
@@ -235,16 +251,16 @@ Stage-2
     Stage-0
       Reducer 2
       File Output Operator [FS_6]
-        Group By Operator [GBY_4] (rows=1/1 width=960)
+        Group By Operator [GBY_4] (rows=1/1 width=984)
           Output:["_col0","_col1"],aggregations:["compute_stats(VALUE._col0)","compute_stats(VALUE._col1)"]
         <-Map 1 [SIMPLE_EDGE]
           SHUFFLE [RS_3]
             Group By Operator [GBY_2] (rows=1/1 width=984)
               Output:["_col0","_col1"],aggregations:["compute_stats(key, 16)","compute_stats(value, 16)"]
-              Select Operator [SEL_1] (rows=500/500 width=178)
+              Select Operator [SEL_1] (rows=500/500 width=10)
                 Output:["key","value"]
-                TableScan [TS_0] (rows=500/500 width=178)
-                  default@src,src,Tbl:COMPLETE,Col:COMPLETE,Output:["key","value"]
+                TableScan [TS_0] (rows=500/500 width=10)
+                  default@src_stats,src_stats,Tbl:COMPLETE,Col:NONE,Output:["key","value"]
 
 PREHOOK: query: CREATE TEMPORARY MACRO SIGMOID (x DOUBLE) 1.0 / (1.0 + EXP(-x))
 PREHOOK: type: CREATEMACRO
diff --git a/ql/src/test/results/clientpositive/tez/explainanalyze_5.q.out b/ql/src/test/results/clientpositive/tez/explainanalyze_5.q.out
index 39bc6f4ef7..7da21db68a 100644
--- a/ql/src/test/results/clientpositive/tez/explainanalyze_5.q.out
+++ b/ql/src/test/results/clientpositive/tez/explainanalyze_5.q.out
@@ -1,33 +1,49 @@
-PREHOOK: query: analyze table src compute statistics
-PREHOOK: type: QUERY
+PREHOOK: query: drop table src_stats
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table src_stats
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table src_stats as select * from src
+PREHOOK: type: CREATETABLE_AS_SELECT
 PREHOOK: Input: default@src
-PREHOOK: Output: default@src
-FAILED: Hive Internal Error: java.lang.RuntimeException(Cannot overwrite read-only table: src)
-java.lang.RuntimeException: Cannot overwrite read-only table: src
-#### A masked pattern was here ####
-
-PREHOOK: query: explain analyze analyze table src compute statistics
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_stats
+POSTHOOK: query: create table src_stats as select * from src
+POSTHOOK: type: CREATETABLE_AS_SELECT
+POSTHOOK: Input: default@src
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_stats
+POSTHOOK: Lineage: src_stats.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: src_stats.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: analyze table src_stats compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src_stats
+PREHOOK: Output: default@src_stats
+POSTHOOK: query: analyze table src_stats compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src_stats
+POSTHOOK: Output: default@src_stats
+PREHOOK: query: explain analyze analyze table src_stats compute statistics
 PREHOOK: type: QUERY
-POSTHOOK: query: explain analyze analyze table src compute statistics
+POSTHOOK: query: explain analyze analyze table src_stats compute statistics
 POSTHOOK: type: QUERY
 Stage-2
   Stats-Aggr Operator
     Stage-0
       Map 1
-      TableScan [TS_0] (rows=500/0 width=10)
-        default@src,src,Tbl:COMPLETE,Col:COMPLETE
+      TableScan [TS_0] (rows=500/500 width=10)
+        default@src_stats,src_stats,Tbl:COMPLETE,Col:COMPLETE
 
-PREHOOK: query: analyze table src compute statistics for columns
+PREHOOK: query: analyze table src_stats compute statistics for columns
 PREHOOK: type: QUERY
-PREHOOK: Input: default@src
+PREHOOK: Input: default@src_stats
 #### A masked pattern was here ####
-POSTHOOK: query: analyze table src compute statistics for columns
+POSTHOOK: query: analyze table src_stats compute statistics for columns
 POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
+POSTHOOK: Input: default@src_stats
 #### A masked pattern was here ####
-PREHOOK: query: explain analyze analyze table src compute statistics for columns
+PREHOOK: query: explain analyze analyze table src_stats compute statistics for columns
 PREHOOK: type: QUERY
-POSTHOOK: query: explain analyze analyze table src compute statistics for columns
+POSTHOOK: query: explain analyze analyze table src_stats compute statistics for columns
 POSTHOOK: type: QUERY
 Vertex dependency in root stage
 Reducer 2 <- Map 1 (SIMPLE_EDGE)
@@ -37,16 +53,16 @@ Stage-2
     Stage-0
       Reducer 2
       File Output Operator [FS_6]
-        Group By Operator [GBY_4] (rows=1/1 width=960)
+        Group By Operator [GBY_4] (rows=1/1 width=984)
           Output:["_col0","_col1"],aggregations:["compute_stats(VALUE._col0)","compute_stats(VALUE._col1)"]
         <-Map 1 [SIMPLE_EDGE]
           SHUFFLE [RS_3]
             Group By Operator [GBY_2] (rows=1/1 width=984)
               Output:["_col0","_col1"],aggregations:["compute_stats(key, 16)","compute_stats(value, 16)"]
-              Select Operator [SEL_1] (rows=500/500 width=178)
+              Select Operator [SEL_1] (rows=500/500 width=10)
                 Output:["key","value"]
-                TableScan [TS_0] (rows=500/500 width=178)
-                  default@src,src,Tbl:COMPLETE,Col:COMPLETE,Output:["key","value"]
+                TableScan [TS_0] (rows=500/500 width=10)
+                  default@src_stats,src_stats,Tbl:COMPLETE,Col:NONE,Output:["key","value"]
 
 PREHOOK: query: drop table src_multi2
 PREHOOK: type: DROPTABLE
