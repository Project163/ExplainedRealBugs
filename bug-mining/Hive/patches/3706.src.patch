diff --git a/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java b/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java
index 049de54a16..8148faa874 100644
--- a/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java
+++ b/hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatMultiOutputFormat.java
@@ -46,6 +46,7 @@
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.plan.FetchWork;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
+import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.LongWritable;
@@ -379,11 +380,12 @@ private List<String> getTableData(String table, String database) throws Exceptio
       List<Partition> partitions = hive.getPartitions(tbl);
       List<PartitionDesc> partDesc = new ArrayList<PartitionDesc>();
       List<Path> partLocs = new ArrayList<Path>();
+      TableDesc tableDesc = Utilities.getTableDesc(tbl);
       for (Partition part : partitions) {
         partLocs.add(part.getDataLocation());
-        partDesc.add(Utilities.getPartitionDesc(part));
+        partDesc.add(Utilities.getPartitionDescFromTableDesc(tableDesc, part, true));
       }
-      work = new FetchWork(partLocs, partDesc, Utilities.getTableDesc(tbl));
+      work = new FetchWork(partLocs, partDesc, tableDesc);
       work.setLimit(100);
     } else {
       work = new FetchWork(tbl.getDataLocation(), Utilities.getTableDesc(tbl));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index d8e463de23..d578f11f49 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
+import static com.google.common.base.Preconditions.checkNotNull;
+
 import java.beans.DefaultPersistenceDelegate;
 import java.beans.Encoder;
 import java.beans.ExceptionListener;
@@ -80,6 +82,7 @@
 import java.util.zip.InflaterInputStream;
 
 import org.antlr.runtime.CommonToken;
+import org.apache.calcite.util.ChunkList;
 import org.apache.commons.codec.binary.Base64;
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.lang.WordUtils;
@@ -1230,9 +1233,9 @@ public static PartitionDesc getPartitionDesc(Partition part) throws HiveExceptio
     return (new PartitionDesc(part));
   }
 
-  public static PartitionDesc getPartitionDescFromTableDesc(TableDesc tblDesc, Partition part)
-      throws HiveException {
-    return new PartitionDesc(part, tblDesc);
+  public static PartitionDesc getPartitionDescFromTableDesc(TableDesc tblDesc, Partition part,
+    boolean usePartSchemaProperties) throws HiveException {
+    return new PartitionDesc(part, tblDesc, usePartSchemaProperties);
   }
 
   private static String getOpTreeSkel_helper(Operator<?> op, String indent) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
index 29854d80a2..693d8c7e9f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -487,8 +487,6 @@ public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntit
     ArrayList<PartitionDesc> partDesc = new ArrayList<PartitionDesc>();
 
     Path tblDir = null;
-    TableDesc tblDesc = null;
-
     plan.setNameToSplitSample(parseCtx.getNameToSplitSample());
 
     if (partsList == null) {
@@ -575,6 +573,8 @@ public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntit
     //This read entity is a direct read entity and not an indirect read (that is when
     // this is being read because it is a dependency of a view).
     boolean isDirectRead = (parentViewInfo == null);
+    TableDesc tblDesc = null;
+    boolean initTableDesc = false;
 
     for (Partition part : parts) {
       if (part.getTable().isPartitioned()) {
@@ -647,12 +647,18 @@ public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntit
 
       // is it a partitioned table ?
       if (!part.getTable().isPartitioned()) {
-        assert ((tblDir == null) && (tblDesc == null));
+        assert (tblDir == null);
 
         tblDir = paths[0];
-        tblDesc = Utilities.getTableDesc(part.getTable());
+        if (!initTableDesc) {
+          tblDesc = Utilities.getTableDesc(part.getTable());
+          initTableDesc = true;
+        }
       } else if (tblDesc == null) {
-        tblDesc = Utilities.getTableDesc(part.getTable());
+        if (!initTableDesc) {
+          tblDesc = Utilities.getTableDesc(part.getTable());
+          initTableDesc = true;
+        }
       }
 
       if (props != null) {
@@ -678,7 +684,7 @@ public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntit
             partDesc.add(Utilities.getPartitionDesc(part));
           }
           else {
-            partDesc.add(Utilities.getPartitionDescFromTableDesc(tblDesc, part));
+            partDesc.add(Utilities.getPartitionDescFromTableDesc(tblDesc, part, false));
           }
         } catch (HiveException e) {
           LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
index 317454debc..3859177368 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
@@ -369,9 +369,10 @@ public final void setFiltered(boolean filtered) {
 
     private FetchWork convertToWork() throws HiveException {
       inputs.clear();
+      TableDesc tableDesc = Utilities.getTableDesc(table);
       if (!table.isPartitioned()) {
         inputs.add(new ReadEntity(table, parent, !table.isView() && parent == null));
-        FetchWork work = new FetchWork(table.getPath(), Utilities.getTableDesc(table));
+        FetchWork work = new FetchWork(table.getPath(), tableDesc);
         PlanUtils.configureInputJobPropertiesForStorageHandler(work.getTblDesc());
         work.setSplitSample(splitSample);
         return work;
@@ -382,7 +383,7 @@ private FetchWork convertToWork() throws HiveException {
       for (Partition partition : partsList.getNotDeniedPartns()) {
         inputs.add(new ReadEntity(partition, parent, parent == null));
         listP.add(partition.getDataLocation());
-        partP.add(Utilities.getPartitionDesc(partition));
+        partP.add(Utilities.getPartitionDescFromTableDesc(tableDesc, partition, true));
       }
       Table sourceTable = partsList.getSourceTable();
       inputs.add(new ReadEntity(sourceTable, parent, parent == null));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java
index b123511882..864301cf9e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java
@@ -81,18 +81,41 @@ public PartitionDesc(final TableDesc table, final LinkedHashMap<String, String>
   }
 
   public PartitionDesc(final Partition part) throws HiveException {
-    this.tableDesc = Utilities.getTableDesc(part.getTable());
+    PartitionDescConstructorHelper(part, Utilities.getTableDesc(part.getTable()), true);
     setProperties(part.getMetadataFromPartitionSchema());
-    partSpec = part.getSpec();
-    setInputFileFormatClass(part.getInputFormatClass());
-    setOutputFileFormatClass(part.getOutputFormatClass());
   }
 
-  public PartitionDesc(final Partition part,final TableDesc tblDesc) throws HiveException {
+  /** 
+   * @param part Partition
+   * @param tblDesc Table Descriptor
+   * @param usePartSchemaProperties Use Partition Schema Properties to set the
+   * partition descriptor properties. This is usually set to true by the caller
+   * if the table is partitioned, i.e. if the table has partition columns.
+   * @throws HiveException
+   */
+  public PartitionDesc(final Partition part,final TableDesc tblDesc,
+    boolean usePartSchemaProperties)
+    throws HiveException {
+    PartitionDescConstructorHelper(part,tblDesc, usePartSchemaProperties);
+    //We use partition schema properties to set the partition descriptor properties
+    // if usePartSchemaProperties is set to true.
+    if (usePartSchemaProperties) {
+      setProperties(part.getMetadataFromPartitionSchema());
+    } else {
+      // each partition maintains a large properties
+      setProperties(part.getSchemaFromTableSchema(tblDesc.getProperties()));
+    }
+  }
+
+  private void PartitionDescConstructorHelper(final Partition part,final TableDesc tblDesc, boolean setInputFileFormat)
+    throws HiveException {
     this.tableDesc = tblDesc;
-    setProperties(part.getSchemaFromTableSchema(tblDesc.getProperties())); // each partition maintains a large properties
-    partSpec = part.getSpec();
-    setOutputFileFormatClass(part.getInputFormatClass());
+    this.partSpec = part.getSpec();
+    if (setInputFileFormat) {
+      setInputFileFormatClass(part.getInputFormatClass());
+    } else {
+      setOutputFileFormatClass(part.getInputFormatClass());
+    }
     setOutputFileFormatClass(part.getOutputFormatClass());
   }
 
