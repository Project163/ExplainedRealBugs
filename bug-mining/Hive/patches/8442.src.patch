diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOptimisedBootstrap.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOptimisedBootstrap.java
index 42ef25756a..8d9429e33b 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOptimisedBootstrap.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOptimisedBootstrap.java
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.fs.ContentSummary;
 import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.QuotaUsage;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
@@ -32,6 +33,8 @@
 import org.apache.hadoop.hive.metastore.txn.TxnStore;
 import org.apache.hadoop.hive.metastore.txn.TxnUtils;
 import org.apache.hadoop.hive.metastore.utils.MetaStoreUtils;
+import org.apache.hadoop.hive.ql.exec.repl.ReplAck;
+import org.apache.hadoop.hive.ql.parse.repl.load.FailoverMetaData;
 import org.apache.hadoop.hive.ql.parse.repl.metric.MetricCollector;
 import org.apache.hadoop.hive.ql.exec.repl.OptimisedBootstrapUtils;
 import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;
@@ -1231,4 +1234,97 @@ public void testDbParametersAfterOptimizedBootstrap() throws Throwable {
     primary.run(String.format("select * from %s.t1", primaryDbName))
             .verifyResults(new String[]{"1", "2", "3"});
   }
+  @Test
+  public void testReverseFailoverBeforeOptimizedBootstrap() throws Throwable {
+    primary.run("use " + primaryDbName)
+            .run("create  table t1 (id string)")
+            .run("insert into table t1 values ('A')")
+            .dump(primaryDbName);
+    replica.load(replicatedDbName, primaryDbName);
+
+    primary.dump(primaryDbName);
+    replica.load(replicatedDbName, primaryDbName);
+    //initiate a controlled failover from primary to replica.
+    List<String> failoverConfigs = Arrays.asList("'" + HiveConf.ConfVars.HIVE_REPL_FAILOVER_START + "'='true'");
+    primary.dump(primaryDbName, failoverConfigs);
+    replica.load(replicatedDbName, primaryDbName, failoverConfigs);
+    primary.run("use " + primaryDbName)
+            .run("insert into t1 values('B')"); //modify primary after failover.
+    //initiate a controlled failover from replica to primary before the first cycle of optimized bootstrap is run.
+    WarehouseInstance.Tuple reverseDump = replica.run("use " + replicatedDbName)
+            .run("create table t2 (col int)")
+            .run("insert into t2 values(1),(2)")
+            .dump(replicatedDbName, failoverConfigs);
+
+    // the first reverse dump should NOT be failover ready.
+    FileSystem fs = new Path(reverseDump.dumpLocation).getFileSystem(conf);
+    assertTrue(fs.exists(new Path(reverseDump.dumpLocation, EVENT_ACK_FILE)));
+    Path dumpPath = new Path(reverseDump.dumpLocation, ReplUtils.REPL_HIVE_BASE_DIR);
+    assertFalse(fs.exists(new Path(dumpPath, FailoverMetaData.FAILOVER_METADATA)));
+    assertFalse(fs.exists(new Path(dumpPath, ReplAck.FAILOVER_READY_MARKER.toString())));
+    // ensure load was successful.
+    primary.load(primaryDbName, replicatedDbName, failoverConfigs);
+    assertTrue(fs.exists(new Path(reverseDump.dumpLocation, TABLE_DIFF_COMPLETE_DIRECTORY)));
+    assertTrue(fs.exists(new Path(dumpPath, ReplAck.LOAD_ACKNOWLEDGEMENT.toString())));
+    HashSet<String> tableDiffEntries = getTablesFromTableDiffFile(new Path(reverseDump.dumpLocation), conf);
+    assertTrue(!tableDiffEntries.isEmpty()); // we have modified a table t1 at source
+
+    assertTrue(MetaStoreUtils.isDbBeingFailedOverAtEndpoint(primary.getDatabase(primaryDbName),
+            MetaStoreUtils.FailoverEndpoint.SOURCE));
+    assertTrue(MetaStoreUtils.isDbBeingFailedOverAtEndpoint(replica.getDatabase(replicatedDbName),
+            MetaStoreUtils.FailoverEndpoint.TARGET));
+
+    //do a second dump, this dump should NOT be failover ready as some tables need to be bootstrapped (here it is t1).
+    reverseDump = replica.dump(replicatedDbName, failoverConfigs);
+    assertTrue(fs.exists(new Path(reverseDump.dumpLocation, OptimisedBootstrapUtils.BOOTSTRAP_TABLES_LIST)));
+    dumpPath = new Path(reverseDump.dumpLocation, ReplUtils.REPL_HIVE_BASE_DIR);
+    assertFalse(fs.exists(new Path(dumpPath, ReplAck.FAILOVER_READY_MARKER.toString())));
+
+    primary.load(primaryDbName, replicatedDbName, failoverConfigs);
+    //ensure optimized bootstrap was successful
+    primary.run(String.format("select * from %s.t1", primaryDbName))
+            .verifyResults(new String[]{"A"})
+            .run(String.format("select * from %s.t2", primaryDbName))
+            .verifyResults(new String[]{"1", "2"});
+
+    assertFalse(MetaStoreUtils.isDbBeingFailedOverAtEndpoint(primary.getDatabase(primaryDbName),
+            MetaStoreUtils.FailoverEndpoint.SOURCE));
+    assertFalse(MetaStoreUtils.isDbBeingFailedOverAtEndpoint(replica.getDatabase(replicatedDbName),
+            MetaStoreUtils.FailoverEndpoint.TARGET));
+
+    //do a third dump, this should be failover ready.
+    reverseDump = replica.dump(replicatedDbName, failoverConfigs);
+    dumpPath = new Path(reverseDump.dumpLocation, ReplUtils.REPL_HIVE_BASE_DIR);
+    assertTrue(fs.exists(new Path(dumpPath, ReplAck.FAILOVER_READY_MARKER.toString())));
+    assertTrue(fs.exists(new Path(dumpPath, FailoverMetaData.FAILOVER_METADATA)));
+
+    primary.load(primaryDbName, replicatedDbName, failoverConfigs);
+    assertTrue(MetaStoreUtils.isDbBeingFailedOverAtEndpoint(primary.getDatabase(primaryDbName),
+            MetaStoreUtils.FailoverEndpoint.TARGET));
+    assertTrue(MetaStoreUtils.isDbBeingFailedOverAtEndpoint(replica.getDatabase(replicatedDbName),
+            MetaStoreUtils.FailoverEndpoint.SOURCE));
+
+    //initiate a failover from primary to replica.
+    WarehouseInstance.Tuple forwardDump = primary.dump(primaryDbName, failoverConfigs);
+    assertTrue(fs.exists(new Path(forwardDump.dumpLocation, EVENT_ACK_FILE)));
+    dumpPath = new Path(forwardDump.dumpLocation, ReplUtils.REPL_HIVE_BASE_DIR);
+    assertFalse(fs.exists(new Path(dumpPath, FailoverMetaData.FAILOVER_METADATA)));
+    assertFalse(fs.exists(new Path(dumpPath, ReplAck.FAILOVER_READY_MARKER.toString())));
+
+    replica.load(replicatedDbName, primaryDbName, failoverConfigs);
+    assertTrue(fs.exists(new Path(forwardDump.dumpLocation, TABLE_DIFF_COMPLETE_DIRECTORY)));
+    tableDiffEntries = getTablesFromTableDiffFile(new Path(forwardDump.dumpLocation), conf);
+    assertTrue(tableDiffEntries.isEmpty()); // nothing was modified
+    // here second dump will be failover ready, since no tables need to be bootstrapped.
+    forwardDump = primary.dump(primaryDbName, failoverConfigs);
+    assertTrue(fs.exists(new Path(forwardDump.dumpLocation, OptimisedBootstrapUtils.BOOTSTRAP_TABLES_LIST)));
+    dumpPath = new Path(forwardDump.dumpLocation, ReplUtils.REPL_HIVE_BASE_DIR);
+    assertTrue(fs.exists(new Path(dumpPath, ReplAck.FAILOVER_READY_MARKER.toString())));
+
+    replica.load(replicatedDbName, primaryDbName, failoverConfigs);
+    assertTrue(MetaStoreUtils.isDbBeingFailedOverAtEndpoint(primary.getDatabase(primaryDbName),
+            MetaStoreUtils.FailoverEndpoint.SOURCE));
+    assertTrue(MetaStoreUtils.isDbBeingFailedOverAtEndpoint(replica.getDatabase(replicatedDbName),
+            MetaStoreUtils.FailoverEndpoint.TARGET));
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
index c20147dfa5..8f2d94d5a6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
@@ -463,7 +463,7 @@ private void addAtlasDumpTask(boolean bootstrap, Path prevHiveDumpDir) {
 
 
   private void finishRemainingTasks() throws HiveException {
-    boolean isFailoverInProgress = shouldFailover() && !work.isBootstrap();
+    boolean isFailoverInProgress = shouldFailover() && !work.isBootstrap() && !createEventMarker;
     if (isFailoverInProgress) {
       Utils.create(new Path(work.getCurrentDumpPath(), ReplUtils.REPL_HIVE_BASE_DIR + File.separator
               + ReplAck.FAILOVER_READY_MARKER), conf);
@@ -831,7 +831,7 @@ private Long incrementalDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive
       setReplSourceFor(hiveDb, dbName, db);
     }
     if (shouldFailover()) {
-      if (!MetaStoreUtils.isDbBeingFailedOver(db)) {
+      if (!MetaStoreUtils.isDbBeingFailedOverAtEndpoint(db, MetaStoreUtils.FailoverEndpoint.SOURCE)) {
         setReplFailoverEnabledAtSource(db);
       }
       fetchFailoverMetadata(hiveDb);
@@ -863,14 +863,13 @@ private Long incrementalDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive
       replLogger.startLog();
       Map<String, Long> metricMap = new HashMap<>();
       metricMap.put(ReplUtils.MetricName.EVENTS.name(), estimatedNumEvents);
-
+      int size = tablesForBootstrap.size();
+      if (size > 0) {
+        metricMap.put(ReplUtils.MetricName.TABLES.name(), (long) tablesForBootstrap.size());
+      }
       if (conf.getBoolVar(HiveConf.ConfVars.HIVE_REPL_FAILOVER_START)) {
         work.getMetricCollector().reportFailoverStart(getName(), metricMap, work.getFailoverMetadata());
       } else {
-        int size = tablesForBootstrap.size();
-        if (size > 0) {
-          metricMap.put(ReplUtils.MetricName.TABLES.name(), (long) tablesForBootstrap.size());
-        }
         work.getMetricCollector().reportStageStart(getName(), metricMap);
       }
       long dumpedCount = resumeFrom - work.eventFrom;
