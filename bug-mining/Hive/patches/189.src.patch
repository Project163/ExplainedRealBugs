diff --git a/common/src/java/org/apache/hadoop/hive/common/JavaUtils.java b/common/src/java/org/apache/hadoop/hive/common/JavaUtils.java
index a332b8dacd..3398d922e3 100644
--- a/common/src/java/org/apache/hadoop/hive/common/JavaUtils.java
+++ b/common/src/java/org/apache/hadoop/hive/common/JavaUtils.java
@@ -24,9 +24,12 @@
 public class JavaUtils {
 
   /**
-   * Standard way of getting classloader across all of Hive.
+   * Standard way of getting classloader in Hive code (outside of Hadoop).
+   * 
    * Uses the context loader to get access to classpaths to auxiliary and jars
    * added with 'add jar' command. Falls back to current classloader.
+   * 
+   * In Hadoop-related code, we use Configuration.getClassLoader().
    */
   public static ClassLoader getClassLoader() {
     ClassLoader classLoader = Thread.currentThread().getContextClassLoader();
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index a35060dcfd..f0073d5bd2 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -104,6 +104,10 @@ public static enum ConfVars {
     // hive jar
     HIVEJAR("hive.jar.path", ""), 
     HIVEAUXJARS("hive.aux.jars.path", ""),
+    
+    // hive added files and jars
+    HIVEADDEDFILES("hive.added.files.path", ""),
+    HIVEADDEDJARS("hive.added.jars.path", ""),
    
     // for hive script operator
     HIVETABLENAME("hive.table.name", ""),
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
index 4ae9b6a070..3c0f5534e0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
@@ -40,6 +40,7 @@
 import org.apache.hadoop.mapred.FileInputFormat;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.plan.mapredWork;
 import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.partitionDesc;
@@ -105,12 +106,21 @@ private void initializeFiles(String prop, String files) {
   public void initialize(HiveConf conf) {
     super.initialize(conf);
     job = new JobConf(conf, ExecDriver.class);
-    initializeFiles(
-        "tmpfiles",
-        getResourceFiles(job, SessionState.ResourceType.FILE));
-    initializeFiles(
-        "tmpjars",
-        getResourceFiles(job, SessionState.ResourceType.JAR));
+    // NOTE: initialize is only called if it is in non-local mode.
+    // In case it's in non-local mode, we need to move the SessionState files
+    // and jars to jobConf.
+    // In case it's in local mode, MapRedTask will set the jobConf.
+    //
+    // "tmpfiles" and "tmpjars" are set by the method ExecDriver.execute(),
+    // which will be called by both local and NON-local mode.
+    String addedFiles = getResourceFiles(job, SessionState.ResourceType.FILE);
+    if (StringUtils.isNotBlank(addedFiles)) {
+      HiveConf.setVar(job, ConfVars.HIVEADDEDFILES, addedFiles);
+    }
+    String addedJars = getResourceFiles(job, SessionState.ResourceType.JAR);
+    if (StringUtils.isNotBlank(addedJars)) {
+      HiveConf.setVar(job, ConfVars.HIVEADDEDJARS, addedJars);
+    }
   }
 
   /**
@@ -342,12 +352,24 @@ public int execute() {
     job.setOutputKeyClass(Text.class);
     job.setOutputValueClass(Text.class);
 
+    // Transfer HIVEAUXJARS and HIVEADDEDJARS to "tmpjars" so hadoop understands it
     String auxJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEAUXJARS);
-    if (StringUtils.isNotBlank(auxJars)) {
-      LOG.info("adding libjars: " + auxJars);
-      job.set("tmpjars", auxJars);
+    String addedJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDJARS);
+    if (StringUtils.isNotBlank(auxJars) || StringUtils.isNotBlank(addedJars)) {
+      String allJars = 
+        StringUtils.isNotBlank(auxJars)
+        ? (StringUtils.isNotBlank(addedJars) ? addedJars + "," + auxJars : auxJars)
+        : addedJars;
+      LOG.info("adding libjars: " + allJars);
+      initializeFiles("tmpjars", allJars);
     }
 
+    // Transfer HIVEADDEDFILES to "tmpfiles" so hadoop understands it
+    String addedFiles = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDFILES);
+    if (StringUtils.isNotBlank(addedFiles)) {
+      initializeFiles("tmpfiles", addedFiles);
+    }
+    
     int returnVal = 0;
     RunningJob rj = null, orig_rj = null;
     boolean success = false;
@@ -530,16 +552,26 @@ public static void main(String[] args) throws IOException, HiveException {
         .equals("local");
     if (localMode) {
       String auxJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS);
-      if (StringUtils.isNotBlank(auxJars)) {
-        try {
-          Utilities.addToClassPath(StringUtils.split(auxJars, ","));
-        } catch (Exception e) {
-          throw new HiveException(e.getMessage(), e);
+      String addedJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEADDEDJARS);
+      try {
+        ClassLoader loader = conf.getClassLoader();
+        if (StringUtils.isNotBlank(auxJars)) {
+          loader = Utilities.addToClassPath(loader, StringUtils.split(auxJars, ","));
         }
+        if (StringUtils.isNotBlank(addedJars)) {
+          loader = Utilities.addToClassPath(loader, StringUtils.split(addedJars, ","));
+        }
+        conf.setClassLoader(loader);
+        // Also set this to the Thread ContextClassLoader, so new threads will inherit
+        // this class loader, and propagate into newly created Configurations by those
+        // new threads.
+        Thread.currentThread().setContextClassLoader(loader);
+      } catch (Exception e) {
+        throw new HiveException(e.getMessage(), e);
       }
     }
 
-    mapredWork plan = Utilities.deserializeMapRedWork(pathData);
+    mapredWork plan = Utilities.deserializeMapRedWork(pathData, conf);
     ExecDriver ed = new ExecDriver(plan, conf, isSilent);
     int ret = ed.execute();
     if (ret != 0) {
@@ -621,7 +653,8 @@ private void addInputPaths(JobConf job, mapredWork work, String hiveScratchDir)
         !work.getAliasToWork().isEmpty()) {
       String oneAlias = (String)work.getAliasToWork().keySet().toArray()[0];
       
-      Class<? extends HiveOutputFormat> outFileFormat = (Class<? extends HiveOutputFormat>)Class.forName("org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat");
+      Class<? extends HiveOutputFormat> outFileFormat = (Class<? extends HiveOutputFormat>)
+          job.getClassByName("org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat");
       
       String newFile = hiveScratchDir + File.separator + (++numEmptyPaths);
       Path newPath = new Path(newFile);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
index 2c91055850..e2b99cd006 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.*;
+import java.net.URLClassLoader;
 import java.util.*;
 
 import org.apache.hadoop.io.*;
@@ -55,6 +56,14 @@ private void init() {
   }
   
   public void configure(JobConf job) {
+    try {
+      l4j.info("conf classpath = " 
+          + Arrays.asList(((URLClassLoader)job.getClassLoader()).getURLs()));
+      l4j.info("thread classpath = " 
+          + Arrays.asList(((URLClassLoader)Thread.currentThread().getContextClassLoader()).getURLs()));
+    } catch (Exception e) {
+      l4j.info("cannot get classpath: " + e.getMessage());
+    }
     try {
       init();
       jc = job;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
index bee1dbe24e..700e378f99 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.*;
+import java.net.URLClassLoader;
 import java.util.*;
 
 import org.apache.hadoop.mapred.*;
@@ -69,6 +70,14 @@ public class ExecReducer extends MapReduceBase implements Reducer {
   }
 
   public void configure(JobConf job) {
+    try {
+      l4j.info("conf classpath = " 
+          + Arrays.asList(((URLClassLoader)job.getClassLoader()).getURLs()));
+      l4j.info("thread classpath = " 
+          + Arrays.asList(((URLClassLoader)Thread.currentThread().getContextClassLoader()).getURLs()));
+    } catch (Exception e) {
+      l4j.info("cannot get classpath: " + e.getMessage());
+    }
     jc = job;
     mapredWork gWork = Utilities.getMapRedWork(job);
     reducer = gWork.getReducer();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
index b8cdd0253e..7d64ad2f59 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
@@ -20,6 +20,7 @@
 
 import java.util.*;
 import java.io.*;
+import java.net.URLClassLoader;
 
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
@@ -153,7 +154,7 @@ private MapOpCtx initObjectInspector(Configuration hconf, String onefile) throws
       if ((className == "") || (className == null)) {
         throw new HiveException("SerDe class or the SerDe class name is not set for table: " + td.getProperties().getProperty("name"));
       }
-      sdclass = MapOperator.class.getClassLoader().loadClass(className);
+      sdclass = hconf.getClassByName(className);
     }
     
     deserializer = (Deserializer) sdclass.newInstance();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
index 5e3c99c923..b288fdda03 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
@@ -28,6 +28,7 @@
 import org.apache.hadoop.hive.ql.plan.mapredWork;
 import org.apache.hadoop.hive.ql.exec.Utilities.*;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.session.SessionState;
 
 import org.apache.commons.lang.StringUtils;
@@ -48,27 +49,31 @@ public int execute() {
       String hadoopExec = conf.getVar(HiveConf.ConfVars.HADOOPBIN);
       String hiveJar = conf.getJar();
 
-      String addedJars = ExecDriver.getResourceFiles(conf, SessionState.ResourceType.JAR);
-      if (!StringUtils.isEmpty(addedJars)) {
-        // Add addedJars to auxJars
+      String libJarsOption;
+      {
+        String addedJars = ExecDriver.getResourceFiles(conf, SessionState.ResourceType.JAR);
+        conf.setVar(ConfVars.HIVEADDEDJARS, addedJars);
+
         String auxJars = conf.getAuxJars();
-        if (StringUtils.isEmpty(auxJars)) {
-          auxJars = addedJars;
+        // Put auxjars and addedjars together into libjars
+        if (StringUtils.isEmpty(addedJars)) {
+          if (StringUtils.isEmpty(auxJars)) {
+            libJarsOption = " ";
+          } else {
+            libJarsOption = " -libjars " + auxJars + " ";
+          }
         } else {
-          auxJars = auxJars + "," + addedJars;
+          if (StringUtils.isEmpty(auxJars)) {
+            libJarsOption = " -libjars " + addedJars + " ";
+          } else {
+            libJarsOption = " -libjars " + addedJars + "," + auxJars + " ";
+          }   
         }
-        conf.setAuxJars(auxJars);
-      }
-      // Generate the hiveCOnfArgs after potentially adding the jars
-      String hiveConfArgs = ExecDriver.generateCmdLine(conf);
-
-      String auxJars = conf.getAuxJars();
-      if (StringUtils.isEmpty(auxJars)) {
-        auxJars = " ";
-      } else {
-        auxJars = " -libjars " + auxJars + " ";
       }
 
+      // Generate the hiveConfArgs after potentially adding the jars
+      String hiveConfArgs = ExecDriver.generateCmdLine(conf);
+      
       mapredWork plan = getWork();
 
       File planFile = File.createTempFile("plan", ".xml");
@@ -78,7 +83,7 @@ public int execute() {
 
       String isSilent = "true".equalsIgnoreCase(System.getProperty("test.silent"))
                         ? "-silent" : "";
-      String cmdLine = hadoopExec + " jar " + auxJars + " " + hiveJar 
+      String cmdLine = hadoopExec + " jar " + libJarsOption + " " + hiveJar 
           + " org.apache.hadoop.hive.ql.exec.ExecDriver -plan "
           + planFile.toString() + " " + isSilent + " " + hiveConfArgs; 
       
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index a4cafdec0c..04b8b1ee8d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -99,7 +99,7 @@ public static mapredWork getMapRedWork (Configuration job) {
           if(gWork != null)
             return (gWork);
           InputStream in = new FileInputStream("HIVE_PLAN");
-          mapredWork ret = deserializeMapRedWork(in);
+          mapredWork ret = deserializeMapRedWork(in, job);
           gWork = ret;
         }
         gWork.initialize();
@@ -187,8 +187,8 @@ public static void serializeMapRedWork(mapredWork w, OutputStream out) {
     e.close();
   }
 
-  public static mapredWork deserializeMapRedWork (InputStream in) {
-    XMLDecoder d = new XMLDecoder(in);
+  public static mapredWork deserializeMapRedWork (InputStream in, Configuration conf) {
+    XMLDecoder d = new XMLDecoder(in, null, null, conf.getClassLoader());
     mapredWork ret = (mapredWork)d.readObject();
     d.close();
     return (ret);
@@ -723,9 +723,8 @@ public static String getNameMessage(Exception e) {
    * @param newPaths
    *          Array of classpath elements
    */
-  public static void addToClassPath(String[] newPaths) throws Exception {
-    Thread curThread = Thread.currentThread();
-    URLClassLoader loader = (URLClassLoader) curThread.getContextClassLoader();
+  public static ClassLoader addToClassPath(ClassLoader cloader, String[] newPaths) throws Exception {
+    URLClassLoader loader = (URLClassLoader)cloader;
     List<URL> curPath = Arrays.asList(loader.getURLs());
     ArrayList<URL> newPath = new ArrayList<URL>();
 
@@ -746,8 +745,7 @@ public static void addToClassPath(String[] newPaths) throws Exception {
       }
     }
 
-    loader = new URLClassLoader(curPath.toArray(new URL[0]), loader);
-    curThread.setContextClassLoader(loader);
+    return new URLClassLoader(curPath.toArray(new URL[0]), loader);
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
index f0b04927a4..4ca65c8c31 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
@@ -22,12 +22,16 @@
 import java.io.DataOutput;
 import java.io.IOException;
 import java.lang.reflect.Method;
+import java.net.URLClassLoader;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.plan.mapredWork;
@@ -61,7 +65,7 @@ public class HiveInputFormat<K extends WritableComparable,
    * HiveInputSplit encapsulates an InputSplit with its corresponding inputFormatClass.
    * The reason that it derives from FileSplit is to make sure "map.input.file" in MapTask.
    */
-  public static class HiveInputSplit extends FileSplit implements InputSplit {
+  public static class HiveInputSplit extends FileSplit implements InputSplit, Configurable {
 
     InputSplit inputSplit;
     String     inputFormatClassName;
@@ -121,7 +125,8 @@ public String[] getLocations() throws IOException {
     public void readFields(DataInput in) throws IOException {
       String inputSplitClassName = in.readUTF();
       try {
-        inputSplit = (InputSplit) ReflectionUtils.newInstance(Class.forName(inputSplitClassName), job);
+        inputSplit = (InputSplit) ReflectionUtils.newInstance(
+            conf.getClassByName(inputSplitClassName), conf);
       } catch (Exception e) {
         throw new IOException("Cannot create an instance of InputSplit class = "
             + inputSplitClassName + ":" + e.getMessage());
@@ -135,19 +140,31 @@ public void write(DataOutput out) throws IOException {
       inputSplit.write(out);
       out.writeUTF(inputFormatClassName);
     }
+
+    Configuration conf;
+    
+    @Override
+    public Configuration getConf() {
+      return conf;
+    }
+
+    @Override
+    public void setConf(Configuration conf) {
+      this.conf = conf;
+    }
   }
 
-  static JobConf job;
+  JobConf job;
 
   public void configure(JobConf job) {
-    HiveInputFormat.job = job;
+    this.job = job;
   }
 
   /**
    * A cache of InputFormat instances.
    */
   private static Map<Class,InputFormat<WritableComparable, Writable>> inputFormats;
-  static InputFormat<WritableComparable, Writable> getInputFormatFromCache(Class inputFormatClass) throws IOException {
+  static InputFormat<WritableComparable, Writable> getInputFormatFromCache(Class inputFormatClass, JobConf job) throws IOException {
     if (inputFormats == null) {
       inputFormats = new HashMap<Class, InputFormat<WritableComparable, Writable>>();
     }
@@ -174,12 +191,12 @@ public RecordReader getRecordReader(InputSplit split, JobConf job,
     Class inputFormatClass = null;
     try {
       inputFormatClassName = hsplit.inputFormatClassName();
-      inputFormatClass = Class.forName(inputFormatClassName);
+      inputFormatClass = job.getClassByName(inputFormatClassName);
     } catch (Exception e) {
       throw new IOException("cannot find class " + inputFormatClassName);
     }
 
-    InputFormat inputFormat = getInputFormatFromCache(inputFormatClass);
+    InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);
 
     return new HiveRecordReader(inputFormat.getRecordReader(inputSplit, job, reporter));
   }
@@ -208,7 +225,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
       tableDesc table = getTableDescFromPath(dir);
       // create a new InputFormat instance if this is the first time to see this class
       Class inputFormatClass = table.getInputFileFormatClass();
-      InputFormat inputFormat = getInputFormatFromCache(inputFormatClass);
+      InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);
 
       FileInputFormat.setInputPaths(newjob, dir);
       newjob.setInputFormat(inputFormat.getClass());
@@ -234,7 +251,7 @@ public void validateInput(JobConf job) throws IOException {
     for (Path dir: dirs) {
       tableDesc table = getTableDescFromPath(dir);
       // create a new InputFormat instance if this is the first time to see this class
-      InputFormat inputFormat = getInputFormatFromCache(table.getInputFileFormatClass());
+      InputFormat inputFormat = getInputFormatFromCache(table.getInputFileFormatClass(), job);
 
       FileInputFormat.setInputPaths(newjob, dir);
       newjob.setInputFormat(inputFormat.getClass());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java b/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
index 802c977152..af87a937fe 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
@@ -981,8 +981,8 @@ private void init() throws IOException {
         throw new VersionMismatchException(VERSION[3], version);
 
       try {
-        Class<?> keyCls = Class.forName(Text.readString(in));
-        Class<?> valCls = Class.forName(Text.readString(in));
+        Class<?> keyCls = conf.getClassByName(Text.readString(in));
+        Class<?> valCls = conf.getClassByName(Text.readString(in));
         if (!keyCls.equals(KeyBuffer.class)
             || !valCls.equals(ValueBuffer.class))
           throw new IOException(file + " not a RCFile");
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index 4c5cf5b57d..f768876d5d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -314,7 +314,9 @@ public static String validateFile(Set<String> curFiles, String newFile) {
   public static boolean registerJar(String newJar) {
     LogHelper console = getConsole();
     try {
-      Utilities.addToClassPath(StringUtils.split(newJar, ","));
+      ClassLoader loader = Thread.currentThread().getContextClassLoader();
+      Thread.currentThread().setContextClassLoader(
+          Utilities.addToClassPath(loader, StringUtils.split(newJar, ",")));
       console.printInfo("Added " + newJar + " to class path");
       return true;
     } catch (Exception e) {
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/ThriftDeserializer.java b/serde/src/java/org/apache/hadoop/hive/serde2/ThriftDeserializer.java
index bd2258f5b3..2379c92b7d 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/ThriftDeserializer.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/ThriftDeserializer.java
@@ -39,7 +39,7 @@ public void initialize(Configuration job, Properties tbl) throws SerDeException
       // per Table basis
 
       String className = tbl.getProperty(org.apache.hadoop.hive.serde.Constants.SERIALIZATION_CLASS);
-      Class<?> recordClass = Class.forName(className);
+      Class<?> recordClass = job.getClassByName(className);
 
       String protoName = tbl.getProperty(org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT);
       if (protoName == null) {
