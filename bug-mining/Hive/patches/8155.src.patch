diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index b464165623..fd9507003b 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -5536,10 +5536,12 @@ public static enum ConfVars {
 
     HIVE_QUERY_REEXECUTION_ENABLED("hive.query.reexecution.enabled", true,
         "Enable query reexecutions"),
-    HIVE_QUERY_REEXECUTION_STRATEGIES("hive.query.reexecution.strategies", "overlay,reoptimize,reexecute_lost_am,dagsubmit",
+    HIVE_QUERY_REEXECUTION_STRATEGIES("hive.query.reexecution.strategies",
+        "overlay,reoptimize,reexecute_lost_am,dagsubmit,recompile_without_cbo",
         "comma separated list of plugin can be used:\n"
             + "  overlay: hiveconf subtree 'reexec.overlay' is used as an overlay in case of an execution errors out\n"
             + "  reoptimize: collects operator statistics during execution and recompile the query after a failure\n"
+            + "  recompile_without_cbo: recompiles query after a CBO failure\n"
             + "  reexecute_lost_am: reexecutes query if it failed due to tez am node gets decommissioned"),
     HIVE_QUERY_REEXECUTION_STATS_PERSISTENCE("hive.query.reexecution.stats.persist.scope", "metastore",
         new StringSet("query", "hiveserver", "metastore"),
@@ -5561,7 +5563,8 @@ public static enum ConfVars {
         "Size of the runtime statistics cache. Unit is: OperatorStat entry; a query plan consist ~100."),
     HIVE_QUERY_PLANMAPPER_LINK_RELNODES("hive.query.planmapper.link.relnodes", true,
         "Whether to link Calcite nodes to runtime statistics."),
-
+    HIVE_QUERY_MAX_RECOMPILATION_COUNT("hive.query.recompilation.max.count", 1,
+        "Maximum number of re-compilations for a single query."),
     HIVE_SCHEDULED_QUERIES_EXECUTOR_ENABLED("hive.scheduled.queries.executor.enabled", true,
         "Controls whether HS2 will run scheduled query executor."),
     HIVE_SCHEDULED_QUERIES_NAMESPACE("hive.scheduled.queries.namespace", "hive",
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/reexec/TestReExecuteKilledTezAMQueryPlugin.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/reexec/TestReExecuteKilledTezAMQueryPlugin.java
index adcc30186d..7b71432d07 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/reexec/TestReExecuteKilledTezAMQueryPlugin.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/reexec/TestReExecuteKilledTezAMQueryPlugin.java
@@ -70,7 +70,7 @@ static HiveConf defaultConf() throws Exception {
   public static void beforeTest() throws Exception {
     conf = defaultConf();
     conf.setVar(HiveConf.ConfVars.USERS_IN_ADMIN_ROLE, System.getProperty("user.name"));
-    conf.set(HiveConf.ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES.varname, "reexecute_lost_am");
+    conf.set(HiveConf.ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES.varname, "recompile_without_cbo,reexecute_lost_am");
     MiniHS2.cleanupLocalDir();
     Class.forName(MiniHS2.getJdbcDriverName());
     miniHS2 = new MiniHS2(conf, MiniHS2.MiniClusterType.LLAP);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java b/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java
index 8620fd0143..c556139f4e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java
@@ -54,6 +54,7 @@
 import org.apache.hadoop.hive.ql.plan.PlanUtils;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
+import org.apache.hadoop.hive.ql.reexec.ReCompileException;
 import org.apache.hadoop.hive.ql.security.authorization.command.CommandAuthorizer;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
@@ -94,7 +95,7 @@ public Compiler(Context context, DriverContext driverContext, DriverState driver
   public QueryPlan compile(String rawCommand, boolean deferClose) throws CommandProcessorException {
     initialize(rawCommand);
 
-    boolean compileError = false;
+    Throwable compileException = null;
     boolean parsed = false;
     QueryPlan plan = null;
     try {
@@ -111,15 +112,15 @@ public QueryPlan compile(String rawCommand, boolean deferClose) throws CommandPr
       authorize(sem);
       explainOutput(sem, plan);
     } catch (CommandProcessorException cpe) {
-      compileError = true;
+      compileException = cpe.getCause();
       throw cpe;
     } catch (Exception e) {
-      compileError = true;
+      compileException = e;
       DriverUtils.checkInterrupted(driverState, driverContext, "during query compilation: " + e.getMessage(), null,
           null);
       handleException(e);
     } finally {
-      cleanUp(compileError, parsed, deferClose);
+      cleanUp(compileException, parsed, deferClose);
     }
 
     return plan;
@@ -472,17 +473,19 @@ private void handleException(Exception e) throws CommandProcessorException {
       errorMessage += ". Failed command: " + driverContext.getQueryString();
     }
 
-    CONSOLE.printError(errorMessage, "\n" + StringUtils.stringifyException(e));
+    if (!(e instanceof ReCompileException)) {
+      CONSOLE.printError(errorMessage, "\n" + StringUtils.stringifyException(e));
+    }
     throw DriverUtils.createProcessorException(driverContext, error.getErrorCode(), errorMessage, error.getSQLState(),
         e);
   }
 
-  private void cleanUp(boolean compileError, boolean parsed, boolean deferClose) {
+  private void cleanUp(Throwable compileException, boolean parsed, boolean deferClose) {
     // Trigger post compilation hook. Note that if the compilation fails here then
     // before/after execution hook will never be executed.
     if (parsed) {
       try {
-        driverContext.getHookRunner().runAfterCompilationHook(context.getCmd(), compileError);
+        driverContext.getHookRunner().runAfterCompilationHook(driverContext, context, compileException);
       } catch (Exception e) {
         LOG.warn("Failed when invoking query after-compilation hook.", e);
       }
@@ -497,7 +500,7 @@ private void cleanUp(boolean compileError, boolean parsed, boolean deferClose) {
       LOG.info("Compiling command(queryId={}) has been interrupted after {} seconds", driverContext.getQueryId(),
           duration);
     } else {
-      driverState.compilationFinishedWithLocking(compileError);
+      driverState.compilationFinishedWithLocking(compileException != null);
       LOG.info("Completed compiling command(queryId={}); Time taken: {} seconds", driverContext.getQueryId(),
           duration);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 1696f2a502..8d1657eae4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -474,6 +474,7 @@ private void compileInternal(String command, boolean deferClose) throws CommandP
    * @param resetTaskIds Resets taskID counter if true.
    * @return 0 for ok
    */
+  @VisibleForTesting
   public int compile(String command, boolean resetTaskIds) {
     try {
       compile(command, resetTaskIds, false);
@@ -494,7 +495,7 @@ public int compile(String command, boolean resetTaskIds) {
    */
   @VisibleForTesting
   public void compile(String command, boolean resetTaskIds, boolean deferClose) throws CommandProcessorException {
-    preparForCompile(resetTaskIds);
+    prepareForCompile(resetTaskIds);
 
     Compiler compiler = new Compiler(context, driverContext, driverState);
     QueryPlan plan = compiler.compile(command, deferClose);
@@ -503,7 +504,7 @@ public void compile(String command, boolean resetTaskIds, boolean deferClose) th
     compileFinished(deferClose);
   }
 
-  private void preparForCompile(boolean resetTaskIds) throws CommandProcessorException {
+  private void prepareForCompile(boolean resetTaskIds) throws CommandProcessorException {
     driverTxnHandler.createTxnManager();
     DriverState.setDriverState(driverState);
     prepareContext();
@@ -515,6 +516,7 @@ private void preparForCompile(boolean resetTaskIds) throws CommandProcessorExcep
   }
 
   private void prepareContext() throws CommandProcessorException {
+    String originalCboInfo = context != null ? context.cboInfo : null;
     if (context != null && context.getExplainAnalyze() != AnalyzeState.RUNNING) {
       // close the existing ctx etc before compiling a new query, but does not destroy driver
       closeInProcess(false);
@@ -522,7 +524,8 @@ private void prepareContext() throws CommandProcessorException {
 
     if (context == null) {
       context = new Context(driverContext.getConf());
-      }
+      context.setCboInfo(originalCboInfo);
+    }
 
     context.setHiveTxnManager(driverContext.getTxnManager());
     context.setStatsSource(driverContext.getStatsSource());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java
index 8817e42ed5..3cedff97f6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java
@@ -19,12 +19,14 @@
 package org.apache.hadoop.hive.ql;
 
 import java.util.ArrayList;
+import java.util.List;
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.reexec.IReExecutionPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecDriver;
 import org.apache.hadoop.hive.ql.reexec.ReExecuteLostAMQueryPlugin;
+import org.apache.hadoop.hive.ql.reexec.ReCompileWithoutCBOPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecutionOverlayPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecutionDagSubmitPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReOptimizePlugin;
@@ -52,7 +54,7 @@ public static IDriver newDriver(QueryState queryState, QueryInfo queryInfo) {
 
     String strategies = queryState.getConf().getVar(ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES);
     strategies = Strings.nullToEmpty(strategies).trim().toLowerCase();
-    ArrayList<IReExecutionPlugin> plugins = new ArrayList<>();
+    List<IReExecutionPlugin> plugins = new ArrayList<>();
     for (String string : strategies.split(",")) {
       if (string.trim().isEmpty()) {
         continue;
@@ -70,9 +72,12 @@ private static IReExecutionPlugin buildReExecPlugin(String name) throws RuntimeE
     if ("reoptimize".equals(name)) {
       return new ReOptimizePlugin();
     }
-    if("reexecute_lost_am".equals(name)) {
+    if ("reexecute_lost_am".equals(name)) {
       return new ReExecuteLostAMQueryPlugin();
     }
+    if ("recompile_without_cbo".equals(name)) {
+      return new ReCompileWithoutCBOPlugin();
+    }
     if (name.equals("dagsubmit")) {
       return new ReExecutionDagSubmitPlugin();
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Executor.java b/ql/src/java/org/apache/hadoop/hive/ql/Executor.java
index 6b5ee95d61..14b235d7ad 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Executor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Executor.java
@@ -97,11 +97,7 @@ public void execute() throws CommandProcessorException {
       LOG.info("Executing command(queryId=" + driverContext.getQueryId() + "): " + driverContext.getQueryString());
 
       // TODO: should this use getUserFromAuthenticator?
-      hookContext = new PrivateHookContext(driverContext.getPlan(), driverContext.getQueryState(),
-          context.getPathToCS(), SessionState.get().getUserName(), SessionState.get().getUserIpAddress(),
-          InetAddress.getLocalHost().getHostAddress(), driverContext.getOperationId(),
-          SessionState.get().getSessionId(), Thread.currentThread().getName(), SessionState.get().isHiveServerQuery(),
-          SessionState.getPerfLogger(), driverContext.getQueryInfo(), context);
+      hookContext = new PrivateHookContext(driverContext, context);
 
       preExecutionActions();
       preExecutionCacheActions();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/HookRunner.java b/ql/src/java/org/apache/hadoop/hive/ql/HookRunner.java
index 5c1b99642e..ad519e55a6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/HookRunner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/HookRunner.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.ql.hooks.HookContext;
 import org.apache.hadoop.hive.ql.hooks.HiveHooks;
 import org.apache.hadoop.hive.ql.hooks.MetricsQueryLifeTimeHook;
+import org.apache.hadoop.hive.ql.hooks.PrivateHookContext;
 import org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHook;
 import org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookContext;
 import org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookContextImpl;
@@ -121,19 +122,29 @@ void runBeforeCompileHook(String command) {
   }
 
   /**
-  * Dispatches {@link QueryLifeTimeHook#afterCompile(QueryLifeTimeHookContext, boolean)}.
-  *
-  * @param command the Hive command that is being run
-  * @param compileError true if there was an error while compiling the command, false otherwise
-  */
-  void runAfterCompilationHook(String command, boolean compileError) {
+   * Dispatches {@link QueryLifeTimeHook#afterCompile(QueryLifeTimeHookContext, boolean)}.
+   *
+   * @param driverContext the DriverContext used for generating the HookContext
+   * @param analyzerContext the SemanticAnalyzer context for this query
+   * @param compileException the exception if one was thrown during the compilation
+   * @throws Exception during {@link PrivateHookContext} creation
+   */
+  void runAfterCompilationHook(DriverContext driverContext, Context analyzerContext, Throwable compileException)
+      throws Exception {
     List<QueryLifeTimeHook> queryHooks = hooks.getHooks(QUERY_LIFETIME_HOOKS);
     if (!queryHooks.isEmpty()) {
+      HookContext hookContext = new PrivateHookContext(driverContext, analyzerContext);
+      hookContext.setException(compileException);
+
       QueryLifeTimeHookContext qhc =
-          new QueryLifeTimeHookContextImpl.Builder().withHiveConf(conf).withCommand(command).build();
+          new QueryLifeTimeHookContextImpl.Builder()
+              .withHiveConf(conf)
+              .withCommand(analyzerContext.getCmd())
+              .withHookContext(hookContext)
+              .build();
 
       for (QueryLifeTimeHook hook : queryHooks) {
-        hook.afterCompile(qhc, compileError);
+        hook.afterCompile(qhc, compileException != null);
       }
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java b/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java
index 528c3afb6f..3a86d24180 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java
@@ -20,6 +20,7 @@
 package org.apache.hadoop.hive.ql.hooks;
 
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -88,7 +89,6 @@ public HiveConf.ConfVars getConfVar() {
     public String getDescription() {
       return this.description;
     }
-    
   }
 
   private QueryPlan queryPlan;
@@ -125,8 +125,8 @@ public HookContext(QueryPlan queryPlan, QueryState queryState,
     this.conf = queryState.getConf();
     this.inputPathToContentSummary = inputPathToContentSummary;
     completeTaskList = new ArrayList<TaskRunner>();
-    inputs = queryPlan.getInputs();
-    outputs = queryPlan.getOutputs();
+    inputs = queryPlan == null ? Collections.emptySet() : queryPlan.getInputs();
+    outputs = queryPlan == null ? Collections.emptySet() : queryPlan.getOutputs();
     ugi = Utils.getUGI();
     linfo = queryState.getLineageState().getLineageInfo();
     depMap = queryState.getLineageState().getIndex();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/hooks/PrivateHookContext.java b/ql/src/java/org/apache/hadoop/hive/ql/hooks/PrivateHookContext.java
index 605436b5f5..43febc8add 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/hooks/PrivateHookContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/hooks/PrivateHookContext.java
@@ -18,16 +18,20 @@
 
 package org.apache.hadoop.hive.ql.hooks;
 
+import java.net.InetAddress;
+import java.net.UnknownHostException;
 import java.util.Map;
 
 import org.apache.hadoop.fs.ContentSummary;
 import org.apache.hadoop.hive.common.classification.InterfaceAudience;
 import org.apache.hadoop.hive.common.classification.InterfaceStability;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.DriverContext;
 import org.apache.hadoop.hive.ql.QueryInfo;
 import org.apache.hadoop.hive.ql.QueryPlan;
 import org.apache.hadoop.hive.ql.QueryState;
 import org.apache.hadoop.hive.ql.log.PerfLogger;
+import org.apache.hadoop.hive.ql.session.SessionState;
 
 @InterfaceAudience.Private
 @InterfaceStability.Unstable
@@ -44,6 +48,14 @@ public PrivateHookContext(QueryPlan queryPlan, QueryState queryState,
     this.ctx = ctx;
   }
 
+  public PrivateHookContext(DriverContext driverContext, Context context) throws Exception {
+    this(driverContext.getPlan(), driverContext.getQueryState(),
+        context.getPathToCS(), SessionState.get().getUserName(), SessionState.get().getUserIpAddress(),
+        InetAddress.getLocalHost().getHostAddress(), driverContext.getOperationId(),
+        SessionState.get().getSessionId(), Thread.currentThread().getName(), SessionState.get().isHiveServerQuery(),
+        SessionState.getPerfLogger(), driverContext.getQueryInfo(), context);
+  }
+
   public Context getContext() {
     return ctx;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/CBOFallbackStrategy.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/CBOFallbackStrategy.java
index 08a5f59cc0..a5f9d9bfff 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/CBOFallbackStrategy.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/CBOFallbackStrategy.java
@@ -25,7 +25,7 @@
 /**
  * A strategy defining when CBO fallbacks to the legacy optimizer.
  */
-enum CBOFallbackStrategy {
+public enum CBOFallbackStrategy {
   /**
    * Never use the legacy optimizer, all CBO errors are fatal.
    */
@@ -34,6 +34,11 @@ enum CBOFallbackStrategy {
     boolean isFatal(Exception e) {
       return true;
     }
+
+    @Override
+    public boolean allowsRetry() {
+      return false;
+    }
   },
   /**
    * Use the legacy optimizer only when the CBO exception is not related to subqueries and views.
@@ -45,6 +50,11 @@ boolean isFatal(Exception e) {
       return e instanceof CalciteSubquerySemanticException || e instanceof CalciteViewSemanticException
           || e instanceof CalciteSubqueryRuntimeException;
     }
+
+    @Override
+    public boolean allowsRetry() {
+      return true;
+    }
   },
   /**
    * Always use the legacy optimizer, CBO errors are not fatal.
@@ -54,6 +64,11 @@ boolean isFatal(Exception e) {
     boolean isFatal(Exception e) {
       return false;
     }
+
+    @Override
+    public boolean allowsRetry() {
+      return true;
+    }
   },
   /**
    * Specific strategy only for tests.
@@ -67,10 +82,17 @@ boolean isFatal(Exception e) {
       }
       return !(e instanceof CalciteSemanticException);
     }
+
+    @Override
+    public boolean allowsRetry() {
+      return true;
+    }
   };
 
   /**
    * Returns true if the specified exception is fatal (must not fallback to legacy optimizer), and false otherwise.
    */
   abstract boolean isFatal(Exception e);
+
+  public abstract boolean allowsRetry();
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
index 34055f0326..65391d318e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
@@ -302,6 +302,7 @@
 import org.apache.hadoop.hive.ql.plan.SelectDesc;
 import org.apache.hadoop.hive.ql.plan.mapper.EmptyStatsSource;
 import org.apache.hadoop.hive.ql.plan.mapper.StatsSource;
+import org.apache.hadoop.hive.ql.reexec.ReCompileException;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFArray;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
@@ -524,11 +525,10 @@ private static RelOptPlanner createPlanner(
   @Override
   @SuppressWarnings("rawtypes")
   Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticException {
-    Operator sinkOp = null;
-    boolean skipCalcitePlan = false;
+    final Operator sinkOp;
 
     if (!runCBO) {
-      skipCalcitePlan = true;
+      sinkOp = super.genOPTree(ast, plannerCtx);
     } else {
       PreCboCtx cboCtx = (PreCboCtx) plannerCtx;
       List<ASTNode> oldHints = new ArrayList<>();
@@ -555,7 +555,6 @@ Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticExcept
         profilesCBO = obtainCBOProfiles(queryProperties);
 
         disableJoinMerge = true;
-        boolean reAnalyzeAST = false;
         final boolean materializedView = getQB().isMaterializedView();
 
         try {
@@ -673,8 +672,7 @@ Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticExcept
           }
           this.ctx.setCboInfo(cboMsg);
 
-          // Determine if we should re-throw the exception OR if we try to mark plan as reAnalyzeAST to retry
-          // planning as non-CBO.
+          // Determine if we should re-throw the exception OR if we try to mark the query to retry as non-CBO.
           if (fallbackStrategy.isFatal(e)) {
             if (e instanceof RuntimeException || e instanceof SemanticException) {
               // These types of exceptions do not need wrapped
@@ -683,20 +681,12 @@ Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticExcept
             // Wrap all other errors (Should only hit in tests)
             throw new SemanticException(e);
           } else {
-            reAnalyzeAST = true;
+            throw new ReCompileException(this.ctx.getCboInfo());
           }
         } finally {
           runCBO = false;
           disableJoinMerge = defaultJoinMerge;
           disableSemJoinReordering = false;
-          if (reAnalyzeAST) {
-            init(true);
-            prunedPartitions.clear();
-            // Assumption: At this point Parse Tree gen & resolution will always
-            // be true (since we started out that way).
-            super.genResolvedParseTree(ast, new PlannerContext());
-            skipCalcitePlan = true;
-          }
         }
       } else {
         String msg;
@@ -706,14 +696,10 @@ Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticExcept
           msg = "Plan not optimized by CBO.";
         }
         this.ctx.setCboInfo(msg);
-        skipCalcitePlan = true;
+        sinkOp = super.genOPTree(ast, plannerCtx);
       }
     }
 
-    if (skipCalcitePlan) {
-      sinkOp = super.genOPTree(ast, plannerCtx);
-    }
-
     return sinkOp;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
index a144839fa9..a183051906 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
@@ -45,6 +45,7 @@
 import org.apache.hadoop.hive.ql.parse.ExplainConfiguration.VectorizationDetailLevel;
 import org.apache.hadoop.hive.ql.plan.ExplainWork;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
+import org.apache.hadoop.hive.ql.reexec.ReCompileException;
 import org.apache.hadoop.hive.ql.stats.StatsAggregator;
 import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator;
@@ -156,7 +157,11 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
           while (driver.getResults(new ArrayList<String>())) {
           }
         } catch (CommandProcessorException e) {
-          throw new SemanticException(e.getMessage(), e);
+          if (e.getCause() instanceof ReCompileException) {
+            throw (ReCompileException) e.getCause();
+          } else {
+            throw new SemanticException(e.getMessage(), e);
+          }
         }
         config.setOpIdToRuntimeNumRows(aggregateStats(config.getExplainRootPath()));
       } catch (IOException e1) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/IReExecutionPlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/IReExecutionPlugin.java
index be62fc0075..484078820d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/IReExecutionPlugin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/IReExecutionPlugin.java
@@ -42,24 +42,72 @@ public interface IReExecutionPlugin {
   /**
    * Called before executing the query.
    */
-  void beforeExecute(int executionIndex, boolean explainReOptimization);
+  default void beforeExecute(int executionIndex, boolean explainReOptimization) {
+    // default noop
+  }
 
   /**
    * The query have failed, does this plugin advises to re-execute it again?
    */
-  boolean shouldReExecute(int executionNum);
+  default boolean shouldReExecute(int executionNum) {
+    // default no
+    return false;
+  }
 
   /**
-   * The plugin should prepare for the re-compilaton of the query.
+   * The plugin should prepare for the re-compilation of the query.
    */
-  void prepareToReExecute();
+  default void prepareToReExecute() {
+    // default noop
+  }
 
   /**
-   * The query have failed; and have been recompiled - does this plugin advises to re-execute it again?
+   * The query has failed; and have been recompiled - does this plugin advises to re-execute it again?
    */
-  boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper);
+  default boolean shouldReExecuteAfterCompile(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {
+    // default no
+    return false;
+  }
 
-  void afterExecute(PlanMapper planMapper, boolean successfull);
+  /**
+   * Called after the driver executed the query - delivers the status and the plan data.
+   * @param planMapper
+   * @param successful
+   */
+  default void afterExecute(PlanMapper planMapper, boolean successful) {
+    // default noop
+  }
+
+  /**
+   * Called before the compilation happen.
+   * @param compilationNum Number of the previous compilations
+   */
+  default void beforeCompile(int compilationNum) {
+    // default noop
+  }
+
+  /**
+   * The query has failed. Should we try recompilation?
+   * @param compilationNum Number of the previous compilations
+   * @return
+   */
+  default boolean shouldReCompile(int compilationNum) {
+    // default no
+    return false;
+  }
 
+  /**
+   * The plugin should prepare for the recompilation of the query
+   */
+  default void prepareToReCompile() {
+    // default noop
+  }
 
+  /**
+   * Called after the compilation - delivers the result of the new compilation.
+   * @param successful
+   */
+  default void afterCompile(boolean successful) {
+    // default noop
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReCompileException.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReCompileException.java
new file mode 100644
index 0000000000..6164f81be5
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReCompileException.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.reexec;
+
+/**
+ * This exception is thrown when CBO failed and we would like to try to recompile the query with CBO off.
+ * For more details see: {@link ReExecDriver}, {@link ReCompileWithoutCBOPlugin}
+ */
+public class ReCompileException extends RuntimeException {
+  private final String cboMessage;
+
+  public ReCompileException(String cboMessage) {
+    super();
+    this.cboMessage = cboMessage;
+  }
+
+  public String getCboMessage() {
+    return cboMessage;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReCompileWithoutCBOPlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReCompileWithoutCBOPlugin.java
new file mode 100644
index 0000000000..0effb71859
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReCompileWithoutCBOPlugin.java
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.reexec;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHook;
+import org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookContext;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Re-compiles the query without CBO
+ */
+public class ReCompileWithoutCBOPlugin implements IReExecutionPlugin {
+  private static final Logger LOG = LoggerFactory.getLogger(ReCompileWithoutCBOPlugin.class);
+
+
+  private Driver driver;
+  private boolean retryPossible;
+  private String cboMsg;
+
+  class LocalHook implements QueryLifeTimeHook {
+    @Override
+    public void beforeCompile(QueryLifeTimeHookContext ctx) {
+      // noop
+    }
+
+    @Override
+    public void afterCompile(QueryLifeTimeHookContext ctx, boolean hasError) {
+      if (hasError) {
+        Throwable throwable = ctx.getHookContext().getException();
+        retryPossible = throwable != null && throwable instanceof ReCompileException;
+        cboMsg = retryPossible ? ((ReCompileException) throwable).getCboMessage() : null;
+        LOG.debug("Recompile check result {} with CBO message {}", retryPossible, cboMsg);
+      } else {
+        retryPossible = false;
+      }
+    }
+
+    @Override
+    public void beforeExecution(QueryLifeTimeHookContext ctx) {
+      // noop
+    }
+
+    @Override
+    public void afterExecution(QueryLifeTimeHookContext ctx, boolean hasError) {
+      // noop
+    }
+  }
+
+  @Override
+  public void initialize(Driver driver) {
+    this.driver = driver;
+    driver.getHookRunner().addLifeTimeHook(new ReCompileWithoutCBOPlugin.LocalHook());
+  }
+
+  @Override
+  public void prepareToReCompile() {
+    HiveConf conf = driver.getConf();
+    conf.setBoolVar(HiveConf.ConfVars.HIVE_CBO_ENABLED, false);
+    driver.getContext().setCboInfo(cboMsg);
+  }
+
+  @Override
+  public boolean shouldReCompile(int executionNum) {
+    return retryPossible && executionNum == 1;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java
index c307085366..3061d0a762 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.reexec;
 
 import java.io.IOException;
-import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 
 import org.antlr.runtime.tree.Tree;
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.ql.exec.FetchTask;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.CBOFallbackStrategy;
 import org.apache.hadoop.hive.ql.parse.HiveParser;
 import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHook;
 import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
@@ -45,6 +46,7 @@
 import org.apache.hadoop.hive.ql.plan.mapper.StatsSource;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
+import org.apache.hadoop.hive.ql.session.SessionState;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -56,73 +58,92 @@
  * Covers the IDriver interface, handles query re-execution; and asks clear questions from the underlying re-execution plugins.
  */
 public class ReExecDriver implements IDriver {
+  private static final Logger LOG = LoggerFactory.getLogger(ReExecDriver.class);
+  private static final SessionState.LogHelper CONSOLE = new SessionState.LogHelper(LOG);
 
-  private class HandleReOptimizationExplain implements HiveSemanticAnalyzerHook {
-
-    @Override
-    public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast) throws SemanticException {
-      if (ast.getType() == HiveParser.TOK_EXPLAIN) {
-        int childCount = ast.getChildCount();
-        for (int i = 1; i < childCount; i++) {
-          if (ast.getChild(i).getType() == HiveParser.KW_REOPTIMIZATION) {
-            explainReOptimization = true;
-            ast.deleteChild(i);
-            break;
-          }
-        }
-        if (explainReOptimization && firstExecution()) {
-          Tree execTree = ast.getChild(0);
-          execTree.setParent(ast.getParent());
-          ast.getParent().setChild(0, execTree);
-          return (ASTNode) execTree;
-        }
-      }
-      return ast;
-    }
-
-    @Override
-    public void postAnalyze(HiveSemanticAnalyzerHookContext context, List<Task<?>> rootTasks)
-        throws SemanticException {
-    }
-  }
+  private final Driver coreDriver;
+  private final QueryState queryState;
+  private final List<IReExecutionPlugin> plugins;
 
-  private static final Logger LOG = LoggerFactory.getLogger(ReExecDriver.class);
   private boolean explainReOptimization;
-  private Driver coreDriver;
-  private QueryState queryState;
   private String currentQuery;
   private int executionIndex;
 
-  private ArrayList<IReExecutionPlugin> plugins;
+  public ReExecDriver(QueryState queryState, QueryInfo queryInfo, List<IReExecutionPlugin> plugins) {
+    this.queryState = queryState;
+    this.coreDriver = new Driver(queryState, queryInfo, null);
+    this.plugins = plugins;
 
-  @Override
-  public HiveConf getConf() {
-    return queryState.getConf();
+    coreDriver.getHookRunner().addSemanticAnalyzerHook(new HandleReOptimizationExplain());
+    plugins.forEach(p -> p.initialize(coreDriver));
+  }
+
+  @VisibleForTesting
+  public int compile(String command, boolean resetTaskIds) {
+    return coreDriver.compile(command, resetTaskIds);
   }
 
   private boolean firstExecution() {
     return executionIndex == 0;
   }
 
-  public ReExecDriver(QueryState queryState, QueryInfo queryInfo, ArrayList<IReExecutionPlugin> plugins) {
-    this.queryState = queryState;
-    coreDriver = new Driver(queryState, queryInfo, null);
-    coreDriver.getHookRunner().addSemanticAnalyzerHook(new HandleReOptimizationExplain());
-    this.plugins = plugins;
-
-    for (IReExecutionPlugin p : plugins) {
-      p.initialize(coreDriver);
+  private void checkHookConfig() throws CommandProcessorException {
+    String strategies = coreDriver.getConf().getVar(ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES);
+    CBOFallbackStrategy fallbackStrategy =
+        CBOFallbackStrategy.valueOf(coreDriver.getConf().getVar(ConfVars.HIVE_CBO_FALLBACK_STRATEGY));
+    if (fallbackStrategy.allowsRetry() &&
+        (strategies == null || !Arrays.stream(strategies.split(",")).anyMatch("recompile_without_cbo"::equals))) {
+      String errorMsg = "Invalid configuration. If fallbackStrategy is set to " + fallbackStrategy.name() + " then " +
+          ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES.varname + " should contain 'recompile_without_cbo'";
+      CONSOLE.printError(errorMsg);
+      throw new CommandProcessorException(errorMsg);
     }
   }
 
-  public int compile(String command, boolean resetTaskIds) {
-    return coreDriver.compile(command, resetTaskIds);
-  }
-
   @Override
   public CommandProcessorResponse compileAndRespond(String statement) throws CommandProcessorException {
     currentQuery = statement;
-    return coreDriver.compileAndRespond(statement);
+
+    checkHookConfig();
+
+    int compileIndex = 0;
+    int maxCompilations = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_RECOMPILATION_COUNT);
+    while (true) {
+      compileIndex++;
+
+      final int currentIndex = compileIndex;
+      plugins.forEach(p -> p.beforeCompile(currentIndex));
+
+      LOG.info("Compile #{} of query", compileIndex);
+      CommandProcessorResponse cpr = null;
+      CommandProcessorException cpe = null;
+      try {
+        cpr = coreDriver.compileAndRespond(statement);
+      } catch (CommandProcessorException e) {
+        cpe = e;
+      }
+
+      final boolean success = cpe == null;
+      plugins.forEach(p -> p.afterCompile(success));
+
+      // If the compilation was successful return the result
+      if (success) {
+        return cpr;
+      }
+
+      if (compileIndex >= maxCompilations || !plugins.stream().anyMatch(p -> p.shouldReCompile(currentIndex))) {
+        // If we do not have to recompile, return the last error
+        throw cpe;
+      }
+
+      // Prepare for the recompile and start the next loop
+      plugins.forEach(IReExecutionPlugin::prepareToReCompile);
+    }
+  }
+
+  @Override
+  public HiveConf getConf() {
+    return queryState.getConf();
   }
 
   @Override
@@ -148,11 +169,11 @@ public void setOperationId(String operationId) {
   @Override
   public CommandProcessorResponse run() throws CommandProcessorException {
     executionIndex = 0;
-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);
-
+    int maxExecutions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);
 
     while (true) {
       executionIndex++;
+
       for (IReExecutionPlugin p : plugins) {
         p.beforeExecute(executionIndex, explainReOptimization);
       }
@@ -167,12 +188,13 @@ public CommandProcessorResponse run() throws CommandProcessorException {
       }
 
       PlanMapper oldPlanMapper = coreDriver.getPlanMapper();
-      afterExecute(oldPlanMapper, cpr != null);
+      boolean success = cpr != null;
+      plugins.forEach(p -> p.afterExecute(oldPlanMapper, success));
 
       boolean shouldReExecute = explainReOptimization && executionIndex==1;
-      shouldReExecute |= cpr == null && shouldReExecute();
+      shouldReExecute |= cpr == null && plugins.stream().anyMatch(p -> p.shouldReExecute(executionIndex));
 
-      if (executionIndex >= maxExecutuions || !shouldReExecute) {
+      if (executionIndex >= maxExecutions || !shouldReExecute) {
         if (cpr != null) {
           return cpr;
         } else {
@@ -180,7 +202,8 @@ public CommandProcessorResponse run() throws CommandProcessorException {
         }
       }
       LOG.info("Preparing to re-execute query");
-      prepareToReExecute();
+      plugins.forEach(IReExecutionPlugin::prepareToReExecute);
+
       try {
         coreDriver.compileAndRespond(currentQuery);
       } catch (CommandProcessorException e) {
@@ -190,7 +213,8 @@ public CommandProcessorResponse run() throws CommandProcessorException {
       }
 
       PlanMapper newPlanMapper = coreDriver.getPlanMapper();
-      if (!explainReOptimization && !shouldReExecuteAfterCompile(oldPlanMapper, newPlanMapper)) {
+      if (!explainReOptimization &&
+          !plugins.stream().anyMatch(p -> p.shouldReExecuteAfterCompile(executionIndex, oldPlanMapper, newPlanMapper))) {
         LOG.info("re-running the query would probably not yield better results; returning with last error");
         // FIXME: retain old error; or create a new one?
         return cpr;
@@ -198,44 +222,12 @@ public CommandProcessorResponse run() throws CommandProcessorException {
     }
   }
 
-  private void afterExecute(PlanMapper planMapper, boolean success) {
-    for (IReExecutionPlugin p : plugins) {
-      p.afterExecute(planMapper, success);
-    }
-  }
-
-  private boolean shouldReExecuteAfterCompile(PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {
-    boolean ret = false;
-    for (IReExecutionPlugin p : plugins) {
-      boolean shouldReExecute = p.shouldReExecute(executionIndex, oldPlanMapper, newPlanMapper);
-      LOG.debug("{}.shouldReExecuteAfterCompile = {}", p, shouldReExecute);
-      ret |= shouldReExecute;
-    }
-    return ret;
-  }
-
-  private boolean shouldReExecute() {
-    boolean ret = false;
-    for (IReExecutionPlugin p : plugins) {
-      boolean shouldReExecute = p.shouldReExecute(executionIndex);
-      LOG.debug("{}.shouldReExecute = {}", p, shouldReExecute);
-      ret |= shouldReExecute;
-    }
-    return ret;
-  }
-
   @Override
   public CommandProcessorResponse run(String command) throws CommandProcessorException {
     compileAndRespond(command);
     return run();
   }
 
-  private void prepareToReExecute() {
-    for (IReExecutionPlugin p : plugins) {
-      p.prepareToReExecute();
-    }
-  }
-
   @Override
   public boolean getResults(List res) throws IOException {
     return coreDriver.getResults(res);
@@ -294,4 +286,32 @@ public boolean hasResultSet() {
     return explainReOptimization || coreDriver.hasResultSet();
   }
 
+  private class HandleReOptimizationExplain implements HiveSemanticAnalyzerHook {
+
+    @Override
+    public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast) throws SemanticException {
+      if (ast.getType() == HiveParser.TOK_EXPLAIN) {
+        int childCount = ast.getChildCount();
+        for (int i = 1; i < childCount; i++) {
+          if (ast.getChild(i).getType() == HiveParser.KW_REOPTIMIZATION) {
+            explainReOptimization = true;
+            ast.deleteChild(i);
+            break;
+          }
+        }
+        if (explainReOptimization && firstExecution()) {
+          Tree execTree = ast.getChild(0);
+          execTree.setParent(ast.getParent());
+          ast.getParent().setChild(0, execTree);
+          return (ASTNode) execTree;
+        }
+      }
+      return ast;
+    }
+
+    @Override
+    public void postAnalyze(HiveSemanticAnalyzerHookContext context, List<Task<?>> rootTasks)
+        throws SemanticException {
+    }
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecuteLostAMQueryPlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecuteLostAMQueryPlugin.java
index e779ad2ef9..ce400159be 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecuteLostAMQueryPlugin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecuteLostAMQueryPlugin.java
@@ -63,25 +63,13 @@ public void initialize(Driver driver) {
     driver.getHookRunner().addOnFailureHook(new LocalHook());
   }
 
-  @Override
-  public void beforeExecute(int executionIndex, boolean explainReOptimization) {
-  }
-
   @Override
   public boolean shouldReExecute(int executionNum) {
     return retryPossible;
   }
 
   @Override
-  public void prepareToReExecute() {
-  }
-
-  @Override
-  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {
+  public boolean shouldReExecuteAfterCompile(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {
     return retryPossible;
   }
-
-  @Override
-  public void afterExecute(PlanMapper planMapper, boolean successfull) {
-  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionDagSubmitPlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionDagSubmitPlugin.java
index b019a006a0..99c7a046ff 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionDagSubmitPlugin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionDagSubmitPlugin.java
@@ -60,25 +60,13 @@ public void initialize(Driver driver) {
   private boolean retryPossible;
 
   @Override
-  public void prepareToReExecute() {
-  }
-
-  @Override
-  public boolean shouldReExecute(int executionNum, PlanMapper pm1, PlanMapper pm2) {
+  public boolean shouldReExecuteAfterCompile(int executionNum, PlanMapper pm1, PlanMapper pm2) {
     return retryPossible;
   }
 
-  @Override
-  public void beforeExecute(int executionIndex, boolean explainReOptimization) {
-  }
-
   @Override
   public boolean shouldReExecute(int executionNum) {
     return retryPossible;
   }
 
-  @Override
-  public void afterExecute(PlanMapper planMapper, boolean success) {
-  }
-
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java
index 83df334931..ff794cd099 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java
@@ -79,16 +79,7 @@ public boolean shouldReExecute(int executionNum) {
   }
 
   @Override
-  public boolean shouldReExecute(int executionNum, PlanMapper pm1, PlanMapper pm2) {
+  public boolean shouldReExecuteAfterCompile(int executionNum, PlanMapper pm1, PlanMapper pm2) {
     return executionNum == 1;
   }
-
-  @Override
-  public void beforeExecute(int executionIndex, boolean explainReOptimization) {
-  }
-
-  @Override
-  public void afterExecute(PlanMapper planMapper, boolean success) {
-  }
-
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReOptimizePlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReOptimizePlugin.java
index 09045af4bc..fd9b47ccba 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReOptimizePlugin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReOptimizePlugin.java
@@ -101,7 +101,7 @@ public void prepareToReExecute() {
   }
 
   @Override
-  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {
+  public boolean shouldReExecuteAfterCompile(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {
     boolean planDidChange = !planEquals(oldPlanMapper, newPlanMapper);
     LOG.info("planDidChange: {}", planDidChange);
     return planDidChange;
diff --git a/ql/src/test/queries/clientnegative/cbo_fallback_wrong_configuration_exception.q b/ql/src/test/queries/clientnegative/cbo_fallback_wrong_configuration_exception.q
new file mode 100644
index 0000000000..d25da067ff
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/cbo_fallback_wrong_configuration_exception.q
@@ -0,0 +1,6 @@
+--! qt:dataset:part
+-- =ALL is not allowed and initially triggers a CalciteSubquerySemanticException
+set hive.cbo.fallback.strategy=ALWAYS;
+set hive.query.reexecution.strategies=overlay,reoptimize,reexecute_lost_am,dagsubmit;
+-- In ALWAYS mode CalciteSubquerySemanticException should be retried but the wrong configuration should prevent it
+explain select * from part where p_type = ALL(select max(p_type) from part);
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/retry_failure.q b/ql/src/test/queries/clientpositive/retry_failure.q
index b1bc789be1..095be12cb6 100644
--- a/ql/src/test/queries/clientpositive/retry_failure.q
+++ b/ql/src/test/queries/clientpositive/retry_failure.q
@@ -8,7 +8,7 @@ set zzz=1;
 set reexec.overlay.zzz=2;
 
 set hive.query.reexecution.enabled=true;
-set hive.query.reexecution.strategies=overlay;
+set hive.query.reexecution.strategies=overlay,recompile_without_cbo;
 set hive.fetch.task.conversion=none;
 set tez.queue.name=default;
 
diff --git a/ql/src/test/queries/clientpositive/retry_failure_oom.q b/ql/src/test/queries/clientpositive/retry_failure_oom.q
index 663077b739..84ad7e109e 100644
--- a/ql/src/test/queries/clientpositive/retry_failure_oom.q
+++ b/ql/src/test/queries/clientpositive/retry_failure_oom.q
@@ -7,7 +7,7 @@ set zzz=1;
 set reexec.overlay.zzz=2;
 
 set hive.query.reexecution.enabled=true;
-set hive.query.reexecution.strategies=overlay,reoptimize;
+set hive.query.reexecution.strategies=overlay,reoptimize,recompile_without_cbo;
 
 select assert_true_oom(${hiveconf:zzz} > a) from tx group by a;
 
diff --git a/ql/src/test/queries/clientpositive/retry_failure_reorder.q b/ql/src/test/queries/clientpositive/retry_failure_reorder.q
index 9bb1364a79..5978b811b7 100644
--- a/ql/src/test/queries/clientpositive/retry_failure_reorder.q
+++ b/ql/src/test/queries/clientpositive/retry_failure_reorder.q
@@ -41,7 +41,7 @@ select (${hiveconf:zzz} > sum(u*v*w)) from tu
         where w>9 and u>1 and v>3;
 
 
-set hive.query.reexecution.strategies=overlay,reoptimize;
+set hive.query.reexecution.strategies=overlay,reoptimize,recompile_without_cbo;
 set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter;
 
 explain reoptimization
diff --git a/ql/src/test/queries/clientpositive/retry_failure_stat_changes.q b/ql/src/test/queries/clientpositive/retry_failure_stat_changes.q
index 78825385ca..58b452d1b8 100644
--- a/ql/src/test/queries/clientpositive/retry_failure_stat_changes.q
+++ b/ql/src/test/queries/clientpositive/retry_failure_stat_changes.q
@@ -10,7 +10,7 @@ insert into px values (2,2),(3,3),(5,5),(7,7),(11,11);
 
 set hive.explain.user=true;
 set hive.query.reexecution.enabled=true;
-set hive.query.reexecution.strategies=overlay,reoptimize;
+set hive.query.reexecution.strategies=overlay,reoptimize,recompile_without_cbo;
 
 explain REOPTIMIZATION 
 select sum(u*p) from tx_n2 join px on (u=p) where u<10 and p>2;
diff --git a/ql/src/test/queries/clientpositive/runtime_stats_hs2.q b/ql/src/test/queries/clientpositive/runtime_stats_hs2.q
index 1a02eac474..33c23b035f 100644
--- a/ql/src/test/queries/clientpositive/runtime_stats_hs2.q
+++ b/ql/src/test/queries/clientpositive/runtime_stats_hs2.q
@@ -8,7 +8,7 @@ insert into px_n0 values (2,2),(3,3),(5,5),(7,7),(11,11);
 set hive.explain.user=true;
 set hive.query.reexecution.enabled=true;
 set hive.query.reexecution.always.collect.operator.stats=true;
-set hive.query.reexecution.strategies=overlay,reoptimize;
+set hive.query.reexecution.strategies=overlay,reoptimize,recompile_without_cbo;
 set hive.query.reexecution.stats.persist.scope=hiveserver;
 
 -- join output estimate is underestimated: 1 row
diff --git a/ql/src/test/queries/clientpositive/vector_retry_failure.q b/ql/src/test/queries/clientpositive/vector_retry_failure.q
index a5e44e7ff0..601d5d0688 100644
--- a/ql/src/test/queries/clientpositive/vector_retry_failure.q
+++ b/ql/src/test/queries/clientpositive/vector_retry_failure.q
@@ -8,7 +8,7 @@ set zzz=1;
 set reexec.overlay.zzz=2;
 
 set hive.query.reexecution.enabled=true;
-set hive.query.reexecution.strategies=overlay;
+set hive.query.reexecution.strategies=overlay,recompile_without_cbo;
 
 explain vectorization expression
 select assert_true(${hiveconf:zzz} > a) from tx_n0 group by a;
diff --git a/ql/src/test/results/clientnegative/cbo_fallback_wrong_configuration_exception.q.out b/ql/src/test/results/clientnegative/cbo_fallback_wrong_configuration_exception.q.out
new file mode 100644
index 0000000000..bd04c57257
--- /dev/null
+++ b/ql/src/test/results/clientnegative/cbo_fallback_wrong_configuration_exception.q.out
@@ -0,0 +1 @@
+Invalid configuration. If fallbackStrategy is set to ALWAYS then hive.query.reexecution.strategies should contain 'recompile_without_cbo'
