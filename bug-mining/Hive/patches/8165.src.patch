diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
index 1375ee8b7c..e98df6b44d 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
@@ -20,6 +20,7 @@
 package org.apache.iceberg.mr.hive.vector;
 
 import java.io.IOException;
+import java.nio.ByteBuffer;
 import java.util.List;
 import java.util.Map;
 import org.apache.commons.lang3.ArrayUtils;
@@ -49,6 +50,7 @@
 import org.apache.iceberg.orc.VectorizedReadUtils;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.types.Types;
+import org.apache.orc.impl.OrcTail;
 
 /**
  * Utility class to create vectorized readers for Hive.
@@ -120,20 +122,29 @@ public static <D> CloseableIterable<D> reader(InputFile inputFile, FileScanTask
           // we need to set Long.MIN_VALUE as last modification time in the fileId triplet.
           SyntheticFileId fileId = new SyntheticFileId(path, task.file().fileSizeInBytes(), Long.MIN_VALUE);
 
-          VectorizedReadUtils.handleIcebergProjection(inputFile, task, job, fileId);
+          // Metadata information has to be passed along in the OrcSplit. Without specifying this, the vectorized
+          // reader will assume that the ORC file ends at the task's start + length, and might fail reading the tail..
+          ByteBuffer serializedOrcTail = VectorizedReadUtils.getSerializedOrcTail(inputFile, fileId, job);
+          OrcTail orcTail = VectorizedReadUtils.deserializeToOrcTail(serializedOrcTail);
+
+          VectorizedReadUtils.handleIcebergProjection(task, job,
+              VectorizedReadUtils.deserializeToShadedOrcTail(serializedOrcTail).getSchema());
 
           RecordReader<NullWritable, VectorizedRowBatch> recordReader = null;
 
+          long start = task.start();
+          long length = task.length();
+
           // If LLAP enabled, try to retrieve an LLAP record reader - this might yield to null in some special cases
           if (HiveConf.getBoolVar(job, HiveConf.ConfVars.LLAP_IO_ENABLED, LlapProxy.isDaemon()) &&
               LlapProxy.getIo() != null) {
             recordReader = LlapProxy.getIo().llapVectorizedOrcReaderForPath(fileId, path, null, readColumnIds,
-                job, task.start(), task.length(), reporter);
+                job, start, length, reporter);
           }
 
           if (recordReader == null) {
-            InputSplit split = new OrcSplit(path, fileId, task.start(), task.length(), (String[]) null, null, false,
-                 false, com.google.common.collect.Lists.newArrayList(), 0, task.length(), path.getParent(), null);
+            InputSplit split = new OrcSplit(path, fileId, start, length, (String[]) null, orcTail,
+                false, false, com.google.common.collect.Lists.newArrayList(), 0, length, path.getParent(), null);
             recordReader = new VectorizedOrcInputFormat().getRecordReader(split, job, reporter);
           }
           return createVectorizedRowBatchIterable(recordReader, job, partitionColIndices, partitionValues);
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/VectorizedReadUtils.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/VectorizedReadUtils.java
index 30f66b4cdd..287dd04840 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/VectorizedReadUtils.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/VectorizedReadUtils.java
@@ -20,6 +20,7 @@
 package org.apache.iceberg.orc;
 
 import java.io.IOException;
+import java.nio.ByteBuffer;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.io.CacheTag;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -32,8 +33,8 @@
 import org.apache.hadoop.hive.ql.plan.TableScanDesc;
 import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
 import org.apache.hadoop.mapred.JobConf;
-import org.apache.hive.iceberg.org.apache.orc.Reader;
 import org.apache.hive.iceberg.org.apache.orc.TypeDescription;
+import org.apache.hive.iceberg.org.apache.orc.impl.OrcTail;
 import org.apache.hive.iceberg.org.apache.orc.impl.ReaderImpl;
 import org.apache.iceberg.FileScanTask;
 import org.apache.iceberg.Schema;
@@ -46,7 +47,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * Utilities that rely on Iceberg code from org.apache.iceberg.orc package.
+ * Utilities that rely on Iceberg code from org.apache.iceberg.orc package and are required for ORC vectorization.
  */
 public class VectorizedReadUtils {
 
@@ -56,9 +57,17 @@ private VectorizedReadUtils() {
 
   }
 
-  private static TypeDescription getSchemaForFile(InputFile inputFile, SyntheticFileId fileId, JobConf job)
+  /**
+   * Opens the ORC inputFile and reads the metadata information to construct a byte buffer with OrcTail content.
+   * @param inputFile - the original ORC file - this needs to be accessed to retrieve the original schema for mapping
+   * @param job - JobConf instance to adjust
+   * @param fileId - FileID for the input file, serves as cache key in an LLAP setup
+   * @throws IOException - errors relating to accessing the ORC file
+   */
+  public static ByteBuffer getSerializedOrcTail(InputFile inputFile, SyntheticFileId fileId, JobConf job)
       throws IOException {
-    TypeDescription schema = null;
+
+    ByteBuffer result = null;
 
     if (HiveConf.getBoolVar(job, HiveConf.ConfVars.LLAP_IO_ENABLED, LlapProxy.isDaemon()) &&
         LlapProxy.getIo() != null) {
@@ -76,7 +85,7 @@ private static TypeDescription getSchemaForFile(InputFile inputFile, SyntheticFi
         // Iceberg expects org.apache.hive.iceberg.org.apache.orc.TypeDescription as it shades ORC, while LLAP provides
         // the unshaded org.apache.orc.TypeDescription type.
         BufferChunk tailBuffer = LlapProxy.getIo().getOrcTailFromCache(path, job, cacheTag, fileId).getTailBuffer();
-        schema = ReaderImpl.extractFileTail(tailBuffer.getData()).getSchema();
+        result = tailBuffer.getData();
       } catch (IOException ioe) {
         LOG.warn("LLAP is turned on but was unable to get file metadata information through its cache for {}",
             path, ioe);
@@ -85,32 +94,47 @@ private static TypeDescription getSchemaForFile(InputFile inputFile, SyntheticFi
     }
 
     // Fallback to simple ORC reader file opening method in lack of or failure of LLAP.
-    if (schema == null) {
-      try (Reader orcFileReader = ORC.newFileReader(inputFile, job)) {
-        schema = orcFileReader.getSchema();
+    if (result == null) {
+      try (ReaderImpl orcFileReader = (ReaderImpl) ORC.newFileReader(inputFile, job)) {
+        result = orcFileReader.getSerializedFileFooter();
       }
     }
 
-    return schema;
+    return result;
+  }
 
+  /**
+   * Returns an unshaded version of the OrcTail of the supplied input file. Used by Hive classes.
+   * @param serializedTail - ByteBuffer containing the tail bytes
+   * @throws IOException - errors relating to deserialization
+   */
+  public static org.apache.orc.impl.OrcTail deserializeToOrcTail(ByteBuffer serializedTail) throws IOException {
+    return org.apache.orc.impl.ReaderImpl.extractFileTail(serializedTail);
+  }
+
+  /**
+   * Returns an Iceberg-shaded version of the OrcTail of the supplied input file. Used by Iceberg classes.
+   * @param serializedTail - ByteBuffer containing the tail bytes
+   * @throws IOException - errors relating to deserialization
+   */
+  public static OrcTail deserializeToShadedOrcTail(ByteBuffer serializedTail) throws IOException {
+    return ReaderImpl.extractFileTail(serializedTail);
   }
 
   /**
    * Adjusts the jobConf so that column reorders and renames that might have happened since this ORC file was written
    * are properly mapped to the schema of the original file.
-   * @param inputFile - the original ORC file - this needs to be accessed to retrieve the original schema for mapping
    * @param task - Iceberg task - required for
    * @param job - JobConf instance to adjust
-   * @param fileId - FileID for the input file, serves as cache key in an LLAP setup
+   * @param fileSchema - ORC file schema of the input file
    * @throws IOException - errors relating to accessing the ORC file
    */
-  public static void handleIcebergProjection(InputFile inputFile, FileScanTask task, JobConf job,
-      SyntheticFileId fileId) throws IOException {
+  public static void handleIcebergProjection(FileScanTask task, JobConf job, TypeDescription fileSchema)
+      throws IOException {
 
     // We need to map with the current (i.e. current Hive table columns) full schema (without projections),
     // as OrcInputFormat will take care of the projections by the use of an include boolean array
     Schema currentSchema = task.spec().schema();
-    TypeDescription fileSchema = getSchemaForFile(inputFile, fileId, job);
 
     TypeDescription readOrcSchema;
     if (ORCSchemaUtil.hasIds(fileSchema)) {
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java
index 299135ad9c..c131304180 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java
@@ -22,6 +22,7 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.AppendFiles;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DataFiles;
@@ -39,14 +40,18 @@
  */
 public class GenericAppenderHelper {
 
+  private static final String ORC_CONFIG_PREFIX = "^orc.*";
+
   private final Table table;
   private final FileFormat fileFormat;
   private final TemporaryFolder tmp;
+  private final Configuration conf;
 
-  public GenericAppenderHelper(Table table, FileFormat fileFormat, TemporaryFolder tmp) {
+  public GenericAppenderHelper(Table table, FileFormat fileFormat, TemporaryFolder tmp, Configuration conf) {
     this.table = table;
     this.fileFormat = fileFormat;
     this.tmp = tmp;
+    this.conf = conf;
   }
 
   public void appendToTable(DataFile... dataFiles) {
@@ -73,13 +78,20 @@ public DataFile writeFile(StructLike partition, List<Record> records) throws IOE
     Preconditions.checkNotNull(table, "table not set");
     File file = tmp.newFile();
     Assert.assertTrue(file.delete());
-    return appendToLocalFile(table, file, fileFormat, partition, records);
+    return appendToLocalFile(table, file, fileFormat, partition, records, conf);
   }
 
-  private static DataFile appendToLocalFile(
-      Table table, File file, FileFormat format, StructLike partition, List<Record> records)
+  private static DataFile appendToLocalFile(Table table, File file, FileFormat format, StructLike partition,
+      List<Record> records, Configuration conf)
       throws IOException {
-    FileAppender<Record> appender = new GenericAppenderFactory(table.schema()).newAppender(
+    GenericAppenderFactory appenderFactory = new GenericAppenderFactory(table.schema());
+
+    // Push down ORC related settings to appender
+    if (FileFormat.ORC.equals(format)) {
+      appenderFactory.setAll(conf.getValByRegex(ORC_CONFIG_PREFIX));
+    }
+
+    FileAppender<Record> appender = appenderFactory.newAppender(
         Files.localOutput(file), format);
     try (FileAppender<Record> fileAppender = appender) {
       fileAppender.addAll(records);
@@ -92,6 +104,7 @@ private static DataFile appendToLocalFile(
         .withMetrics(appender.metrics())
         .withFormat(format)
         .withPartition(partition)
+        .withSplitOffsets(appender.splitOffsets())
         .build();
   }
 }
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/TestHelper.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/TestHelper.java
index ef18f4d362..c7170420bb 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/TestHelper.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/TestHelper.java
@@ -125,7 +125,7 @@ public DataFile writeFile(StructLike partition, List<Record> records) throws IOE
   }
 
   private GenericAppenderHelper appender() {
-    return new GenericAppenderHelper(table, fileFormat, tmp);
+    return new GenericAppenderHelper(table, fileFormat, tmp, conf);
   }
 
   public static class RecordsBuilder {
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergSelects.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergSelects.java
index 96e5dc2f5f..3b58f1f4ee 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergSelects.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergSelects.java
@@ -26,6 +26,7 @@
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.apache.iceberg.data.Record;
+import org.apache.iceberg.mr.InputFormatConfig;
 import org.apache.iceberg.mr.TestHelper;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.types.Type;
@@ -35,6 +36,7 @@
 
 import static org.apache.iceberg.types.Types.NestedField.optional;
 import static org.apache.iceberg.types.Types.NestedField.required;
+import static org.junit.Assume.assumeTrue;
 
 /**
  * Runs miscellaneous select statements on Iceberg tables, and verifies the result. Tests meant to verify simple
@@ -229,4 +231,35 @@ public void testMultiColumnPruning() throws IOException {
     Assert.assertEquals(1, result.size());
     Assert.assertArrayEquals(new Object[]{"val"}, result.get(0));
   }
+
+  /**
+   * Tests that vectorized ORC reading code path correctly handles when the same ORC file is split into multiple parts.
+   * Although the split offsets and length will not always include the file tail that contains the metadata, the
+   * vectorized reader needs to make sure to handle the tail reading regardless of the offsets. If this is not done
+   * correctly, the last SELECT query will fail.
+   * @throws Exception - any test error
+   */
+  @Test
+  public void testVectorizedOrcMultipleSplits() throws Exception {
+    assumeTrue(isVectorized && FileFormat.ORC.equals(fileFormat));
+
+    // This data will be held by a ~870kB ORC file
+    List<Record> records = TestHelper.generateRandomRecords(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA,
+        20000, 0L);
+
+    // To support splitting the ORC file, we need to specify the stripe size to a small value. It looks like the min
+    // value is about 220kB, no smaller stripes are written by ORC. Anyway, this setting will produce 4 stripes.
+    shell.setHiveSessionValue("orc.stripe.size", "210000");
+
+    testTables.createTable(shell, "targettab", HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA,
+        fileFormat, records);
+
+    // Will request 4 splits, separated on the exact stripe boundaries within the ORC file.
+    // (Would request 5 if ORC split generation wouldn't be split (aka stripe) offset aware).
+    shell.setHiveSessionValue(InputFormatConfig.SPLIT_SIZE, "210000");
+    List<Object[]> result = shell.executeStatement("SELECT * FROM targettab ORDER BY last_name");
+
+    Assert.assertEquals(20000, result.size());
+
+  }
 }
