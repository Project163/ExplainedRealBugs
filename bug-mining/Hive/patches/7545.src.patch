diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java
index fab83b6501..f4e109d1b0 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java
@@ -394,7 +394,7 @@ public Void execute(int size) throws MetastoreException {
     }.run();
   }
 
-  private static String makePartExpr(Map<String, String> spec)
+  public static String makePartExpr(Map<String, String> spec)
     throws MetaException {
     StringBuilder suffixBuf = new StringBuilder();
     int i = 0;
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MsckPartitionExpressionProxy.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MsckPartitionExpressionProxy.java
index d842825559..f9f0c86bfa 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MsckPartitionExpressionProxy.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MsckPartitionExpressionProxy.java
@@ -18,12 +18,19 @@
  */
 
 import java.nio.charset.StandardCharsets;
+import java.util.ArrayList;
+import java.util.HashSet;
 import java.util.List;
+import java.util.Set;
 
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.FileMetadataExprType;
 import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.utils.FileUtils;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
+import org.apache.hadoop.util.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 // This is added as part of moving MSCK code from ql to standalone-metastore. There is a metastore API to drop
 // partitions by name but we cannot use it because msck typically will contain partition value (year=2014). We almost
@@ -36,6 +43,8 @@
 // should use SearchArgument (storage-api) to construct the filter expression and not depend on ql, but the usecase
 // for msck is pretty simple and this specific implementation should suffice.
 public class MsckPartitionExpressionProxy implements PartitionExpressionProxy {
+  private static final Logger LOG = LoggerFactory.getLogger(MsckPartitionExpressionProxy.class);
+
   @Override
   public String convertExprToFilter(final byte[] exprBytes, final String defaultPartitionName) throws MetaException {
     return new String(exprBytes, StandardCharsets.UTF_8);
@@ -44,6 +53,47 @@ public String convertExprToFilter(final byte[] exprBytes, final String defaultPa
   @Override
   public boolean filterPartitionsByExpr(List<FieldSchema> partColumns, byte[] expr, String
     defaultPartitionName, List<String> partitionNames) throws MetaException {
+    String partExpr = new String(expr, StandardCharsets.UTF_8);
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Partition expr: {}", expr);
+    }
+    //This is to find in partitionNames all that match expr
+    //reverse of the Msck.makePartExpr
+    Set<String> partValueSet = new HashSet<>();
+    String[] parts = partExpr.split(" AND ");
+    for ( String part : parts){
+      String[] colAndValue = part.split("=");
+      String key = FileUtils.unescapePathName(colAndValue[0]);
+      //take the value inside without the single quote marks '2018-10-30' becomes 2018-10-31
+      String value = FileUtils.unescapePathName(colAndValue[1].substring(1, colAndValue[1].length()-1));
+      partValueSet.add(key+"="+value);
+    }
+
+    List<String> partNamesSeq =  new ArrayList<>();
+    for (String partition : partitionNames){
+      boolean isMatch = true;
+      //list of partitions [year=2001/month=1, year=2002/month=2, year=2001/month=3]
+      //Given expr: e.g. year='2001' AND month='1'. Only when all the expressions in the expr can be found,
+      //do we add the partition to the filtered result [year=2001/month=1]
+      String [] partnames = partition.split("/");
+      for (String part: partnames) {
+        if (!partValueSet.contains(FileUtils.unescapePathName(part))){
+          isMatch = false;
+          break;
+        }
+      }
+      if (isMatch){
+        partNamesSeq.add(partition);
+      }
+    }
+    partitionNames.clear();
+    partitionNames.addAll(partNamesSeq);
+    LOG.info("The returned partition list is of size: {}", partitionNames.size());
+    if (LOG.isDebugEnabled()) {
+      for(String s : partitionNames){
+        LOG.debug("Matched partition: {}", s);
+      }
+    }
     return false;
   }
 
diff --git a/standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestPartitionManagement.java b/standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestPartitionManagement.java
index 1961a70cd7..732d7eefea 100644
--- a/standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestPartitionManagement.java
+++ b/standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestPartitionManagement.java
@@ -24,6 +24,7 @@
 
 import java.io.IOException;
 import java.net.URI;
+import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
@@ -40,6 +41,7 @@
 import org.apache.hadoop.hive.metastore.annotation.MetastoreUnitTest;
 import org.apache.hadoop.hive.metastore.api.Catalog;
 import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
@@ -654,6 +656,44 @@ public void testNoPartitionRetentionForReplTarget() throws TException, Interrupt
     assertEquals(3, partitions.size());
   }
 
+  @Test
+  public void testPartitionExprFilter() throws TException, IOException {
+    String dbName = "db10";
+    String tableName = "tbl10";
+    Map<String, Column> colMap = buildAllColumns();
+    List<String> partKeys = Lists.newArrayList("state", "dt", "modts");
+    List<String> partKeyTypes = Lists.newArrayList("string", "date", "timestamp");
+
+    List<List<String>> partVals = Lists.newArrayList(
+        Lists.newArrayList("__HIVE_DEFAULT_PARTITION__", "1990-01-01", "__HIVE_DEFAULT_PARTITION__"),
+        Lists.newArrayList("CA", "1986-04-28", "2020-02-21 08:30:01"),
+        Lists.newArrayList("MN", "2018-11-31", "2020-02-21 08:19:01"));
+    createMetadata(DEFAULT_CATALOG_NAME, dbName, tableName, partKeys, partKeyTypes, partVals, colMap, false);
+    Table table = client.getTable(dbName, tableName);
+
+    table.getParameters().put(PartitionManagementTask.DISCOVER_PARTITIONS_TBLPROPERTY, "true");
+    table.getParameters().put("EXTERNAL", "true");
+    table.setTableType(TableType.EXTERNAL_TABLE.name());
+    client.alter_table(dbName, tableName, table);
+
+    List<Partition> partitions = client.listPartitions(dbName, tableName, (short) -1);
+    assertEquals(3, partitions.size());
+
+    String tableLocation = table.getSd().getLocation();
+    URI location = URI.create(tableLocation);
+    Path tablePath = new Path(location);
+    FileSystem fs = FileSystem.get(location, conf);
+    String partPath = partitions.get(1).getSd().getLocation();
+    Path newPart1 = new Path(tablePath, partPath);
+    fs.delete(newPart1);
+
+    conf.set(MetastoreConf.ConfVars.PARTITION_MANAGEMENT_DATABASE_PATTERN.getVarname(), "*db10*");
+    conf.set(ConfVars.PARTITION_MANAGEMENT_TABLE_TYPES.getVarname(), TableType.EXTERNAL_TABLE.name());
+    runPartitionManagementTask(conf);
+    partitions = client.listPartitions(dbName, tableName, (short) -1);
+    assertEquals(2, partitions.size());
+  }
+
   private void runPartitionManagementTask(Configuration conf) {
     PartitionManagementTask task = new PartitionManagementTask();
     task.setConf(conf);
@@ -669,4 +709,6 @@ public Column(final String colName, final String colType) {
       this.colType = colType;
     }
   }
+
+
 }
