diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java
index d504b3c496..94afabaf2c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java
@@ -287,7 +287,8 @@ private List<ColumnStatistics> constructColumnStatsFromPackedRows() throws HiveE
       Table tbl = db.getTable(dbName,tableName);
       List<FieldSchema> partColSchema = tbl.getPartCols();
       // Partition columns are appended at end, we only care about stats column
-      for (int i = 0; i < fields.size() - partColSchema.size(); i++) {
+      int numOfStatCols = isTblLevel ? fields.size() : fields.size() - partColSchema.size();
+      for (int i = 0; i < numOfStatCols; i++) {
         // Get the field objectInspector, fieldName and the field object.
         ObjectInspector foi = fields.get(i).getFieldObjectInspector();
         Object f = (list == null ? null : list.get(i));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java
index 6c9876d41c..195d4f1feb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java
@@ -300,8 +300,11 @@ private String genRewrittenQuery(List<String> colNames, int numBitVectors, Map<S
       rewrittenQueryBuilder.append(numBitVectors);
       rewrittenQueryBuilder.append(" )");
     }
-    for (FieldSchema fs : tbl.getPartCols()) {
-      rewrittenQueryBuilder.append(" , " + fs.getName());
+
+    if (isPartitionStats) {
+      for (FieldSchema fs : tbl.getPartCols()) {
+        rewrittenQueryBuilder.append(" , " + fs.getName());
+      }
     }
     rewrittenQueryBuilder.append(" from ");
     rewrittenQueryBuilder.append(tbl.getTableName());
diff --git a/ql/src/test/queries/clientpositive/columnstats_partlvl.q b/ql/src/test/queries/clientpositive/columnstats_partlvl.q
index 99b859cc06..7ec101edf6 100644
--- a/ql/src/test/queries/clientpositive/columnstats_partlvl.q
+++ b/ql/src/test/queries/clientpositive/columnstats_partlvl.q
@@ -25,3 +25,9 @@ analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics
 
 describe formatted Employee_Part.employeeID   partition (employeeSalary=2000.0);
 describe formatted Employee_Part.employeeName partition (employeeSalary=2000.0);
+
+explain 
+analyze table Employee_Part  compute statistics for columns;
+analyze table Employee_Part  compute statistics for columns;
+
+describe formatted Employee_Part.employeeID;
diff --git a/ql/src/test/results/clientpositive/columnstats_partlvl.q.out b/ql/src/test/results/clientpositive/columnstats_partlvl.q.out
index 28c21e65b9..e7196d0c43 100644
--- a/ql/src/test/results/clientpositive/columnstats_partlvl.q.out
+++ b/ql/src/test/results/clientpositive/columnstats_partlvl.q.out
@@ -509,3 +509,72 @@ POSTHOOK: Input: default@employee_part
 # col_name            	data_type           	min                 	max                 	num_nulls           	distinct_count      	avg_col_len         	max_col_len         	num_trues           	num_falses          	comment             
 	 	 	 	 	 	 	 	 	 	 
 employeeName        	string              	                    	                    	1                   	9                   	4.3076923076923075  	6                   	                    	                    	from deserializer   
+PREHOOK: query: explain 
+analyze table Employee_Part  compute statistics for columns
+PREHOOK: type: QUERY
+POSTHOOK: query: explain 
+analyze table Employee_Part  compute statistics for columns
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+  Stage-1 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: employee_part
+            Select Operator
+              expressions: employeeid (type: int), employeename (type: string)
+              outputColumnNames: employeeid, employeename
+              Group By Operator
+                aggregations: compute_stats(employeeid, 16), compute_stats(employeename, 16)
+                mode: hash
+                outputColumnNames: _col0, _col1
+                Reduce Output Operator
+                  sort order: 
+                  value expressions: _col0 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>), _col1 (type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: compute_stats(VALUE._col0), compute_stats(VALUE._col1)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions: _col0 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>), _col1 (type: struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint,numdistinctvalues:bigint>)
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-1
+    Column Stats Work
+      Column Stats Desc:
+          Columns: employeeid, employeename
+          Column Types: int, string
+          Table: employee_part
+
+PREHOOK: query: analyze table Employee_Part  compute statistics for columns
+PREHOOK: type: QUERY
+PREHOOK: Input: default@employee_part
+PREHOOK: Input: default@employee_part@employeesalary=2000.0
+PREHOOK: Input: default@employee_part@employeesalary=4000.0
+#### A masked pattern was here ####
+POSTHOOK: query: analyze table Employee_Part  compute statistics for columns
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@employee_part
+POSTHOOK: Input: default@employee_part@employeesalary=2000.0
+POSTHOOK: Input: default@employee_part@employeesalary=4000.0
+#### A masked pattern was here ####
+PREHOOK: query: describe formatted Employee_Part.employeeID
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@employee_part
+POSTHOOK: query: describe formatted Employee_Part.employeeID
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@employee_part
+# col_name            	data_type           	min                 	max                 	num_nulls           	distinct_count      	avg_col_len         	max_col_len         	num_trues           	num_falses          	comment             
+	 	 	 	 	 	 	 	 	 	 
+employeeID          	int                 	16                  	34                  	2                   	14                  	                    	                    	                    	                    	from deserializer   
