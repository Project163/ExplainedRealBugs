diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index f22d9245e7..197a20fc31 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -2351,7 +2351,7 @@ public static void setColumnTypeList(JobConf jobConf, Operator op, boolean exclu
     jobConf.set(serdeConstants.LIST_COLUMN_TYPES, columnTypesString);
   }
 
-  public static void validatePartSpec(Table tbl, Map<String, String> partSpec)
+  public static void validatePartSpecColumnNames(Table tbl, Map<String, String> partSpec)
       throws SemanticException {
 
     List<FieldSchema> parts = tbl.getPartitionKeys();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index da80d817bb..4b7fc7378c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -1134,48 +1134,53 @@ protected boolean analyzeStoredAdDirs(ASTNode child) {
     return storedAsDirs;
   }
 
-  private static void getPartExprNodeDesc(ASTNode astNode,
-      Map<ASTNode, ExprNodeDesc> astExprNodeMap)
-          throws SemanticException, HiveException {
+  private static boolean getPartExprNodeDesc(ASTNode astNode,
+      Map<ASTNode, ExprNodeDesc> astExprNodeMap) throws SemanticException {
 
-    if ((astNode == null) || (astNode.getChildren() == null) || 
-        (astNode.getChildren().size() == 0)) {
-      return;
+    if (astNode == null) {
+      return true;
+    } else if ((astNode.getChildren() == null) || (astNode.getChildren().size() == 0)) {
+      return astNode.getType() != HiveParser.TOK_PARTVAL;
     }
 
     TypeCheckCtx typeCheckCtx = new TypeCheckCtx(null);
+    boolean result = true;
     for (Node childNode : astNode.getChildren()) {
       ASTNode childASTNode = (ASTNode)childNode;
 
       if (childASTNode.getType() != HiveParser.TOK_PARTVAL) {
-        getPartExprNodeDesc(childASTNode, astExprNodeMap);
+        result = getPartExprNodeDesc(childASTNode, astExprNodeMap) && result;
       } else {
-        if (childASTNode.getChildren().size() <= 1) {
-          throw new HiveException("This is dynamic partitioning");
+        boolean isDynamicPart = childASTNode.getChildren().size() <= 1;
+        result = !isDynamicPart && result;
+        if (!isDynamicPart) {
+          ASTNode partVal = (ASTNode)childASTNode.getChildren().get(1);
+          astExprNodeMap.put((ASTNode)childASTNode.getChildren().get(0),
+            TypeCheckProcFactory.genExprNode(partVal, typeCheckCtx).get(partVal));
         }
-
-        ASTNode partValASTChild = (ASTNode)childASTNode.getChildren().get(1);
-        astExprNodeMap.put((ASTNode)childASTNode.getChildren().get(0),
-            TypeCheckProcFactory.genExprNode(partValASTChild, typeCheckCtx).get(partValASTChild));
       }
     }
+    return result;
   }
 
   public static void validatePartSpec(Table tbl, Map<String, String> partSpec,
       ASTNode astNode, HiveConf conf) throws SemanticException {
-    Map<ASTNode, ExprNodeDesc> astExprNodeMap = new HashMap<ASTNode, ExprNodeDesc>();
-
-    Utilities.validatePartSpec(tbl, partSpec);
+    Utilities.validatePartSpecColumnNames(tbl, partSpec);
 
     if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_TYPE_CHECK_ON_INSERT)) {
       return;
     }
 
-    try {
-      getPartExprNodeDesc(astNode, astExprNodeMap);
-    } catch (HiveException e) {
-      return;
+    Map<ASTNode, ExprNodeDesc> astExprNodeMap = new HashMap<ASTNode, ExprNodeDesc>();
+    if (!getPartExprNodeDesc(astNode, astExprNodeMap)) {
+      STATIC_LOG.warn("Dynamic partitioning is used; only validating "
+          + astExprNodeMap.size() + " columns");
     }
+
+    if (astExprNodeMap.isEmpty()) {
+      return; // All columns are dynamic, nothing to do.
+    }
+
     List<FieldSchema> parts = tbl.getPartitionKeys();
     Map<String, String> partCols = new HashMap<String, String>(parts.size());
     for (FieldSchema col : parts) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index d0e1a91597..08b9e58314 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -5267,7 +5267,7 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)
         }
         dpCtx = qbm.getDPCtx(dest);
         if (dpCtx == null) {
-          Utilities.validatePartSpec(dest_tab, partSpec);
+          Utilities.validatePartSpecColumnNames(dest_tab, partSpec);
           dpCtx = new DynamicPartitionCtx(dest_tab, partSpec,
               conf.getVar(HiveConf.ConfVars.DEFAULTPARTITIONNAME),
               conf.getIntVar(HiveConf.ConfVars.DYNAMICPARTITIONMAXPARTSPERNODE));
diff --git a/ql/src/test/queries/clientnegative/illegal_partition_type4.q b/ql/src/test/queries/clientnegative/illegal_partition_type4.q
new file mode 100644
index 0000000000..50f486e624
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/illegal_partition_type4.q
@@ -0,0 +1,3 @@
+create table tab1(s string) PARTITIONED BY(dt date, st string);
+alter table tab1 add partition (dt=date 'foo', st='foo');
+drop table tab1;
diff --git a/ql/src/test/results/clientnegative/illegal_partition_type4.q.out b/ql/src/test/results/clientnegative/illegal_partition_type4.q.out
new file mode 100644
index 0000000000..8393318b19
--- /dev/null
+++ b/ql/src/test/results/clientnegative/illegal_partition_type4.q.out
@@ -0,0 +1,6 @@
+PREHOOK: query: create table tab1(s string) PARTITIONED BY(dt date, st string)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table tab1(s string) PARTITIONED BY(dt date, st string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tab1
+FAILED: SemanticException Unable to convert date literal string to date value.
