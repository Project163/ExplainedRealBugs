diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
index dfe65e72c2..37c397068f 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.ql.parse;
 
+import org.apache.commons.io.IOUtils;
 import org.apache.commons.lang3.RandomStringUtils;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.commons.lang3.exception.ExceptionUtils;
@@ -38,11 +39,13 @@
 import org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder;
 import org.apache.hadoop.hive.metastore.utils.MetaStoreUtils;
 import org.apache.hadoop.hive.ql.ErrorMsg;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.parse.WarehouseInstance.Tuple;
 import org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder;
 import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;
 import org.apache.hadoop.hive.ql.parse.repl.PathBuilder;
+import org.apache.hadoop.hive.ql.parse.repl.dump.Utils;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 import org.apache.hadoop.hive.ql.util.DependencyResolver;
 import org.apache.hadoop.security.UserGroupInformation;
@@ -69,6 +72,8 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
+import static org.apache.hadoop.hdfs.protocol.HdfsConstants.QUOTA_DONT_SET;
+import static org.apache.hadoop.hdfs.protocol.HdfsConstants.QUOTA_RESET;
 import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;
 import static org.apache.hadoop.hive.ql.exec.repl.ReplAck.LOAD_ACKNOWLEDGEMENT;
 import static org.apache.hadoop.hive.ql.exec.repl.ReplAck.NON_RECOVERABLE_MARKER;
@@ -79,6 +84,7 @@
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
 public class TestReplicationScenariosAcrossInstances extends BaseReplicationAcrossInstances {
   private static final String NS_REMOTE = "nsRemote";
@@ -2237,6 +2243,53 @@ private void ensureFailedAdminRepl(List<String> clause, boolean dump) throws Thr
     }
   }
 
+  @Test
+  public void testReplicationUtils() throws Throwable {
+    Path testPath = new Path("/tmp/testReplicationUtils" + System.currentTimeMillis());
+    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();
+    fs.mkdirs(testPath);
+    Path filePath1 = new Path(testPath, "file1");
+    fs.setQuota(testPath, QUOTA_DONT_SET, 0);
+    try {
+      Utils.writeOutput("abc", filePath1, conf);
+      fail("Expected exception due to quota violation");
+    } catch (Exception e) {
+      // Expected.
+    }
+
+    // Remove the quota & retry, this time it should be successful.
+    fs.setQuota(testPath, QUOTA_DONT_SET, QUOTA_RESET);
+    Utils.writeOutput("abc" + Utilities.newLineCode, filePath1, conf);
+
+    // Check the contents of the file are written correctly.
+    try (FSDataInputStream stream = fs.open(filePath1)) {
+      assertEquals("abc" + Utilities.newLineCode + "\n", IOUtils.toString(stream, Charset.defaultCharset()));
+    }
+
+    // Check the Utils with writing a list of entries
+    Path filePath2 = new Path(testPath, "file2");
+    fs.setQuota(testPath, QUOTA_DONT_SET, 0);
+    List<List<String>> data = Arrays.asList(Arrays.asList("a", "b"));
+    try {
+      Utils.writeOutput(data, filePath2, conf, true);
+      fail("Expected exception due to quota violation");
+    } catch (Exception e) {
+      // Expected.
+    }
+
+    // Remove the quota & retry, this time it should be successful.
+    fs.setQuota(testPath, QUOTA_DONT_SET, QUOTA_RESET);
+
+    // Write the contents.
+    Utils.writeOutput(data, filePath2, conf, true);
+
+    // Check the contents of the file are written correctly.
+    try (FSDataInputStream stream = fs.open(filePath2)) {
+      assertEquals("a" + "\t" + "b" + "\n", IOUtils.toString(stream,
+          Charset.defaultCharset()));
+    }
+  }
+
   private void ensureInvalidUrl(List<String> atlasClause, String endpoint, boolean dump) throws Throwable {
     try {
       if (dump) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java
index 4ca65cd625..a2a1390b01 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java
@@ -130,7 +130,9 @@ public static void writeOutput(List<List<String>> listValues, Path outputFile, H
             outStream.write(Utilities.newLineCode);
           }
         } finally {
-          IOUtils.closeStream(outStream);
+          if (outStream != null) {
+            outStream.close();
+          }
         }
         return null;
       });
@@ -198,7 +200,9 @@ public static void writeOutput(String content, Path outputFile, HiveConf hiveCon
           outStream.writeBytes(content);
           outStream.write(Utilities.newLineCode);
         } finally {
-          IOUtils.closeStream(outStream);
+          if (outStream != null) {
+            outStream.close();
+          }
         }
         return null;
       });
