diff --git a/CHANGES.txt b/CHANGES.txt
index f6a6c8a293..dc8dc70b2d 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -180,6 +180,9 @@ Trunk - Unreleased
     HIVE-467. Scratch data location should be on different filesystems for
     different types of intermediate data. (Joydeep Sen Sarma via rmurthy)
 
+    HIVE-514. Partition key names should be case insensitive in alter table add
+    partition statement. (Prasad Chakka via zshao)
+
 Release 0.3.1 - Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/conf/hive-default.xml b/conf/hive-default.xml
index f7b60801aa..3dd3bc2e46 100644
--- a/conf/hive-default.xml
+++ b/conf/hive-default.xml
@@ -10,6 +10,16 @@
 <!-- resource).                                                                                 -->
 
 <!-- Hive Execution Parameters -->
+<property>
+  <name>mapred.reduce.tasks</name>
+  <value>-1</value>
+    <description>The default number of reduce tasks per job.  Typically set
+  to a prime close to the number of available hosts.  Ignored when
+  mapred.job.tracker is "local". Hadoop set this to 1 by default, whereas hive uses -1 as its default value.
+  By setting this property to -1, Hive will automatically figure out what should be the number of reducers.
+  </description>
+</property>
+
 <property>
   <name>hive.exec.scratchdir</name>
   <value>/tmp/hive-${user.name}</value>
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
index 9e41bb9071..f70822640e 100755
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
@@ -203,7 +203,7 @@ public static String makePartName(List<FieldSchema> partCols, List<String> vals)
       if(i > 0) {
         name.append(Path.SEPARATOR);
       }
-      name.append((partCols.get(i)).getName());
+      name.append((partCols.get(i)).getName().toLowerCase());
       name.append('=');
       name.append(vals.get(i));
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
index 53085c4c6e..a0068821c1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
@@ -87,7 +87,11 @@ public Partition(Table tbl, Map<String, String> partSpec,
 
     List<String> pvals = new ArrayList<String>();
     for (FieldSchema field : tbl.getPartCols()) {
-      pvals.add(partSpec.get(field.getName()));
+      String val = partSpec.get(field.getName());
+      if (val == null) {
+        throw new HiveException("partition spec is invalid. field.getName() does not exist in input.");
+      }
+      pvals.add(val);
     }
     
     org.apache.hadoop.hive.metastore.api.Partition tpart = 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index dca14a38aa..2abf2c275e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -274,7 +274,7 @@ public tableSpec(Hive db, HiveConf conf, ASTNode ast) throws SemanticException {
         for (int i = 0; i < partspec.getChildCount(); ++i) {
           ASTNode partspec_val = (ASTNode) partspec.getChild(i);
           String val = stripQuotes(partspec_val.getChild(1).getText());
-          partSpec.put(unescapeIdentifier(partspec_val.getChild(0).getText()), val);
+          partSpec.put(unescapeIdentifier(partspec_val.getChild(0).getText().toLowerCase()), val);
         }
         try {
           // this doesn't create partition. partition is created in MoveTask
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 142a28df0f..6418fff225 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -658,7 +658,7 @@ private List<Map<String, String>> getPartitionSpecs(CommonTree ast) throws Seman
         for (int i = 0; i < partspec.getChildCount(); ++i) {
           CommonTree partspec_val = (CommonTree) partspec.getChild(i);
           String val = stripQuotes(partspec_val.getChild(1).getText());
-          partSpec.put(partspec_val.getChild(0).getText(), val);
+          partSpec.put(partspec_val.getChild(0).getText().toLowerCase(), val);
         }
         partSpecs.add(partSpec);
       }
diff --git a/ql/src/test/queries/clientpositive/loadpart1.q b/ql/src/test/queries/clientpositive/loadpart1.q
index eec57ee918..401547af1b 100644
--- a/ql/src/test/queries/clientpositive/loadpart1.q
+++ b/ql/src/test/queries/clientpositive/loadpart1.q
@@ -1,11 +1,19 @@
 drop table hive_test_src;
 drop table hive_test_dst;
+
 create table hive_test_src ( col1 string ) stored as textfile ;
 load data local inpath '../data/files/test.dat' overwrite into table hive_test_src ;
+
 create table hive_test_dst ( col1 string ) partitioned by ( pcol1 string , pcol2 string) stored as sequencefile;
-insert overwrite table hive_test_dst partition ( pcol1='test_part', pcol2='test_part') select col1 from hive_test_src ;
+insert overwrite table hive_test_dst partition ( pcol1='test_part', pCol2='test_Part') select col1 from hive_test_src ;
+select * from hive_test_dst where pcol1='test_part' and pcol2='test_Part';
+
+insert overwrite table hive_test_dst partition ( pCol1='test_part', pcol2='test_Part') select col1 from hive_test_src ;
 select * from hive_test_dst where pcol1='test_part' and pcol2='test_part';
-insert overwrite table hive_test_dst partition ( pcol1='test_part', pcol2='test_part') select col1 from hive_test_src ;
+
+select * from hive_test_dst where pcol1='test_part';
 select * from hive_test_dst where pcol1='test_part' and pcol2='test_part';
+select * from hive_test_dst where pcol1='test_Part';
+
 drop table hive_test_src;
 drop table hive_test_dst;
diff --git a/ql/src/test/results/clientpositive/loadpart1.q.out b/ql/src/test/results/clientpositive/loadpart1.q.out
index ae9d90e5ff..bdc9e10fad 100644
--- a/ql/src/test/results/clientpositive/loadpart1.q.out
+++ b/ql/src/test/results/clientpositive/loadpart1.q.out
@@ -3,29 +3,35 @@ query: drop table hive_test_dst
 query: create table hive_test_src ( col1 string ) stored as textfile
 query: load data local inpath '../data/files/test.dat' overwrite into table hive_test_src
 query: create table hive_test_dst ( col1 string ) partitioned by ( pcol1 string , pcol2 string) stored as sequencefile
-query: insert overwrite table hive_test_dst partition ( pcol1='test_part', pcol2='test_part') select col1 from hive_test_src
+query: insert overwrite table hive_test_dst partition ( pcol1='test_part', pCol2='test_Part') select col1 from hive_test_src
 Input: default/hive_test_src
-Output: default/hive_test_dst/pcol1=test_part/pcol2=test_part
-query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_part'
-Input: default/hive_test_dst/pcol1=test_part/pcol2=test_part
-Output: /data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/608894653/647768977.10000
-1	test_part	test_part
-2	test_part	test_part
-3	test_part	test_part
-4	test_part	test_part
-5	test_part	test_part
-6	test_part	test_part
-query: insert overwrite table hive_test_dst partition ( pcol1='test_part', pcol2='test_part') select col1 from hive_test_src
+Output: default/hive_test_dst/pcol1=test_part/pcol2=test_Part
+query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_Part'
+Input: default/hive_test_dst/pcol1=test_part/pcol2=test_Part
+Output: /Users/pchakka/workspace/oshive/ql/../build/ql/tmp/20877141/262522507.10000
+1	test_part	test_Part
+2	test_part	test_Part
+3	test_part	test_Part
+4	test_part	test_Part
+5	test_part	test_Part
+6	test_part	test_Part
+query: insert overwrite table hive_test_dst partition ( pCol1='test_part', pcol2='test_Part') select col1 from hive_test_src
 Input: default/hive_test_src
-Output: default/hive_test_dst/pcol1=test_part/pcol2=test_part
+Output: default/hive_test_dst/pcol1=test_part/pcol2=test_Part
+query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_part'
+Output: /Users/pchakka/workspace/oshive/ql/../build/ql/tmp/1586437457/438136405.10000
+query: select * from hive_test_dst where pcol1='test_part'
+Input: default/hive_test_dst/pcol1=test_part/pcol2=test_Part
+Output: /Users/pchakka/workspace/oshive/ql/../build/ql/tmp/991232921/143300248.10000
+1	test_part	test_Part
+2	test_part	test_Part
+3	test_part	test_Part
+4	test_part	test_Part
+5	test_part	test_Part
+6	test_part	test_Part
 query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_part'
-Input: default/hive_test_dst/pcol1=test_part/pcol2=test_part
-Output: /data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/255138744/205167628.10000
-1	test_part	test_part
-2	test_part	test_part
-3	test_part	test_part
-4	test_part	test_part
-5	test_part	test_part
-6	test_part	test_part
+Output: /Users/pchakka/workspace/oshive/ql/../build/ql/tmp/118754797/315488459.10000
+query: select * from hive_test_dst where pcol1='test_Part'
+Output: /Users/pchakka/workspace/oshive/ql/../build/ql/tmp/27905267/502654394.10000
 query: drop table hive_test_src
 query: drop table hive_test_dst
