diff --git a/CHANGES.txt b/CHANGES.txt
index be9e1b1251..1ac4583211 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -152,6 +152,9 @@ Trunk -  Unreleased
     HIVE-858. Better logging for script operator even in case of failure
     (Zheng Shao via namit)
 
+    HIVE-861. NumberFormatException in sum and average
+    (Zheng Shao via namit)
+
 Release 0.4.0 -  Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
index d629b8e33a..d81b12a580 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
@@ -19,6 +19,8 @@
 
 import java.util.ArrayList;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.exec.description;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -36,6 +38,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.util.StringUtils;
 
 @description(
     name = "avg",
@@ -43,6 +46,8 @@
 )
 public class GenericUDAFAverage implements GenericUDAFResolver {
 
+  static final Log LOG = LogFactory.getLog(GenericUDAFAverage.class.getName());
+  
   @Override
   public GenericUDAFEvaluator getEvaluator(
       TypeInfo[] parameters) throws SemanticException {
@@ -148,16 +153,26 @@ public void reset(AggregationBuffer agg) throws HiveException {
       myagg.sum = 0;      
     }
     
+    boolean warned = false;
+    
     @Override
     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
       assert(parameters.length == 1);
       Object p = parameters[0];
       if (p != null) {
         AverageAgg myagg = (AverageAgg)agg;
-        double v = PrimitiveObjectInspectorUtils.getDouble(p, 
+        try {
+          double v = PrimitiveObjectInspectorUtils.getDouble(p, 
             (PrimitiveObjectInspector)inputOI);
-        myagg.count ++;
-        myagg.sum += v;
+          myagg.count ++;
+          myagg.sum += v;
+        } catch (NumberFormatException e) {
+          if (!warned) {
+            warned = true;
+            LOG.warn(getClass().getSimpleName() + " " + StringUtils.stringifyException(e));
+            LOG.warn(getClass().getSimpleName() + " ignoring similar exceptions.");
+          }
+        }
       }
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
index fbc7a27f5d..29050030b5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
@@ -17,6 +17,8 @@
  */
 package org.apache.hadoop.hive.ql.udf.generic;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.exec.description;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -29,6 +31,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.util.StringUtils;
 
 @description(
     name = "sum",
@@ -36,6 +39,8 @@
 )
 public class GenericUDAFSum implements GenericUDAFResolver {
 
+  static final Log LOG = LogFactory.getLog(GenericUDAFSum.class.getName());
+  
   @Override
   public GenericUDAFEvaluator getEvaluator(
       TypeInfo[] parameters) throws SemanticException {
@@ -100,10 +105,20 @@ public void reset(AggregationBuffer agg) throws HiveException {
       myagg.sum = 0;      
     }
 
+    boolean warned = false;
+    
     @Override
     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
       assert(parameters.length == 1);
-      merge(agg, parameters[0]);
+      try {
+        merge(agg, parameters[0]);
+      } catch (NumberFormatException e) {
+        if (!warned) {
+          warned = true;
+          LOG.warn(getClass().getSimpleName() + " " + StringUtils.stringifyException(e));
+          LOG.warn(getClass().getSimpleName() + " ignoring similar exceptions.");
+        }
+      }
     }
 
     @Override
@@ -168,10 +183,19 @@ public void reset(AggregationBuffer agg) throws HiveException {
       myagg.sum = 0;      
     }
 
+    boolean warned = false;
+    
     @Override
     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {
       assert(parameters.length == 1);
-      merge(agg, parameters[0]);
+      try {
+        merge(agg, parameters[0]);
+      } catch (NumberFormatException e) {
+        if (!warned) {
+          warned = true;
+          LOG.warn(getClass().getSimpleName() + " " + StringUtils.stringifyException(e));
+        }
+      }
     }
 
     @Override
@@ -183,8 +207,8 @@ public Object terminatePartial(AggregationBuffer agg) throws HiveException {
     public void merge(AggregationBuffer agg, Object partial) throws HiveException {
       if (partial != null) {
         SumLongAgg myagg = (SumLongAgg)agg;
-        myagg.empty = false;
         myagg.sum += PrimitiveObjectInspectorUtils.getLong(partial, inputOI); 
+        myagg.empty = false;
       }
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFVariance.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFVariance.java
index 929cc1c24d..7d831f9cc1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFVariance.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFVariance.java
@@ -19,6 +19,8 @@
 
 import java.util.ArrayList;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.exec.description;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -36,6 +38,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.util.StringUtils;
 
 /**
  * Compute the variance. This class is extended by:
@@ -50,6 +53,8 @@
 )
 public class GenericUDAFVariance implements GenericUDAFResolver {
   
+  static final Log LOG = LogFactory.getLog(GenericUDAFVariance.class.getName());
+  
   @Override
   public GenericUDAFEvaluator getEvaluator(
       TypeInfo[] parameters) throws SemanticException {
@@ -192,6 +197,8 @@ public void reset(AggregationBuffer agg) throws HiveException {
       myagg.variance = 0;
     }
     
+    boolean warned = false;
+    
     @Override
     public void iterate(AggregationBuffer agg, Object[] parameters) 
     throws HiveException {
@@ -199,21 +206,29 @@ public void iterate(AggregationBuffer agg, Object[] parameters)
       Object p = parameters[0];
       if (p != null) {
         StdAgg myagg = (StdAgg)agg;
-        double v = PrimitiveObjectInspectorUtils.getDouble(p, 
-            (PrimitiveObjectInspector)inputOI);
-       
-        if(myagg.count != 0) { // if count==0 => the variance is going to be 0
-                              // after 1 iteration
-          double alpha = (myagg.sum + v) / (myagg.count+1) 
-                          - myagg.sum / myagg.count;
-          double betha = (myagg.sum + v) / (myagg.count+1) - v;
-          
-          // variance = variance1 + variance2 + n*alpha^2 + m*betha^2
-          // => variance += n*alpha^2 + betha^2
-          myagg.variance += myagg.count*alpha*alpha + betha*betha;
+        try {
+          double v = PrimitiveObjectInspectorUtils.getDouble(p, 
+              (PrimitiveObjectInspector)inputOI);
+         
+          if(myagg.count != 0) { // if count==0 => the variance is going to be 0
+                                // after 1 iteration
+            double alpha = (myagg.sum + v) / (myagg.count+1) 
+                            - myagg.sum / myagg.count;
+            double betha = (myagg.sum + v) / (myagg.count+1) - v;
+            
+            // variance = variance1 + variance2 + n*alpha^2 + m*betha^2
+            // => variance += n*alpha^2 + betha^2
+            myagg.variance += myagg.count*alpha*alpha + betha*betha;
+          }
+          myagg.count++;
+          myagg.sum += v;
+        } catch (NumberFormatException e) {
+          if (!warned) {
+            warned = true;
+            LOG.warn(getClass().getSimpleName() + " " + StringUtils.stringifyException(e));
+            LOG.warn(getClass().getSimpleName() + " ignoring similar exceptions.");
+          }
         }
-        myagg.count++;
-        myagg.sum += v;
       }
     }
 
diff --git a/ql/src/test/queries/clientpositive/udaf_number_format.q b/ql/src/test/queries/clientpositive/udaf_number_format.q
new file mode 100644
index 0000000000..e3ad888891
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/udaf_number_format.q
@@ -0,0 +1,13 @@
+EXPLAIN SELECT
+  sum('a'),
+  avg('a'),
+  variance('a'),
+  std('a')
+FROM src;
+
+SELECT
+  sum('a'),
+  avg('a'),
+  variance('a'),
+  std('a')
+FROM src;
diff --git a/ql/src/test/results/clientpositive/udaf_number_format.q.out b/ql/src/test/results/clientpositive/udaf_number_format.q.out
new file mode 100644
index 0000000000..3441b0dacd
--- /dev/null
+++ b/ql/src/test/results/clientpositive/udaf_number_format.q.out
@@ -0,0 +1,100 @@
+PREHOOK: query: EXPLAIN SELECT
+  sum('a'),
+  avg('a'),
+  variance('a'),
+  std('a')
+FROM src
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN SELECT
+  sum('a'),
+  avg('a'),
+  variance('a'),
+  std('a')
+FROM src
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION sum 'a')) (TOK_SELEXPR (TOK_FUNCTION avg 'a')) (TOK_SELEXPR (TOK_FUNCTION variance 'a')) (TOK_SELEXPR (TOK_FUNCTION std 'a')))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Select Operator
+              Group By Operator
+                aggregations:
+                      expr: sum('a')
+                      expr: avg('a')
+                      expr: variance('a')
+                      expr: std('a')
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Reduce Output Operator
+                  sort order: 
+                  tag: -1
+                  value expressions:
+                        expr: _col0
+                        type: double
+                        expr: _col1
+                        type: struct<count:bigint,sum:double>
+                        expr: _col2
+                        type: struct<count:bigint,sum:double,variance:double>
+                        expr: _col3
+                        type: struct<count:bigint,sum:double,variance:double>
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: sum(VALUE._col0)
+                expr: avg(VALUE._col1)
+                expr: variance(VALUE._col2)
+                expr: std(VALUE._col3)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2, _col3
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: double
+                  expr: _col1
+                  type: double
+                  expr: _col2
+                  type: double
+                  expr: _col3
+                  type: double
+            outputColumnNames: _col0, _col1, _col2, _col3
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: SELECT
+  sum('a'),
+  avg('a'),
+  variance('a'),
+  std('a')
+FROM src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/tmp/865310068/10000
+POSTHOOK: query: SELECT
+  sum('a'),
+  avg('a'),
+  variance('a'),
+  std('a')
+FROM src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/tmp/865310068/10000
+0.0	NULL	NULL	NULL
