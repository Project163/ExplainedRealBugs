diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
index 2f79828f41..9ef9ca4867 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.llap.io.api.impl;
 
+import java.util.ArrayList;
+
 import java.io.IOException;
 import java.util.LinkedList;
 import java.util.List;
@@ -98,7 +100,6 @@ public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols
     this.executor = executor;
     this.jobConf = job;
     this.split = split;
-    this.columnIds = includedCols;
     this.sarg = ConvertAstToSearchArg.createFromConf(job);
     this.columnNames = ColumnProjectionUtils.getReadColumnNames(job);
     final String fragmentId = LlapTezUtils.getFragmentId(job);
@@ -122,12 +123,13 @@ public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols
     rbCtx = ctx != null ? ctx : LlapInputFormat.createFakeVrbCtx(mapWork);
     if (includedCols == null) {
       // Assume including everything means the VRB will have everything.
-      this.columnCount = rbCtx.getRowColumnTypeInfos().length;
-    } else {
-      this.columnCount = columnIds.size();
+      includedCols = new ArrayList<>(rbCtx.getRowColumnTypeInfos().length);
+      for (int i = 0; i < rbCtx.getRowColumnTypeInfos().length; ++i) {
+        includedCols.add(i);
+      }
     }
-
-
+    this.columnIds = includedCols;
+    this.columnCount = columnIds.size();
 
     int partitionColumnCount = rbCtx.getPartitionColumnCount();
     if (partitionColumnCount > 0) {
@@ -169,8 +171,7 @@ private boolean checkOrcSchemaEvolution() {
     SchemaEvolution schemaEvolution = new SchemaEvolution(
         fileSchema, rp.getReaderSchema(), includedColumns);
     for (int i = 0; i < columnCount; ++i) {
-      int colId = columnIds == null ? i : columnIds.get(i);
-      if (!schemaEvolution.isPPDSafeConversion(colId)) {
+      if (!schemaEvolution.isPPDSafeConversion(columnIds.get(i))) {
         LlapIoImpl.LOG.warn("Unsupported schema evolution! Disabling Llap IO for {}", split);
         return false;
       }
@@ -214,7 +215,7 @@ public boolean next(NullWritable key, VectorizedRowBatch value) throws IOExcepti
     // VRB was created from VrbCtx, so we already have pre-allocated column vectors
     for (int i = 0; i < cvb.cols.length; ++i) {
       // Return old CVs (if any) to caller. We assume these things all have the same schema.
-      cvb.swapColumnVector(i, value.cols, columnIds == null ? i : columnIds.get(i));
+      cvb.swapColumnVector(i, value.cols, columnIds.get(i));
     }
     value.selectedInUse = false;
     value.size = cvb.size;
