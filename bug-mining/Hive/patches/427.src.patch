diff --git a/CHANGES.txt b/CHANGES.txt
index 5749a4edae..ca30773b83 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -79,6 +79,10 @@ Trunk -  Unreleased
     HIVE-1464. improve  test query performance
     (Joydeep Sen Sarma via Ning Zhang)
 
+    HIVE-1475. .gitignore files being placed in test warehouse directories 
+    causing build failure
+    (Joydeep Sen Sarma via Ning Zhang)
+
 Release 0.6.0 -  Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/build-common.xml b/build-common.xml
index fcb9d33f24..4dd7d1dcc2 100644
--- a/build-common.xml
+++ b/build-common.xml
@@ -58,7 +58,7 @@
   <path id="test.classpath">
     <pathelement location="${test.build.classes}" />
     <pathelement location="" />
-    <pathelement location="${test.data.dir}/conf"/>
+    <pathelement location="${test.src.data.dir}/conf"/>
     <pathelement location="${hive.conf.dir}"/>
     <fileset dir="${hive.root}" includes="testlibs/*.jar"/>
     <path refid="classpath"/>
@@ -254,11 +254,8 @@
     <mkdir dir="${test.log.dir}/clientnegative"/>
     <mkdir dir="${test.log.dir}/positive"/>
     <mkdir dir="${test.log.dir}/negative"/>
-    <copy todir="${test.data.dir}">
-      <fileset dir="${test.src.data.dir}">
-        <exclude name="**/.svn"/>
-      </fileset>
-    </copy>
+    <mkdir dir="${test.data.dir}/warehouse"/>
+    <mkdir dir="${test.data.dir}/metadb"/>
   </target>
 
   <target name="setup"/>
@@ -407,11 +404,11 @@
       <jvmarg value="-Xdebug"/>
       <jvmarg value="-Xrunjdwp:transport=dt_socket,address=8000,server=y,suspend=y"/> -->
       <env key="HADOOP_HOME" value="${hadoop.root}"/>
-      <env key="HADOOP_CLASSPATH" value="${test.data.dir}/conf"/>
+      <env key="HADOOP_CLASSPATH" value="${test.src.data.dir}/conf"/>
       <env key="TZ" value="US/Pacific"/>
       <sysproperty key="test.output.overwrite" value="${overwrite}"/>
       <sysproperty key="test.service.standalone.server" value="${standalone}"/>
-      <sysproperty key="log4j.configuration" value="file://${test.data.dir}/conf/hive-log4j.properties"/>
+      <sysproperty key="log4j.configuration" value="file://${test.src.data.dir}/conf/hive-log4j.properties"/>
       <sysproperty key="derby.stream.error.file" value="${test.build.dir}/derby.log"/>
       <sysproperty key="hive.aux.jars.path" value="file://${test.build.dir}/test-udfs.jar"/>
       <sysproperty key="ql.test.query.clientpositive.dir" value="${ql.test.query.clientpositive.dir}"/>
diff --git a/contrib/build.xml b/contrib/build.xml
index cc8ad0ebae..1353b52ad1 100644
--- a/contrib/build.xml
+++ b/contrib/build.xml
@@ -33,7 +33,7 @@
     <pathelement location="${test.build.classes}" />
     <pathelement location="" />
     <pathelement location="${hadoop.conf.dir}"/>
-    <pathelement location="${test.data.dir}/conf"/>
+    <pathelement location="${test.src.data.dir}/conf"/>
     <pathelement location="${hive.conf.dir}"/>
     <pathelement location="${hive.root}/cli/lib/jline-0.9.94.jar"/>
     <pathelement location="${hadoop.test.jar}"/>
diff --git a/data/warehouse/src/.gitignore b/data/warehouse/src/.gitignore
index 0dd9890504..e69de29bb2 100644
--- a/data/warehouse/src/.gitignore
+++ b/data/warehouse/src/.gitignore
@@ -1 +0,0 @@
-# Dummy file to make Git recognize this empty directory
diff --git a/hbase-handler/build.xml b/hbase-handler/build.xml
index bbd2d2ffcb..c6ce81f9ea 100644
--- a/hbase-handler/build.xml
+++ b/hbase-handler/build.xml
@@ -34,7 +34,7 @@
     <pathelement location="${test.build.classes}" />
     <pathelement location="" />
     <pathelement location="${hadoop.conf.dir}"/>
-    <pathelement location="${test.data.dir}/conf"/>
+    <pathelement location="${test.src.data.dir}/conf"/>
     <pathelement location="${hive.conf.dir}"/>
     <pathelement location="${hive.root}/cli/lib/jline-0.9.94.jar"/>
     <pathelement location="${hadoop.test.jar}"/>
diff --git a/hwi/build.xml b/hwi/build.xml
index 9c6d7e6b09..486f7bcea6 100644
--- a/hwi/build.xml
+++ b/hwi/build.xml
@@ -34,7 +34,7 @@
   <path id="test.classpath">
     <pathelement location="${test.build.classes}" />
     <pathelement location="" />
-    <pathelement location="${test.data.dir}/conf"/>
+    <pathelement location="${test.src.data.dir}/conf"/>
     <pathelement location="${hive.conf.dir}"/>
     <!-- We are running unit tests like the one inside ql -->
     <pathelement location="${build.dir.hive}/ql/test/classes"/> 
diff --git a/jdbc/build.xml b/jdbc/build.xml
index 3ec3d90eb3..9d9a59eb76 100644
--- a/jdbc/build.xml
+++ b/jdbc/build.xml
@@ -32,7 +32,7 @@
   <path id="test.classpath">
     <pathelement location="${test.build.classes}" />
     <pathelement location="" />
-    <pathelement location="${test.data.dir}/conf"/>
+    <pathelement location="${test.src.data.dir}/conf"/>
     <pathelement location="${hive.conf.dir}"/>
     <fileset dir="${test.src.data.dir}" includes="files/*.jar"/>
     <fileset dir="${hive.root}" includes="testlibs/*.jar"/>
diff --git a/odbc/build.xml b/odbc/build.xml
index ef3e2024e8..64cf988170 100644
--- a/odbc/build.xml
+++ b/odbc/build.xml
@@ -32,7 +32,7 @@
   <path id="test.classpath">
     <pathelement location="${test.build.classes}" />
     <pathelement location="" />
-    <pathelement location="${test.data.dir}/conf"/>
+    <pathelement location="${test.src.data.dir}/conf"/>
     <pathelement location="${hive.conf.dir}"/>
     <fileset dir="${test.src.data.dir}" includes="files/*.jar"/>
     <fileset dir="${hive.root}" includes="testlibs/*.jar"/>
diff --git a/ql/build.xml b/ql/build.xml
index 71507800f9..025dd407de 100644
--- a/ql/build.xml
+++ b/ql/build.xml
@@ -36,7 +36,7 @@
     <pathelement location="${test.build.classes}" />
     <pathelement location="" />
     <pathelement location="${hadoop.conf.dir}"/>
-    <pathelement location="${test.data.dir}/conf"/>
+    <pathelement location="${test.src.data.dir}/conf"/>
     <pathelement location="${hive.conf.dir}"/>
     <pathelement location="${hive.root}/cli/lib/jline-0.9.94.jar"/>
     <pathelement location="${hadoop.test.jar}"/>
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
index 5ba1576610..d840c16596 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
@@ -528,7 +528,7 @@ public void cliInit(String tname, boolean recreate) throws Exception {
       createSources();
     }
 
-    CliSessionState ss = new CliSessionState(new HiveConf(Driver.class));
+    CliSessionState ss = new CliSessionState(conf);
     assert ss != null;
     ss.in = System.in;
 
diff --git a/ql/src/test/queries/clientnegative/addpart1.q b/ql/src/test/queries/clientnegative/addpart1.q
index 68988fba73..a7c9fe91f6 100644
--- a/ql/src/test/queries/clientnegative/addpart1.q
+++ b/ql/src/test/queries/clientnegative/addpart1.q
@@ -1,4 +1,4 @@
-drop table addpart1;
+
 create table addpart1 (a int) partitioned by (b string, c string);
 
 alter table addpart1 add partition (b='f', c='s');
@@ -9,4 +9,3 @@ alter table addpart1 add partition (b='f', c='');
 
 show prtitions addpart1;
 
-drop table addpart1;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientnegative/alter_non_native.q b/ql/src/test/queries/clientnegative/alter_non_native.q
index e021aba253..73ae853778 100644
--- a/ql/src/test/queries/clientnegative/alter_non_native.q
+++ b/ql/src/test/queries/clientnegative/alter_non_native.q
@@ -1,4 +1,4 @@
-DROP TABLE non_native1;
+
 CREATE TABLE non_native1(key int, value string) 
 STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler';
 
diff --git a/ql/src/test/queries/clientnegative/altern1.q b/ql/src/test/queries/clientnegative/altern1.q
index 59387651ae..60414c1f3a 100644
--- a/ql/src/test/queries/clientnegative/altern1.q
+++ b/ql/src/test/queries/clientnegative/altern1.q
@@ -1,4 +1,4 @@
-drop table altern1;
+
 create table altern1(a int, b int) partitioned by (ds string);
 alter table altern1 replace columns(a int, b int, ds string);
-drop table altern1;
+
diff --git a/ql/src/test/queries/clientnegative/create_insert_outputformat.q b/ql/src/test/queries/clientnegative/create_insert_outputformat.q
index e770135e6f..a052663055 100644
--- a/ql/src/test/queries/clientnegative/create_insert_outputformat.q
+++ b/ql/src/test/queries/clientnegative/create_insert_outputformat.q
@@ -1,4 +1,4 @@
-DROP TABLE table_test_output_fomat;
+
 
 CREATE TABLE table_test_output_format(key INT, value STRING) STORED AS
   INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
@@ -8,4 +8,4 @@ FROM src
 INSERT OVERWRITE TABLE table_test_output_format SELECT src.key, src.value LIMIT 10;
 
 describe table_test_output_format;
-DROP TABLE table_test_output_format;
+
diff --git a/ql/src/test/queries/clientnegative/create_view_failure1.q b/ql/src/test/queries/clientnegative/create_view_failure1.q
index a7a1370f97..c9060c6766 100644
--- a/ql/src/test/queries/clientnegative/create_view_failure1.q
+++ b/ql/src/test/queries/clientnegative/create_view_failure1.q
@@ -1,4 +1,4 @@
-DROP TABLE xxx12;
+
 DROP VIEW xxx12;
 
 -- views and tables share the same namespace
diff --git a/ql/src/test/queries/clientnegative/create_view_failure2.q b/ql/src/test/queries/clientnegative/create_view_failure2.q
index 4cbe2c9a2d..6fdcd4a9d3 100644
--- a/ql/src/test/queries/clientnegative/create_view_failure2.q
+++ b/ql/src/test/queries/clientnegative/create_view_failure2.q
@@ -1,4 +1,4 @@
-DROP TABLE xxx4;
+
 DROP VIEW xxx4;
 
 -- views and tables share the same namespace
diff --git a/ql/src/test/queries/clientnegative/ctas.q b/ql/src/test/queries/clientnegative/ctas.q
index 8685b1abfc..507a7a76b1 100644
--- a/ql/src/test/queries/clientnegative/ctas.q
+++ b/ql/src/test/queries/clientnegative/ctas.q
@@ -1,5 +1,5 @@
-drop table nzhang_ctas4;
+
 
 create external table nzhang_ctas4 as select key, value from src;
 
-drop table nzhang_ctas4;
+
diff --git a/ql/src/test/queries/clientnegative/ddltime.q b/ql/src/test/queries/clientnegative/ddltime.q
index 9c9dbcc19b..3517a6046d 100644
--- a/ql/src/test/queries/clientnegative/ddltime.q
+++ b/ql/src/test/queries/clientnegative/ddltime.q
@@ -1,6 +1,6 @@
-drop table T2;
+
 create table T2 like srcpart;
 
 insert overwrite table T2 partition (ds = '2010-06-21', hr='1') select /*+ HOLD_DDLTIME */ key, value from src where key > 10;
 
-drop table T2;
+
diff --git a/ql/src/test/queries/clientnegative/deletejar.q b/ql/src/test/queries/clientnegative/deletejar.q
index 6a43170447..7b0c92311a 100644
--- a/ql/src/test/queries/clientnegative/deletejar.q
+++ b/ql/src/test/queries/clientnegative/deletejar.q
@@ -1,4 +1,4 @@
-DROP TABLE DELETEJAR;
+
 ADD JAR ../data/files/TestSerDe.jar;
 DELETE JAR ../data/files/TestSerDe.jar;
 CREATE TABLE DELETEJAR(KEY STRING, VALUE STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.TestSerDe' STORED AS TEXTFILE;
diff --git a/ql/src/test/queries/clientnegative/drop_view_failure1.q b/ql/src/test/queries/clientnegative/drop_view_failure1.q
index 52322364cb..79cb4e445b 100644
--- a/ql/src/test/queries/clientnegative/drop_view_failure1.q
+++ b/ql/src/test/queries/clientnegative/drop_view_failure1.q
@@ -1,4 +1,4 @@
-DROP TABLE xxx1;
+
 
 CREATE TABLE xxx1(key int);
 
diff --git a/ql/src/test/queries/clientnegative/dyn_part1.q b/ql/src/test/queries/clientnegative/dyn_part1.q
index a8b3be0572..9f0b6c7a0c 100644
--- a/ql/src/test/queries/clientnegative/dyn_part1.q
+++ b/ql/src/test/queries/clientnegative/dyn_part1.q
@@ -2,10 +2,10 @@ set hive.exec.dynamic.partition=true;
 set hive.exec.dynamic.partition.mode=nostrict;
 set hive.exec.max.dynamic.partitions=2;
 
-drop table dynamic_partition;
+
 create table dynamic_partition (key string) partitioned by (value string);
 
 insert overwrite table dynamic_partition partition(hr) select key, value from src;
 
-drop table dynamic_partition;
+
 
diff --git a/ql/src/test/queries/clientnegative/dyn_part2.q b/ql/src/test/queries/clientnegative/dyn_part2.q
index 614585b7f3..00a92783c0 100644
--- a/ql/src/test/queries/clientnegative/dyn_part2.q
+++ b/ql/src/test/queries/clientnegative/dyn_part2.q
@@ -1,4 +1,4 @@
-drop table nzhang_part1;
+
 create table nzhang_part1 (key string, value string) partitioned by (ds string, hr string);
 
 set hive.exec.dynamic.partition=true;
@@ -7,5 +7,5 @@ insert overwrite table nzhang_part1 partition(ds='11', hr) select key, value fro
 
 show partitions nzhang_part1;
 
-drop table nzhang_part1;
+
 
diff --git a/ql/src/test/queries/clientnegative/external1.q b/ql/src/test/queries/clientnegative/external1.q
index 4157732cae..38be98c7bf 100644
--- a/ql/src/test/queries/clientnegative/external1.q
+++ b/ql/src/test/queries/clientnegative/external1.q
@@ -1,4 +1,4 @@
 set hive.cli.errors.ignore=true;
-drop table external1;
+
 create external table external1(a int, b int) location 'invalidscheme://data.s3ndemo.hive/kv';
 describe external1;
diff --git a/ql/src/test/queries/clientnegative/external2.q b/ql/src/test/queries/clientnegative/external2.q
index 577a9e736d..afc7d9cb8c 100644
--- a/ql/src/test/queries/clientnegative/external2.q
+++ b/ql/src/test/queries/clientnegative/external2.q
@@ -1,5 +1,5 @@
 set hive.cli.errors.ignore=true;
-drop table external2;
+
 create external table external2(a int, b int) partitioned by (ds string);
 alter table external2 add partition (ds='2008-01-01') location 'invalidscheme://data.s3ndemo.hive/pkv/2008-01-01';
 describe external2 partition (ds='2008-01-01');
diff --git a/ql/src/test/queries/clientnegative/invalid_create_tbl1.q b/ql/src/test/queries/clientnegative/invalid_create_tbl1.q
index dc9a938b9e..2e1ea6b005 100644
--- a/ql/src/test/queries/clientnegative/invalid_create_tbl1.q
+++ b/ql/src/test/queries/clientnegative/invalid_create_tbl1.q
@@ -1,4 +1,4 @@
-DROP TABLE inv_valid_tbl1;
+
 CREATE TABLE inv_valid_tbl1 COMMENT 'This is a thrift based table'
     PARTITIONED BY(aint DATETIME, country STRING)
     CLUSTERED BY(aint) SORTED BY(lint) INTO 32 BUCKETS
diff --git a/ql/src/test/queries/clientnegative/invalidate_view1.q b/ql/src/test/queries/clientnegative/invalidate_view1.q
index 58ca0d7b2f..dd39c5eb4a 100644
--- a/ql/src/test/queries/clientnegative/invalidate_view1.q
+++ b/ql/src/test/queries/clientnegative/invalidate_view1.q
@@ -1,6 +1,6 @@
 DROP VIEW xxx8;
 DROP VIEW xxx9;
-DROP TABLE xxx10;
+
 -- create two levels of view reference, then invalidate intermediate view
 -- by dropping a column from underlying table, and verify that
 -- querying outermost view results in full error context
diff --git a/ql/src/test/queries/clientnegative/load_non_native.q b/ql/src/test/queries/clientnegative/load_non_native.q
index 83e7e0e294..387aaed9a1 100644
--- a/ql/src/test/queries/clientnegative/load_non_native.q
+++ b/ql/src/test/queries/clientnegative/load_non_native.q
@@ -1,4 +1,4 @@
-DROP TABLE non_native2;
+
 CREATE TABLE non_native2(key int, value string) 
 STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler';
 
diff --git a/ql/src/test/queries/clientnegative/load_wrong_fileformat.q b/ql/src/test/queries/clientnegative/load_wrong_fileformat.q
index 1c17d5a9ff..16feeca226 100644
--- a/ql/src/test/queries/clientnegative/load_wrong_fileformat.q
+++ b/ql/src/test/queries/clientnegative/load_wrong_fileformat.q
@@ -1,6 +1,6 @@
 -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE load_wrong_fileformat_T1;
+
 CREATE TABLE load_wrong_fileformat_T1(name STRING) STORED AS SEQUENCEFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE load_wrong_fileformat_T1;
diff --git a/ql/src/test/queries/clientnegative/load_wrong_fileformat_rc_seq.q b/ql/src/test/queries/clientnegative/load_wrong_fileformat_rc_seq.q
index e4af72e3c5..7e589fbfde 100644
--- a/ql/src/test/queries/clientnegative/load_wrong_fileformat_rc_seq.q
+++ b/ql/src/test/queries/clientnegative/load_wrong_fileformat_rc_seq.q
@@ -1,6 +1,6 @@
 -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE T1;
+
 CREATE TABLE T1(name STRING) STORED AS RCFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.seq' INTO TABLE T1;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientnegative/load_wrong_fileformat_txt_seq.q b/ql/src/test/queries/clientnegative/load_wrong_fileformat_txt_seq.q
index 90d118a5f1..ff5ed4e2e3 100644
--- a/ql/src/test/queries/clientnegative/load_wrong_fileformat_txt_seq.q
+++ b/ql/src/test/queries/clientnegative/load_wrong_fileformat_txt_seq.q
@@ -1,6 +1,6 @@
 -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE T1;
+
 CREATE TABLE T1(name STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.seq' INTO TABLE T1;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientnegative/nopart_insert.q b/ql/src/test/queries/clientnegative/nopart_insert.q
index a8b3e0c8db..4841f9e11c 100644
--- a/ql/src/test/queries/clientnegative/nopart_insert.q
+++ b/ql/src/test/queries/clientnegative/nopart_insert.q
@@ -1,8 +1,7 @@
-DROP TABLE nopart_insert;
+
 CREATE TABLE nopart_insert(a STRING, b STRING) PARTITIONED BY (ds STRING);
 
 INSERT OVERWRITE TABLE nopart_insert 
 SELECT TRANSFORM(src.key, src.value) USING '../data/scripts/error_script' AS (tkey, tvalue)
 FROM src;
 
-DROP TABLE nopart_insert;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientnegative/nopart_load.q b/ql/src/test/queries/clientnegative/nopart_load.q
index 89536277e7..6e5ad6eb41 100644
--- a/ql/src/test/queries/clientnegative/nopart_load.q
+++ b/ql/src/test/queries/clientnegative/nopart_load.q
@@ -1,6 +1,5 @@
-DROP TABLE nopart_load;
+
 CREATE TABLE nopart_load(a STRING, b STRING) PARTITIONED BY (ds STRING);
 
 load data local inpath '../data/files/kv1.txt' overwrite into table nopart_load ;
 
-DROP TABLE nopart_load;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientnegative/smb_bucketmapjoin.q b/ql/src/test/queries/clientnegative/smb_bucketmapjoin.q
index 146ae39925..880323c604 100644
--- a/ql/src/test/queries/clientnegative/smb_bucketmapjoin.q
+++ b/ql/src/test/queries/clientnegative/smb_bucketmapjoin.q
@@ -2,10 +2,10 @@ set hive.enforce.bucketing = true;
 set hive.enforce.sorting = true;
 set hive.exec.reducers.max = 1;
 
-drop table smb_bucket4_1;
+
 CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS;
 
-drop table smb_bucket4_2;
+
 CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS;
 
 insert overwrite table smb_bucket4_1
@@ -19,5 +19,5 @@ set hive.optimize.bucketmapjoin.sortedmerge = true;
 
 select /*+mapjoin(a)*/ * from smb_bucket4_1 a left outer join smb_bucket4_2 b on a.key = b.key;
 
-drop table smb_bucket4_1;
-drop table smb_bucket4_2;
+
+
diff --git a/ql/src/test/queries/clientnegative/union2.q b/ql/src/test/queries/clientnegative/union2.q
index d598233cec..403d19d3fc 100644
--- a/ql/src/test/queries/clientnegative/union2.q
+++ b/ql/src/test/queries/clientnegative/union2.q
@@ -1,5 +1,5 @@
-drop table union2_t1;
-drop table union2_t2;
+
+
 create table if not exists union2_t1(r string, c string, v string);
 create table if not exists union2_t2(s string, c string, v string);
 
diff --git a/ql/src/test/queries/clientpositive/add_part_exist.q b/ql/src/test/queries/clientpositive/add_part_exist.q
index 3c8120d2d4..54d009659b 100644
--- a/ql/src/test/queries/clientpositive/add_part_exist.q
+++ b/ql/src/test/queries/clientpositive/add_part_exist.q
@@ -12,5 +12,3 @@ SHOW PARTITIONS add_part_test;
 
 ALTER TABLE add_part_test ADD IF NOT EXISTS PARTITION (ds='2010-01-01') PARTITION (ds='2010-01-02') PARTITION (ds='2010-01-03');
 SHOW PARTITIONS add_part_test;
-
-DROP TABLE add_part_test;
diff --git a/ql/src/test/queries/clientpositive/alter1.q b/ql/src/test/queries/clientpositive/alter1.q
index 485dec634e..6f956854c4 100644
--- a/ql/src/test/queries/clientpositive/alter1.q
+++ b/ql/src/test/queries/clientpositive/alter1.q
@@ -1,4 +1,3 @@
-drop table alter1;
 create table alter1(a int, b int);
 describe extended alter1;
 alter table alter1 set tblproperties ('a'='1', 'c'='3');
@@ -25,5 +24,3 @@ describe extended alter1;
 
 alter table alter1 replace columns (a int, b int, c string);
 describe alter1;
-
-drop table alter1;
diff --git a/ql/src/test/queries/clientpositive/alter2.q b/ql/src/test/queries/clientpositive/alter2.q
index 8568970ce4..2388e21108 100644
--- a/ql/src/test/queries/clientpositive/alter2.q
+++ b/ql/src/test/queries/clientpositive/alter2.q
@@ -1,4 +1,3 @@
-drop table alter2;
 create table alter2(a int, b int) partitioned by (insertdate string);
 describe extended alter2;
 show partitions alter2;
@@ -19,4 +18,3 @@ show partitions alter2;
 alter table alter2 add partition (insertdate='2008-01-02') location '2008/01/02';
 describe extended alter2;
 show partitions alter2;
-drop table alter2;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/alter3.q b/ql/src/test/queries/clientpositive/alter3.q
index bc97f21e15..d9d3b07b28 100644
--- a/ql/src/test/queries/clientpositive/alter3.q
+++ b/ql/src/test/queries/clientpositive/alter3.q
@@ -1,8 +1,3 @@
-drop table alter3_src;
-drop table alter3;
-drop table alter3_renamed;
-drop table alter3_like_renamed;
-
 create table alter3_src ( col1 string ) stored as textfile ;
 load data local inpath '../data/files/test.dat' overwrite into table alter3_src ;
 
@@ -24,8 +19,3 @@ select col1 from alter3_src;
 alter table alter3_like rename to alter3_like_renamed;
 
 describe extended alter3_like_renamed;
-
-drop table alter3_src;
-drop table alter3;
-drop table alter3_renamed;
-drop table alter3_like_renamed;
diff --git a/ql/src/test/queries/clientpositive/alter4.q b/ql/src/test/queries/clientpositive/alter4.q
index 7687e89a7d..35fa441804 100644
--- a/ql/src/test/queries/clientpositive/alter4.q
+++ b/ql/src/test/queries/clientpositive/alter4.q
@@ -1,9 +1,5 @@
-DROP TABLE set_bucketing_test;
-
 CREATE TABLE set_bucketing_test (key INT, value STRING) CLUSTERED BY (key) INTO 10 BUCKETS;
 DESCRIBE EXTENDED set_bucketing_test;
 
 ALTER TABLE set_bucketing_test NOT CLUSTERED;
 DESCRIBE EXTENDED set_bucketing_test;
-
-DROP TABLE set_bucketing_test;
diff --git a/ql/src/test/queries/clientpositive/archive.q b/ql/src/test/queries/clientpositive/archive.q
index 5553e3e428..9b036f0422 100644
--- a/ql/src/test/queries/clientpositive/archive.q
+++ b/ql/src/test/queries/clientpositive/archive.q
@@ -33,7 +33,6 @@ SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key;
 ALTER TABLE srcpart UNARCHIVE PARTITION (ds='2008-04-08', hr='12');
 SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key;
 
-DROP TABLE harbucket;
 
 CREATE TABLE old_name(key INT) 
 PARTITIONED by (ds STRING);
@@ -45,5 +44,3 @@ FROM (SELECT * FROM old_name WHERE ds='1') subq1) subq2;
 ALTER TABLE old_name RENAME TO new_name;
 SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col 
 FROM (SELECT * FROM new_name WHERE ds='1') subq1) subq2;
-
-DROP TABLE new_name;
diff --git a/ql/src/test/queries/clientpositive/binary_output_format.q b/ql/src/test/queries/clientpositive/binary_output_format.q
index c6996bdce0..c5d9abd062 100644
--- a/ql/src/test/queries/clientpositive/binary_output_format.q
+++ b/ql/src/test/queries/clientpositive/binary_output_format.q
@@ -1,5 +1,3 @@
-DROP TABLE dest1;
-
 -- Create a table with binary output format
 CREATE TABLE dest1(mydata STRING)
 ROW FORMAT SERDE
@@ -39,5 +37,3 @@ FROM src;
 
 -- Test the result
 SELECT * FROM dest1;
-
-DROP TABLE dest1;
diff --git a/ql/src/test/queries/clientpositive/binarysortable_1.q b/ql/src/test/queries/clientpositive/binarysortable_1.q
index ee44d57f9f..a98a2305cf 100644
--- a/ql/src/test/queries/clientpositive/binarysortable_1.q
+++ b/ql/src/test/queries/clientpositive/binarysortable_1.q
@@ -19,6 +19,3 @@ FROM (
         FROM mytable
         GROUP BY key
 ) a;
-
-
-DROP TABLE mytable;
diff --git a/ql/src/test/queries/clientpositive/bucket1.q b/ql/src/test/queries/clientpositive/bucket1.q
index ae2c4d6500..ca9b9025e2 100644
--- a/ql/src/test/queries/clientpositive/bucket1.q
+++ b/ql/src/test/queries/clientpositive/bucket1.q
@@ -1,7 +1,6 @@
 set hive.enforce.bucketing = true;
 set hive.exec.reducers.max = 200;
 
-drop table bucket1_1;
 CREATE TABLE bucket1_1(key int, value string) CLUSTERED BY (key) INTO 100 BUCKETS;
 
 explain extended
@@ -12,5 +11,3 @@ insert overwrite table bucket1_1
 select * from src;
 
 select * from bucket1_1 order by key;
-
-drop table bucket1_1;
diff --git a/ql/src/test/queries/clientpositive/bucket2.q b/ql/src/test/queries/clientpositive/bucket2.q
index 150e370fd7..d4bddf72ed 100644
--- a/ql/src/test/queries/clientpositive/bucket2.q
+++ b/ql/src/test/queries/clientpositive/bucket2.q
@@ -1,7 +1,6 @@
 set hive.enforce.bucketing = true;
 set hive.exec.reducers.max = 1;
 
-drop table bucket2_1;
 CREATE TABLE bucket2_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS;
 
 explain extended
@@ -15,5 +14,3 @@ explain
 select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key;
 
 select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key;
-
-drop table bucket2_1;
diff --git a/ql/src/test/queries/clientpositive/bucket3.q b/ql/src/test/queries/clientpositive/bucket3.q
index df566444b8..d891b90b7b 100644
--- a/ql/src/test/queries/clientpositive/bucket3.q
+++ b/ql/src/test/queries/clientpositive/bucket3.q
@@ -1,7 +1,6 @@
 set hive.enforce.bucketing = true;
 set hive.exec.reducers.max = 1;
 
-drop table bucket3_1;
 CREATE TABLE bucket3_1(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS;
 
 explain extended
@@ -18,5 +17,3 @@ explain
 select * from bucket3_1 tablesample (bucket 1 out of 2) s where ds = '1' order by key;
 
 select * from bucket3_1 tablesample (bucket 1 out of 2) s where ds = '1' order by key;
-
-drop table bucket3_1;
diff --git a/ql/src/test/queries/clientpositive/bucket4.q b/ql/src/test/queries/clientpositive/bucket4.q
index ad08a578e1..bf06d21c21 100644
--- a/ql/src/test/queries/clientpositive/bucket4.q
+++ b/ql/src/test/queries/clientpositive/bucket4.q
@@ -2,7 +2,6 @@ set hive.enforce.bucketing = true;
 set hive.enforce.sorting = true;
 set hive.exec.reducers.max = 1;
 
-drop table bucket4_1;
 CREATE TABLE bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
 
 explain extended
@@ -16,5 +15,3 @@ explain
 select * from bucket4_1 tablesample (bucket 1 out of 2) s;
 
 select * from bucket4_1 tablesample (bucket 1 out of 2) s;
-
-drop table bucket4_1;
diff --git a/ql/src/test/queries/clientpositive/bucket_groupby.q b/ql/src/test/queries/clientpositive/bucket_groupby.q
index afc8187ad6..ddce79700b 100644
--- a/ql/src/test/queries/clientpositive/bucket_groupby.q
+++ b/ql/src/test/queries/clientpositive/bucket_groupby.q
@@ -1,4 +1,3 @@
-drop table clustergroupby;
 create table clustergroupby(key string, value string) partitioned by(ds string);
 describe extended clustergroupby;
 
@@ -67,5 +66,3 @@ select key, count(1) from clustergroupby  where ds='103' group by key limit 10;
 explain
 select key, count(1) from clustergroupby  where ds='103'  group by value, key limit 10;
 select key, count(1) from clustergroupby  where ds='103' group by  value, key limit 10;
-
-drop table clustergroupby;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/bucketizedhiveinputformat.q b/ql/src/test/queries/clientpositive/bucketizedhiveinputformat.q
index 45fb90690d..9d336f2f85 100644
--- a/ql/src/test/queries/clientpositive/bucketizedhiveinputformat.q
+++ b/ql/src/test/queries/clientpositive/bucketizedhiveinputformat.q
@@ -2,12 +2,10 @@ set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
 set mapred.min.split.size = 64;
 set dfs.block.size=64; 
 
-DROP TABLE T1;
 CREATE TABLE T1(name STRING) STORED AS TEXTFILE;
 
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE T1;
 
-DROP TABLE T2;
 CREATE TABLE T2(name STRING) STORED AS SEQUENCEFILE;
 
 EXPLAIN INSERT OVERWRITE TABLE T2 SELECT * FROM (
@@ -28,14 +26,9 @@ SELECT tmp1.name as name FROM (
 EXPLAIN SELECT COUNT(1) FROM T2;
 SELECT COUNT(1) FROM T2;
 
-DROP TABLE T3;
 CREATE TABLE T3(name STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE T3;
 LOAD DATA LOCAL INPATH '../data/files/kv2.txt' INTO TABLE T3;
 
 EXPLAIN SELECT COUNT(1) FROM T3;
 SELECT COUNT(1) FROM T3;
-
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
diff --git a/ql/src/test/queries/clientpositive/bucketmapjoin1.q b/ql/src/test/queries/clientpositive/bucketmapjoin1.q
index 846778b96e..a906fff194 100644
--- a/ql/src/test/queries/clientpositive/bucketmapjoin1.q
+++ b/ql/src/test/queries/clientpositive/bucketmapjoin1.q
@@ -1,10 +1,3 @@
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
-
 CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
 load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
 load data local inpath '../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;
@@ -88,11 +81,3 @@ select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_t
 select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key;
-
-
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
diff --git a/ql/src/test/queries/clientpositive/bucketmapjoin2.q b/ql/src/test/queries/clientpositive/bucketmapjoin2.q
index cdeaa97c5a..f0dbfa818b 100644
--- a/ql/src/test/queries/clientpositive/bucketmapjoin2.q
+++ b/ql/src/test/queries/clientpositive/bucketmapjoin2.q
@@ -1,10 +1,3 @@
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
-
 CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
 load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
 load data local inpath '../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;
@@ -85,10 +78,3 @@ select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_t
 select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key;
-
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
diff --git a/ql/src/test/queries/clientpositive/bucketmapjoin3.q b/ql/src/test/queries/clientpositive/bucketmapjoin3.q
index 8c6b320c68..8fda802315 100644
--- a/ql/src/test/queries/clientpositive/bucketmapjoin3.q
+++ b/ql/src/test/queries/clientpositive/bucketmapjoin3.q
@@ -1,10 +1,3 @@
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
-
 CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
 load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
 load data local inpath '../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;
@@ -83,10 +76,3 @@ select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_t
 select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key;
-
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
diff --git a/ql/src/test/queries/clientpositive/bucketmapjoin4.q b/ql/src/test/queries/clientpositive/bucketmapjoin4.q
index 54fa877854..13ff50d52b 100644
--- a/ql/src/test/queries/clientpositive/bucketmapjoin4.q
+++ b/ql/src/test/queries/clientpositive/bucketmapjoin4.q
@@ -1,10 +1,3 @@
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
-
 CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
 load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
 load data local inpath '../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;
@@ -84,10 +77,3 @@ select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_t
 select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key;
-
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/bucketmapjoin5.q b/ql/src/test/queries/clientpositive/bucketmapjoin5.q
index 8f1cac86ed..2df49b64f3 100644
--- a/ql/src/test/queries/clientpositive/bucketmapjoin5.q
+++ b/ql/src/test/queries/clientpositive/bucketmapjoin5.q
@@ -1,10 +1,3 @@
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
-
 CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
 load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
 load data local inpath '../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;
@@ -90,10 +83,3 @@ select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_t
 select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key;
-
-drop table bucketmapjoin_hash_result_2;
-drop table bucketmapjoin_hash_result_1;
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
-drop table srcbucket_mapjoin_part_2;
diff --git a/ql/src/test/queries/clientpositive/bucketmapjoin6.q b/ql/src/test/queries/clientpositive/bucketmapjoin6.q
index 2ff07d2655..8138e5dbde 100644
--- a/ql/src/test/queries/clientpositive/bucketmapjoin6.q
+++ b/ql/src/test/queries/clientpositive/bucketmapjoin6.q
@@ -1,7 +1,5 @@
-drop table tmp1;
 create table tmp1 (a string, b string) clustered by (a) sorted by (a) into 10 buckets;
 
-drop table tmp2;
 create table tmp2 (a string, b string) clustered by (a) sorted by (a) into 10 buckets;
 
 
@@ -18,7 +16,6 @@ set hive.optimize.bucketmapjoin.sortedmerge = true;
 set hive.merge.mapfiles=false;
 set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
 
-drop table tmp3;
 create table tmp3 (a string, b string, c string) clustered by (a) sorted by (a) into 10 buckets;
 
 
@@ -27,7 +24,3 @@ insert overwrite table tmp3
   from tmp1 i join tmp2 l ON i.a = l.a;
 
 select * from tmp3 order by a, b, c;
-
-drop table tmp1;
-drop table tmp2;
-drop table tmp3;
diff --git a/ql/src/test/queries/clientpositive/bucketmapjoin_negative.q b/ql/src/test/queries/clientpositive/bucketmapjoin_negative.q
index aea907d22a..d7634333e2 100644
--- a/ql/src/test/queries/clientpositive/bucketmapjoin_negative.q
+++ b/ql/src/test/queries/clientpositive/bucketmapjoin_negative.q
@@ -1,6 +1,6 @@
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
+
+
+
 
 CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
 load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
@@ -22,6 +22,6 @@ select /*+mapjoin(b)*/ a.key, a.value, b.value
 from srcbucket_mapjoin a join srcbucket_mapjoin_part b 
 on a.key=b.key where b.ds="2008-04-08";
 
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/bucketmapjoin_negative2.q b/ql/src/test/queries/clientpositive/bucketmapjoin_negative2.q
index 7fd89a7762..901f056759 100644
--- a/ql/src/test/queries/clientpositive/bucketmapjoin_negative2.q
+++ b/ql/src/test/queries/clientpositive/bucketmapjoin_negative2.q
@@ -1,7 +1,3 @@
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part_2;
-
 CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
 load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
 load data local inpath '../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;
@@ -20,7 +16,3 @@ insert overwrite table bucketmapjoin_tmp_result
 select /*+mapjoin(b)*/ a.key, a.value, b.value 
 from srcbucket_mapjoin a join srcbucket_mapjoin_part_2 b 
 on a.key=b.key;
-
-drop table bucketmapjoin_tmp_result;
-drop table srcbucket_mapjoin;
-drop table srcbucket_mapjoin_part_2;
diff --git a/ql/src/test/queries/clientpositive/columnarserde_create_shortcut.q b/ql/src/test/queries/clientpositive/columnarserde_create_shortcut.q
index 7958184bcb..a4ab6649f2 100644
--- a/ql/src/test/queries/clientpositive/columnarserde_create_shortcut.q
+++ b/ql/src/test/queries/clientpositive/columnarserde_create_shortcut.q
@@ -1,4 +1,3 @@
-drop table columnarserde_create_shortcut;
 CREATE TABLE columnarserde_create_shortcut(a array<int>, b array<string>, c map<string,string>, d int, e string) STORED AS RCFILE;
 
 EXPLAIN
@@ -12,9 +11,6 @@ SELECT columnarserde_create_shortcut.* FROM columnarserde_create_shortcut DISTRI
 
 SELECT columnarserde_create_shortcut.a[0], columnarserde_create_shortcut.b[0], columnarserde_create_shortcut.c['key2'], columnarserde_create_shortcut.d, columnarserde_create_shortcut.e FROM columnarserde_create_shortcut DISTRIBUTE BY 1;
 
-drop table columnarserde_create_shortcut;
-
-DROP TABLE columnShortcutTable;
 CREATE table columnShortcutTable (key STRING, value STRING) STORED AS RCFILE;
 
 FROM src
@@ -26,5 +22,3 @@ ALTER TABLE columnShortcutTable ADD COLUMNS (c string);
 SELECT columnShortcutTable.* FROM columnShortcutTable;
 ALTER TABLE columnShortcutTable REPLACE COLUMNS (key int);
 SELECT columnShortcutTable.* FROM columnShortcutTable;
-
-DROP TABLE columnShortcutTable;
diff --git a/ql/src/test/queries/clientpositive/combine1.q b/ql/src/test/queries/clientpositive/combine1.q
index b3eb9461f8..9ac9eb0fc4 100644
--- a/ql/src/test/queries/clientpositive/combine1.q
+++ b/ql/src/test/queries/clientpositive/combine1.q
@@ -7,9 +7,6 @@ set mapred.max.split.size=256;
 
 set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;
 
-drop table combine1_1;
-
-
 create table combine1_1(key string, value string) stored as textfile;
 
 insert overwrite table combine1_1
@@ -18,5 +15,3 @@ select * from src;
 
 select key, value from combine1_1;
 
-drop table combine1_1;
-
diff --git a/ql/src/test/queries/clientpositive/combine2.q b/ql/src/test/queries/clientpositive/combine2.q
index b91341fe92..f4b2436aa4 100644
--- a/ql/src/test/queries/clientpositive/combine2.q
+++ b/ql/src/test/queries/clientpositive/combine2.q
@@ -6,8 +6,6 @@ set mapred.max.split.size=256;
 set hive.exec.dynamic.partition=true;
 set hive.exec.dynamic.partition.mode=nonstrict;
 
-drop table combine2;
-
 create table combine2(key string) partitioned by (value string);
 
 insert overwrite table combine2 partition(value) 
@@ -34,6 +32,3 @@ explain
 select ds, count(1) from srcpart where ds is not null group by ds;
 
 select ds, count(1) from srcpart where ds is not null group by ds;
-
-drop table combine2;
-
diff --git a/ql/src/test/queries/clientpositive/cp_mj_rc.q b/ql/src/test/queries/clientpositive/cp_mj_rc.q
index f9b2cac1ed..62d38e3caa 100644
--- a/ql/src/test/queries/clientpositive/cp_mj_rc.q
+++ b/ql/src/test/queries/clientpositive/cp_mj_rc.q
@@ -1,6 +1,3 @@
-drop table src_six_columns;
-drop table src_two_columns;
-
 create table src_six_columns (k1 string, v1 string, k2 string, v2 string, k3 string, v3 string) stored as rcfile;
 insert overwrite table src_six_columns select value, value, key, value, value, value from src;
 create table src_two_columns (k1 string, v1 string) stored as rcfile;
@@ -8,6 +5,3 @@ insert overwrite table src_two_columns select key, value from src;
 SELECT /*+ MAPJOIN(six) */ six.*, two.k1 from src_six_columns six join src_two_columns two on (six.k3=two.k1);
 
 SELECT /*+ MAPJOIN(two) */ two.*, six.k3 from src_six_columns six join src_two_columns two on (six.k3=two.k1);
-
-drop table src_six_columns;
-drop table src_two_columns;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/create_1.q b/ql/src/test/queries/clientpositive/create_1.q
index 82630885fe..02a3b2e847 100644
--- a/ql/src/test/queries/clientpositive/create_1.q
+++ b/ql/src/test/queries/clientpositive/create_1.q
@@ -1,9 +1,5 @@
 set fs.default.name=invalidscheme:///;
 
-DROP TABLE table1;
-DROP TABLE table2;
-DROP TABLE table3;
-
 CREATE TABLE table1 (a STRING, b STRING) STORED AS TEXTFILE;
 DESCRIBE table1;
 DESCRIBE EXTENDED table1;
@@ -19,7 +15,3 @@ ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
 STORED AS TEXTFILE;
 DESCRIBE table3;
 DESCRIBE EXTENDED table3;
-
-DROP TABLE table1;
-DROP TABLE table2;
-DROP TABLE table3;
diff --git a/ql/src/test/queries/clientpositive/create_escape.q b/ql/src/test/queries/clientpositive/create_escape.q
index 084a7b3596..49f7c5fe60 100644
--- a/ql/src/test/queries/clientpositive/create_escape.q
+++ b/ql/src/test/queries/clientpositive/create_escape.q
@@ -1,7 +1,3 @@
-DROP TABLE table1;
-DROP TABLE table2;
-DROP TABLE table3;
-
 CREATE TABLE table1 (a STRING, b STRING)
 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' ESCAPED BY '\\'
 STORED AS TEXTFILE;
@@ -12,5 +8,3 @@ DESCRIBE EXTENDED table1;
 INSERT OVERWRITE TABLE table1 SELECT key, '\\\t\\' FROM src WHERE key = 86;
 
 SELECT * FROM table1;
-
-DROP TABLE table1;
diff --git a/ql/src/test/queries/clientpositive/create_insert_outputformat.q b/ql/src/test/queries/clientpositive/create_insert_outputformat.q
index 30878c6ae4..f18d68866a 100644
--- a/ql/src/test/queries/clientpositive/create_insert_outputformat.q
+++ b/ql/src/test/queries/clientpositive/create_insert_outputformat.q
@@ -1,4 +1,4 @@
-DROP TABLE table_test_output_format;
+
 
 CREATE TABLE table_test_output_format(key INT, value STRING) STORED AS
   INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
@@ -7,9 +7,9 @@ CREATE TABLE table_test_output_format(key INT, value STRING) STORED AS
 FROM src
 INSERT OVERWRITE TABLE table_test_output_format SELECT src.key, src.value LIMIT 10;
 describe table_test_output_format;
-DROP TABLE table_test_output_format;
 
-DROP TABLE table_test_output_format_sequencefile;
+
+
 CREATE TABLE table_test_output_format_sequencefile(key INT, value STRING) STORED AS
   INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
   OUTPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileOutputFormat';
@@ -17,9 +17,9 @@ CREATE TABLE table_test_output_format_sequencefile(key INT, value STRING) STORED
 FROM src
 INSERT OVERWRITE TABLE table_test_output_format_sequencefile SELECT src.key, src.value LIMIT 10;
 describe table_test_output_format_sequencefile;
-DROP TABLE table_test_output_format_sequencefile;
 
-DROP TABLE table_test_output_format_hivesequencefile;
+
+
 CREATE TABLE table_test_output_format_hivesequencefile(key INT, value STRING) STORED AS
   INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
   OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat';
@@ -27,4 +27,4 @@ CREATE TABLE table_test_output_format_hivesequencefile(key INT, value STRING) ST
 FROM src
 INSERT OVERWRITE TABLE table_test_output_format_hivesequencefile SELECT src.key, src.value LIMIT 10;
 describe table_test_output_format_hivesequencefile;
-DROP TABLE table_test_output_format_hivesequencefile;
+
diff --git a/ql/src/test/queries/clientpositive/create_like.q b/ql/src/test/queries/clientpositive/create_like.q
index 276d8880a2..2edde83923 100644
--- a/ql/src/test/queries/clientpositive/create_like.q
+++ b/ql/src/test/queries/clientpositive/create_like.q
@@ -1,6 +1,6 @@
-DROP TABLE table1;
-DROP TABLE table2;
-DROP TABLE table3;
+
+
+
 
 CREATE TABLE table1 (a STRING, b STRING) STORED AS TEXTFILE;
 DESCRIBE table1;
@@ -24,6 +24,6 @@ INSERT OVERWRITE TABLE table2 SELECT key, value FROM src WHERE key = 100;
 SELECT * FROM table1;
 SELECT * FROM table2;
 
-DROP TABLE table1;
-DROP TABLE table2;
-DROP TABLE table3;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/create_nested_type.q b/ql/src/test/queries/clientpositive/create_nested_type.q
index cf2e99e3fb..2debd0d71d 100644
--- a/ql/src/test/queries/clientpositive/create_nested_type.q
+++ b/ql/src/test/queries/clientpositive/create_nested_type.q
@@ -1,4 +1,4 @@
-DROP TABLE table1;
+
 
 CREATE TABLE table1 (
        a STRING,
@@ -13,4 +13,4 @@ LOAD DATA LOCAL INPATH '../data/files/create_nested_type.txt' OVERWRITE INTO TAB
 
 SELECT * from table1;
 
-DROP TABLE table1;
+
diff --git a/ql/src/test/queries/clientpositive/create_struct_table.q b/ql/src/test/queries/clientpositive/create_struct_table.q
index 74b8a49bc7..dd5aa63e45 100644
--- a/ql/src/test/queries/clientpositive/create_struct_table.q
+++ b/ql/src/test/queries/clientpositive/create_struct_table.q
@@ -1,4 +1,4 @@
-drop table abc;
+
 create table abc(strct struct<a:int, b:string, c:string>)
 row format delimited
   fields terminated by '\t'
@@ -9,4 +9,4 @@ overwrite into table abc;
 
 SELECT strct, strct.a, strct.b FROM abc LIMIT 10;
 
-drop table abc;
+
diff --git a/ql/src/test/queries/clientpositive/create_view.q b/ql/src/test/queries/clientpositive/create_view.q
index eb58974cd4..bb1dff327b 100644
--- a/ql/src/test/queries/clientpositive/create_view.q
+++ b/ql/src/test/queries/clientpositive/create_view.q
@@ -17,7 +17,7 @@ DROP VIEW view16;
 DROP TEMPORARY FUNCTION test_translate;
 DROP TEMPORARY FUNCTION test_max;
 DROP TEMPORARY FUNCTION test_explode;
-DROP TABLE table1;
+
 
 SELECT * FROM src WHERE key=86;
 CREATE VIEW view1 AS SELECT value FROM src WHERE key=86;
@@ -192,7 +192,7 @@ LIMIT 10;
 
 -- this should work since currently we don't track view->table
 -- dependencies for implementing RESTRICT
-DROP TABLE table1;
+
 
 DROP VIEW view1;
 DROP VIEW view2;
diff --git a/ql/src/test/queries/clientpositive/ct_case_insensitive.q b/ql/src/test/queries/clientpositive/ct_case_insensitive.q
index aafb878e72..00e9722087 100644
--- a/ql/src/test/queries/clientpositive/ct_case_insensitive.q
+++ b/ql/src/test/queries/clientpositive/ct_case_insensitive.q
@@ -1,5 +1,3 @@
-DROP TABLE tmp_pyang_bucket3;
 CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS;
 DROP TABLE tmp_pyang_bucket3;
 CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) SORTED BY (USERID) INTO 32 BUCKETS;
-DROP TABLE tmp_pyang_bucket3;
diff --git a/ql/src/test/queries/clientpositive/ctas.q b/ql/src/test/queries/clientpositive/ctas.q
index 5813289f62..18654b419d 100644
--- a/ql/src/test/queries/clientpositive/ctas.q
+++ b/ql/src/test/queries/clientpositive/ctas.q
@@ -1,10 +1,10 @@
-drop table nzhang_ctas1;
-drop table nzhang_ctas2;
-drop table nzhang_ctas3;
-drop table nzhang_ctas4;
-drop table nzhang_ctas5;
-drop table nzhang_ctas6;
-drop table nzhang_ctas7;
+
+
+
+
+
+
+
 
 create table nzhang_Tmp(a int, b string);
 select * from nzhang_Tmp;
@@ -51,11 +51,11 @@ create table nzhang_ctas6 (key string, `to` string);
 insert overwrite table nzhang_ctas6 select key, value from src limit 10;
 create table nzhang_ctas7 as select key, `to` from nzhang_ctas6;
 
-drop table nzhang_ctas1;
-drop table nzhang_ctas2;
-drop table nzhang_ctas3;
-drop table nzhang_ctas4;
-drop table nzhang_ctas5;
-drop table nzhang_ctas6;
-drop table nzhang_ctas7;
-drop table nzhang_Tmp;
+
+
+
+
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/ddltime.q b/ql/src/test/queries/clientpositive/ddltime.q
index d6bae1a4af..3eead6f29d 100644
--- a/ql/src/test/queries/clientpositive/ddltime.q
+++ b/ql/src/test/queries/clientpositive/ddltime.q
@@ -19,7 +19,7 @@ insert overwrite table T1 select * from src;
 
 desc extended T1;
 
-drop table T1;
+
 
 create table if not exists T2 like srcpart;
 desc extended T2;
@@ -42,4 +42,4 @@ insert overwrite table T2 partition (ds='2010-06-01', hr='1') select key, value
 
 desc extended T2 partition(ds='2010-06-01', hr='1');
 
-drop table T2;
+
diff --git a/ql/src/test/queries/clientpositive/diff_part_input_formats.q b/ql/src/test/queries/clientpositive/diff_part_input_formats.q
index cae71a37c9..c5741d0050 100644
--- a/ql/src/test/queries/clientpositive/diff_part_input_formats.q
+++ b/ql/src/test/queries/clientpositive/diff_part_input_formats.q
@@ -6,4 +6,4 @@ ALTER TABLE part_test ADD PARTITION(ds='1');
 ALTER TABLE part_test SET FILEFORMAT RCFILE;
 ALTER TABLE part_test ADD PARTITION(ds='2');
 SELECT count(1) FROM part_test WHERE ds='3';
-DROP TABLE part_test;
+
diff --git a/ql/src/test/queries/clientpositive/disable_file_format_check.q b/ql/src/test/queries/clientpositive/disable_file_format_check.q
index a4ea8c51d7..6ea4156b34 100644
--- a/ql/src/test/queries/clientpositive/disable_file_format_check.q
+++ b/ql/src/test/queries/clientpositive/disable_file_format_check.q
@@ -5,5 +5,5 @@ load data local inpath '../data/files/kv1.seq' overwrite into table kv_fileforma
 create table kv_fileformat_check_seq (key string, value string) stored as sequencefile;
 load data local inpath '../data/files/kv1.txt' overwrite into table kv_fileformat_check_seq;
 
-drop table kv_fileformat_check_seq;
-drop table kv_fileformat_check_txt;
+
+
diff --git a/ql/src/test/queries/clientpositive/disable_merge_for_bucketing.q b/ql/src/test/queries/clientpositive/disable_merge_for_bucketing.q
index af7fbfb3ae..f09a24f336 100644
--- a/ql/src/test/queries/clientpositive/disable_merge_for_bucketing.q
+++ b/ql/src/test/queries/clientpositive/disable_merge_for_bucketing.q
@@ -2,7 +2,7 @@ set hive.enforce.bucketing = true;
 set hive.exec.reducers.max = 1;
 set hive.merge.mapredfiles=true;
 
-drop table bucket2_1;
+
 CREATE TABLE bucket2_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS;
 
 explain extended
@@ -18,4 +18,3 @@ select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key;
 select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key;
 
 
-drop table bucket2_1;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/drop_multi_partitions.q b/ql/src/test/queries/clientpositive/drop_multi_partitions.q
index 8ae96e3dfd..d13e9b6b0c 100644
--- a/ql/src/test/queries/clientpositive/drop_multi_partitions.q
+++ b/ql/src/test/queries/clientpositive/drop_multi_partitions.q
@@ -11,5 +11,5 @@ alter table mp drop partition (b='1');
 
 show partitions mp;
 
-drop table mp;
+
 
diff --git a/ql/src/test/queries/clientpositive/fileformat_mix.q b/ql/src/test/queries/clientpositive/fileformat_mix.q
index fafce182ed..1e0c164733 100644
--- a/ql/src/test/queries/clientpositive/fileformat_mix.q
+++ b/ql/src/test/queries/clientpositive/fileformat_mix.q
@@ -1,4 +1,4 @@
-drop table fileformat_mix_test;
+
 
 create table fileformat_mix_test (src int, value string) partitioned by (ds string);
 alter table fileformat_mix_test set fileformat Sequencefile;
@@ -14,4 +14,3 @@ select count(1) from fileformat_mix_test;
 
 select src from fileformat_mix_test;
 
-drop table fileformat_mix_test;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/fileformat_sequencefile.q b/ql/src/test/queries/clientpositive/fileformat_sequencefile.q
index 6ef0a7a3ff..895c70f8af 100644
--- a/ql/src/test/queries/clientpositive/fileformat_sequencefile.q
+++ b/ql/src/test/queries/clientpositive/fileformat_sequencefile.q
@@ -14,4 +14,4 @@ INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 10;
 
 SELECT dest1.* FROM dest1;
 
-DROP TABLE dest1;
+
diff --git a/ql/src/test/queries/clientpositive/fileformat_text.q b/ql/src/test/queries/clientpositive/fileformat_text.q
index f5388610f6..8233aded97 100644
--- a/ql/src/test/queries/clientpositive/fileformat_text.q
+++ b/ql/src/test/queries/clientpositive/fileformat_text.q
@@ -14,4 +14,4 @@ INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 10;
 
 SELECT dest1.* FROM dest1;
 
-DROP TABLE dest1;
+
diff --git a/ql/src/test/queries/clientpositive/filter_join_breaktask.q b/ql/src/test/queries/clientpositive/filter_join_breaktask.q
index e522ca654f..fe24da7441 100644
--- a/ql/src/test/queries/clientpositive/filter_join_breaktask.q
+++ b/ql/src/test/queries/clientpositive/filter_join_breaktask.q
@@ -1,4 +1,4 @@
-DROP TABLE filter_join_breaktask;
+
 CREATE TABLE filter_join_breaktask(key int, value string) partitioned by (ds string);
 
 INSERT OVERWRITE TABLE filter_join_breaktask PARTITION(ds='2008-04-08')
@@ -14,4 +14,3 @@ SELECT f.key, g.value
 FROM filter_join_breaktask f JOIN filter_join_breaktask m ON( f.key = m.key AND f.ds='2008-04-08' AND m.ds='2008-04-08' AND f.key is not null) 
 JOIN filter_join_breaktask g ON(g.value = m.value AND g.ds='2008-04-08' AND m.ds='2008-04-08' AND m.value is not null AND m.value !='');
 
-DROP TABLE filter_join_breaktask;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/filter_join_breaktask2.q b/ql/src/test/queries/clientpositive/filter_join_breaktask2.q
index 13b6cc13f9..f8d855b259 100644
--- a/ql/src/test/queries/clientpositive/filter_join_breaktask2.q
+++ b/ql/src/test/queries/clientpositive/filter_join_breaktask2.q
@@ -1,7 +1,7 @@
-drop table T1;
-drop table T2;
-drop table T3;
-drop table T4;
+
+
+
+
 
 create table T1(c1 string, c2 string, c3 string, c4 string, c5 string, c6 string, c7 string) 
 partitioned by (ds string);
@@ -34,7 +34,7 @@ FROM T1 a JOIN T2 b
      JOIN T4 d 
        ON (c.c0 = d.c0 AND c.ds='2010-04-17' AND d.ds='2010-04-17');
 
-drop table T1;
-drop table T2;
-drop table T3;
-drop table T4;
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/groupby10.q b/ql/src/test/queries/clientpositive/groupby10.q
index acc3c98cff..aef4a30188 100644
--- a/ql/src/test/queries/clientpositive/groupby10.q
+++ b/ql/src/test/queries/clientpositive/groupby10.q
@@ -1,9 +1,9 @@
 set hive.map.aggr=false;
 set hive.groupby.skewindata=true;
 
-drop table dest1;
-drop table dest2;
-drop table INPUT;
+
+
+
 
 CREATE TABLE dest1(key INT, val1 INT, val2 INT);
 CREATE TABLE dest2(key INT, val1 INT, val2 INT);
@@ -23,6 +23,6 @@ INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(d
 SELECT * from dest1;
 SELECT * from dest2;
 
-drop table INPUT;
-drop table dest1;
-drop table dest2;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/groupby11.q b/ql/src/test/queries/clientpositive/groupby11.q
index 2b7a8d058f..0bf92ac448 100644
--- a/ql/src/test/queries/clientpositive/groupby11.q
+++ b/ql/src/test/queries/clientpositive/groupby11.q
@@ -1,8 +1,8 @@
 set hive.map.aggr=false;
 set hive.groupby.skewindata=true;
 
-drop table dest1;
-drop table dest2;
+
+
 
 CREATE TABLE dest1(key STRING, val1 INT, val2 INT) partitioned by (ds string);
 CREATE TABLE dest2(key STRING, val1 INT, val2 INT) partitioned by (ds string);
@@ -23,5 +23,5 @@ INSERT OVERWRITE TABLE dest2  partition(ds='111')
 SELECT * from dest1;
 SELECT * from dest2;
 
-drop table dest1;
-drop table dest2;
+
+
diff --git a/ql/src/test/queries/clientpositive/groupby3.q b/ql/src/test/queries/clientpositive/groupby3.q
index 6f04d6ff13..8f24584a58 100755
--- a/ql/src/test/queries/clientpositive/groupby3.q
+++ b/ql/src/test/queries/clientpositive/groupby3.q
@@ -30,4 +30,4 @@ INSERT OVERWRITE TABLE dest1 SELECT
 
 SELECT dest1.* FROM dest1;
 
-DROP TABLE dest1;
+
diff --git a/ql/src/test/queries/clientpositive/groupby3_map.q b/ql/src/test/queries/clientpositive/groupby3_map.q
index d1f10268bf..7ecc71dfab 100644
--- a/ql/src/test/queries/clientpositive/groupby3_map.q
+++ b/ql/src/test/queries/clientpositive/groupby3_map.q
@@ -31,4 +31,4 @@ INSERT OVERWRITE TABLE dest1 SELECT
 
 SELECT dest1.* FROM dest1;
 
-DROP TABLE dest1;
+
diff --git a/ql/src/test/queries/clientpositive/groupby3_map_skew.q b/ql/src/test/queries/clientpositive/groupby3_map_skew.q
index c7a8f73ac4..07d10c2d74 100644
--- a/ql/src/test/queries/clientpositive/groupby3_map_skew.q
+++ b/ql/src/test/queries/clientpositive/groupby3_map_skew.q
@@ -31,4 +31,4 @@ INSERT OVERWRITE TABLE dest1 SELECT
 
 SELECT dest1.* FROM dest1;
 
-DROP TABLE dest1;
+
diff --git a/ql/src/test/queries/clientpositive/groupby3_noskew.q b/ql/src/test/queries/clientpositive/groupby3_noskew.q
index 7fa59ab540..d33f12c574 100644
--- a/ql/src/test/queries/clientpositive/groupby3_noskew.q
+++ b/ql/src/test/queries/clientpositive/groupby3_noskew.q
@@ -32,5 +32,5 @@ INSERT OVERWRITE TABLE dest1 SELECT
 
 SELECT dest1.* FROM dest1;
 
-DROP TABLE dest1;
+
 
diff --git a/ql/src/test/queries/clientpositive/groupby9.q b/ql/src/test/queries/clientpositive/groupby9.q
index 35f7b99392..df8eb97aa2 100644
--- a/ql/src/test/queries/clientpositive/groupby9.q
+++ b/ql/src/test/queries/clientpositive/groupby9.q
@@ -1,5 +1,5 @@
-drop table DEST1;
-drop table DEST2;
+
+
 
 CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
 CREATE TABLE DEST2(key INT, val1 STRING, val2 STRING) STORED AS TEXTFILE;
@@ -16,5 +16,5 @@ INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SR
 SELECT DEST1.* FROM DEST1;
 SELECT DEST2.* FROM DEST2;
 
-drop table DEST1;
-drop table DEST2;
+
+
diff --git a/ql/src/test/queries/clientpositive/implicit_cast1.q b/ql/src/test/queries/clientpositive/implicit_cast1.q
index 72a055a6ec..37fb8568c9 100644
--- a/ql/src/test/queries/clientpositive/implicit_cast1.q
+++ b/ql/src/test/queries/clientpositive/implicit_cast1.q
@@ -9,5 +9,5 @@ SELECT implicit_test1.*
 FROM implicit_test1
 WHERE implicit_test1.a <> 0;
 
-DROP TABLE implicit_test1;
+
 
diff --git a/ql/src/test/queries/clientpositive/init_file.q b/ql/src/test/queries/clientpositive/init_file.q
index f821b03721..f69a88d036 100644
--- a/ql/src/test/queries/clientpositive/init_file.q
+++ b/ql/src/test/queries/clientpositive/init_file.q
@@ -2,4 +2,4 @@
 -- automatically by test_init_file.sql
 
 select * from tbl_created_by_init;
-drop table tbl_created_by_init;
+
diff --git a/ql/src/test/queries/clientpositive/input1.q b/ql/src/test/queries/clientpositive/input1.q
index b745ca334b..3f2cd96de9 100644
--- a/ql/src/test/queries/clientpositive/input1.q
+++ b/ql/src/test/queries/clientpositive/input1.q
@@ -5,5 +5,5 @@ DESCRIBE TEST1;
 
 DESCRIBE TEST1; 
 
-DROP TABLE TEST1;
+
 
diff --git a/ql/src/test/queries/clientpositive/input10.q b/ql/src/test/queries/clientpositive/input10.q
index 30bd23a582..aef5c9bfe4 100644
--- a/ql/src/test/queries/clientpositive/input10.q
+++ b/ql/src/test/queries/clientpositive/input10.q
@@ -5,5 +5,5 @@ DESCRIBE TEST10;
 
 DESCRIBE TEST10;
 
-DROP TABLE TEST10;
+
 
diff --git a/ql/src/test/queries/clientpositive/input15.q b/ql/src/test/queries/clientpositive/input15.q
index 1772a429ab..2b76497fe5 100644
--- a/ql/src/test/queries/clientpositive/input15.q
+++ b/ql/src/test/queries/clientpositive/input15.q
@@ -5,5 +5,3 @@ CREATE TABLE TEST15(key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATE
 
 DESCRIBE TEST15;
 
-DROP TABLE TEST15;
-
diff --git a/ql/src/test/queries/clientpositive/input16.q b/ql/src/test/queries/clientpositive/input16.q
index d22a45dcd9..82e6d81426 100644
--- a/ql/src/test/queries/clientpositive/input16.q
+++ b/ql/src/test/queries/clientpositive/input16.q
@@ -4,4 +4,3 @@ ADD JAR ../data/files/TestSerDe.jar;
 CREATE TABLE INPUT16(KEY STRING, VALUE STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.TestSerDe' STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1_cb.txt' INTO TABLE INPUT16;
 SELECT INPUT16.VALUE, INPUT16.KEY FROM INPUT16;
-DROP TABLE INPUT16;
diff --git a/ql/src/test/queries/clientpositive/input16_cc.q b/ql/src/test/queries/clientpositive/input16_cc.q
index d2a6fe1440..19c1c82fb6 100644
--- a/ql/src/test/queries/clientpositive/input16_cc.q
+++ b/ql/src/test/queries/clientpositive/input16_cc.q
@@ -5,4 +5,4 @@ ADD JAR ../data/files/TestSerDe.jar;
 CREATE TABLE INPUT16_CC(KEY STRING, VALUE STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.TestSerDe'  with serdeproperties ('testserde.default.serialization.format'='\003', 'dummy.prop.not.used'='dummyy.val') STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1_cc.txt' INTO TABLE INPUT16_CC;
 SELECT INPUT16_CC.VALUE, INPUT16_CC.KEY FROM INPUT16_CC;
-DROP TABLE INPUT16_CC;
+
diff --git a/ql/src/test/queries/clientpositive/input19.q b/ql/src/test/queries/clientpositive/input19.q
index ccdb77fcfb..b4f22d92f5 100644
--- a/ql/src/test/queries/clientpositive/input19.q
+++ b/ql/src/test/queries/clientpositive/input19.q
@@ -1,5 +1,5 @@
-drop table apachelog;
+
 create table apachelog(ipaddress STRING,identd STRING,user STRING,finishtime STRING,requestline string,returncode INT,size INT) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe' WITH SERDEPROPERTIES (  'serialization.format'= 'org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol',  'quote.delim'= '("|\\[|\\])',  'field.delim'=' ',  'serialization.null.format'='-'  ) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/apache.access.log' INTO TABLE apachelog;
 SELECT a.* FROM apachelog a;
-drop table apachelog;
+
diff --git a/ql/src/test/queries/clientpositive/input1_limit.q b/ql/src/test/queries/clientpositive/input1_limit.q
index 834a0223ec..5ed59e3580 100644
--- a/ql/src/test/queries/clientpositive/input1_limit.q
+++ b/ql/src/test/queries/clientpositive/input1_limit.q
@@ -13,6 +13,6 @@ INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key < 100 LIMIT
 SELECT dest1.* FROM dest1;
 SELECT dest2.* FROM dest2;
 
-DROP TABLE dest1;
-DROP TABLE dest2;
+
+
 
diff --git a/ql/src/test/queries/clientpositive/input2.q b/ql/src/test/queries/clientpositive/input2.q
index 0e136625cb..00693851ef 100644
--- a/ql/src/test/queries/clientpositive/input2.q
+++ b/ql/src/test/queries/clientpositive/input2.q
@@ -1,6 +1,3 @@
-DROP TABLE TEST2a;
-DROP TABLE TEST2b;
-
 CREATE TABLE TEST2a(A INT, B DOUBLE) STORED AS TEXTFILE;
 DESCRIBE TEST2a;
 DESC TEST2a;
@@ -13,6 +10,3 @@ DROP TABLE TEST2b;
 
 EXPLAIN
 SHOW TABLES;
-
-DROP TABLE TEST2a;
-DROP TABLE TEST2b;
diff --git a/ql/src/test/queries/clientpositive/input21.q b/ql/src/test/queries/clientpositive/input21.q
index ed27476bb3..d7c814e580 100644
--- a/ql/src/test/queries/clientpositive/input21.q
+++ b/ql/src/test/queries/clientpositive/input21.q
@@ -1,4 +1,4 @@
-DROP TABLE src_null;
+
 
 CREATE TABLE src_null(a STRING, b STRING, c STRING, d STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/null.txt' INTO TABLE src_null;
@@ -7,4 +7,4 @@ EXPLAIN SELECT * FROM src_null DISTRIBUTE BY c SORT BY d;
 
 SELECT * FROM src_null DISTRIBUTE BY c SORT BY d;
 
-DROP TABLE src_null;
+
diff --git a/ql/src/test/queries/clientpositive/input22.q b/ql/src/test/queries/clientpositive/input22.q
index 54833c7f98..853947be57 100644
--- a/ql/src/test/queries/clientpositive/input22.q
+++ b/ql/src/test/queries/clientpositive/input22.q
@@ -12,4 +12,4 @@ FROM (SELECT INPUT4.*, INPUT4.KEY as KEY2
       FROM INPUT4) a
 ORDER BY KEY2 LIMIT 10;
 
-DROP TABLE INPUT4;
+
diff --git a/ql/src/test/queries/clientpositive/input24.q b/ql/src/test/queries/clientpositive/input24.q
index fe2581b6d1..95b2377f51 100644
--- a/ql/src/test/queries/clientpositive/input24.q
+++ b/ql/src/test/queries/clientpositive/input24.q
@@ -1,4 +1,4 @@
-drop table tst;
+
 create table tst(a int, b int) partitioned by (d string);
 alter table tst add partition (d='2009-01-01');
 explain
@@ -6,4 +6,4 @@ select count(1) from tst x where x.d='2009-01-01';
 
 select count(1) from tst x where x.d='2009-01-01';
 
-drop table tst;
+
diff --git a/ql/src/test/queries/clientpositive/input25.q b/ql/src/test/queries/clientpositive/input25.q
index 6f1f44513b..e48368ff2a 100644
--- a/ql/src/test/queries/clientpositive/input25.q
+++ b/ql/src/test/queries/clientpositive/input25.q
@@ -1,4 +1,4 @@
-drop table tst;
+
 create table tst(a int, b int) partitioned by (d string);
 alter table tst add partition (d='2009-01-01');
 alter table tst add partition (d='2009-02-02');
@@ -16,4 +16,4 @@ select * from (
   select * from tst x where x.d='2009-02-02' limit 10
 ) subq;
 
-drop table tst;
+
diff --git a/ql/src/test/queries/clientpositive/input28.q b/ql/src/test/queries/clientpositive/input28.q
index da8a3ce850..12e857df8a 100644
--- a/ql/src/test/queries/clientpositive/input28.q
+++ b/ql/src/test/queries/clientpositive/input28.q
@@ -1,4 +1,4 @@
-drop table tst;
+
 create table tst(a string, b string) partitioned by (d string);
 alter table tst add partition (d='2009-01-01');
 
@@ -7,4 +7,4 @@ select tst.a, src.value from tst join src ON (tst.a = src.key);
 
 select * from tst where tst.d='2009-01-01';
 
-drop table tst;
+
diff --git a/ql/src/test/queries/clientpositive/input3.q b/ql/src/test/queries/clientpositive/input3.q
index e8544714dc..2efa7a4d3d 100644
--- a/ql/src/test/queries/clientpositive/input3.q
+++ b/ql/src/test/queries/clientpositive/input3.q
@@ -1,6 +1,6 @@
-DROP TABLE TEST3a;
-DROP TABLE TEST3b;
-DROP TABLE TEST3c;
+
+
+
 
 CREATE TABLE TEST3a(A INT, B DOUBLE) STORED AS TEXTFILE; 
 DESCRIBE TEST3a; 
@@ -21,6 +21,6 @@ ALTER TABLE TEST3c REPLACE COLUMNS (R1 INT, R2 DOUBLE);
 ALTER TABLE TEST3c REPLACE COLUMNS (R1 INT, R2 DOUBLE);
 DESCRIBE EXTENDED TEST3c;
 
-DROP TABLE TEST3a;
-DROP TABLE TEST3b;
-DROP TABLE TEST3c;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/input30.q b/ql/src/test/queries/clientpositive/input30.q
index a868f62863..c84c131082 100644
--- a/ql/src/test/queries/clientpositive/input30.q
+++ b/ql/src/test/queries/clientpositive/input30.q
@@ -1,5 +1,5 @@
-drop table tst_dest30;
-drop table dest30;
+
+
 
 
 create table dest30(a int);
@@ -19,5 +19,5 @@ set hive.test.mode=false;
 
 select * from tst_dest30;
 
-drop table tst_dest30;
-drop table dest30;
+
+
diff --git a/ql/src/test/queries/clientpositive/input31.q b/ql/src/test/queries/clientpositive/input31.q
index 6bef385582..ece940903a 100644
--- a/ql/src/test/queries/clientpositive/input31.q
+++ b/ql/src/test/queries/clientpositive/input31.q
@@ -1,5 +1,5 @@
-drop table tst_dest31;
-drop table dest31;
+
+
 
 set hive.test.mode=true;
 set hive.test.mode.prefix=tst_;
@@ -18,7 +18,7 @@ set hive.test.mode=false;
 
 select * from tst_dest31;
 
-drop table tst_dest31;
-drop table dest31;
+
+
 
 
diff --git a/ql/src/test/queries/clientpositive/input32.q b/ql/src/test/queries/clientpositive/input32.q
index ccec170486..cdc04b8c05 100644
--- a/ql/src/test/queries/clientpositive/input32.q
+++ b/ql/src/test/queries/clientpositive/input32.q
@@ -1,5 +1,5 @@
-drop table tst_dest32;
-drop table dest32;
+
+
 
 set hive.test.mode=true;
 set hive.test.mode.prefix=tst_;
@@ -19,7 +19,7 @@ set hive.test.mode=false;
 
 select * from tst_dest32;
 
-drop table tst_dest32;
-drop table dest32;
+
+
 
 
diff --git a/ql/src/test/queries/clientpositive/input37.q b/ql/src/test/queries/clientpositive/input37.q
index 639d4d6086..6fd136afec 100644
--- a/ql/src/test/queries/clientpositive/input37.q
+++ b/ql/src/test/queries/clientpositive/input37.q
@@ -13,4 +13,4 @@ FROM
 group by url;
 
 
-DROP TABLE documents;
+
diff --git a/ql/src/test/queries/clientpositive/input38.q b/ql/src/test/queries/clientpositive/input38.q
index d5ca05ae46..29195f5fb5 100644
--- a/ql/src/test/queries/clientpositive/input38.q
+++ b/ql/src/test/queries/clientpositive/input38.q
@@ -1,4 +1,4 @@
-drop table dest1;
+
 CREATE TABLE dest1(key STRING, value STRING) STORED AS TEXTFILE;
 
 EXPLAIN
@@ -19,4 +19,4 @@ INSERT OVERWRITE TABLE dest1 SELECT tmap.key, tmap.value;
 
 SELECT dest1.* FROM dest1;
 
-drop table dest1;
+
diff --git a/ql/src/test/queries/clientpositive/input39.q b/ql/src/test/queries/clientpositive/input39.q
index 1744ffef96..ba09577fed 100644
--- a/ql/src/test/queries/clientpositive/input39.q
+++ b/ql/src/test/queries/clientpositive/input39.q
@@ -1,5 +1,5 @@
-drop table t1;
-drop table t2;
+
+
 
 create table t1(key string, value string) partitioned by (ds string);
 create table t2(key string, value string) partitioned by (ds string);
@@ -23,5 +23,5 @@ select count(1) from t1 join t2 on t1.key=t2.key where t1.ds='1' and t2.ds='1';
 
 set hive.test.mode=false;
 
-drop table t1;
-drop table t2;
+
+
diff --git a/ql/src/test/queries/clientpositive/input3_limit.q b/ql/src/test/queries/clientpositive/input3_limit.q
index e326fd934e..3584820aca 100644
--- a/ql/src/test/queries/clientpositive/input3_limit.q
+++ b/ql/src/test/queries/clientpositive/input3_limit.q
@@ -1,9 +1,9 @@
-DROP TABLE T1;
+
 CREATE TABLE T1(key STRING, value STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE T1;
 LOAD DATA LOCAL INPATH '../data/files/kv2.txt' INTO TABLE T1;
 
-DROP TABLE T2;
+
 CREATE TABLE T2(key STRING, value STRING);
 
 EXPLAIN 
@@ -13,5 +13,5 @@ INSERT OVERWRITE TABLE T2 SELECT * FROM (SELECT * FROM T1 DISTRIBUTE BY key SORT
 
 SELECT * FROM T2 SORT BY key, value;
 
-DROP TABLE T1;
-DROP TABLE T2;
+
+
diff --git a/ql/src/test/queries/clientpositive/input4.q b/ql/src/test/queries/clientpositive/input4.q
index 7444e2febe..62dc8173c5 100644
--- a/ql/src/test/queries/clientpositive/input4.q
+++ b/ql/src/test/queries/clientpositive/input4.q
@@ -3,4 +3,4 @@ EXPLAIN
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE INPUT4;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE INPUT4;
 SELECT INPUT4.VALUE, INPUT4.KEY FROM INPUT4;
-DROP TABLE INPUT4;
+
diff --git a/ql/src/test/queries/clientpositive/input40.q b/ql/src/test/queries/clientpositive/input40.q
index 7f3eb4c21f..2b64294028 100644
--- a/ql/src/test/queries/clientpositive/input40.q
+++ b/ql/src/test/queries/clientpositive/input40.q
@@ -1,5 +1,5 @@
-drop table tmp_insert_test;
-drop table tmp_insert_test_p;
+
+
 
 create table tmp_insert_test (key string, value string) stored as textfile;
 load data local inpath '../data/files/kv1.txt' into table tmp_insert_test;
@@ -16,5 +16,5 @@ select * from tmp_insert_test_p where ds= '2009-08-01'
 order by key;
 
 
-drop table tmp_insert_test;
-drop table tmp_insert_test_p;
+
+
diff --git a/ql/src/test/queries/clientpositive/input41.q b/ql/src/test/queries/clientpositive/input41.q
index 30cddb09fd..5bc396e311 100644
--- a/ql/src/test/queries/clientpositive/input41.q
+++ b/ql/src/test/queries/clientpositive/input41.q
@@ -11,4 +11,4 @@ select * from
 
 select * from dest_sp x order by x.cnt limit 2;
 
-drop table dest_sp;
+
diff --git a/ql/src/test/queries/clientpositive/input4_cb_delim.q b/ql/src/test/queries/clientpositive/input4_cb_delim.q
index a11a1fad68..8c57dd3f25 100644
--- a/ql/src/test/queries/clientpositive/input4_cb_delim.q
+++ b/ql/src/test/queries/clientpositive/input4_cb_delim.q
@@ -1,4 +1,4 @@
 CREATE TABLE INPUT4_CB(KEY STRING, VALUE STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\002' LINES TERMINATED BY '\012' STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1_cb.txt' INTO TABLE INPUT4_CB;
 SELECT INPUT4_CB.VALUE, INPUT4_CB.KEY FROM INPUT4_CB;
-DROP TABLE INPUT4_CB
+
diff --git a/ql/src/test/queries/clientpositive/input_columnarserde.q b/ql/src/test/queries/clientpositive/input_columnarserde.q
index 94d5517017..9394382b2b 100644
--- a/ql/src/test/queries/clientpositive/input_columnarserde.q
+++ b/ql/src/test/queries/clientpositive/input_columnarserde.q
@@ -1,4 +1,4 @@
-drop table input_columnarserde;
+
 CREATE TABLE input_columnarserde(a array<int>, b array<string>, c map<string,string>, d int, e string)
 ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
@@ -17,4 +17,3 @@ SELECT input_columnarserde.* FROM input_columnarserde DISTRIBUTE BY 1;
 
 SELECT input_columnarserde.a[0], input_columnarserde.b[0], input_columnarserde.c['key2'], input_columnarserde.d, input_columnarserde.e FROM input_columnarserde DISTRIBUTE BY 1;
 
-drop table input_columnarserde;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/input_dfs.q b/ql/src/test/queries/clientpositive/input_dfs.q
index 4404efedd3..4f5824df5c 100644
--- a/ql/src/test/queries/clientpositive/input_dfs.q
+++ b/ql/src/test/queries/clientpositive/input_dfs.q
@@ -1,2 +1,2 @@
-dfs -cat ../build/ql/test/data/files/kv1.txt;
+dfs -cat ../data/files/kv1.txt;
 
diff --git a/ql/src/test/queries/clientpositive/input_lazyserde.q b/ql/src/test/queries/clientpositive/input_lazyserde.q
index 14d108a741..4adfaaf362 100644
--- a/ql/src/test/queries/clientpositive/input_lazyserde.q
+++ b/ql/src/test/queries/clientpositive/input_lazyserde.q
@@ -27,4 +27,4 @@ DROP TABLE dest1;
 CREATE TABLE dest1(a map<string,string>) ROW FORMAT DELIMITED FIELDS TERMINATED BY '1' ESCAPED BY '\\';
 INSERT OVERWRITE TABLE dest1 SELECT src_thrift.mstringstring FROM src_thrift DISTRIBUTE BY 1;
 SELECT * from dest1;
-DROP TABLE dest1;
+
diff --git a/ql/src/test/queries/clientpositive/input_part10.q b/ql/src/test/queries/clientpositive/input_part10.q
index a8dcabdbcd..e6d6c22640 100644
--- a/ql/src/test/queries/clientpositive/input_part10.q
+++ b/ql/src/test/queries/clientpositive/input_part10.q
@@ -17,4 +17,4 @@ DESCRIBE EXTENDED part_special PARTITION(ds='2008 04 08', ts = '10:11:12=455');
 
 SELECT * FROM part_special WHERE ds='2008 04 08' AND ts = '10:11:12=455';
 
-DROP TABLE part_special;
+
diff --git a/ql/src/test/queries/clientpositive/input_part2.q b/ql/src/test/queries/clientpositive/input_part2.q
index 8c43382f6a..c9aaf25363 100644
--- a/ql/src/test/queries/clientpositive/input_part2.q
+++ b/ql/src/test/queries/clientpositive/input_part2.q
@@ -13,4 +13,4 @@ INSERT OVERWRITE TABLE dest2 SELECT srcpart.key, srcpart.value, srcpart.hr, srcp
 SELECT dest1.* FROM dest1 sort by key,value,ds,hr;
 SELECT dest2.* FROM dest2 sort by key,value,ds,hr;
 
-drop table dest2;
+
diff --git a/ql/src/test/queries/clientpositive/input_part5.q b/ql/src/test/queries/clientpositive/input_part5.q
index a0e77de024..ea9f70bdcb 100644
--- a/ql/src/test/queries/clientpositive/input_part5.q
+++ b/ql/src/test/queries/clientpositive/input_part5.q
@@ -1,4 +1,4 @@
-drop table tmptable;
+
 create table tmptable(key string, value string, hr string, ds string);
 
 EXPLAIN
@@ -9,4 +9,4 @@ insert overwrite table tmptable
 SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08' and x.key < 100;
 
 select * from tmptable x sort by x.key,x.value,x.ds,x.hr;
-drop table tmptable;
+
diff --git a/ql/src/test/queries/clientpositive/inputddl1.q b/ql/src/test/queries/clientpositive/inputddl1.q
index 5bde9b227b..b41f0b887c 100644
--- a/ql/src/test/queries/clientpositive/inputddl1.q
+++ b/ql/src/test/queries/clientpositive/inputddl1.q
@@ -4,5 +4,3 @@ CREATE TABLE INPUTDDL1(key INT, value STRING) STORED AS TEXTFILE;
 CREATE TABLE INPUTDDL1(key INT, value STRING) STORED AS TEXTFILE; 
 
 SELECT INPUTDDL1.* from INPUTDDL1;
-
-DROP TABLE INPUTDDL1;
diff --git a/ql/src/test/queries/clientpositive/inputddl2.q b/ql/src/test/queries/clientpositive/inputddl2.q
index 8f2dbd5695..e347791124 100644
--- a/ql/src/test/queries/clientpositive/inputddl2.q
+++ b/ql/src/test/queries/clientpositive/inputddl2.q
@@ -2,5 +2,5 @@ EXPLAIN
 CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE;
 CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE;
 DESCRIBE INPUTDDL2;
-DROP TABLE INPUTDDL2;
+
 
diff --git a/ql/src/test/queries/clientpositive/inputddl3.q b/ql/src/test/queries/clientpositive/inputddl3.q
index 54ed64ffb9..946cf54e40 100644
--- a/ql/src/test/queries/clientpositive/inputddl3.q
+++ b/ql/src/test/queries/clientpositive/inputddl3.q
@@ -2,4 +2,4 @@ EXPLAIN
 CREATE TABLE INPUTDDL3(key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;
 CREATE TABLE INPUTDDL3(key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;
 DESCRIBE INPUTDDL3;
-DROP TABLE INPUTDDL3;
+
diff --git a/ql/src/test/queries/clientpositive/inputddl4.q b/ql/src/test/queries/clientpositive/inputddl4.q
index bee29bba82..0e07d8da6b 100644
--- a/ql/src/test/queries/clientpositive/inputddl4.q
+++ b/ql/src/test/queries/clientpositive/inputddl4.q
@@ -1,5 +1,5 @@
 -- a simple test to test sorted/clustered syntax
-DROP TABLE INPUTDDL4;
+
 CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
                        page_url STRING, referrer_url STRING, 
                        friends ARRAY<BIGINT>, properties MAP<STRING, STRING>,
@@ -9,4 +9,4 @@ CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
     CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS;
 DESCRIBE INPUTDDL4;
 DESCRIBE EXTENDED INPUTDDL4;
-DROP TABLE INPUTDDL4;
+
diff --git a/ql/src/test/queries/clientpositive/inputddl5.q b/ql/src/test/queries/clientpositive/inputddl5.q
index dcf2c44436..9a7ca5da12 100644
--- a/ql/src/test/queries/clientpositive/inputddl5.q
+++ b/ql/src/test/queries/clientpositive/inputddl5.q
@@ -5,4 +5,4 @@ LOAD DATA LOCAL INPATH '../data/files/kv4.txt' INTO TABLE INPUTDDL5;
 DESCRIBE INPUTDDL5;
 SELECT INPUTDDL5.name from INPUTDDL5;
 SELECT count(1) FROM INPUTDDL5 WHERE INPUTDDL5.name = _UTF-8 0xE982B5E993AE;
-DROP TABLE INPUTDDL5;
+
diff --git a/ql/src/test/queries/clientpositive/inputddl6.q b/ql/src/test/queries/clientpositive/inputddl6.q
index ca605d719d..e65096308c 100644
--- a/ql/src/test/queries/clientpositive/inputddl6.q
+++ b/ql/src/test/queries/clientpositive/inputddl6.q
@@ -1,7 +1,6 @@
 -- test for describe extended table
 -- test for describe extended table partition
 -- test for alter table drop partition
-DROP TABLE INPUTDDL6;
 CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE INPUTDDL6 PARTITION (ds='2008-04-09');
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE INPUTDDL6 PARTITION (ds='2008-04-08');
diff --git a/ql/src/test/queries/clientpositive/inputddl7.q b/ql/src/test/queries/clientpositive/inputddl7.q
index 006a0b6152..8a73935fee 100644
--- a/ql/src/test/queries/clientpositive/inputddl7.q
+++ b/ql/src/test/queries/clientpositive/inputddl7.q
@@ -1,22 +1,22 @@
 -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE T1;
+
 CREATE TABLE T1(name STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE T1;
 SELECT COUNT(1) FROM T1;
 
-DROP TABLE T2;
+
 CREATE TABLE T2(name STRING) STORED AS SEQUENCEFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.seq' INTO TABLE T2;
 SELECT COUNT(1) FROM T2;
 
-DROP TABLE T3;
+
 CREATE TABLE T3(name STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE T3 PARTITION (ds='2008-04-09');
 SELECT COUNT(1) FROM T3 where T3.ds='2008-04-09';
 
-DROP TABLE T4;
+
 CREATE TABLE T4(name STRING) PARTITIONED BY(ds STRING) STORED AS SEQUENCEFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.seq' INTO TABLE T4 PARTITION (ds='2008-04-09');
 SELECT COUNT(1) FROM T4 where T4.ds='2008-04-09';
@@ -27,7 +27,7 @@ DESCRIBE EXTENDED T3 PARTITION (ds='2008-04-09');
 DESCRIBE EXTENDED T4 PARTITION (ds='2008-04-09');
 
 
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
-DROP TABLE T4;
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/inputddl8.q b/ql/src/test/queries/clientpositive/inputddl8.q
index 1ab3d33d58..66717a158b 100644
--- a/ql/src/test/queries/clientpositive/inputddl8.q
+++ b/ql/src/test/queries/clientpositive/inputddl8.q
@@ -1,4 +1,4 @@
-DROP TABLE INPUTDDL8;
+
 CREATE TABLE INPUTDDL8 COMMENT 'This is a thrift based table'
     PARTITIONED BY(ds STRING, country STRING)
     CLUSTERED BY(aint) SORTED BY(lint) INTO 32 BUCKETS
@@ -7,4 +7,4 @@ CREATE TABLE INPUTDDL8 COMMENT 'This is a thrift based table'
                           'serialization.format' = 'com.facebook.thrift.protocol.TBinaryProtocol')
     STORED AS SEQUENCEFILE;
 DESCRIBE EXTENDED INPUTDDL8;
-DROP TABLE INPUTDDL8;
+
diff --git a/ql/src/test/queries/clientpositive/insert1.q b/ql/src/test/queries/clientpositive/insert1.q
index b747f91320..19136623a5 100644
--- a/ql/src/test/queries/clientpositive/insert1.q
+++ b/ql/src/test/queries/clientpositive/insert1.q
@@ -1,7 +1,7 @@
-drop table insert1;
-drop table insert2;
+
+
 create table insert1(key int, value string) stored as textfile;
 create table insert2(key int, value string) stored as textfile;
 insert overwrite table insert1 select a.key, a.value from insert2 a WHERE (a.key=-1);
-drop table insert1;
-drop table insert2;
+
+
diff --git a/ql/src/test/queries/clientpositive/insertexternal1.q b/ql/src/test/queries/clientpositive/insertexternal1.q
index 466fcbf650..894e3b512c 100644
--- a/ql/src/test/queries/clientpositive/insertexternal1.q
+++ b/ql/src/test/queries/clientpositive/insertexternal1.q
@@ -1,4 +1,4 @@
-drop table texternal;
+
 
 create table texternal(key string, val string) partitioned by (insertdate string);
 
diff --git a/ql/src/test/queries/clientpositive/join19.q b/ql/src/test/queries/clientpositive/join19.q
index 314220ce54..5a6b741f19 100644
--- a/ql/src/test/queries/clientpositive/join19.q
+++ b/ql/src/test/queries/clientpositive/join19.q
@@ -1,4 +1,3 @@
-drop TABLE triples;
 CREATE TABLE triples (foo string, subject string, predicate string, object string, foo2 string);
 
 EXPLAIN
@@ -56,5 +55,4 @@ WHERE
 t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL'
 ) t66
 ON (t66.subject=t55.object);
-drop TABLE triples;
 
diff --git a/ql/src/test/queries/clientpositive/join24.q b/ql/src/test/queries/clientpositive/join24.q
index 2e5477a9ce..65c9256d6f 100644
--- a/ql/src/test/queries/clientpositive/join24.q
+++ b/ql/src/test/queries/clientpositive/join24.q
@@ -5,4 +5,4 @@ SELECT a.key, count(1) FROM src a group by a.key;
 
 SELECT sum(a.cnt)  FROM tst1 a JOIN tst1 b ON a.key = b.key;
 
-drop table tst1;
+
diff --git a/ql/src/test/queries/clientpositive/join25.q b/ql/src/test/queries/clientpositive/join25.q
index d8e17412d6..c0e4cbb93a 100644
--- a/ql/src/test/queries/clientpositive/join25.q
+++ b/ql/src/test/queries/clientpositive/join25.q
@@ -1,6 +1,6 @@
 set hive.mapjoin.numrows = 2;
 
-drop table dest_j1;
+
 
 CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE;
 
@@ -15,5 +15,5 @@ FROM src1 x JOIN src y ON (x.key = y.key);
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join26.q b/ql/src/test/queries/clientpositive/join26.q
index 96b5f9ac85..cecf8da533 100644
--- a/ql/src/test/queries/clientpositive/join26.q
+++ b/ql/src/test/queries/clientpositive/join26.q
@@ -13,5 +13,5 @@ JOIN srcpart z ON (x.key = z.key and z.ds='2008-04-08' and z.hr=11);
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join27.q b/ql/src/test/queries/clientpositive/join27.q
index fa0fdf3d5f..2c6e957657 100644
--- a/ql/src/test/queries/clientpositive/join27.q
+++ b/ql/src/test/queries/clientpositive/join27.q
@@ -1,4 +1,4 @@
-drop table dest_j1;
+
 
 CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE;
 
@@ -13,5 +13,5 @@ FROM src1 x JOIN src y ON (x.value = y.value);
 
 select * from dest_j1 x order by x.key, x.value;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join28.q b/ql/src/test/queries/clientpositive/join28.q
index 9100a75bdb..0638e9c345 100644
--- a/ql/src/test/queries/clientpositive/join28.q
+++ b/ql/src/test/queries/clientpositive/join28.q
@@ -1,4 +1,4 @@
-drop table dest_j1;
+
 
 CREATE TABLE dest_j1(key STRING, value STRING) STORED AS TEXTFILE;
 
@@ -19,5 +19,5 @@ FROM
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join29.q b/ql/src/test/queries/clientpositive/join29.q
index b9ba5f7d55..5bad3116da 100644
--- a/ql/src/test/queries/clientpositive/join29.q
+++ b/ql/src/test/queries/clientpositive/join29.q
@@ -1,5 +1,3 @@
-drop TABLE dest_j1;
-
 CREATE TABLE dest_j1(key STRING, cnt1 INT, cnt2 INT);
 
 EXPLAIN 
@@ -14,5 +12,3 @@ FROM (select x.key, count(1) as cnt from src1 x group by x.key) subq1 JOIN
      (select y.key, count(1) as cnt from src y group by y.key) subq2 ON (subq1.key = subq2.key);
 
 select * from dest_j1 x order by x.key;
-
-drop TABLE dest_j1;
diff --git a/ql/src/test/queries/clientpositive/join30.q b/ql/src/test/queries/clientpositive/join30.q
index c0411fab1d..036a16d3b1 100644
--- a/ql/src/test/queries/clientpositive/join30.q
+++ b/ql/src/test/queries/clientpositive/join30.q
@@ -1,5 +1,3 @@
-drop TABLE dest_j1;
-
 CREATE TABLE dest_j1(key INT, cnt INT);
 
 EXPLAIN
@@ -10,5 +8,3 @@ INSERT OVERWRITE TABLE dest_j1
 SELECT /*+ MAPJOIN(x) */ x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;
 
 select * from dest_j1 x order by x.key;
-
-drop TABLE dest_j1;
diff --git a/ql/src/test/queries/clientpositive/join31.q b/ql/src/test/queries/clientpositive/join31.q
index cb1d561cbd..21b0711608 100644
--- a/ql/src/test/queries/clientpositive/join31.q
+++ b/ql/src/test/queries/clientpositive/join31.q
@@ -1,5 +1,3 @@
-drop TABLE dest_j1;
-
 CREATE TABLE dest_j1(key STRING, cnt INT);
 
 EXPLAIN 
@@ -16,5 +14,3 @@ FROM (select x.key, count(1) as cnt from src1 x group by x.key) subq1 JOIN
 group by subq1.key;
 
 select * from dest_j1 x order by x.key;
-
-drop TABLE dest_j1;
diff --git a/ql/src/test/queries/clientpositive/join32.q b/ql/src/test/queries/clientpositive/join32.q
index 8959608dfd..f29d25a94b 100644
--- a/ql/src/test/queries/clientpositive/join32.q
+++ b/ql/src/test/queries/clientpositive/join32.q
@@ -13,5 +13,5 @@ JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join33.q b/ql/src/test/queries/clientpositive/join33.q
index ba58afb48c..7915c57d5d 100644
--- a/ql/src/test/queries/clientpositive/join33.q
+++ b/ql/src/test/queries/clientpositive/join33.q
@@ -13,5 +13,5 @@ JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join34.q b/ql/src/test/queries/clientpositive/join34.q
index 502f775568..ab0ee16083 100644
--- a/ql/src/test/queries/clientpositive/join34.q
+++ b/ql/src/test/queries/clientpositive/join34.q
@@ -1,4 +1,4 @@
-drop table dest_j1;
+
 
 CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;
 
@@ -23,5 +23,5 @@ JOIN src1 x ON (x.key = subq1.key);
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join35.q b/ql/src/test/queries/clientpositive/join35.q
index 22420d7a95..7f3a4868d0 100644
--- a/ql/src/test/queries/clientpositive/join35.q
+++ b/ql/src/test/queries/clientpositive/join35.q
@@ -1,4 +1,4 @@
-drop table dest_j1;
+
 
 CREATE TABLE dest_j1(key STRING, value STRING, val2 INT) STORED AS TEXTFILE;
 
@@ -23,5 +23,5 @@ JOIN src1 x ON (x.key = subq1.key);
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join36.q b/ql/src/test/queries/clientpositive/join36.q
index 6f65335119..9912610da9 100644
--- a/ql/src/test/queries/clientpositive/join36.q
+++ b/ql/src/test/queries/clientpositive/join36.q
@@ -1,8 +1,8 @@
 set hive.mapjoin.numrows = 2;
 
-drop table dest_j1;
-drop table tmp1;
-drop table tmp2;
+
+
+
 
 CREATE TABLE tmp1(key INT, cnt INT);
 CREATE TABLE tmp2(key INT, cnt INT);
@@ -25,5 +25,5 @@ FROM tmp1 x JOIN tmp2 y ON (x.key = y.key);
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join37.q b/ql/src/test/queries/clientpositive/join37.q
index 0338f74d4d..a971f4ae07 100644
--- a/ql/src/test/queries/clientpositive/join37.q
+++ b/ql/src/test/queries/clientpositive/join37.q
@@ -1,6 +1,6 @@
 set hive.mapjoin.numrows = 2;
 
-drop table dest_j1;
+
 
 CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE;
 
@@ -15,5 +15,5 @@ FROM src1 x JOIN src y ON (x.key = y.key);
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join38.q b/ql/src/test/queries/clientpositive/join38.q
index 2631b0ede3..7fbe377585 100644
--- a/ql/src/test/queries/clientpositive/join38.q
+++ b/ql/src/test/queries/clientpositive/join38.q
@@ -1,4 +1,4 @@
-drop table tmp;
+
 
 create table tmp(col0 string, col1 string,col2 string,col3 string,col4 string,col5 string,col6 string,col7 string,col8 string,col9 string,col10 string,col11 string);
 
@@ -17,4 +17,4 @@ SELECT /*+ MAPJOIN(a) */ a.value, b.col5, count(1) as count
 where b.col11 = 111
 group by a.value, b.col5;
 
-drop table tmp;
+
diff --git a/ql/src/test/queries/clientpositive/join39.q b/ql/src/test/queries/clientpositive/join39.q
index 9f23752fbb..30ba369ed4 100644
--- a/ql/src/test/queries/clientpositive/join39.q
+++ b/ql/src/test/queries/clientpositive/join39.q
@@ -1,6 +1,6 @@
 set hive.mapjoin.cache.numrows = 2;
 
-drop table dest_j1;
+
 
 CREATE TABLE dest_j1(key STRING, value STRING, key1 string, val2 STRING) STORED AS TEXTFILE;
 
@@ -11,5 +11,5 @@ FROM src x left outer JOIN (select * from src where key <= 100) y ON (x.key = y.
 
 select * from dest_j1 x order by x.key;
 
-drop table dest_j1;
+
 
diff --git a/ql/src/test/queries/clientpositive/join_hive_626.q b/ql/src/test/queries/clientpositive/join_hive_626.q
index 2f1c0bfef0..31b0c8c91c 100644
--- a/ql/src/test/queries/clientpositive/join_hive_626.q
+++ b/ql/src/test/queries/clientpositive/join_hive_626.q
@@ -1,6 +1,6 @@
-drop table hive_foo;
-drop table hive_bar;
-drop table hive_count;
+
+
+
 
 create table hive_foo (foo_id int, foo_name string, foo_a string, foo_b string, 
 foo_c string, foo_d string) row format delimited fields terminated by ','
@@ -25,6 +25,6 @@ select hive_foo.foo_name, hive_bar.bar_name, n from hive_foo join hive_bar on hi
 hive_bar.foo_id join hive_count on hive_count.bar_id = hive_bar.bar_id;
 
 
-drop table hive_foo;
-drop table hive_bar;
-drop table hive_count;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/join_map_ppr.q b/ql/src/test/queries/clientpositive/join_map_ppr.q
index fbb537601a..d8943c4204 100644
--- a/ql/src/test/queries/clientpositive/join_map_ppr.q
+++ b/ql/src/test/queries/clientpositive/join_map_ppr.q
@@ -35,7 +35,7 @@ WHERE z.ds='2008-04-08' and z.hr=11;
 
 select * from dest_j1 x order by x.key;
 
-drop table src_copy;
-drop table src1_copy;
-drop table dest_j1;
+
+
+
 
diff --git a/ql/src/test/queries/clientpositive/join_rc.q b/ql/src/test/queries/clientpositive/join_rc.q
index 1e500bf1ea..8a7c2d3eeb 100644
--- a/ql/src/test/queries/clientpositive/join_rc.q
+++ b/ql/src/test/queries/clientpositive/join_rc.q
@@ -1,5 +1,5 @@
-drop table join_rc1;
-drop table join_rc2;
+
+
 create table join_rc1(key string, value string) stored as RCFile;
 create table join_rc2(key string, value string) stored as RCFile;
 insert overwrite table join_rc1 select * from src;
@@ -12,5 +12,5 @@ FROM join_rc1 JOIN join_rc2 ON join_rc1.key = join_rc2.key;
 select join_rc1.key, join_rc2.value
 FROM join_rc1 JOIN join_rc2 ON join_rc1.key = join_rc2.key;
 
-drop table join_rc1;
-drop table join_rc2;
+
+
diff --git a/ql/src/test/queries/clientpositive/join_reorder.q b/ql/src/test/queries/clientpositive/join_reorder.q
index 30dd71aa0e..b92a79ba07 100644
--- a/ql/src/test/queries/clientpositive/join_reorder.q
+++ b/ql/src/test/queries/clientpositive/join_reorder.q
@@ -1,6 +1,6 @@
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
+
+
+
 
 CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
 CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
@@ -66,6 +66,6 @@ FROM UNIQUEJOIN
   PRESERVE T3 c (c.key, c.val)
 SELECT /*+ STREAMTABLE(b) */ a.key, b.key, c.key;
 
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/join_reorder2.q b/ql/src/test/queries/clientpositive/join_reorder2.q
index 32beb7efc5..238c0adad3 100644
--- a/ql/src/test/queries/clientpositive/join_reorder2.q
+++ b/ql/src/test/queries/clientpositive/join_reorder2.q
@@ -1,7 +1,7 @@
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
-DROP TABLE T4;
+
+
+
+
 
 CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
 CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
@@ -38,7 +38,7 @@ FROM T1 a JOIN T2 b ON a.key = b.key
           JOIN T4 d ON a.key + 1 = d.key + 1;
 
 
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
-DROP TABLE T4;
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/join_reorder3.q b/ql/src/test/queries/clientpositive/join_reorder3.q
index 21a967dd89..1bda28fbc3 100644
--- a/ql/src/test/queries/clientpositive/join_reorder3.q
+++ b/ql/src/test/queries/clientpositive/join_reorder3.q
@@ -1,7 +1,7 @@
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
-DROP TABLE T4;
+
+
+
+
 
 CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
 CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
@@ -38,7 +38,7 @@ FROM T1 a JOIN T2 b ON a.key = b.key
           JOIN T4 d ON a.key + 1 = d.key + 1;
 
 
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
-DROP TABLE T4;
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/lateral_view.q b/ql/src/test/queries/clientpositive/lateral_view.q
index 0f616a1bb3..bac6da9460 100644
--- a/ql/src/test/queries/clientpositive/lateral_view.q
+++ b/ql/src/test/queries/clientpositive/lateral_view.q
@@ -1,5 +1,5 @@
-DROP TABLE tmp_pyang_lv;
-DROP TABLE tmp_pyang_src_rcfile;
+
+
 
 CREATE TABLE tmp_pyang_lv (inputs string) STORED AS RCFILE;
 INSERT OVERWRITE TABLE tmp_pyang_lv SELECT key FROM src;
@@ -51,5 +51,5 @@ LATERAL VIEW explode(value) myTable AS myCol;
 SELECT value, myCol from (SELECT key, array(value[0]) AS value FROM tmp_pyang_src_rcfile GROUP BY value[0], key) a
 LATERAL VIEW explode(value) myTable AS myCol;
 
-DROP TABLE tmp_pyang_src_rcfile;
-DROP TABLE tmp_pyang_lv;
+
+
diff --git a/ql/src/test/queries/clientpositive/lineage1.q b/ql/src/test/queries/clientpositive/lineage1.q
index 7ead0fb53f..4743b7f413 100644
--- a/ql/src/test/queries/clientpositive/lineage1.q
+++ b/ql/src/test/queries/clientpositive/lineage1.q
@@ -1,4 +1,4 @@
-drop table dest_l1;
+
 
 CREATE TABLE dest_l1(key INT, value STRING) STORED AS TEXTFILE;
 
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part1.q b/ql/src/test/queries/clientpositive/load_dyn_part1.q
index 352c1a7e78..52b4937d4a 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part1.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part1.q
@@ -1,7 +1,7 @@
 show partitions srcpart;
 
-drop table nzhang_part1;
-drop table nzhang_part2;
+
+
 
 create table if not exists nzhang_part1 like srcpart;
 create table if not exists nzhang_part2 like srcpart;
@@ -26,5 +26,5 @@ show partitions nzhang_part2;
 select * from nzhang_part1 where ds is not null and hr is not null;
 select * from nzhang_part2 where ds is not null and hr is not null;
 
-drop table nzhang_part1;
-drop table nzhang_part2;
+
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part10.q b/ql/src/test/queries/clientpositive/load_dyn_part10.q
index effcf59800..9517664675 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part10.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part10.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part10;
+
 
 create table if not exists nzhang_part10 like srcpart;
 describe extended nzhang_part10;
@@ -21,4 +21,4 @@ show partitions nzhang_part10;
 
 select * from nzhang_part10 where ds is not null and hr is not null;
 
-drop table nzhang_part10;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part11.q b/ql/src/test/queries/clientpositive/load_dyn_part11.q
index 65dd985cb2..4cfbfe8eac 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part11.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part11.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part;
+
 create table if not exists nzhang_part like srcpart;
 describe extended nzhang_part;
 
@@ -14,4 +14,4 @@ insert overwrite table nzhang_part partition (ds="2010-03-03", hr) select key, v
 select * from nzhang_part where ds = '2010-03-03' and hr = '11';
 select * from nzhang_part where ds = '2010-03-03' and hr = '12';
 
-drop table nzhang_part;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part12.q b/ql/src/test/queries/clientpositive/load_dyn_part12.q
index ba2f88b63a..dd8bd530af 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part12.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part12.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part12;
+
 
 create table if not exists nzhang_part12 like srcpart;
 describe extended nzhang_part12;
@@ -16,4 +16,4 @@ show partitions nzhang_part12;
 
 select * from nzhang_part12 where ds is not null and hr is not null;
 
-drop table nzhang_part12;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part13.q b/ql/src/test/queries/clientpositive/load_dyn_part13.q
index 2e1ef018fa..0b8d44f7cf 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part13.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part13.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part13;
+
 
 create table if not exists nzhang_part13 like srcpart;
 describe extended nzhang_part13;
@@ -34,4 +34,4 @@ show partitions nzhang_part13;
 
 select * from nzhang_part13 where ds is not null and hr is not null;
 
-drop table nzhang_part13;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part14.q b/ql/src/test/queries/clientpositive/load_dyn_part14.q
index 4b51010fad..bc3b9a5e0c 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part14.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part14.q
@@ -1,4 +1,4 @@
-drop table nzhang_part14;
+
 create table if not exists nzhang_part14 (key string) 
   partitioned by (value string);
 
@@ -32,4 +32,4 @@ show partitions nzhang_part14;
 select * from nzhang_part14 where value <> 'a'
 order by key, value;
 
-drop table nzhang_part14;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part2.q b/ql/src/test/queries/clientpositive/load_dyn_part2.q
index 80cfb8f3ef..03aa5df035 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part2.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part2.q
@@ -1,4 +1,4 @@
-drop table nzhang_part_bucket;
+
 create table if not exists nzhang_part_bucket (key string, value string) 
   partitioned by (ds string, hr string) 
   clustered by (key) into 10 buckets;
@@ -19,5 +19,5 @@ show partitions nzhang_part_bucket;
 select * from nzhang_part_bucket where ds='2010-03-23' and hr='11' order by key;
 select * from nzhang_part_bucket where ds='2010-03-23' and hr='12' order by key;
 
-drop table nzhang_part_bucket;
+
 
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part3.q b/ql/src/test/queries/clientpositive/load_dyn_part3.q
index 0e77e7de87..e4c8c17f63 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part3.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part3.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part3;
+
 
 create table if not exists nzhang_part3 like srcpart;
 describe extended nzhang_part3;
@@ -16,4 +16,4 @@ insert overwrite table nzhang_part3 partition (ds, hr) select key, value, ds, hr
 
 select * from nzhang_part3 where ds is not null and hr is not null;
 
-drop table nzhang_part3;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part4.q b/ql/src/test/queries/clientpositive/load_dyn_part4.q
index 5f5aee8b28..3f3a0c8d51 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part4.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part4.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part4;
+
 
 create table if not exists nzhang_part4 like srcpart;
 describe extended nzhang_part4;
@@ -21,4 +21,4 @@ select * from nzhang_part4 where ds='2008-04-08' and hr is not null;
 
 select * from nzhang_part4 where ds is not null and hr is not null;
 
-drop table nzhang_part4;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part5.q b/ql/src/test/queries/clientpositive/load_dyn_part5.q
index bed98b844b..5780f5d439 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part5.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part5.q
@@ -1,4 +1,4 @@
-drop table nzhang_part5;
+
 
 create table if not exists nzhang_part5 (key string) partitioned by (value string);
 describe extended nzhang_part5;
@@ -19,4 +19,4 @@ show partitions nzhang_part5;
 select * from nzhang_part5 where value='val_0';
 select * from nzhang_part5 where value='val_2';
 
-drop table nzhang_part5;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part6.q b/ql/src/test/queries/clientpositive/load_dyn_part6.q
index bbb874dcd8..b5e85aebf4 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part6.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part6.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part6;
+
 
 create table if not exists nzhang_part6 like srcpart;
 describe extended nzhang_part6;
@@ -13,4 +13,4 @@ insert overwrite table nzhang_part6 partition (ds="2010-03-03", hr) select key,
 
 select * from nzhang_part6 where ds = '2010-03-03' and hr = '11';
 select * from nzhang_part6 where ds = '2010-03-03' and hr = '12';
-drop table nzhang_part6;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part7.q b/ql/src/test/queries/clientpositive/load_dyn_part7.q
index 1cfb95ff4c..b9e1da0326 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part7.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part7.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part7;
+
 
 create table if not exists nzhang_part7 like srcpart;
 describe extended nzhang_part7;
@@ -11,4 +11,4 @@ insert overwrite table nzhang_part7 partition (ds='2010-03-03', hr='12') select
 show partitions nzhang_part7;
 
 select * from nzhang_part7 where ds is not null and hr is not null;
-drop table nzhang_part7;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part8.q b/ql/src/test/queries/clientpositive/load_dyn_part8.q
index 7359931016..8073500c0b 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part8.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part8.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part8;
+
 
 create table if not exists nzhang_part8 like srcpart;
 describe extended nzhang_part8;
@@ -21,4 +21,4 @@ insert overwrite table nzhang_part8 partition(ds='2008-12-31', hr) select key, v
 show partitions nzhang_part8;
 
 select * from nzhang_part8 where ds is not null and hr is not null;
-drop table nzhang_part8;
+
diff --git a/ql/src/test/queries/clientpositive/load_dyn_part9.q b/ql/src/test/queries/clientpositive/load_dyn_part9.q
index 13e6e0fa44..01fa596cdf 100644
--- a/ql/src/test/queries/clientpositive/load_dyn_part9.q
+++ b/ql/src/test/queries/clientpositive/load_dyn_part9.q
@@ -1,6 +1,6 @@
 show partitions srcpart;
 
-drop table nzhang_part9;
+
 
 create table if not exists nzhang_part9 like srcpart;
 describe extended nzhang_part9;
@@ -20,4 +20,4 @@ insert overwrite table nzhang_part9 partition (ds, hr) select key, value, ds, hr
 show partitions nzhang_part9;
 
 select * from nzhang_part9 where ds is not null and hr is not null;
-drop table nzhang_part9;
+
diff --git a/ql/src/test/queries/clientpositive/loadpart1.q b/ql/src/test/queries/clientpositive/loadpart1.q
index 401547af1b..0813bb23c3 100644
--- a/ql/src/test/queries/clientpositive/loadpart1.q
+++ b/ql/src/test/queries/clientpositive/loadpart1.q
@@ -1,5 +1,5 @@
-drop table hive_test_src;
-drop table hive_test_dst;
+
+
 
 create table hive_test_src ( col1 string ) stored as textfile ;
 load data local inpath '../data/files/test.dat' overwrite into table hive_test_src ;
@@ -15,5 +15,5 @@ select * from hive_test_dst where pcol1='test_part';
 select * from hive_test_dst where pcol1='test_part' and pcol2='test_part';
 select * from hive_test_dst where pcol1='test_Part';
 
-drop table hive_test_src;
-drop table hive_test_dst;
+
+
diff --git a/ql/src/test/queries/clientpositive/loadpart_err.q b/ql/src/test/queries/clientpositive/loadpart_err.q
index 8fadc3a556..90510a7c25 100644
--- a/ql/src/test/queries/clientpositive/loadpart_err.q
+++ b/ql/src/test/queries/clientpositive/loadpart_err.q
@@ -1,5 +1,5 @@
 set hive.cli.errors.ignore=true;
-DROP TABLE loadpart1;
+
 CREATE TABLE loadpart1(a STRING, b STRING) PARTITIONED BY (ds STRING);
 
 INSERT OVERWRITE TABLE loadpart1 PARTITION (ds='2009-01-01')
@@ -12,4 +12,4 @@ SHOW PARTITIONS loadpart1;
 LOAD DATA LOCAL INPATH '../data1/files/kv1.txt' INTO TABLE loadpart1 PARTITION(ds='2009-05-05');
 SHOW PARTITIONS loadpart1;
 
-DROP TABLE loadpart1;
+
diff --git a/ql/src/test/queries/clientpositive/merge1.q b/ql/src/test/queries/clientpositive/merge1.q
index c51f682e15..834f2ce374 100644
--- a/ql/src/test/queries/clientpositive/merge1.q
+++ b/ql/src/test/queries/clientpositive/merge1.q
@@ -1,4 +1,3 @@
-drop table dest1;
 set hive.merge.mapredfiles=true;
 
 create table dest1(key int, val int);
@@ -14,7 +13,6 @@ select * from dest1;
 
 drop table dest1;
 
-
 create table test_src(key string, value string) partitioned by (ds string);
 create table dest1(key string);
 
@@ -29,6 +27,3 @@ set hive.merge.smallfiles.avgsize=16;
 explain
 insert overwrite table dest1 select key from test_src;
 insert overwrite table dest1 select key from test_src;
-
-drop table test_src;
-drop table dest1;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/multi_insert.q b/ql/src/test/queries/clientpositive/multi_insert.q
index 642b11d440..944e3bfb12 100644
--- a/ql/src/test/queries/clientpositive/multi_insert.q
+++ b/ql/src/test/queries/clientpositive/multi_insert.q
@@ -1,5 +1,5 @@
-drop table src_multi1;
-drop table src_multi2;
+
+
 create table src_multi1 like src;
 create table src_multi2 like src;
 
diff --git a/ql/src/test/queries/clientpositive/null_column.q b/ql/src/test/queries/clientpositive/null_column.q
index 7646cec552..fa4a863944 100644
--- a/ql/src/test/queries/clientpositive/null_column.q
+++ b/ql/src/test/queries/clientpositive/null_column.q
@@ -1,6 +1,6 @@
-drop table temp_null;
-drop table tt;
-drop table tt_b;
+
+
+
 
 create table temp_null(a int) stored as textfile;
 load data local inpath '../data/files/test.dat' overwrite into table temp_null;
@@ -18,12 +18,12 @@ select * from tt_b;
 insert overwrite directory "../build/ql/test/data/warehouse/null_columns.out" select null, null from temp_null;
 dfs -cat ../build/ql/test/data/warehouse/null_columns.out/*;
 
-drop table temp_null2;
+
 create table temp_null2 (key string, value string) partitioned by (ds string);
 insert overwrite table temp_null2 partition(ds='2010-04-01') select '1',NULL from src limit 1;
 select * from temp_null2 where ds is not null;
 
-drop table tt;
-drop table tt_b;
-drop table temp_null;
-drop table temp_null2;
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/nullgroup3.q b/ql/src/test/queries/clientpositive/nullgroup3.q
index 68ebe55412..a5bc9ff1f3 100644
--- a/ql/src/test/queries/clientpositive/nullgroup3.q
+++ b/ql/src/test/queries/clientpositive/nullgroup3.q
@@ -1,4 +1,3 @@
-DROP TABLE tstparttbl;
 CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE tstparttbl PARTITION (ds='2008-04-09');
 LOAD DATA LOCAL INPATH '../data/files/nullfile.txt' INTO TABLE tstparttbl PARTITION (ds='2008-04-08');
@@ -6,7 +5,6 @@ explain
 select count(1) from tstparttbl;
 select count(1) from tstparttbl;
 
-DROP TABLE tstparttbl2;
 CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/nullfile.txt' INTO TABLE tstparttbl2 PARTITION (ds='2008-04-09');
 LOAD DATA LOCAL INPATH '../data/files/nullfile.txt' INTO TABLE tstparttbl2 PARTITION (ds='2008-04-08');
@@ -28,6 +26,3 @@ LOAD DATA LOCAL INPATH '../data/files/nullfile.txt' INTO TABLE tstparttbl2 PARTI
 explain
 select count(1) from tstparttbl2;
 select count(1) from tstparttbl2;
-
-DROP TABLE tstparttbl;
-DROP TABLE tstparttbl2;
diff --git a/ql/src/test/queries/clientpositive/nullgroup5.q b/ql/src/test/queries/clientpositive/nullgroup5.q
index 200bffcfe9..12773b6159 100644
--- a/ql/src/test/queries/clientpositive/nullgroup5.q
+++ b/ql/src/test/queries/clientpositive/nullgroup5.q
@@ -1,8 +1,8 @@
-DROP TABLE tstparttbl;
+
 CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE tstparttbl PARTITION (ds='2009-04-09');
 
-DROP TABLE tstparttbl2;
+
 CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE tstparttbl2 PARTITION (ds='2009-04-09');
 
@@ -22,5 +22,5 @@ select u.* from
 )u;
 
 
-DROP TABLE tstparttbl;
-DROP TABLE tstparttbl2;
+
+
diff --git a/ql/src/test/queries/clientpositive/nullinput.q b/ql/src/test/queries/clientpositive/nullinput.q
index 119358fb5e..4a58ed514a 100644
--- a/ql/src/test/queries/clientpositive/nullinput.q
+++ b/ql/src/test/queries/clientpositive/nullinput.q
@@ -1,4 +1,4 @@
 create table tstnullinut(a string, b string);
 select x.* from tstnullinut x;
 select x.a, count(1) from tstnullinut x group by x.a;
-drop table tstnullinut;
+
diff --git a/ql/src/test/queries/clientpositive/nullinput2.q b/ql/src/test/queries/clientpositive/nullinput2.q
index a89e084682..f5c0af8fa5 100644
--- a/ql/src/test/queries/clientpositive/nullinput2.q
+++ b/ql/src/test/queries/clientpositive/nullinput2.q
@@ -1,8 +1,8 @@
-drop table nulltbl;
+
 
 create table nulltbl(key int) partitioned by (ds string);
 select key from nulltbl where ds='101';
 
 select count(1) from nulltbl where ds='101';
 
-drop table nulltbl;
+
diff --git a/ql/src/test/queries/clientpositive/nullscript.q b/ql/src/test/queries/clientpositive/nullscript.q
index f62c48f5b7..989527699a 100644
--- a/ql/src/test/queries/clientpositive/nullscript.q
+++ b/ql/src/test/queries/clientpositive/nullscript.q
@@ -1,4 +1,4 @@
-DROP TABLE nullscript;
+
 CREATE TABLE nullscript(KEY STRING, VALUE STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE nullscript;
 LOAD DATA LOCAL INPATH '../data/files/nullfile.txt' INTO TABLE nullscript;
diff --git a/ql/src/test/queries/clientpositive/partition_vs_table_metadata.q b/ql/src/test/queries/clientpositive/partition_vs_table_metadata.q
index fe8529c506..bbfe483a8e 100644
--- a/ql/src/test/queries/clientpositive/partition_vs_table_metadata.q
+++ b/ql/src/test/queries/clientpositive/partition_vs_table_metadata.q
@@ -1,4 +1,4 @@
-drop table partition_vs_table;
+
 
 create table partition_vs_table(key string, value string) partitioned by (ds string);
 
@@ -11,4 +11,3 @@ insert overwrite table partition_vs_table partition(ds='101') select key, value,
 select key, value, newcol from partition_vs_table
 order by key;
 
-drop table partition_vs_table;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/partition_wise_fileformat.q b/ql/src/test/queries/clientpositive/partition_wise_fileformat.q
index fbf71e5de4..f0d58cf61d 100644
--- a/ql/src/test/queries/clientpositive/partition_wise_fileformat.q
+++ b/ql/src/test/queries/clientpositive/partition_wise_fileformat.q
@@ -1,4 +1,4 @@
-drop table partition_test_partitioned;
+
 
 create table partition_test_partitioned(key string, value string) partitioned by (dt string);
 
@@ -29,4 +29,4 @@ select key from partition_test_partitioned where dt=102;
 select key from partition_test_partitioned;
 
 select key from partition_test_partitioned where dt >=100 and dt <= 102;
-drop table partition_test_partitioned;
+
diff --git a/ql/src/test/queries/clientpositive/partition_wise_fileformat2.q b/ql/src/test/queries/clientpositive/partition_wise_fileformat2.q
index 4665217635..ca53aa1248 100644
--- a/ql/src/test/queries/clientpositive/partition_wise_fileformat2.q
+++ b/ql/src/test/queries/clientpositive/partition_wise_fileformat2.q
@@ -1,4 +1,4 @@
-drop table partition_test_partitioned;
+
 
 create table partition_test_partitioned(key string, value string) partitioned by (dt string);
 
@@ -9,4 +9,4 @@ alter table partition_test_partitioned set fileformat Sequencefile;
 insert overwrite table partition_test_partitioned partition(dt=102) select * from src1;
 
 select * from partition_test_partitioned where dt >=100 and dt <= 102;
-drop table partition_test_partitioned;
+
diff --git a/ql/src/test/queries/clientpositive/partition_wise_fileformat3.q b/ql/src/test/queries/clientpositive/partition_wise_fileformat3.q
index f878c5cb82..b85fec538f 100644
--- a/ql/src/test/queries/clientpositive/partition_wise_fileformat3.q
+++ b/ql/src/test/queries/clientpositive/partition_wise_fileformat3.q
@@ -1,4 +1,4 @@
-drop table partition_test_partitioned;
+
 
 create table partition_test_partitioned(key string, value string) partitioned by (dt string);
 
@@ -15,4 +15,4 @@ insert overwrite table partition_test_partitioned partition(dt=101) select * fro
 show table extended like partition_test_partitioned partition(dt=101);
 select key from partition_test_partitioned where dt=101;
 
-drop table partition_test_partitioned;
+
diff --git a/ql/src/test/queries/clientpositive/ppd_constant_expr.q b/ql/src/test/queries/clientpositive/ppd_constant_expr.q
index 57ed682416..0b450aff54 100644
--- a/ql/src/test/queries/clientpositive/ppd_constant_expr.q
+++ b/ql/src/test/queries/clientpositive/ppd_constant_expr.q
@@ -10,4 +10,4 @@ INSERT OVERWRITE TABLE ppd_constant_expr SELECT 4 + NULL, src1.key - NULL, NULL
 
 SELECT ppd_constant_expr.* FROM ppd_constant_expr;
 
-DROP TABLE ppd_constant_expr;
+
diff --git a/ql/src/test/queries/clientpositive/ppd_multi_insert.q b/ql/src/test/queries/clientpositive/ppd_multi_insert.q
index 9a78b5b78a..9e79c179d1 100644
--- a/ql/src/test/queries/clientpositive/ppd_multi_insert.q
+++ b/ql/src/test/queries/clientpositive/ppd_multi_insert.q
@@ -1,7 +1,7 @@
 set hive.optimize.ppd=true;
-DROP TABLE mi1;
-DROP TABLE mi2;
-DROP TABLE mi3;
+
+
+
 CREATE TABLE mi1(key INT, value STRING) STORED AS TEXTFILE;
 CREATE TABLE mi2(key INT, value STRING) STORED AS TEXTFILE;
 CREATE TABLE mi3(key INT) PARTITIONED BY(ds STRING, hr STRING) STORED AS TEXTFILE;
@@ -24,6 +24,6 @@ SELECT mi2.* FROM mi2;
 SELECT mi3.* FROM mi3;
 dfs -cat ../build/ql/test/data/warehouse/mi4.out/*;
 
-DROP TABLE mi1;
-DROP TABLE mi2;
-DROP TABLE mi3;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/progress_1.q b/ql/src/test/queries/clientpositive/progress_1.q
index 06051cb5c7..ad908a02ad 100644
--- a/ql/src/test/queries/clientpositive/progress_1.q
+++ b/ql/src/test/queries/clientpositive/progress_1.q
@@ -1,9 +1,9 @@
 set hive.heartbeat.interval=5; 
 
-DROP TABLE PROGRESS_1;
+
 CREATE TABLE PROGRESS_1(key int, value string) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv6.txt' INTO TABLE PROGRESS_1;
 
 select count(1) from PROGRESS_1 t1 join PROGRESS_1 t2 on t1.key=t2.key;
 
-DROP TABLE PROGRESS_1;
+
diff --git a/ql/src/test/queries/clientpositive/rand_partitionpruner2.q b/ql/src/test/queries/clientpositive/rand_partitionpruner2.q
index bdb9abd98d..e2f280efe7 100644
--- a/ql/src/test/queries/clientpositive/rand_partitionpruner2.q
+++ b/ql/src/test/queries/clientpositive/rand_partitionpruner2.q
@@ -1,5 +1,5 @@
 -- scanning partitioned data
-drop table tmptable;
+
 create table tmptable(key string, value string, hr string, ds string);
 
 explain extended 
@@ -11,4 +11,4 @@ insert overwrite table tmptable
 select a.* from srcpart a where rand(1) < 0.1 and a.ds = '2008-04-08';
 
 select * from tmptable x sort by x.key,x.value,x.ds,x.hr;
-drop table tmptable;
+
diff --git a/ql/src/test/queries/clientpositive/rcfile_bigdata.q b/ql/src/test/queries/clientpositive/rcfile_bigdata.q
index badbbdd0db..70340dc87d 100644
--- a/ql/src/test/queries/clientpositive/rcfile_bigdata.q
+++ b/ql/src/test/queries/clientpositive/rcfile_bigdata.q
@@ -1,7 +1,7 @@
 set hive.map.aggr.hash.percentmemory = 0.3;
 set hive.mapred.local.mem = 256;
 
-DROP TABLE columnTable_Bigdata;
+
 CREATE table columnTable_Bigdata (key STRING, value STRING)
 ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
@@ -15,4 +15,4 @@ INSERT OVERWRITE TABLE columnTable_Bigdata SELECT subq.key, subq.value;
 describe columnTable_Bigdata;
 select count(columnTable_Bigdata.key) from columnTable_Bigdata;
 
-DROP TABLE columnTable_Bigdata;
+
diff --git a/ql/src/test/queries/clientpositive/rcfile_columnar.q b/ql/src/test/queries/clientpositive/rcfile_columnar.q
index 4c806e200e..67b0c1280f 100644
--- a/ql/src/test/queries/clientpositive/rcfile_columnar.q
+++ b/ql/src/test/queries/clientpositive/rcfile_columnar.q
@@ -1,4 +1,4 @@
-DROP TABLE columnTable;
+
 CREATE table columnTable (key STRING, value STRING)
 ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
@@ -12,4 +12,4 @@ describe columnTable;
 
 SELECT columnTable.* FROM columnTable;
 
-DROP TABLE columnTable;
+
diff --git a/ql/src/test/queries/clientpositive/rcfile_default_format.q b/ql/src/test/queries/clientpositive/rcfile_default_format.q
index 84408d4cd5..1106e4cdd4 100644
--- a/ql/src/test/queries/clientpositive/rcfile_default_format.q
+++ b/ql/src/test/queries/clientpositive/rcfile_default_format.q
@@ -14,7 +14,6 @@ SET hive.default.fileformat = TextFile;
 CREATE TABLE textfile_default_format_ctas AS SELECT key,value FROM rcfile_default_format_ctas;
 DESCRIBE EXTENDED textfile_default_format_ctas;
 
-DROP TABLE  rcfile_default_format;
-DROP TABLE  rcfile_default_format_ctas;
-DROP TABLE rcfile_default_format_txtfile;
-DROP TABLE textfile_default_format_ctas;
\ No newline at end of file
+
+
+
diff --git a/ql/src/test/queries/clientpositive/rcfile_lazydecompress.q b/ql/src/test/queries/clientpositive/rcfile_lazydecompress.q
index 6f1255d68d..108cc709e4 100644
--- a/ql/src/test/queries/clientpositive/rcfile_lazydecompress.q
+++ b/ql/src/test/queries/clientpositive/rcfile_lazydecompress.q
@@ -1,4 +1,4 @@
-DROP TABLE rcfileTableLazyDecompress;
+
 CREATE table rcfileTableLazyDecompress (key STRING, value STRING) STORED AS RCFile;
 
 FROM src
@@ -25,4 +25,3 @@ SELECT key, count(1) FROM rcfileTableLazyDecompress where key > 238 group by key
 set mapred.output.compress=false;
 set hive.exec.compress.output=false;
 
-DROP TABLE rcfileTableLazyDecompress;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/rcfile_null_value.q b/ql/src/test/queries/clientpositive/rcfile_null_value.q
index 70fba82dc1..2e4c929fb3 100644
--- a/ql/src/test/queries/clientpositive/rcfile_null_value.q
+++ b/ql/src/test/queries/clientpositive/rcfile_null_value.q
@@ -1,7 +1,7 @@
 CREATE TABLE src1_rc(key STRING, value STRING) STORED AS RCFILE;
 INSERT OVERWRITE TABLE src1_rc SELECT * FROM src1;
 SELECT * FROM src1_rc;
-DROP TABLE src1_rc;
+
 
 CREATE TABLE dest1_rc(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS RCFILE;
 
@@ -36,5 +36,5 @@ INSERT OVERWRITE TABLE dest1_rc SELECT c.c1, c.c2, c.c3, c.c4;
 
 SELECT dest1_rc.* FROM dest1_rc;
 
-DROP TABLE dest1_rc;
+
 
diff --git a/ql/src/test/queries/clientpositive/rcfile_union.q b/ql/src/test/queries/clientpositive/rcfile_union.q
index 4459bdc673..4dc459cb79 100644
--- a/ql/src/test/queries/clientpositive/rcfile_union.q
+++ b/ql/src/test/queries/clientpositive/rcfile_union.q
@@ -1,4 +1,4 @@
-DROP TABLE rcfile_unionTable;
+
 CREATE table rcfile_unionTable (b STRING, c STRING)
 ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
@@ -12,4 +12,3 @@ SELECT b AS cola FROM rcfile_unionTable
 UNION ALL
 SELECT c AS cola FROM rcfile_unionTable) s;
 
-DROP TABLE rcfile_unionTable;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/reduce_deduplicate.q b/ql/src/test/queries/clientpositive/reduce_deduplicate.q
index 29cc55485e..019cb26b7c 100644
--- a/ql/src/test/queries/clientpositive/reduce_deduplicate.q
+++ b/ql/src/test/queries/clientpositive/reduce_deduplicate.q
@@ -2,7 +2,7 @@ set hive.enforce.bucketing = true;
 set hive.exec.reducers.max = 1;
 set hive.exec.script.trust = true;
 
-drop table bucket5_1;
+
 
 CREATE TABLE bucket5_1(key string, value string) CLUSTERED BY (key) INTO 2 BUCKETS;
 explain extended
@@ -15,10 +15,10 @@ select * from src cluster by key;
 select sum(hash(key)),sum(hash(value)) from bucket5_1;
 select sum(hash(key)),sum(hash(value)) from src;
 
-drop table complex_tbl_1;
+
 create table complex_tbl_1(aid string, bid string, t int, ctime string, etime bigint, l string, et string) partitioned by (ds string);
 
-drop table complex_tbl_2;
+
 create table complex_tbl_2(aet string, aes string) partitioned by (ds string);
 
 explain extended
@@ -37,7 +37,7 @@ select s2.* from
 )s
 )s2;
 
-drop table complex_tbl_2;
-drop table complex_tbl_1;
-drop table bucket5_1;
+
+
+
 
diff --git a/ql/src/test/queries/clientpositive/rename_column.q b/ql/src/test/queries/clientpositive/rename_column.q
index e982ebc1ea..ce82ff7d5b 100644
--- a/ql/src/test/queries/clientpositive/rename_column.q
+++ b/ql/src/test/queries/clientpositive/rename_column.q
@@ -22,5 +22,5 @@ DESCRIBE kv_rename_test;
 ALTER TABLE kv_rename_test CHANGE COLUMN a2 a INT AFTER b;
 DESCRIBE kv_rename_test;
 
-DROP TABLE kv_rename_test;
+
 
diff --git a/ql/src/test/queries/clientpositive/repair.q b/ql/src/test/queries/clientpositive/repair.q
index bc097a27ef..5da6a988d7 100644
--- a/ql/src/test/queries/clientpositive/repair.q
+++ b/ql/src/test/queries/clientpositive/repair.q
@@ -1,4 +1,4 @@
-DROP TABLE repairtable;
+
 
 CREATE TABLE repairtable(col STRING) PARTITIONED BY (p1 STRING, p2 STRING);
 
@@ -13,4 +13,4 @@ MSCK REPAIR TABLE repairtable;
 
 MSCK TABLE repairtable;
 
-DROP TABLE repairtable;
+
diff --git a/ql/src/test/queries/clientpositive/sample10.q b/ql/src/test/queries/clientpositive/sample10.q
index 999f1f0676..b7f1958237 100644
--- a/ql/src/test/queries/clientpositive/sample10.q
+++ b/ql/src/test/queries/clientpositive/sample10.q
@@ -21,4 +21,4 @@ select ds, count(1) from srcpartbucket tablesample (bucket 1 out of 2 on key) wh
 
 select * from srcpartbucket where ds is not null;
 
-drop table srcpartbucket;
+
diff --git a/ql/src/test/queries/clientpositive/sample6.q b/ql/src/test/queries/clientpositive/sample6.q
index b7396af35b..ea3511f243 100644
--- a/ql/src/test/queries/clientpositive/sample6.q
+++ b/ql/src/test/queries/clientpositive/sample6.q
@@ -46,5 +46,5 @@ ORDER BY key, value;
 SELECT s.* FROM empty_bucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
 ORDER BY key, value;
 
-drop table empty_bucket;
-drop table dest1;
+
+
diff --git a/ql/src/test/queries/clientpositive/semijoin.q b/ql/src/test/queries/clientpositive/semijoin.q
index aca82a0dff..e55654bca1 100644
--- a/ql/src/test/queries/clientpositive/semijoin.q
+++ b/ql/src/test/queries/clientpositive/semijoin.q
@@ -1,7 +1,7 @@
-drop table t1;
-drop table t2;
-drop table t3;
-drop table t4;
+
+
+
+
 
 create table t1 as select cast(key as int) key, value from src where key <= 10;
 
@@ -77,7 +77,7 @@ select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c
 explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;
 select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;
 
-drop table t1;
-drop table t2;
-drop table t3;
-drop table t4;
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/show_tables.q b/ql/src/test/queries/clientpositive/show_tables.q
index 9ef67f5092..1fa78bfe8d 100644
--- a/ql/src/test/queries/clientpositive/show_tables.q
+++ b/ql/src/test/queries/clientpositive/show_tables.q
@@ -11,5 +11,5 @@ SHOW TABLES 'shtb_test1|shtb_test2';
 
 SHOW TABLES 'shtb_test1|shtb_test2';
 
-DROP TABLE shtb_test1;
-DROP TABLE shtb_test2;
+
+
diff --git a/ql/src/test/queries/clientpositive/skewjoin.q b/ql/src/test/queries/clientpositive/skewjoin.q
index d7eeabe8af..ad917beeef 100644
--- a/ql/src/test/queries/clientpositive/skewjoin.q
+++ b/ql/src/test/queries/clientpositive/skewjoin.q
@@ -1,11 +1,11 @@
 set hive.optimize.skewjoin = true;
 set hive.skewjoin.key = 2;
 
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
-DROP TABLE T4;
-DROP TABLE dest_j1;
+
+
+
+
+
 
 CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
 CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
@@ -131,8 +131,7 @@ SELECT sum(hash(src1.key)), sum(hash(src1.val)), sum(hash(src2.key)) FROM T1 src
 select /*+ mapjoin(v)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k left outer join T1 v on k.key+1=v.key;
 
 
-DROP TABLE dest_j1;
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
-DROP TABLE T4;
\ No newline at end of file
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/smb_mapjoin_1.q b/ql/src/test/queries/clientpositive/smb_mapjoin_1.q
index 0a693822d7..4a707ea3a9 100644
--- a/ql/src/test/queries/clientpositive/smb_mapjoin_1.q
+++ b/ql/src/test/queries/clientpositive/smb_mapjoin_1.q
@@ -1,6 +1,6 @@
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
 
 create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
 create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
@@ -47,6 +47,6 @@ select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a
 select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key;
 
  
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/smb_mapjoin_2.q b/ql/src/test/queries/clientpositive/smb_mapjoin_2.q
index d1de344bf7..5a2b04beb0 100644
--- a/ql/src/test/queries/clientpositive/smb_mapjoin_2.q
+++ b/ql/src/test/queries/clientpositive/smb_mapjoin_2.q
@@ -1,6 +1,6 @@
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
 
 create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
 create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
@@ -47,6 +47,6 @@ select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a
 select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key;
 
  
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/smb_mapjoin_3.q b/ql/src/test/queries/clientpositive/smb_mapjoin_3.q
index 426a37824d..489058122b 100644
--- a/ql/src/test/queries/clientpositive/smb_mapjoin_3.q
+++ b/ql/src/test/queries/clientpositive/smb_mapjoin_3.q
@@ -1,6 +1,6 @@
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
 
 create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
 create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
@@ -47,6 +47,6 @@ select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a
 select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key;
 
  
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/smb_mapjoin_4.q b/ql/src/test/queries/clientpositive/smb_mapjoin_4.q
index b16a2da16a..0817ee3671 100644
--- a/ql/src/test/queries/clientpositive/smb_mapjoin_4.q
+++ b/ql/src/test/queries/clientpositive/smb_mapjoin_4.q
@@ -1,6 +1,6 @@
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
 
 create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
 create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
@@ -66,6 +66,6 @@ select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on
 select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;
 
  
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/smb_mapjoin_5.q b/ql/src/test/queries/clientpositive/smb_mapjoin_5.q
index 6f96585cfe..75aaaf8f4f 100644
--- a/ql/src/test/queries/clientpositive/smb_mapjoin_5.q
+++ b/ql/src/test/queries/clientpositive/smb_mapjoin_5.q
@@ -1,6 +1,6 @@
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
 
 create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
 create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
@@ -66,6 +66,6 @@ select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on
 select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;
 
  
-drop table smb_bucket_3;
-drop table smb_bucket_2;
-drop table smb_bucket_1;
+
+
+
diff --git a/ql/src/test/queries/clientpositive/smb_mapjoin_6.q b/ql/src/test/queries/clientpositive/smb_mapjoin_6.q
index 5dd64ed42b..c4e398c72b 100644
--- a/ql/src/test/queries/clientpositive/smb_mapjoin_6.q
+++ b/ql/src/test/queries/clientpositive/smb_mapjoin_6.q
@@ -2,10 +2,10 @@ set hive.enforce.bucketing = true;
 set hive.enforce.sorting = true;
 set hive.exec.reducers.max = 1;
 
-drop table smb_bucket4_1;
+
 CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS RCFILE;
 
-drop table smb_bucket4_2;
+
 CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS RCFILE;
 
 create table smb_join_results(k1 int, v1 string, k2 int, v2 string);
@@ -71,7 +71,7 @@ explain
 select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key join smb_bucket4_2 c on b.key = c.key where a.key>1000;
 select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key join smb_bucket4_2 c on b.key = c.key where a.key>1000;
 
-drop table smb_join_results;
-drop table normal_join_results;
-drop table smb_bucket4_1;
-drop table smb_bucket4_2;
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/smb_mapjoin_7.q b/ql/src/test/queries/clientpositive/smb_mapjoin_7.q
index 405e261dbd..36775b4af8 100644
--- a/ql/src/test/queries/clientpositive/smb_mapjoin_7.q
+++ b/ql/src/test/queries/clientpositive/smb_mapjoin_7.q
@@ -2,15 +2,15 @@ set hive.enforce.bucketing = true;
 set hive.enforce.sorting = true;
 set hive.exec.reducers.max = 1;
 
-drop table smb_bucket4_1;
+
 CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
 
-drop table smb_bucket4_2;
+
 CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
 
-drop table smb_join_results;
-drop table smb_join_results_empty_bigtable;
-drop table normal_join_results;
+
+
+
 create table smb_join_results(k1 int, v1 string, k2 int, v2 string);
 create table smb_join_results_empty_bigtable(k1 int, v1 string, k2 int, v2 string);
 create table normal_join_results(k1 int, v1 string, k2 int, v2 string);
@@ -47,8 +47,8 @@ select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v
 select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results;
 select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable;
 
-drop table smb_join_results;
-drop table smb_join_results_empty_bigtable;
-drop table normal_join_results;
-drop table smb_bucket4_1;
-drop table smb_bucket4_2;
+
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/smb_mapjoin_8.q b/ql/src/test/queries/clientpositive/smb_mapjoin_8.q
index fdceb724ed..e3eeb8ac45 100644
--- a/ql/src/test/queries/clientpositive/smb_mapjoin_8.q
+++ b/ql/src/test/queries/clientpositive/smb_mapjoin_8.q
@@ -1,7 +1,7 @@
 set hive.enforce.bucketing = true;
 set hive.exec.reducers.max = 1;
 
-drop table smb_bucket_input;
+
 create table smb_bucket_input (key int, value string) stored as rcfile;
 load data local inpath '../data/files/smb_bucket_input.rc' into table smb_bucket_input;
 
@@ -10,11 +10,11 @@ set hive.optimize.bucketmapjoin.sortedmerge = true;
 set hive.enforce.sorting = true;
 set hive.exec.reducers.max = 1;
 
-drop table smb_bucket4_1;
+
 CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS;
-drop table smb_bucket4_2;
+
 CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS;
-drop table smb_bucket4_3;
+
 CREATE TABLE smb_bucket4_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS;
 
 insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=4 or key=2000 or key=4000;
@@ -84,7 +84,6 @@ insert overwrite table smb_bucket4_3 select * from smb_bucket_input where key=50
 select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 full outer join smb_bucket4_3 c on a.key=c.key;
 
-drop table smb_bucket4_1;
-drop table smb_bucket4_2;
-drop table smb_bucket4_3;
-drop table smb_bucket_input;
\ No newline at end of file
+
+
+
diff --git a/ql/src/test/queries/clientpositive/symlink_text_input_format.q b/ql/src/test/queries/clientpositive/symlink_text_input_format.q
index c7ce0abd3e..bb9d6f34ed 100644
--- a/ql/src/test/queries/clientpositive/symlink_text_input_format.q
+++ b/ql/src/test/queries/clientpositive/symlink_text_input_format.q
@@ -1,4 +1,4 @@
-DROP TABLE symlink_text_input_format;
+
 
 EXPLAIN
 CREATE TABLE symlink_text_input_format (key STRING, value STRING) STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat';
@@ -20,5 +20,5 @@ EXPLAIN SELECT count(1) FROM symlink_text_input_format;
 
 SELECT count(1) FROM symlink_text_input_format;
 
-DROP TABLE symlink_text_input_format;
+
 
diff --git a/ql/src/test/queries/clientpositive/tablename_with_select.q b/ql/src/test/queries/clientpositive/tablename_with_select.q
index cd40c2dce5..c48231b66c 100644
--- a/ql/src/test/queries/clientpositive/tablename_with_select.q
+++ b/ql/src/test/queries/clientpositive/tablename_with_select.q
@@ -1,4 +1,4 @@
-DROP TABLE tmp_select;
+
 CREATE TABLE tmp_select(a INT, b STRING);
 DESCRIBE tmp_select;
 
@@ -6,4 +6,4 @@ INSERT OVERWRITE TABLE tmp_select SELECT key, value FROM src;
 
 SELECT a, b FROM tmp_select ORDER BY a;
 
-DROP TABLE tmp_select;
+
diff --git a/ql/src/test/queries/clientpositive/transform1.q b/ql/src/test/queries/clientpositive/transform1.q
index 0ce5aa969e..962077c2ca 100644
--- a/ql/src/test/queries/clientpositive/transform1.q
+++ b/ql/src/test/queries/clientpositive/transform1.q
@@ -1,4 +1,4 @@
-drop table transform1_t1;
+
 create table transform1_t1(a string, b string);
 
 EXPLAIN
@@ -6,9 +6,9 @@ SELECT transform(*) USING 'cat' AS (col array<bigint>) FROM transform1_t1;
 
 SELECT transform(*) USING 'cat' AS (col array<bigint>) FROM transform1_t1;
 
-drop table transform1_t1;
 
-drop table transform1_t2;
+
+
 create table transform1_t2(col array<int>);
 
 insert overwrite table transform1_t2
@@ -19,5 +19,5 @@ SELECT transform('0\0021\0022') USING 'cat' AS (col array<int>) FROM transform1_
 
 SELECT transform('0\0021\0022') USING 'cat' AS (col array<int>) FROM transform1_t2;
 
-drop table transform1_t2;
+
 
diff --git a/ql/src/test/queries/clientpositive/udf_concat_insert1.q b/ql/src/test/queries/clientpositive/udf_concat_insert1.q
index 5a9a6e9203..496f40372d 100644
--- a/ql/src/test/queries/clientpositive/udf_concat_insert1.q
+++ b/ql/src/test/queries/clientpositive/udf_concat_insert1.q
@@ -5,4 +5,3 @@ INSERT OVERWRITE TABLE dest1 SELECT '1234', concat(src.key) WHERE src.key < 100
 
 SELECT dest1.* FROM dest1;
 
-DROP TABLE dest1;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/udf_concat_insert2.q b/ql/src/test/queries/clientpositive/udf_concat_insert2.q
index 16faf8468c..6d1353feac 100644
--- a/ql/src/test/queries/clientpositive/udf_concat_insert2.q
+++ b/ql/src/test/queries/clientpositive/udf_concat_insert2.q
@@ -5,4 +5,4 @@ INSERT OVERWRITE TABLE dest1 SELECT concat('1234', 'abc', 'extra argument'), src
 
 SELECT dest1.* FROM dest1;
 
-DROP TABLE dest1;
+
diff --git a/ql/src/test/queries/clientpositive/udf_field.q b/ql/src/test/queries/clientpositive/udf_field.q
index 7a62d4008f..e995f5cf3c 100644
--- a/ql/src/test/queries/clientpositive/udf_field.q
+++ b/ql/src/test/queries/clientpositive/udf_field.q
@@ -23,7 +23,7 @@ SELECT
   field(4, 1, 2, NULL, 4)
 FROM src LIMIT 1;
 
-DROP TABLE test_table;
+
 CREATE TABLE test_table(col1 STRING, col2 STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE test_table;
 
@@ -40,7 +40,7 @@ select col1,col2,
   field(col1, col2, "66")
 from test_table where col1="86" or col1="66";
 
-DROP TABLE test_table1;
+
 CREATE TABLE test_table1(col1 int, col2 string) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE test_table1;
 
diff --git a/ql/src/test/queries/clientpositive/udf_length.q b/ql/src/test/queries/clientpositive/udf_length.q
index b14c0e836f..b84307970d 100644
--- a/ql/src/test/queries/clientpositive/udf_length.q
+++ b/ql/src/test/queries/clientpositive/udf_length.q
@@ -12,4 +12,3 @@ CREATE TABLE dest1(name STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv4.txt' INTO TABLE dest1;
 EXPLAIN SELECT length(dest1.name) FROM dest1;
 SELECT length(dest1.name) FROM dest1;
-DROP TABLE dest1;
diff --git a/ql/src/test/queries/clientpositive/udf_reverse.q b/ql/src/test/queries/clientpositive/udf_reverse.q
index d042a25604..81f765ec59 100644
--- a/ql/src/test/queries/clientpositive/udf_reverse.q
+++ b/ql/src/test/queries/clientpositive/udf_reverse.q
@@ -13,4 +13,3 @@ DROP TABLE dest1;
 CREATE TABLE dest1(name STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv4.txt' INTO TABLE dest1;
 SELECT count(1) FROM dest1 WHERE reverse(dest1.name) = _UTF-8 0xE993AEE982B5;
-DROP TABLE dest1;
diff --git a/ql/src/test/queries/clientpositive/udf_sentences.q b/ql/src/test/queries/clientpositive/udf_sentences.q
index 89a92094f3..802121371f 100644
--- a/ql/src/test/queries/clientpositive/udf_sentences.q
+++ b/ql/src/test/queries/clientpositive/udf_sentences.q
@@ -5,10 +5,10 @@ SELECT explode(sentences(unhex("486976652065737420756E20657863656C6C656E74206F75
 INSERT OVERWRITE TABLE sent_tmp2
 SELECT explode(val) AS val FROM sent_tmp;
 SELECT hex(val) FROM sent_tmp2;
+
 DROP TABLE sent_tmp;
 DROP TABLE sent_tmp2;
 
-
 CREATE TABLE sent_tmp (val array<string>);
 CREATE TABLE sent_tmp2 (val string);
 INSERT OVERWRITE TABLE sent_tmp
@@ -16,7 +16,7 @@ SELECT explode(sentences(unhex("48697665206973742065696E2061757367657A656963686E
 INSERT OVERWRITE TABLE sent_tmp2
 SELECT explode(val) AS val FROM sent_tmp;
 SELECT hex(val) FROM sent_tmp2;
-DROP TABLE sent_tmp;
-DROP TABLE sent_tmp2;
+
+
 
 SELECT sentences("Hive is an excellent tool for data querying\; and perhaps more versatile than machine translation!! Multiple, ill-formed sentences...confounding punctuation--and yet this UDF still works!!!!") FROM src LIMIT 1;
diff --git a/ql/src/test/queries/clientpositive/union10.q b/ql/src/test/queries/clientpositive/union10.q
index bb7f72f5b4..8a5c5f78c8 100644
--- a/ql/src/test/queries/clientpositive/union10.q
+++ b/ql/src/test/queries/clientpositive/union10.q
@@ -2,7 +2,6 @@ set hive.map.aggr = true;
 
 -- union case: all subqueries are a map-reduce jobs, 3 way union, same input for all sub-queries, followed by filesink
 
-drop table tmptable;
 create table tmptable(key string, value int);
 
 explain 
@@ -24,5 +23,3 @@ insert overwrite table tmptable
 
 select * from tmptable x sort by x.key;
 
-drop table tmptable;
-
diff --git a/ql/src/test/queries/clientpositive/union12.q b/ql/src/test/queries/clientpositive/union12.q
index 412d0dcf92..b0893df97b 100644
--- a/ql/src/test/queries/clientpositive/union12.q
+++ b/ql/src/test/queries/clientpositive/union12.q
@@ -2,7 +2,6 @@ set hive.map.aggr = true;
 
 -- union case: all subqueries are a map-reduce jobs, 3 way union, different inputs for all sub-queries, followed by filesink
 
-drop table tmptable;
 create table tmptable(key string, value int);
 
 explain 
@@ -22,5 +21,3 @@ insert overwrite table tmptable
                                             select 'tst3' as key, count(1) as value from srcbucket s3) unionsrc;
 
 select * from tmptable x sort by x.key;
-
-drop table tmptable;
diff --git a/ql/src/test/queries/clientpositive/union17.q b/ql/src/test/queries/clientpositive/union17.q
index a0dc9905b1..34b0e8cc63 100644
--- a/ql/src/test/queries/clientpositive/union17.q
+++ b/ql/src/test/queries/clientpositive/union17.q
@@ -1,6 +1,3 @@
-drop table DEST1;
-drop table DEST2;
-
 CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE;
 CREATE TABLE DEST2(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;
 
@@ -21,6 +18,3 @@ INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT
 
 SELECT DEST1.* FROM DEST1;
 SELECT DEST2.* FROM DEST2;
-
-drop table DEST1;
-drop table DEST2;
diff --git a/ql/src/test/queries/clientpositive/union18.q b/ql/src/test/queries/clientpositive/union18.q
index aeed36bcf7..6207730963 100644
--- a/ql/src/test/queries/clientpositive/union18.q
+++ b/ql/src/test/queries/clientpositive/union18.q
@@ -1,6 +1,3 @@
-drop table DEST1;
-drop table DEST2;
-
 CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE;
 CREATE TABLE DEST2(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;
 
@@ -21,6 +18,3 @@ INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, unionsrc.value
 
 SELECT DEST1.* FROM DEST1 SORT BY DEST1.key, DEST1.value;
 SELECT DEST2.* FROM DEST2 SORT BY DEST2.key, DEST2.val1, DEST2.val2;
-
-drop table DEST1;
-drop table DEST2;
diff --git a/ql/src/test/queries/clientpositive/union19.q b/ql/src/test/queries/clientpositive/union19.q
index 9c8c012853..dc61b74d02 100644
--- a/ql/src/test/queries/clientpositive/union19.q
+++ b/ql/src/test/queries/clientpositive/union19.q
@@ -1,5 +1,5 @@
-drop table DEST1;
-drop table DEST2;
+
+
 
 CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE;
 CREATE TABLE DEST2(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;
@@ -22,5 +22,5 @@ INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, unionsrc.value
 SELECT DEST1.* FROM DEST1 SORT BY DEST1.key, DEST1.value;
 SELECT DEST2.* FROM DEST2 SORT BY DEST2.key, DEST2.val1, DEST2.val2;
 
-drop table DEST1;
-drop table DEST2;
+
+
diff --git a/ql/src/test/queries/clientpositive/union22.q b/ql/src/test/queries/clientpositive/union22.q
index f7130f3a6c..46eb62d1d4 100644
--- a/ql/src/test/queries/clientpositive/union22.q
+++ b/ql/src/test/queries/clientpositive/union22.q
@@ -1,7 +1,7 @@
-drop table dst_union22;
+
 create table dst_union22(k1 string, k2 string, k3 string, k4 string) partitioned by (ds string);
 
-drop table dst_union22_delta;
+
 create table dst_union22_delta(k0 string, k1 string, k2 string, k3 string, k4 string, k5 string) partitioned by (ds string);
 
 insert overwrite table dst_union22 partition (ds='1')
@@ -40,5 +40,5 @@ subq;
 
 select * from dst_union22 where ds = '2' order by k1;
 
-drop table dst_union22;
-drop table dst_union22_delta;
+
+
diff --git a/ql/src/test/queries/clientpositive/union3.q b/ql/src/test/queries/clientpositive/union3.q
index 067034fe8d..b26a2e2799 100644
--- a/ql/src/test/queries/clientpositive/union3.q
+++ b/ql/src/test/queries/clientpositive/union3.q
@@ -17,7 +17,7 @@ FROM (
   FROM (SELECT * FROM src LIMIT 1) s2
 ) a;
 
-DROP TABLE union_out;
+
 
 CREATE TABLE union_out (id int);
 
diff --git a/ql/src/test/queries/clientpositive/union4.q b/ql/src/test/queries/clientpositive/union4.q
index 98ddfcbb24..ee13e7ad3d 100644
--- a/ql/src/test/queries/clientpositive/union4.q
+++ b/ql/src/test/queries/clientpositive/union4.q
@@ -2,7 +2,7 @@ set hive.map.aggr = true;
 
 -- union case: both subqueries are map-reduce jobs on same input, followed by filesink
 
-drop table tmptable;
+
 create table tmptable(key string, value int);
 
 explain 
@@ -18,4 +18,4 @@ select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as valu
 
 select * from tmptable x sort by x.key;
 
-drop table tmptable;
+
diff --git a/ql/src/test/queries/clientpositive/union6.q b/ql/src/test/queries/clientpositive/union6.q
index 06d54fb61b..b793002821 100644
--- a/ql/src/test/queries/clientpositive/union6.q
+++ b/ql/src/test/queries/clientpositive/union6.q
@@ -2,7 +2,7 @@ set hive.map.aggr = true;
 
 -- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by filesink
 
-drop table tmptable;
+
 create table tmptable(key string, value string);
 
 explain 
@@ -18,4 +18,4 @@ select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, cast(count(1) as
 
 select * from tmptable x sort by x.key, x.value;
 
-drop table tmptable;
+
diff --git a/ql/src/test/queries/clientpositive/uniquejoin.q b/ql/src/test/queries/clientpositive/uniquejoin.q
index d97abb2724..51bcf22bfa 100644
--- a/ql/src/test/queries/clientpositive/uniquejoin.q
+++ b/ql/src/test/queries/clientpositive/uniquejoin.q
@@ -1,7 +1,3 @@
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
-
 CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
 CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
 CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;
@@ -27,7 +23,3 @@ SELECT a.key, b.key, c.key;
 
 FROM UNIQUEJOIN PRESERVE T1 a (a.key), T2 b(b.key)
 SELECT a.key, b.key;
-
-DROP TABLE T1;
-DROP TABLE T2;
-DROP TABLE T3;
diff --git a/ql/src/test/results/clientnegative/addpart1.q.out b/ql/src/test/results/clientnegative/addpart1.q.out
index dc46c6b715..983ad7a38f 100644
--- a/ql/src/test/results/clientnegative/addpart1.q.out
+++ b/ql/src/test/results/clientnegative/addpart1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table addpart1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table addpart1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table addpart1 (a int) partitioned by (b string, c string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table addpart1 (a int) partitioned by (b string, c string)
diff --git a/ql/src/test/results/clientnegative/alter_non_native.q.out b/ql/src/test/results/clientnegative/alter_non_native.q.out
index f57b646ed6..d6799bfbb7 100644
--- a/ql/src/test/results/clientnegative/alter_non_native.q.out
+++ b/ql/src/test/results/clientnegative/alter_non_native.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE non_native1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE non_native1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE non_native1(key int, value string) 
 STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler'
 PREHOOK: type: CREATETABLE
diff --git a/ql/src/test/results/clientnegative/altern1.q.out b/ql/src/test/results/clientnegative/altern1.q.out
index a93cfc4a48..565e59eebe 100644
--- a/ql/src/test/results/clientnegative/altern1.q.out
+++ b/ql/src/test/results/clientnegative/altern1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table altern1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table altern1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table altern1(a int, b int) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table altern1(a int, b int) partitioned by (ds string)
diff --git a/ql/src/test/results/clientnegative/create_insert_outputformat.q.out b/ql/src/test/results/clientnegative/create_insert_outputformat.q.out
index b9abc92837..037b64cfd6 100644
--- a/ql/src/test/results/clientnegative/create_insert_outputformat.q.out
+++ b/ql/src/test/results/clientnegative/create_insert_outputformat.q.out
@@ -1,5 +1 @@
-PREHOOK: query: DROP TABLE table_test_output_fomat
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table_test_output_fomat
-POSTHOOK: type: DROPTABLE
 FAILED: Error in semantic analysis: Output Format must implement HiveOutputFormat, otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat
diff --git a/ql/src/test/results/clientnegative/create_view_failure1.q.out b/ql/src/test/results/clientnegative/create_view_failure1.q.out
index e4be8ea4ee..43cded458b 100644
--- a/ql/src/test/results/clientnegative/create_view_failure1.q.out
+++ b/ql/src/test/results/clientnegative/create_view_failure1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE xxx12
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE xxx12
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: DROP VIEW xxx12
 PREHOOK: type: DROPVIEW
 POSTHOOK: query: DROP VIEW xxx12
@@ -15,6 +11,6 @@ POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@xxx12
 PREHOOK: query: CREATE VIEW xxx12 AS SELECT key FROM src
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/tmp/194996627/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-43-17_176_344624644454180304/10000
 FAILED: Error in metadata: AlreadyExistsException(message:Table xxx12 already exists)
 FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
diff --git a/ql/src/test/results/clientnegative/create_view_failure2.q.out b/ql/src/test/results/clientnegative/create_view_failure2.q.out
index 29e1db94c7..a038067e6e 100644
--- a/ql/src/test/results/clientnegative/create_view_failure2.q.out
+++ b/ql/src/test/results/clientnegative/create_view_failure2.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE xxx4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE xxx4
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: DROP VIEW xxx4
 PREHOOK: type: DROPVIEW
 POSTHOOK: query: DROP VIEW xxx4
@@ -9,11 +5,11 @@ POSTHOOK: type: DROPVIEW
 PREHOOK: query: -- views and tables share the same namespace
 CREATE VIEW xxx4 AS SELECT key FROM src
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/tmp/1327466632/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-43-17_460_630133670553073559/10000
 POSTHOOK: query: -- views and tables share the same namespace
 CREATE VIEW xxx4 AS SELECT key FROM src
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/tmp/1327466632/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-43-17_460_630133670553073559/10000
 POSTHOOK: Output: default@xxx4
 PREHOOK: query: CREATE TABLE xxx4(key int)
 PREHOOK: type: CREATETABLE
diff --git a/ql/src/test/results/clientnegative/ctas.q.out b/ql/src/test/results/clientnegative/ctas.q.out
index 53cd41728b..a39fb7c5b6 100644
--- a/ql/src/test/results/clientnegative/ctas.q.out
+++ b/ql/src/test/results/clientnegative/ctas.q.out
@@ -1,5 +1 @@
-PREHOOK: query: drop table nzhang_ctas4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas4
-POSTHOOK: type: DROPTABLE
 FAILED: Error in semantic analysis: CREATE-TABLE-AS-SELECT cannot create external table.
diff --git a/ql/src/test/results/clientnegative/ddltime.q.out b/ql/src/test/results/clientnegative/ddltime.q.out
index 5ce1d79833..11d00c3b9f 100644
--- a/ql/src/test/results/clientnegative/ddltime.q.out
+++ b/ql/src/test/results/clientnegative/ddltime.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table T2 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table T2 like srcpart
diff --git a/ql/src/test/results/clientnegative/deletejar.q.out b/ql/src/test/results/clientnegative/deletejar.q.out
index e043d776e6..4944fd92db 100644
--- a/ql/src/test/results/clientnegative/deletejar.q.out
+++ b/ql/src/test/results/clientnegative/deletejar.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE DELETEJAR
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE DELETEJAR
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE DELETEJAR(KEY STRING, VALUE STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.TestSerDe' STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 FAILED: Error in metadata: Cannot validate serde: org.apache.hadoop.hive.serde2.TestSerDe
diff --git a/ql/src/test/results/clientnegative/drop_view_failure1.q.out b/ql/src/test/results/clientnegative/drop_view_failure1.q.out
index 42c39c38fa..dab13a6321 100644
--- a/ql/src/test/results/clientnegative/drop_view_failure1.q.out
+++ b/ql/src/test/results/clientnegative/drop_view_failure1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE xxx1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE xxx1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE xxx1(key int)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE xxx1(key int)
diff --git a/ql/src/test/results/clientnegative/dyn_part1.q.out b/ql/src/test/results/clientnegative/dyn_part1.q.out
index 1445f5f703..21e46ad5e9 100644
--- a/ql/src/test/results/clientnegative/dyn_part1.q.out
+++ b/ql/src/test/results/clientnegative/dyn_part1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dynamic_partition
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dynamic_partition
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table dynamic_partition (key string) partitioned by (value string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dynamic_partition (key string) partitioned by (value string)
diff --git a/ql/src/test/results/clientnegative/dyn_part2.q.out b/ql/src/test/results/clientnegative/dyn_part2.q.out
index 4fd3a1eae5..636096d1bf 100644
--- a/ql/src/test/results/clientnegative/dyn_part2.q.out
+++ b/ql/src/test/results/clientnegative/dyn_part2.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table nzhang_part1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table nzhang_part1 (key string, value string) partitioned by (ds string, hr string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table nzhang_part1 (key string, value string) partitioned by (ds string, hr string)
diff --git a/ql/src/test/results/clientnegative/external1.q.out b/ql/src/test/results/clientnegative/external1.q.out
index 21c386fd3b..aab7b9df00 100644
--- a/ql/src/test/results/clientnegative/external1.q.out
+++ b/ql/src/test/results/clientnegative/external1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table external1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table external1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create external table external1(a int, b int) location 'invalidscheme://data.s3ndemo.hive/kv'
 PREHOOK: type: CREATETABLE
 FAILED: Error in metadata: MetaException(message:Got exception: java.io.IOException No FileSystem for scheme: invalidscheme)
diff --git a/ql/src/test/results/clientnegative/external2.q.out b/ql/src/test/results/clientnegative/external2.q.out
index 21d7c9ded1..baebbef75b 100644
--- a/ql/src/test/results/clientnegative/external2.q.out
+++ b/ql/src/test/results/clientnegative/external2.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table external2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table external2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create external table external2(a int, b int) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create external table external2(a int, b int) partitioned by (ds string)
diff --git a/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out b/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out
index a4b728f5ca..d091d8ca43 100644
--- a/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out
+++ b/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out
@@ -1,5 +1 @@
-PREHOOK: query: DROP TABLE inv_valid_tbl1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE inv_valid_tbl1
-POSTHOOK: type: DROPTABLE
 FAILED: Error in semantic analysis: DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use STRING instead.
diff --git a/ql/src/test/results/clientnegative/invalidate_view1.q.out b/ql/src/test/results/clientnegative/invalidate_view1.q.out
index 7ee70ed59b..3097b4cdb2 100644
--- a/ql/src/test/results/clientnegative/invalidate_view1.q.out
+++ b/ql/src/test/results/clientnegative/invalidate_view1.q.out
@@ -6,10 +6,6 @@ PREHOOK: query: DROP VIEW xxx9
 PREHOOK: type: DROPVIEW
 POSTHOOK: query: DROP VIEW xxx9
 POSTHOOK: type: DROPVIEW
-PREHOOK: query: DROP TABLE xxx10
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE xxx10
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: -- create two levels of view reference, then invalidate intermediate view
 -- by dropping a column from underlying table, and verify that
 -- querying outermost view results in full error context
@@ -23,17 +19,17 @@ POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@xxx10
 PREHOOK: query: CREATE VIEW xxx9 AS SELECT * FROM xxx10
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/tmp/1720348561/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-43-31_670_5934534815948379913/10000
 POSTHOOK: query: CREATE VIEW xxx9 AS SELECT * FROM xxx10
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/tmp/1720348561/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-43-31_670_5934534815948379913/10000
 POSTHOOK: Output: default@xxx9
 PREHOOK: query: CREATE VIEW xxx8 AS SELECT * FROM xxx9 xxx
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/tmp/1891844844/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-43-31_706_855643045189903710/10000
 POSTHOOK: query: CREATE VIEW xxx8 AS SELECT * FROM xxx9 xxx
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/tmp/1891844844/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-43-31_706_855643045189903710/10000
 POSTHOOK: Output: default@xxx8
 PREHOOK: query: ALTER TABLE xxx10 REPLACE COLUMNS (key int)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
diff --git a/ql/src/test/results/clientnegative/load_non_native.q.out b/ql/src/test/results/clientnegative/load_non_native.q.out
index 5d3e8371b6..400466de3b 100644
--- a/ql/src/test/results/clientnegative/load_non_native.q.out
+++ b/ql/src/test/results/clientnegative/load_non_native.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE non_native2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE non_native2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE non_native2(key int, value string) 
 STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler'
 PREHOOK: type: CREATETABLE
diff --git a/ql/src/test/results/clientnegative/load_wrong_fileformat.q.out b/ql/src/test/results/clientnegative/load_wrong_fileformat.q.out
index e2e7f94eba..645e143ba8 100644
--- a/ql/src/test/results/clientnegative/load_wrong_fileformat.q.out
+++ b/ql/src/test/results/clientnegative/load_wrong_fileformat.q.out
@@ -1,16 +1,14 @@
 PREHOOK: query: -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE load_wrong_fileformat_T1
-PREHOOK: type: DROPTABLE
+
+CREATE TABLE load_wrong_fileformat_T1(name STRING) STORED AS SEQUENCEFILE
+PREHOOK: type: CREATETABLE
 POSTHOOK: query: -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE load_wrong_fileformat_T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE load_wrong_fileformat_T1(name STRING) STORED AS SEQUENCEFILE
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE load_wrong_fileformat_T1(name STRING) STORED AS SEQUENCEFILE
+
+CREATE TABLE load_wrong_fileformat_T1(name STRING) STORED AS SEQUENCEFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@load_wrong_fileformat_T1
 PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE load_wrong_fileformat_T1
diff --git a/ql/src/test/results/clientnegative/load_wrong_fileformat_rc_seq.q.out b/ql/src/test/results/clientnegative/load_wrong_fileformat_rc_seq.q.out
index 841c2bdaf1..4809d3103b 100644
--- a/ql/src/test/results/clientnegative/load_wrong_fileformat_rc_seq.q.out
+++ b/ql/src/test/results/clientnegative/load_wrong_fileformat_rc_seq.q.out
@@ -1,16 +1,14 @@
 PREHOOK: query: -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE T1
-PREHOOK: type: DROPTABLE
+
+CREATE TABLE T1(name STRING) STORED AS RCFILE
+PREHOOK: type: CREATETABLE
 POSTHOOK: query: -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE T1(name STRING) STORED AS RCFILE
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE T1(name STRING) STORED AS RCFILE
+
+CREATE TABLE T1(name STRING) STORED AS RCFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@T1
 PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv1.seq' INTO TABLE T1
diff --git a/ql/src/test/results/clientnegative/load_wrong_fileformat_txt_seq.q.out b/ql/src/test/results/clientnegative/load_wrong_fileformat_txt_seq.q.out
index f54adcaa1f..9b1ea487af 100644
--- a/ql/src/test/results/clientnegative/load_wrong_fileformat_txt_seq.q.out
+++ b/ql/src/test/results/clientnegative/load_wrong_fileformat_txt_seq.q.out
@@ -1,16 +1,14 @@
 PREHOOK: query: -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE T1
-PREHOOK: type: DROPTABLE
+
+CREATE TABLE T1(name STRING) STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
 POSTHOOK: query: -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE T1(name STRING) STORED AS TEXTFILE
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE T1(name STRING) STORED AS TEXTFILE
+
+CREATE TABLE T1(name STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@T1
 PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv1.seq' INTO TABLE T1
diff --git a/ql/src/test/results/clientnegative/nopart_insert.q.out b/ql/src/test/results/clientnegative/nopart_insert.q.out
index e00bae3d4d..46e724fb52 100644
--- a/ql/src/test/results/clientnegative/nopart_insert.q.out
+++ b/ql/src/test/results/clientnegative/nopart_insert.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE nopart_insert
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE nopart_insert
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE nopart_insert(a STRING, b STRING) PARTITIONED BY (ds STRING)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE nopart_insert(a STRING, b STRING) PARTITIONED BY (ds STRING)
diff --git a/ql/src/test/results/clientnegative/nopart_load.q.out b/ql/src/test/results/clientnegative/nopart_load.q.out
index add311d8e9..84868e4e77 100644
--- a/ql/src/test/results/clientnegative/nopart_load.q.out
+++ b/ql/src/test/results/clientnegative/nopart_load.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE nopart_load
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE nopart_load
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE nopart_load(a STRING, b STRING) PARTITIONED BY (ds STRING)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE nopart_load(a STRING, b STRING) PARTITIONED BY (ds STRING)
diff --git a/ql/src/test/results/clientnegative/smb_bucketmapjoin.q.out b/ql/src/test/results/clientnegative/smb_bucketmapjoin.q.out
index 77ca59ae99..5f0e654c21 100644
--- a/ql/src/test/results/clientnegative/smb_bucketmapjoin.q.out
+++ b/ql/src/test/results/clientnegative/smb_bucketmapjoin.q.out
@@ -1,16 +1,8 @@
-PREHOOK: query: drop table smb_bucket4_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@smb_bucket4_1
-PREHOOK: query: drop table smb_bucket4_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS
diff --git a/ql/src/test/results/clientnegative/union2.q.out b/ql/src/test/results/clientnegative/union2.q.out
index 49cff52ffe..1e6c782fbf 100644
--- a/ql/src/test/results/clientnegative/union2.q.out
+++ b/ql/src/test/results/clientnegative/union2.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table union2_t1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table union2_t1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table union2_t2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table union2_t2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists union2_t1(r string, c string, v string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists union2_t1(r string, c string, v string)
diff --git a/ql/src/test/results/clientpositive/add_part_exist.q.out b/ql/src/test/results/clientpositive/add_part_exist.q.out
index 47edf68fb1..4bcdf8a391 100644
--- a/ql/src/test/results/clientpositive/add_part_exist.q.out
+++ b/ql/src/test/results/clientpositive/add_part_exist.q.out
@@ -49,8 +49,3 @@ POSTHOOK: type: SHOWPARTITIONS
 ds=2010-01-01
 ds=2010-01-02
 ds=2010-01-03
-PREHOOK: query: DROP TABLE add_part_test
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE add_part_test
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@add_part_test
diff --git a/ql/src/test/results/clientpositive/alter1.q.out b/ql/src/test/results/clientpositive/alter1.q.out
index 648459cf8f..ea143a6c51 100644
--- a/ql/src/test/results/clientpositive/alter1.q.out
+++ b/ql/src/test/results/clientpositive/alter1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table alter1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table alter1(a int, b int)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table alter1(a int, b int)
@@ -14,7 +10,7 @@ POSTHOOK: type: DESCTABLE
 a	int	
 b	int	
 	 	 
-Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jsichi, createTime:1272933245, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1272933245}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jssarma, createTime:1279735687, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279735687}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: alter table alter1 set tblproperties ('a'='1', 'c'='3')
 PREHOOK: type: ALTERTABLE_PROPERTIES
 POSTHOOK: query: alter table alter1 set tblproperties ('a'='1', 'c'='3')
@@ -28,7 +24,7 @@ POSTHOOK: type: DESCTABLE
 a	int	
 b	int	
 	 	 
-Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jsichi, createTime:1272933245, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=jsichi,c=3,last_modified_time=1272933245,a=1,transient_lastDdlTime=1272933245}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jssarma, createTime:1279735687, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=jssarma, c=3, last_modified_time=1279735688, a=1, transient_lastDdlTime=1279735688}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: alter table alter1 set tblproperties ('a'='1', 'c'='4', 'd'='3')
 PREHOOK: type: ALTERTABLE_PROPERTIES
 POSTHOOK: query: alter table alter1 set tblproperties ('a'='1', 'c'='4', 'd'='3')
@@ -42,7 +38,7 @@ POSTHOOK: type: DESCTABLE
 a	int	
 b	int	
 	 	 
-Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jsichi, createTime:1272933245, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{d=3,last_modified_by=jsichi,c=4,last_modified_time=1272933245,a=1,transient_lastDdlTime=1272933245}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jssarma, createTime:1279735687, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{d=3, last_modified_by=jssarma, c=4, last_modified_time=1279735688, a=1, transient_lastDdlTime=1279735688}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: alter table alter1 set tblproperties ('EXTERNAL'='TRUE')
 PREHOOK: type: ALTERTABLE_PROPERTIES
 POSTHOOK: query: alter table alter1 set tblproperties ('EXTERNAL'='TRUE')
@@ -56,7 +52,7 @@ POSTHOOK: type: DESCTABLE
 a	int	
 b	int	
 	 	 
-Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jsichi, createTime:1272933245, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{d=3,EXTERNAL=TRUE,last_modified_by=jsichi,c=4,last_modified_time=1272933245,a=1,transient_lastDdlTime=1272933245}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)	
+Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jssarma, createTime:1279735687, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=TRUE, d=3, last_modified_by=jssarma, c=4, last_modified_time=1279735688, a=1, transient_lastDdlTime=1279735688}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)	
 PREHOOK: query: alter table alter1 set tblproperties ('EXTERNAL'='FALSE')
 PREHOOK: type: ALTERTABLE_PROPERTIES
 POSTHOOK: query: alter table alter1 set tblproperties ('EXTERNAL'='FALSE')
@@ -70,7 +66,7 @@ POSTHOOK: type: DESCTABLE
 a	int	
 b	int	
 	 	 
-Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jsichi, createTime:1272933245, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{d=3,EXTERNAL=FALSE,last_modified_by=jsichi,c=4,last_modified_time=1272933246,a=1,transient_lastDdlTime=1272933246}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jssarma, createTime:1279735687, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, d=3, last_modified_by=jssarma, c=4, last_modified_time=1279735688, a=1, transient_lastDdlTime=1279735688}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: alter table alter1 set serdeproperties('s1'='9')
 PREHOOK: type: ALTERTABLE_SERDEPROPERTIES
 POSTHOOK: query: alter table alter1 set serdeproperties('s1'='9')
@@ -84,7 +80,7 @@ POSTHOOK: type: DESCTABLE
 a	int	
 b	int	
 	 	 
-Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jsichi, createTime:1272933245, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{s1=9,serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{d=3,EXTERNAL=FALSE,last_modified_by=jsichi,c=4,last_modified_time=1272933246,a=1,transient_lastDdlTime=1272933246}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jssarma, createTime:1279735687, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{s1=9, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, d=3, last_modified_by=jssarma, c=4, last_modified_time=1279735688, a=1, transient_lastDdlTime=1279735688}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: alter table alter1 set serdeproperties('s1'='10', 's2' ='20')
 PREHOOK: type: ALTERTABLE_SERDEPROPERTIES
 POSTHOOK: query: alter table alter1 set serdeproperties('s1'='10', 's2' ='20')
@@ -98,7 +94,7 @@ POSTHOOK: type: DESCTABLE
 a	int	
 b	int	
 	 	 
-Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jsichi, createTime:1272933245, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{s2=20,s1=10,serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{d=3,EXTERNAL=FALSE,last_modified_by=jsichi,c=4,last_modified_time=1272933246,a=1,transient_lastDdlTime=1272933246}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jssarma, createTime:1279735687, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{s2=20, s1=10, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, d=3, last_modified_by=jssarma, c=4, last_modified_time=1279735689, a=1, transient_lastDdlTime=1279735689}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: alter table alter1 set serde 'org.apache.hadoop.hive.serde2.TestSerDe' with serdeproperties('s1'='9')
 PREHOOK: type: ALTERTABLE_SERIALIZER
 POSTHOOK: query: alter table alter1 set serde 'org.apache.hadoop.hive.serde2.TestSerDe' with serdeproperties('s1'='9')
@@ -112,7 +108,7 @@ POSTHOOK: type: DESCTABLE
 a	string	from deserializer
 b	string	from deserializer
 	 	 
-Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jsichi, createTime:1272933245, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:from deserializer), FieldSchema(name:b, type:int, comment:from deserializer)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.TestSerDe, parameters:{s2=20,s1=9,serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{d=3,EXTERNAL=FALSE,last_modified_by=jsichi,c=4,last_modified_time=1272933246,a=1,transient_lastDdlTime=1272933246}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jssarma, createTime:1279735687, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:from deserializer), FieldSchema(name:b, type:int, comment:from deserializer)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.TestSerDe, parameters:{s2=20, s1=9, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, d=3, last_modified_by=jssarma, c=4, last_modified_time=1279735689, a=1, transient_lastDdlTime=1279735689}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: alter table alter1 set serde 'org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe'
 PREHOOK: type: ALTERTABLE_SERIALIZER
 POSTHOOK: query: alter table alter1 set serde 'org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe'
@@ -126,7 +122,7 @@ POSTHOOK: type: DESCTABLE
 a	string	from deserializer
 b	string	from deserializer
 	 	 
-Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jsichi, createTime:1272933245, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:from deserializer), FieldSchema(name:b, type:string, comment:from deserializer)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{s2=20,s1=9,serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{d=3,EXTERNAL=FALSE,last_modified_by=jsichi,c=4,last_modified_time=1272933246,a=1,transient_lastDdlTime=1272933246}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:alter1, dbName:default, owner:jssarma, createTime:1279735687, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:from deserializer), FieldSchema(name:b, type:string, comment:from deserializer)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{s2=20, s1=9, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, d=3, last_modified_by=jssarma, c=4, last_modified_time=1279735689, a=1, transient_lastDdlTime=1279735689}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: alter table alter1 replace columns (a int, b int, c string)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
 POSTHOOK: query: alter table alter1 replace columns (a int, b int, c string)
@@ -140,8 +136,3 @@ POSTHOOK: type: DESCTABLE
 a	int	
 b	int	
 c	string	
-PREHOOK: query: drop table alter1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@alter1
diff --git a/ql/src/test/results/clientpositive/alter2.q.out b/ql/src/test/results/clientpositive/alter2.q.out
index 89125ce1a5..6de5f19064 100644
--- a/ql/src/test/results/clientpositive/alter2.q.out
+++ b/ql/src/test/results/clientpositive/alter2.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table alter2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table alter2(a int, b int) partitioned by (insertdate string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table alter2(a int, b int) partitioned by (insertdate string)
@@ -15,7 +11,7 @@ a	int
 b	int	
 insertdate	string	
 	 	 
-Detailed Table Information	Table(tableName:alter2, dbName:default, owner:njain, createTime:1253779701, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{})	
+Detailed Table Information	Table(tableName:alter2, dbName:default, owner:jssarma, createTime:1279735690, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{transient_lastDdlTime=1279735690}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: show partitions alter2
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: show partitions alter2
@@ -33,7 +29,7 @@ a	int
 b	int	
 insertdate	string	
 	 	 
-Detailed Table Information	Table(tableName:alter2, dbName:default, owner:njain, createTime:1253779701, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{})	
+Detailed Table Information	Table(tableName:alter2, dbName:default, owner:jssarma, createTime:1279735690, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{transient_lastDdlTime=1279735690}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: show partitions alter2
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: show partitions alter2
@@ -52,7 +48,7 @@ a	int
 b	int	
 insertdate	string	
 	 	 
-Detailed Table Information	Table(tableName:alter2, dbName:default, owner:njain, createTime:1253779701, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{})	
+Detailed Table Information	Table(tableName:alter2, dbName:default, owner:jssarma, createTime:1279735690, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{transient_lastDdlTime=1279735690}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: show partitions alter2
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: show partitions alter2
@@ -77,7 +73,7 @@ a	int
 b	int	
 insertdate	string	
 	 	 
-Detailed Table Information	Table(tableName:alter2, dbName:default, owner:njain, createTime:1253779702, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{EXTERNAL=TRUE})	
+Detailed Table Information	Table(tableName:alter2, dbName:default, owner:jssarma, createTime:1279735690, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1279735690}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)	
 PREHOOK: query: show partitions alter2
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: show partitions alter2
@@ -95,7 +91,7 @@ a	int
 b	int	
 insertdate	string	
 	 	 
-Detailed Table Information	Table(tableName:alter2, dbName:default, owner:njain, createTime:1253779702, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{EXTERNAL=TRUE})	
+Detailed Table Information	Table(tableName:alter2, dbName:default, owner:jssarma, createTime:1279735690, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1279735690}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)	
 PREHOOK: query: show partitions alter2
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: show partitions alter2
@@ -114,15 +110,10 @@ a	int
 b	int	
 insertdate	string	
 	 	 
-Detailed Table Information	Table(tableName:alter2, dbName:default, owner:njain, createTime:1253779702, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{EXTERNAL=TRUE})	
+Detailed Table Information	Table(tableName:alter2, dbName:default, owner:jssarma, createTime:1279735690, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:int, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:insertdate, type:string, comment:null)], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1279735690}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)	
 PREHOOK: query: show partitions alter2
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: show partitions alter2
 POSTHOOK: type: SHOWPARTITIONS
 insertdate=2008-01-01
 insertdate=2008-01-02
-PREHOOK: query: drop table alter2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@alter2
diff --git a/ql/src/test/results/clientpositive/alter3.q.out b/ql/src/test/results/clientpositive/alter3.q.out
index 0f4e5a2533..2a1f4d54f2 100644
--- a/ql/src/test/results/clientpositive/alter3.q.out
+++ b/ql/src/test/results/clientpositive/alter3.q.out
@@ -1,19 +1,3 @@
-PREHOOK: query: drop table alter3_src
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter3_src
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table alter3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table alter3_renamed
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter3_renamed
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table alter3_like_renamed
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter3_like_renamed
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table alter3_src ( col1 string ) stored as textfile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table alter3_src ( col1 string ) stored as textfile
@@ -46,11 +30,11 @@ POSTHOOK: Lineage: alter3 PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE
 PREHOOK: query: select * from alter3 where pcol1='test_part' and pcol2='test_part'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter3@pcol1=test_part/pcol2=test_part
-PREHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/scratchdir/hive_2010-05-03_13-59-16_257_4588179538253670650/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-15_543_4992541295505544702/10000
 POSTHOOK: query: select * from alter3 where pcol1='test_part' and pcol2='test_part'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter3@pcol1=test_part/pcol2=test_part
-POSTHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/scratchdir/hive_2010-05-03_13-59-16_257_4588179538253670650/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-15_543_4992541295505544702/10000
 POSTHOOK: Lineage: alter3 PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
 1	test_part	test_part
 2	test_part	test_part
@@ -74,7 +58,7 @@ col1	string
 pcol1	string	
 pcol2	string	
 	 	 
-Detailed Table Information	Table(tableName:alter3_renamed, dbName:default, owner:jsichi, createTime:1272920351, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:string, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter3_renamed, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:pcol1, type:string, comment:null), FieldSchema(name:pcol2, type:string, comment:null)], parameters:{last_modified_by=jsichi,last_modified_time=1272920356,transient_lastDdlTime=1272920356}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:alter3_renamed, dbName:default, owner:jssarma, createTime:1279735692, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter3_renamed, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:pcol1, type:string, comment:null), FieldSchema(name:pcol2, type:string, comment:null)], parameters:{last_modified_by=jssarma, last_modified_time=1279735695, transient_lastDdlTime=1279735695}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: describe extended alter3_renamed partition (pCol1='test_part', pcol2='test_part')
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: describe extended alter3_renamed partition (pCol1='test_part', pcol2='test_part')
@@ -84,15 +68,15 @@ col1	string
 pcol1	string	
 pcol2	string	
 	 	 
-Detailed Partition Information	Partition(values:[test_part, test_part], dbName:default, tableName:alter3_renamed, createTime:1272920356, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:string, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter3_renamed/pcol1=test_part/pcol2=test_part, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1272920356})	
+Detailed Partition Information	Partition(values:[test_part, test_part], dbName:default, tableName:alter3_renamed, createTime:1279735695, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter3_renamed/pcol1=test_part/pcol2=test_part, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1279735695})	
 PREHOOK: query: select * from alter3_renamed where pcol1='test_part' and pcol2='test_part'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter3_renamed@pcol1=test_part/pcol2=test_part
-PREHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/scratchdir/hive_2010-05-03_13-59-16_800_7077748274776018895/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-15_958_1754525751517388753/10000
 POSTHOOK: query: select * from alter3_renamed where pcol1='test_part' and pcol2='test_part'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter3_renamed@pcol1=test_part/pcol2=test_part
-POSTHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/scratchdir/hive_2010-05-03_13-59-16_800_7077748274776018895/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-15_958_1754525751517388753/10000
 POSTHOOK: Lineage: alter3 PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
 1	test_part	test_part
 2	test_part	test_part
@@ -132,31 +116,4 @@ col1	string
 pcol1	string	
 pcol2	string	
 	 	 
-Detailed Table Information	Table(tableName:alter3_like_renamed, dbName:default, owner:jsichi, createTime:1272920352, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:string, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/alter3_like_renamed, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:pcol1, type:string, comment:null), FieldSchema(name:pcol2, type:string, comment:null)], parameters:{EXTERNAL=FALSE,last_modified_by=jsichi,last_modified_time=1272920360,transient_lastDdlTime=1272920360}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
-PREHOOK: query: drop table alter3_src
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter3_src
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@alter3_src
-POSTHOOK: Lineage: alter3 PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
-POSTHOOK: Lineage: alter3_like PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
-PREHOOK: query: drop table alter3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: alter3 PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
-POSTHOOK: Lineage: alter3_like PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
-PREHOOK: query: drop table alter3_renamed
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter3_renamed
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@alter3_renamed
-POSTHOOK: Lineage: alter3 PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
-POSTHOOK: Lineage: alter3_like PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
-PREHOOK: query: drop table alter3_like_renamed
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table alter3_like_renamed
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@alter3_like_renamed
-POSTHOOK: Lineage: alter3 PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
-POSTHOOK: Lineage: alter3_like PARTITION(pcol1=test_part,pcol2=test_part).col1 SIMPLE [(alter3_src)alter3_src.FieldSchema(name:col1, type:string, comment:null), ]
+Detailed Table Information	Table(tableName:alter3_like_renamed, dbName:default, owner:jssarma, createTime:1279735692, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/alter3_like_renamed, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:pcol1, type:string, comment:null), FieldSchema(name:pcol2, type:string, comment:null)], parameters:{EXTERNAL=FALSE, last_modified_by=jssarma, last_modified_time=1279735698, transient_lastDdlTime=1279735698}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
diff --git a/ql/src/test/results/clientpositive/alter4.q.out b/ql/src/test/results/clientpositive/alter4.q.out
index a3b5153dc8..da7cd9ab1a 100644
--- a/ql/src/test/results/clientpositive/alter4.q.out
+++ b/ql/src/test/results/clientpositive/alter4.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE set_bucketing_test
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE set_bucketing_test
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE set_bucketing_test (key INT, value STRING) CLUSTERED BY (key) INTO 10 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE set_bucketing_test (key INT, value STRING) CLUSTERED BY (key) INTO 10 BUCKETS
@@ -14,7 +10,7 @@ POSTHOOK: type: DESCTABLE
 key	int	
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:set_bucketing_test, dbName:default, owner:pyang, createTime:1277867951, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/set_bucketing_test, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:10, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[key], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1277867951}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:set_bucketing_test, dbName:default, owner:jssarma, createTime:1279735699, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/set_bucketing_test, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:10, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[key], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279735699}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: ALTER TABLE set_bucketing_test NOT CLUSTERED
 PREHOOK: type: null
 POSTHOOK: query: ALTER TABLE set_bucketing_test NOT CLUSTERED
@@ -28,9 +24,4 @@ POSTHOOK: type: DESCTABLE
 key	int	
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:set_bucketing_test, dbName:default, owner:pyang, createTime:1277867951, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/set_bucketing_test, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=pyang, last_modified_time=1277867951, transient_lastDdlTime=1277867951}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
-PREHOOK: query: DROP TABLE set_bucketing_test
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE set_bucketing_test
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@set_bucketing_test
+Detailed Table Information	Table(tableName:set_bucketing_test, dbName:default, owner:jssarma, createTime:1279735699, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/set_bucketing_test, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=jssarma, last_modified_time=1279735699, transient_lastDdlTime=1279735699}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
diff --git a/ql/src/test/results/clientpositive/archive.q.out b/ql/src/test/results/clientpositive/archive.q.out
index b1a18c7355..ba67127d03 100644
--- a/ql/src/test/results/clientpositive/archive.q.out
+++ b/ql/src/test/results/clientpositive/archive.q.out
@@ -5,7 +5,7 @@ FROM (SELECT * FROM srcpart WHERE ds='2008-04-08') subq1) subq2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-12_801_8718664231713136788/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-19_808_5920190331482198474/10000
 POSTHOOK: query: -- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)
 
 SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col 
@@ -13,7 +13,7 @@ FROM (SELECT * FROM srcpart WHERE ds='2008-04-08') subq1) subq2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-12_801_8718664231713136788/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-19_808_5920190331482198474/10000
 48479881068
 PREHOOK: query: ALTER TABLE srcpart ARCHIVE PARTITION (ds='2008-04-08', hr='12')
 PREHOOK: type: ALTERTABLE_ARCHIVE
@@ -24,35 +24,35 @@ FROM (SELECT * FROM srcpart WHERE ds='2008-04-08') subq1) subq2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-20_510_5269010142014944519/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-24_163_645957732896284359/10000
 POSTHOOK: query: SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col 
 FROM (SELECT * FROM srcpart WHERE ds='2008-04-08') subq1) subq2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-20_510_5269010142014944519/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-24_163_645957732896284359/10000
 48479881068
 PREHOOK: query: SELECT key, count(1) FROM srcpart WHERE ds='2008-04-08' AND hr='12' AND key='0' GROUP BY key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-26_238_1201801305984652550/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-27_091_7903410270275371868/10000
 POSTHOOK: query: SELECT key, count(1) FROM srcpart WHERE ds='2008-04-08' AND hr='12' AND key='0' GROUP BY key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-26_238_1201801305984652550/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-27_091_7903410270275371868/10000
 0	3
 PREHOOK: query: SELECT * FROM srcpart a JOIN src b ON a.key=b.key 
 WHERE a.ds='2008-04-08' AND a.hr='12' AND a.key='0'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-32_413_8808816186480793926/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-30_052_8857134521946599131/10000
 POSTHOOK: query: SELECT * FROM srcpart a JOIN src b ON a.key=b.key 
 WHERE a.ds='2008-04-08' AND a.hr='12' AND a.key='0'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-32_413_8808816186480793926/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-30_052_8857134521946599131/10000
 0	val_0	2008-04-08	12	0	val_0
 0	val_0	2008-04-08	12	0	val_0
 0	val_0	2008-04-08	12	0	val_0
@@ -71,13 +71,13 @@ FROM (SELECT * FROM srcpart WHERE ds='2008-04-08') subq1) subq2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-37_857_7662280812791374354/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-33_403_2031008368522834404/10000
 POSTHOOK: query: SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col 
 FROM (SELECT * FROM srcpart WHERE ds='2008-04-08') subq1) subq2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-37_857_7662280812791374354/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-33_403_2031008368522834404/10000
 48479881068
 PREHOOK: query: CREATE TABLE harbucket(key INT) 
 PARTITIONED by (ds STRING)
@@ -100,11 +100,11 @@ POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchem
 PREHOOK: query: SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@harbucket@ds=1
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-47_247_5412318794268628077/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-39_158_8278614490047409485/10000
 POSTHOOK: query: SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@harbucket@ds=1
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-47_247_5412318794268628077/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-39_158_8278614490047409485/10000
 POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 0
 0
@@ -120,11 +120,11 @@ POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchem
 PREHOOK: query: SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@harbucket@ds=1
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-52_696_6661366062442712305/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-43_343_1319156393955499510/10000
 POSTHOOK: query: SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@harbucket@ds=1
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-52_696_6661366062442712305/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-43_343_1319156393955499510/10000
 POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 0
 0
@@ -140,11 +140,11 @@ POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchem
 PREHOOK: query: SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@harbucket@ds=1
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-56_920_7660869602739278397/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-46_534_5753932776517458176/10000
 POSTHOOK: query: SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@harbucket@ds=1
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-29-56_920_7660869602739278397/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-46_534_5753932776517458176/10000
 POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 0
 0
@@ -152,12 +152,6 @@ POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchem
 10
 20
 30
-PREHOOK: query: DROP TABLE harbucket
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE harbucket
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@harbucket
-POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: CREATE TABLE old_name(key INT) 
 PARTITIONED by (ds STRING)
 PREHOOK: type: CREATETABLE
@@ -186,12 +180,12 @@ PREHOOK: query: SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "
 FROM (SELECT * FROM old_name WHERE ds='1') subq1) subq2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@old_name@ds=1
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-30-06_143_8274193601305228676/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-53_532_5887928415090778760/10000
 POSTHOOK: query: SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col 
 FROM (SELECT * FROM old_name WHERE ds='1') subq1) subq2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@old_name@ds=1
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-30-06_143_8274193601305228676/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-53_532_5887928415090778760/10000
 POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: old_name PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 48656137
@@ -207,19 +201,12 @@ PREHOOK: query: SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "
 FROM (SELECT * FROM new_name WHERE ds='1') subq1) subq2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@new_name@ds=1
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-30-10_661_5999329953207292038/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-56_372_476351158155800671/10000
 POSTHOOK: query: SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col 
 FROM (SELECT * FROM new_name WHERE ds='1') subq1) subq2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@new_name@ds=1
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-21_17-30-10_661_5999329953207292038/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-08-56_372_476351158155800671/10000
 POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: old_name PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 NULL
-PREHOOK: query: DROP TABLE new_name
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE new_name
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@new_name
-POSTHOOK: Lineage: harbucket PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: old_name PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/binary_output_format.q.out b/ql/src/test/results/clientpositive/binary_output_format.q.out
index 62a6dd7ee8..29576f61fe 100644
--- a/ql/src/test/results/clientpositive/binary_output_format.q.out
+++ b/ql/src/test/results/clientpositive/binary_output_format.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: -- Create a table with binary output format
 CREATE TABLE dest1(mydata STRING)
 ROW FORMAT SERDE
@@ -92,7 +88,7 @@ STAGE PLANS:
                 File Output Operator
                   compressed: false
                   GlobalTableId: 1
-                  directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10002
+                  directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10002
                   NumFilesPerFileSink: 1
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
@@ -103,22 +99,22 @@ STAGE PLANS:
                         columns.types string
                         file.inputformat org.apache.hadoop.mapred.TextInputFormat
                         file.outputformat org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat
-                        location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+                        location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                         name dest1
                         serialization.ddl struct dest1 { string mydata}
                         serialization.format 1
                         serialization.last.column.takes.rest true
                         serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                        transient_lastDdlTime 1270515762
+                        transient_lastDdlTime 1279735739
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: dest1
                   TotalFiles: 1
                   MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/src [src]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [src]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -129,12 +125,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270515761
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -145,12 +141,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515761
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -162,14 +158,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10002
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat
@@ -179,21 +175,21 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                 name dest1
                 serialization.ddl struct dest1 { string mydata}
                 serialization.format 1
                 serialization.last.column.takes.rest true
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515762
+                transient_lastDdlTime 1279735739
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -205,9 +201,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10002 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10002]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -218,13 +214,13 @@ STAGE PLANS:
               columns.types string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
               name dest1
               serialization.ddl struct dest1 { string mydata}
               serialization.format 1
               serialization.last.column.takes.rest true
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270515762
+              transient_lastDdlTime 1279735739
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -235,13 +231,13 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                 name dest1
                 serialization.ddl struct dest1 { string mydata}
                 serialization.format 1
                 serialization.last.column.takes.rest true
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515762
+                transient_lastDdlTime 1279735739
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest1
             name: dest1
@@ -250,7 +246,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-42_120_5258018612502461655/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-08-59_521_6439138630072861096/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -261,13 +257,13 @@ STAGE PLANS:
                   columns.types string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                   name dest1
                   serialization.ddl struct dest1 { string mydata}
                   serialization.format 1
                   serialization.last.column.takes.rest true
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270515762
+                  transient_lastDdlTime 1279735739
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest1
             TotalFiles: 1
@@ -307,12 +303,12 @@ PREHOOK: query: -- Test the result
 SELECT * FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-46_981_6800349673068207966/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-02_211_5180340221675233412/10000
 POSTHOOK: query: -- Test the result
 SELECT * FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-46_981_6800349673068207966/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-02_211_5180340221675233412/10000
 POSTHOOK: Lineage: dest1.mydata SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
 238	val_238
 86	val_86
@@ -814,9 +810,3 @@ POSTHOOK: Lineage: dest1.mydata SCRIPT [(src)src.FieldSchema(name:key, type:stri
 400	val_400
 200	val_200
 97	val_97
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.mydata SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/binarysortable_1.q.out b/ql/src/test/results/clientpositive/binarysortable_1.q.out
index 21aad89f18..d5621b103b 100644
--- a/ql/src/test/results/clientpositive/binarysortable_1.q.out
+++ b/ql/src/test/results/clientpositive/binarysortable_1.q.out
@@ -116,7 +116,7 @@ FROM (
 ) a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@mytable
-PREHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/1914197516/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-03_011_4547908398997924956/10000
 POSTHOOK: query: SELECT REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(key, '\001', '^A'), '\0', '^@'), '\002', '^B'), value
 FROM (
         SELECT key, sum(value) as value
@@ -125,7 +125,7 @@ FROM (
 ) a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@mytable
-POSTHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/1914197516/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-03_011_4547908398997924956/10000
 ^@^@^@	7.0
 ^@^A^@	9.0
 ^@test^@	2.0
@@ -136,8 +136,3 @@ a^@bc^A^B^A^@	1.0
 test^@^@^A^Atest	6.0
 test^@test	4.0
 test^Atest	5.0
-PREHOOK: query: DROP TABLE mytable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE mytable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@mytable
diff --git a/ql/src/test/results/clientpositive/bucket1.q.out b/ql/src/test/results/clientpositive/bucket1.q.out
index 1f953294c4..ff992a032c 100644
--- a/ql/src/test/results/clientpositive/bucket1.q.out
+++ b/ql/src/test/results/clientpositive/bucket1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table bucket1_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket1_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE bucket1_1(key int, value string) CLUSTERED BY (key) INTO 100 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE bucket1_1(key int, value string) CLUSTERED BY (key) INTO 100 BUCKETS
@@ -49,9 +45,9 @@ STAGE PLANS:
                       type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src [src]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [src]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -62,12 +58,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270515753
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -78,12 +74,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515753
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -99,7 +95,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-              directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-34_525_873235169631577914/10000
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-06_147_3859020501578469948/10000
               NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -111,12 +107,12 @@ STAGE PLANS:
                     columns.types int:string
                     file.inputformat org.apache.hadoop.mapred.TextInputFormat
                     file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/bucket1_1
+                    location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket1_1
                     name bucket1_1
                     serialization.ddl struct bucket1_1 { i32 key, string value}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    transient_lastDdlTime 1270515754
+                    transient_lastDdlTime 1279735746
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: bucket1_1
               TotalFiles: 1
@@ -126,7 +122,7 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-34_525_873235169631577914/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-06_147_3859020501578469948/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -137,15 +133,15 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/bucket1_1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket1_1
                 name bucket1_1
                 serialization.ddl struct bucket1_1 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515754
+                transient_lastDdlTime 1279735746
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucket1_1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-34_525_873235169631577914/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-06_147_3859020501578469948/10001
 
 
 PREHOOK: query: insert overwrite table bucket1_1
@@ -163,11 +159,11 @@ POSTHOOK: Lineage: bucket1_1.value SIMPLE [(src)src.FieldSchema(name:value, type
 PREHOOK: query: select * from bucket1_1 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucket1_1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-39_954_6102377035167270592/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-08_981_1141988062127111870/10000
 POSTHOOK: query: select * from bucket1_1 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucket1_1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-39_954_6102377035167270592/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-08_981_1141988062127111870/10000
 POSTHOOK: Lineage: bucket1_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: bucket1_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	val_0
@@ -670,10 +666,3 @@ POSTHOOK: Lineage: bucket1_1.value SIMPLE [(src)src.FieldSchema(name:value, type
 498	val_498
 498	val_498
 498	val_498
-PREHOOK: query: drop table bucket1_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket1_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucket1_1
-POSTHOOK: Lineage: bucket1_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket1_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/bucket2.q.out b/ql/src/test/results/clientpositive/bucket2.q.out
index 96cb0eb76d..113e699c33 100644
--- a/ql/src/test/results/clientpositive/bucket2.q.out
+++ b/ql/src/test/results/clientpositive/bucket2.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table bucket2_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket2_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE bucket2_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE bucket2_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS
@@ -49,9 +45,9 @@ STAGE PLANS:
                       type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/src [src]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [src]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -62,12 +58,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270515770
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -78,12 +74,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515770
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -99,7 +95,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-              directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-51_261_8059892845101746655/10000
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-11_789_5710176838021451402/10000
               NumFilesPerFileSink: 2
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -111,12 +107,12 @@ STAGE PLANS:
                     columns.types int:string
                     file.inputformat org.apache.hadoop.mapred.TextInputFormat
                     file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/bucket2_1
+                    location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket2_1
                     name bucket2_1
                     serialization.ddl struct bucket2_1 { i32 key, string value}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    transient_lastDdlTime 1270515771
+                    transient_lastDdlTime 1279735751
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: bucket2_1
               TotalFiles: 2
@@ -126,7 +122,7 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-51_261_8059892845101746655/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-11_789_5710176838021451402/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -137,15 +133,15 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/bucket2_1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket2_1
                 name bucket2_1
                 serialization.ddl struct bucket2_1 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515771
+                transient_lastDdlTime 1279735751
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucket2_1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-51_261_8059892845101746655/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-11_789_5710176838021451402/10001
 
 
 PREHOOK: query: insert overwrite table bucket2_1
@@ -225,11 +221,11 @@ STAGE PLANS:
 PREHOOK: query: select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucket2_1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-58_513_3181118859318846401/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-14_833_8954758164660929050/10000
 POSTHOOK: query: select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucket2_1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-02-58_513_3181118859318846401/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-14_833_8954758164660929050/10000
 POSTHOOK: Lineage: bucket2_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: bucket2_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	val_0
@@ -479,10 +475,3 @@ POSTHOOK: Lineage: bucket2_1.value SIMPLE [(src)src.FieldSchema(name:value, type
 498	val_498
 498	val_498
 498	val_498
-PREHOOK: query: drop table bucket2_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket2_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucket2_1
-POSTHOOK: Lineage: bucket2_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket2_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/bucket3.q.out b/ql/src/test/results/clientpositive/bucket3.q.out
index 283badb53c..69fe227eba 100644
--- a/ql/src/test/results/clientpositive/bucket3.q.out
+++ b/ql/src/test/results/clientpositive/bucket3.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table bucket3_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket3_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE bucket3_1(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE bucket3_1(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS
@@ -49,9 +45,9 @@ STAGE PLANS:
                       type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/src [src]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [src]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -62,12 +58,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270515793
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -78,12 +74,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515793
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -99,7 +95,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-              directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-03-14_419_5303321979362543224/10000
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-18_069_2499676602955827966/10000
               NumFilesPerFileSink: 2
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -111,13 +107,13 @@ STAGE PLANS:
                     columns.types int:string
                     file.inputformat org.apache.hadoop.mapred.TextInputFormat
                     file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/bucket3_1
+                    location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket3_1
                     name bucket3_1
                     partition_columns ds
                     serialization.ddl struct bucket3_1 { i32 key, string value}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    transient_lastDdlTime 1270515794
+                    transient_lastDdlTime 1279735757
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: bucket3_1
               TotalFiles: 2
@@ -129,7 +125,7 @@ STAGE PLANS:
           partition:
             ds 1
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-03-14_419_5303321979362543224/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-18_069_2499676602955827966/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -140,16 +136,16 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/bucket3_1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket3_1
                 name bucket3_1
                 partition_columns ds
                 serialization.ddl struct bucket3_1 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515794
+                transient_lastDdlTime 1279735757
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucket3_1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-03-14_419_5303321979362543224/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-18_069_2499676602955827966/10001
 
 
 PREHOOK: query: insert overwrite table bucket3_1 partition (ds='1')
@@ -253,11 +249,11 @@ STAGE PLANS:
 PREHOOK: query: select * from bucket3_1 tablesample (bucket 1 out of 2) s where ds = '1' order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucket3_1@ds=1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-03-24_389_3958955156388135589/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-23_782_5794755668640533078/10000
 POSTHOOK: query: select * from bucket3_1 tablesample (bucket 1 out of 2) s where ds = '1' order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucket3_1@ds=1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-03-24_389_3958955156388135589/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-23_782_5794755668640533078/10000
 POSTHOOK: Lineage: bucket3_1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: bucket3_1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: bucket3_1 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -509,12 +505,3 @@ POSTHOOK: Lineage: bucket3_1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(
 498	val_498	1
 498	val_498	1
 498	val_498	1
-PREHOOK: query: drop table bucket3_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket3_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucket3_1
-POSTHOOK: Lineage: bucket3_1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket3_1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket3_1 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket3_1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/bucket4.q.out b/ql/src/test/results/clientpositive/bucket4.q.out
index 3ea5705c98..8c0e5c7307 100644
--- a/ql/src/test/results/clientpositive/bucket4.q.out
+++ b/ql/src/test/results/clientpositive/bucket4.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table bucket4_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket4_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
@@ -52,9 +48,9 @@ STAGE PLANS:
                       type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src [src]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [src]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -65,12 +61,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270515769
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -81,12 +77,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515769
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -102,7 +98,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-              directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-50_258_5147459717740143983/10000
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-26_887_4786734363166528929/10000
               NumFilesPerFileSink: 2
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -115,12 +111,12 @@ STAGE PLANS:
                     columns.types int:string
                     file.inputformat org.apache.hadoop.mapred.TextInputFormat
                     file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/bucket4_1
+                    location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket4_1
                     name bucket4_1
                     serialization.ddl struct bucket4_1 { i32 key, string value}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    transient_lastDdlTime 1270515770
+                    transient_lastDdlTime 1279735766
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: bucket4_1
               TotalFiles: 2
@@ -130,7 +126,7 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-50_258_5147459717740143983/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-26_887_4786734363166528929/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -142,15 +138,15 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/bucket4_1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket4_1
                 name bucket4_1
                 serialization.ddl struct bucket4_1 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270515770
+                transient_lastDdlTime 1279735766
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucket4_1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-50_258_5147459717740143983/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-09-26_887_4786734363166528929/10001
 
 
 PREHOOK: query: insert overwrite table bucket4_1
@@ -217,11 +213,11 @@ STAGE PLANS:
 PREHOOK: query: select * from bucket4_1 tablesample (bucket 1 out of 2) s
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucket4_1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-55_285_753310371043040764/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-29_642_3068318549775797820/10000
 POSTHOOK: query: select * from bucket4_1 tablesample (bucket 1 out of 2) s
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucket4_1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-02-55_285_753310371043040764/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-29_642_3068318549775797820/10000
 POSTHOOK: Lineage: bucket4_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: bucket4_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	val_0
@@ -471,10 +467,3 @@ POSTHOOK: Lineage: bucket4_1.value SIMPLE [(src)src.FieldSchema(name:value, type
 498	val_498
 498	val_498
 498	val_498
-PREHOOK: query: drop table bucket4_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket4_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucket4_1
-POSTHOOK: Lineage: bucket4_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket4_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/bucket_groupby.q.out b/ql/src/test/results/clientpositive/bucket_groupby.q.out
index 7f8578a45d..ff1b2c807a 100644
--- a/ql/src/test/results/clientpositive/bucket_groupby.q.out
+++ b/ql/src/test/results/clientpositive/bucket_groupby.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table clustergroupby
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table clustergroupby
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table clustergroupby(key string, value string) partitioned by(ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table clustergroupby(key string, value string) partitioned by(ds string)
@@ -15,7 +11,7 @@ key	string
 value	string	
 ds	string	
 	 	 
-Detailed Table Information	Table(tableName:clustergroupby, dbName:default, owner:athusoo, createTime:1270515787, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/clustergroupby, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transient_lastDdlTime=1270515787}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:clustergroupby, dbName:default, owner:jssarma, createTime:1279735772, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/clustergroupby, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transient_lastDdlTime=1279735772}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table clustergroupby partition (ds='100') select key, value from src sort by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -115,11 +111,11 @@ STAGE PLANS:
 PREHOOK: query: select key, count(1) from clustergroupby where ds='100' group by key limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=100
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-13_780_4293831568302982785/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-35_534_8185562718357069120/10000
 POSTHOOK: query: select key, count(1) from clustergroupby where ds='100' group by key limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=100
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-13_780_4293831568302982785/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-35_534_8185562718357069120/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	3
@@ -150,7 +146,7 @@ key	string
 value	string	
 ds	string	
 	 	 
-Detailed Table Information	Table(tableName:clustergroupby, dbName:default, owner:athusoo, createTime:1270515787, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/clustergroupby, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[key], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{last_modified_by=athusoo,last_modified_time=1270515798,transient_lastDdlTime=1270515798}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:clustergroupby, dbName:default, owner:jssarma, createTime:1279735772, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/clustergroupby, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[key], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{last_modified_by=jssarma, last_modified_time=1279735778, transient_lastDdlTime=1279735778}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table clustergroupby partition (ds='101') select key, value from src distribute by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -256,11 +252,11 @@ STAGE PLANS:
 PREHOOK: query: select key, count(1) from clustergroupby  where ds='101' group by key limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=101
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-24_380_6328201988353238872/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-41_016_7689771879965686688/10000
 POSTHOOK: query: select key, count(1) from clustergroupby  where ds='101' group by key limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=101
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-24_380_6328201988353238872/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-41_016_7689771879965686688/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -368,11 +364,11 @@ STAGE PLANS:
 PREHOOK: query: select length(key), count(1) from clustergroupby  where ds='101' group by length(key) limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=101
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-30_930_3497817425124433255/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-43_849_707604862542797827/10000
 POSTHOOK: query: select length(key), count(1) from clustergroupby  where ds='101' group by length(key) limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=101
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-30_930_3497817425124433255/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-43_849_707604862542797827/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -471,11 +467,11 @@ STAGE PLANS:
 PREHOOK: query: select abs(length(key)), count(1) from clustergroupby  where ds='101' group by abs(length(key)) limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=101
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-41_422_5290311054247228986/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-46_664_3688496316777330614/10000
 POSTHOOK: query: select abs(length(key)), count(1) from clustergroupby  where ds='101' group by abs(length(key)) limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=101
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-41_422_5290311054247228986/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-46_664_3688496316777330614/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -584,11 +580,11 @@ STAGE PLANS:
 PREHOOK: query: select key, count(1) from clustergroupby  where ds='101' group by key,3 limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=101
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-47_388_6623885853323229647/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-49_481_9074398325986135965/10000
 POSTHOOK: query: select key, count(1) from clustergroupby  where ds='101' group by key,3 limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=101
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-47_388_6623885853323229647/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-49_481_9074398325986135965/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -701,11 +697,11 @@ STAGE PLANS:
 PREHOOK: query: select key, count(1) from (select value as key, key as value from clustergroupby where ds='101')subq group by key limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=101
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-55_205_578266570097178049/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-52_312_6910555695381377185/10000
 POSTHOOK: query: select key, count(1) from (select value as key, key as value from clustergroupby where ds='101')subq group by key limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=101
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-03-55_205_578266570097178049/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-52_312_6910555695381377185/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -803,12 +799,12 @@ PREHOOK: query: select key, count(1) from clustergroupby  group by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=100
 PREHOOK: Input: default@clustergroupby@ds=101
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-08_207_8200567238299494473/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-55_221_4522384383217894505/10000
 POSTHOOK: query: select key, count(1) from clustergroupby  group by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=100
 POSTHOOK: Input: default@clustergroupby@ds=101
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-08_207_8200567238299494473/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-09-55_221_4522384383217894505/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1233,7 +1229,7 @@ key	string
 value	string	
 ds	string	
 	 	 
-Detailed Table Information	Table(tableName:clustergroupby, dbName:default, owner:athusoo, createTime:1270515787, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/clustergroupby, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[value], sortCols:[Order(col:key, order:1), Order(col:value, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{last_modified_by=athusoo,last_modified_time=1270515867,transient_lastDdlTime=1270515867}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:clustergroupby, dbName:default, owner:jssarma, createTime:1279735772, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/clustergroupby, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[value], sortCols:[Order(col:key, order:1), Order(col:value, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{last_modified_by=jssarma, last_modified_time=1279735798, transient_lastDdlTime=1279735798}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table clustergroupby partition (ds='102') select key, value from src distribute by value sort by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -1341,11 +1337,11 @@ STAGE PLANS:
 PREHOOK: query: select key, count(1) from clustergroupby  where ds='102' group by key limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=102
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-33_613_3501792825410171493/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-01_215_2690537963790427608/10000
 POSTHOOK: query: select key, count(1) from clustergroupby  where ds='102' group by key limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=102
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-33_613_3501792825410171493/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-01_215_2690537963790427608/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1455,11 +1451,11 @@ STAGE PLANS:
 PREHOOK: query: select value, count(1) from clustergroupby  where ds='102'  group by value limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=102
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-38_518_2690806199946056325/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-04_135_3923631383708910626/10000
 POSTHOOK: query: select value, count(1) from clustergroupby  where ds='102'  group by value limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=102
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-38_518_2690806199946056325/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-04_135_3923631383708910626/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1579,11 +1575,11 @@ STAGE PLANS:
 PREHOOK: query: select key, count(1) from clustergroupby  where ds='102'  group by key, value limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=102
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-43_716_5095162971444718926/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-07_055_4285817181416116197/10000
 POSTHOOK: query: select key, count(1) from clustergroupby  where ds='102'  group by key, value limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=102
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-43_716_5095162971444718926/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-07_055_4285817181416116197/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1626,7 +1622,7 @@ key	string
 value	string	
 ds	string	
 	 	 
-Detailed Table Information	Table(tableName:clustergroupby, dbName:default, owner:athusoo, createTime:1270515787, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/clustergroupby, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[value, key], sortCols:[Order(col:key, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{last_modified_by=athusoo,last_modified_time=1270515888,transient_lastDdlTime=1270515888}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:clustergroupby, dbName:default, owner:jssarma, createTime:1279735772, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/clustergroupby, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[value, key], sortCols:[Order(col:key, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{last_modified_by=jssarma, last_modified_time=1279735809, transient_lastDdlTime=1279735809}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table clustergroupby partition (ds='103') select key, value from src distribute by value, key sort by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -1738,11 +1734,11 @@ STAGE PLANS:
 PREHOOK: query: select key, count(1) from clustergroupby  where ds='103' group by key limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=103
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-53_057_86001272118042631/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-12_709_7466980661091036250/10000
 POSTHOOK: query: select key, count(1) from clustergroupby  where ds='103' group by key limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=103
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-53_057_86001272118042631/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-12_709_7466980661091036250/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1866,11 +1862,11 @@ STAGE PLANS:
 PREHOOK: query: select key, count(1) from clustergroupby  where ds='103' group by  value, key limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@clustergroupby@ds=103
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-57_724_2028785942026261985/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-15_551_6887415689280301293/10000
 POSTHOOK: query: select key, count(1) from clustergroupby  where ds='103' group by  value, key limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@clustergroupby@ds=103
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-04-57_724_2028785942026261985/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-10-15_551_6887415689280301293/10000
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1889,16 +1885,3 @@ POSTHOOK: Lineage: clustergroupby PARTITION(ds=103).value SIMPLE [(src)src.Field
 111	1
 113	2
 114	1
-PREHOOK: query: drop table clustergroupby
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table clustergroupby
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@clustergroupby
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=101).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=102).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=102).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=103).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: clustergroupby PARTITION(ds=103).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/bucketizedhiveinputformat.q.out b/ql/src/test/results/clientpositive/bucketizedhiveinputformat.q.out
index fdbac2dc2e..9ef362642c 100644
--- a/ql/src/test/results/clientpositive/bucketizedhiveinputformat.q.out
+++ b/ql/src/test/results/clientpositive/bucketizedhiveinputformat.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T1(name STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T1(name STRING) STORED AS TEXTFILE
@@ -12,10 +8,6 @@ PREHOOK: type: LOAD
 POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE T1
 POSTHOOK: type: LOAD
 POSTHOOK: Output: default@t1
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T2(name STRING) STORED AS SEQUENCEFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T2(name STRING) STORED AS SEQUENCEFILE
@@ -163,7 +155,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-03-34_477_4466762848424481616/10003 
+        file:/tmp/jssarma/hive_2010-07-21_11-10-19_047_380113812021070738/10003 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -271,18 +263,13 @@ STAGE PLANS:
 PREHOOK: query: SELECT COUNT(1) FROM T2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-06-18_958_8326164509200163533/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-11-40_372_2216491831322314327/10000
 POSTHOOK: query: SELECT COUNT(1) FROM T2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-06-18_958_8326164509200163533/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-11-40_372_2216491831322314327/10000
 POSTHOOK: Lineage: t2.name SIMPLE [(t1)t1.FieldSchema(name:name, type:string, comment:null), ]
 5000000
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: t2.name SIMPLE [(t1)t1.FieldSchema(name:name, type:string, comment:null), ]
 PREHOOK: query: CREATE TABLE T3(name STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T3(name STRING) STORED AS TEXTFILE
@@ -360,28 +347,10 @@ STAGE PLANS:
 PREHOOK: query: SELECT COUNT(1) FROM T3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-06-45_448_8066146192994060790/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-11-52_373_3173950263104621965/10000
 POSTHOOK: query: SELECT COUNT(1) FROM T3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-06-45_448_8066146192994060790/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-11-52_373_3173950263104621965/10000
 POSTHOOK: Lineage: t2.name SIMPLE [(t1)t1.FieldSchema(name:name, type:string, comment:null), ]
 1000
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-POSTHOOK: Lineage: t2.name SIMPLE [(t1)t1.FieldSchema(name:name, type:string, comment:null), ]
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-POSTHOOK: Lineage: t2.name SIMPLE [(t1)t1.FieldSchema(name:name, type:string, comment:null), ]
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t3
-POSTHOOK: Lineage: t2.name SIMPLE [(t1)t1.FieldSchema(name:name, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/bucketmapjoin1.q.out b/ql/src/test/results/clientpositive/bucketmapjoin1.q.out
index f728c20a9a..51d82366d2 100644
--- a/ql/src/test/results/clientpositive/bucketmapjoin1.q.out
+++ b/ql/src/test/results/clientpositive/bucketmapjoin1.q.out
@@ -1,27 +1,3 @@
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
@@ -161,7 +137,7 @@ STAGE PLANS:
                     File Output Operator
                       compressed: false
                       GlobalTableId: 1
-                      directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10002
+                      directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10002
                       NumFilesPerFileSink: 1
                       table:
                           input format: org.apache.hadoop.mapred.TextInputFormat
@@ -172,12 +148,12 @@ STAGE PLANS:
                             columns.types string:string:string
                             file.inputformat org.apache.hadoop.mapred.TextInputFormat
                             file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                            location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                             name bucketmapjoin_tmp_result
                             serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                             serialization.format 1
                             serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                            transient_lastDdlTime 1270585153
+                            transient_lastDdlTime 1279735917
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                           name: bucketmapjoin_tmp_result
                       TotalFiles: 1
@@ -237,7 +213,7 @@ STAGE PLANS:
                           File Output Operator
                             compressed: false
                             GlobalTableId: 1
-                            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10002
+                            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10002
                             NumFilesPerFileSink: 1
                             table:
                                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -248,12 +224,12 @@ STAGE PLANS:
                                   columns.types string:string:string
                                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                                   name bucketmapjoin_tmp_result
                                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                                   serialization.format 1
                                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                  transient_lastDdlTime 1270585153
+                                  transient_lastDdlTime 1279735917
                                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                                 name: bucketmapjoin_tmp_result
                             TotalFiles: 1
@@ -262,15 +238,15 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 b {srcbucket20.txt=[srcbucket20.txt, srcbucket22.txt], srcbucket21.txt=[srcbucket21.txt, srcbucket23.txt]}
               Alias Bucket File Name Mapping:
-                b {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt, file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt, file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt]}
+                b {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt, file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt, file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt 1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin 
           Partition
             base file name: srcbucket_mapjoin
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -282,12 +258,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
               name srcbucket_mapjoin
               serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585151
+              transient_lastDdlTime 1279735915
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -299,12 +275,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
                 name srcbucket_mapjoin
                 serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585151
+                transient_lastDdlTime 1279735915
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin
             name: srcbucket_mapjoin
@@ -316,14 +292,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -333,20 +309,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585153
+                transient_lastDdlTime 1279735917
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -362,9 +338,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -375,12 +351,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585153
+              transient_lastDdlTime 1279735917
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -391,12 +367,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585153
+                transient_lastDdlTime 1279735917
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -405,7 +381,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-13_941_4614688152721840484/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-11-57_999_8177681136669893312/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -416,12 +392,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270585153
+                  transient_lastDdlTime 1279735917
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -450,11 +426,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-24_821_7662587634179981749/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-03_524_5411435410955837296/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-24_821_7662587634179981749/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-03_524_5411435410955837296/10000
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
@@ -503,11 +479,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-43_327_1075311711903399633/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-14_143_9203520501338252100/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-43_327_1075311711903399633/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-14_143_9203520501338252100/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -546,14 +522,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-50_682_4185618914647684444/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-19_237_1383860260604681150/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-50_682_4185618914647684444/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-19_237_1383860260604681150/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -653,7 +629,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -664,12 +640,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270585153
+                              transient_lastDdlTime 1279735934
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
@@ -724,7 +700,7 @@ STAGE PLANS:
                         File Output Operator
                           compressed: false
                           GlobalTableId: 1
-                          directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10002
+                          directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10002
                           NumFilesPerFileSink: 1
                           table:
                               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -735,12 +711,12 @@ STAGE PLANS:
                                 columns.types string:string:string
                                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                                 name bucketmapjoin_tmp_result
                                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                                 serialization.format 1
                                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                transient_lastDdlTime 1270585153
+                                transient_lastDdlTime 1279735934
                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                               name: bucketmapjoin_tmp_result
                           TotalFiles: 1
@@ -749,17 +725,17 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 a {srcbucket20.txt=[srcbucket20.txt], srcbucket21.txt=[srcbucket21.txt], srcbucket22.txt=[srcbucket20.txt], srcbucket23.txt=[srcbucket21.txt]}
               Alias Bucket File Name Mapping:
-                a {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
+                a {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt 1
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt 2
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt 3
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt 2
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt 3
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 [b]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 [b]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 
           Partition
             base file name: ds=2008-04-08
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -773,13 +749,13 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part
               name srcbucket_mapjoin_part
               partition_columns ds
               serialization.ddl struct srcbucket_mapjoin_part { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585152
+              transient_lastDdlTime 1279735916
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -791,13 +767,13 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part
                 name srcbucket_mapjoin_part
                 partition_columns ds
                 serialization.ddl struct srcbucket_mapjoin_part { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585152
+                transient_lastDdlTime 1279735916
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin_part
             name: srcbucket_mapjoin_part
@@ -809,14 +785,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -826,20 +802,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585153
+                transient_lastDdlTime 1279735934
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -855,9 +831,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -868,12 +844,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585153
+              transient_lastDdlTime 1279735934
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -884,12 +860,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585153
+                transient_lastDdlTime 1279735934
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -898,7 +874,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-19-54_595_8256597777535200115/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-22_909_644555599687524895/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -909,12 +885,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270585153
+                  transient_lastDdlTime 1279735934
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -955,11 +931,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-06_292_5981515132996001139/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-28_300_3928482371352153550/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-06_292_5981515132996001139/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-28_300_3928482371352153550/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -1044,11 +1020,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-23_102_7566495174318785352/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-40_692_5656314140325909907/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-23_102_7566495174318785352/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-40_692_5656314140325909907/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
@@ -1111,14 +1087,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-30_613_330654965864285966/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-46_030_8024623277815754853/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-30_613_330654965864285966/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-46_030_8024623277815754853/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
@@ -1144,177 +1120,3 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
 0	0	0
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_1
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_tmp_result
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/bucketmapjoin2.q.out b/ql/src/test/results/clientpositive/bucketmapjoin2.q.out
index da2133721b..daf9cf64d3 100644
--- a/ql/src/test/results/clientpositive/bucketmapjoin2.q.out
+++ b/ql/src/test/results/clientpositive/bucketmapjoin2.q.out
@@ -1,27 +1,3 @@
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
@@ -154,7 +130,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10002
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -165,12 +141,12 @@ STAGE PLANS:
                           columns.types string:string:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                           name bucketmapjoin_tmp_result
                           serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1270585242
+                          transient_lastDdlTime 1279735971
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: bucketmapjoin_tmp_result
                     TotalFiles: 1
@@ -228,7 +204,7 @@ STAGE PLANS:
                           File Output Operator
                             compressed: false
                             GlobalTableId: 1
-                            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10002
+                            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10002
                             NumFilesPerFileSink: 1
                             table:
                                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -239,12 +215,12 @@ STAGE PLANS:
                                   columns.types string:string:string
                                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                                   name bucketmapjoin_tmp_result
                                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                                   serialization.format 1
                                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                  transient_lastDdlTime 1270585242
+                                  transient_lastDdlTime 1279735971
                                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                                 name: bucketmapjoin_tmp_result
                             TotalFiles: 1
@@ -253,15 +229,15 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 b {srcbucket20.txt=[srcbucket22.txt], srcbucket21.txt=[srcbucket23.txt]}
               Alias Bucket File Name Mapping:
-                b {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt]}
+                b {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt 1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin 
           Partition
             base file name: srcbucket_mapjoin
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -273,12 +249,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
               name srcbucket_mapjoin
               serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585239
+              transient_lastDdlTime 1279735969
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -290,12 +266,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
                 name srcbucket_mapjoin
                 serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585239
+                transient_lastDdlTime 1279735969
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin
             name: srcbucket_mapjoin
@@ -307,14 +283,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -324,20 +300,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585242
+                transient_lastDdlTime 1279735971
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -353,9 +329,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -366,12 +342,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585242
+              transient_lastDdlTime 1279735971
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -382,12 +358,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585242
+                transient_lastDdlTime 1279735971
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -396,7 +372,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-42_248_1656544165360904214/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-12-51_864_289604097939000581/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -407,12 +383,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270585242
+                  transient_lastDdlTime 1279735971
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -441,11 +417,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-51_476_6849100594964713524/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-57_285_8323175681686865540/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-20-51_476_6849100594964713524/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-12-57_285_8323175681686865540/10000
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
@@ -494,11 +470,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-05_728_8205471923586302660/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-08_258_8554752852721376861/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-05_728_8205471923586302660/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-08_258_8554752852721376861/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -537,14 +513,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-12_789_6204727183274463456/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-13_785_3291149328245299772/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-12_789_6204727183274463456/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-13_785_3291149328245299772/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -642,7 +618,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -653,12 +629,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270585242
+                              transient_lastDdlTime 1279735988
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
@@ -706,7 +682,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -717,12 +693,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270585242
+                              transient_lastDdlTime 1279735988
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
@@ -731,15 +707,15 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 a {srcbucket22.txt=[srcbucket20.txt], srcbucket23.txt=[srcbucket21.txt]}
               Alias Bucket File Name Mapping:
-                a {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
+                a {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt 1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 [b]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 [b]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 
           Partition
             base file name: ds=2008-04-08
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -753,13 +729,13 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
               name srcbucket_mapjoin_part_2
               partition_columns ds
               serialization.ddl struct srcbucket_mapjoin_part_2 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585241
+              transient_lastDdlTime 1279735970
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -771,13 +747,13 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
                 name srcbucket_mapjoin_part_2
                 partition_columns ds
                 serialization.ddl struct srcbucket_mapjoin_part_2 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585241
+                transient_lastDdlTime 1279735970
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin_part_2
             name: srcbucket_mapjoin_part_2
@@ -789,14 +765,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -806,20 +782,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585242
+                transient_lastDdlTime 1279735988
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -835,9 +811,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -848,12 +824,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585242
+              transient_lastDdlTime 1279735988
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -864,12 +840,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585242
+                transient_lastDdlTime 1279735988
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -878,7 +854,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-17_599_730776999837906559/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-17_474_4845430914820476351/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -889,12 +865,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270585242
+                  transient_lastDdlTime 1279735988
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -935,11 +911,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-25_773_8218205998648860194/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-22_914_419984116759457763/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-25_773_8218205998648860194/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-22_914_419984116759457763/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -1024,11 +1000,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-39_905_1457912158968511755/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-33_677_5753676967649563553/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-39_905_1457912158968511755/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-33_677_5753676967649563553/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
@@ -1091,14 +1067,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-46_993_1687004669182115612/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-38_913_1186146182635523380/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-46_993_1687004669182115612/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-38_913_1186146182635523380/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
@@ -1124,177 +1100,3 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
 NULL	NULL	NULL
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_1
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_tmp_result
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/bucketmapjoin3.q.out b/ql/src/test/results/clientpositive/bucketmapjoin3.q.out
index 55150a04db..8d9789fdd4 100644
--- a/ql/src/test/results/clientpositive/bucketmapjoin3.q.out
+++ b/ql/src/test/results/clientpositive/bucketmapjoin3.q.out
@@ -1,27 +1,3 @@
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
@@ -164,7 +140,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -175,12 +151,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270585316
+                              transient_lastDdlTime 1279736024
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
@@ -238,7 +214,7 @@ STAGE PLANS:
                           File Output Operator
                             compressed: false
                             GlobalTableId: 1
-                            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10002
+                            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10002
                             NumFilesPerFileSink: 1
                             table:
                                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -249,12 +225,12 @@ STAGE PLANS:
                                   columns.types string:string:string
                                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                                   name bucketmapjoin_tmp_result
                                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                                   serialization.format 1
                                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                  transient_lastDdlTime 1270585316
+                                  transient_lastDdlTime 1279736024
                                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                                 name: bucketmapjoin_tmp_result
                             TotalFiles: 1
@@ -263,15 +239,15 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 b {srcbucket22.txt=[srcbucket20.txt, srcbucket22.txt], srcbucket23.txt=[srcbucket21.txt, srcbucket23.txt]}
               Alias Bucket File Name Mapping:
-                b {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt, file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt, file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt]}
+                b {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt, file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt, file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt 1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 [a]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 [a]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 
           Partition
             base file name: ds=2008-04-08
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -285,13 +261,13 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
               name srcbucket_mapjoin_part_2
               partition_columns ds
               serialization.ddl struct srcbucket_mapjoin_part_2 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585316
+              transient_lastDdlTime 1279736023
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -303,13 +279,13 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
                 name srcbucket_mapjoin_part_2
                 partition_columns ds
                 serialization.ddl struct srcbucket_mapjoin_part_2 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585316
+                transient_lastDdlTime 1279736023
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin_part_2
             name: srcbucket_mapjoin_part_2
@@ -321,14 +297,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -338,20 +314,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585316
+                transient_lastDdlTime 1279736024
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -367,9 +343,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -380,12 +356,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585316
+              transient_lastDdlTime 1279736024
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -396,12 +372,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585316
+                transient_lastDdlTime 1279736024
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -410,7 +386,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-21-56_898_2673660382336894847/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-13-44_453_5946629419972816172/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -421,12 +397,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270585316
+                  transient_lastDdlTime 1279736024
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -455,11 +431,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-05_242_8855147289781300707/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-49_836_2279023775244897756/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-05_242_8855147289781300707/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-13-49_836_2279023775244897756/10000
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
@@ -508,11 +484,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-21_066_2881843686071536980/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-00_416_6913608996788723816/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-21_066_2881843686071536980/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-00_416_6913608996788723816/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -551,14 +527,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-28_875_6950978851171606368/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-05_840_3009901706260125245/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-28_875_6950978851171606368/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-05_840_3009901706260125245/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -656,7 +632,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -667,12 +643,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270585316
+                              transient_lastDdlTime 1279736040
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
@@ -730,7 +706,7 @@ STAGE PLANS:
                           File Output Operator
                             compressed: false
                             GlobalTableId: 1
-                            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10002
+                            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10002
                             NumFilesPerFileSink: 1
                             table:
                                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -741,12 +717,12 @@ STAGE PLANS:
                                   columns.types string:string:string
                                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                                   name bucketmapjoin_tmp_result
                                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                                   serialization.format 1
                                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                  transient_lastDdlTime 1270585316
+                                  transient_lastDdlTime 1279736040
                                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                                 name: bucketmapjoin_tmp_result
                             TotalFiles: 1
@@ -755,17 +731,17 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 a {srcbucket20.txt=[srcbucket22.txt], srcbucket21.txt=[srcbucket23.txt], srcbucket22.txt=[srcbucket22.txt], srcbucket23.txt=[srcbucket23.txt]}
               Alias Bucket File Name Mapping:
-                a {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt]}
+                a {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt 1
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt 2
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt 3
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt 2
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt 3
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 [b]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 [b]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 
           Partition
             base file name: ds=2008-04-08
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -779,13 +755,13 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part
               name srcbucket_mapjoin_part
               partition_columns ds
               serialization.ddl struct srcbucket_mapjoin_part { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585315
+              transient_lastDdlTime 1279736023
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -797,13 +773,13 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part
                 name srcbucket_mapjoin_part
                 partition_columns ds
                 serialization.ddl struct srcbucket_mapjoin_part { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585315
+                transient_lastDdlTime 1279736023
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin_part
             name: srcbucket_mapjoin_part
@@ -815,14 +791,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -832,20 +808,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585316
+                transient_lastDdlTime 1279736040
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -861,9 +837,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -874,12 +850,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270585316
+              transient_lastDdlTime 1279736040
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -890,12 +866,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270585316
+                transient_lastDdlTime 1279736040
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -904,7 +880,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-33_840_3466084031668733835/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-08_582_9180675256028772614/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -915,12 +891,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270585316
+                  transient_lastDdlTime 1279736040
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -961,11 +937,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-42_981_2840060153277077702/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-14_952_4306486872899832092/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-22-42_981_2840060153277077702/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-14_952_4306486872899832092/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -1050,11 +1026,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-23-00_031_5823005516510430721/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-27_262_8641490829427712090/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-23-00_031_5823005516510430721/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-27_262_8641490829427712090/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -1117,14 +1093,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-23-07_768_1978016191657897006/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-32_501_8471356144377928353/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-23-07_768_1978016191657897006/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-32_501_8471356144377928353/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -1150,177 +1126,3 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
 0	0	0
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_1
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_tmp_result
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin_part_2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/bucketmapjoin4.q.out b/ql/src/test/results/clientpositive/bucketmapjoin4.q.out
index 98b71b3031..04692b7987 100644
--- a/ql/src/test/results/clientpositive/bucketmapjoin4.q.out
+++ b/ql/src/test/results/clientpositive/bucketmapjoin4.q.out
@@ -1,27 +1,3 @@
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
@@ -154,7 +130,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10002
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -165,12 +141,12 @@ STAGE PLANS:
                           columns.types string:string:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                           name bucketmapjoin_tmp_result
                           serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1270584673
+                          transient_lastDdlTime 1279736077
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: bucketmapjoin_tmp_result
                     TotalFiles: 1
@@ -218,7 +194,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -229,12 +205,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270584673
+                              transient_lastDdlTime 1279736077
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
@@ -243,15 +219,15 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 b {srcbucket20.txt=[srcbucket20.txt], srcbucket21.txt=[srcbucket21.txt]}
               Alias Bucket File Name Mapping:
-                b {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
+                b {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt 1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin 
           Partition
             base file name: srcbucket_mapjoin
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -263,12 +239,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
               name srcbucket_mapjoin
               serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584671
+              transient_lastDdlTime 1279736075
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -280,12 +256,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
                 name srcbucket_mapjoin
                 serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584671
+                transient_lastDdlTime 1279736075
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin
             name: srcbucket_mapjoin
@@ -297,14 +273,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -314,20 +290,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584673
+                transient_lastDdlTime 1279736077
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -343,9 +319,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -356,12 +332,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584673
+              transient_lastDdlTime 1279736077
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -372,12 +348,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584673
+                transient_lastDdlTime 1279736077
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -386,7 +362,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-13_998_7329941088298190472/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-14-37_359_7882787011554545381/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -397,12 +373,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270584673
+                  transient_lastDdlTime 1279736077
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -429,11 +405,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-24_223_7876043218413061096/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-42_513_8950313021476459985/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-24_223_7876043218413061096/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-42_513_8950313021476459985/10000
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
@@ -480,11 +456,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-39_796_4513293516103830395/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-52_798_7853968826467475434/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-39_796_4513293516103830395/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-52_798_7853968826467475434/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -523,14 +499,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-47_319_1461580913286853723/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-58_049_5457597101616694369/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-47_319_1461580913286853723/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-14-58_049_5457597101616694369/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -618,7 +594,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10002
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -629,12 +605,12 @@ STAGE PLANS:
                           columns.types string:string:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                           name bucketmapjoin_tmp_result
                           serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1270584673
+                          transient_lastDdlTime 1279736092
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: bucketmapjoin_tmp_result
                     TotalFiles: 1
@@ -682,7 +658,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -693,12 +669,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270584673
+                              transient_lastDdlTime 1279736092
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
@@ -707,15 +683,15 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 a {srcbucket20.txt=[srcbucket20.txt], srcbucket21.txt=[srcbucket21.txt]}
               Alias Bucket File Name Mapping:
-                a {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
+                a {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt 1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin [b]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin [b]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin 
           Partition
             base file name: srcbucket_mapjoin
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -727,12 +703,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
               name srcbucket_mapjoin
               serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584671
+              transient_lastDdlTime 1279736075
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -744,12 +720,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
                 name srcbucket_mapjoin
                 serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584671
+                transient_lastDdlTime 1279736075
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin
             name: srcbucket_mapjoin
@@ -761,14 +737,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -778,20 +754,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584673
+                transient_lastDdlTime 1279736092
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -807,9 +783,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -820,12 +796,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584673
+              transient_lastDdlTime 1279736092
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -836,12 +812,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584673
+                transient_lastDdlTime 1279736092
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -850,7 +826,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-51_245_1777155812376648754/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-00_747_9056802752837912387/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -861,12 +837,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270584673
+                  transient_lastDdlTime 1279736092
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -905,11 +881,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-59_100_3501901590071019314/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-06_050_1984218270610080776/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-11-59_100_3501901590071019314/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-06_050_1984218270610080776/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -992,11 +968,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-12-13_616_4159794452577561003/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-16_364_6927203184308335043/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-12-13_616_4159794452577561003/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-16_364_6927203184308335043/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
@@ -1059,14 +1035,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-12-20_792_5741060792585939337/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-21_487_4696539047259572290/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-12-20_792_5741060792585939337/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-21_487_4696539047259572290/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
@@ -1092,177 +1068,3 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
 0	0	0
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_1
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_tmp_result
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin)b.FieldSchema(name:value, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/bucketmapjoin5.q.out b/ql/src/test/results/clientpositive/bucketmapjoin5.q.out
index b260af2068..fcf0c2d348 100644
--- a/ql/src/test/results/clientpositive/bucketmapjoin5.q.out
+++ b/ql/src/test/results/clientpositive/bucketmapjoin5.q.out
@@ -1,27 +1,3 @@
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
@@ -184,7 +160,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10002
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -195,12 +171,12 @@ STAGE PLANS:
                           columns.types string:string:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                           name bucketmapjoin_tmp_result
                           serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1270584880
+                          transient_lastDdlTime 1279736127
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: bucketmapjoin_tmp_result
                     TotalFiles: 1
@@ -248,7 +224,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -259,12 +235,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270584880
+                              transient_lastDdlTime 1279736127
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
@@ -273,22 +249,22 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 a {srcbucket20.txt=[srcbucket20.txt], srcbucket21.txt=[srcbucket21.txt], srcbucket22.txt=[srcbucket20.txt], srcbucket23.txt=[srcbucket21.txt], ds=2008-04-09/srcbucket20.txt=[srcbucket20.txt], ds=2008-04-09/srcbucket21.txt=[srcbucket21.txt], ds=2008-04-09/srcbucket22.txt=[srcbucket20.txt], ds=2008-04-09/srcbucket23.txt=[srcbucket21.txt]}
               Alias Bucket File Name Mapping:
-                a {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket20.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket21.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket22.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket23.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
+                a {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket20.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket21.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket22.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket23.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt 1
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt 2
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt 3
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket20.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket21.txt 1
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket22.txt 2
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket23.txt 3
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket20.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket21.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket22.txt 2
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08/srcbucket23.txt 3
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket20.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket21.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket22.txt 2
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09/srcbucket23.txt 3
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 [b]
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09 [b]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 [b]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09 [b]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-08 
           Partition
             base file name: ds=2008-04-08
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -302,13 +278,13 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part
               name srcbucket_mapjoin_part
               partition_columns ds
               serialization.ddl struct srcbucket_mapjoin_part { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584877
+              transient_lastDdlTime 1279736124
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -320,17 +296,17 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part
                 name srcbucket_mapjoin_part
                 partition_columns ds
                 serialization.ddl struct srcbucket_mapjoin_part { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584877
+                transient_lastDdlTime 1279736124
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin_part
             name: srcbucket_mapjoin_part
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part/ds=2008-04-09 
           Partition
             base file name: ds=2008-04-09
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -344,13 +320,13 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part
               name srcbucket_mapjoin_part
               partition_columns ds
               serialization.ddl struct srcbucket_mapjoin_part { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584877
+              transient_lastDdlTime 1279736124
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -362,13 +338,13 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part
                 name srcbucket_mapjoin_part
                 partition_columns ds
                 serialization.ddl struct srcbucket_mapjoin_part { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584877
+                transient_lastDdlTime 1279736124
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin_part
             name: srcbucket_mapjoin_part
@@ -380,14 +356,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -397,20 +373,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584880
+                transient_lastDdlTime 1279736127
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -426,9 +402,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -439,12 +415,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584880
+              transient_lastDdlTime 1279736127
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -455,12 +431,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584880
+                transient_lastDdlTime 1279736127
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -469,7 +445,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-40_283_968548719978651164/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-27_050_3859053918333083391/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -480,12 +456,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270584880
+                  transient_lastDdlTime 1279736127
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -516,11 +492,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-52_156_4855266710517903794/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-34_483_4663724029568697547/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-14-52_156_4855266710517903794/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-34_483_4663724029568697547/10000
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
@@ -571,11 +547,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-09_738_8775012863049281501/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-47_270_4545636427863902378/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-09_738_8775012863049281501/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-47_270_4545636427863902378/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -614,14 +590,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-17_170_2507838828904708527/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-52_543_2406511845804161610/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-17_170_2507838828904708527/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-15-52_543_2406511845804161610/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -709,7 +685,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10002
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -720,12 +696,12 @@ STAGE PLANS:
                           columns.types string:string:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                           name bucketmapjoin_tmp_result
                           serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1270584880
+                          transient_lastDdlTime 1279736147
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: bucketmapjoin_tmp_result
                     TotalFiles: 1
@@ -773,7 +749,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -784,12 +760,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270584880
+                              transient_lastDdlTime 1279736147
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
@@ -798,18 +774,18 @@ STAGE PLANS:
               Alias Bucket Base File Name Mapping:
                 a {srcbucket22.txt=[srcbucket20.txt], srcbucket23.txt=[srcbucket21.txt], ds=2008-04-09/srcbucket22.txt=[srcbucket20.txt], ds=2008-04-09/srcbucket23.txt=[srcbucket21.txt]}
               Alias Bucket File Name Mapping:
-                a {file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09/srcbucket22.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09/srcbucket23.txt=[file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
+                a {file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09/srcbucket22.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket20.txt], file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09/srcbucket23.txt=[file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin/srcbucket21.txt]}
               Alias Bucket Output File Name Mapping:
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt 1
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09/srcbucket22.txt 0
-                file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09/srcbucket23.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket22.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08/srcbucket23.txt 1
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09/srcbucket22.txt 0
+                file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09/srcbucket23.txt 1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 [b]
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09 [b]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 [b]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09 [b]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-08 
           Partition
             base file name: ds=2008-04-08
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -823,13 +799,13 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
               name srcbucket_mapjoin_part_2
               partition_columns ds
               serialization.ddl struct srcbucket_mapjoin_part_2 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584879
+              transient_lastDdlTime 1279736126
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -841,17 +817,17 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
                 name srcbucket_mapjoin_part_2
                 partition_columns ds
                 serialization.ddl struct srcbucket_mapjoin_part_2 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584879
+                transient_lastDdlTime 1279736126
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin_part_2
             name: srcbucket_mapjoin_part_2
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2/ds=2008-04-09 
           Partition
             base file name: ds=2008-04-09
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -865,13 +841,13 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
               name srcbucket_mapjoin_part_2
               partition_columns ds
               serialization.ddl struct srcbucket_mapjoin_part_2 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584879
+              transient_lastDdlTime 1279736126
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -883,13 +859,13 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin_part_2
                 name srcbucket_mapjoin_part_2
                 partition_columns ds
                 serialization.ddl struct srcbucket_mapjoin_part_2 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584879
+                transient_lastDdlTime 1279736126
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin_part_2
             name: srcbucket_mapjoin_part_2
@@ -901,14 +877,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10002
-          destination: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -918,20 +894,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584880
+                transient_lastDdlTime 1279736147
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -947,9 +923,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10002 [file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10002]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -960,12 +936,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584880
+              transient_lastDdlTime 1279736147
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -976,12 +952,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584880
+                transient_lastDdlTime 1279736147
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -990,7 +966,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-21_973_5804199576184173549/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-15-55_337_7214106352789078536/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -1001,12 +977,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270584880
+                  transient_lastDdlTime 1279736147
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
@@ -1049,11 +1025,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-31_179_7313595341297279681/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-16-00_681_6039313017560460235/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-31_179_7313595341297279681/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-16-00_681_6039313017560460235/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
@@ -1140,11 +1116,11 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 PREHOOK: query: select count(1) from bucketmapjoin_tmp_result
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_tmp_result
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-46_464_3385157090510772728/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-16-12_250_4593300394239034330/10000
 POSTHOOK: query: select count(1) from bucketmapjoin_tmp_result
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_tmp_result
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-46_464_3385157090510772728/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-16-12_250_4593300394239034330/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
@@ -1207,14 +1183,14 @@ on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucketmapjoin_hash_result_2
 PREHOOK: Input: default@bucketmapjoin_hash_result_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-54_066_2106291188917250080/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-16-17_491_993666857276206775/10000
 POSTHOOK: query: select a.key-b.key, a.value1-b.value1, a.value2-b.value2
 from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
 on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucketmapjoin_hash_result_2
 POSTHOOK: Input: default@bucketmapjoin_hash_result_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-15-54_066_2106291188917250080/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-16-17_491_993666857276206775/10000
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
@@ -1240,177 +1216,3 @@ POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_pa
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
 NULL	NULL	NULL
-PREHOOK: query: drop table bucketmapjoin_hash_result_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_hash_result_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_hash_result_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_hash_result_1
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_tmp_result
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part_2
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_1.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.key EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value1 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value1, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_hash_result_2.value2 EXPRESSION [(bucketmapjoin_tmp_result)bucketmapjoin_tmp_result.FieldSchema(name:value2, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.key SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value1 SIMPLE [(srcbucket_mapjoin)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: bucketmapjoin_tmp_result.value2 SIMPLE [(srcbucket_mapjoin_part_2)b.FieldSchema(name:key, type:int, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/bucketmapjoin6.q.out b/ql/src/test/results/clientpositive/bucketmapjoin6.q.out
index db41bc8b46..6c74f3cf9d 100644
--- a/ql/src/test/results/clientpositive/bucketmapjoin6.q.out
+++ b/ql/src/test/results/clientpositive/bucketmapjoin6.q.out
@@ -1,16 +1,8 @@
-PREHOOK: query: drop table tmp1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table tmp1 (a string, b string) clustered by (a) sorted by (a) into 10 buckets
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tmp1 (a string, b string) clustered by (a) sorted by (a) into 10 buckets
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tmp1
-PREHOOK: query: drop table tmp2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table tmp2 (a string, b string) clustered by (a) sorted by (a) into 10 buckets
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tmp2 (a string, b string) clustered by (a) sorted by (a) into 10 buckets
@@ -38,14 +30,6 @@ POSTHOOK: Lineage: tmp1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, co
 POSTHOOK: Lineage: tmp1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp2.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table tmp3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: tmp1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp2.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: create table tmp3 (a string, b string, c string) clustered by (a) sorted by (a) into 10 buckets
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tmp3 (a string, b string, c string) clustered by (a) sorted by (a) into 10 buckets
@@ -79,11 +63,11 @@ POSTHOOK: Lineage: tmp3.c SIMPLE [(tmp2)l.FieldSchema(name:b, type:string, comme
 PREHOOK: query: select * from tmp3 order by a, b, c
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp3
-PREHOOK: Output: file:/data/users/nzhang/work/999/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-04-20_11-17-38_126_6207291158988738744/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-16-34_797_3291869263216721850/10000
 POSTHOOK: query: select * from tmp3 order by a, b, c
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp3
-POSTHOOK: Output: file:/data/users/nzhang/work/999/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-04-20_11-17-38_126_6207291158988738744/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-16-34_797_3291869263216721850/10000
 POSTHOOK: Lineage: tmp1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -164,39 +148,3 @@ POSTHOOK: Lineage: tmp3.c SIMPLE [(tmp2)l.FieldSchema(name:b, type:string, comme
 5	val_5	val_5
 8	val_8	val_8
 9	val_9	val_9
-PREHOOK: query: drop table tmp1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp1
-POSTHOOK: Lineage: tmp1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp2.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp3.a SIMPLE [(tmp1)i.FieldSchema(name:a, type:string, comment:null), ]
-POSTHOOK: Lineage: tmp3.b SIMPLE [(tmp1)i.FieldSchema(name:b, type:string, comment:null), ]
-POSTHOOK: Lineage: tmp3.c SIMPLE [(tmp2)l.FieldSchema(name:b, type:string, comment:null), ]
-PREHOOK: query: drop table tmp2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp2
-POSTHOOK: Lineage: tmp1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp2.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp3.a SIMPLE [(tmp1)i.FieldSchema(name:a, type:string, comment:null), ]
-POSTHOOK: Lineage: tmp3.b SIMPLE [(tmp1)i.FieldSchema(name:b, type:string, comment:null), ]
-POSTHOOK: Lineage: tmp3.c SIMPLE [(tmp2)l.FieldSchema(name:b, type:string, comment:null), ]
-PREHOOK: query: drop table tmp3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp3
-POSTHOOK: Lineage: tmp1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp2.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp3.a SIMPLE [(tmp1)i.FieldSchema(name:a, type:string, comment:null), ]
-POSTHOOK: Lineage: tmp3.b SIMPLE [(tmp1)i.FieldSchema(name:b, type:string, comment:null), ]
-POSTHOOK: Lineage: tmp3.c SIMPLE [(tmp2)l.FieldSchema(name:b, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out b/ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out
index 2f97fcae49..36cad6c1c4 100644
--- a/ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out
+++ b/ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
@@ -119,7 +107,7 @@ STAGE PLANS:
                     File Output Operator
                       compressed: false
                       GlobalTableId: 1
-                      directory: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10002
+                      directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10002
                       NumFilesPerFileSink: 1
                       table:
                           input format: org.apache.hadoop.mapred.TextInputFormat
@@ -130,12 +118,12 @@ STAGE PLANS:
                             columns.types string:string:string
                             file.inputformat org.apache.hadoop.mapred.TextInputFormat
                             file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                            location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                             name bucketmapjoin_tmp_result
                             serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                             serialization.format 1
                             serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                            transient_lastDdlTime 1268349432
+                            transient_lastDdlTime 1279736200
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                           name: bucketmapjoin_tmp_result
                       TotalFiles: 1
@@ -195,7 +183,7 @@ STAGE PLANS:
                           File Output Operator
                             compressed: false
                             GlobalTableId: 1
-                            directory: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10002
+                            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10002
                             NumFilesPerFileSink: 1
                             table:
                                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -206,21 +194,21 @@ STAGE PLANS:
                                   columns.types string:string:string
                                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                  location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                                   name bucketmapjoin_tmp_result
                                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                                   serialization.format 1
                                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                  transient_lastDdlTime 1268349432
+                                  transient_lastDdlTime 1279736200
                                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                                 name: bucketmapjoin_tmp_result
                             TotalFiles: 1
                             MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
       Path -> Partition:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/srcbucket_mapjoin 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin 
           Partition
             base file name: srcbucket_mapjoin
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -232,12 +220,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/srcbucket_mapjoin
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
               name srcbucket_mapjoin
               serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1268349430
+              transient_lastDdlTime 1279736199
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -249,12 +237,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/srcbucket_mapjoin
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
                 name srcbucket_mapjoin
                 serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1268349430
+                transient_lastDdlTime 1279736199
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin
             name: srcbucket_mapjoin
@@ -266,14 +254,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10002
-          destination: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -283,20 +271,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1268349432
+                transient_lastDdlTime 1279736200
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -312,9 +300,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10002 [file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10002]
       Path -> Partition:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -325,12 +313,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1268349432
+              transient_lastDdlTime 1279736200
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -341,12 +329,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1268349432
+                transient_lastDdlTime 1279736200
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -355,7 +343,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-12_145_6529332471381915674/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-40_189_5186388434790344681/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -366,30 +354,15 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1268349432
+                  transient_lastDdlTime 1279736200
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
             MultiFileSpray: false
 
 
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_tmp_result
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin
-PREHOOK: query: drop table srcbucket_mapjoin_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part
diff --git a/ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out b/ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out
index 85c32ce627..186016f77c 100644
--- a/ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out
+++ b/ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
@@ -117,7 +105,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10002
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -128,12 +116,12 @@ STAGE PLANS:
                           columns.types string:string:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                           name bucketmapjoin_tmp_result
                           serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1268349439
+                          transient_lastDdlTime 1279736202
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: bucketmapjoin_tmp_result
                     TotalFiles: 1
@@ -181,7 +169,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -192,21 +180,21 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                               name bucketmapjoin_tmp_result
                               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1268349439
+                              transient_lastDdlTime 1279736202
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: bucketmapjoin_tmp_result
                         TotalFiles: 1
                         MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin [a]
       Path -> Partition:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/srcbucket_mapjoin 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin 
           Partition
             base file name: srcbucket_mapjoin
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -218,12 +206,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/srcbucket_mapjoin
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
               name srcbucket_mapjoin
               serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1268349439
+              transient_lastDdlTime 1279736201
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -235,12 +223,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/srcbucket_mapjoin
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket_mapjoin
                 name srcbucket_mapjoin
                 serialization.ddl struct srcbucket_mapjoin { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1268349439
+                transient_lastDdlTime 1279736201
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket_mapjoin
             name: srcbucket_mapjoin
@@ -252,14 +240,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10002
-          destination: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -269,20 +257,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1268349439
+                transient_lastDdlTime 1279736202
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
-          tmp directory: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -298,9 +286,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10002 [file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10002]
       Path -> Partition:
-        file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -311,12 +299,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
               name bucketmapjoin_tmp_result
               serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1268349439
+              transient_lastDdlTime 1279736202
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -327,12 +315,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                 name bucketmapjoin_tmp_result
                 serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1268349439
+                transient_lastDdlTime 1279736202
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucketmapjoin_tmp_result
             name: bucketmapjoin_tmp_result
@@ -341,7 +329,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/scratchdir/hive_2010-03-11_15-17-19_959_8591498588687716490/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-16-42_513_7749242910411955898/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -352,30 +340,15 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/Users/heyongqiang/Documents/workspace/Hive-460/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucketmapjoin_tmp_result
                   name bucketmapjoin_tmp_result
                   serialization.ddl struct bucketmapjoin_tmp_result { string key, string value1, string value2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1268349439
+                  transient_lastDdlTime 1279736202
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: bucketmapjoin_tmp_result
             TotalFiles: 1
             MultiFileSpray: false
 
 
-PREHOOK: query: drop table bucketmapjoin_tmp_result
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucketmapjoin_tmp_result
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucketmapjoin_tmp_result
-PREHOOK: query: drop table srcbucket_mapjoin
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin
-PREHOOK: query: drop table srcbucket_mapjoin_part_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcbucket_mapjoin_part_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcbucket_mapjoin_part_2
diff --git a/ql/src/test/results/clientpositive/columnarserde_create_shortcut.q.out b/ql/src/test/results/clientpositive/columnarserde_create_shortcut.q.out
index 20e0e23b9b..cd8dc72172 100644
--- a/ql/src/test/results/clientpositive/columnarserde_create_shortcut.q.out
+++ b/ql/src/test/results/clientpositive/columnarserde_create_shortcut.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table columnarserde_create_shortcut
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table columnarserde_create_shortcut
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE columnarserde_create_shortcut(a array<int>, b array<string>, c map<string,string>, d int, e string) STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE columnarserde_create_shortcut(a array<int>, b array<string>, c map<string,string>, d int, e string) STORED AS RCFILE
@@ -99,11 +95,11 @@ POSTHOOK: Lineage: columnarserde_create_shortcut.e SIMPLE [(src_thrift)src_thrif
 PREHOOK: query: SELECT columnarserde_create_shortcut.* FROM columnarserde_create_shortcut DISTRIBUTE BY 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@columnarserde_create_shortcut
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-14_061_5293390502058553414/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-40_562_3691730586844357088/10000
 POSTHOOK: query: SELECT columnarserde_create_shortcut.* FROM columnarserde_create_shortcut DISTRIBUTE BY 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@columnarserde_create_shortcut
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-14_061_5293390502058553414/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-40_562_3691730586844357088/10000
 POSTHOOK: Lineage: columnarserde_create_shortcut.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -123,11 +119,11 @@ null	null	null	0	NULL
 PREHOOK: query: SELECT columnarserde_create_shortcut.a[0], columnarserde_create_shortcut.b[0], columnarserde_create_shortcut.c['key2'], columnarserde_create_shortcut.d, columnarserde_create_shortcut.e FROM columnarserde_create_shortcut DISTRIBUTE BY 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@columnarserde_create_shortcut
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-18_867_3257442361136725757/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-43_320_5896959591483174656/10000
 POSTHOOK: query: SELECT columnarserde_create_shortcut.a[0], columnarserde_create_shortcut.b[0], columnarserde_create_shortcut.c['key2'], columnarserde_create_shortcut.d, columnarserde_create_shortcut.e FROM columnarserde_create_shortcut DISTRIBUTE BY 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@columnarserde_create_shortcut
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-18_867_3257442361136725757/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-43_320_5896959591483174656/10000
 POSTHOOK: Lineage: columnarserde_create_shortcut.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -144,25 +140,6 @@ POSTHOOK: Lineage: columnarserde_create_shortcut.e SIMPLE [(src_thrift)src_thrif
 8	80	NULL	1638581578	record_8
 9	90	NULL	336964413	record_9
 NULL	NULL	NULL	0	NULL
-PREHOOK: query: drop table columnarserde_create_shortcut
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table columnarserde_create_shortcut
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@columnarserde_create_shortcut
-POSTHOOK: Lineage: columnarserde_create_shortcut.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.d SIMPLE [(src_thrift)src_thrift.FieldSchema(name:aint, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.e SIMPLE [(src_thrift)src_thrift.FieldSchema(name:astring, type:string, comment:from deserializer), ]
-PREHOOK: query: DROP TABLE columnShortcutTable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE columnShortcutTable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: columnarserde_create_shortcut.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.d SIMPLE [(src_thrift)src_thrift.FieldSchema(name:aint, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.e SIMPLE [(src_thrift)src_thrift.FieldSchema(name:astring, type:string, comment:from deserializer), ]
 PREHOOK: query: CREATE table columnShortcutTable (key STRING, value STRING) STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE table columnShortcutTable (key STRING, value STRING) STORED AS RCFILE
@@ -206,11 +183,11 @@ value	string	from deserializer
 PREHOOK: query: SELECT columnShortcutTable.* FROM columnShortcutTable
 PREHOOK: type: QUERY
 PREHOOK: Input: default@columnshortcuttable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-28_297_7270524001790481999/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-48_648_1393169269395631017/10000
 POSTHOOK: query: SELECT columnShortcutTable.* FROM columnShortcutTable
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@columnshortcuttable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-28_297_7270524001790481999/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-48_648_1393169269395631017/10000
 POSTHOOK: Lineage: columnarserde_create_shortcut.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -244,11 +221,11 @@ POSTHOOK: Lineage: columnshortcuttable.value SIMPLE [(src)src.FieldSchema(name:v
 PREHOOK: query: SELECT columnShortcutTable.* FROM columnShortcutTable
 PREHOOK: type: QUERY
 PREHOOK: Input: default@columnshortcuttable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-28_514_9146229860466115009/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-49_046_3922128926987640091/10000
 POSTHOOK: query: SELECT columnShortcutTable.* FROM columnShortcutTable
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@columnshortcuttable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-28_514_9146229860466115009/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-49_046_3922128926987640091/10000
 POSTHOOK: Lineage: columnarserde_create_shortcut.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -282,11 +259,11 @@ POSTHOOK: Lineage: columnshortcuttable.value SIMPLE [(src)src.FieldSchema(name:v
 PREHOOK: query: SELECT columnShortcutTable.* FROM columnShortcutTable
 PREHOOK: type: QUERY
 PREHOOK: Input: default@columnshortcuttable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-28_617_3904819857604214172/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-49_171_4446044278977976579/10000
 POSTHOOK: query: SELECT columnShortcutTable.* FROM columnShortcutTable
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@columnshortcuttable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-28_617_3904819857604214172/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-49_171_4446044278977976579/10000
 POSTHOOK: Lineage: columnarserde_create_shortcut.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
 POSTHOOK: Lineage: columnarserde_create_shortcut.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -304,15 +281,3 @@ POSTHOOK: Lineage: columnshortcuttable.value SIMPLE [(src)src.FieldSchema(name:v
 278
 98
 484
-PREHOOK: query: DROP TABLE columnShortcutTable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE columnShortcutTable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@columnshortcuttable
-POSTHOOK: Lineage: columnarserde_create_shortcut.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.d SIMPLE [(src_thrift)src_thrift.FieldSchema(name:aint, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: columnarserde_create_shortcut.e SIMPLE [(src_thrift)src_thrift.FieldSchema(name:astring, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: columnshortcuttable.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: columnshortcuttable.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/combine1.q.out b/ql/src/test/results/clientpositive/combine1.q.out
index 7a0ddd80ab..f2a28c0d47 100644
--- a/ql/src/test/results/clientpositive/combine1.q.out
+++ b/ql/src/test/results/clientpositive/combine1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table combine1_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table combine1_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table combine1_1(key string, value string) stored as textfile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table combine1_1(key string, value string) stored as textfile
@@ -22,11 +18,11 @@ POSTHOOK: Lineage: combine1_1.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select key, value from combine1_1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@combine1_1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-07-13_390_8320671278851184902/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-52_177_5648847098652116494/10000
 POSTHOOK: query: select key, value from combine1_1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@combine1_1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-07-13_390_8320671278851184902/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-52_177_5648847098652116494/10000
 POSTHOOK: Lineage: combine1_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: combine1_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 238	val_238
@@ -529,10 +525,3 @@ POSTHOOK: Lineage: combine1_1.value SIMPLE [(src)src.FieldSchema(name:value, typ
 400	val_400
 200	val_200
 97	val_97
-PREHOOK: query: drop table combine1_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table combine1_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@combine1_1
-POSTHOOK: Lineage: combine1_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: combine1_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/combine2.q.out b/ql/src/test/results/clientpositive/combine2.q.out
index 7de33a6bc8..44deb63b75 100644
--- a/ql/src/test/results/clientpositive/combine2.q.out
+++ b/ql/src/test/results/clientpositive/combine2.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table combine2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table combine2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table combine2(key string) partitioned by (value string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table combine2(key string) partitioned by (value string)
@@ -139,7 +135,7 @@ PREHOOK: Input: default@combine2@value=val_5
 PREHOOK: Input: default@combine2@value=val_8
 PREHOOK: Input: default@combine2@value=val_9
 PREHOOK: Input: default@combine2@value=|
-PREHOOK: Output: file:/data/users/nzhang/work/870/apache-hive/build/ql/scratchdir/hive_2010-05-03_14-20-13_883_855858049469186899/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-58_631_7891940326859316299/10000
 POSTHOOK: query: select key, value from combine2 where value is not null order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@combine2@value=2010-04-21 09%3A45%3A00
@@ -150,7 +146,7 @@ POSTHOOK: Input: default@combine2@value=val_5
 POSTHOOK: Input: default@combine2@value=val_8
 POSTHOOK: Input: default@combine2@value=val_9
 POSTHOOK: Input: default@combine2@value=|
-POSTHOOK: Output: file:/data/users/nzhang/work/870/apache-hive/build/ql/scratchdir/hive_2010-05-03_14-20-13_883_855858049469186899/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-17-58_631_7891940326859316299/10000
 POSTHOOK: Lineage: combine2 PARTITION(value=2010-04-21 09:45:00).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: combine2 PARTITION(value=val_0).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: combine2 PARTITION(value=val_2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -224,18 +220,18 @@ STAGE PLANS:
                             type: bigint
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=2010-04-21 09%3A45%3A00 [combine2]
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_0 [combine2]
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_2 [combine2]
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_4 [combine2]
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_5 [combine2]
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_8 [combine2]
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_9 [combine2]
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=| [combine2]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=2010-04-21 09%3A45%3A00 [combine2]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_0 [combine2]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_2 [combine2]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_4 [combine2]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_5 [combine2]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_8 [combine2]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_9 [combine2]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=| [combine2]
       Path -> Partition:
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=2010-04-21 09%3A45%3A00 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=2010-04-21 09%3A45%3A00 
           Partition
-            base file name: file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=2010-04-21 09%3A45%3A00
+            base file name: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=2010-04-21 09%3A45%3A00
             input format: org.apache.hadoop.mapred.TextInputFormat
             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
             partition values:
@@ -246,13 +242,13 @@ STAGE PLANS:
               columns.types string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
               name combine2
               partition_columns value
               serialization.ddl struct combine2 { string key}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1272921606
+              transient_lastDdlTime 1279736275
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -263,17 +259,17 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
                 name combine2
                 partition_columns value
                 serialization.ddl struct combine2 { string key}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1272921606
+                transient_lastDdlTime 1279736275
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: combine2
             name: combine2
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_0 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_0 
           Partition
             base file name: value=val_0
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -286,13 +282,13 @@ STAGE PLANS:
               columns.types string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
               name combine2
               partition_columns value
               serialization.ddl struct combine2 { string key}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1272921606
+              transient_lastDdlTime 1279736275
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -303,17 +299,17 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
                 name combine2
                 partition_columns value
                 serialization.ddl struct combine2 { string key}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1272921606
+                transient_lastDdlTime 1279736275
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: combine2
             name: combine2
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_2 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_2 
           Partition
             base file name: value=val_2
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -326,13 +322,13 @@ STAGE PLANS:
               columns.types string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
               name combine2
               partition_columns value
               serialization.ddl struct combine2 { string key}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1272921606
+              transient_lastDdlTime 1279736275
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -343,17 +339,17 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
                 name combine2
                 partition_columns value
                 serialization.ddl struct combine2 { string key}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1272921606
+                transient_lastDdlTime 1279736275
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: combine2
             name: combine2
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_4 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_4 
           Partition
             base file name: value=val_4
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -366,13 +362,13 @@ STAGE PLANS:
               columns.types string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
               name combine2
               partition_columns value
               serialization.ddl struct combine2 { string key}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1272921606
+              transient_lastDdlTime 1279736275
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -383,17 +379,17 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
                 name combine2
                 partition_columns value
                 serialization.ddl struct combine2 { string key}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1272921606
+                transient_lastDdlTime 1279736275
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: combine2
             name: combine2
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_5 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_5 
           Partition
             base file name: value=val_5
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -406,13 +402,13 @@ STAGE PLANS:
               columns.types string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
               name combine2
               partition_columns value
               serialization.ddl struct combine2 { string key}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1272921606
+              transient_lastDdlTime 1279736275
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -423,17 +419,17 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
                 name combine2
                 partition_columns value
                 serialization.ddl struct combine2 { string key}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1272921606
+                transient_lastDdlTime 1279736275
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: combine2
             name: combine2
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_8 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_8 
           Partition
             base file name: value=val_8
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -446,13 +442,13 @@ STAGE PLANS:
               columns.types string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
               name combine2
               partition_columns value
               serialization.ddl struct combine2 { string key}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1272921606
+              transient_lastDdlTime 1279736275
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -463,17 +459,17 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
                 name combine2
                 partition_columns value
                 serialization.ddl struct combine2 { string key}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1272921606
+                transient_lastDdlTime 1279736275
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: combine2
             name: combine2
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=val_9 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=val_9 
           Partition
             base file name: value=val_9
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -486,13 +482,13 @@ STAGE PLANS:
               columns.types string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
               name combine2
               partition_columns value
               serialization.ddl struct combine2 { string key}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1272921606
+              transient_lastDdlTime 1279736275
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -503,19 +499,19 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
                 name combine2
                 partition_columns value
                 serialization.ddl struct combine2 { string key}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1272921606
+                transient_lastDdlTime 1279736275
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: combine2
             name: combine2
-        file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=| 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=| 
           Partition
-            base file name: file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2/value=|
+            base file name: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2/value=|
             input format: org.apache.hadoop.mapred.TextInputFormat
             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
             partition values:
@@ -526,13 +522,13 @@ STAGE PLANS:
               columns.types string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
               name combine2
               partition_columns value
               serialization.ddl struct combine2 { string key}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1272921606
+              transient_lastDdlTime 1279736275
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -543,13 +539,13 @@ STAGE PLANS:
                 columns.types string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/870/apache-hive/build/ql/test/data/warehouse/combine2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/combine2
                 name combine2
                 partition_columns value
                 serialization.ddl struct combine2 { string key}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1272921606
+                transient_lastDdlTime 1279736275
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: combine2
             name: combine2
@@ -568,7 +564,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-              directory: file:/data/users/nzhang/work/870/apache-hive/build/ql/scratchdir/hive_2010-05-03_14-20-20_454_1592342701152243108/10001
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-18-02_660_5546731649050577338/10001
               NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -595,7 +591,7 @@ PREHOOK: Input: default@combine2@value=val_5
 PREHOOK: Input: default@combine2@value=val_8
 PREHOOK: Input: default@combine2@value=val_9
 PREHOOK: Input: default@combine2@value=|
-PREHOOK: Output: file:/data/users/nzhang/work/870/apache-hive/build/ql/scratchdir/hive_2010-05-03_14-20-21_322_7197686956634622367/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-02_861_7906523373378058182/10000
 POSTHOOK: query: select count(1) from combine2 where value is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@combine2@value=2010-04-21 09%3A45%3A00
@@ -606,7 +602,7 @@ POSTHOOK: Input: default@combine2@value=val_5
 POSTHOOK: Input: default@combine2@value=val_8
 POSTHOOK: Input: default@combine2@value=val_9
 POSTHOOK: Input: default@combine2@value=|
-POSTHOOK: Output: file:/data/users/nzhang/work/870/apache-hive/build/ql/scratchdir/hive_2010-05-03_14-20-21_322_7197686956634622367/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-02_861_7906523373378058182/10000
 POSTHOOK: Lineage: combine2 PARTITION(value=2010-04-21 09:45:00).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: combine2 PARTITION(value=val_0).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: combine2 PARTITION(value=val_2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -713,14 +709,14 @@ PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
 PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/870/apache-hive/build/ql/scratchdir/hive_2010-05-03_14-20-28_859_8819683951488328325/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-06_997_989404447897197159/10000
 POSTHOOK: query: select ds, count(1) from srcpart where ds is not null group by ds
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
 POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/870/apache-hive/build/ql/scratchdir/hive_2010-05-03_14-20-28_859_8819683951488328325/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-06_997_989404447897197159/10000
 POSTHOOK: Lineage: combine2 PARTITION(value=2010-04-21 09:45:00).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: combine2 PARTITION(value=val_0).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: combine2 PARTITION(value=val_2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -731,16 +727,3 @@ POSTHOOK: Lineage: combine2 PARTITION(value=val_9).key EXPRESSION [(src)src.Fiel
 POSTHOOK: Lineage: combine2 PARTITION(value=|).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
 2008-04-08	1000
 2008-04-09	1000
-PREHOOK: query: drop table combine2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table combine2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@combine2
-POSTHOOK: Lineage: combine2 PARTITION(value=2010-04-21 09:45:00).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: combine2 PARTITION(value=val_0).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: combine2 PARTITION(value=val_2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: combine2 PARTITION(value=val_4).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: combine2 PARTITION(value=val_5).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: combine2 PARTITION(value=val_8).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: combine2 PARTITION(value=val_9).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: combine2 PARTITION(value=|).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/cp_mj_rc.q.out b/ql/src/test/results/clientpositive/cp_mj_rc.q.out
index c3ecec193b..77da7ac26b 100644
--- a/ql/src/test/results/clientpositive/cp_mj_rc.q.out
+++ b/ql/src/test/results/clientpositive/cp_mj_rc.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table src_six_columns
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table src_six_columns
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table src_two_columns
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table src_two_columns
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table src_six_columns (k1 string, v1 string, k2 string, v2 string, k3 string, v3 string) stored as rcfile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table src_six_columns (k1 string, v1 string, k2 string, v2 string, k3 string, v3 string) stored as rcfile
@@ -56,12 +48,12 @@ PREHOOK: query: SELECT /*+ MAPJOIN(six) */ six.*, two.k1 from src_six_columns si
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_two_columns
 PREHOOK: Input: default@src_six_columns
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-26-53_781_222438189233988194/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-15_717_1117205058531623654/10000
 POSTHOOK: query: SELECT /*+ MAPJOIN(six) */ six.*, two.k1 from src_six_columns six join src_two_columns two on (six.k3=two.k1)
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_two_columns
 POSTHOOK: Input: default@src_six_columns
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-26-53_781_222438189233988194/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-15_717_1117205058531623654/10000
 POSTHOOK: Lineage: src_six_columns.k1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: src_six_columns.k2 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_six_columns.k3 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -74,38 +66,12 @@ PREHOOK: query: SELECT /*+ MAPJOIN(two) */ two.*, six.k3 from src_six_columns si
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_two_columns
 PREHOOK: Input: default@src_six_columns
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-26-58_197_6334871238575402837/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-18_496_6407982316158329228/10000
 POSTHOOK: query: SELECT /*+ MAPJOIN(two) */ two.*, six.k3 from src_six_columns six join src_two_columns two on (six.k3=two.k1)
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_two_columns
 POSTHOOK: Input: default@src_six_columns
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-26-58_197_6334871238575402837/10000
-POSTHOOK: Lineage: src_six_columns.k1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.k2 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.k3 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.v1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.v2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.v3 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_two_columns.k1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src_two_columns.v1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table src_six_columns
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table src_six_columns
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@src_six_columns
-POSTHOOK: Lineage: src_six_columns.k1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.k2 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.k3 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.v1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.v2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_six_columns.v3 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_two_columns.k1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src_two_columns.v1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table src_two_columns
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table src_two_columns
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@src_two_columns
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-18_496_6407982316158329228/10000
 POSTHOOK: Lineage: src_six_columns.k1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: src_six_columns.k2 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_six_columns.k3 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/create_1.q.out b/ql/src/test/results/clientpositive/create_1.q.out
index 8ddb626a92..26e00ed789 100644
--- a/ql/src/test/results/clientpositive/create_1.q.out
+++ b/ql/src/test/results/clientpositive/create_1.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: DROP TABLE table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE table2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE table3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE table1 (a STRING, b STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE table1 (a STRING, b STRING) STORED AS TEXTFILE
@@ -28,7 +16,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table1, dbName:default, owner:nzhang, createTime:1254242350, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
+Detailed Table Information	Table(tableName:table1, dbName:default, owner:jssarma, createTime:1279736301, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736301}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: CREATE TABLE IF NOT EXISTS table1 (a STRING, b STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE IF NOT EXISTS table1 (a STRING, b STRING) STORED AS TEXTFILE
@@ -51,7 +39,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	int	
 	 	 
-Detailed Table Information	Table(tableName:table2, dbName:default, owner:nzhang, createTime:1254242351, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/test/data/warehouse/table2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
+Detailed Table Information	Table(tableName:table2, dbName:default, owner:jssarma, createTime:1279736301, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736301}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: CREATE TABLE table3 (a STRING, b STRING)
 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
 STORED AS TEXTFILE
@@ -74,19 +62,4 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table3, dbName:default, owner:nzhang, createTime:1254242351, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/nzhang/work/31/apache-hive-trunk/build/ql/test/data/warehouse/table3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=9,field.delim=	}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})
-PREHOOK: query: DROP TABLE table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table1
-PREHOOK: query: DROP TABLE table2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table2
-PREHOOK: query: DROP TABLE table3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table3
+Detailed Table Information	Table(tableName:table3, dbName:default, owner:jssarma, createTime:1279736302, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=	, field.delim=
diff --git a/ql/src/test/results/clientpositive/create_escape.q.out b/ql/src/test/results/clientpositive/create_escape.q.out
index db96cb99b4..b7d14e9797 100644
--- a/ql/src/test/results/clientpositive/create_escape.q.out
+++ b/ql/src/test/results/clientpositive/create_escape.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: DROP TABLE table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE table2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE table3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE table1 (a STRING, b STRING)
 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' ESCAPED BY '\\'
 STORED AS TEXTFILE
@@ -32,7 +20,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table1, dbName:default, owner:athusoo, createTime:1270516040, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=	,escape.delim=\,field.delim=
+Detailed Table Information	Table(tableName:table1, dbName:default, owner:jssarma, createTime:1279736306, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=	, escape.delim=\, field.delim=
 PREHOOK: query: INSERT OVERWRITE TABLE table1 SELECT key, '\\\t\\' FROM src WHERE key = 86
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -46,18 +34,11 @@ POSTHOOK: Lineage: table1.b SIMPLE []
 PREHOOK: query: SELECT * FROM table1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-07-24_966_4021548022979419216/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-29_057_3198903831648659084/10000
 POSTHOOK: query: SELECT * FROM table1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-07-24_966_4021548022979419216/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-29_057_3198903831648659084/10000
 POSTHOOK: Lineage: table1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: table1.b SIMPLE []
 86	\	\
-PREHOOK: query: DROP TABLE table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table1
-POSTHOOK: Lineage: table1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table1.b SIMPLE []
diff --git a/ql/src/test/results/clientpositive/create_insert_outputformat.q.out b/ql/src/test/results/clientpositive/create_insert_outputformat.q.out
index 49ad36fc0d..1ca228146c 100644
--- a/ql/src/test/results/clientpositive/create_insert_outputformat.q.out
+++ b/ql/src/test/results/clientpositive/create_insert_outputformat.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE table_test_output_format
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table_test_output_format
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE table_test_output_format(key INT, value STRING) STORED AS
   INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
   OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
@@ -31,19 +27,6 @@ POSTHOOK: Lineage: table_test_output_format.key EXPRESSION [(src)src.FieldSchema
 POSTHOOK: Lineage: table_test_output_format.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 key	int	
 value	string	
-PREHOOK: query: DROP TABLE table_test_output_format
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table_test_output_format
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table_test_output_format
-POSTHOOK: Lineage: table_test_output_format.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE table_test_output_format_sequencefile
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table_test_output_format_sequencefile
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: table_test_output_format.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: CREATE TABLE table_test_output_format_sequencefile(key INT, value STRING) STORED AS
   INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
   OUTPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileOutputFormat'
@@ -79,23 +62,6 @@ POSTHOOK: Lineage: table_test_output_format_sequencefile.key EXPRESSION [(src)sr
 POSTHOOK: Lineage: table_test_output_format_sequencefile.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 key	int	
 value	string	
-PREHOOK: query: DROP TABLE table_test_output_format_sequencefile
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table_test_output_format_sequencefile
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table_test_output_format_sequencefile
-POSTHOOK: Lineage: table_test_output_format.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format_sequencefile.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format_sequencefile.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE table_test_output_format_hivesequencefile
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table_test_output_format_hivesequencefile
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: table_test_output_format.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format_sequencefile.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format_sequencefile.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: CREATE TABLE table_test_output_format_hivesequencefile(key INT, value STRING) STORED AS
   INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
   OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
@@ -137,14 +103,3 @@ POSTHOOK: Lineage: table_test_output_format_sequencefile.key EXPRESSION [(src)sr
 POSTHOOK: Lineage: table_test_output_format_sequencefile.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 key	int	
 value	string	
-PREHOOK: query: DROP TABLE table_test_output_format_hivesequencefile
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table_test_output_format_hivesequencefile
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table_test_output_format_hivesequencefile
-POSTHOOK: Lineage: table_test_output_format.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format_hivesequencefile.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format_hivesequencefile.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format_sequencefile.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table_test_output_format_sequencefile.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/create_like.q.out b/ql/src/test/results/clientpositive/create_like.q.out
index 05294d8e31..d45d59d53b 100644
--- a/ql/src/test/results/clientpositive/create_like.q.out
+++ b/ql/src/test/results/clientpositive/create_like.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: DROP TABLE table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE table2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE table3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE table1 (a STRING, b STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE table1 (a STRING, b STRING) STORED AS TEXTFILE
@@ -28,7 +16,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table1, dbName:default, owner:athusoo, createTime:1270516019, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516019}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:table1, dbName:default, owner:jssarma, createTime:1279736324, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736324}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: CREATE TABLE table2 LIKE table1
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE table2 LIKE table1
@@ -47,7 +35,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table2, dbName:default, owner:athusoo, createTime:1270516019, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/table2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1270516019}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:table2, dbName:default, owner:jssarma, createTime:1279736325, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279736325}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: CREATE TABLE IF NOT EXISTS table2 LIKE table1
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE IF NOT EXISTS table2 LIKE table1
@@ -74,7 +62,7 @@ POSTHOOK: type: DESCTABLE
 a	string	
 b	string	
 	 	 
-Detailed Table Information	Table(tableName:table3, dbName:default, owner:athusoo, createTime:1270516019, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/table3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=TRUE,transient_lastDdlTime=1270516019}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:table3, dbName:default, owner:jssarma, createTime:1279736325, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1279736325}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)	
 PREHOOK: query: INSERT OVERWRITE TABLE table1 SELECT key, value FROM src WHERE key = 86
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -100,11 +88,11 @@ POSTHOOK: Lineage: table2.b SIMPLE [(src)src.FieldSchema(name:value, type:string
 PREHOOK: query: SELECT * FROM table1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-07-08_643_4444156953953253763/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-50_868_6881405350249810394/10000
 POSTHOOK: query: SELECT * FROM table1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-07-08_643_4444156953953253763/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-50_868_6881405350249810394/10000
 POSTHOOK: Lineage: table1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: table1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: table2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -113,41 +101,14 @@ POSTHOOK: Lineage: table2.b SIMPLE [(src)src.FieldSchema(name:value, type:string
 PREHOOK: query: SELECT * FROM table2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-07-08_694_5758296445929331715/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-50_925_7813524751422798542/10000
 POSTHOOK: query: SELECT * FROM table2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-07-08_694_5758296445929331715/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-50_925_7813524751422798542/10000
 POSTHOOK: Lineage: table1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: table1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: table2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: table2.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 100	val_100
 100	val_100
-PREHOOK: query: DROP TABLE table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table1
-POSTHOOK: Lineage: table1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: table2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table2.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE table2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table2
-POSTHOOK: Lineage: table1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: table2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table2.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE table3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table3
-POSTHOOK: Lineage: table1.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table1.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: table2.a SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: table2.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/create_nested_type.q.out b/ql/src/test/results/clientpositive/create_nested_type.q.out
index 774b0625d2..982cf7a44b 100644
--- a/ql/src/test/results/clientpositive/create_nested_type.q.out
+++ b/ql/src/test/results/clientpositive/create_nested_type.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE table1 (
        a STRING,
        b ARRAY<STRING>,
@@ -34,7 +30,7 @@ b	array<string>
 c	array<map<string,string>>	
 d	map<string,array<string>>	
 	 	 
-Detailed Table Information	Table(tableName:table1, dbName:default, owner:njain, createTime:1253779851, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:array<string>, comment:null), FieldSchema(name:c, type:array<map<string,string>>, comment:null), FieldSchema(name:d, type:map<string,array<string>>, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
+Detailed Table Information	Table(tableName:table1, dbName:default, owner:jssarma, createTime:1279736331, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:array<string>, comment:null), FieldSchema(name:c, type:array<map<string,string>>, comment:null), FieldSchema(name:d, type:map<string,array<string>>, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736331}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/create_nested_type.txt' OVERWRITE INTO TABLE table1
 PREHOOK: type: LOAD
 POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/create_nested_type.txt' OVERWRITE INTO TABLE table1
@@ -43,17 +39,12 @@ POSTHOOK: Output: default@table1
 PREHOOK: query: SELECT * from table1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1594663066/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-51_565_1719241911025738925/10000
 POSTHOOK: query: SELECT * from table1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1594663066/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-51_565_1719241911025738925/10000
 a0	["b00","b01"]	[{"c001":"C001","c002":"C002"},{"c011":null,"c012":"C012"}]	{"d01":["d011","d012"],"d02":["d021","d022"]}
 a1	["b10"]	[{"c001":"C001","c002":"C002"}]	{"d01":["d011","d012"],"d02":null}
 a2	[]	[{"c001":null,"c002":"C002"},{"c011":"C011","c012":"C012"}]	{"d01":[null,"d012"],"d02":["d021","d022"]}
 a3	null	null	null
-PREHOOK: query: DROP TABLE table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table1
diff --git a/ql/src/test/results/clientpositive/create_struct_table.q.out b/ql/src/test/results/clientpositive/create_struct_table.q.out
index a8d163f60f..ca13edcaf1 100644
--- a/ql/src/test/results/clientpositive/create_struct_table.q.out
+++ b/ql/src/test/results/clientpositive/create_struct_table.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table abc
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table abc
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table abc(strct struct<a:int, b:string, c:string>)
 row format delimited
   fields terminated by '\t'
@@ -23,11 +19,11 @@ POSTHOOK: Output: default@abc
 PREHOOK: query: SELECT strct, strct.a, strct.b FROM abc LIMIT 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@abc
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/611539169/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-52_074_635150256423713835/10000
 POSTHOOK: query: SELECT strct, strct.a, strct.b FROM abc LIMIT 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@abc
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/611539169/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-52_074_635150256423713835/10000
 {"a":238,"b":"val_238","c":null}	238	val_238
 {"a":86,"b":"val_86","c":null}	86	val_86
 {"a":311,"b":"val_311","c":null}	311	val_311
@@ -38,8 +34,3 @@ POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/611539169/1000
 {"a":278,"b":"val_278","c":null}	278	val_278
 {"a":98,"b":"val_98","c":null}	98	val_98
 {"a":484,"b":"val_484","c":null}	484	val_484
-PREHOOK: query: drop table abc
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table abc
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@abc
diff --git a/ql/src/test/results/clientpositive/create_view.q.out b/ql/src/test/results/clientpositive/create_view.q.out
index 836811d85d..5c91befb21 100644
--- a/ql/src/test/results/clientpositive/create_view.q.out
+++ b/ql/src/test/results/clientpositive/create_view.q.out
@@ -74,71 +74,67 @@ PREHOOK: query: DROP TEMPORARY FUNCTION test_explode
 PREHOOK: type: DROPFUNCTION
 POSTHOOK: query: DROP TEMPORARY FUNCTION test_explode
 POSTHOOK: type: DROPFUNCTION
-PREHOOK: query: DROP TABLE table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE table1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: SELECT * FROM src WHERE key=86
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-51_249_8922015804755915765/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-57_921_7591165424312533742/10000
 POSTHOOK: query: SELECT * FROM src WHERE key=86
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-51_249_8922015804755915765/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-18-57_921_7591165424312533742/10000
 86	val_86
 PREHOOK: query: CREATE VIEW view1 AS SELECT value FROM src WHERE key=86
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-55_984_3430099555842364804/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-00_421_2109283399258862165/10000
 POSTHOOK: query: CREATE VIEW view1 AS SELECT value FROM src WHERE key=86
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-55_984_3430099555842364804/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-00_421_2109283399258862165/10000
 POSTHOOK: Output: default@view1
 PREHOOK: query: CREATE VIEW view2 AS SELECT * FROM src
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-56_249_108391416081868894/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-00_454_3605012159234387182/10000
 POSTHOOK: query: CREATE VIEW view2 AS SELECT * FROM src
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-56_249_108391416081868894/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-00_454_3605012159234387182/10000
 POSTHOOK: Output: default@view2
 PREHOOK: query: CREATE VIEW view3(valoo) 
 TBLPROPERTIES ("fear" = "factor")
 AS SELECT upper(value) FROM src WHERE key=86
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-56_437_8091674377507713014/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-00_480_4023689453233021298/10000
 POSTHOOK: query: CREATE VIEW view3(valoo) 
 TBLPROPERTIES ("fear" = "factor")
 AS SELECT upper(value) FROM src WHERE key=86
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-56_437_8091674377507713014/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-00_480_4023689453233021298/10000
 POSTHOOK: Output: default@view3
 PREHOOK: query: SELECT * from view1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-56_507_8760093099562298778/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-00_511_3538329728957553516/10000
 POSTHOOK: query: SELECT * from view1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-07-56_507_8760093099562298778/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-00_511_3538329728957553516/10000
 val_86
 PREHOOK: query: SELECT * from view2 where key=18
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-00_929_2355503113072117248/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-03_015_2577597731172760421/10000
 POSTHOOK: query: SELECT * from view2 where key=18
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-00_929_2355503113072117248/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-03_015_2577597731172760421/10000
 18	val_18
 18	val_18
 PREHOOK: query: SELECT * from view3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-04_967_4103779794841926239/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-05_549_1542870472733242854/10000
 POSTHOOK: query: SELECT * from view3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-04_967_4103779794841926239/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-05_549_1542870472733242854/10000
 VAL_86
 PREHOOK: query: -- test EXPLAIN output for CREATE VIEW
 EXPLAIN
@@ -239,7 +235,7 @@ POSTHOOK: query: DESCRIBE EXTENDED view1
 POSTHOOK: type: DESCTABLE
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:view1, dbName:default, owner:athusoo, createTime:1270516076, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516076}, viewOriginalText:SELECT value FROM src WHERE key=86, viewExpandedText:SELECT `src`.`value` FROM `src` WHERE `src`.`key`=86, tableType:VIRTUAL_VIEW)	
+Detailed Table Information	Table(tableName:view1, dbName:default, owner:jssarma, createTime:1279736340, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736340}, viewOriginalText:SELECT value FROM src WHERE key=86, viewExpandedText:SELECT `src`.`value` FROM `src` WHERE `src`.`key`=86, tableType:VIRTUAL_VIEW)	
 PREHOOK: query: DESCRIBE view2
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: DESCRIBE view2
@@ -253,7 +249,7 @@ POSTHOOK: type: DESCTABLE
 key	string	
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:view2, dbName:default, owner:athusoo, createTime:1270516076, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516076}, viewOriginalText:SELECT * FROM src, viewExpandedText:SELECT `src`.`key`, `src`.`value` FROM `src`, tableType:VIRTUAL_VIEW)	
+Detailed Table Information	Table(tableName:view2, dbName:default, owner:jssarma, createTime:1279736340, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736340}, viewOriginalText:SELECT * FROM src, viewExpandedText:SELECT `src`.`key`, `src`.`value` FROM `src`, tableType:VIRTUAL_VIEW)	
 PREHOOK: query: DESCRIBE view3
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: DESCRIBE view3
@@ -265,7 +261,7 @@ POSTHOOK: query: DESCRIBE EXTENDED view3
 POSTHOOK: type: DESCTABLE
 valoo	string	
 	 	 
-Detailed Table Information	Table(tableName:view3, dbName:default, owner:athusoo, createTime:1270516076, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:valoo, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516076,fear=factor}, viewOriginalText:SELECT upper(value) FROM src WHERE key=86, viewExpandedText:SELECT `_c0` AS `valoo` FROM (SELECT upper(`src`.`value`) FROM `src` WHERE `src`.`key`=86) `view3`, tableType:VIRTUAL_VIEW)	
+Detailed Table Information	Table(tableName:view3, dbName:default, owner:jssarma, createTime:1279736340, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:valoo, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736340, fear=factor}, viewOriginalText:SELECT upper(value) FROM src WHERE key=86, viewExpandedText:SELECT `_c0` AS `valoo` FROM (SELECT upper(`src`.`value`) FROM `src` WHERE `src`.`key`=86) `view3`, tableType:VIRTUAL_VIEW)	
 PREHOOK: query: ALTER VIEW view3 SET TBLPROPERTIES ("biggest" = "loser")
 PREHOOK: type: ALTERVIEW_PROPERTIES
 POSTHOOK: query: ALTER VIEW view3 SET TBLPROPERTIES ("biggest" = "loser")
@@ -278,7 +274,7 @@ POSTHOOK: query: DESCRIBE EXTENDED view3
 POSTHOOK: type: DESCTABLE
 valoo	string	
 	 	 
-Detailed Table Information	Table(tableName:view3, dbName:default, owner:athusoo, createTime:1270516076, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:valoo, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=athusoo,last_modified_time=1270516090,biggest=loser,transient_lastDdlTime=1270516090,fear=factor}, viewOriginalText:SELECT upper(value) FROM src WHERE key=86, viewExpandedText:SELECT `_c0` AS `valoo` FROM (SELECT upper(`src`.`value`) FROM `src` WHERE `src`.`key`=86) `view3`, tableType:VIRTUAL_VIEW)	
+Detailed Table Information	Table(tableName:view3, dbName:default, owner:jssarma, createTime:1279736340, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:valoo, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=jssarma, last_modified_time=1279736348, biggest=loser, transient_lastDdlTime=1279736348, fear=factor}, viewOriginalText:SELECT upper(value) FROM src WHERE key=86, viewExpandedText:SELECT `_c0` AS `valoo` FROM (SELECT upper(`src`.`value`) FROM `src` WHERE `src`.`key`=86) `view3`, tableType:VIRTUAL_VIEW)	
 PREHOOK: query: CREATE TABLE table1 (key int)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE table1 (key int)
@@ -294,7 +290,7 @@ DESCRIBE EXTENDED table1
 POSTHOOK: type: DESCTABLE
 key	int	
 	 	 
-Detailed Table Information	Table(tableName:table1, dbName:default, owner:athusoo, createTime:1270516090, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516090}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:table1, dbName:default, owner:jssarma, createTime:1279736349, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736349}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: DESCRIBE EXTENDED src1
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: DESCRIBE EXTENDED src1
@@ -302,7 +298,7 @@ POSTHOOK: type: DESCTABLE
 key	string	default
 value	string	default
 	 	 
-Detailed Table Information	Table(tableName:src1, dbName:default, owner:null, createTime:1270516069, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/src1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516069}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:src1, dbName:default, owner:null, createTime:1279735684, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279735685}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: -- use DESCRIBE EXTENDED on a base table as a point of comparison for
 -- view descriptions
 DESCRIBE EXTENDED table1
@@ -313,7 +309,7 @@ DESCRIBE EXTENDED table1
 POSTHOOK: type: DESCTABLE
 key	int	
 	 	 
-Detailed Table Information	Table(tableName:table1, dbName:default, owner:athusoo, createTime:1270516090, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516090}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:table1, dbName:default, owner:jssarma, createTime:1279736349, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/table1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736349}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: INSERT OVERWRITE TABLE table1 SELECT key FROM src WHERE key = 86
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -326,29 +322,29 @@ POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:st
 PREHOOK: query: SELECT * FROM table1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-14_929_2065389972532664204/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-12_054_6185622979764502547/10000
 POSTHOOK: query: SELECT * FROM table1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-14_929_2065389972532664204/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-12_054_6185622979764502547/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 86
 PREHOOK: query: CREATE VIEW view4 AS SELECT * FROM table1
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-14_977_3015839202999327586/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-12_107_373418218143741165/10000
 POSTHOOK: query: CREATE VIEW view4 AS SELECT * FROM table1
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-14_977_3015839202999327586/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-12_107_373418218143741165/10000
 POSTHOOK: Output: default@view4
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: SELECT * FROM view4
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-15_021_9184931574613208716/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-12_159_5736721309726450669/10000
 POSTHOOK: query: SELECT * FROM view4
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-15_021_9184931574613208716/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-12_159_5736721309726450669/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 86
 PREHOOK: query: DESCRIBE view4
@@ -367,21 +363,21 @@ POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:st
 PREHOOK: query: SELECT * FROM table1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-19_105_5319518691601572932/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-14_884_6210545047734052322/10000
 POSTHOOK: query: SELECT * FROM table1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-19_105_5319518691601572932/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-14_884_6210545047734052322/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 86	NULL
 PREHOOK: query: SELECT * FROM view4
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-19_169_7551758402210110903/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-14_937_5926817710279534745/10000
 POSTHOOK: query: SELECT * FROM view4
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-19_169_7551758402210110903/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-14_937_5926817710279534745/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 86
 PREHOOK: query: DESCRIBE table1
@@ -400,21 +396,21 @@ key	int
 PREHOOK: query: CREATE VIEW view5 AS SELECT v1.key as key1, v2.key as key2
 FROM view4 v1 join view4 v2
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-23_879_1296898702625082815/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-17_650_1581979006666486814/10000
 POSTHOOK: query: CREATE VIEW view5 AS SELECT v1.key as key1, v2.key as key2
 FROM view4 v1 join view4 v2
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-23_879_1296898702625082815/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-17_650_1581979006666486814/10000
 POSTHOOK: Output: default@view5
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: SELECT * FROM view5
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-24_036_4857028717949657394/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-18_112_5156849292712269194/10000
 POSTHOOK: query: SELECT * FROM view5
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-24_036_4857028717949657394/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-18_112_5156849292712269194/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 86	86
 PREHOOK: query: DESCRIBE view5
@@ -429,13 +425,13 @@ PREHOOK: query: -- verify that column name and comment in DDL portion
 CREATE VIEW view6(valoo COMMENT 'I cannot spell') AS
 SELECT upper(value) as blarg FROM src WHERE key=86
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-29_046_710120165826100548/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-20_848_5361966232039317942/10000
 POSTHOOK: query: -- verify that column name and comment in DDL portion
 -- overrides column alias in SELECT
 CREATE VIEW view6(valoo COMMENT 'I cannot spell') AS
 SELECT upper(value) as blarg FROM src WHERE key=86
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-29_046_710120165826100548/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-20_848_5361966232039317942/10000
 POSTHOOK: Output: default@view6
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE view6
@@ -451,7 +447,7 @@ WHERE key > 80 AND key < 100
 ORDER BY key, value
 LIMIT 10
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-29_265_1329913665876875445/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-21_026_524519195839168130/10000
 POSTHOOK: query: -- verify that ORDER BY and LIMIT are both supported in view def
 CREATE VIEW view7 AS
 SELECT * FROM src
@@ -459,17 +455,17 @@ WHERE key > 80 AND key < 100
 ORDER BY key, value
 LIMIT 10
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-29_265_1329913665876875445/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-21_026_524519195839168130/10000
 POSTHOOK: Output: default@view7
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: SELECT * FROM view7
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-29_421_7101883282785856237/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-21_142_2584542483416509596/10000
 POSTHOOK: query: SELECT * FROM view7
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-29_421_7101883282785856237/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-21_142_2584542483416509596/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 82	val_82
 83	val_83
@@ -487,14 +483,14 @@ PREHOOK: query: -- top-level ORDER BY should override the one inside the view
 SELECT * FROM view7 ORDER BY key DESC, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-34_345_8066508064336105515/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-23_826_7709192776984057918/10000
 POSTHOOK: query: -- top-level ORDER BY should override the one inside the view
 -- (however, the inside ORDER BY should still influence the evaluation
 -- of the limit)
 SELECT * FROM view7 ORDER BY key DESC, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-34_345_8066508064336105515/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-23_826_7709192776984057918/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 90	val_90
 90	val_90
@@ -510,12 +506,12 @@ PREHOOK: query: -- top-level LIMIT should override if lower
 SELECT * FROM view7 LIMIT 5
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-43_427_4325024040225755725/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-29_094_7201546132722765129/10000
 POSTHOOK: query: -- top-level LIMIT should override if lower
 SELECT * FROM view7 LIMIT 5
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-43_427_4325024040225755725/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-29_094_7201546132722765129/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 82	val_82
 83	val_83
@@ -526,12 +522,12 @@ PREHOOK: query: -- but not if higher
 SELECT * FROM view7 LIMIT 20
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-49_324_8898744127113466306/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-31_669_4716567495375910798/10000
 POSTHOOK: query: -- but not if higher
 SELECT * FROM view7 LIMIT 20
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-49_324_8898744127113466306/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-31_669_4716567495375910798/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 82	val_82
 83	val_83
@@ -556,12 +552,12 @@ PREHOOK: query: CREATE VIEW view8(c) AS
 SELECT test_translate('abc', 'a', 'b')
 FROM table1
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-53_840_2964907692712697676/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-34_300_8840699164640193963/10000
 POSTHOOK: query: CREATE VIEW view8(c) AS
 SELECT test_translate('abc', 'a', 'b')
 FROM table1
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-53_840_2964907692712697676/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-34_300_8840699164640193963/10000
 POSTHOOK: Output: default@view8
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE EXTENDED view8
@@ -571,17 +567,17 @@ POSTHOOK: type: DESCTABLE
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 c	string	
 	 	 
-Detailed Table Information	Table(tableName:view8, dbName:default, owner:athusoo, createTime:1270516133, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:c, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516133}, viewOriginalText:SELECT test_translate('abc', 'a', 'b')	 
+Detailed Table Information	Table(tableName:view8, dbName:default, owner:jssarma, createTime:1279736374, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:c, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736374}, viewOriginalText:SELECT test_translate('abc', 'a', 'b')	 
 FROM table1, viewExpandedText:SELECT `_c0` AS `c` FROM (SELECT `test_translate`('abc', 'a', 'b')	 	 
 FROM `table1`) `view8`, tableType:VIRTUAL_VIEW)		 
 PREHOOK: query: SELECT * FROM view8
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-53_924_7826463408574432406/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-34_450_3256858177096010496/10000
 POSTHOOK: query: SELECT * FROM view8
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-53_924_7826463408574432406/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-34_450_3256858177096010496/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 bbc
 PREHOOK: query: -- test usage of a UDAF within a view
@@ -597,12 +593,12 @@ PREHOOK: query: CREATE VIEW view9(m) AS
 SELECT test_max(length(value))
 FROM src
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-58_747_2163708409319152021/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-36_956_3482633729704300290/10000
 POSTHOOK: query: CREATE VIEW view9(m) AS
 SELECT test_max(length(value))
 FROM src
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-58_747_2163708409319152021/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-36_956_3482633729704300290/10000
 POSTHOOK: Output: default@view9
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE EXTENDED view9
@@ -612,29 +608,29 @@ POSTHOOK: type: DESCTABLE
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 m	int	
 	 	 
-Detailed Table Information	Table(tableName:view9, dbName:default, owner:athusoo, createTime:1270516138, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:m, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516138}, viewOriginalText:SELECT test_max(length(value))	 
+Detailed Table Information	Table(tableName:view9, dbName:default, owner:jssarma, createTime:1279736376, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:m, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736376}, viewOriginalText:SELECT test_max(length(value))	 
 FROM src, viewExpandedText:SELECT `_c0` AS `m` FROM (SELECT `test_max`(length(`src`.`value`))	 	 
 FROM `src`) `view9`, tableType:VIRTUAL_VIEW)		 
 PREHOOK: query: SELECT * FROM view9
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-59_002_4960552290114258251/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-37_098_8931758755865355991/10000
 POSTHOOK: query: SELECT * FROM view9
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-08-59_002_4960552290114258251/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-37_098_8931758755865355991/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 7
 PREHOOK: query: -- test usage of a subselect within a view
 CREATE VIEW view10 AS
 SELECT slurp.* FROM (SELECT * FROM src WHERE key=86) slurp
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-03_445_2038640404964168331/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-39_728_8538567105732339323/10000
 POSTHOOK: query: -- test usage of a subselect within a view
 CREATE VIEW view10 AS
 SELECT slurp.* FROM (SELECT * FROM src WHERE key=86) slurp
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-03_445_2038640404964168331/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-39_728_8538567105732339323/10000
 POSTHOOK: Output: default@view10
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE EXTENDED view10
@@ -645,15 +641,15 @@ POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:st
 key	string	
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:view10, dbName:default, owner:athusoo, createTime:1270516143, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516143}, viewOriginalText:SELECT slurp.* FROM (SELECT * FROM src WHERE key=86) slurp, viewExpandedText:SELECT `slurp`.`key`, `slurp`.`value` FROM (SELECT `src`.`key`, `src`.`value` FROM `src` WHERE `src`.`key`=86) `slurp`, tableType:VIRTUAL_VIEW)	
+Detailed Table Information	Table(tableName:view10, dbName:default, owner:jssarma, createTime:1279736379, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736379}, viewOriginalText:SELECT slurp.* FROM (SELECT * FROM src WHERE key=86) slurp, viewExpandedText:SELECT `slurp`.`key`, `slurp`.`value` FROM (SELECT `src`.`key`, `src`.`value` FROM `src` WHERE `src`.`key`=86) `slurp`, tableType:VIRTUAL_VIEW)	
 PREHOOK: query: SELECT * FROM view10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-03_534_4513987569168219590/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-39_954_121140881764531324/10000
 POSTHOOK: query: SELECT * FROM view10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-03_534_4513987569168219590/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-39_954_121140881764531324/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 86	val_86
 PREHOOK: query: -- test usage of a UDTF within a view
@@ -669,12 +665,12 @@ PREHOOK: query: CREATE VIEW view11 AS
 SELECT test_explode(array(1,2,3)) AS (boom)
 FROM table1
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-08_172_6937271646873245957/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-42_507_941005864769279541/10000
 POSTHOOK: query: CREATE VIEW view11 AS
 SELECT test_explode(array(1,2,3)) AS (boom)
 FROM table1
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-08_172_6937271646873245957/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-42_507_941005864769279541/10000
 POSTHOOK: Output: default@view11
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE EXTENDED view11
@@ -684,17 +680,17 @@ POSTHOOK: type: DESCTABLE
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 boom	int	
 	 	 
-Detailed Table Information	Table(tableName:view11, dbName:default, owner:athusoo, createTime:1270516148, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:boom, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516148}, viewOriginalText:SELECT test_explode(array(1,2,3)) AS (boom)	 
+Detailed Table Information	Table(tableName:view11, dbName:default, owner:jssarma, createTime:1279736382, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:boom, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736382}, viewOriginalText:SELECT test_explode(array(1,2,3)) AS (boom)	 
 FROM table1, viewExpandedText:SELECT `test_explode`(array(1,2,3)) AS (`boom`)	 	 
 FROM `table1`, tableType:VIRTUAL_VIEW)		 
 PREHOOK: query: SELECT * FROM view11
 PREHOOK: type: QUERY
 PREHOOK: Input: default@table1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-08_313_4664224396935808807/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-42_621_147481859612435905/10000
 POSTHOOK: query: SELECT * FROM view11
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@table1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-08_313_4664224396935808807/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-42_621_147481859612435905/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 1
 2
@@ -703,12 +699,12 @@ PREHOOK: query: -- test usage of LATERAL within a view
 CREATE VIEW view12 AS
 SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-12_286_8427548702068975898/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-45_155_5582073945596947542/10000
 POSTHOOK: query: -- test usage of LATERAL within a view
 CREATE VIEW view12 AS
 SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-12_286_8427548702068975898/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-45_155_5582073945596947542/10000
 POSTHOOK: Output: default@view12
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE EXTENDED view12
@@ -720,17 +716,17 @@ key	string
 value	string	
 mycol	int	
 	 	 
-Detailed Table Information	Table(tableName:view12, dbName:default, owner:athusoo, createTime:1270516152, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:mycol, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516152}, viewOriginalText:SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol, viewExpandedText:SELECT `src`.`key`, `src`.`value`, `mytable`.`mycol` FROM `src` LATERAL VIEW explode(array(1,2,3)) `myTable` AS `myCol`, tableType:VIRTUAL_VIEW)	
+Detailed Table Information	Table(tableName:view12, dbName:default, owner:jssarma, createTime:1279736385, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:mycol, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736385}, viewOriginalText:SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol, viewExpandedText:SELECT `src`.`key`, `src`.`value`, `mytable`.`mycol` FROM `src` LATERAL VIEW explode(array(1,2,3)) `myTable` AS `myCol`, tableType:VIRTUAL_VIEW)	
 PREHOOK: query: SELECT * FROM view12
 ORDER BY key ASC, myCol ASC LIMIT 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-12_406_2588118547562629364/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-45_288_6471519897685463611/10000
 POSTHOOK: query: SELECT * FROM view12
 ORDER BY key ASC, myCol ASC LIMIT 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-12_406_2588118547562629364/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-45_288_6471519897685463611/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 0	val_0	1
 PREHOOK: query: -- test usage of LATERAL with a view as the LHS
@@ -738,13 +734,13 @@ SELECT * FROM view2 LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
 ORDER BY key ASC, myCol ASC LIMIT 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-17_007_2569567271321396526/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-47_967_4711091040062160152/10000
 POSTHOOK: query: -- test usage of LATERAL with a view as the LHS
 SELECT * FROM view2 LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
 ORDER BY key ASC, myCol ASC LIMIT 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-17_007_2569567271321396526/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-47_967_4711091040062160152/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 0	val_0	1
 PREHOOK: query: -- test usage of TABLESAMPLE within a view
@@ -752,13 +748,13 @@ CREATE VIEW view13 AS
 SELECT s.key
 FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 ON key) s
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-21_328_3318797810129325812/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-50_665_3879547564260377848/10000
 POSTHOOK: query: -- test usage of TABLESAMPLE within a view
 CREATE VIEW view13 AS
 SELECT s.key
 FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 ON key) s
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-21_328_3318797810129325812/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-50_665_3879547564260377848/10000
 POSTHOOK: Output: default@view13
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE EXTENDED view13
@@ -768,19 +764,19 @@ POSTHOOK: type: DESCTABLE
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 key	int	
 	 	 
-Detailed Table Information	Table(tableName:view13, dbName:default, owner:athusoo, createTime:1270516161, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516161}, viewOriginalText:SELECT s.key	 
+Detailed Table Information	Table(tableName:view13, dbName:default, owner:jssarma, createTime:1279736390, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736390}, viewOriginalText:SELECT s.key	 
 FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 ON key) s, viewExpandedText:SELECT `s`.`key`	 	 
 FROM `srcbucket` TABLESAMPLE (BUCKET 1 OUT OF 5 ON `key`) `s`, tableType:VIRTUAL_VIEW)		 
 PREHOOK: query: SELECT * FROM view13
 ORDER BY key LIMIT 12
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcbucket
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-21_411_3195453265987665126/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-50_808_1357112183697677370/10000
 POSTHOOK: query: SELECT * FROM view13
 ORDER BY key LIMIT 12
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcbucket
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-21_411_3195453265987665126/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-50_808_1357112183697677370/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 0
 0
@@ -807,7 +803,7 @@ JOIN
       select s4.key as key, s4.value as value from src s4 where s4.key < 10) unionsrc2
 ON (unionsrc1.key = unionsrc2.key)
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-25_913_4823304300133994589/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-53_382_1837567325290565612/10000
 POSTHOOK: query: -- test usage of JOIN+UNION+AGG all within same view
 CREATE VIEW view14 AS
 SELECT unionsrc1.key as k1, unionsrc1.value as v1,
@@ -821,7 +817,7 @@ JOIN
       select s4.key as key, s4.value as value from src s4 where s4.key < 10) unionsrc2
 ON (unionsrc1.key = unionsrc2.key)
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-25_913_4823304300133994589/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-53_382_1837567325290565612/10000
 POSTHOOK: Output: default@view14
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE EXTENDED view14
@@ -834,7 +830,7 @@ v1	string
 k2	string	
 v2	string	
 	 	 
-Detailed Table Information	Table(tableName:view14, dbName:default, owner:athusoo, createTime:1270516166, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:k1, type:string, comment:null), FieldSchema(name:v1, type:string, comment:null), FieldSchema(name:k2, type:string, comment:null), FieldSchema(name:v2, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516166}, viewOriginalText:SELECT unionsrc1.key as k1, unionsrc1.value as v1,	 
+Detailed Table Information	Table(tableName:view14, dbName:default, owner:jssarma, createTime:1279736393, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:k1, type:string, comment:null), FieldSchema(name:v1, type:string, comment:null), FieldSchema(name:k2, type:string, comment:null), FieldSchema(name:v2, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736393}, viewOriginalText:SELECT unionsrc1.key as k1, unionsrc1.value as v1,	 
        unionsrc2.key as k2, unionsrc2.value as v2	 	 
 FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1	 	 
                          UNION  ALL	 	 
@@ -857,12 +853,12 @@ PREHOOK: query: SELECT * FROM view14
 ORDER BY k1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-26_119_2761258753248431304/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-53_542_3266426952139641562/10000
 POSTHOOK: query: SELECT * FROM view14
 ORDER BY k1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-26_119_2761258753248431304/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-19-53_542_3266426952139641562/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 0	val_0	0	val_0
 0	val_0	0	val_0
@@ -893,14 +889,14 @@ SELECT key,COUNT(value) AS value_count
 FROM src
 GROUP BY key
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-58_798_7555941970013621980/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-12_519_296683891105463355/10000
 POSTHOOK: query: -- test usage of GROUP BY within view
 CREATE VIEW view15 AS
 SELECT key,COUNT(value) AS value_count
 FROM src
 GROUP BY key
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-58_798_7555941970013621980/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-12_519_296683891105463355/10000
 POSTHOOK: Output: default@view15
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE EXTENDED view15
@@ -911,7 +907,7 @@ POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:st
 key	string	
 value_count	bigint	
 	 	 
-Detailed Table Information	Table(tableName:view15, dbName:default, owner:athusoo, createTime:1270516198, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value_count, type:bigint, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516198}, viewOriginalText:SELECT key,COUNT(value) AS value_count	 
+Detailed Table Information	Table(tableName:view15, dbName:default, owner:jssarma, createTime:1279736412, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value_count, type:bigint, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736412}, viewOriginalText:SELECT key,COUNT(value) AS value_count	 
 FROM src	 	 
 GROUP BY key, viewExpandedText:SELECT `src`.`key`,COUNT(`src`.`value`) AS `value_count`	 	 
 FROM `src`	 	 
@@ -921,13 +917,13 @@ ORDER BY value_count DESC, key
 LIMIT 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-58_914_3308702425437265515/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-12_632_2632827347997497908/10000
 POSTHOOK: query: SELECT * FROM view15
 ORDER BY value_count DESC, key
 LIMIT 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-09-58_914_3308702425437265515/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-12_632_2632827347997497908/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 230	5
 348	5
@@ -944,13 +940,13 @@ CREATE VIEW view16 AS
 SELECT DISTINCT value
 FROM src
 PREHOOK: type: CREATEVIEW
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-07_365_8609706981588395246/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-17_744_4594166188705296834/10000
 POSTHOOK: query: -- test usage of DISTINCT within view
 CREATE VIEW view16 AS
 SELECT DISTINCT value
 FROM src
 POSTHOOK: type: CREATEVIEW
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-07_365_8609706981588395246/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-17_744_4594166188705296834/10000
 POSTHOOK: Output: default@view16
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 PREHOOK: query: DESCRIBE EXTENDED view16
@@ -960,7 +956,7 @@ POSTHOOK: type: DESCTABLE
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:view16, dbName:default, owner:athusoo, createTime:1270516207, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516207}, viewOriginalText:SELECT DISTINCT value	 
+Detailed Table Information	Table(tableName:view16, dbName:default, owner:jssarma, createTime:1279736417, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:null, parameters:{}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736417}, viewOriginalText:SELECT DISTINCT value	 
 FROM src, viewExpandedText:SELECT DISTINCT `src`.`value`	 	 
 FROM `src`, tableType:VIRTUAL_VIEW)		 
 PREHOOK: query: SELECT * FROM view16
@@ -968,13 +964,13 @@ ORDER BY value
 LIMIT 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-07_470_4237294142974167361/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-17_856_3789669241902298524/10000
 POSTHOOK: query: SELECT * FROM view16
 ORDER BY value
 LIMIT 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-07_470_4237294142974167361/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-17_856_3789669241902298524/10000
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 val_0
 val_10
@@ -988,17 +984,15 @@ val_113
 val_114
 PREHOOK: query: -- this should work since currently we don't track view->table
 -- dependencies for implementing RESTRICT
-DROP TABLE table1
-PREHOOK: type: DROPTABLE
+
+
+DROP VIEW view1
+PREHOOK: type: DROPVIEW
 POSTHOOK: query: -- this should work since currently we don't track view->table
 -- dependencies for implementing RESTRICT
-DROP TABLE table1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@table1
-POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP VIEW view1
-PREHOOK: type: DROPVIEW
-POSTHOOK: query: DROP VIEW view1
+
+
+DROP VIEW view1
 POSTHOOK: type: DROPVIEW
 POSTHOOK: Output: default@view1
 POSTHOOK: Lineage: table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/ct_case_insensitive.q.out b/ql/src/test/results/clientpositive/ct_case_insensitive.q.out
index 91df2556f2..00cf27ea5e 100644
--- a/ql/src/test/results/clientpositive/ct_case_insensitive.q.out
+++ b/ql/src/test/results/clientpositive/ct_case_insensitive.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE tmp_pyang_bucket3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tmp_pyang_bucket3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS
@@ -17,8 +13,3 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) SORTED BY (USERID) INTO 32 BUCKETS
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tmp_pyang_bucket3
-PREHOOK: query: DROP TABLE tmp_pyang_bucket3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tmp_pyang_bucket3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp_pyang_bucket3
diff --git a/ql/src/test/results/clientpositive/ctas.q.out b/ql/src/test/results/clientpositive/ctas.q.out
index cbd9ad7e8d..deb841146c 100644
--- a/ql/src/test/results/clientpositive/ctas.q.out
+++ b/ql/src/test/results/clientpositive/ctas.q.out
@@ -1,31 +1,3 @@
-PREHOOK: query: drop table nzhang_ctas1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table nzhang_ctas2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table nzhang_ctas3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table nzhang_ctas4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas4
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table nzhang_ctas5
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas5
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table nzhang_ctas6
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas6
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table nzhang_ctas7
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas7
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table nzhang_Tmp(a int, b string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table nzhang_Tmp(a int, b string)
@@ -34,11 +6,11 @@ POSTHOOK: Output: default@nzhang_Tmp
 PREHOOK: query: select * from nzhang_Tmp
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_tmp
-PREHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-08-43_508_5766942786569220462/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-24_032_2060756190371221889/10000
 POSTHOOK: query: select * from nzhang_Tmp
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_tmp
-POSTHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-08-43_508_5766942786569220462/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-24_032_2060756190371221889/10000
 PREHOOK: query: explain create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: explain create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10
@@ -92,7 +64,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/tmp/nzhang/hive_2010-07-16_18-08-43_756_2205398290785640792/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-20-24_081_2842030423917771818/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -120,7 +92,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:///data/users/nzhang/work/900/apache-hive/build/ql/test/data/warehouse/nzhang_ctas1
+          destination: file:///mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_ctas1
 
   Stage: Stage-3
       Create Table Operator:
@@ -144,11 +116,11 @@ POSTHOOK: Output: default@nzhang_CTAS1
 PREHOOK: query: select * from nzhang_CTAS1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_ctas1
-PREHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-08-50_541_5612971983311357030/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-29_265_2242843702948387375/10000
 POSTHOOK: query: select * from nzhang_CTAS1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_ctas1
-POSTHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-08-50_541_5612971983311357030/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-29_265_2242843702948387375/10000
 0	val_0
 0	val_0
 0	val_0
@@ -212,7 +184,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/tmp/nzhang/hive_2010-07-16_18-08-50_701_829270117025762937/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-20-29_318_3494652198341465493/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -240,7 +212,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:///data/users/nzhang/work/900/apache-hive/build/ql/test/data/warehouse/nzhang_ctas2
+          destination: file:///mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_ctas2
 
   Stage: Stage-3
       Create Table Operator:
@@ -264,11 +236,11 @@ POSTHOOK: Output: default@nzhang_ctas2
 PREHOOK: query: select * from nzhang_ctas2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_ctas2
-PREHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-08-57_363_8244890779707184718/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-34_445_4830318461592445646/10000
 POSTHOOK: query: select * from nzhang_ctas2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_ctas2
-POSTHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-08-57_363_8244890779707184718/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-34_445_4830318461592445646/10000
 0	val_0
 0	val_0
 0	val_0
@@ -332,7 +304,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/tmp/nzhang/hive_2010-07-16_18-08-57_497_4624357276892180919/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-20-34_503_7616200575358243425/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -360,7 +332,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:///data/users/nzhang/work/900/apache-hive/build/ql/test/data/warehouse/nzhang_ctas3
+          destination: file:///mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_ctas3
 
   Stage: Stage-3
       Create Table Operator:
@@ -385,11 +357,11 @@ POSTHOOK: Output: default@nzhang_ctas3
 PREHOOK: query: select * from nzhang_ctas3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_ctas3
-PREHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-09-04_109_6050862695060860601/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-39_614_692276775971124139/10000
 POSTHOOK: query: select * from nzhang_ctas3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_ctas3
-POSTHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-09-04_109_6050862695060860601/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-39_614_692276775971124139/10000
 0.0	val_0_con
 0.0	val_0_con
 0.0	val_0_con
@@ -418,11 +390,11 @@ POSTHOOK: type: CREATETABLE
 PREHOOK: query: select * from nzhang_ctas3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_ctas3
-PREHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-09-04_321_2229875924776565788/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-39_719_3985398018110959794/10000
 POSTHOOK: query: select * from nzhang_ctas3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_ctas3
-POSTHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-09-04_321_2229875924776565788/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-39_719_3985398018110959794/10000
 0.0	val_0_con
 0.0	val_0_con
 0.0	val_0_con
@@ -486,7 +458,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/tmp/nzhang/hive_2010-07-16_18-09-04_407_2759642794438562703/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-20-39_777_7063025954650345962/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -514,7 +486,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:///data/users/nzhang/work/900/apache-hive/build/ql/test/data/warehouse/nzhang_ctas4
+          destination: file:///mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_ctas4
 
   Stage: Stage-3
       Create Table Operator:
@@ -539,11 +511,11 @@ POSTHOOK: Output: default@nzhang_ctas4
 PREHOOK: query: select * from nzhang_ctas4
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_ctas4
-PREHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-09-10_791_2627346368025263857/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-44_738_680021174157430213/10000
 POSTHOOK: query: select * from nzhang_ctas4
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_ctas4
-POSTHOOK: Output: file:/tmp/nzhang/hive_2010-07-16_18-09-10_791_2627346368025263857/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-20-44_738_680021174157430213/10000
 0	val_0
 0	val_0
 0	val_0
@@ -596,9 +568,9 @@ STAGE PLANS:
                       type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/nzhang/work/900/apache-hive/build/ql/test/data/warehouse/src [src]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [src]
       Path -> Partition:
-        file:/data/users/nzhang/work/900/apache-hive/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -609,12 +581,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/900/apache-hive/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1279328922
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -625,12 +597,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/900/apache-hive/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1279328922
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -640,7 +612,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-              directory: file:/tmp/nzhang/hive_2010-07-16_18-09-10_882_1309336938603159683/10002
+              directory: file:/tmp/jssarma/hive_2010-07-21_11-20-44_839_5430947126614955400/10002
               NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -655,7 +627,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/tmp/nzhang/hive_2010-07-16_18-09-10_882_1309336938603159683/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-20-44_839_5430947126614955400/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -671,9 +643,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/tmp/nzhang/hive_2010-07-16_18-09-10_882_1309336938603159683/10002 [file:/tmp/nzhang/hive_2010-07-16_18-09-10_882_1309336938603159683/10002]
+        file:/tmp/jssarma/hive_2010-07-21_11-20-44_839_5430947126614955400/10002 [file:/tmp/jssarma/hive_2010-07-21_11-20-44_839_5430947126614955400/10002]
       Path -> Partition:
-        file:/tmp/nzhang/hive_2010-07-16_18-09-10_882_1309336938603159683/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-20-44_839_5430947126614955400/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -695,7 +667,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-              directory: file:/data/users/nzhang/work/900/apache-hive/build/ql/scratchdir/hive_2010-07-16_18-09-10_882_1309336938603159683/10001
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-20-44_839_5430947126614955400/10001
               NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -714,8 +686,8 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/nzhang/work/900/apache-hive/build/ql/scratchdir/hive_2010-07-16_18-09-10_882_1309336938603159683/10001
-          destination: file:///data/users/nzhang/work/900/apache-hive/build/ql/test/data/warehouse/nzhang_ctas5
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-20-44_839_5430947126614955400/10001
+          destination: file:///mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_ctas5
 
   Stage: Stage-3
       Create Table Operator:
@@ -763,59 +735,3 @@ POSTHOOK: Input: default@nzhang_ctas6
 POSTHOOK: Output: default@nzhang_ctas7
 POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table nzhang_ctas1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_ctas1
-POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table nzhang_ctas2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_ctas2
-POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table nzhang_ctas3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_ctas3
-POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table nzhang_ctas4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_ctas4
-POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table nzhang_ctas5
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas5
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_ctas5
-POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table nzhang_ctas6
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas6
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_ctas6
-POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table nzhang_ctas7
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_ctas7
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_ctas7
-POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table nzhang_Tmp
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_Tmp
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_tmp
-POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/ddltime.q.out b/ql/src/test/results/clientpositive/ddltime.q.out
index d0e23ce7c6..fbfe77768c 100644
--- a/ql/src/test/results/clientpositive/ddltime.q.out
+++ b/ql/src/test/results/clientpositive/ddltime.q.out
@@ -10,7 +10,7 @@ POSTHOOK: type: DESCTABLE
 key	string	default
 value	string	default
 	 	 
-Detailed Table Information	Table(tableName:t1, dbName:default, owner:null, createTime:1277236615, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1277236615}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:t1, dbName:default, owner:null, createTime:1279736455, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279736455}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table T1 select * from src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -30,7 +30,7 @@ POSTHOOK: Lineage: t1.value SIMPLE [(src)src.FieldSchema(name:value, type:string
 key	string	default
 value	string	default
 	 	 
-Detailed Table Information	Table(tableName:t1, dbName:default, owner:null, createTime:1277236615, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1277236620}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:t1, dbName:default, owner:null, createTime:1279736455, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279736459}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table T1 select /*+ HOLD_DDLTIME*/ * from src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -54,7 +54,7 @@ POSTHOOK: Lineage: t1.value SIMPLE [(src)src.FieldSchema(name:value, type:string
 key	string	default
 value	string	default
 	 	 
-Detailed Table Information	Table(tableName:t1, dbName:default, owner:null, createTime:1277236615, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1277236620}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:t1, dbName:default, owner:null, createTime:1279736455, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279736459}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table T1 select * from src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -82,18 +82,7 @@ POSTHOOK: Lineage: t1.value SIMPLE [(src)src.FieldSchema(name:value, type:string
 key	string	default
 value	string	default
 	 	 
-Detailed Table Information	Table(tableName:t1, dbName:default, owner:null, createTime:1277236615, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1277236629}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
-PREHOOK: query: drop table T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-POSTHOOK: Lineage: t1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+Detailed Table Information	Table(tableName:t1, dbName:default, owner:null, createTime:1279736455, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279736467}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: create table if not exists T2 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists T2 like srcpart
@@ -120,7 +109,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:t2, dbName:default, owner:null, createTime:1277236629, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/t2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1277236629}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:t2, dbName:default, owner:null, createTime:1279736467, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279736467}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table T2 partition (ds = '2010-06-21', hr = '1') select key, value from src where key > 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -154,7 +143,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Partition Information	Partition(values:[2010-06-21, 1], dbName:default, tableName:t2, createTime:1277236634, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/t2/ds=2010-06-21/hr=1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1277236634})	
+Detailed Partition Information	Partition(values:[2010-06-21, 1], dbName:default, tableName:t2, createTime:1279736471, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t2/ds=2010-06-21/hr=1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1279736471})	
 PREHOOK: query: insert overwrite table T2 partition (ds = '2010-06-21', hr='1') select /*+ HOLD_DDLTIME */ key, value from src where key > 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -192,7 +181,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Partition Information	Partition(values:[2010-06-21, 1], dbName:default, tableName:t2, createTime:1277236634, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/t2/ds=2010-06-21/hr=1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1277236634})	
+Detailed Partition Information	Partition(values:[2010-06-21, 1], dbName:default, tableName:t2, createTime:1279736471, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t2/ds=2010-06-21/hr=1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1279736471})	
 PREHOOK: query: insert overwrite table T2 partition (ds='2010-06-01', hr='1') select key, value from src where key > 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -234,21 +223,4 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Partition Information	Partition(values:[2010-06-01, 1], dbName:default, tableName:t2, createTime:1277236643, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/t2/ds=2010-06-01/hr=1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1277236643})	
-PREHOOK: query: drop table T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-POSTHOOK: Lineage: t1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-06-01,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-06-01,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-06-21,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-06-21,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-06-21,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-06-21,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+Detailed Partition Information	Partition(values:[2010-06-01, 1], dbName:default, tableName:t2, createTime:1279736478, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t2/ds=2010-06-01/hr=1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1279736478})	
diff --git a/ql/src/test/results/clientpositive/diff_part_input_formats.q.out b/ql/src/test/results/clientpositive/diff_part_input_formats.q.out
index a99b7d70f7..4d7a4abe43 100644
--- a/ql/src/test/results/clientpositive/diff_part_input_formats.q.out
+++ b/ql/src/test/results/clientpositive/diff_part_input_formats.q.out
@@ -27,13 +27,8 @@ POSTHOOK: type: ALTERTABLE_ADDPARTS
 POSTHOOK: Output: default@part_test@ds=2
 PREHOOK: query: SELECT count(1) FROM part_test WHERE ds='3'
 PREHOOK: type: QUERY
-PREHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-15_10-14-51_112_6812982446306518733/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-20_502_7998672376160685808/10000
 POSTHOOK: query: SELECT count(1) FROM part_test WHERE ds='3'
 POSTHOOK: type: QUERY
-POSTHOOK: Output: file:/data/users/pyang/task/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-06-15_10-14-51_112_6812982446306518733/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-20_502_7998672376160685808/10000
 0
-PREHOOK: query: DROP TABLE part_test
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE part_test
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@part_test
diff --git a/ql/src/test/results/clientpositive/disable_file_format_check.q.out b/ql/src/test/results/clientpositive/disable_file_format_check.q.out
index ef50c2392e..f198cdfe99 100644
--- a/ql/src/test/results/clientpositive/disable_file_format_check.q.out
+++ b/ql/src/test/results/clientpositive/disable_file_format_check.q.out
@@ -18,13 +18,3 @@ PREHOOK: type: LOAD
 POSTHOOK: query: load data local inpath '../data/files/kv1.txt' overwrite into table kv_fileformat_check_seq
 POSTHOOK: type: LOAD
 POSTHOOK: Output: default@kv_fileformat_check_seq
-PREHOOK: query: drop table kv_fileformat_check_seq
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table kv_fileformat_check_seq
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@kv_fileformat_check_seq
-PREHOOK: query: drop table kv_fileformat_check_txt
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table kv_fileformat_check_txt
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@kv_fileformat_check_txt
diff --git a/ql/src/test/results/clientpositive/disable_merge_for_bucketing.q.out b/ql/src/test/results/clientpositive/disable_merge_for_bucketing.q.out
index 1d694c890b..f22a5c314f 100644
--- a/ql/src/test/results/clientpositive/disable_merge_for_bucketing.q.out
+++ b/ql/src/test/results/clientpositive/disable_merge_for_bucketing.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table bucket2_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket2_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE bucket2_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE bucket2_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS
@@ -49,9 +45,9 @@ STAGE PLANS:
                       type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src [src]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [src]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -62,12 +58,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516041
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -78,12 +74,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516041
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -99,7 +95,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-              directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-07-21_813_2816895778195265258/10000
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-21-24_032_5058530528484682613/10000
               NumFilesPerFileSink: 2
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -111,12 +107,12 @@ STAGE PLANS:
                     columns.types int:string
                     file.inputformat org.apache.hadoop.mapred.TextInputFormat
                     file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/bucket2_1
+                    location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket2_1
                     name bucket2_1
                     serialization.ddl struct bucket2_1 { i32 key, string value}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    transient_lastDdlTime 1270516041
+                    transient_lastDdlTime 1279736484
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: bucket2_1
               TotalFiles: 2
@@ -126,7 +122,7 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-07-21_813_2816895778195265258/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-21-24_032_5058530528484682613/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -137,15 +133,15 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/bucket2_1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket2_1
                 name bucket2_1
                 serialization.ddl struct bucket2_1 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516041
+                transient_lastDdlTime 1279736484
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucket2_1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-07-21_813_2816895778195265258/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-21-24_032_5058530528484682613/10001
 
 
 PREHOOK: query: insert overwrite table bucket2_1
@@ -225,11 +221,11 @@ STAGE PLANS:
 PREHOOK: query: select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucket2_1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-07-28_295_4764288900823189670/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-26_856_8066476091371346194/10000
 POSTHOOK: query: select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucket2_1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-07-28_295_4764288900823189670/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-26_856_8066476091371346194/10000
 POSTHOOK: Lineage: bucket2_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: bucket2_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	val_0
@@ -479,10 +475,3 @@ POSTHOOK: Lineage: bucket2_1.value SIMPLE [(src)src.FieldSchema(name:value, type
 498	val_498
 498	val_498
 498	val_498
-PREHOOK: query: drop table bucket2_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket2_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucket2_1
-POSTHOOK: Lineage: bucket2_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket2_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/drop_multi_partitions.q.out b/ql/src/test/results/clientpositive/drop_multi_partitions.q.out
index ca7381e96a..054f9d4d2d 100644
--- a/ql/src/test/results/clientpositive/drop_multi_partitions.q.out
+++ b/ql/src/test/results/clientpositive/drop_multi_partitions.q.out
@@ -53,8 +53,3 @@ PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: show partitions mp
 POSTHOOK: type: SHOWPARTITIONS
 b=2/c=2
-PREHOOK: query: drop table mp
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table mp
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@mp
diff --git a/ql/src/test/results/clientpositive/fileformat_mix.q.out b/ql/src/test/results/clientpositive/fileformat_mix.q.out
index 9fe7d894a0..39e2cbdc0e 100644
--- a/ql/src/test/results/clientpositive/fileformat_mix.q.out
+++ b/ql/src/test/results/clientpositive/fileformat_mix.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table fileformat_mix_test
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table fileformat_mix_test
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table fileformat_mix_test (src int, value string) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table fileformat_mix_test (src int, value string) partitioned by (ds string)
@@ -44,12 +40,12 @@ PREHOOK: query: select count(1) from fileformat_mix_test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@fileformat_mix_test@ds=1
 PREHOOK: Input: default@fileformat_mix_test@ds=2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-20_21-21-12_115_4377005987442515283/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-45_393_3654336984212621494/10000
 POSTHOOK: query: select count(1) from fileformat_mix_test
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@fileformat_mix_test@ds=1
 POSTHOOK: Input: default@fileformat_mix_test@ds=2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-20_21-21-12_115_4377005987442515283/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-45_393_3654336984212621494/10000
 POSTHOOK: Lineage: fileformat_mix_test PARTITION(ds=1).src EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: fileformat_mix_test PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 500
@@ -57,12 +53,12 @@ PREHOOK: query: select src from fileformat_mix_test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@fileformat_mix_test@ds=1
 PREHOOK: Input: default@fileformat_mix_test@ds=2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-20_21-21-17_269_3478340175683868925/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-48_001_5704820361095123784/10000
 POSTHOOK: query: select src from fileformat_mix_test
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@fileformat_mix_test@ds=1
 POSTHOOK: Input: default@fileformat_mix_test@ds=2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-20_21-21-17_269_3478340175683868925/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-48_001_5704820361095123784/10000
 POSTHOOK: Lineage: fileformat_mix_test PARTITION(ds=1).src EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: fileformat_mix_test PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 238
@@ -565,10 +561,3 @@ POSTHOOK: Lineage: fileformat_mix_test PARTITION(ds=1).value SIMPLE [(src)src.Fi
 400
 200
 97
-PREHOOK: query: drop table fileformat_mix_test
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table fileformat_mix_test
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@fileformat_mix_test
-POSTHOOK: Lineage: fileformat_mix_test PARTITION(ds=1).src EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fileformat_mix_test PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/fileformat_sequencefile.q.out b/ql/src/test/results/clientpositive/fileformat_sequencefile.q.out
index 185ca3d540..146410eb88 100644
--- a/ql/src/test/results/clientpositive/fileformat_sequencefile.q.out
+++ b/ql/src/test/results/clientpositive/fileformat_sequencefile.q.out
@@ -43,7 +43,7 @@ POSTHOOK: type: DESCTABLE
 key	int	
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:dest1, dbName:default, owner:athusoo, createTime:1270516226, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516226}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:dest1, dbName:default, owner:jssarma, createTime:1279736511, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736511}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: FROM src
 INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 10
 PREHOOK: type: QUERY
@@ -59,11 +59,11 @@ POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:str
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-30_861_6151697846173493077/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-53_917_3907368248535068834/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-30_861_6151697846173493077/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-53_917_3907368248535068834/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	val_0
@@ -76,10 +76,3 @@ POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:str
 2	val_2
 5	val_5
 9	val_9
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/fileformat_text.q.out b/ql/src/test/results/clientpositive/fileformat_text.q.out
index 38bcde6754..23f93d8e81 100644
--- a/ql/src/test/results/clientpositive/fileformat_text.q.out
+++ b/ql/src/test/results/clientpositive/fileformat_text.q.out
@@ -43,7 +43,7 @@ POSTHOOK: type: DESCTABLE
 key	int	
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:dest1, dbName:default, owner:athusoo, createTime:1270516127, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/dest1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516127}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:dest1, dbName:default, owner:jssarma, createTime:1279736514, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279736514}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: FROM src
 INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 10
 PREHOOK: type: QUERY
@@ -59,11 +59,11 @@ POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:str
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-08-51_909_8811972959551956946/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-57_089_3981013039375138901/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-08-51_909_8811972959551956946/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-21-57_089_3981013039375138901/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	val_0
@@ -76,10 +76,3 @@ POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:str
 2	val_2
 5	val_5
 9	val_9
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/filter_join_breaktask.q.out b/ql/src/test/results/clientpositive/filter_join_breaktask.q.out
index 0e33a59334..efd7c825ee 100644
--- a/ql/src/test/results/clientpositive/filter_join_breaktask.q.out
+++ b/ql/src/test/results/clientpositive/filter_join_breaktask.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE filter_join_breaktask
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE filter_join_breaktask
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE filter_join_breaktask(key int, value string) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE filter_join_breaktask(key int, value string) partitioned by (ds string)
@@ -123,7 +119,7 @@ STAGE PLANS:
               serialization.ddl struct filter_join_breaktask { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1276212172
+              transient_lastDdlTime 1279736517
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -140,7 +136,7 @@ STAGE PLANS:
                 serialization.ddl struct filter_join_breaktask { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1276212172
+                transient_lastDdlTime 1279736517
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: filter_join_breaktask
             name: filter_join_breaktask
@@ -171,7 +167,7 @@ STAGE PLANS:
                 File Output Operator
                   compressed: false
                   GlobalTableId: 0
-                  directory: file:/tmp/jssarma/hive_2010-06-10_16-22-55_084_1486227885854106344/10002
+                  directory: file:/tmp/jssarma/hive_2010-07-21_11-22-00_300_3159170495555394870/10002
                   NumFilesPerFileSink: 1
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -227,7 +223,7 @@ STAGE PLANS:
       Needs Tagging: true
       Path -> Alias:
         file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/filter_join_breaktask/ds=2008-04-08 [g]
-        file:/tmp/jssarma/hive_2010-06-10_16-22-55_084_1486227885854106344/10002 [$INTNAME]
+        file:/tmp/jssarma/hive_2010-07-21_11-22-00_300_3159170495555394870/10002 [$INTNAME]
       Path -> Partition:
         file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/filter_join_breaktask/ds=2008-04-08 
           Partition
@@ -248,7 +244,7 @@ STAGE PLANS:
               serialization.ddl struct filter_join_breaktask { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1276212172
+              transient_lastDdlTime 1279736517
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -265,11 +261,11 @@ STAGE PLANS:
                 serialization.ddl struct filter_join_breaktask { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1276212172
+                transient_lastDdlTime 1279736517
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: filter_join_breaktask
             name: filter_join_breaktask
-        file:/tmp/jssarma/hive_2010-06-10_16-22-55_084_1486227885854106344/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-22-00_300_3159170495555394870/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -304,7 +300,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-06-10_16-22-55_084_1486227885854106344/10001
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-22-00_300_3159170495555394870/10001
               NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -326,13 +322,13 @@ FROM filter_join_breaktask f JOIN filter_join_breaktask m ON( f.key = m.key AND
 JOIN filter_join_breaktask g ON(g.value = m.value AND g.ds='2008-04-08' AND m.ds='2008-04-08' AND m.value is not null AND m.value !='')
 PREHOOK: type: QUERY
 PREHOOK: Input: default@filter_join_breaktask@ds=2008-04-08
-PREHOOK: Output: file:/tmp/jssarma/hive_2010-06-10_16-22-55_434_9085757094579036575/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-00_469_3112953081846350734/10000
 POSTHOOK: query: SELECT f.key, g.value 
 FROM filter_join_breaktask f JOIN filter_join_breaktask m ON( f.key = m.key AND f.ds='2008-04-08' AND m.ds='2008-04-08' AND f.key is not null) 
 JOIN filter_join_breaktask g ON(g.value = m.value AND g.ds='2008-04-08' AND m.ds='2008-04-08' AND m.value is not null AND m.value !='')
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@filter_join_breaktask@ds=2008-04-08
-POSTHOOK: Output: file:/tmp/jssarma/hive_2010-06-10_16-22-55_434_9085757094579036575/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-00_469_3112953081846350734/10000
 POSTHOOK: Lineage: filter_join_breaktask PARTITION(ds=2008-04-08).key EXPRESSION [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: filter_join_breaktask PARTITION(ds=2008-04-08).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 146	val_146
@@ -347,10 +343,3 @@ POSTHOOK: Lineage: filter_join_breaktask PARTITION(ds=2008-04-08).value SIMPLE [
 406	val_406
 66	val_66
 98	val_98
-PREHOOK: query: DROP TABLE filter_join_breaktask
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE filter_join_breaktask
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@filter_join_breaktask
-POSTHOOK: Lineage: filter_join_breaktask PARTITION(ds=2008-04-08).key EXPRESSION [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: filter_join_breaktask PARTITION(ds=2008-04-08).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/filter_join_breaktask2.q.out b/ql/src/test/results/clientpositive/filter_join_breaktask2.q.out
index 8fc2064ecd..d983402b5b 100644
--- a/ql/src/test/results/clientpositive/filter_join_breaktask2.q.out
+++ b/ql/src/test/results/clientpositive/filter_join_breaktask2.q.out
@@ -1,19 +1,3 @@
-PREHOOK: query: drop table T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T4
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table T1(c1 string, c2 string, c3 string, c4 string, c5 string, c6 string, c7 string) 
 partitioned by (ds string)
 PREHOOK: type: CREATETABLE
@@ -269,11 +253,11 @@ POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c9 SIMPLE []
 PREHOOK: query: select * from T2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2@ds=2010-04-17
-PREHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-08_819_6769669287322680370/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-18_463_1385997241277658790/10000
 POSTHOOK: query: select * from T2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2@ds=2010-04-17
-POSTHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-08_819_6769669287322680370/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-18_463_1385997241277658790/10000
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c1 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c2 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c3 SIMPLE []
@@ -398,11 +382,11 @@ POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c9 SIMPLE []
 PREHOOK: query: select * from T1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1@ds=2010-04-17
-PREHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-09_308_2908418676652974905/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-18_668_3928437893588448271/10000
 POSTHOOK: query: select * from T1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1@ds=2010-04-17
-POSTHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-09_308_2908418676652974905/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-18_668_3928437893588448271/10000
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c1 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c2 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c3 SIMPLE []
@@ -527,11 +511,11 @@ POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c9 SIMPLE []
 PREHOOK: query: select * from T3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t3@ds=2010-04-17
-PREHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-09_632_4357906996789662185/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-18_801_6024512080264702124/10000
 POSTHOOK: query: select * from T3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t3@ds=2010-04-17
-POSTHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-09_632_4357906996789662185/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-18_801_6024512080264702124/10000
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c1 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c2 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c3 SIMPLE []
@@ -656,11 +640,11 @@ POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c9 SIMPLE []
 PREHOOK: query: select * from T4
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t4@ds=2010-04-17
-PREHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-09_923_4310274942964197494/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-18_931_7314109222032805369/10000
 POSTHOOK: query: select * from T4
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t4@ds=2010-04-17
-POSTHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-09_923_4310274942964197494/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-18_931_7314109222032805369/10000
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c1 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c2 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c3 SIMPLE []
@@ -794,7 +778,7 @@ PREHOOK: Input: default@t4@ds=2010-04-17
 PREHOOK: Input: default@t2@ds=2010-04-17
 PREHOOK: Input: default@t3@ds=2010-04-17
 PREHOOK: Input: default@t1@ds=2010-04-17
-PREHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-10_432_8791433634827654875/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-19_056_2171086064566020577/10000
 POSTHOOK: query: SELECT a.c1 as a_c1, b.c1 b_c1, d.c0 as d_c0
 FROM T1 a JOIN T2 b 
        ON (a.c1 = b.c1 AND a.ds='2010-04-17' AND b.ds='2010-04-17')
@@ -807,7 +791,7 @@ POSTHOOK: Input: default@t4@ds=2010-04-17
 POSTHOOK: Input: default@t2@ds=2010-04-17
 POSTHOOK: Input: default@t3@ds=2010-04-17
 POSTHOOK: Input: default@t1@ds=2010-04-17
-POSTHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-55-10_432_8791433634827654875/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-19_056_2171086064566020577/10000
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c1 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c2 SIMPLE []
 POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c3 SIMPLE []
@@ -929,503 +913,3 @@ POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c82 EXPRESSION []
 POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c83 EXPRESSION []
 POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c9 SIMPLE []
 5	5	4
-PREHOOK: query: drop table T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c0 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c10 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c11 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c12 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c13 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c14 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c15 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c16 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c17 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c18 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c19 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c20 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c21 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c22 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c23 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c24 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c25 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c8 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c9 SIMPLE []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c0 EXPRESSION []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c1 EXPRESSION []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c0 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c10 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c11 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c12 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c13 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c14 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c15 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c16 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c17 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c18 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c19 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c20 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c21 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c22 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c23 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c24 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c25 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c26 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c27 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c28 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c29 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c30 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c31 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c32 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c33 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c34 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c35 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c36 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c37 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c38 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c39 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c40 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c41 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c42 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c43 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c44 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c45 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c46 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c47 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c48 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c49 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c50 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c51 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c52 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c53 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c54 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c55 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c56 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c57 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c58 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c59 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c60 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c61 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c62 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c63 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c64 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c65 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c66 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c67 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c68 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c69 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c70 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c71 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c72 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c73 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c74 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c75 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c76 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c77 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c78 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c79 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c8 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c80 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c81 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c82 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c83 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c9 SIMPLE []
-PREHOOK: query: drop table T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c0 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c10 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c11 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c12 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c13 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c14 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c15 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c16 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c17 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c18 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c19 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c20 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c21 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c22 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c23 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c24 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c25 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c8 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c9 SIMPLE []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c0 EXPRESSION []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c1 EXPRESSION []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c0 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c10 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c11 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c12 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c13 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c14 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c15 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c16 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c17 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c18 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c19 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c20 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c21 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c22 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c23 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c24 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c25 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c26 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c27 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c28 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c29 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c30 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c31 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c32 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c33 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c34 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c35 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c36 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c37 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c38 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c39 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c40 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c41 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c42 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c43 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c44 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c45 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c46 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c47 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c48 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c49 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c50 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c51 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c52 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c53 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c54 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c55 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c56 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c57 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c58 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c59 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c60 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c61 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c62 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c63 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c64 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c65 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c66 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c67 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c68 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c69 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c70 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c71 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c72 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c73 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c74 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c75 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c76 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c77 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c78 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c79 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c8 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c80 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c81 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c82 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c83 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c9 SIMPLE []
-PREHOOK: query: drop table T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t3
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c0 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c10 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c11 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c12 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c13 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c14 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c15 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c16 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c17 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c18 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c19 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c20 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c21 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c22 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c23 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c24 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c25 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c8 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c9 SIMPLE []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c0 EXPRESSION []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c1 EXPRESSION []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c0 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c10 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c11 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c12 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c13 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c14 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c15 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c16 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c17 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c18 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c19 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c20 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c21 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c22 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c23 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c24 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c25 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c26 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c27 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c28 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c29 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c30 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c31 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c32 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c33 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c34 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c35 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c36 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c37 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c38 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c39 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c40 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c41 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c42 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c43 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c44 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c45 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c46 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c47 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c48 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c49 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c50 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c51 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c52 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c53 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c54 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c55 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c56 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c57 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c58 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c59 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c60 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c61 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c62 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c63 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c64 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c65 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c66 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c67 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c68 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c69 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c70 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c71 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c72 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c73 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c74 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c75 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c76 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c77 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c78 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c79 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c8 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c80 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c81 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c82 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c83 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c9 SIMPLE []
-PREHOOK: query: drop table T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table T4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t4
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t1 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c0 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c10 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c11 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c12 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c13 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c14 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c15 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c16 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c17 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c18 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c19 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c20 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c21 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c22 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c23 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c24 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c25 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c8 SIMPLE []
-POSTHOOK: Lineage: t2 PARTITION(ds=2010-04-17).c9 SIMPLE []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c0 EXPRESSION []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c1 EXPRESSION []
-POSTHOOK: Lineage: t3 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c0 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c1 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c10 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c11 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c12 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c13 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c14 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c15 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c16 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c17 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c18 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c19 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c2 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c20 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c21 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c22 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c23 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c24 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c25 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c26 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c27 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c28 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c29 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c3 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c30 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c31 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c32 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c33 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c34 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c35 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c36 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c37 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c38 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c39 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c4 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c40 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c41 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c42 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c43 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c44 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c45 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c46 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c47 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c48 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c49 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c5 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c50 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c51 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c52 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c53 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c54 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c55 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c56 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c57 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c58 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c59 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c6 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c60 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c61 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c62 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c63 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c64 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c65 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c66 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c67 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c68 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c69 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c7 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c70 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c71 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c72 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c73 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c74 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c75 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c76 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c77 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c78 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c79 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c8 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c80 SIMPLE []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c81 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c82 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c83 EXPRESSION []
-POSTHOOK: Lineage: t4 PARTITION(ds=2010-04-17).c9 SIMPLE []
diff --git a/ql/src/test/results/clientpositive/groupby10.q.out b/ql/src/test/results/clientpositive/groupby10.q.out
index 83bc5b8174..57fc59ce6e 100644
--- a/ql/src/test/results/clientpositive/groupby10.q.out
+++ b/ql/src/test/results/clientpositive/groupby10.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table dest2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table INPUT
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table INPUT
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest1(key INT, val1 INT, val2 INT)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(key INT, val1 INT, val2 INT)
@@ -107,7 +95,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-33_732_8719088178849924632/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-22-31_118_2529581520401612430/10004 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -173,7 +161,7 @@ STAGE PLANS:
   Stage: Stage-4
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-33_732_8719088178849924632/10005 
+        file:/tmp/jssarma/hive_2010-07-21_11-22-31_118_2529581520401612430/10005 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -260,11 +248,11 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(input)input.FieldSchema(name:value, t
 PREHOOK: query: SELECT * from dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-49_016_4991831970418138430/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-39_144_1975011296044036004/10000
 POSTHOOK: query: SELECT * from dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-49_016_4991831970418138430/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-39_144_1975011296044036004/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
 POSTHOOK: Lineage: dest1.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
@@ -294,11 +282,11 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(input)input.FieldSchema(name:value, t
 PREHOOK: query: SELECT * from dest2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-49_066_3993931016056082294/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-39_198_7481470784996277044/10000
 POSTHOOK: query: SELECT * from dest2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-10-49_066_3993931016056082294/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-39_198_7481470784996277044/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
 POSTHOOK: Lineage: dest1.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
@@ -325,36 +313,3 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(input)input.FieldSchema(name:value, t
 401	401	401
 409	409	409
 484	484	484
-PREHOOK: query: drop table INPUT
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table INPUT
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@input
-POSTHOOK: Lineage: dest1.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: dest1.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest1.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest2.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: dest2.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: dest1.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest1.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest2.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: dest2.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table dest2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: dest1.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest1.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest2.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: dest2.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/groupby11.q.out b/ql/src/test/results/clientpositive/groupby11.q.out
index ac4c486e66..123bd3e98c 100644
--- a/ql/src/test/results/clientpositive/groupby11.q.out
+++ b/ql/src/test/results/clientpositive/groupby11.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table dest2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest1(key STRING, val1 INT, val2 INT) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(key STRING, val1 INT, val2 INT) partitioned by (ds string)
@@ -99,7 +91,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-08-54_528_3582940419290173877/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-22-39_568_3957054152816650703/10004 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -167,7 +159,7 @@ STAGE PLANS:
   Stage: Stage-4
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-08-54_528_3582940419290173877/10005 
+        file:/tmp/jssarma/hive_2010-07-21_11-22-39_568_3957054152816650703/10005 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -260,11 +252,11 @@ POSTHOOK: Lineage: dest2 PARTITION(ds=111).val2 EXPRESSION [(src)src.FieldSchema
 PREHOOK: query: SELECT * from dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1@ds=111
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-09-07_685_5264951064800815110/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-47_442_688004092056696440/10000
 POSTHOOK: query: SELECT * from dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1@ds=111
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-09-07_685_5264951064800815110/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-47_442_688004092056696440/10000
 POSTHOOK: Lineage: dest1 PARTITION(ds=111).key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1 PARTITION(ds=111).val1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1 PARTITION(ds=111).val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -583,11 +575,11 @@ val_98	2	1	111
 PREHOOK: query: SELECT * from dest2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest2@ds=111
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-09-07_785_8160854463566208453/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-47_517_4221724803022357819/10000
 POSTHOOK: query: SELECT * from dest2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest2@ds=111
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-09-07_785_8160854463566208453/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-22-47_517_4221724803022357819/10000
 POSTHOOK: Lineage: dest1 PARTITION(ds=111).key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1 PARTITION(ds=111).val1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1 PARTITION(ds=111).val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -903,25 +895,3 @@ POSTHOOK: Lineage: dest2 PARTITION(ds=111).val2 EXPRESSION [(src)src.FieldSchema
 96	1	1	111
 97	2	1	111
 98	2	1	111
-PREHOOK: query: drop table dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1 PARTITION(ds=111).key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1 PARTITION(ds=111).val1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1 PARTITION(ds=111).val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2 PARTITION(ds=111).key EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2 PARTITION(ds=111).val1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2 PARTITION(ds=111).val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: drop table dest2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1 PARTITION(ds=111).key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1 PARTITION(ds=111).val1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1 PARTITION(ds=111).val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2 PARTITION(ds=111).key EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2 PARTITION(ds=111).val1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2 PARTITION(ds=111).val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/groupby3.q.out b/ql/src/test/results/clientpositive/groupby3.q.out
index 7dbe1aac66..a3c0fa7a52 100644
--- a/ql/src/test/results/clientpositive/groupby3.q.out
+++ b/ql/src/test/results/clientpositive/groupby3.q.out
@@ -83,7 +83,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-08-30_974_5290708260154041100/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-23-30_013_9206830132841854931/10002 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -223,11 +223,11 @@ POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:st
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-08-40_661_8875038042592803685/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-23-35_334_4005630322306317272/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-08-40_661_8875038042592803685/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-23-35_334_4005630322306317272/10000
 POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c3 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -238,17 +238,3 @@ POSTHOOK: Lineage: dest1.c7 EXPRESSION [(src)src.FieldSchema(name:value, type:st
 POSTHOOK: Lineage: dest1.c8 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 130091.0	260.182	256.10355987055016	98.0	0.0	142.92680950752379	143.06995106518903	20428.07287599999	20469.010897795582
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c3 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c4 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c5 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c6 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c7 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c8 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/groupby3_map.q.out b/ql/src/test/results/clientpositive/groupby3_map.q.out
index 6ac844d22d..d4576bfdf1 100644
--- a/ql/src/test/results/clientpositive/groupby3_map.q.out
+++ b/ql/src/test/results/clientpositive/groupby3_map.q.out
@@ -207,11 +207,11 @@ POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:st
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-08-49_083_7812532907486997795/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-23-38_629_5098069129422470787/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-08-49_083_7812532907486997795/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-23-38_629_5098069129422470787/10000
 POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c3 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -222,17 +222,3 @@ POSTHOOK: Lineage: dest1.c7 EXPRESSION [(src)src.FieldSchema(name:value, type:st
 POSTHOOK: Lineage: dest1.c8 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 130091.0	260.182	256.10355987055016	98.0	0.0	142.9268095075238	143.06995106518906	20428.072876	20469.01089779559
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c3 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c4 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c5 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c6 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c7 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c8 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/groupby3_map_skew.q.out b/ql/src/test/results/clientpositive/groupby3_map_skew.q.out
index 9ec39efbe6..b003bf1cc0 100644
--- a/ql/src/test/results/clientpositive/groupby3_map_skew.q.out
+++ b/ql/src/test/results/clientpositive/groupby3_map_skew.q.out
@@ -119,7 +119,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-11-19_485_6759193827936294497/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-23-38_951_8559728103103624706/10002 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -259,11 +259,11 @@ POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:st
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-11-30_179_7629474680624414038/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-23-44_381_5135448855404252228/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-11-30_179_7629474680624414038/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-23-44_381_5135448855404252228/10000
 POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c3 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -274,17 +274,3 @@ POSTHOOK: Lineage: dest1.c7 EXPRESSION [(src)src.FieldSchema(name:value, type:st
 POSTHOOK: Lineage: dest1.c8 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 130091.0	260.182	256.10355987055016	98.0	0.0	142.9268095075238	143.06995106518906	20428.072876	20469.01089779559
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c3 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c4 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c5 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c6 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c7 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c8 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/groupby3_noskew.q.out b/ql/src/test/results/clientpositive/groupby3_noskew.q.out
index 3185702048..826aa117cb 100644
--- a/ql/src/test/results/clientpositive/groupby3_noskew.q.out
+++ b/ql/src/test/results/clientpositive/groupby3_noskew.q.out
@@ -171,11 +171,11 @@ POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:st
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-09-38_039_6030452822453768062/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-23-47_426_1452039016187035932/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-09-38_039_6030452822453768062/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-23-47_426_1452039016187035932/10000
 POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c3 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -186,17 +186,3 @@ POSTHOOK: Lineage: dest1.c7 EXPRESSION [(src)src.FieldSchema(name:value, type:st
 POSTHOOK: Lineage: dest1.c8 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 130091.0	260.182	256.10355987055016	98.0	0.0	142.92680950752379	143.06995106518903	20428.07287599999	20469.010897795582
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.c1 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c3 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c4 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c5 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c6 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c7 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c8 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.c9 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/groupby9.q.out b/ql/src/test/results/clientpositive/groupby9.q.out
index 2f2d8999f3..9736662c99 100644
--- a/ql/src/test/results/clientpositive/groupby9.q.out
+++ b/ql/src/test/results/clientpositive/groupby9.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table DEST1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table DEST2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE
@@ -95,7 +87,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-09-55_073_4811751815642872862/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-25-41_473_5912399545173757449/10004 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -154,7 +146,7 @@ STAGE PLANS:
   Stage: Stage-4
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-09-55_073_4811751815642872862/10005 
+        file:/tmp/jssarma/hive_2010-07-21_11-25-41_473_5912399545173757449/10005 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -243,11 +235,11 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)src.FieldSchema(name:value, type:
 PREHOOK: query: SELECT DEST1.* FROM DEST1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-10-08_620_486578954348947231/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-25-49_248_6786053973649103964/10000
 POSTHOOK: query: SELECT DEST1.* FROM DEST1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-10-08_620_486578954348947231/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-25-49_248_6786053973649103964/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -565,11 +557,11 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)src.FieldSchema(name:value, type:
 PREHOOK: query: SELECT DEST2.* FROM DEST2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-10-08_680_6077462527451103689/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-25-49_307_6015579282653689221/10000
 POSTHOOK: query: SELECT DEST2.* FROM DEST2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-10-08_680_6077462527451103689/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-25-49_307_6015579282653689221/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -884,23 +876,3 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)src.FieldSchema(name:value, type:
 96	val_96	1
 97	val_97	1
 98	val_98	1
-PREHOOK: query: drop table DEST1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table DEST2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/implicit_cast1.q.out b/ql/src/test/results/clientpositive/implicit_cast1.q.out
index 5e37e4a438..f21b839b0d 100644
--- a/ql/src/test/results/clientpositive/implicit_cast1.q.out
+++ b/ql/src/test/results/clientpositive/implicit_cast1.q.out
@@ -59,15 +59,10 @@ FROM implicit_test1
 WHERE implicit_test1.a <> 0
 PREHOOK: type: QUERY
 PREHOOK: Input: default@implicit_test1
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/439003851/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-26-24_976_2073073634228618169/10000
 POSTHOOK: query: SELECT implicit_test1.*
 FROM implicit_test1
 WHERE implicit_test1.a <> 0
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@implicit_test1
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/439003851/10000
-PREHOOK: query: DROP TABLE implicit_test1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE implicit_test1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@implicit_test1
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-26-24_976_2073073634228618169/10000
diff --git a/ql/src/test/results/clientpositive/init_file.q.out b/ql/src/test/results/clientpositive/init_file.q.out
index bb016fd487..f585754db1 100644
--- a/ql/src/test/results/clientpositive/init_file.q.out
+++ b/ql/src/test/results/clientpositive/init_file.q.out
@@ -9,16 +9,11 @@ PREHOOK: query: -- tbl_created_by_init is supposed to have been created for us
 select * from tbl_created_by_init
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tbl_created_by_init
-PREHOOK: Output: file:/var/folders/7P/7PeC14kXFIWq0PIYyexGbmKuXUk/-Tmp-/jsichi/hive_2010-06-18_15-26-08_648_7822849778594453961/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-26-27_744_1100043554292552055/10000
 POSTHOOK: query: -- tbl_created_by_init is supposed to have been created for us
 -- automatically by test_init_file.sql
 
 select * from tbl_created_by_init
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tbl_created_by_init
-POSTHOOK: Output: file:/var/folders/7P/7PeC14kXFIWq0PIYyexGbmKuXUk/-Tmp-/jsichi/hive_2010-06-18_15-26-08_648_7822849778594453961/10000
-PREHOOK: query: drop table tbl_created_by_init
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tbl_created_by_init
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tbl_created_by_init
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-26-27_744_1100043554292552055/10000
diff --git a/ql/src/test/results/clientpositive/input1.q.out b/ql/src/test/results/clientpositive/input1.q.out
index b1fd739f4d..cc935d9079 100644
--- a/ql/src/test/results/clientpositive/input1.q.out
+++ b/ql/src/test/results/clientpositive/input1.q.out
@@ -33,8 +33,3 @@ POSTHOOK: query: DESCRIBE TEST1
 POSTHOOK: type: DESCTABLE
 a	int	
 b	double	
-PREHOOK: query: DROP TABLE TEST1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@test1
diff --git a/ql/src/test/results/clientpositive/input10.q.out b/ql/src/test/results/clientpositive/input10.q.out
index 78e671cedf..e22a2f4560 100644
--- a/ql/src/test/results/clientpositive/input10.q.out
+++ b/ql/src/test/results/clientpositive/input10.q.out
@@ -35,8 +35,3 @@ key	int
 value	string	
 ds	string	
 hr	string	
-PREHOOK: query: DROP TABLE TEST10
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST10
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@test10
diff --git a/ql/src/test/results/clientpositive/input15.q.out b/ql/src/test/results/clientpositive/input15.q.out
index acfd3e28c3..f0a8fd73ed 100644
--- a/ql/src/test/results/clientpositive/input15.q.out
+++ b/ql/src/test/results/clientpositive/input15.q.out
@@ -35,8 +35,3 @@ POSTHOOK: query: DESCRIBE TEST15
 POSTHOOK: type: DESCTABLE
 key	int	
 value	string	
-PREHOOK: query: DROP TABLE TEST15
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST15
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@test15
diff --git a/ql/src/test/results/clientpositive/input16.q.out b/ql/src/test/results/clientpositive/input16.q.out
index 52c83e5bad..0b58a7fb39 100644
--- a/ql/src/test/results/clientpositive/input16.q.out
+++ b/ql/src/test/results/clientpositive/input16.q.out
@@ -17,11 +17,11 @@ POSTHOOK: Output: default@input16
 PREHOOK: query: SELECT INPUT16.VALUE, INPUT16.KEY FROM INPUT16
 PREHOOK: type: QUERY
 PREHOOK: Input: default@input16
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1455551235/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_14-10-01_833_5335657696200375421/10000
 POSTHOOK: query: SELECT INPUT16.VALUE, INPUT16.KEY FROM INPUT16
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@input16
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1455551235/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_14-10-01_833_5335657696200375421/10000
 val_238	238
 val_86	86
 val_311	311
@@ -522,8 +522,3 @@ val_403	403
 val_400	400
 val_200	200
 val_97	97
-PREHOOK: query: DROP TABLE INPUT16
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUT16
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@input16
diff --git a/ql/src/test/results/clientpositive/input16_cc.q.out b/ql/src/test/results/clientpositive/input16_cc.q.out
index fc4c40e712..b6e612ea0a 100644
--- a/ql/src/test/results/clientpositive/input16_cc.q.out
+++ b/ql/src/test/results/clientpositive/input16_cc.q.out
@@ -19,11 +19,11 @@ POSTHOOK: Output: default@input16_cc
 PREHOOK: query: SELECT INPUT16_CC.VALUE, INPUT16_CC.KEY FROM INPUT16_CC
 PREHOOK: type: QUERY
 PREHOOK: Input: default@input16_cc
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1052706043/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_14-18-05_263_4460949840611638279/10000
 POSTHOOK: query: SELECT INPUT16_CC.VALUE, INPUT16_CC.KEY FROM INPUT16_CC
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@input16_cc
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1052706043/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_14-18-05_263_4460949840611638279/10000
 val_238	238
 val_86	86
 val_311	311
@@ -524,8 +524,3 @@ val_403	403
 val_400	400
 val_200	200
 val_97	97
-PREHOOK: query: DROP TABLE INPUT16_CC
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUT16_CC
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@input16_cc
diff --git a/ql/src/test/results/clientpositive/input19.q.out b/ql/src/test/results/clientpositive/input19.q.out
index 5509c43905..ba31b5fd28 100644
--- a/ql/src/test/results/clientpositive/input19.q.out
+++ b/ql/src/test/results/clientpositive/input19.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table apachelog
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table apachelog
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table apachelog(ipaddress STRING,identd STRING,user STRING,finishtime STRING,requestline string,returncode INT,size INT) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe' WITH SERDEPROPERTIES (  'serialization.format'= 'org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol',  'quote.delim'= '("|\\[|\\])',  'field.delim'=' ',  'serialization.null.format'='-'  ) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table apachelog(ipaddress STRING,identd STRING,user STRING,finishtime STRING,requestline string,returncode INT,size INT) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe' WITH SERDEPROPERTIES (  'serialization.format'= 'org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol',  'quote.delim'= '("|\\[|\\])',  'field.delim'=' ',  'serialization.null.format'='-'  ) STORED AS TEXTFILE
@@ -15,14 +11,9 @@ POSTHOOK: Output: default@apachelog
 PREHOOK: query: SELECT a.* FROM apachelog a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@apachelog
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/.ptest_3/build/ql/tmp/1231763776/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-26-57_493_8424717254986801325/10000
 POSTHOOK: query: SELECT a.* FROM apachelog a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@apachelog
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/.ptest_3/build/ql/tmp/1231763776/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-26-57_493_8424717254986801325/10000
 127.0.0.1	NULL	frank	10/Oct/2000:13:55:36 -0700	GET /apache_pb.gif HTTP/1.0	200	2326
-PREHOOK: query: drop table apachelog
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table apachelog
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@apachelog
diff --git a/ql/src/test/results/clientpositive/input1_limit.q.out b/ql/src/test/results/clientpositive/input1_limit.q.out
index e6df9210c7..192eceb7bb 100644
--- a/ql/src/test/results/clientpositive/input1_limit.q.out
+++ b/ql/src/test/results/clientpositive/input1_limit.q.out
@@ -104,7 +104,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-14-04_330_2459663668263401934/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-26-57_854_4999061439601735218/10004 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -164,11 +164,11 @@ POSTHOOK: Lineage: dest2.value SIMPLE [(src)src.FieldSchema(name:value, type:str
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-14-12_951_2870105461951020454/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-03_051_2929820140133939709/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-14-12_951_2870105461951020454/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-03_051_2929820140133939709/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -186,11 +186,11 @@ POSTHOOK: Lineage: dest2.value SIMPLE [(src)src.FieldSchema(name:value, type:str
 PREHOOK: query: SELECT dest2.* FROM dest2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-14-13_000_4947993081086639811/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-03_103_6672174195466398495/10000
 POSTHOOK: query: SELECT dest2.* FROM dest2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-14-13_000_4947993081086639811/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-03_103_6672174195466398495/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -200,21 +200,3 @@ POSTHOOK: Lineage: dest2.value SIMPLE [(src)src.FieldSchema(name:value, type:str
 98	val_98
 66	val_66
 37	val_37
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE dest2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/input2.q.out b/ql/src/test/results/clientpositive/input2.q.out
index 68e51d819e..6a54133c0a 100644
--- a/ql/src/test/results/clientpositive/input2.q.out
+++ b/ql/src/test/results/clientpositive/input2.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: DROP TABLE TEST2a
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST2a
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE TEST2b
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST2b
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE TEST2a(A INT, B DOUBLE) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE TEST2a(A INT, B DOUBLE) STORED AS TEXTFILE
@@ -95,11 +87,3 @@ STAGE PLANS:
       limit: -1
 
 
-PREHOOK: query: DROP TABLE TEST2a
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST2a
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE TEST2b
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST2b
-POSTHOOK: type: DROPTABLE
diff --git a/ql/src/test/results/clientpositive/input21.q.out b/ql/src/test/results/clientpositive/input21.q.out
index a678f5ba25..a353786a1b 100644
--- a/ql/src/test/results/clientpositive/input21.q.out
+++ b/ql/src/test/results/clientpositive/input21.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE src_null
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE src_null
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE src_null(a STRING, b STRING, c STRING, d STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE src_null(a STRING, b STRING, c STRING, d STRING) STORED AS TEXTFILE
@@ -76,11 +72,11 @@ STAGE PLANS:
 PREHOOK: query: SELECT * FROM src_null DISTRIBUTE BY c SORT BY d
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_null
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/944884170/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-09_926_7521845290777085588/10000
 POSTHOOK: query: SELECT * FROM src_null DISTRIBUTE BY c SORT BY d
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_null
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/944884170/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-09_926_7521845290777085588/10000
 1.0	1	same	0
 1.0	1	same	1
 1.0	1	same	2
@@ -91,8 +87,3 @@ NULL	NULL	same	6
 1.0	NULL	same	7
 1.0	1	same	8
 1.0	1	same	9
-PREHOOK: query: DROP TABLE src_null
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE src_null
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@src_null
diff --git a/ql/src/test/results/clientpositive/input22.q.out b/ql/src/test/results/clientpositive/input22.q.out
index 0a90a3011d..8d84832247 100644
--- a/ql/src/test/results/clientpositive/input22.q.out
+++ b/ql/src/test/results/clientpositive/input22.q.out
@@ -74,14 +74,14 @@ FROM (SELECT INPUT4.*, INPUT4.KEY as KEY2
 ORDER BY KEY2 LIMIT 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@input4
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/569155063/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-12_928_5119963589127065058/10000
 POSTHOOK: query: SELECT a.KEY2
 FROM (SELECT INPUT4.*, INPUT4.KEY as KEY2
       FROM INPUT4) a
 ORDER BY KEY2 LIMIT 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@input4
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/569155063/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-12_928_5119963589127065058/10000
 0
 0
 0
@@ -92,8 +92,3 @@ POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/569155063/1000
 103
 104
 104
-PREHOOK: query: DROP TABLE INPUT4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUT4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@input4
diff --git a/ql/src/test/results/clientpositive/input24.q.out b/ql/src/test/results/clientpositive/input24.q.out
index b31bf84228..b43dab24f4 100644
--- a/ql/src/test/results/clientpositive/input24.q.out
+++ b/ql/src/test/results/clientpositive/input24.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table tst
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table tst(a int, b int) partitioned by (d string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tst(a int, b int) partitioned by (d string)
@@ -80,14 +76,9 @@ STAGE PLANS:
 PREHOOK: query: select count(1) from tst x where x.d='2009-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tst@d=2009-01-01
-PREHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/1844534662/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-18_997_2954899292364767462/10000
 POSTHOOK: query: select count(1) from tst x where x.d='2009-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tst@d=2009-01-01
-POSTHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/1844534662/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-18_997_2954899292364767462/10000
 0
-PREHOOK: query: drop table tst
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tst
diff --git a/ql/src/test/results/clientpositive/input25.q.out b/ql/src/test/results/clientpositive/input25.q.out
index f7f6e238d4..c6b6b06f92 100644
--- a/ql/src/test/results/clientpositive/input25.q.out
+++ b/ql/src/test/results/clientpositive/input25.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table tst
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table tst(a int, b int) partitioned by (d string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tst(a int, b int) partitioned by (d string)
@@ -88,7 +84,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/heyongqiang/hive-trunk/.ptest_2/build/ql/tmp/1330722084/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-27-21_972_4570921732718376646/10002 
           Union
             Select Operator
               expressions:
@@ -105,7 +101,7 @@ STAGE PLANS:
                 table:
                     input format: org.apache.hadoop.mapred.TextInputFormat
                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-        file:/data/users/heyongqiang/hive-trunk/.ptest_2/build/ql/tmp/1330722084/10003 
+        file:/tmp/jssarma/hive_2010-07-21_11-27-21_972_4570921732718376646/10003 
           Union
             Select Operator
               expressions:
@@ -180,7 +176,7 @@ PREHOOK: query: select * from (
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tst@d=2009-01-01
 PREHOOK: Input: default@tst@d=2009-02-02
-PREHOOK: Output: file:/data/users/heyongqiang/hive-trunk/.ptest_2/build/ql/tmp/2012659727/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-22_116_2207462845190114193/10000
 POSTHOOK: query: select * from (
   select * from tst x where x.d='2009-01-01' limit 10
     union all
@@ -189,9 +185,4 @@ POSTHOOK: query: select * from (
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tst@d=2009-01-01
 POSTHOOK: Input: default@tst@d=2009-02-02
-POSTHOOK: Output: file:/data/users/heyongqiang/hive-trunk/.ptest_2/build/ql/tmp/2012659727/10000
-PREHOOK: query: drop table tst
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tst
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-22_116_2207462845190114193/10000
diff --git a/ql/src/test/results/clientpositive/input28.q.out b/ql/src/test/results/clientpositive/input28.q.out
index 673bf3338a..17fcc0ab25 100644
--- a/ql/src/test/results/clientpositive/input28.q.out
+++ b/ql/src/test/results/clientpositive/input28.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table tst
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table tst(a string, b string) partitioned by (d string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tst(a string, b string) partitioned by (d string)
@@ -29,17 +25,10 @@ POSTHOOK: Lineage: tst PARTITION(d=2009-01-01).b SIMPLE [(src)src.FieldSchema(na
 PREHOOK: query: select * from tst where tst.d='2009-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tst@d=2009-01-01
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-11-18_835_8722090511424029889/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-41_262_4407769166268455742/10000
 POSTHOOK: query: select * from tst where tst.d='2009-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tst@d=2009-01-01
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-11-18_835_8722090511424029889/10000
-POSTHOOK: Lineage: tst PARTITION(d=2009-01-01).a SIMPLE [(tst)tst.FieldSchema(name:d, type:string, comment:null), ]
-POSTHOOK: Lineage: tst PARTITION(d=2009-01-01).b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table tst
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tst
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-41_262_4407769166268455742/10000
 POSTHOOK: Lineage: tst PARTITION(d=2009-01-01).a SIMPLE [(tst)tst.FieldSchema(name:d, type:string, comment:null), ]
 POSTHOOK: Lineage: tst PARTITION(d=2009-01-01).b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/input3.q.out b/ql/src/test/results/clientpositive/input3.q.out
index 5e1a645b7d..188f90974f 100644
--- a/ql/src/test/results/clientpositive/input3.q.out
+++ b/ql/src/test/results/clientpositive/input3.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: DROP TABLE TEST3a
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST3a
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE TEST3b
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST3b
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE TEST3c
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST3c
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE TEST3a(A INT, B DOUBLE) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE TEST3a(A INT, B DOUBLE) STORED AS TEXTFILE
@@ -165,18 +153,4 @@ POSTHOOK: type: DESCTABLE
 r1	int	
 r2	double	
 	 	 
-Detailed Table Information	Table(tableName:test3c, dbName:default, owner:njain, createTime:1269906076, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:r1, type:int, comment:null), FieldSchema(name:r2, type:double, comment:null)], location:file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/test3c, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=njain,last_modified_time=1269906077,transient_lastDdlTime=1269906077}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
-PREHOOK: query: DROP TABLE TEST3a
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST3a
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@test3a
-PREHOOK: query: DROP TABLE TEST3b
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST3b
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE TEST3c
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE TEST3c
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@test3c
+Detailed Table Information	Table(tableName:test3c, dbName:default, owner:jssarma, createTime:1279736864, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:r1, type:int, comment:null), FieldSchema(name:r2, type:double, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/test3c, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=jssarma, last_modified_time=1279736865, transient_lastDdlTime=1279736865}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
diff --git a/ql/src/test/results/clientpositive/input30.q.out b/ql/src/test/results/clientpositive/input30.q.out
index 3a8c743444..23891b8901 100644
--- a/ql/src/test/results/clientpositive/input30.q.out
+++ b/ql/src/test/results/clientpositive/input30.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table tst_dest30
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst_dest30
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table dest30
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest30
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table dest30(a int)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dest30(a int)
@@ -106,22 +98,10 @@ POSTHOOK: Lineage: tst_dest30.a EXPRESSION [(src)src.null, ]
 PREHOOK: query: select * from tst_dest30
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tst_dest30
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-11-25_544_4956721364351044321/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-48_308_1489139289605344338/10000
 POSTHOOK: query: select * from tst_dest30
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tst_dest30
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-11-25_544_4956721364351044321/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-48_308_1489139289605344338/10000
 POSTHOOK: Lineage: tst_dest30.a EXPRESSION [(src)src.null, ]
 18
-PREHOOK: query: drop table tst_dest30
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst_dest30
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tst_dest30
-POSTHOOK: Lineage: tst_dest30.a EXPRESSION [(src)src.null, ]
-PREHOOK: query: drop table dest30
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest30
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest30
-POSTHOOK: Lineage: tst_dest30.a EXPRESSION [(src)src.null, ]
diff --git a/ql/src/test/results/clientpositive/input31.q.out b/ql/src/test/results/clientpositive/input31.q.out
index a3a68312c1..605f08f5b2 100644
--- a/ql/src/test/results/clientpositive/input31.q.out
+++ b/ql/src/test/results/clientpositive/input31.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table tst_dest31
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst_dest31
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table dest31
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest31
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table tst_dest31(a int)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tst_dest31(a int)
@@ -110,22 +102,10 @@ POSTHOOK: Lineage: tst_dest31.a EXPRESSION [(srcbucket)srcbucket.null, ]
 PREHOOK: query: select * from tst_dest31
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tst_dest31
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-14-47_969_5252355882529594546/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-51_528_6418079216640080758/10000
 POSTHOOK: query: select * from tst_dest31
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tst_dest31
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-14-47_969_5252355882529594546/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-51_528_6418079216640080758/10000
 POSTHOOK: Lineage: tst_dest31.a EXPRESSION [(srcbucket)srcbucket.null, ]
 493
-PREHOOK: query: drop table tst_dest31
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst_dest31
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tst_dest31
-POSTHOOK: Lineage: tst_dest31.a EXPRESSION [(srcbucket)srcbucket.null, ]
-PREHOOK: query: drop table dest31
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest31
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest31
-POSTHOOK: Lineage: tst_dest31.a EXPRESSION [(srcbucket)srcbucket.null, ]
diff --git a/ql/src/test/results/clientpositive/input32.q.out b/ql/src/test/results/clientpositive/input32.q.out
index 2bd779b088..d26474912c 100644
--- a/ql/src/test/results/clientpositive/input32.q.out
+++ b/ql/src/test/results/clientpositive/input32.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table tst_dest32
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst_dest32
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table dest32
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest32
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table dest32(a int)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dest32(a int)
@@ -102,22 +94,10 @@ POSTHOOK: Lineage: tst_dest32.a EXPRESSION [(srcbucket)srcbucket.null, ]
 PREHOOK: query: select * from tst_dest32
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tst_dest32
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-12-44_362_8153903898810014347/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-54_520_2654638815572437304/10000
 POSTHOOK: query: select * from tst_dest32
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tst_dest32
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-12-44_362_8153903898810014347/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-27-54_520_2654638815572437304/10000
 POSTHOOK: Lineage: tst_dest32.a EXPRESSION [(srcbucket)srcbucket.null, ]
 1000
-PREHOOK: query: drop table tst_dest32
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst_dest32
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tst_dest32
-POSTHOOK: Lineage: tst_dest32.a EXPRESSION [(srcbucket)srcbucket.null, ]
-PREHOOK: query: drop table dest32
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest32
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest32
-POSTHOOK: Lineage: tst_dest32.a EXPRESSION [(srcbucket)srcbucket.null, ]
diff --git a/ql/src/test/results/clientpositive/input37.q.out b/ql/src/test/results/clientpositive/input37.q.out
index 59edb046df..60c5fb1688 100644
--- a/ql/src/test/results/clientpositive/input37.q.out
+++ b/ql/src/test/results/clientpositive/input37.q.out
@@ -18,7 +18,7 @@ FROM
 group by url
 PREHOOK: type: QUERY
 PREHOOK: Input: default@documents
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/282167247/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-09_514_9061274612297235392/10000
 POSTHOOK: query: select url, count(1) 
 FROM
 (
@@ -29,11 +29,6 @@ FROM
 group by url
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@documents
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/282167247/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-09_514_9061274612297235392/10000
 1uauniajqtunlsvadmxhlxvngxpqjuzbpzvdiwmzphmbaicduzkgxgtdeiunduosu.html	4
 4uzsbtwvdypfitqfqdjosynqp.html	4
-PREHOOK: query: DROP TABLE documents
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE documents
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@documents
diff --git a/ql/src/test/results/clientpositive/input38.q.out b/ql/src/test/results/clientpositive/input38.q.out
index 5659dbd488..a614f9b86e 100644
--- a/ql/src/test/results/clientpositive/input38.q.out
+++ b/ql/src/test/results/clientpositive/input38.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest1(key STRING, value STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(key STRING, value STRING) STORED AS TEXTFILE
@@ -79,7 +75,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-12-53_095_8197621741613421666/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-28-12_445_5695557680495519967/10000
 
   Stage: Stage-0
     Move Operator
@@ -94,7 +90,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-12-53_095_8197621741613421666/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-28-12_445_5695557680495519967/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -141,11 +137,11 @@ POSTHOOK: Lineage: dest1.value SCRIPT [(src)src.FieldSchema(name:key, type:strin
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-12-57_512_6617436684028270134/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-15_024_5142138661614792399/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-12-57_512_6617436684028270134/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-15_024_5142138661614792399/10000
 POSTHOOK: Lineage: dest1.key SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
 238	val_238	3	7
@@ -648,10 +644,3 @@ POSTHOOK: Lineage: dest1.value SCRIPT [(src)src.FieldSchema(name:key, type:strin
 400	val_400	3	7
 200	val_200	3	7
 97	val_97	3	7
-PREHOOK: query: drop table dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/input39.q.out b/ql/src/test/results/clientpositive/input39.q.out
index 57a18d4ebc..9b8979980d 100644
--- a/ql/src/test/results/clientpositive/input39.q.out
+++ b/ql/src/test/results/clientpositive/input39.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table t1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table t2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table t1(key string, value string) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table t1(key string, value string) partitioned by (ds string)
@@ -158,7 +150,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-11-59_547_6173111859756543137/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-28-22_923_4771955741892029143/10002 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -193,12 +185,12 @@ PREHOOK: query: select count(1) from t1 join t2 on t1.key=t2.key where t1.ds='1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2@ds=1
 PREHOOK: Input: default@t1@ds=1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-00_052_7056729807398553638/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-23_078_4826392180572466990/10000
 POSTHOOK: query: select count(1) from t1 join t2 on t1.key=t2.key where t1.ds='1' and t2.ds='1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2@ds=1
 POSTHOOK: Input: default@t1@ds=1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-00_052_7056729807398553638/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-23_078_4826392180572466990/10000
 POSTHOOK: Lineage: t1 PARTITION(ds=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: t1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: t1 PARTITION(ds=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -206,25 +198,3 @@ POSTHOOK: Lineage: t1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:va
 POSTHOOK: Lineage: t2 PARTITION(ds=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: t2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 18
-PREHOOK: query: drop table t1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-POSTHOOK: Lineage: t1 PARTITION(ds=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t1 PARTITION(ds=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table t2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-POSTHOOK: Lineage: t1 PARTITION(ds=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t1 PARTITION(ds=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t1 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: t2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/input3_limit.q.out b/ql/src/test/results/clientpositive/input3_limit.q.out
index 725e44cf5c..d00e3be202 100644
--- a/ql/src/test/results/clientpositive/input3_limit.q.out
+++ b/ql/src/test/results/clientpositive/input3_limit.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T1(key STRING, value STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T1(key STRING, value STRING) STORED AS TEXTFILE
@@ -17,10 +13,6 @@ PREHOOK: type: LOAD
 POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv2.txt' INTO TABLE T1
 POSTHOOK: type: LOAD
 POSTHOOK: Output: default@t1
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T2(key STRING, value STRING)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T2(key STRING, value STRING)
@@ -90,7 +82,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-15-07_068_6853046437347700201/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-28-28_770_5844411471048586267/10002 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -135,11 +127,11 @@ POSTHOOK: Lineage: t2.value SIMPLE [(t1)t1.FieldSchema(name:value, type:string,
 PREHOOK: query: SELECT * FROM T2 SORT BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-15-16_045_761099202785601542/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-33_846_8194327293225084148/10000
 POSTHOOK: query: SELECT * FROM T2 SORT BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-15-16_045_761099202785601542/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-33_846_8194327293225084148/10000
 POSTHOOK: Lineage: t2.key SIMPLE [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: t2.value SIMPLE [(t1)t1.FieldSchema(name:value, type:string, comment:null), ]
 0	val_0
@@ -162,17 +154,3 @@ POSTHOOK: Lineage: t2.value SIMPLE [(t1)t1.FieldSchema(name:value, type:string,
 104	val_104
 104	val_105
 104	val_105
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-POSTHOOK: Lineage: t2.key SIMPLE [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: t2.value SIMPLE [(t1)t1.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-POSTHOOK: Lineage: t2.key SIMPLE [(t1)t1.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: t2.value SIMPLE [(t1)t1.FieldSchema(name:value, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/input4.q.out b/ql/src/test/results/clientpositive/input4.q.out
index 4402a1c340..46e087231f 100644
--- a/ql/src/test/results/clientpositive/input4.q.out
+++ b/ql/src/test/results/clientpositive/input4.q.out
@@ -19,8 +19,8 @@ STAGE DEPENDENCIES:
 STAGE PLANS:
   Stage: Stage-0
     Copy
-      source: file:/data/users/njain/hive5/hive5/data/files/kv1.txt
-      destination: file:/data/users/njain/hive5/hive5/build/ql/tmp/1092488123/10000
+      source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/data/files/kv1.txt
+      destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-28-36_668_8539148391422299238/10000
 
   Stage: Stage-1
     Move Operator
@@ -41,11 +41,11 @@ POSTHOOK: Output: default@input4
 PREHOOK: query: SELECT INPUT4.VALUE, INPUT4.KEY FROM INPUT4
 PREHOOK: type: QUERY
 PREHOOK: Input: default@input4
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/78834933/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-36_825_182661124432042155/10000
 POSTHOOK: query: SELECT INPUT4.VALUE, INPUT4.KEY FROM INPUT4
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@input4
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/78834933/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-36_825_182661124432042155/10000
 val_238	238
 val_86	86
 val_311	311
@@ -546,8 +546,3 @@ val_403	403
 val_400	400
 val_200	200
 val_97	97
-PREHOOK: query: DROP TABLE INPUT4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUT4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@input4
diff --git a/ql/src/test/results/clientpositive/input40.q.out b/ql/src/test/results/clientpositive/input40.q.out
index fedeb606b4..5aae476424 100644
--- a/ql/src/test/results/clientpositive/input40.q.out
+++ b/ql/src/test/results/clientpositive/input40.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table tmp_insert_test
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp_insert_test
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table tmp_insert_test_p
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp_insert_test_p
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table tmp_insert_test (key string, value string) stored as textfile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tmp_insert_test (key string, value string) stored as textfile
@@ -19,11 +11,11 @@ POSTHOOK: Output: default@tmp_insert_test
 PREHOOK: query: select * from tmp_insert_test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_insert_test
-PREHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/1950785675/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-39_638_2152892791432727451/10000
 POSTHOOK: query: select * from tmp_insert_test
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_insert_test
-POSTHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/1950785675/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-39_638_2152892791432727451/10000
 238	val_238
 86	val_86
 311	val_311
@@ -538,12 +530,12 @@ PREHOOK: query: select * from tmp_insert_test_p where ds= '2009-08-01'
 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_insert_test_p@ds=2009-08-01
-PREHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/237111013/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-39_834_15988274396181587/10000
 POSTHOOK: query: select * from tmp_insert_test_p where ds= '2009-08-01'
 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_insert_test_p@ds=2009-08-01
-POSTHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/237111013/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-39_834_15988274396181587/10000
 0	val_0	2009-08-01
 0	val_0	2009-08-01
 0	val_0	2009-08-01
@@ -1053,12 +1045,12 @@ PREHOOK: query: select * from tmp_insert_test_p where ds= '2009-08-01'
 order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_insert_test_p@ds=2009-08-01
-PREHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/1350476431/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-42_667_8118898466258302182/10000
 POSTHOOK: query: select * from tmp_insert_test_p where ds= '2009-08-01'
 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_insert_test_p@ds=2009-08-01
-POSTHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/1350476431/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-42_667_8118898466258302182/10000
 0	val_1	2009-08-01
 0	val_1	2009-08-01
 0	val_0	2009-08-01
@@ -2059,13 +2051,3 @@ POSTHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/135047643
 98	val_98	2009-08-01
 98	val_98	2009-08-01
 99	val_100	2009-08-01
-PREHOOK: query: drop table tmp_insert_test
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp_insert_test
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp_insert_test
-PREHOOK: query: drop table tmp_insert_test_p
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp_insert_test_p
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp_insert_test_p
diff --git a/ql/src/test/results/clientpositive/input41.q.out b/ql/src/test/results/clientpositive/input41.q.out
index ae278de971..834d5a7647 100644
--- a/ql/src/test/results/clientpositive/input41.q.out
+++ b/ql/src/test/results/clientpositive/input41.q.out
@@ -25,17 +25,11 @@ POSTHOOK: Lineage: dest_sp.cnt EXPRESSION [(src)src.null, (srcpart)srcpart.null,
 PREHOOK: query: select * from dest_sp x order by x.cnt limit 2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_sp
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-15-40_207_9006053612393059372/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-55_506_7975559281496695000/10000
 POSTHOOK: query: select * from dest_sp x order by x.cnt limit 2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_sp
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-15-40_207_9006053612393059372/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-28-55_506_7975559281496695000/10000
 POSTHOOK: Lineage: dest_sp.cnt EXPRESSION [(src)src.null, (srcpart)srcpart.null, ]
 0
 500
-PREHOOK: query: drop table dest_sp
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_sp
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_sp
-POSTHOOK: Lineage: dest_sp.cnt EXPRESSION [(src)src.null, (srcpart)srcpart.null, ]
diff --git a/ql/src/test/results/clientpositive/input4_cb_delim.q.out b/ql/src/test/results/clientpositive/input4_cb_delim.q.out
index f20745c450..ea1edd4670 100644
--- a/ql/src/test/results/clientpositive/input4_cb_delim.q.out
+++ b/ql/src/test/results/clientpositive/input4_cb_delim.q.out
@@ -11,11 +11,11 @@ POSTHOOK: Output: default@input4_cb
 PREHOOK: query: SELECT INPUT4_CB.VALUE, INPUT4_CB.KEY FROM INPUT4_CB
 PREHOOK: type: QUERY
 PREHOOK: Input: default@input4_cb
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/767747881/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-08_139_1251320511804274707/10000
 POSTHOOK: query: SELECT INPUT4_CB.VALUE, INPUT4_CB.KEY FROM INPUT4_CB
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@input4_cb
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/767747881/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-08_139_1251320511804274707/10000
 val_238	238
 val_86	86
 val_311	311
@@ -516,8 +516,3 @@ val_403	403
 val_400	400
 val_200	200
 val_97	97
-PREHOOK: query: DROP TABLE INPUT4_CB
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUT4_CB
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@input4_cb
diff --git a/ql/src/test/results/clientpositive/input_columnarserde.q.out b/ql/src/test/results/clientpositive/input_columnarserde.q.out
index e16fa5d898..1a03212cae 100644
--- a/ql/src/test/results/clientpositive/input_columnarserde.q.out
+++ b/ql/src/test/results/clientpositive/input_columnarserde.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table input_columnarserde
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table input_columnarserde
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE input_columnarserde(a array<int>, b array<string>, c map<string,string>, d int, e string)
 ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
@@ -109,11 +105,11 @@ POSTHOOK: Lineage: input_columnarserde.e SIMPLE [(src_thrift)src_thrift.FieldSch
 PREHOOK: query: SELECT input_columnarserde.* FROM input_columnarserde DISTRIBUTE BY 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@input_columnarserde
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-13-41_460_8307039355218827807/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-33_249_4936798916691606497/10000
 POSTHOOK: query: SELECT input_columnarserde.* FROM input_columnarserde DISTRIBUTE BY 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@input_columnarserde
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-13-41_460_8307039355218827807/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-33_249_4936798916691606497/10000
 POSTHOOK: Lineage: input_columnarserde.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: input_columnarserde.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
 POSTHOOK: Lineage: input_columnarserde.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -133,11 +129,11 @@ null	null	null	0	NULL
 PREHOOK: query: SELECT input_columnarserde.a[0], input_columnarserde.b[0], input_columnarserde.c['key2'], input_columnarserde.d, input_columnarserde.e FROM input_columnarserde DISTRIBUTE BY 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@input_columnarserde
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-13-45_834_5356308050640112043/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-35_826_902275362669356666/10000
 POSTHOOK: query: SELECT input_columnarserde.a[0], input_columnarserde.b[0], input_columnarserde.c['key2'], input_columnarserde.d, input_columnarserde.e FROM input_columnarserde DISTRIBUTE BY 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@input_columnarserde
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-13-45_834_5356308050640112043/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-35_826_902275362669356666/10000
 POSTHOOK: Lineage: input_columnarserde.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: input_columnarserde.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
 POSTHOOK: Lineage: input_columnarserde.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -154,13 +150,3 @@ POSTHOOK: Lineage: input_columnarserde.e SIMPLE [(src_thrift)src_thrift.FieldSch
 8	80	NULL	1638581578	record_8
 9	90	NULL	336964413	record_9
 NULL	NULL	NULL	0	NULL
-PREHOOK: query: drop table input_columnarserde
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table input_columnarserde
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@input_columnarserde
-POSTHOOK: Lineage: input_columnarserde.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
-POSTHOOK: Lineage: input_columnarserde.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
-POSTHOOK: Lineage: input_columnarserde.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
-POSTHOOK: Lineage: input_columnarserde.d SIMPLE [(src_thrift)src_thrift.FieldSchema(name:aint, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: input_columnarserde.e SIMPLE [(src_thrift)src_thrift.FieldSchema(name:astring, type:string, comment:from deserializer), ]
diff --git a/ql/src/test/results/clientpositive/input_lazyserde.q.out b/ql/src/test/results/clientpositive/input_lazyserde.q.out
index 4a86f2a465..b7f6d8e291 100644
--- a/ql/src/test/results/clientpositive/input_lazyserde.q.out
+++ b/ql/src/test/results/clientpositive/input_lazyserde.q.out
@@ -107,11 +107,11 @@ POSTHOOK: Lineage: dest1.e SIMPLE [(src_thrift)src_thrift.FieldSchema(name:astri
 PREHOOK: query: SELECT dest1.* FROM dest1 DISTRIBUTE BY 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-15_939_1440433246542522104/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-47_206_1738690132485609113/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1 DISTRIBUTE BY 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-15_939_1440433246542522104/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-47_206_1738690132485609113/10000
 POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: dest1.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
 POSTHOOK: Lineage: dest1.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -131,11 +131,11 @@ null	null	null	0	NULL
 PREHOOK: query: SELECT dest1.a[0], dest1.b[0], dest1.c['key2'], dest1.d, dest1.e FROM dest1 DISTRIBUTE BY 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-20_841_920219133158979282/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-49_829_7164503488670177874/10000
 POSTHOOK: query: SELECT dest1.a[0], dest1.b[0], dest1.c['key2'], dest1.d, dest1.e FROM dest1 DISTRIBUTE BY 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-20_841_920219133158979282/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-49_829_7164503488670177874/10000
 POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: dest1.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
 POSTHOOK: Lineage: dest1.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -189,11 +189,11 @@ POSTHOOK: Lineage: dest1.e SIMPLE [(src_thrift)src_thrift.FieldSchema(name:astri
 PREHOOK: query: SELECT * from dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-29_834_2453314592950577513/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-54_936_8554551256357567949/10000
 POSTHOOK: query: SELECT * from dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-29_834_2453314592950577513/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-54_936_8554551256357567949/10000
 POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: dest1.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
@@ -251,11 +251,11 @@ POSTHOOK: Lineage: dest1.e SIMPLE [(src_thrift)src_thrift.FieldSchema(name:astri
 PREHOOK: query: SELECT * from dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-33_807_4626246450052288822/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-57_561_6501047538497175840/10000
 POSTHOOK: query: SELECT * from dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-33_807_4626246450052288822/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-29-57_561_6501047538497175840/10000
 POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
 POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
@@ -274,15 +274,3 @@ POSTHOOK: Lineage: dest1.e SIMPLE [(src_thrift)src_thrift.FieldSchema(name:astri
 {"key_8":"value_8"}
 {"key_9":"value_9"}
 null
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
-POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lint, type:array<int>, comment:from deserializer), ]
-POSTHOOK: Lineage: dest1.a SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
-POSTHOOK: Lineage: dest1.b SIMPLE [(src_thrift)src_thrift.FieldSchema(name:lstring, type:array<string>, comment:from deserializer), ]
-POSTHOOK: Lineage: dest1.c SIMPLE [(src_thrift)src_thrift.FieldSchema(name:mstringstring, type:map<string,string>, comment:from deserializer), ]
-POSTHOOK: Lineage: dest1.d SIMPLE [(src_thrift)src_thrift.FieldSchema(name:aint, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: dest1.e SIMPLE [(src_thrift)src_thrift.FieldSchema(name:astring, type:string, comment:from deserializer), ]
diff --git a/ql/src/test/results/clientpositive/input_part10.q.out b/ql/src/test/results/clientpositive/input_part10.q.out
index 63b7145037..46c5e80e29 100644
--- a/ql/src/test/results/clientpositive/input_part10.q.out
+++ b/ql/src/test/results/clientpositive/input_part10.q.out
@@ -102,22 +102,15 @@ b	string
 ds	string	
 ts	string	
 	 	 
-Detailed Partition Information	Partition(values:[2008 04 08, 10:11:12=455], dbName:default, tableName:part_special, createTime:1270516600, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/part_special/ds=2008 04 08/ts=10%3A11%3A12%3D455, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1270516600})	
+Detailed Partition Information	Partition(values:[2008 04 08, 10:11:12=455], dbName:default, tableName:part_special, createTime:1279737004, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:a, type:string, comment:null), FieldSchema(name:b, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/part_special/ds=2008 04 08/ts=10%3A11%3A12%3D455, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1279737004})	
 PREHOOK: query: SELECT * FROM part_special WHERE ds='2008 04 08' AND ts = '10:11:12=455'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@part_special@ds=2008 04 08/ts=10%3A11%3A12%3D455
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-40_448_5970205880764458638/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-04_674_1957557010167845537/10000
 POSTHOOK: query: SELECT * FROM part_special WHERE ds='2008 04 08' AND ts = '10:11:12=455'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@part_special@ds=2008 04 08/ts=10%3A11%3A12%3D455
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-16-40_448_5970205880764458638/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-04_674_1957557010167845537/10000
 POSTHOOK: Lineage: part_special PARTITION(ds=2008 04 08,ts=10:11:12=455).a SIMPLE []
 POSTHOOK: Lineage: part_special PARTITION(ds=2008 04 08,ts=10:11:12=455).b SIMPLE []
 1	2	2008 04 08	10:11:12=455
-PREHOOK: query: DROP TABLE part_special
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE part_special
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@part_special
-POSTHOOK: Lineage: part_special PARTITION(ds=2008 04 08,ts=10:11:12=455).a SIMPLE []
-POSTHOOK: Lineage: part_special PARTITION(ds=2008 04 08,ts=10:11:12=455).b SIMPLE []
diff --git a/ql/src/test/results/clientpositive/input_part2.q.out b/ql/src/test/results/clientpositive/input_part2.q.out
index 88826eeb9c..e614646f2a 100644
--- a/ql/src/test/results/clientpositive/input_part2.q.out
+++ b/ql/src/test/results/clientpositive/input_part2.q.out
@@ -69,7 +69,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10004
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10004
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -80,12 +80,12 @@ STAGE PLANS:
                           columns.types int:string:string:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest1
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                           name dest1
                           serialization.ddl struct dest1 { i32 key, string value, string hr, string ds}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1270516376
+                          transient_lastDdlTime 1279737005
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: dest1
                     TotalFiles: 1
@@ -120,7 +120,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 2
-                    directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10005
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10005
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -131,22 +131,22 @@ STAGE PLANS:
                           columns.types int:string:string:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest2
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest2
                           name dest2
                           serialization.ddl struct dest2 { i32 key, string value, string hr, string ds}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1270516376
+                          transient_lastDdlTime 1279737005
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: dest2
                     TotalFiles: 1
                     MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 [srcpart]
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 [srcpart]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 [srcpart]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 [srcpart]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
           Partition
             base file name: hr=12
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -160,13 +160,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516374
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -177,17 +177,17 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516374
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
           Partition
             base file name: hr=12
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -201,13 +201,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516374
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -218,13 +218,13 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516374
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
@@ -236,14 +236,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10004
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10004
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -253,20 +253,20 @@ STAGE PLANS:
                 columns.types int:string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                 name dest1
                 serialization.ddl struct dest1 { i32 key, string value, string hr, string ds}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516376
+                transient_lastDdlTime 1279737005
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10001
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10004 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10004 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -284,9 +284,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10004 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10004]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10004 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10004]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10004 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10004 
           Partition
             base file name: 10004
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -297,12 +297,12 @@ STAGE PLANS:
               columns.types int:string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest1
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
               name dest1
               serialization.ddl struct dest1 { i32 key, string value, string hr, string ds}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516376
+              transient_lastDdlTime 1279737005
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -313,12 +313,12 @@ STAGE PLANS:
                 columns.types int:string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                 name dest1
                 serialization.ddl struct dest1 { i32 key, string value, string hr, string ds}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516376
+                transient_lastDdlTime 1279737005
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest1
             name: dest1
@@ -327,7 +327,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -338,12 +338,12 @@ STAGE PLANS:
                   columns.types int:string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest1
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                   name dest1
                   serialization.ddl struct dest1 { i32 key, string value, string hr, string ds}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270516376
+                  transient_lastDdlTime 1279737005
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest1
             TotalFiles: 1
@@ -356,14 +356,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10005
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10002
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10005
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10002
 
   Stage: Stage-1
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10002
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10002
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -373,20 +373,20 @@ STAGE PLANS:
                 columns.types int:string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest2
                 name dest2
                 serialization.ddl struct dest2 { i32 key, string value, string hr, string ds}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516376
+                transient_lastDdlTime 1279737005
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest2
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10003
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10003
 
   Stage: Stage-6
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10005 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10005 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -404,9 +404,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10005 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10005]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10005 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10005]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10005 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10005 
           Partition
             base file name: 10005
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -417,12 +417,12 @@ STAGE PLANS:
               columns.types int:string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest2
               name dest2
               serialization.ddl struct dest2 { i32 key, string value, string hr, string ds}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516376
+              transient_lastDdlTime 1279737005
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -433,12 +433,12 @@ STAGE PLANS:
                 columns.types int:string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest2
                 name dest2
                 serialization.ddl struct dest2 { i32 key, string value, string hr, string ds}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516376
+                transient_lastDdlTime 1279737005
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest2
             name: dest2
@@ -447,7 +447,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-12-56_344_4511101399410096223/10002
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-05_098_8127089578831075147/10002
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -458,12 +458,12 @@ STAGE PLANS:
                   columns.types int:string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest2
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest2
                   name dest2
                   serialization.ddl struct dest2 { i32 key, string value, string hr, string ds}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270516376
+                  transient_lastDdlTime 1279737005
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest2
             TotalFiles: 1
@@ -497,11 +497,11 @@ POSTHOOK: Lineage: dest2.value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, typ
 PREHOOK: query: SELECT dest1.* FROM dest1 sort by key,value,ds,hr
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-13-14_720_9039761864198236527/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-12_828_6857029233330905446/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1 sort by key,value,ds,hr
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-13-14_720_9039761864198236527/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-12_828_6857029233330905446/10000
 POSTHOOK: Lineage: dest1.ds SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.hr SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.key EXPRESSION [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -597,11 +597,11 @@ POSTHOOK: Lineage: dest2.value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, typ
 PREHOOK: query: SELECT dest2.* FROM dest2 sort by key,value,ds,hr
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-13-21_979_5910588511102117456/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-15_409_5600979597397235313/10000
 POSTHOOK: query: SELECT dest2.* FROM dest2 sort by key,value,ds,hr
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-13-21_979_5910588511102117456/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-15_409_5600979597397235313/10000
 POSTHOOK: Lineage: dest1.ds SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.hr SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.key EXPRESSION [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -694,16 +694,3 @@ POSTHOOK: Lineage: dest2.value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, typ
 97	val_97	12	2008-04-09
 98	val_98	12	2008-04-09
 98	val_98	12	2008-04-09
-PREHOOK: query: drop table dest2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1.ds SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.hr SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: dest1.value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: dest2.ds SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.hr SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: dest2.value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/input_part5.q.out b/ql/src/test/results/clientpositive/input_part5.q.out
index 609ecc09a5..451ea3f703 100644
--- a/ql/src/test/results/clientpositive/input_part5.q.out
+++ b/ql/src/test/results/clientpositive/input_part5.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmptable
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table tmptable(key string, value string, hr string, ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tmptable(key string, value string, hr string, ds string)
@@ -67,7 +63,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-13-28_547_1265657918980229703/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-18_979_6381759242501013340/10000
 
   Stage: Stage-0
     Move Operator
@@ -82,7 +78,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-13-28_547_1265657918980229703/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-30-18_979_6381759242501013340/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -129,11 +125,11 @@ POSTHOOK: Lineage: tmptable.value SIMPLE [(srcpart)x.FieldSchema(name:hr, type:s
 PREHOOK: query: select * from tmptable x sort by x.key,x.value,x.ds,x.hr
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmptable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-13-36_369_7129520619490807626/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-24_165_4575568288735476319/10000
 POSTHOOK: query: select * from tmptable x sort by x.key,x.value,x.ds,x.hr
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmptable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-13-36_369_7129520619490807626/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-24_165_4575568288735476319/10000
 POSTHOOK: Lineage: tmptable.ds SIMPLE [(srcpart)x.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: tmptable.hr SIMPLE [(srcpart)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmptable.key SIMPLE [(srcpart)x.FieldSchema(name:ds, type:string, comment:null), ]
@@ -306,12 +302,3 @@ POSTHOOK: Lineage: tmptable.value SIMPLE [(srcpart)x.FieldSchema(name:hr, type:s
 98	val_98	2008-04-08	11
 98	val_98	2008-04-08	12
 98	val_98	2008-04-08	12
-PREHOOK: query: drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmptable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmptable
-POSTHOOK: Lineage: tmptable.ds SIMPLE [(srcpart)x.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: tmptable.hr SIMPLE [(srcpart)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmptable.key SIMPLE [(srcpart)x.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: tmptable.value SIMPLE [(srcpart)x.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/inputddl1.q.out b/ql/src/test/results/clientpositive/inputddl1.q.out
index 89f84a14a1..f89022af35 100644
--- a/ql/src/test/results/clientpositive/inputddl1.q.out
+++ b/ql/src/test/results/clientpositive/inputddl1.q.out
@@ -31,13 +31,8 @@ POSTHOOK: Output: default@INPUTDDL1
 PREHOOK: query: SELECT INPUTDDL1.* from INPUTDDL1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@inputddl1
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1810212600/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-53_062_1897784041995229554/10000
 POSTHOOK: query: SELECT INPUTDDL1.* from INPUTDDL1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@inputddl1
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1810212600/10000
-PREHOOK: query: DROP TABLE INPUTDDL1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUTDDL1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@inputddl1
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-53_062_1897784041995229554/10000
diff --git a/ql/src/test/results/clientpositive/inputddl2.q.out b/ql/src/test/results/clientpositive/inputddl2.q.out
index 64273a7fd2..6e7aca0eef 100644
--- a/ql/src/test/results/clientpositive/inputddl2.q.out
+++ b/ql/src/test/results/clientpositive/inputddl2.q.out
@@ -37,8 +37,3 @@ key	int
 value	string	
 ds	string	
 country	string	
-PREHOOK: query: DROP TABLE INPUTDDL2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUTDDL2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@inputddl2
diff --git a/ql/src/test/results/clientpositive/inputddl3.q.out b/ql/src/test/results/clientpositive/inputddl3.q.out
index 539a82c411..c811a1baf0 100644
--- a/ql/src/test/results/clientpositive/inputddl3.q.out
+++ b/ql/src/test/results/clientpositive/inputddl3.q.out
@@ -35,8 +35,3 @@ POSTHOOK: query: DESCRIBE INPUTDDL3
 POSTHOOK: type: DESCTABLE
 key	int	
 value	string	
-PREHOOK: query: DROP TABLE INPUTDDL3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUTDDL3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@inputddl3
diff --git a/ql/src/test/results/clientpositive/inputddl4.q.out b/ql/src/test/results/clientpositive/inputddl4.q.out
index a35f655599..bf5970fb09 100644
--- a/ql/src/test/results/clientpositive/inputddl4.q.out
+++ b/ql/src/test/results/clientpositive/inputddl4.q.out
@@ -1,10 +1,6 @@
 PREHOOK: query: -- a simple test to test sorted/clustered syntax
-DROP TABLE INPUTDDL4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: -- a simple test to test sorted/clustered syntax
-DROP TABLE INPUTDDL4
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
+
+CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
                        page_url STRING, referrer_url STRING, 
                        friends ARRAY<BIGINT>, properties MAP<STRING, STRING>,
                        ip STRING COMMENT 'IP Address of the User') 
@@ -12,7 +8,9 @@ PREHOOK: query: CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
     PARTITIONED BY(ds STRING, country STRING) 
     CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS
 PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
+POSTHOOK: query: -- a simple test to test sorted/clustered syntax
+
+CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
                        page_url STRING, referrer_url STRING, 
                        friends ARRAY<BIGINT>, properties MAP<STRING, STRING>,
                        ip STRING COMMENT 'IP Address of the User') 
@@ -48,9 +46,4 @@ ip	string	IP Address of the User
 ds	string	
 country	string	
 	 	 
-Detailed Table Information	Table(tableName:inputddl4, dbName:default, owner:pyang, createTime:1264208851, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:viewtime, type:string, comment:null), FieldSchema(name:userid, type:int, comment:null), FieldSchema(name:page_url, type:string, comment:null), FieldSchema(name:referrer_url, type:string, comment:null), FieldSchema(name:friends, type:array<bigint>, comment:null), FieldSchema(name:properties, type:map<string,string>, comment:null), FieldSchema(name:ip, type:string, comment:IP Address of the User)], location:file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/inputddl4, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:32, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[userid], sortCols:[Order(col:viewtime, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], parameters:{transient_lastDdlTime=1264208851,comment=This is the page view table}, viewOriginalText:null, viewExpandedText:null)	
-PREHOOK: query: DROP TABLE INPUTDDL4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUTDDL4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@inputddl4
+Detailed Table Information	Table(tableName:inputddl4, dbName:default, owner:jssarma, createTime:1279737054, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:viewtime, type:string, comment:null), FieldSchema(name:userid, type:int, comment:null), FieldSchema(name:page_url, type:string, comment:null), FieldSchema(name:referrer_url, type:string, comment:null), FieldSchema(name:friends, type:array<bigint>, comment:null), FieldSchema(name:properties, type:map<string,string>, comment:null), FieldSchema(name:ip, type:string, comment:IP Address of the User)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/inputddl4, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:32, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[userid], sortCols:[Order(col:viewtime, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], parameters:{transient_lastDdlTime=1279737054, comment=This is the page view table}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
diff --git a/ql/src/test/results/clientpositive/inputddl5.q.out b/ql/src/test/results/clientpositive/inputddl5.q.out
index 789bde724f..7bd7e24943 100644
--- a/ql/src/test/results/clientpositive/inputddl5.q.out
+++ b/ql/src/test/results/clientpositive/inputddl5.q.out
@@ -20,23 +20,18 @@ name	string
 PREHOOK: query: SELECT INPUTDDL5.name from INPUTDDL5
 PREHOOK: type: QUERY
 PREHOOK: Input: default@inputddl5
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1156409019/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-54_835_1744174828053981951/10000
 POSTHOOK: query: SELECT INPUTDDL5.name from INPUTDDL5
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@inputddl5
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1156409019/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-54_835_1744174828053981951/10000
 
 PREHOOK: query: SELECT count(1) FROM INPUTDDL5 WHERE INPUTDDL5.name = _UTF-8 0xE982B5E993AE
 PREHOOK: type: QUERY
 PREHOOK: Input: default@inputddl5
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/609959614/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-57_281_5492145936569181144/10000
 POSTHOOK: query: SELECT count(1) FROM INPUTDDL5 WHERE INPUTDDL5.name = _UTF-8 0xE982B5E993AE
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@inputddl5
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/609959614/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-30-57_281_5492145936569181144/10000
 1
-PREHOOK: query: DROP TABLE INPUTDDL5
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUTDDL5
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@inputddl5
diff --git a/ql/src/test/results/clientpositive/inputddl6.q.out b/ql/src/test/results/clientpositive/inputddl6.q.out
index 5a89a3d73d..2c626897c1 100644
--- a/ql/src/test/results/clientpositive/inputddl6.q.out
+++ b/ql/src/test/results/clientpositive/inputddl6.q.out
@@ -1,16 +1,12 @@
 PREHOOK: query: -- test for describe extended table
 -- test for describe extended table partition
 -- test for alter table drop partition
-DROP TABLE INPUTDDL6
-PREHOOK: type: DROPTABLE
+CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
 POSTHOOK: query: -- test for describe extended table
 -- test for describe extended table partition
 -- test for alter table drop partition
-DROP TABLE INPUTDDL6
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
+CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@INPUTDDL6
 PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE INPUTDDL6 PARTITION (ds='2008-04-09')
@@ -31,7 +27,7 @@ key	string
 value	string	
 ds	string	
 	 	 
-Detailed Table Information	Table(tableName:inputddl6, dbName:default, owner:pyang, createTime:1264209075, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/inputddl6, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transient_lastDdlTime=1264209075}, viewOriginalText:null, viewExpandedText:null)	
+Detailed Table Information	Table(tableName:inputddl6, dbName:default, owner:jssarma, createTime:1279737060, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/inputddl6, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transient_lastDdlTime=1279737060}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: DESCRIBE EXTENDED INPUTDDL6 PARTITION (ds='2008-04-08')
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: DESCRIBE EXTENDED INPUTDDL6 PARTITION (ds='2008-04-08')
@@ -40,7 +36,7 @@ key	string
 value	string	
 ds	string	
 	 	 
-Detailed Partition Information	Partition(values:[2008-04-08], dbName:default, tableName:inputddl6, createTime:1264209075, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/inputddl6/ds=2008-04-08, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1264209075})	
+Detailed Partition Information	Partition(values:[2008-04-08], dbName:default, tableName:inputddl6, createTime:1279737060, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/inputddl6/ds=2008-04-08, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1279737060})	
 PREHOOK: query: SHOW PARTITIONS INPUTDDL6
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: SHOW PARTITIONS INPUTDDL6
diff --git a/ql/src/test/results/clientpositive/inputddl7.q.out b/ql/src/test/results/clientpositive/inputddl7.q.out
index f485b3d8cd..f130aa255e 100644
--- a/ql/src/test/results/clientpositive/inputddl7.q.out
+++ b/ql/src/test/results/clientpositive/inputddl7.q.out
@@ -1,16 +1,14 @@
 PREHOOK: query: -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE T1
-PREHOOK: type: DROPTABLE
+
+CREATE TABLE T1(name STRING) STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
 POSTHOOK: query: -- test for loading into tables with the correct file format
 -- test for loading into partitions with the correct file format
 
-DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE T1(name STRING) STORED AS TEXTFILE
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE T1(name STRING) STORED AS TEXTFILE
+
+CREATE TABLE T1(name STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@T1
 PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE T1
@@ -21,16 +19,12 @@ POSTHOOK: Output: default@t1
 PREHOOK: query: SELECT COUNT(1) FROM T1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/61579512/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-01_263_7107969806665063947/10000
 POSTHOOK: query: SELECT COUNT(1) FROM T1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/61579512/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-01_263_7107969806665063947/10000
 500
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T2(name STRING) STORED AS SEQUENCEFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T2(name STRING) STORED AS SEQUENCEFILE
@@ -44,16 +38,12 @@ POSTHOOK: Output: default@t2
 PREHOOK: query: SELECT COUNT(1) FROM T2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1996309805/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-03_852_7925766013771881851/10000
 POSTHOOK: query: SELECT COUNT(1) FROM T2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1996309805/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-03_852_7925766013771881851/10000
 500
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T3(name STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T3(name STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
@@ -67,16 +57,12 @@ POSTHOOK: Output: default@t3@ds=2008-04-09
 PREHOOK: query: SELECT COUNT(1) FROM T3 where T3.ds='2008-04-09'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t3@ds=2008-04-09
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/440386486/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-06_508_6701973609385610242/10000
 POSTHOOK: query: SELECT COUNT(1) FROM T3 where T3.ds='2008-04-09'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t3@ds=2008-04-09
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/440386486/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-06_508_6701973609385610242/10000
 500
-PREHOOK: query: DROP TABLE T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T4
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T4(name STRING) PARTITIONED BY(ds STRING) STORED AS SEQUENCEFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T4(name STRING) PARTITIONED BY(ds STRING) STORED AS SEQUENCEFILE
@@ -90,11 +76,11 @@ POSTHOOK: Output: default@t4@ds=2008-04-09
 PREHOOK: query: SELECT COUNT(1) FROM T4 where T4.ds='2008-04-09'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t4@ds=2008-04-09
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1615485265/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-09_249_5639659187396085133/10000
 POSTHOOK: query: SELECT COUNT(1) FROM T4 where T4.ds='2008-04-09'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t4@ds=2008-04-09
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1615485265/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-09_249_5639659187396085133/10000
 500
 PREHOOK: query: DESCRIBE EXTENDED T1
 PREHOOK: type: DESCTABLE
@@ -102,14 +88,14 @@ POSTHOOK: query: DESCRIBE EXTENDED T1
 POSTHOOK: type: DESCTABLE
 name	string	
 	 	 
-Detailed Table Information	Table(tableName:t1, dbName:default, owner:njain, createTime:1253780757, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
+Detailed Table Information	Table(tableName:t1, dbName:default, owner:jssarma, createTime:1279737061, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279737061}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: DESCRIBE EXTENDED T2
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: DESCRIBE EXTENDED T2
 POSTHOOK: type: DESCTABLE
 name	string	
 	 	 
-Detailed Table Information	Table(tableName:t2, dbName:default, owner:njain, createTime:1253780761, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/t2, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{})	
+Detailed Table Information	Table(tableName:t2, dbName:default, owner:jssarma, createTime:1279737063, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t2, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279737063}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: DESCRIBE EXTENDED T3 PARTITION (ds='2008-04-09')
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: DESCRIBE EXTENDED T3 PARTITION (ds='2008-04-09')
@@ -117,7 +103,7 @@ POSTHOOK: type: DESCTABLE
 name	string	
 ds	string	
 	 	 
-Detailed Partition Information	Partition(values:[2008-04-09], dbName:default, tableName:t3, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/t3/ds=2008-04-09, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{})	
+Detailed Partition Information	Partition(values:[2008-04-09], dbName:default, tableName:t3, createTime:1279737066, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t3/ds=2008-04-09, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1279737066})	
 PREHOOK: query: DESCRIBE EXTENDED T4 PARTITION (ds='2008-04-09')
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: DESCRIBE EXTENDED T4 PARTITION (ds='2008-04-09')
@@ -125,24 +111,4 @@ POSTHOOK: type: DESCTABLE
 name	string	
 ds	string	
 	 	 
-Detailed Partition Information	Partition(values:[2008-04-09], dbName:default, tableName:t4, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/t4/ds=2008-04-09, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{})	
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t3
-PREHOOK: query: DROP TABLE T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t4
+Detailed Partition Information	Partition(values:[2008-04-09], dbName:default, tableName:t4, createTime:1279737069, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/t4/ds=2008-04-09, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1279737069})	
diff --git a/ql/src/test/results/clientpositive/inputddl8.q.out b/ql/src/test/results/clientpositive/inputddl8.q.out
index f8ad16cb8a..f54bf1eae6 100644
--- a/ql/src/test/results/clientpositive/inputddl8.q.out
+++ b/ql/src/test/results/clientpositive/inputddl8.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE INPUTDDL8
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUTDDL8
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE INPUTDDL8 COMMENT 'This is a thrift based table'
     PARTITIONED BY(ds STRING, country STRING)
     CLUSTERED BY(aint) SORTED BY(lint) INTO 32 BUCKETS
@@ -32,9 +28,4 @@ mstringstring	map<string,string>	from deserializer
 ds	string	
 country	string	
 	 	 
-Detailed Table Information	Table(tableName:inputddl8, dbName:default, owner:pyang, createTime:1264209638, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[], location:file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/inputddl8, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:32, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.Complex,serialization.format=com.facebook.thrift.protocol.TBinaryProtocol}), bucketCols:[aint], sortCols:[Order(col:lint, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], parameters:{transient_lastDdlTime=1264209638,comment=This is a thrift based table}, viewOriginalText:null, viewExpandedText:null)	
-PREHOOK: query: DROP TABLE INPUTDDL8
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE INPUTDDL8
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@inputddl8
+Detailed Table Information	Table(tableName:inputddl8, dbName:default, owner:jssarma, createTime:1279737072, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/inputddl8, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:32, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.Complex, serialization.format=com.facebook.thrift.protocol.TBinaryProtocol}), bucketCols:[aint], sortCols:[Order(col:lint, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], parameters:{transient_lastDdlTime=1279737072, comment=This is a thrift based table}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
diff --git a/ql/src/test/results/clientpositive/insert1.q.out b/ql/src/test/results/clientpositive/insert1.q.out
index 800b587bcc..8b907bfb14 100644
--- a/ql/src/test/results/clientpositive/insert1.q.out
+++ b/ql/src/test/results/clientpositive/insert1.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table insert1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table insert1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table insert2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table insert2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table insert1(key int, value string) stored as textfile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table insert1(key int, value string) stored as textfile
@@ -26,17 +18,3 @@ POSTHOOK: Input: default@insert2
 POSTHOOK: Output: default@insert1
 POSTHOOK: Lineage: insert1.key SIMPLE [(insert2)a.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: insert1.value SIMPLE [(insert2)a.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table insert1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table insert1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@insert1
-POSTHOOK: Lineage: insert1.key SIMPLE [(insert2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: insert1.value SIMPLE [(insert2)a.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table insert2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table insert2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@insert2
-POSTHOOK: Lineage: insert1.key SIMPLE [(insert2)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: insert1.value SIMPLE [(insert2)a.FieldSchema(name:value, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/insertexternal1.q.out b/ql/src/test/results/clientpositive/insertexternal1.q.out
index 5fee9f4265..86f2157072 100644
--- a/ql/src/test/results/clientpositive/insertexternal1.q.out
+++ b/ql/src/test/results/clientpositive/insertexternal1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table texternal
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table texternal
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table texternal(key string, val string) partitioned by (insertdate string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table texternal(key string, val string) partitioned by (insertdate string)
@@ -25,11 +21,11 @@ POSTHOOK: Lineage: texternal PARTITION(insertdate=2008-01-01).val SIMPLE [(src)s
 PREHOOK: query: select * from texternal where insertdate='2008-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@texternal@insertdate=2008-01-01
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-14-31_283_8182934918703262704/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-18_438_8696467469328137226/10000
 POSTHOOK: query: select * from texternal where insertdate='2008-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@texternal@insertdate=2008-01-01
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-14-31_283_8182934918703262704/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-31-18_438_8696467469328137226/10000
 POSTHOOK: Lineage: texternal PARTITION(insertdate=2008-01-01).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: texternal PARTITION(insertdate=2008-01-01).val SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 238	val_238	2008-01-01
diff --git a/ql/src/test/results/clientpositive/join19.q.out b/ql/src/test/results/clientpositive/join19.q.out
index 93b86c61fa..823f5d8ef8 100644
--- a/ql/src/test/results/clientpositive/join19.q.out
+++ b/ql/src/test/results/clientpositive/join19.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop TABLE triples
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop TABLE triples
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE triples (foo string, subject string, predicate string, object string, foo2 string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE triples (foo string, subject string, predicate string, object string, foo2 string)
@@ -417,8 +413,3 @@ STAGE PLANS:
       limit: -1
 
 
-PREHOOK: query: drop TABLE triples
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop TABLE triples
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@triples
diff --git a/ql/src/test/results/clientpositive/join24.q.out b/ql/src/test/results/clientpositive/join24.q.out
index 7b0bd801cc..83ce328fa8 100644
--- a/ql/src/test/results/clientpositive/join24.q.out
+++ b/ql/src/test/results/clientpositive/join24.q.out
@@ -18,18 +18,11 @@ POSTHOOK: Lineage: tst1.key SIMPLE [(src)a.FieldSchema(name:key, type:string, co
 PREHOOK: query: SELECT sum(a.cnt)  FROM tst1 a JOIN tst1 b ON a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tst1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-18-43_917_116815481322601521/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-31_747_8781349854837521584/10000
 POSTHOOK: query: SELECT sum(a.cnt)  FROM tst1 a JOIN tst1 b ON a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tst1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-18-43_917_116815481322601521/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-31_747_8781349854837521584/10000
 POSTHOOK: Lineage: tst1.cnt EXPRESSION [(src)a.null, ]
 POSTHOOK: Lineage: tst1.key SIMPLE [(src)a.FieldSchema(name:key, type:string, comment:default), ]
 500
-PREHOOK: query: drop table tst1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tst1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tst1
-POSTHOOK: Lineage: tst1.cnt EXPRESSION [(src)a.null, ]
-POSTHOOK: Lineage: tst1.key SIMPLE [(src)a.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join25.q.out b/ql/src/test/results/clientpositive/join25.q.out
index 00a26bda45..564c66715e 100644
--- a/ql/src/test/results/clientpositive/join25.q.out
+++ b/ql/src/test/results/clientpositive/join25.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE
@@ -146,7 +142,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-14-59_106_8844394843747408854/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-37_079_7805207249179113033/10000
 
   Stage: Stage-0
     Move Operator
@@ -161,7 +157,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-14-59_106_8844394843747408854/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-37_079_7805207249179113033/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -207,11 +203,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-04_626_7067751955955642337/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-39_761_8502467097748439510/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-04_626_7067751955955642337/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-39_761_8502467097748439510/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
@@ -252,11 +248,3 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:st
 406	val_406	val_406
 406	val_406	val_406
 406	val_406	val_406
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join26.q.out b/ql/src/test/results/clientpositive/join26.q.out
index 4b76b429a5..54d5889aad 100644
--- a/ql/src/test/results/clientpositive/join26.q.out
+++ b/ql/src/test/results/clientpositive/join26.q.out
@@ -83,7 +83,7 @@ STAGE PLANS:
                         File Output Operator
                           compressed: false
                           GlobalTableId: 1
-                          directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10002
+                          directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10002
                           NumFilesPerFileSink: 1
                           table:
                               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -94,12 +94,12 @@ STAGE PLANS:
                                 columns.types string:string:string
                                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                                 name dest_j1
                                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                                 serialization.format 1
                                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                transient_lastDdlTime 1270516526
+                                transient_lastDdlTime 1279737162
                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                               name: dest_j1
                           TotalFiles: 1
@@ -153,7 +153,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -164,12 +164,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                               name dest_j1
                               serialization.ddl struct dest_j1 { string key, string value, string val2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270516526
+                              transient_lastDdlTime 1279737162
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: dest_j1
                         TotalFiles: 1
@@ -213,7 +213,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -224,21 +224,21 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                               name dest_j1
                               serialization.ddl struct dest_j1 { string key, string value, string val2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270516526
+                              transient_lastDdlTime 1279737162
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: dest_j1
                         TotalFiles: 1
                         MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [z]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [z]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
             base file name: hr=11
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -252,13 +252,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516525
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -269,13 +269,13 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516525
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
@@ -287,14 +287,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10002
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -304,20 +304,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516526
+                transient_lastDdlTime 1279737162
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -333,9 +333,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10002 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10002]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -346,12 +346,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
               name dest_j1
               serialization.ddl struct dest_j1 { string key, string value, string val2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516526
+              transient_lastDdlTime 1279737162
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -362,12 +362,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516526
+                transient_lastDdlTime 1279737162
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
             name: dest_j1
@@ -376,7 +376,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-27_003_361825154981961236/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-42_624_6168816752113611858/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -387,12 +387,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                   name dest_j1
                   serialization.ddl struct dest_j1 { string key, string value, string val2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270516526
+                  transient_lastDdlTime 1279737162
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest_j1
             TotalFiles: 1
@@ -423,11 +423,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-33_053_7397223625816469001/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-45_481_4410605154645734426/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-33_053_7397223625816469001/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-45_481_4410605154645734426/10000
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
@@ -538,11 +538,3 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:st
 98	val_98	val_98
 98	val_98	val_98
 98	val_98	val_98
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/join27.q.out b/ql/src/test/results/clientpositive/join27.q.out
index 30f8679f3f..fb40e0f134 100644
--- a/ql/src/test/results/clientpositive/join27.q.out
+++ b/ql/src/test/results/clientpositive/join27.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE
@@ -146,7 +142,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-18-55_275_8815718514417289033/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-48_393_5629633206505878334/10000
 
   Stage: Stage-0
     Move Operator
@@ -161,7 +157,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-18-55_275_8815718514417289033/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-48_393_5629633206505878334/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -207,11 +203,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key, x.value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-19-00_972_3538933034681174700/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-51_077_7574097645865695379/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key, x.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-19-00_972_3538933034681174700/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-51_077_7574097645865695379/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
@@ -256,11 +252,3 @@ NULL	val_484	val_484
 406	val_406	val_406
 406	val_406	val_406
 406	val_406	val_406
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join28.q.out b/ql/src/test/results/clientpositive/join28.q.out
index 3be6f8b025..1f81868a62 100644
--- a/ql/src/test/results/clientpositive/join28.q.out
+++ b/ql/src/test/results/clientpositive/join28.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key STRING, value STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key STRING, value STRING) STORED AS TEXTFILE
@@ -222,7 +218,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-11_119_8886693013773508618/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-53_953_3029870996442907172/10000
 
   Stage: Stage-0
     Move Operator
@@ -237,7 +233,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-11_119_8886693013773508618/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-53_953_3029870996442907172/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -288,11 +284,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-16_630_6883273602303197437/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-56_855_199027769742346440/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-16_630_6883273602303197437/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-32-56_855_199027769742346440/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
 128	val_128
@@ -402,10 +398,3 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:st
 98	val_98
 98	val_98
 98	val_98
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/join29.q.out b/ql/src/test/results/clientpositive/join29.q.out
index bb6699f2c9..e61fbb5467 100644
--- a/ql/src/test/results/clientpositive/join29.q.out
+++ b/ql/src/test/results/clientpositive/join29.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop TABLE dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop TABLE dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key STRING, cnt1 INT, cnt2 INT)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key STRING, cnt1 INT, cnt2 INT)
@@ -91,7 +87,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-39_938_6824990071701609235/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-32-59_660_3940646832112023058/10002 
             Common Join Operator
               condition map:
                    Inner Join 0 to 1
@@ -142,11 +138,11 @@ STAGE PLANS:
       Local Work:
         Map Reduce Local Work
           Alias -> Map Local Tables:
-            file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-39_938_6824990071701609235/10004 
+            file:/tmp/jssarma/hive_2010-07-21_11-32-59_660_3940646832112023058/10004 
               Fetch Operator
                 limit: -1
           Alias -> Map Local Operator Tree:
-            file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-39_938_6824990071701609235/10004 
+            file:/tmp/jssarma/hive_2010-07-21_11-32-59_660_3940646832112023058/10004 
                 Common Join Operator
                   condition map:
                        Inner Join 0 to 1
@@ -202,7 +198,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-39_938_6824990071701609235/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-59_660_3940646832112023058/10000
 
   Stage: Stage-0
     Move Operator
@@ -217,7 +213,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-39_938_6824990071701609235/10003 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-32-59_660_3940646832112023058/10003 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -321,11 +317,11 @@ POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-53_522_4492151849840756172/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-08_454_5846290866810953063/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-53_522_4492151849840756172/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-08_454_5846290866810953063/10000
 POSTHOOK: Lineage: dest_j1.cnt1 EXPRESSION [(src1)x.null, ]
 POSTHOOK: Lineage: dest_j1.cnt2 EXPRESSION [(src)y.null, ]
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
@@ -344,11 +340,3 @@ POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string
 406	1	4
 66	1	1
 98	1	2
-PREHOOK: query: drop TABLE dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop TABLE dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.cnt1 EXPRESSION [(src1)x.null, ]
-POSTHOOK: Lineage: dest_j1.cnt2 EXPRESSION [(src)y.null, ]
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join30.q.out b/ql/src/test/results/clientpositive/join30.q.out
index 5e4876b1a1..8eaf6f5d83 100644
--- a/ql/src/test/results/clientpositive/join30.q.out
+++ b/ql/src/test/results/clientpositive/join30.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop TABLE dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop TABLE dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key INT, cnt INT)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key INT, cnt INT)
@@ -80,7 +76,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-19-07_275_5672281842975707829/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-14_350_1200822154670358029/10002 
           Select Operator
             expressions:
                   expr: _col0
@@ -173,11 +169,11 @@ POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-19-17_499_3730497864506101153/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-19_511_4245286174894412683/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-19-17_499_3730497864506101153/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-19_511_4245286174894412683/10000
 POSTHOOK: Lineage: dest_j1.cnt EXPRESSION [(src1)x.null, (src)y.null, ]
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 66	1
@@ -195,10 +191,3 @@ POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:st
 369	3
 401	5
 406	4
-PREHOOK: query: drop TABLE dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop TABLE dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.cnt EXPRESSION [(src1)x.null, (src)y.null, ]
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join31.q.out b/ql/src/test/results/clientpositive/join31.q.out
index 1508f84dfb..183f2048be 100644
--- a/ql/src/test/results/clientpositive/join31.q.out
+++ b/ql/src/test/results/clientpositive/join31.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop TABLE dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop TABLE dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key STRING, cnt INT)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key STRING, cnt INT)
@@ -89,7 +85,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-22_650_521308826200384563/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-22_239_7108281462009864168/10002 
             Common Join Operator
               condition map:
                    Inner Join 0 to 1
@@ -111,11 +107,11 @@ STAGE PLANS:
       Local Work:
         Map Reduce Local Work
           Alias -> Map Local Tables:
-            file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-22_650_521308826200384563/10004 
+            file:/tmp/jssarma/hive_2010-07-21_11-33-22_239_7108281462009864168/10004 
               Fetch Operator
                 limit: -1
           Alias -> Map Local Operator Tree:
-            file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-22_650_521308826200384563/10004 
+            file:/tmp/jssarma/hive_2010-07-21_11-33-22_239_7108281462009864168/10004 
                 Common Join Operator
                   condition map:
                        Inner Join 0 to 1
@@ -138,7 +134,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-22_650_521308826200384563/10003 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-22_239_7108281462009864168/10003 
           Select Operator
             expressions:
                   expr: _col0
@@ -291,11 +287,11 @@ POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-39_838_1536789358563422775/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-32_487_8490983650111837625/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-39_838_1536789358563422775/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-32_487_8490983650111837625/10000
 POSTHOOK: Lineage: dest_j1.cnt EXPRESSION [(src1)x.null, (src)y.null, ]
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 128	1
@@ -313,10 +309,3 @@ POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string
 406	1
 66	1
 98	1
-PREHOOK: query: drop TABLE dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop TABLE dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.cnt EXPRESSION [(src1)x.null, (src)y.null, ]
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join32.q.out b/ql/src/test/results/clientpositive/join32.q.out
index ecb68ba7bf..89246c972e 100644
--- a/ql/src/test/results/clientpositive/join32.q.out
+++ b/ql/src/test/results/clientpositive/join32.q.out
@@ -48,7 +48,7 @@ STAGE PLANS:
               File Output Operator
                 compressed: false
                 GlobalTableId: 0
-                directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10003
+                directory: file:/tmp/jssarma/hive_2010-07-21_11-33-35_233_5606297326105736066/10003
                 NumFilesPerFileSink: 1
                 table:
                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -84,7 +84,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 0
-                    directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10003
+                    directory: file:/tmp/jssarma/hive_2010-07-21_11-33-35_233_5606297326105736066/10003
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -97,9 +97,9 @@ STAGE PLANS:
                     MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src [y]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [y]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -110,12 +110,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516558
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -126,12 +126,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516558
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -139,7 +139,7 @@ STAGE PLANS:
   Stage: Stage-1
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10003 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-35_233_5606297326105736066/10003 
           Select Operator
             expressions:
                   expr: _col0
@@ -182,7 +182,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10002
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -193,12 +193,12 @@ STAGE PLANS:
                           columns.types string:string:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                           name dest_j1
                           serialization.ddl struct dest_j1 { string key, string value, string val2}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1270516559
+                          transient_lastDdlTime 1279737215
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: dest_j1
                     TotalFiles: 1
@@ -261,7 +261,7 @@ STAGE PLANS:
                             File Output Operator
                               compressed: false
                               GlobalTableId: 1
-                              directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10002
+                              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10002
                               NumFilesPerFileSink: 1
                               table:
                                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -272,21 +272,21 @@ STAGE PLANS:
                                     columns.types string:string:string
                                     file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                     file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                    location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                                    location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                                     name dest_j1
                                     serialization.ddl struct dest_j1 { string key, string value, string val2}
                                     serialization.format 1
                                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                    transient_lastDdlTime 1270516559
+                                    transient_lastDdlTime 1279737215
                                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                                   name: dest_j1
                               TotalFiles: 1
                               MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10003 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10003]
+        file:/tmp/jssarma/hive_2010-07-21_11-33-35_233_5606297326105736066/10003 [file:/tmp/jssarma/hive_2010-07-21_11-33-35_233_5606297326105736066/10003]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10003 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-35_233_5606297326105736066/10003 
           Partition
             base file name: 10003
             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -310,14 +310,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10002
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -327,20 +327,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516559
+                transient_lastDdlTime 1279737215
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -356,9 +356,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10002 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10002]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -369,12 +369,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
               name dest_j1
               serialization.ddl struct dest_j1 { string key, string value, string val2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516559
+              transient_lastDdlTime 1279737215
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -385,12 +385,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516559
+                transient_lastDdlTime 1279737215
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
             name: dest_j1
@@ -399,7 +399,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-15-59_030_681842595399428889/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-35_233_5606297326105736066/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -410,12 +410,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                   name dest_j1
                   serialization.ddl struct dest_j1 { string key, string value, string val2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270516559
+                  transient_lastDdlTime 1279737215
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest_j1
             TotalFiles: 1
@@ -446,11 +446,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-07_866_6969133175886922420/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-40_558_8997400592657848578/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-07_866_6969133175886922420/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-40_558_8997400592657848578/10000
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 EXPRESSION [(src)y.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
@@ -539,11 +539,3 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:st
 98	val_98	val_98
 98	val_98	val_98
 98	val_98	val_98
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 EXPRESSION [(src)y.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/join33.q.out b/ql/src/test/results/clientpositive/join33.q.out
index 94f077843b..33af9f83d5 100644
--- a/ql/src/test/results/clientpositive/join33.q.out
+++ b/ql/src/test/results/clientpositive/join33.q.out
@@ -45,7 +45,7 @@ STAGE PLANS:
               File Output Operator
                 compressed: false
                 GlobalTableId: 0
-                directory: file:/tmp/jssarma/hive_2010-06-10_16-21-45_664_3974168394039456921/10002
+                directory: file:/tmp/jssarma/hive_2010-07-21_11-33-43_308_1055825397797894449/10002
                 NumFilesPerFileSink: 1
                 table:
                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -81,7 +81,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 0
-                    directory: file:/tmp/jssarma/hive_2010-06-10_16-21-45_664_3974168394039456921/10002
+                    directory: file:/tmp/jssarma/hive_2010-07-21_11-33-43_308_1055825397797894449/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -112,7 +112,7 @@ STAGE PLANS:
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1276212105
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -128,7 +128,7 @@ STAGE PLANS:
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1276212105
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -136,7 +136,7 @@ STAGE PLANS:
   Stage: Stage-1
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/tmp/jssarma/hive_2010-06-10_16-21-45_664_3974168394039456921/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-43_308_1055825397797894449/10002 
           Select Operator
             expressions:
                   expr: _col0
@@ -193,7 +193,7 @@ STAGE PLANS:
       Needs Tagging: true
       Path -> Alias:
         file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [z]
-        file:/tmp/jssarma/hive_2010-06-10_16-21-45_664_3974168394039456921/10002 [file:/tmp/jssarma/hive_2010-06-10_16-21-45_664_3974168394039456921/10002]
+        file:/tmp/jssarma/hive_2010-07-21_11-33-43_308_1055825397797894449/10002 [file:/tmp/jssarma/hive_2010-07-21_11-33-43_308_1055825397797894449/10002]
       Path -> Partition:
         file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
@@ -215,7 +215,7 @@ STAGE PLANS:
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1276212103
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -232,11 +232,11 @@ STAGE PLANS:
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1276212103
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
-        file:/tmp/jssarma/hive_2010-06-10_16-21-45_664_3974168394039456921/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-43_308_1055825397797894449/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -273,7 +273,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-06-10_16-21-45_664_3974168394039456921/10000
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-43_308_1055825397797894449/10000
               NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -289,7 +289,7 @@ STAGE PLANS:
                     serialization.ddl struct dest_j1 { string key, string value, string val2}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    transient_lastDdlTime 1276212105
+                    transient_lastDdlTime 1279737223
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: dest_j1
               TotalFiles: 1
@@ -299,7 +299,7 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-06-10_16-21-45_664_3974168394039456921/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-43_308_1055825397797894449/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -314,10 +314,10 @@ STAGE PLANS:
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1276212105
+                transient_lastDdlTime 1279737223
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
-          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-06-10_16-21-45_664_3974168394039456921/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-43_308_1055825397797894449/10001
 
 
 PREHOOK: query: INSERT OVERWRITE TABLE dest_j1
@@ -344,11 +344,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/tmp/jssarma/hive_2010-06-10_16-21-51_616_8853310441674539967/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-48_544_7786473350800117933/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/tmp/jssarma/hive_2010-06-10_16-21-51_616_8853310441674539967/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-48_544_7786473350800117933/10000
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 EXPRESSION [(src)y.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
@@ -437,11 +437,3 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:st
 98	val_98	val_98
 98	val_98	val_98
 98	val_98	val_98
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 EXPRESSION [(src)y.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/join34.q.out b/ql/src/test/results/clientpositive/join34.q.out
index e869731165..38b5063947 100644
--- a/ql/src/test/results/clientpositive/join34.q.out
+++ b/ql/src/test/results/clientpositive/join34.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE
@@ -95,7 +91,7 @@ STAGE PLANS:
                           File Output Operator
                             compressed: false
                             GlobalTableId: 1
-                            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10002
+                            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10002
                             NumFilesPerFileSink: 1
                             table:
                                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -106,12 +102,12 @@ STAGE PLANS:
                                   columns.types string:string:string
                                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/dest_j1
+                                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                                   name dest_j1
                                   serialization.ddl struct dest_j1 { string key, string value, string val2}
                                   serialization.format 1
                                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                  transient_lastDdlTime 1270516546
+                                  transient_lastDdlTime 1279737231
                                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                                 name: dest_j1
                             TotalFiles: 1
@@ -170,7 +166,7 @@ STAGE PLANS:
                           File Output Operator
                             compressed: false
                             GlobalTableId: 1
-                            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10002
+                            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10002
                             NumFilesPerFileSink: 1
                             table:
                                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -181,12 +177,12 @@ STAGE PLANS:
                                   columns.types string:string:string
                                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/dest_j1
+                                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                                   name dest_j1
                                   serialization.ddl struct dest_j1 { string key, string value, string val2}
                                   serialization.format 1
                                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                  transient_lastDdlTime 1270516546
+                                  transient_lastDdlTime 1279737231
                                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                                 name: dest_j1
                             TotalFiles: 1
@@ -234,7 +230,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -245,21 +241,21 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/dest_j1
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                               name dest_j1
                               serialization.ddl struct dest_j1 { string key, string value, string val2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270516546
+                              transient_lastDdlTime 1279737231
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: dest_j1
                         TotalFiles: 1
                         MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/src [null-subquery1:subq1-subquery1:x, null-subquery2:subq1-subquery2:x1]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [null-subquery1:subq1-subquery1:x, null-subquery2:subq1-subquery2:x1]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -270,12 +266,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516546
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -286,12 +282,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516546
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -303,14 +299,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10002
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -320,20 +316,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516546
+                transient_lastDdlTime 1279737231
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -349,9 +345,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10002 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10002]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -362,12 +358,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/dest_j1
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
               name dest_j1
               serialization.ddl struct dest_j1 { string key, string value, string val2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516546
+              transient_lastDdlTime 1279737231
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -378,12 +374,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516546
+                transient_lastDdlTime 1279737231
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
             name: dest_j1
@@ -392,7 +388,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-46_856_8198391753816630008/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-51_338_162260050541565435/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -403,12 +399,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/dest_j1
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                   name dest_j1
                   serialization.ddl struct dest_j1 { string key, string value, string val2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270516546
+                  transient_lastDdlTime 1279737231
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest_j1
             TotalFiles: 1
@@ -445,11 +441,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-52_696_7531573484610449997/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-54_152_7377053077102537377/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-52_696_7531573484610449997/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-33-54_152_7377053077102537377/10000
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 EXPRESSION [(src)x.FieldSchema(name:value, type:string, comment:default), (src)x1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
@@ -487,11 +483,3 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:st
 406	val_406	val_406
 406	val_406	val_406
 406	val_406	val_406
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 EXPRESSION [(src)x.FieldSchema(name:value, type:string, comment:default), (src)x1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join35.q.out b/ql/src/test/results/clientpositive/join35.q.out
index 54926d53e0..21449bce96 100644
--- a/ql/src/test/results/clientpositive/join35.q.out
+++ b/ql/src/test/results/clientpositive/join35.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key STRING, value STRING, val2 INT) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key STRING, value STRING, val2 INT) STORED AS TEXTFILE
@@ -84,9 +80,9 @@ STAGE PLANS:
                             type: bigint
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src [null-subquery1:subq1-subquery1:x]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [null-subquery1:subq1-subquery1:x]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -97,12 +93,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516573
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -113,12 +109,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516573
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -142,7 +138,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-              directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10002
+              directory: file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10002
               NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -157,7 +153,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10002 
           Union
             Common Join Operator
               condition map:
@@ -201,7 +197,7 @@ STAGE PLANS:
                     File Output Operator
                       compressed: false
                       GlobalTableId: 1
-                      directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10003
+                      directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10003
                       NumFilesPerFileSink: 1
                       table:
                           input format: org.apache.hadoop.mapred.TextInputFormat
@@ -212,17 +208,17 @@ STAGE PLANS:
                             columns.types string:string:int
                             file.inputformat org.apache.hadoop.mapred.TextInputFormat
                             file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                            location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                             name dest_j1
                             serialization.ddl struct dest_j1 { string key, string value, i32 val2}
                             serialization.format 1
                             serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                            transient_lastDdlTime 1270516573
+                            transient_lastDdlTime 1279737236
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                           name: dest_j1
                       TotalFiles: 1
                       MultiFileSpray: false
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10004 
           Union
             Common Join Operator
               condition map:
@@ -266,7 +262,7 @@ STAGE PLANS:
                     File Output Operator
                       compressed: false
                       GlobalTableId: 1
-                      directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10003
+                      directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10003
                       NumFilesPerFileSink: 1
                       table:
                           input format: org.apache.hadoop.mapred.TextInputFormat
@@ -277,12 +273,12 @@ STAGE PLANS:
                             columns.types string:string:int
                             file.inputformat org.apache.hadoop.mapred.TextInputFormat
                             file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                            location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                             name dest_j1
                             serialization.ddl struct dest_j1 { string key, string value, i32 val2}
                             serialization.format 1
                             serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                            transient_lastDdlTime 1270516573
+                            transient_lastDdlTime 1279737236
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                           name: dest_j1
                       TotalFiles: 1
@@ -339,7 +335,7 @@ STAGE PLANS:
                         File Output Operator
                           compressed: false
                           GlobalTableId: 1
-                          directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10003
+                          directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10003
                           NumFilesPerFileSink: 1
                           table:
                               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -350,22 +346,22 @@ STAGE PLANS:
                                 columns.types string:string:int
                                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                                 name dest_j1
                                 serialization.ddl struct dest_j1 { string key, string value, i32 val2}
                                 serialization.format 1
                                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                transient_lastDdlTime 1270516573
+                                transient_lastDdlTime 1279737236
                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                               name: dest_j1
                           TotalFiles: 1
                           MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10002 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10002]
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10004 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10004]
+        file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10002 [file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10002]
+        file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10004 [file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10004]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -381,7 +377,7 @@ STAGE PLANS:
                 columns _col0,_col1
                 columns.types string,bigint
                 escape.delim \
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10004 
           Partition
             base file name: 10004
             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -405,14 +401,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10003
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10003
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -422,20 +418,20 @@ STAGE PLANS:
                 columns.types string:string:int
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, i32 val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516573
+                transient_lastDdlTime 1279737236
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10001
 
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10003 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10003 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -451,9 +447,9 @@ STAGE PLANS:
                     type: int
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10003 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10003]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10003 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10003]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10003 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10003 
           Partition
             base file name: 10003
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -464,12 +460,12 @@ STAGE PLANS:
               columns.types string:string:int
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
               name dest_j1
               serialization.ddl struct dest_j1 { string key, string value, i32 val2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516573
+              transient_lastDdlTime 1279737236
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -480,12 +476,12 @@ STAGE PLANS:
                 columns.types string:string:int
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, i32 val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516573
+                transient_lastDdlTime 1279737236
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
             name: dest_j1
@@ -494,7 +490,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-33-56_971_2721332696665877458/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -505,12 +501,12 @@ STAGE PLANS:
                   columns.types string:string:int
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/dest_j1
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                   name dest_j1
                   serialization.ddl struct dest_j1 { string key, string value, i32 val2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270516573
+                  transient_lastDdlTime 1279737236
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest_j1
             TotalFiles: 1
@@ -560,9 +556,9 @@ STAGE PLANS:
                             type: bigint
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src [null-subquery2:subq1-subquery2:x1]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [null-subquery2:subq1-subquery2:x1]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -573,12 +569,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516573
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -589,12 +585,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516573
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -618,7 +614,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-              directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-13_455_4995174938879288327/10004
+              directory: file:/tmp/jssarma/hive_2010-07-21_11-33-56_971_2721332696665877458/10004
               NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -661,11 +657,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-31_652_61835448429533557/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-07_547_7908401550562955902/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-31_652_61835448429533557/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-07_547_7908401550562955902/10000
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 EXPRESSION [(src)x.null, (src)x1.null, ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
@@ -682,11 +678,3 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:st
 369		3
 401	val_401	5
 406	val_406	4
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 EXPRESSION [(src)x.null, (src)x1.null, ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join36.q.out b/ql/src/test/results/clientpositive/join36.q.out
index ff5244f83f..fe9f917df1 100644
--- a/ql/src/test/results/clientpositive/join36.q.out
+++ b/ql/src/test/results/clientpositive/join36.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table tmp1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table tmp2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE tmp1(key INT, cnt INT)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE tmp1(key INT, cnt INT)
@@ -176,7 +164,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-19-51_782_4316865711705219377/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-34-15_535_6309433655713597513/10000
 
   Stage: Stage-0
     Move Operator
@@ -191,7 +179,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-19-51_782_4316865711705219377/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-34-15_535_6309433655713597513/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -241,11 +229,11 @@ POSTHOOK: Lineage: tmp2.key EXPRESSION [(src)src.FieldSchema(name:key, type:stri
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-19-56_079_1877087600971898109/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-18_197_2935477618280588903/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-19-56_079_1877087600971898109/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-18_197_2935477618280588903/10000
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(tmp1)x.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(tmp2)y.FieldSchema(name:cnt, type:int, comment:null), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(tmp1)x.FieldSchema(name:cnt, type:int, comment:null), ]
@@ -562,15 +550,3 @@ POSTHOOK: Lineage: tmp2.key EXPRESSION [(src)src.FieldSchema(name:key, type:stri
 496	1	1
 497	1	1
 498	3	3
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(tmp1)x.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(tmp2)y.FieldSchema(name:cnt, type:int, comment:null), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(tmp1)x.FieldSchema(name:cnt, type:int, comment:null), ]
-POSTHOOK: Lineage: tmp1.cnt EXPRESSION [(src)src.null, ]
-POSTHOOK: Lineage: tmp1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp2.cnt EXPRESSION [(src)src.null, ]
-POSTHOOK: Lineage: tmp2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join37.q.out b/ql/src/test/results/clientpositive/join37.q.out
index 80bb69359b..828af5d264 100644
--- a/ql/src/test/results/clientpositive/join37.q.out
+++ b/ql/src/test/results/clientpositive/join37.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE
@@ -146,7 +142,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-58_145_8617517541911578826/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-34-21_008_268143953010712985/10000
 
   Stage: Stage-0
     Move Operator
@@ -161,7 +157,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-15-58_145_8617517541911578826/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-34-21_008_268143953010712985/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -207,11 +203,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-16-03_220_6561496077281240464/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-23_746_1310159606592895751/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-16-03_220_6561496077281240464/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-23_746_1310159606592895751/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
@@ -252,11 +248,3 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:st
 406	val_406	val_406
 406	val_406	val_406
 406	val_406	val_406
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src1)x.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join38.q.out b/ql/src/test/results/clientpositive/join38.q.out
index cc056954d5..b1e3e6d077 100644
--- a/ql/src/test/results/clientpositive/join38.q.out
+++ b/ql/src/test/results/clientpositive/join38.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table tmp
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table tmp(col0 string, col1 string,col2 string,col3 string,col4 string,col5 string,col6 string,col7 string,col8 string,col9 string,col10 string,col11 string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tmp(col0 string, col1 string,col2 string,col3 string,col4 string,col5 string,col6 string,col7 string,col8 string,col9 string,col10 string,col11 string)
@@ -30,11 +26,11 @@ POSTHOOK: Lineage: tmp.col9 EXPRESSION [(src)src.FieldSchema(name:key, type:stri
 PREHOOK: query: select * from tmp
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-41_890_1216587155753076086/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-29_174_1856986099030897576/10000
 POSTHOOK: query: select * from tmp
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-41_890_1216587155753076086/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-29_174_1856986099030897576/10000
 POSTHOOK: Lineage: tmp.col0 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp.col1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp.col10 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -142,7 +138,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-41_968_6961135931629257781/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-34-29_228_6035667266137509751/10002 
           Select Operator
             expressions:
                   expr: _col1
@@ -230,7 +226,7 @@ group by a.value, b.col5
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-42_398_5553003257697465955/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-29_328_1884952246433236749/10000
 POSTHOOK: query: FROM src a JOIN tmp b ON (a.key = b.col11)
 SELECT /*+ MAPJOIN(a) */ a.value, b.col5, count(1) as count
 where b.col11 = 111
@@ -238,7 +234,7 @@ group by a.value, b.col5
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-16-42_398_5553003257697465955/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-29_328_1884952246433236749/10000
 POSTHOOK: Lineage: tmp.col0 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp.col1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp.col10 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -252,20 +248,3 @@ POSTHOOK: Lineage: tmp.col7 EXPRESSION [(src)src.FieldSchema(name:key, type:stri
 POSTHOOK: Lineage: tmp.col8 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp.col9 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 val_111	105	2
-PREHOOK: query: drop table tmp
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmp
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp
-POSTHOOK: Lineage: tmp.col0 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col10 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col11 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col3 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col4 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col5 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col6 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col7 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col8 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp.col9 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join39.q.out b/ql/src/test/results/clientpositive/join39.q.out
index 5b5c6e53ff..f812165621 100644
--- a/ql/src/test/results/clientpositive/join39.q.out
+++ b/ql/src/test/results/clientpositive/join39.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_j1(key STRING, value STRING, key1 string, val2 STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_j1(key STRING, value STRING, key1 string, val2 STRING) STORED AS TEXTFILE
@@ -26,11 +22,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)x.FieldSchema(name:value, type:str
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-09_939_2652321174464424560/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-38_483_2873236764151910976/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-09_939_2652321174464424560/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-34-38_483_2873236764151910976/10000
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.key1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -601,12 +597,3 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)x.FieldSchema(name:value, type:str
 98	val_98	98	val_98
 98	val_98	98	val_98
 98	val_98	98	val_98
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.key1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)x.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join_hive_626.q.out b/ql/src/test/results/clientpositive/join_hive_626.q.out
index d4eb21f680..271a5d4847 100644
--- a/ql/src/test/results/clientpositive/join_hive_626.q.out
+++ b/ql/src/test/results/clientpositive/join_hive_626.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table hive_foo
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_foo
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table hive_bar
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_bar
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table hive_count
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_count
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table hive_foo (foo_id int, foo_name string, foo_a string, foo_b string, 
 foo_c string, foo_d string) row format delimited fields terminated by ','
 stored as textfile
@@ -187,27 +175,12 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@hive_foo
 PREHOOK: Input: default@hive_count
 PREHOOK: Input: default@hive_bar
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-27-47_249_881439767445940507/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-35-37_392_1859121185876168833/10000
 POSTHOOK: query: select hive_foo.foo_name, hive_bar.bar_name, n from hive_foo join hive_bar on hive_foo.foo_id =
 hive_bar.foo_id join hive_count on hive_count.bar_id = hive_bar.bar_id
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@hive_foo
 POSTHOOK: Input: default@hive_count
 POSTHOOK: Input: default@hive_bar
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-27-47_249_881439767445940507/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-35-37_392_1859121185876168833/10000
 foo1	bar10	2
-PREHOOK: query: drop table hive_foo
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_foo
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@hive_foo
-PREHOOK: query: drop table hive_bar
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_bar
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@hive_bar
-PREHOOK: query: drop table hive_count
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_count
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@hive_count
diff --git a/ql/src/test/results/clientpositive/join_map_ppr.q.out b/ql/src/test/results/clientpositive/join_map_ppr.q.out
index 57b62fd9f2..4421d5205b 100644
--- a/ql/src/test/results/clientpositive/join_map_ppr.q.out
+++ b/ql/src/test/results/clientpositive/join_map_ppr.q.out
@@ -84,7 +84,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -95,12 +95,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                               name dest_j1
                               serialization.ddl struct dest_j1 { string key, string value, string val2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270516832
+                              transient_lastDdlTime 1279737342
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: dest_j1
                         TotalFiles: 1
@@ -163,7 +163,7 @@ STAGE PLANS:
                         File Output Operator
                           compressed: false
                           GlobalTableId: 1
-                          directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10002
+                          directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10002
                           NumFilesPerFileSink: 1
                           table:
                               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -174,12 +174,12 @@ STAGE PLANS:
                                 columns.types string:string:string
                                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                                 name dest_j1
                                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                                 serialization.format 1
                                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                transient_lastDdlTime 1270516832
+                                transient_lastDdlTime 1279737342
                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                               name: dest_j1
                           TotalFiles: 1
@@ -232,7 +232,7 @@ STAGE PLANS:
                         File Output Operator
                           compressed: false
                           GlobalTableId: 1
-                          directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10002
+                          directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10002
                           NumFilesPerFileSink: 1
                           table:
                               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -243,21 +243,21 @@ STAGE PLANS:
                                 columns.types string:string:string
                                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                                 name dest_j1
                                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                                 serialization.format 1
                                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                transient_lastDdlTime 1270516832
+                                transient_lastDdlTime 1279737342
                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                               name: dest_j1
                           TotalFiles: 1
                           MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [z]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [z]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
             base file name: hr=11
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -271,13 +271,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516830
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -288,13 +288,13 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516830
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
@@ -306,14 +306,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10002
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -323,20 +323,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516832
+                transient_lastDdlTime 1279737342
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -352,9 +352,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10002 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10002]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -365,12 +365,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
               name dest_j1
               serialization.ddl struct dest_j1 { string key, string value, string val2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516832
+              transient_lastDdlTime 1279737342
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -381,12 +381,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516832
+                transient_lastDdlTime 1279737342
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
             name: dest_j1
@@ -395,7 +395,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-32_378_853446091627412214/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-42_920_5027167476967323602/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -406,12 +406,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                   name dest_j1
                   serialization.ddl struct dest_j1 { string key, string value, string val2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270516832
+                  transient_lastDdlTime 1279737342
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest_j1
             TotalFiles: 1
@@ -443,11 +443,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:st
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-38_085_7637738477039016757/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-35-45_785_6484231589452690821/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-38_085_7637738477039016757/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-35-45_785_6484231589452690821/10000
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
@@ -690,7 +690,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10002
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10002
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -701,12 +701,12 @@ STAGE PLANS:
                               columns.types string:string:string
                               file.inputformat org.apache.hadoop.mapred.TextInputFormat
                               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                               name dest_j1
                               serialization.ddl struct dest_j1 { string key, string value, string val2}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1270516832
+                              transient_lastDdlTime 1279737345
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: dest_j1
                         TotalFiles: 1
@@ -769,7 +769,7 @@ STAGE PLANS:
                         File Output Operator
                           compressed: false
                           GlobalTableId: 1
-                          directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10002
+                          directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10002
                           NumFilesPerFileSink: 1
                           table:
                               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -780,12 +780,12 @@ STAGE PLANS:
                                 columns.types string:string:string
                                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                                 name dest_j1
                                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                                 serialization.format 1
                                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                transient_lastDdlTime 1270516832
+                                transient_lastDdlTime 1279737345
                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                               name: dest_j1
                           TotalFiles: 1
@@ -838,7 +838,7 @@ STAGE PLANS:
                         File Output Operator
                           compressed: false
                           GlobalTableId: 1
-                          directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10002
+                          directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10002
                           NumFilesPerFileSink: 1
                           table:
                               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -849,21 +849,21 @@ STAGE PLANS:
                                 columns.types string:string:string
                                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                                 name dest_j1
                                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                                 serialization.format 1
                                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                transient_lastDdlTime 1270516832
+                                transient_lastDdlTime 1279737345
                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                               name: dest_j1
                           TotalFiles: 1
                           MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [z]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [z]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
             base file name: hr=11
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -877,13 +877,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516830
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -894,13 +894,13 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516830
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
@@ -912,14 +912,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10002
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -929,20 +929,20 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516832
+                transient_lastDdlTime 1279737345
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -958,9 +958,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10002 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10002]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -971,12 +971,12 @@ STAGE PLANS:
               columns.types string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
               name dest_j1
               serialization.ddl struct dest_j1 { string key, string value, string val2}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270516832
+              transient_lastDdlTime 1279737345
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -987,12 +987,12 @@ STAGE PLANS:
                 columns.types string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                 name dest_j1
                 serialization.ddl struct dest_j1 { string key, string value, string val2}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270516832
+                transient_lastDdlTime 1279737345
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest_j1
             name: dest_j1
@@ -1001,7 +1001,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-50_930_3552914040610493954/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-35-53_321_2531534402913708550/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -1012,12 +1012,12 @@ STAGE PLANS:
                   columns.types string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest_j1
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest_j1
                   name dest_j1
                   serialization.ddl struct dest_j1 { string key, string value, string val2}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270516832
+                  transient_lastDdlTime 1279737345
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest_j1
             TotalFiles: 1
@@ -1056,11 +1056,11 @@ POSTHOOK: Lineage: src_copy.value SIMPLE [(src)src.FieldSchema(name:value, type:
 PREHOOK: query: select * from dest_j1 x order by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-55_757_2911779365097170567/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-35-56_284_4358743879920378285/10000
 POSTHOOK: query: select * from dest_j1 x order by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-20-55_757_2911779365097170567/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-35-56_284_4358743879920378285/10000
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1_copy)x.FieldSchema(name:key, type:string, comment:null), ]
 POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
@@ -1178,48 +1178,3 @@ POSTHOOK: Lineage: src_copy.value SIMPLE [(src)src.FieldSchema(name:value, type:
 98	val_98	val_98
 98	val_98	val_98
 98	val_98	val_98
-PREHOOK: query: drop table src_copy
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table src_copy
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@src_copy
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1_copy)x.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src_copy)y.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: src1_copy.key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src1_copy.value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_copy.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src_copy.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table src1_copy
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table src1_copy
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@src1_copy
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1_copy)x.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src_copy)y.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: src1_copy.key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src1_copy.value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_copy.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src_copy.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.key SIMPLE [(src1_copy)x.FieldSchema(name:key, type:string, comment:null), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src)y.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.val2 SIMPLE [(src_copy)y.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(srcpart)z.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: src1_copy.key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src1_copy.value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src_copy.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src_copy.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join_rc.q.out b/ql/src/test/results/clientpositive/join_rc.q.out
index 29acf25a4f..acb3b3272b 100644
--- a/ql/src/test/results/clientpositive/join_rc.q.out
+++ b/ql/src/test/results/clientpositive/join_rc.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table join_rc1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table join_rc1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table join_rc2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table join_rc2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table join_rc1(key string, value string) stored as RCFile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table join_rc1(key string, value string) stored as RCFile
@@ -124,13 +116,13 @@ FROM join_rc1 JOIN join_rc2 ON join_rc1.key = join_rc2.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@join_rc2
 PREHOOK: Input: default@join_rc1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-17-33_792_8150215655631569138/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-04_236_2954829201146901111/10000
 POSTHOOK: query: select join_rc1.key, join_rc2.value
 FROM join_rc1 JOIN join_rc2 ON join_rc1.key = join_rc2.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@join_rc2
 POSTHOOK: Input: default@join_rc1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-17-33_792_8150215655631569138/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-04_236_2954829201146901111/10000
 POSTHOOK: Lineage: join_rc1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: join_rc1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: join_rc2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1163,21 +1155,3 @@ POSTHOOK: Lineage: join_rc2.value SIMPLE [(src)src.FieldSchema(name:value, type:
 98	val_98
 98	val_98
 98	val_98
-PREHOOK: query: drop table join_rc1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table join_rc1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@join_rc1
-POSTHOOK: Lineage: join_rc1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: join_rc1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: join_rc2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: join_rc2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table join_rc2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table join_rc2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@join_rc2
-POSTHOOK: Lineage: join_rc1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: join_rc1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: join_rc2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: join_rc2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/join_reorder.q.out b/ql/src/test/results/clientpositive/join_reorder.q.out
index 895de9fc36..1dc65ef066 100644
--- a/ql/src/test/results/clientpositive/join_reorder.q.out
+++ b/ql/src/test/results/clientpositive/join_reorder.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
@@ -203,13 +191,13 @@ SELECT a.key, a.val, c.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-27-56_522_2427479678690495417/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-07_706_4137842264568073660/10000
 POSTHOOK: query: FROM T1 a JOIN src c ON c.key+1=a.key
 SELECT a.key, a.val, c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-27-56_522_2427479678690495417/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-07_706_4137842264568073660/10000
 1	11	0
 1	11	0
 1	11	0
@@ -219,13 +207,13 @@ SELECT /*+ STREAMTABLE(a) */ a.key, a.val, c.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-01_499_4645186819885871483/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-10_458_6076144350106106313/10000
 POSTHOOK: query: FROM T1 a JOIN src c ON c.key+1=a.key
 SELECT /*+ STREAMTABLE(a) */ a.key, a.val, c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-01_499_4645186819885871483/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-10_458_6076144350106106313/10000
 1	11	0
 1	11	0
 1	11	0
@@ -512,7 +500,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-07_922_2294786151582488765/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-13_343_7536917868494596794/10000
 POSTHOOK: query: FROM T1 a
   LEFT OUTER JOIN T2 b ON (b.key=a.key)
   RIGHT OUTER JOIN T3 c ON (c.val = a.val)
@@ -521,7 +509,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-07_922_2294786151582488765/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-13_343_7536917868494596794/10000
 2	2	12	12
 NULL	NULL	NULL	14
 NULL	NULL	NULL	16
@@ -534,7 +522,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-20_301_8638454003102682278/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-18_608_5119660399295658126/10000
 POSTHOOK: query: FROM T1 a
   LEFT OUTER JOIN T2 b ON (b.key=a.key)
   RIGHT OUTER JOIN T3 c ON (c.val = a.val)
@@ -543,7 +531,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-20_301_8638454003102682278/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-18_608_5119660399295658126/10000
 2	2	12	12
 NULL	NULL	NULL	14
 NULL	NULL	NULL	16
@@ -781,7 +769,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-31_833_1632343405751503469/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-24_350_4665014421054179440/10000
 POSTHOOK: query: FROM UNIQUEJOIN
   PRESERVE T1 a (a.key, a.val),
   PRESERVE T2 b (b.key, b.val),
@@ -791,7 +779,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-31_833_1632343405751503469/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-24_350_4665014421054179440/10000
 1	NULL	NULL
 2	NULL	2
 NULL	2	NULL
@@ -812,7 +800,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-38_626_3188343531104185080/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-34_969_3293491720663599421/10000
 POSTHOOK: query: FROM UNIQUEJOIN
   PRESERVE T1 a (a.key, a.val),
   PRESERVE T2 b (b.key, b.val),
@@ -822,7 +810,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_1/build/ql/scratchdir/hive_2010-02-12_22-28-38_626_3188343531104185080/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-34_969_3293491720663599421/10000
 1	NULL	NULL
 2	NULL	2
 NULL	2	NULL
@@ -834,18 +822,3 @@ NULL	NULL	6
 8	8	NULL
 8	8	NULL
 8	NULL	NULL
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t3
diff --git a/ql/src/test/results/clientpositive/join_reorder2.q.out b/ql/src/test/results/clientpositive/join_reorder2.q.out
index a67aacfb26..601c10af24 100644
--- a/ql/src/test/results/clientpositive/join_reorder2.q.out
+++ b/ql/src/test/results/clientpositive/join_reorder2.q.out
@@ -1,19 +1,3 @@
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T4
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
@@ -198,7 +182,7 @@ PREHOOK: Input: default@t4
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-28-02_101_1898487455882506837/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-44_351_4187236122706744713/10000
 POSTHOOK: query: SELECT /*+ STREAMTABLE(a) */ *
 FROM T1 a JOIN T2 b ON a.key = b.key
           JOIN T3 c ON b.key = c.key
@@ -208,7 +192,7 @@ POSTHOOK: Input: default@t4
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-28-02_101_1898487455882506837/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-44_351_4187236122706744713/10000
 2	12	2	22	2	12	2	12
 PREHOOK: query: EXPLAIN
 SELECT /*+ STREAMTABLE(a) */ *
@@ -431,7 +415,7 @@ PREHOOK: Input: default@t4
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-28-09_022_5817489663400235377/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-51_711_5469326650666419261/10000
 POSTHOOK: query: SELECT /*+ STREAMTABLE(a) */ *
 FROM T1 a JOIN T2 b ON a.key = b.key
           JOIN T3 c ON a.val = c.val
@@ -441,25 +425,5 @@ POSTHOOK: Input: default@t4
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-28-09_022_5817489663400235377/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-36-51_711_5469326650666419261/10000
 2	22	2	12	2	12	2	12
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t3
-PREHOOK: query: DROP TABLE T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t4
diff --git a/ql/src/test/results/clientpositive/join_reorder3.q.out b/ql/src/test/results/clientpositive/join_reorder3.q.out
index 566719cc1b..a7e5b969ef 100644
--- a/ql/src/test/results/clientpositive/join_reorder3.q.out
+++ b/ql/src/test/results/clientpositive/join_reorder3.q.out
@@ -1,19 +1,3 @@
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T4
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
@@ -198,7 +182,7 @@ PREHOOK: Input: default@t4
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_0/build/ql/scratchdir/hive_2010-02-12_22-33-34_907_1935969450376975821/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-00_490_2765894359578078501/10000
 POSTHOOK: query: SELECT /*+ STREAMTABLE(a,c) */ *
 FROM T1 a JOIN T2 b ON a.key = b.key
           JOIN T3 c ON b.key = c.key
@@ -208,7 +192,7 @@ POSTHOOK: Input: default@t4
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_0/build/ql/scratchdir/hive_2010-02-12_22-33-34_907_1935969450376975821/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-00_490_2765894359578078501/10000
 2	12	2	22	2	12	2	12
 PREHOOK: query: EXPLAIN
 SELECT /*+ STREAMTABLE(a,c) */ *
@@ -431,7 +415,7 @@ PREHOOK: Input: default@t4
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_0/build/ql/scratchdir/hive_2010-02-12_22-33-41_586_5996486802122601404/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-03_448_3407530204384318986/10000
 POSTHOOK: query: SELECT /*+ STREAMTABLE(a,c) */ *
 FROM T1 a JOIN T2 b ON a.key = b.key
           JOIN T3 c ON a.val = c.val
@@ -441,25 +425,5 @@ POSTHOOK: Input: default@t4
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_0/build/ql/scratchdir/hive_2010-02-12_22-33-41_586_5996486802122601404/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-03_448_3407530204384318986/10000
 2	22	2	12	2	12	2	12
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t3
-PREHOOK: query: DROP TABLE T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t4
diff --git a/ql/src/test/results/clientpositive/lateral_view.q.out b/ql/src/test/results/clientpositive/lateral_view.q.out
index 7f75d808d0..1671506a85 100644
--- a/ql/src/test/results/clientpositive/lateral_view.q.out
+++ b/ql/src/test/results/clientpositive/lateral_view.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: DROP TABLE tmp_pyang_lv
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tmp_pyang_lv
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE tmp_pyang_src_rcfile
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tmp_pyang_src_rcfile
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE tmp_pyang_lv (inputs string) STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE tmp_pyang_lv (inputs string) STORED AS RCFILE
@@ -115,7 +107,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/tmp/pyang/hive_2010-07-14_16-15-34_051_4828871152684194272/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-37-17_144_3458539882275515544/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -451,24 +443,24 @@ PREHOOK: query: -- Verify that * selects columns from both tables
 SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol SORT BY key ASC, myCol ASC LIMIT 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-34_600_5107346587153071440/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-17_490_228117844421732265/10000
 POSTHOOK: query: -- Verify that * selects columns from both tables
 SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol SORT BY key ASC, myCol ASC LIMIT 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-34_600_5107346587153071440/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-17_490_228117844421732265/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 0	val_0	1
 PREHOOK: query: -- TABLE.* should be supported
 SELECT myTable.* FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LIMIT 3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-41_257_8220843170923127190/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-22_740_2713465299397417423/10000
 POSTHOOK: query: -- TABLE.* should be supported
 SELECT myTable.* FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LIMIT 3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-41_257_8220843170923127190/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-22_740_2713465299397417423/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 1
 2
@@ -477,12 +469,12 @@ PREHOOK: query: -- Multiple lateral views should result in a Cartesian product
 SELECT myTable.myCol, myTable2.myCol2 FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LATERAL VIEW explode(array('a', 'b', 'c')) myTable2 AS myCol2 LIMIT 9
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-44_673_2747911293056086153/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-25_345_4555727829445494206/10000
 POSTHOOK: query: -- Multiple lateral views should result in a Cartesian product
 SELECT myTable.myCol, myTable2.myCol2 FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LATERAL VIEW explode(array('a', 'b', 'c')) myTable2 AS myCol2 LIMIT 9
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-44_673_2747911293056086153/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-25_345_4555727829445494206/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 1	a
 1	b
@@ -497,12 +489,12 @@ PREHOOK: query: -- Should be able to reference tables generated earlier
 SELECT myTable2.* FROM src LATERAL VIEW explode(array(array(1,2,3))) myTable AS myCol LATERAL VIEW explode(myTable.myCol) myTable2 AS myCol2 LIMIT 3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-47_980_4164961629359858242/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-27_957_3802707944191728309/10000
 POSTHOOK: query: -- Should be able to reference tables generated earlier
 SELECT myTable2.* FROM src LATERAL VIEW explode(array(array(1,2,3))) myTable AS myCol LATERAL VIEW explode(myTable.myCol) myTable2 AS myCol2 LIMIT 3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-47_980_4164961629359858242/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-27_957_3802707944191728309/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 1
 2
@@ -575,11 +567,11 @@ STAGE PLANS:
 PREHOOK: query: SELECT myCol from tmp_pyang_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_pyang_lv
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-51_579_4337534379576799491/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-30_705_7614845439925178391/10000
 POSTHOOK: query: SELECT myCol from tmp_pyang_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_pyang_lv
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-15-51_579_4337534379576799491/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-30_705_7614845439925178391/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 1
 2
@@ -604,11 +596,11 @@ POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(n
 PREHOOK: query: SELECT key,value from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_pyang_src_rcfile
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-01_099_1803034064573776934/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-38_346_8286485984240319426/10000
 POSTHOOK: query: SELECT key,value from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_pyang_src_rcfile
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-01_099_1803034064573776934/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-38_346_8286485984240319426/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -635,11 +627,11 @@ POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(n
 PREHOOK: query: SELECT myCol from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_pyang_src_rcfile
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-05_104_3522643641494524502/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-40_934_9042604436385795850/10000
 POSTHOOK: query: SELECT myCol from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_pyang_src_rcfile
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-05_104_3522643641494524502/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-40_934_9042604436385795850/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -666,11 +658,11 @@ val_119
 PREHOOK: query: SELECT * from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_pyang_src_rcfile
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-08_316_5289580697756818313/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-43_563_2397125834453795185/10000
 POSTHOOK: query: SELECT * from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_pyang_src_rcfile
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-08_316_5289580697756818313/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-43_563_2397125834453795185/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -700,14 +692,14 @@ SELECT * from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 )subq
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_pyang_src_rcfile
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-11_498_461410201661197582/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-46_264_3992245203524780732/10000
 POSTHOOK: query: SELECT subq.key,subq.value 
 FROM (
 SELECT * from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 )subq
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_pyang_src_rcfile
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-11_498_461410201661197582/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-46_264_3992245203524780732/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -737,14 +729,14 @@ SELECT * from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 )subq
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_pyang_src_rcfile
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-14_661_8437009580148501289/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-48_829_4806162384243621220/10000
 POSTHOOK: query: SELECT subq.myCol
 FROM (
 SELECT * from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 )subq
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_pyang_src_rcfile
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-14_661_8437009580148501289/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-48_829_4806162384243621220/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -774,14 +766,14 @@ SELECT key, value from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable
 )subq
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_pyang_src_rcfile
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-18_398_4323350114519412048/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-51_450_6950611993397322223/10000
 POSTHOOK: query: SELECT subq.key 
 FROM (
 SELECT key, value from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
 )subq
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_pyang_src_rcfile
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-18_398_4323350114519412048/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-51_450_6950611993397322223/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -924,12 +916,12 @@ PREHOOK: query: SELECT value, myCol from (SELECT key, array(value[0]) AS value F
 LATERAL VIEW explode(value) myTable AS myCol
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_pyang_src_rcfile
-PREHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-22_146_140933306084614689/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-54_210_4766434103306434571/10000
 POSTHOOK: query: SELECT value, myCol from (SELECT key, array(value[0]) AS value FROM tmp_pyang_src_rcfile GROUP BY value[0], key) a
 LATERAL VIEW explode(value) myTable AS myCol
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_pyang_src_rcfile
-POSTHOOK: Output: file:/tmp/pyang/hive_2010-07-14_16-16-22_146_140933306084614689/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-37-54_210_4766434103306434571/10000
 POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -946,19 +938,3 @@ POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(n
 ["val_116"]	val_116
 ["val_118"]	val_118
 ["val_119"]	val_119
-PREHOOK: query: DROP TABLE tmp_pyang_src_rcfile
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tmp_pyang_src_rcfile
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp_pyang_src_rcfile
-POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp_pyang_src_rcfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE tmp_pyang_lv
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tmp_pyang_lv
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp_pyang_lv
-POSTHOOK: Lineage: tmp_pyang_lv.inputs SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp_pyang_src_rcfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp_pyang_src_rcfile.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/lineage1.q.out b/ql/src/test/results/clientpositive/lineage1.q.out
index 97ee88746c..4b89517df4 100644
--- a/ql/src/test/results/clientpositive/lineage1.q.out
+++ b/ql/src/test/results/clientpositive/lineage1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest_l1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest_l1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE dest_l1(key INT, value STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_l1(key INT, value STRING) STORED AS TEXTFILE
@@ -105,7 +101,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/build/ql/scratchdir/hive_2010-04-22_15-52-46_260_8748362802646516479/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-38-08_578_3994995071945892234/10002 
           Union
             Select Operator
               expressions:
@@ -129,7 +125,7 @@ STAGE PLANS:
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: dest_l1
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/build/ql/scratchdir/hive_2010-04-22_15-52-46_260_8748362802646516479/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-38-08_578_3994995071945892234/10004 
           Union
             Select Operator
               expressions:
@@ -161,7 +157,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/build/ql/scratchdir/hive_2010-04-22_15-52-46_260_8748362802646516479/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-38-08_578_3994995071945892234/10000
 
   Stage: Stage-0
     Move Operator
@@ -176,7 +172,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/build/ql/scratchdir/hive_2010-04-22_15-52-46_260_8748362802646516479/10003 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-38-08_578_3994995071945892234/10003 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
diff --git a/ql/src/test/results/clientpositive/load_dyn_part1.q.out b/ql/src/test/results/clientpositive/load_dyn_part1.q.out
index 73d572d003..608daf8345 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part1.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part1.q.out
@@ -6,14 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table nzhang_part2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part1 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part1 like srcpart
@@ -33,7 +25,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part1, dbName:default, owner:null, createTime:1271267550, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267550}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part1, dbName:default, owner:null, createTime:1279737499, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part1, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737499}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: explain
 from srcpart
 insert overwrite table nzhang_part1 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
@@ -191,12 +183,12 @@ PREHOOK: query: select * from nzhang_part1 where ds is not null and hr is not nu
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part1@ds=2008-04-08/hr=11
 PREHOOK: Input: default@nzhang_part1@ds=2008-04-08/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-37_444_3227664036860209538/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-22_823_8146644774573377248/10000
 POSTHOOK: query: select * from nzhang_part1 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part1@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@nzhang_part1@ds=2008-04-08/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-37_444_3227664036860209538/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-22_823_8146644774573377248/10000
 POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -1209,12 +1201,12 @@ PREHOOK: query: select * from nzhang_part2 where ds is not null and hr is not nu
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part2@ds=2008-12-31/hr=11
 PREHOOK: Input: default@nzhang_part2@ds=2008-12-31/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-37_769_8779363378584503667/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-22_956_5486551710928767625/10000
 POSTHOOK: query: select * from nzhang_part2 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part2@ds=2008-12-31/hr=11
 POSTHOOK: Input: default@nzhang_part2@ds=2008-12-31/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-37_769_8779363378584503667/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-22_956_5486551710928767625/10000
 POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -2223,29 +2215,3 @@ POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=12).value SIMPLE [(sr
 400	val_400	2008-12-31	12
 200	val_200	2008-12-31	12
 97	val_97	2008-12-31	12
-PREHOOK: query: drop table nzhang_part1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part1
-POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-PREHOOK: query: drop table nzhang_part2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part2
-POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part10.q.out b/ql/src/test/results/clientpositive/load_dyn_part10.q.out
index 71a0d9777d..ec66342ab6 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part10.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part10.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part10
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part10
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part10 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part10 like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part10, dbName:default, owner:null, createTime:1271267562, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part10, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267562}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part10, dbName:default, owner:null, createTime:1279737503, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part10, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737503}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: explain
 from srcpart
 insert overwrite table nzhang_part10 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08'
@@ -117,12 +113,12 @@ PREHOOK: query: select * from nzhang_part10 where ds is not null and hr is not n
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part10@ds=2008-12-31/hr=11
 PREHOOK: Input: default@nzhang_part10@ds=2008-12-31/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-47_140_4020149388903312321/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-26_462_5702859057182730002/10000
 POSTHOOK: query: select * from nzhang_part10 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part10@ds=2008-12-31/hr=11
 POSTHOOK: Input: default@nzhang_part10@ds=2008-12-31/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-47_140_4020149388903312321/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-26_462_5702859057182730002/10000
 POSTHOOK: Lineage: nzhang_part10 PARTITION(ds=2008-12-31,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part10 PARTITION(ds=2008-12-31,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part10 PARTITION(ds=2008-12-31,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -1127,12 +1123,3 @@ POSTHOOK: Lineage: nzhang_part10 PARTITION(ds=2008-12-31,hr=12).value SIMPLE [(s
 400	val_400	2008-12-31	12
 200	val_200	2008-12-31	12
 97	val_97	2008-12-31	12
-PREHOOK: query: drop table nzhang_part10
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part10
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part10
-POSTHOOK: Lineage: nzhang_part10 PARTITION(ds=2008-12-31,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part10 PARTITION(ds=2008-12-31,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part10 PARTITION(ds=2008-12-31,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part10 PARTITION(ds=2008-12-31,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part11.q.out b/ql/src/test/results/clientpositive/load_dyn_part11.q.out
index 054156043d..053e002278 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part11.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part11.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part, dbName:default, owner:null, createTime:1271267570, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267570}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part, dbName:default, owner:null, createTime:1279737506, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737506}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table nzhang_part partition (ds="2010-03-03", hr) select key, value, hr from srcpart where ds is not null and hr is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
@@ -46,11 +42,11 @@ POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(src
 PREHOOK: query: select * from nzhang_part where ds = '2010-03-03' and hr = '11'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part@ds=2010-03-03/hr=11
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-55_641_8045894456254007066/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-29_671_8910076593286729265/10000
 POSTHOOK: query: select * from nzhang_part where ds = '2010-03-03' and hr = '11'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part@ds=2010-03-03/hr=11
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-55_641_8045894456254007066/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-29_671_8910076593286729265/10000
 POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -1058,11 +1054,11 @@ POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(src
 PREHOOK: query: select * from nzhang_part where ds = '2010-03-03' and hr = '12'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part@ds=2010-03-03/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-55_845_6778768104814705748/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-29_767_8476842886490876328/10000
 POSTHOOK: query: select * from nzhang_part where ds = '2010-03-03' and hr = '12'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part@ds=2010-03-03/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-52-55_845_6778768104814705748/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-29_767_8476842886490876328/10000
 POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -2067,12 +2063,3 @@ POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(src
 400	val_400	2010-03-03	12
 200	val_200	2010-03-03	12
 97	val_97	2010-03-03	12
-PREHOOK: query: drop table nzhang_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part
-POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part12.q.out b/ql/src/test/results/clientpositive/load_dyn_part12.q.out
index a504ed334d..e2c284d4b7 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part12.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part12.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part12
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part12
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part12 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part12 like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part12, dbName:default, owner:null, createTime:1271267579, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part12, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267579}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part12, dbName:default, owner:null, createTime:1279737510, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part12, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737510}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table nzhang_part12 partition (ds="2010-03-03", hr) select key, value, cast(hr*2 as int) from srcpart where ds is not null and hr is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
@@ -57,12 +53,12 @@ PREHOOK: query: select * from nzhang_part12 where ds is not null and hr is not n
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part12@ds=2010-03-03/hr=22
 PREHOOK: Input: default@nzhang_part12@ds=2010-03-03/hr=24
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-53-03_745_4700114641729716301/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-33_165_2144010220785579549/10000
 POSTHOOK: query: select * from nzhang_part12 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part12@ds=2010-03-03/hr=22
 POSTHOOK: Input: default@nzhang_part12@ds=2010-03-03/hr=24
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-53-03_745_4700114641729716301/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-33_165_2144010220785579549/10000
 POSTHOOK: Lineage: nzhang_part12 PARTITION(ds=2010-03-03,hr=22).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part12 PARTITION(ds=2010-03-03,hr=22).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part12 PARTITION(ds=2010-03-03,hr=24).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -2067,12 +2063,3 @@ POSTHOOK: Lineage: nzhang_part12 PARTITION(ds=2010-03-03,hr=24).value SIMPLE [(s
 400	val_400	2010-03-03	24
 200	val_200	2010-03-03	24
 97	val_97	2010-03-03	24
-PREHOOK: query: drop table nzhang_part12
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part12
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part12
-POSTHOOK: Lineage: nzhang_part12 PARTITION(ds=2010-03-03,hr=22).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part12 PARTITION(ds=2010-03-03,hr=22).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part12 PARTITION(ds=2010-03-03,hr=24).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part12 PARTITION(ds=2010-03-03,hr=24).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part13.q.out b/ql/src/test/results/clientpositive/load_dyn_part13.q.out
index 055a9c9d90..757b4ad25b 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part13.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part13.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part13
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part13
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part13 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part13 like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part13, dbName:default, owner:null, createTime:1271267587, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part13, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267587}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part13, dbName:default, owner:null, createTime:1279737513, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part13, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737513}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: explain
 insert overwrite table nzhang_part13 partition (ds="2010-03-03", hr) 
 select * from (
@@ -191,12 +187,12 @@ PREHOOK: query: select * from nzhang_part13 where ds is not null and hr is not n
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part13@ds=2010-03-03/hr=22
 PREHOOK: Input: default@nzhang_part13@ds=2010-03-03/hr=33
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-53-12_256_3141269474960312939/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-36_782_8524700985752178497/10000
 POSTHOOK: query: select * from nzhang_part13 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part13@ds=2010-03-03/hr=22
 POSTHOOK: Input: default@nzhang_part13@ds=2010-03-03/hr=33
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-53-12_256_3141269474960312939/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-36_782_8524700985752178497/10000
 POSTHOOK: Lineage: nzhang_part13 PARTITION(ds=2010-03-03,hr=22).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: nzhang_part13 PARTITION(ds=2010-03-03,hr=22).value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: nzhang_part13 PARTITION(ds=2010-03-03,hr=33).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -235,12 +231,3 @@ POSTHOOK: Lineage: nzhang_part13 PARTITION(ds=2010-03-03,hr=33).value EXPRESSION
 26	val_26	2010-03-03	33
 28	val_28	2010-03-03	33
 37	val_37	2010-03-03	33
-PREHOOK: query: drop table nzhang_part13
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part13
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part13
-POSTHOOK: Lineage: nzhang_part13 PARTITION(ds=2010-03-03,hr=22).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part13 PARTITION(ds=2010-03-03,hr=22).value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part13 PARTITION(ds=2010-03-03,hr=33).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part13 PARTITION(ds=2010-03-03,hr=33).value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part14.q.out b/ql/src/test/results/clientpositive/load_dyn_part14.q.out
index 6eb89087c7..db7b49c396 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part14.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part14.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table nzhang_part14
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part14
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part14 (key string) 
   partitioned by (value string)
 PREHOOK: type: CREATETABLE
@@ -16,7 +12,7 @@ POSTHOOK: type: DESCTABLE
 key	string	
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part14, dbName:default, owner:jsichi, createTime:1274389243, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null)], location:file:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/nzhang_part14, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:value, type:string, comment:null)], parameters:{transient_lastDdlTime=1274389243}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part14, dbName:default, owner:jssarma, createTime:1279737517, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part14, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:value, type:string, comment:null)], parameters:{transient_lastDdlTime=1279737517}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: explain
 insert overwrite table nzhang_part14 partition(value) 
 select key, value from (
@@ -83,7 +79,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/Users/jsichi/open/hive-trunk/build/ql/scratchdir/hive_2010-05-20_14-00-43_465_6450594620359325391/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-38-37_310_8056013693306794455/10002 
           Union
             Select Operator
               expressions:
@@ -100,7 +96,7 @@ STAGE PLANS:
                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                     name: nzhang_part14
-        file:/Users/jsichi/open/hive-trunk/build/ql/scratchdir/hive_2010-05-20_14-00-43_465_6450594620359325391/10003 
+        file:/tmp/jssarma/hive_2010-07-21_11-38-37_310_8056013693306794455/10003 
           Union
             Select Operator
               expressions:
@@ -117,7 +113,7 @@ STAGE PLANS:
                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                     name: nzhang_part14
-        file:/Users/jsichi/open/hive-trunk/build/ql/scratchdir/hive_2010-05-20_14-00-43_465_6450594620359325391/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-38-37_310_8056013693306794455/10004 
           Union
             Select Operator
               expressions:
@@ -249,13 +245,13 @@ order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part14@value= 
 PREHOOK: Input: default@nzhang_part14@value=__HIVE_DEFAULT_PARTITION__
-PREHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/scratchdir/hive_2010-05-20_14-01-03_221_2936213374422819453/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-47_899_258856640110284282/10000
 POSTHOOK: query: select * from nzhang_part14 where value <> 'a'
 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part14@value= 
 POSTHOOK: Input: default@nzhang_part14@value=__HIVE_DEFAULT_PARTITION__
-POSTHOOK: Output: file:/Users/jsichi/open/hive-trunk/build/ql/scratchdir/hive_2010-05-20_14-01-03_221_2936213374422819453/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-47_899_258856640110284282/10000
 POSTHOOK: Lineage: nzhang_part14 PARTITION(value= ).key EXPRESSION []
 POSTHOOK: Lineage: nzhang_part14 PARTITION(value=__HIVE_DEFAULT_PARTITION__).key EXPRESSION []
 k1	__HIVE_DEFAULT_PARTITION__
@@ -264,10 +260,3 @@ k2	__HIVE_DEFAULT_PARTITION__
 k2	__HIVE_DEFAULT_PARTITION__
 k3	 
 k3	 
-PREHOOK: query: drop table nzhang_part14
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part14
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part14
-POSTHOOK: Lineage: nzhang_part14 PARTITION(value= ).key EXPRESSION []
-POSTHOOK: Lineage: nzhang_part14 PARTITION(value=__HIVE_DEFAULT_PARTITION__).key EXPRESSION []
diff --git a/ql/src/test/results/clientpositive/load_dyn_part2.q.out b/ql/src/test/results/clientpositive/load_dyn_part2.q.out
index 0495946d93..b04063275b 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part2.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part2.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table nzhang_part_bucket
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part_bucket
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part_bucket (key string, value string) 
   partitioned by (ds string, hr string) 
   clustered by (key) into 10 buckets
@@ -20,7 +16,7 @@ value	string
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part_bucket, dbName:default, owner:nzhang, createTime:1271267614, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part_bucket, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:10, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[key], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{transient_lastDdlTime=1271267614}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part_bucket, dbName:default, owner:jssarma, createTime:1279737530, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part_bucket, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:10, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[key], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{transient_lastDdlTime=1279737530}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: explain
 insert overwrite table nzhang_part_bucket partition (ds='2010-03-23', hr) select key, value, hr from srcpart where ds is not null and hr is not null
 PREHOOK: type: QUERY
@@ -127,11 +123,11 @@ ds=2010-03-23/hr=12
 PREHOOK: query: select * from nzhang_part_bucket where ds='2010-03-23' and hr='11' order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part_bucket@ds=2010-03-23/hr=11
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-53-42_252_5131661556862540128/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-55_332_7977928498639952021/10000
 POSTHOOK: query: select * from nzhang_part_bucket where ds='2010-03-23' and hr='11' order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part_bucket@ds=2010-03-23/hr=11
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-53-42_252_5131661556862540128/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-38-55_332_7977928498639952021/10000
 POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -1139,11 +1135,11 @@ POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=12).value SIMPL
 PREHOOK: query: select * from nzhang_part_bucket where ds='2010-03-23' and hr='12' order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part_bucket@ds=2010-03-23/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-53-49_136_4048550724530357597/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-00_003_5219381574950480620/10000
 POSTHOOK: query: select * from nzhang_part_bucket where ds='2010-03-23' and hr='12' order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part_bucket@ds=2010-03-23/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-53-49_136_4048550724530357597/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-00_003_5219381574950480620/10000
 POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -2148,12 +2144,3 @@ POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=12).value SIMPL
 98	val_98	2010-03-23	12
 98	val_98	2010-03-23	12
 98	val_98	2010-03-23	12
-PREHOOK: query: drop table nzhang_part_bucket
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part_bucket
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part_bucket
-POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part_bucket PARTITION(ds=2010-03-23,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part3.q.out b/ql/src/test/results/clientpositive/load_dyn_part3.q.out
index 28b9c78188..163465d2ab 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part3.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part3.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part3 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part3 like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part3, dbName:default, owner:null, createTime:1271267638, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267638}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part3, dbName:default, owner:null, createTime:1279737545, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part3, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737545}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: explain
 insert overwrite table nzhang_part3 partition (ds, hr) select key, value, ds, hr from srcpart where ds is not null and hr is not null
 PREHOOK: type: QUERY
@@ -117,14 +113,14 @@ PREHOOK: Input: default@nzhang_part3@ds=2008-04-08/hr=11
 PREHOOK: Input: default@nzhang_part3@ds=2008-04-08/hr=12
 PREHOOK: Input: default@nzhang_part3@ds=2008-04-09/hr=11
 PREHOOK: Input: default@nzhang_part3@ds=2008-04-09/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-03_241_207111621559915409/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-08_100_2828629619476369079/10000
 POSTHOOK: query: select * from nzhang_part3 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part3@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@nzhang_part3@ds=2008-04-08/hr=12
 POSTHOOK: Input: default@nzhang_part3@ds=2008-04-09/hr=11
 POSTHOOK: Input: default@nzhang_part3@ds=2008-04-09/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-03_241_207111621559915409/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-08_100_2828629619476369079/10000
 POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -2133,16 +2129,3 @@ POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-09,hr=12).value SIMPLE [(sr
 400	val_400	2008-04-09	12
 200	val_200	2008-04-09	12
 97	val_97	2008-04-09	12
-PREHOOK: query: drop table nzhang_part3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part3
-POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-09,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-09,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-09,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-09,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part4.q.out b/ql/src/test/results/clientpositive/load_dyn_part4.q.out
index d68b419650..45b88a3256 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part4.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part4.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part4
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part4 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part4 like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part4, dbName:default, owner:null, createTime:1271267646, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part4, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267646}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part4, dbName:default, owner:null, createTime:1279737548, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part4, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737548}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table nzhang_part4 partition (ds='2008-04-08', hr='existing_value') select key, value from src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -149,13 +145,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=11
 PREHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=12
 PREHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=existing_value
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-15_131_3367551704810236456/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-14_641_4468498316142880869/10000
 POSTHOOK: query: select * from nzhang_part4 where ds='2008-04-08' and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=12
 POSTHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=existing_value
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-15_131_3367551704810236456/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-14_641_4468498316142880869/10000
 POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -1673,7 +1669,7 @@ PREHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=12
 PREHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=existing_value
 PREHOOK: Input: default@nzhang_part4@ds=2008-04-09/hr=11
 PREHOOK: Input: default@nzhang_part4@ds=2008-04-09/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-15_358_2831086492275041519/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-14_849_6905120863948825983/10000
 POSTHOOK: query: select * from nzhang_part4 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=11
@@ -1681,7 +1677,7 @@ POSTHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=12
 POSTHOOK: Input: default@nzhang_part4@ds=2008-04-08/hr=existing_value
 POSTHOOK: Input: default@nzhang_part4@ds=2008-04-09/hr=11
 POSTHOOK: Input: default@nzhang_part4@ds=2008-04-09/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-15_358_2831086492275041519/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-14_849_6905120863948825983/10000
 POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -4192,18 +4188,3 @@ POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-09,hr=12).value SIMPLE [(sr
 400	val_400	2008-04-09	12
 200	val_200	2008-04-09	12
 97	val_97	2008-04-09	12
-PREHOOK: query: drop table nzhang_part4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part4
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=existing_value).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-08,hr=existing_value).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-09,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-09,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-09,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part4 PARTITION(ds=2008-04-09,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part5.q.out b/ql/src/test/results/clientpositive/load_dyn_part5.q.out
index 113c3972f8..fcbd16311d 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part5.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part5.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table nzhang_part5
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part5
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part5 (key string) partitioned by (value string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part5 (key string) partitioned by (value string)
@@ -14,7 +10,7 @@ POSTHOOK: type: DESCTABLE
 key	string	
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part5, dbName:default, owner:nzhang, createTime:1271267658, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part5, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:value, type:string, comment:null)], parameters:{transient_lastDdlTime=1271267658}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part5, dbName:default, owner:jssarma, createTime:1279737555, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part5, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:value, type:string, comment:null)], parameters:{transient_lastDdlTime=1279737555}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: explain
 insert overwrite table nzhang_part5 partition (value) select key, value from src
 PREHOOK: type: QUERY
@@ -1313,11 +1309,11 @@ value=val_98
 PREHOOK: query: select * from nzhang_part5 where value='val_0'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part5@value=val_0
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-50_927_7594481289864323902/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-48_159_2504137671156334886/10000
 POSTHOOK: query: select * from nzhang_part5 where value='val_0'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part5@value=val_0
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-50_927_7594481289864323902/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-48_159_2504137671156334886/10000
 POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_0).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_103).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1633,11 +1629,11 @@ POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_9).key SIMPLE [(src)src.Fiel
 PREHOOK: query: select * from nzhang_part5 where value='val_2'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part5@value=val_2
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-51_399_9037980484858557048/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-48_475_4435589346967527703/10000
 POSTHOOK: query: select * from nzhang_part5 where value='val_2'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part5@value=val_2
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-54-51_399_9037980484858557048/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-48_475_4435589346967527703/10000
 POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_0).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_103).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1948,317 +1944,3 @@ POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_97).key SIMPLE [(src)src.Fie
 POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_98).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_9).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 2	val_2
-PREHOOK: query: drop table nzhang_part5
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part5
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part5
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_0).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_103).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_104).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_105).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_10).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_111).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_113).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_114).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_116).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_118).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_119).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_120).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_125).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_126).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_128).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_129).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_131).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_133).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_134).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_136).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_137).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_138).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_143).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_145).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_146).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_149).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_150).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_152).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_153).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_155).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_156).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_157).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_158).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_15).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_160).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_162).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_163).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_164).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_165).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_166).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_167).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_168).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_169).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_170).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_172).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_174).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_175).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_176).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_177).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_178).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_179).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_17).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_180).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_181).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_183).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_186).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_187).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_189).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_18).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_190).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_191).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_192).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_193).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_194).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_195).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_196).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_197).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_199).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_19).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_200).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_201).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_202).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_203).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_205).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_207).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_208).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_209).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_20).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_213).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_214).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_216).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_217).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_218).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_219).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_221).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_222).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_223).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_224).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_226).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_228).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_229).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_230).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_233).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_235).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_237).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_238).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_239).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_241).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_242).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_244).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_247).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_248).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_249).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_24).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_252).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_255).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_256).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_257).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_258).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_260).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_262).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_263).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_265).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_266).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_26).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_272).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_273).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_274).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_275).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_277).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_278).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_27).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_280).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_281).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_282).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_283).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_284).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_285).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_286).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_287).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_288).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_289).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_28).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_291).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_292).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_296).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_298).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_302).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_305).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_306).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_307).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_308).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_309).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_30).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_310).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_311).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_315).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_316).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_317).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_318).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_321).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_322).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_323).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_325).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_327).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_331).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_332).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_333).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_335).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_336).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_338).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_339).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_33).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_341).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_342).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_344).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_345).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_348).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_34).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_351).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_353).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_356).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_35).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_360).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_362).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_364).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_365).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_366).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_367).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_368).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_369).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_373).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_374).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_375).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_377).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_378).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_379).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_37).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_382).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_384).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_386).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_389).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_392).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_393).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_394).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_395).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_396).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_397).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_399).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_400).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_401).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_402).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_403).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_404).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_406).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_407).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_409).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_411).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_413).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_414).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_417).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_418).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_419).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_41).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_421).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_424).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_427).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_429).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_42).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_430).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_431).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_432).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_435).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_436).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_437).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_438).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_439).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_43).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_443).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_444).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_446).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_448).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_449).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_44).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_452).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_453).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_454).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_455).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_457).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_458).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_459).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_460).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_462).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_463).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_466).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_467).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_468).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_469).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_470).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_472).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_475).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_477).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_478).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_479).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_47).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_480).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_481).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_482).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_483).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_484).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_485).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_487).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_489).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_490).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_491).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_492).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_493).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_494).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_495).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_496).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_497).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_498).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_4).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_51).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_53).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_54).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_57).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_58).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_5).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_64).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_65).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_66).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_67).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_69).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_70).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_72).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_74).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_76).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_77).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_78).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_80).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_82).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_83).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_84).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_85).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_86).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_87).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_8).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_90).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_92).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_95).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_96).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_97).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_98).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: nzhang_part5 PARTITION(value=val_9).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part6.q.out b/ql/src/test/results/clientpositive/load_dyn_part6.q.out
index 779774a2b8..2d3b804f46 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part6.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part6.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part6
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part6
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part6 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part6 like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part6, dbName:default, owner:null, createTime:1271267698, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part6, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267698}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part6, dbName:default, owner:null, createTime:1279737591, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part6, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737591}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table nzhang_part6 partition (ds="2010-03-03", hr) select key, value, hr from srcpart where ds is not null and hr is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
@@ -46,11 +42,11 @@ POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(sr
 PREHOOK: query: select * from nzhang_part6 where ds = '2010-03-03' and hr = '11'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part6@ds=2010-03-03/hr=11
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-03_332_2957711798932473064/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-54_205_1397414990480966807/10000
 POSTHOOK: query: select * from nzhang_part6 where ds = '2010-03-03' and hr = '11'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part6@ds=2010-03-03/hr=11
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-03_332_2957711798932473064/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-54_205_1397414990480966807/10000
 POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -1058,11 +1054,11 @@ POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(sr
 PREHOOK: query: select * from nzhang_part6 where ds = '2010-03-03' and hr = '12'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part6@ds=2010-03-03/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-03_456_1743903974128155104/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-54_297_2170754306379128750/10000
 POSTHOOK: query: select * from nzhang_part6 where ds = '2010-03-03' and hr = '12'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part6@ds=2010-03-03/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-03_456_1743903974128155104/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-54_297_2170754306379128750/10000
 POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -2067,12 +2063,3 @@ POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(sr
 400	val_400	2010-03-03	12
 200	val_200	2010-03-03	12
 97	val_97	2010-03-03	12
-PREHOOK: query: drop table nzhang_part6
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part6
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part6
-POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part6 PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part7.q.out b/ql/src/test/results/clientpositive/load_dyn_part7.q.out
index 9e37031aae..86eda78ff2 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part7.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part7.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part7
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part7
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part7 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part7 like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part7, dbName:default, owner:null, createTime:1271267706, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part7, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267706}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part7, dbName:default, owner:null, createTime:1279737594, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part7, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737594}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: insert overwrite table nzhang_part7 partition (ds='2010-03-03', hr='12') select key, value from srcpart where ds = '2008-04-08' and hr = '12'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
@@ -45,11 +41,11 @@ ds=2010-03-03/hr=12
 PREHOOK: query: select * from nzhang_part7 where ds is not null and hr is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part7@ds=2010-03-03/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-10_491_3925213404431690499/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-57_928_4887604849335253138/10000
 POSTHOOK: query: select * from nzhang_part7 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part7@ds=2010-03-03/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-10_491_3925213404431690499/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-39-57_928_4887604849335253138/10000
 POSTHOOK: Lineage: nzhang_part7 PARTITION(ds=2010-03-03,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part7 PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 238	val_238	2010-03-03	12
@@ -552,10 +548,3 @@ POSTHOOK: Lineage: nzhang_part7 PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(sr
 400	val_400	2010-03-03	12
 200	val_200	2010-03-03	12
 97	val_97	2010-03-03	12
-PREHOOK: query: drop table nzhang_part7
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part7
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part7
-POSTHOOK: Lineage: nzhang_part7 PARTITION(ds=2010-03-03,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part7 PARTITION(ds=2010-03-03,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part8.q.out b/ql/src/test/results/clientpositive/load_dyn_part8.q.out
index 6246cc9fe7..8bfb074af3 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part8.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part8.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part8
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part8
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part8 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part8 like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part8, dbName:default, owner:null, createTime:1271267713, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part8, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267713}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part8, dbName:default, owner:null, createTime:1279737598, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part8, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737598}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: explain extended
 from srcpart
 insert overwrite table nzhang_part8 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
@@ -69,7 +65,7 @@ STAGE PLANS:
                 File Output Operator
                   compressed: false
                   GlobalTableId: 1
-                  directory: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-13_580_2964480998050044336/10000
+                  directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-39-58_707_2788889105281579650/10000
                   NumFilesPerFileSink: 1
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
@@ -81,13 +77,13 @@ STAGE PLANS:
                         columns.types string:string
                         file.inputformat org.apache.hadoop.mapred.TextInputFormat
                         file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part8
+                        location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part8
                         name nzhang_part8
                         partition_columns ds/hr
                         serialization.ddl struct nzhang_part8 { string key, string value}
                         serialization.format 1
                         serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                        transient_lastDdlTime 1271267713
+                        transient_lastDdlTime 1279737598
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: nzhang_part8
                   TotalFiles: 1
@@ -109,7 +105,7 @@ STAGE PLANS:
                 File Output Operator
                   compressed: false
                   GlobalTableId: 2
-                  directory: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-13_580_2964480998050044336/10002
+                  directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-39-58_707_2788889105281579650/10002
                   NumFilesPerFileSink: 1
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
@@ -121,25 +117,25 @@ STAGE PLANS:
                         columns.types string:string
                         file.inputformat org.apache.hadoop.mapred.TextInputFormat
                         file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part8
+                        location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part8
                         name nzhang_part8
                         partition_columns ds/hr
                         serialization.ddl struct nzhang_part8 { string key, string value}
                         serialization.format 1
                         serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                        transient_lastDdlTime 1271267713
+                        transient_lastDdlTime 1279737598
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: nzhang_part8
                   TotalFiles: 1
                   MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [srcpart]
-        file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 [srcpart]
-        file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 [srcpart]
-        file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 [srcpart]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [srcpart]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 [srcpart]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 [srcpart]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 [srcpart]
       Path -> Partition:
-        file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
             base file name: hr=11
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -153,13 +149,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1271267711
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -170,17 +166,17 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1271267711
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
-        file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
           Partition
             base file name: hr=12
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -194,13 +190,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1271267711
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -211,17 +207,17 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1271267711
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
-        file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
           Partition
             base file name: hr=11
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -235,13 +231,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1271267711
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -252,17 +248,17 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1271267711
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
-        file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
           Partition
             base file name: hr=12
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -276,13 +272,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1271267711
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -293,13 +289,13 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1271267711
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
@@ -311,7 +307,7 @@ STAGE PLANS:
             ds 
             hr 
           replace: true
-          source: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-13_580_2964480998050044336/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-39-58_707_2788889105281579650/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -322,16 +318,16 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part8
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part8
                 name nzhang_part8
                 partition_columns ds/hr
                 serialization.ddl struct nzhang_part8 { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1271267713
+                transient_lastDdlTime 1279737598
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: nzhang_part8
-          tmp directory: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-13_580_2964480998050044336/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-39-58_707_2788889105281579650/10001
 
   Stage: Stage-1
     Move Operator
@@ -340,7 +336,7 @@ STAGE PLANS:
             ds 2008-12-31
             hr 
           replace: true
-          source: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-13_580_2964480998050044336/10002
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-39-58_707_2788889105281579650/10002
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -351,16 +347,16 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part8
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part8
                 name nzhang_part8
                 partition_columns ds/hr
                 serialization.ddl struct nzhang_part8 { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1271267713
+                transient_lastDdlTime 1279737598
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: nzhang_part8
-          tmp directory: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-13_580_2964480998050044336/10003
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-39-58_707_2788889105281579650/10003
 
 
 PREHOOK: query: from srcpart
@@ -413,14 +409,14 @@ PREHOOK: Input: default@nzhang_part8@ds=2008-04-08/hr=11
 PREHOOK: Input: default@nzhang_part8@ds=2008-04-08/hr=12
 PREHOOK: Input: default@nzhang_part8@ds=2008-12-31/hr=11
 PREHOOK: Input: default@nzhang_part8@ds=2008-12-31/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-18_378_7457366475211374753/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-01_856_8428582034149971032/10000
 POSTHOOK: query: select * from nzhang_part8 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part8@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@nzhang_part8@ds=2008-04-08/hr=12
 POSTHOOK: Input: default@nzhang_part8@ds=2008-12-31/hr=11
 POSTHOOK: Input: default@nzhang_part8@ds=2008-12-31/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-18_378_7457366475211374753/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-01_856_8428582034149971032/10000
 POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -2429,16 +2425,3 @@ POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-12-31,hr=12).value SIMPLE [(sr
 400	val_400	2008-12-31	12
 200	val_200	2008-12-31	12
 97	val_97	2008-12-31	12
-PREHOOK: query: drop table nzhang_part8
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part8
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part8
-POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-12-31,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-12-31,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-12-31,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part8 PARTITION(ds=2008-12-31,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/load_dyn_part9.q.out b/ql/src/test/results/clientpositive/load_dyn_part9.q.out
index fdfeaefaf7..22a3848b7f 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part9.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part9.q.out
@@ -6,10 +6,6 @@ ds=2008-04-08/hr=11
 ds=2008-04-08/hr=12
 ds=2008-04-09/hr=11
 ds=2008-04-09/hr=12
-PREHOOK: query: drop table nzhang_part9
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part9
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table if not exists nzhang_part9 like srcpart
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists nzhang_part9 like srcpart
@@ -24,7 +20,7 @@ value	string	default
 ds	string	
 hr	string	
 	 	 
-Detailed Table Information	Table(tableName:nzhang_part9, dbName:default, owner:null, createTime:1271267721, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/data/users/nzhang/work/876/apache-hive/build/ql/test/data/warehouse/nzhang_part9, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE,transient_lastDdlTime=1271267721}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:nzhang_part9, dbName:default, owner:null, createTime:1279737602, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:default), FieldSchema(name:value, type:string, comment:default)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/nzhang_part9, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{EXTERNAL=FALSE, transient_lastDdlTime=1279737602}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: explain
 from srcpart
 insert overwrite table nzhang_part9 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
@@ -119,12 +115,12 @@ PREHOOK: query: select * from nzhang_part9 where ds is not null and hr is not nu
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nzhang_part9@ds=2008-04-08/hr=11
 PREHOOK: Input: default@nzhang_part9@ds=2008-04-08/hr=12
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-25_862_5372560267403711450/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-05_489_7236996026599225380/10000
 POSTHOOK: query: select * from nzhang_part9 where ds is not null and hr is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nzhang_part9@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@nzhang_part9@ds=2008-04-08/hr=12
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-14_10-55-25_862_5372560267403711450/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-05_489_7236996026599225380/10000
 POSTHOOK: Lineage: nzhang_part9 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part9 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: nzhang_part9 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -1129,12 +1125,3 @@ POSTHOOK: Lineage: nzhang_part9 PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(sr
 400	val_400	2008-04-08	12
 200	val_200	2008-04-08	12
 97	val_97	2008-04-08	12
-PREHOOK: query: drop table nzhang_part9
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nzhang_part9
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nzhang_part9
-POSTHOOK: Lineage: nzhang_part9 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part9 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part9 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: nzhang_part9 PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/loadpart1.q.out b/ql/src/test/results/clientpositive/loadpart1.q.out
index 603e77c3ec..5785946bb8 100644
--- a/ql/src/test/results/clientpositive/loadpart1.q.out
+++ b/ql/src/test/results/clientpositive/loadpart1.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table hive_test_src
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_test_src
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table hive_test_dst
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_test_dst
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table hive_test_src ( col1 string ) stored as textfile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table hive_test_src ( col1 string ) stored as textfile
@@ -33,11 +25,11 @@ POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1
 PREHOOK: query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_Part'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-27_161_4058867181423672724/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-08_582_5840765576374175576/10000
 POSTHOOK: query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_Part'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-27_161_4058867181423672724/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-08_582_5840765576374175576/10000
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
 1	test_part	test_Part
 2	test_part	test_Part
@@ -57,20 +49,20 @@ POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
 PREHOOK: query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_part'
 PREHOOK: type: QUERY
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-32_005_7262990256756311796/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-11_329_4921545927597783630/10000
 POSTHOOK: query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_part'
 POSTHOOK: type: QUERY
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-32_005_7262990256756311796/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-11_329_4921545927597783630/10000
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
 PREHOOK: query: select * from hive_test_dst where pcol1='test_part'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-32_055_1756029915176080993/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-11_375_7311986483705850817/10000
 POSTHOOK: query: select * from hive_test_dst where pcol1='test_part'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-32_055_1756029915176080993/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-11_375_7311986483705850817/10000
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
 1	test_part	test_Part
@@ -81,31 +73,17 @@ POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1
 6	test_part	test_Part
 PREHOOK: query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_part'
 PREHOOK: type: QUERY
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-32_130_4753679815952473777/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-11_440_6906366422294316277/10000
 POSTHOOK: query: select * from hive_test_dst where pcol1='test_part' and pcol2='test_part'
 POSTHOOK: type: QUERY
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-32_130_4753679815952473777/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-11_440_6906366422294316277/10000
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
 PREHOOK: query: select * from hive_test_dst where pcol1='test_Part'
 PREHOOK: type: QUERY
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-32_181_3514948387985518789/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-11_474_301584563410557410/10000
 POSTHOOK: query: select * from hive_test_dst where pcol1='test_Part'
 POSTHOOK: type: QUERY
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-18-32_181_3514948387985518789/10000
-POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
-POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
-PREHOOK: query: drop table hive_test_src
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_test_src
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@hive_test_src
-POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
-POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
-PREHOOK: query: drop table hive_test_dst
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hive_test_dst
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@hive_test_dst
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-40-11_474_301584563410557410/10000
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
 POSTHOOK: Lineage: hive_test_dst PARTITION(pcol1=test_part,pcol2=test_Part).col1 SIMPLE [(hive_test_src)hive_test_src.FieldSchema(name:col1, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/loadpart_err.q.out b/ql/src/test/results/clientpositive/loadpart_err.q.out
index bf2ac02d53..2e2ba7f13f 100644
--- a/ql/src/test/results/clientpositive/loadpart_err.q.out
+++ b/ql/src/test/results/clientpositive/loadpart_err.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE loadpart1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE loadpart1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE loadpart1(a STRING, b STRING) PARTITIONED BY (ds STRING)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE loadpart1(a STRING, b STRING) PARTITIONED BY (ds STRING)
@@ -25,13 +21,8 @@ PREHOOK: query: SHOW PARTITIONS loadpart1
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: SHOW PARTITIONS loadpart1
 POSTHOOK: type: SHOWPARTITIONS
-FAILED: Error in semantic analysis: line 3:23 Invalid Path '../data1/files/kv1.txt': No files matching path file:/data/users/njain/hive5/hive5/data1/files/kv1.txt
+FAILED: Error in semantic analysis: line 3:23 Invalid Path '../data1/files/kv1.txt': No files matching path file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/data1/files/kv1.txt
 PREHOOK: query: SHOW PARTITIONS loadpart1
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: SHOW PARTITIONS loadpart1
 POSTHOOK: type: SHOWPARTITIONS
-PREHOOK: query: DROP TABLE loadpart1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE loadpart1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@loadpart1
diff --git a/ql/src/test/results/clientpositive/merge1.q.out b/ql/src/test/results/clientpositive/merge1.q.out
index 5188ca41b3..f0cdd0eaa8 100644
--- a/ql/src/test/results/clientpositive/merge1.q.out
+++ b/ql/src/test/results/clientpositive/merge1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table dest1(key int, val int)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dest1(key int, val int)
@@ -98,7 +94,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-19-12_103_8509010210619285541/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-01-35_891_8961223626140768169/10000
 
   Stage: Stage-0
     Move Operator
@@ -113,7 +109,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-19-12_103_8509010210619285541/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-01-35_891_8961223626140768169/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -152,11 +148,11 @@ POSTHOOK: Lineage: dest1.val EXPRESSION [(src)src.null, ]
 PREHOOK: query: select * from dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-19-16_584_5540363340401862209/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-01-38_782_3169792692854776166/10000
 POSTHOOK: query: select * from dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-19-16_584_5540363340401862209/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-01-38_782_3169792692854776166/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.val EXPRESSION [(src)src.null, ]
 0	3
@@ -565,7 +561,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-19-26_073_2981993365400111398/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-01-44_324_4141481189577468/10000
 
   Stage: Stage-0
     Move Operator
@@ -580,7 +576,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-19-26_073_2981993365400111398/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-01-44_324_4141481189577468/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -670,7 +666,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-19-35_067_370016273932178726/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-01-49_887_6167797302674604272/10000
 
   Stage: Stage-0
     Move Operator
@@ -685,7 +681,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-19-35_067_370016273932178726/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-01-49_887_6167797302674604272/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -725,29 +721,3 @@ POSTHOOK: Lineage: test_src PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(n
 POSTHOOK: Lineage: test_src PARTITION(ds=101).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: test_src PARTITION(ds=102).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: test_src PARTITION(ds=102).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table test_src
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table test_src
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@test_src
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key SIMPLE [(test_src)test_src.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: dest1.key SIMPLE [(test_src)test_src.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: dest1.val EXPRESSION [(src)src.null, ]
-POSTHOOK: Lineage: test_src PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: test_src PARTITION(ds=101).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: test_src PARTITION(ds=102).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: test_src PARTITION(ds=102).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key SIMPLE [(test_src)test_src.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: dest1.key SIMPLE [(test_src)test_src.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: dest1.val EXPRESSION [(src)src.null, ]
-POSTHOOK: Lineage: test_src PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: test_src PARTITION(ds=101).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: test_src PARTITION(ds=102).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: test_src PARTITION(ds=102).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/multi_insert.q.out b/ql/src/test/results/clientpositive/multi_insert.q.out
index e6f85bb5d1..2f030750b0 100644
--- a/ql/src/test/results/clientpositive/multi_insert.q.out
+++ b/ql/src/test/results/clientpositive/multi_insert.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table src_multi1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table src_multi1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table src_multi2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table src_multi2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table src_multi1 like src
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table src_multi1 like src
@@ -122,11 +114,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-34_250_2581601890579997147/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-21_712_5062335963199774462/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-34_250_2581601890579997147/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-21_712_5062335963199774462/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -144,11 +136,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-38_695_3600146020464319543/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-24_226_5046921944761310383/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-38_695_3600146020464319543/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-24_226_5046921944761310383/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -243,7 +235,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-43_573_8372259879569928385/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-41-26_695_7647953576858754386/10000
 
   Stage: Stage-0
     Move Operator
@@ -258,7 +250,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-43_573_8372259879569928385/10004 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-41-26_695_7647953576858754386/10004 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -288,7 +280,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-43_573_8372259879569928385/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-41-26_695_7647953576858754386/10002
 
   Stage: Stage-1
     Move Operator
@@ -303,7 +295,7 @@ STAGE PLANS:
   Stage: Stage-6
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-43_573_8372259879569928385/10005 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-41-26_695_7647953576858754386/10005 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -352,11 +344,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-49_456_7216820125442151889/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-29_398_374250277965899089/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-49_456_7216820125442151889/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-29_398_374250277965899089/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -378,11 +370,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-53_629_1156332665615404428/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-31_972_5943245082125077624/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-18-53_629_1156332665615404428/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-31_972_5943245082125077624/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -522,11 +514,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-03_446_7449215537744570078/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-37_232_5055054932535323703/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-03_446_7449215537744570078/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-37_232_5055054932535323703/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -552,11 +544,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-09_158_6457821197220351304/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-39_724_5919666556486138657/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-09_158_6457821197220351304/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-39_724_5919666556486138657/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -667,7 +659,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-14_113_8938827832591803037/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-41-42_204_3144075225672991179/10000
 
   Stage: Stage-0
     Move Operator
@@ -682,7 +674,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-14_113_8938827832591803037/10004 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-41-42_204_3144075225672991179/10004 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -712,7 +704,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-14_113_8938827832591803037/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-41-42_204_3144075225672991179/10002
 
   Stage: Stage-1
     Move Operator
@@ -727,7 +719,7 @@ STAGE PLANS:
   Stage: Stage-6
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-14_113_8938827832591803037/10005 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-41-42_204_3144075225672991179/10005 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -784,11 +776,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-19_486_1592932188566236916/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-44_892_3027065579813818242/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-19_486_1592932188566236916/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-44_892_3027065579813818242/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -818,11 +810,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-23_513_9027380053403100599/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-47_356_9057856219474591094/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-23_513_9027380053403100599/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-47_356_9057856219474591094/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -988,7 +980,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-27_834_3545130340055692875/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-41-49_873_5452620989462977001/10004 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -1076,11 +1068,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-37_004_7588365783973531047/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-55_158_3039517705245630432/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-37_004_7588365783973531047/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-55_158_3039517705245630432/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1110,11 +1102,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-41_063_7955882570645843879/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-57_720_498471983118793979/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-41_063_7955882570645843879/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-41-57_720_498471983118793979/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1285,7 +1277,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-44_979_7633672053254257151/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-00_181_6805544255394938954/10000
 
   Stage: Stage-0
     Move Operator
@@ -1300,7 +1292,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-44_979_7633672053254257151/10004 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-00_181_6805544255394938954/10004 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -1326,7 +1318,7 @@ STAGE PLANS:
   Stage: Stage-6
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-44_979_7633672053254257151/10005 
+        file:/tmp/jssarma/hive_2010-07-21_11-42-00_181_6805544255394938954/10005 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -1373,7 +1365,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-44_979_7633672053254257151/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-00_181_6805544255394938954/10002
 
   Stage: Stage-1
     Move Operator
@@ -1388,7 +1380,7 @@ STAGE PLANS:
   Stage: Stage-7
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-44_979_7633672053254257151/10006 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-00_181_6805544255394938954/10006 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -1453,11 +1445,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-53_503_2476681795898723280/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-05_457_2938335370779332907/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-53_503_2476681795898723280/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-05_457_2938335370779332907/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1491,11 +1483,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-57_919_4446429596950029445/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-07_979_1197234407834858516/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-19-57_919_4446429596950029445/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-07_979_1197234407834858516/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1674,7 +1666,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-02_655_8851495220285705466/10004 
+        file:/tmp/jssarma/hive_2010-07-21_11-42-10_540_8287802089011554187/10004 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -1770,11 +1762,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-14_001_2137537666402080168/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-15_956_8233613409144913424/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-14_001_2137537666402080168/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-15_956_8233613409144913424/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1812,11 +1804,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-18_582_608052595593605574/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-18_525_6028070244185370122/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-18_582_608052595593605574/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-18_525_6028070244185370122/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -2003,7 +1995,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-22_841_8682732744441489601/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-21_003_5167136917621088373/10000
 
   Stage: Stage-0
     Move Operator
@@ -2018,7 +2010,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-22_841_8682732744441489601/10004 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-21_003_5167136917621088373/10004 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -2044,7 +2036,7 @@ STAGE PLANS:
   Stage: Stage-6
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-22_841_8682732744441489601/10005 
+        file:/tmp/jssarma/hive_2010-07-21_11-42-21_003_5167136917621088373/10005 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -2091,7 +2083,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-22_841_8682732744441489601/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-21_003_5167136917621088373/10002
 
   Stage: Stage-1
     Move Operator
@@ -2106,7 +2098,7 @@ STAGE PLANS:
   Stage: Stage-7
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-22_841_8682732744441489601/10006 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-21_003_5167136917621088373/10006 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -2179,11 +2171,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-32_932_5517499324893028633/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-26_240_3559046036845532/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-32_932_5517499324893028633/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-26_240_3559046036845532/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -2225,11 +2217,11 @@ POSTHOOK: Lineage: src_multi2.value SIMPLE [(src)src.FieldSchema(name:value, typ
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-37_148_3376220707415365030/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-28_796_451544540094033007/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-37_148_3376220707415365030/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-28_796_451544540094033007/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -2495,11 +2487,11 @@ POSTHOOK: Lineage: src_multi2.value EXPRESSION [(src)src.FieldSchema(name:value,
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-45_663_5087600964304715840/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-34_097_7757454628091368486/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-45_663_5087600964304715840/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-34_097_7757454628091368486/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -2559,11 +2551,11 @@ POSTHOOK: Lineage: src_multi2.value EXPRESSION [(src)src.FieldSchema(name:value,
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-49_404_8405829546880992933/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-36_786_8996732990789151375/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-49_404_8405829546880992933/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-36_786_8996732990789151375/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -2788,7 +2780,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-53_509_6333015869581777161/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-39_255_2282025271684185143/10000
 
   Stage: Stage-0
     Move Operator
@@ -2803,7 +2795,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-53_509_6333015869581777161/10004 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-39_255_2282025271684185143/10004 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -2833,7 +2825,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-53_509_6333015869581777161/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-39_255_2282025271684185143/10002
 
   Stage: Stage-1
     Move Operator
@@ -2848,7 +2840,7 @@ STAGE PLANS:
   Stage: Stage-6
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-53_509_6333015869581777161/10005 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-39_255_2282025271684185143/10005 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -2929,11 +2921,11 @@ POSTHOOK: Lineage: src_multi2.value EXPRESSION [(src)src.FieldSchema(name:value,
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-58_931_9046309055063469540/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-42_021_1773614549788907879/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-20-58_931_9046309055063469540/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-42_021_1773614549788907879/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -2997,11 +2989,11 @@ POSTHOOK: Lineage: src_multi2.value EXPRESSION [(src)src.FieldSchema(name:value,
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-02_917_8039712053222808218/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-44_498_2123904018906456758/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-02_917_8039712053222808218/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-44_498_2123904018906456758/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -3303,11 +3295,11 @@ POSTHOOK: Lineage: src_multi2.value EXPRESSION [(src)src.FieldSchema(name:value,
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-26_449_7713657971977228391/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-49_805_7878050643299431709/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-26_449_7713657971977228391/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-49_805_7878050643299431709/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -3375,11 +3367,11 @@ POSTHOOK: Lineage: src_multi2.value EXPRESSION [(src)src.FieldSchema(name:value,
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-30_393_5944254380728218238/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-52_268_1999896450202641016/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-30_393_5944254380728218238/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-52_268_1999896450202641016/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -3620,7 +3612,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-34_989_7490461749283801722/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-54_799_7180997344452135463/10000
 
   Stage: Stage-0
     Move Operator
@@ -3635,7 +3627,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-34_989_7490461749283801722/10004 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-54_799_7180997344452135463/10004 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -3665,7 +3657,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-34_989_7490461749283801722/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-54_799_7180997344452135463/10002
 
   Stage: Stage-1
     Move Operator
@@ -3680,7 +3672,7 @@ STAGE PLANS:
   Stage: Stage-6
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-34_989_7490461749283801722/10005 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-42-54_799_7180997344452135463/10005 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -3769,11 +3761,11 @@ POSTHOOK: Lineage: src_multi2.value EXPRESSION [(src)src.FieldSchema(name:value,
 PREHOOK: query: select * from src_multi1 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-40_465_8017594571024915624/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-57_592_4817069631526155036/10000
 POSTHOOK: query: select * from src_multi1 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-40_465_8017594571024915624/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-42-57_592_4817069631526155036/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -3845,11 +3837,11 @@ POSTHOOK: Lineage: src_multi2.value EXPRESSION [(src)src.FieldSchema(name:value,
 PREHOOK: query: select * from src_multi2 order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_multi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-43_966_4724737620338694550/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-00_113_6879485217176937186/10000
 POSTHOOK: query: select * from src_multi2 order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_multi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-21-43_966_4724737620338694550/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-00_113_6879485217176937186/10000
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src_multi1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/null_column.q.out b/ql/src/test/results/clientpositive/null_column.q.out
index 0628e87b58..461fbc20ab 100644
--- a/ql/src/test/results/clientpositive/null_column.q.out
+++ b/ql/src/test/results/clientpositive/null_column.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table temp_null
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table temp_null
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table tt
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tt
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table tt_b
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tt_b
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table temp_null(a int) stored as textfile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table temp_null(a int) stored as textfile
@@ -23,11 +11,11 @@ POSTHOOK: Output: default@temp_null
 PREHOOK: query: select null, null from temp_null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@temp_null
-PREHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-11-46_460_130526701412066561/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-29_063_5497458015896967287/10000
 POSTHOOK: query: select null, null from temp_null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@temp_null
-POSTHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-11-46_460_130526701412066561/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-29_063_5497458015896967287/10000
 NULL	NULL
 NULL	NULL
 NULL	NULL
@@ -52,11 +40,11 @@ POSTHOOK: Lineage: tt.b SIMPLE []
 PREHOOK: query: select * from tt
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tt
-PREHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-11-54_210_4646908468388375810/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-34_033_7899160687995145341/10000
 POSTHOOK: query: select * from tt
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tt
-POSTHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-11-54_210_4646908468388375810/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-34_033_7899160687995145341/10000
 POSTHOOK: Lineage: tt.a EXPRESSION []
 POSTHOOK: Lineage: tt.b SIMPLE []
 NULL	NULL
@@ -87,11 +75,11 @@ POSTHOOK: Lineage: tt_b.b EXPRESSION []
 PREHOOK: query: select * from tt_b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tt_b
-PREHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-11-58_088_4831793622839318296/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-36_556_8509051297680496039/10000
 POSTHOOK: query: select * from tt_b
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tt_b
-POSTHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-11-58_088_4831793622839318296/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-36_556_8509051297680496039/10000
 POSTHOOK: Lineage: tt.a EXPRESSION []
 POSTHOOK: Lineage: tt.b SIMPLE []
 POSTHOOK: Lineage: tt_b.a EXPRESSION []
@@ -120,14 +108,6 @@ POSTHOOK: Lineage: tt_b.b EXPRESSION []
 \N\N
 \N\N
 \N\N
-PREHOOK: query: drop table temp_null2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table temp_null2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: tt.a EXPRESSION []
-POSTHOOK: Lineage: tt.b SIMPLE []
-POSTHOOK: Lineage: tt_b.a EXPRESSION []
-POSTHOOK: Lineage: tt_b.b EXPRESSION []
 PREHOOK: query: create table temp_null2 (key string, value string) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table temp_null2 (key string, value string) partitioned by (ds string)
@@ -154,11 +134,11 @@ POSTHOOK: Lineage: tt_b.b EXPRESSION []
 PREHOOK: query: select * from temp_null2 where ds is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@temp_null2@ds=2010-04-01
-PREHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-12-05_597_2273874996761576508/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-41_617_3166554512331390680/10000
 POSTHOOK: query: select * from temp_null2 where ds is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@temp_null2@ds=2010-04-01
-POSTHOOK: Output: file:/data/users/nzhang/work/784/apache-hive/build/ql/scratchdir/hive_2010-05-11_12-12-05_597_2273874996761576508/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-43-41_617_3166554512331390680/10000
 POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).key SIMPLE []
 POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).value SIMPLE []
 POSTHOOK: Lineage: tt.a EXPRESSION []
@@ -166,47 +146,3 @@ POSTHOOK: Lineage: tt.b SIMPLE []
 POSTHOOK: Lineage: tt_b.a EXPRESSION []
 POSTHOOK: Lineage: tt_b.b EXPRESSION []
 1	NULL	2010-04-01
-PREHOOK: query: drop table tt
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tt
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tt
-POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).key SIMPLE []
-POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).value SIMPLE []
-POSTHOOK: Lineage: tt.a EXPRESSION []
-POSTHOOK: Lineage: tt.b SIMPLE []
-POSTHOOK: Lineage: tt_b.a EXPRESSION []
-POSTHOOK: Lineage: tt_b.b EXPRESSION []
-PREHOOK: query: drop table tt_b
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tt_b
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tt_b
-POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).key SIMPLE []
-POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).value SIMPLE []
-POSTHOOK: Lineage: tt.a EXPRESSION []
-POSTHOOK: Lineage: tt.b SIMPLE []
-POSTHOOK: Lineage: tt_b.a EXPRESSION []
-POSTHOOK: Lineage: tt_b.b EXPRESSION []
-PREHOOK: query: drop table temp_null
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table temp_null
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@temp_null
-POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).key SIMPLE []
-POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).value SIMPLE []
-POSTHOOK: Lineage: tt.a EXPRESSION []
-POSTHOOK: Lineage: tt.b SIMPLE []
-POSTHOOK: Lineage: tt_b.a EXPRESSION []
-POSTHOOK: Lineage: tt_b.b EXPRESSION []
-PREHOOK: query: drop table temp_null2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table temp_null2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@temp_null2
-POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).key SIMPLE []
-POSTHOOK: Lineage: temp_null2 PARTITION(ds=2010-04-01).value SIMPLE []
-POSTHOOK: Lineage: tt.a EXPRESSION []
-POSTHOOK: Lineage: tt.b SIMPLE []
-POSTHOOK: Lineage: tt_b.a EXPRESSION []
-POSTHOOK: Lineage: tt_b.b EXPRESSION []
diff --git a/ql/src/test/results/clientpositive/nullgroup3.q.out b/ql/src/test/results/clientpositive/nullgroup3.q.out
index a983785f67..23daa17696 100644
--- a/ql/src/test/results/clientpositive/nullgroup3.q.out
+++ b/ql/src/test/results/clientpositive/nullgroup3.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE tstparttbl
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tstparttbl
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
@@ -78,17 +74,13 @@ PREHOOK: query: select count(1) from tstparttbl
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tstparttbl@ds=2008-04-08
 PREHOOK: Input: default@tstparttbl@ds=2008-04-09
-PREHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/429292939/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-11_573_4565213151672229951/10000
 POSTHOOK: query: select count(1) from tstparttbl
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tstparttbl@ds=2008-04-08
 POSTHOOK: Input: default@tstparttbl@ds=2008-04-09
-POSTHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/429292939/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-11_573_4565213151672229951/10000
 500
-PREHOOK: query: DROP TABLE tstparttbl2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tstparttbl2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
@@ -165,12 +157,12 @@ PREHOOK: query: select count(1) from tstparttbl2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tstparttbl2@ds=2008-04-08
 PREHOOK: Input: default@tstparttbl2@ds=2008-04-09
-PREHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/675788842/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-14_641_615139224470628595/10000
 POSTHOOK: query: select count(1) from tstparttbl2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tstparttbl2@ds=2008-04-08
 POSTHOOK: Input: default@tstparttbl2@ds=2008-04-09
-POSTHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/675788842/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-14_641_615139224470628595/10000
 0
 PREHOOK: query: DROP TABLE tstparttbl
 PREHOOK: type: DROPTABLE
@@ -253,12 +245,12 @@ PREHOOK: query: select count(1) from tstparttbl
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tstparttbl@ds=2008-04-08
 PREHOOK: Input: default@tstparttbl@ds=2008-04-09
-PREHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/1506435272/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-17_697_2205434267515314781/10000
 POSTHOOK: query: select count(1) from tstparttbl
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tstparttbl@ds=2008-04-08
 POSTHOOK: Input: default@tstparttbl@ds=2008-04-09
-POSTHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/1506435272/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-17_697_2205434267515314781/10000
 500
 PREHOOK: query: DROP TABLE tstparttbl2
 PREHOOK: type: DROPTABLE
@@ -341,20 +333,10 @@ PREHOOK: query: select count(1) from tstparttbl2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tstparttbl2@ds=2008-04-08
 PREHOOK: Input: default@tstparttbl2@ds=2008-04-09
-PREHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/1727986109/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-20_708_4976141185424807073/10000
 POSTHOOK: query: select count(1) from tstparttbl2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tstparttbl2@ds=2008-04-08
 POSTHOOK: Input: default@tstparttbl2@ds=2008-04-09
-POSTHOOK: Output: file:/data/users/heyongqiang/hive-trunk-commit/build/ql/tmp/1727986109/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-20_708_4976141185424807073/10000
 0
-PREHOOK: query: DROP TABLE tstparttbl
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tstparttbl
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tstparttbl
-PREHOOK: query: DROP TABLE tstparttbl2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tstparttbl2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tstparttbl2
diff --git a/ql/src/test/results/clientpositive/nullgroup5.q.out b/ql/src/test/results/clientpositive/nullgroup5.q.out
index e0c1453e38..be81975ba6 100644
--- a/ql/src/test/results/clientpositive/nullgroup5.q.out
+++ b/ql/src/test/results/clientpositive/nullgroup5.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE tstparttbl
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tstparttbl
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
@@ -12,10 +8,6 @@ PREHOOK: type: LOAD
 POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE tstparttbl PARTITION (ds='2009-04-09')
 POSTHOOK: type: LOAD
 POSTHOOK: Output: default@tstparttbl@ds=2009-04-09
-PREHOOK: query: DROP TABLE tstparttbl2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tstparttbl2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
@@ -131,7 +123,7 @@ PREHOOK: query: select u.* from
 )u
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tstparttbl2@ds=2009-04-09
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/379898854/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-39_608_1509511246265009929/10000
 POSTHOOK: query: select u.* from
 (
   select key, value from tstparttbl x where x.ds='2009-04-05'
@@ -140,7 +132,7 @@ POSTHOOK: query: select u.* from
 )u
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tstparttbl2@ds=2009-04-09
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/379898854/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-39_608_1509511246265009929/10000
 238	val_238
 86	val_86
 311	val_311
@@ -641,13 +633,3 @@ POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/379898854/1000
 400	val_400
 200	val_200
 97	val_97
-PREHOOK: query: DROP TABLE tstparttbl
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tstparttbl
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tstparttbl
-PREHOOK: query: DROP TABLE tstparttbl2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tstparttbl2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tstparttbl2
diff --git a/ql/src/test/results/clientpositive/nullinput.q.out b/ql/src/test/results/clientpositive/nullinput.q.out
index 9ca843dd17..46d79bc010 100644
--- a/ql/src/test/results/clientpositive/nullinput.q.out
+++ b/ql/src/test/results/clientpositive/nullinput.q.out
@@ -6,21 +6,16 @@ POSTHOOK: Output: default@tstnullinut
 PREHOOK: query: select x.* from tstnullinut x
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tstnullinut
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1976338873/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-42_572_4519312906185121533/10000
 POSTHOOK: query: select x.* from tstnullinut x
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tstnullinut
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1976338873/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-42_572_4519312906185121533/10000
 PREHOOK: query: select x.a, count(1) from tstnullinut x group by x.a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tstnullinut
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1008263015/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-42_620_1570615621828082286/10000
 POSTHOOK: query: select x.a, count(1) from tstnullinut x group by x.a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tstnullinut
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1008263015/10000
-PREHOOK: query: drop table tstnullinut
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tstnullinut
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tstnullinut
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-42_620_1570615621828082286/10000
diff --git a/ql/src/test/results/clientpositive/nullinput2.q.out b/ql/src/test/results/clientpositive/nullinput2.q.out
index 84007a3888..77b43e107e 100644
--- a/ql/src/test/results/clientpositive/nullinput2.q.out
+++ b/ql/src/test/results/clientpositive/nullinput2.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table nulltbl
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nulltbl
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table nulltbl(key int) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table nulltbl(key int) partitioned by (ds string)
@@ -9,19 +5,14 @@ POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@nulltbl
 PREHOOK: query: select key from nulltbl where ds='101'
 PREHOOK: type: QUERY
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/1897224698/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-45_538_2037787558373086452/10000
 POSTHOOK: query: select key from nulltbl where ds='101'
 POSTHOOK: type: QUERY
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/1897224698/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-45_538_2037787558373086452/10000
 PREHOOK: query: select count(1) from nulltbl where ds='101'
 PREHOOK: type: QUERY
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/1803910053/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-48_052_5788292989741758557/10000
 POSTHOOK: query: select count(1) from nulltbl where ds='101'
 POSTHOOK: type: QUERY
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/1803910053/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-48_052_5788292989741758557/10000
 0
-PREHOOK: query: drop table nulltbl
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table nulltbl
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@nulltbl
diff --git a/ql/src/test/results/clientpositive/nullscript.q.out b/ql/src/test/results/clientpositive/nullscript.q.out
index 73814bca8d..f11b1eb7a8 100644
--- a/ql/src/test/results/clientpositive/nullscript.q.out
+++ b/ql/src/test/results/clientpositive/nullscript.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE nullscript
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE nullscript
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE nullscript(KEY STRING, VALUE STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE nullscript(KEY STRING, VALUE STRING) STORED AS TEXTFILE
@@ -62,11 +58,11 @@ STAGE PLANS:
 PREHOOK: query: select transform(key) using '/bin/cat' as key1 from nullscript
 PREHOOK: type: QUERY
 PREHOOK: Input: default@nullscript
-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/109237316/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-51_131_1687271560149359862/10000
 POSTHOOK: query: select transform(key) using '/bin/cat' as key1 from nullscript
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@nullscript
-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/109237316/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-44-51_131_1687271560149359862/10000
 238
 86
 311
diff --git a/ql/src/test/results/clientpositive/partition_vs_table_metadata.q.out b/ql/src/test/results/clientpositive/partition_vs_table_metadata.q.out
index 6f8aca9629..3e673e5db7 100644
--- a/ql/src/test/results/clientpositive/partition_vs_table_metadata.q.out
+++ b/ql/src/test/results/clientpositive/partition_vs_table_metadata.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table partition_vs_table
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table partition_vs_table
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table partition_vs_table(key string, value string) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table partition_vs_table(key string, value string) partitioned by (ds string)
@@ -43,13 +39,13 @@ order by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_vs_table@ds=100
 PREHOOK: Input: default@partition_vs_table@ds=101
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-23-19_287_8020563381793175788/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-18_342_8996179008267998708/10000
 POSTHOOK: query: select key, value, newcol from partition_vs_table
 order by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_vs_table@ds=100
 POSTHOOK: Input: default@partition_vs_table@ds=101
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-23-19_287_8020563381793175788/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-18_342_8996179008267998708/10000
 POSTHOOK: Lineage: partition_vs_table PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_vs_table PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_vs_table PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1055,13 +1051,3 @@ POSTHOOK: Lineage: partition_vs_table PARTITION(ds=101).value SIMPLE [(src)src.F
 98	val_98	NULL
 98	val_98	98
 98	val_98	98
-PREHOOK: query: drop table partition_vs_table
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table partition_vs_table
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@partition_vs_table
-POSTHOOK: Lineage: partition_vs_table PARTITION(ds=100).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_vs_table PARTITION(ds=100).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_vs_table PARTITION(ds=101).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_vs_table PARTITION(ds=101).newcol SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_vs_table PARTITION(ds=101).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/partition_wise_fileformat.q.out b/ql/src/test/results/clientpositive/partition_wise_fileformat.q.out
index 006b8b80cb..ee30b79fe1 100644
--- a/ql/src/test/results/clientpositive/partition_wise_fileformat.q.out
+++ b/ql/src/test/results/clientpositive/partition_wise_fileformat.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table partition_test_partitioned
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table partition_test_partitioned
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table partition_test_partitioned(key string, value string) partitioned by (dt string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table partition_test_partitioned(key string, value string) partitioned by (dt string)
@@ -24,8 +20,8 @@ POSTHOOK: type: SHOW_TABLESTATUS
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/partition_test_partitioned
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned
 inputformat:org.apache.hadoop.mapred.TextInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
 columns:struct columns { string key, string value}
@@ -36,7 +32,7 @@ totalFileSize:216
 maxFileSize:216
 minFileSize:216
 lastAccessTime:0
-lastUpdateTime:1272566036000
+lastUpdateTime:1279737923000
 
 PREHOOK: query: show table extended like partition_test_partitioned partition(dt=100)
 PREHOOK: type: SHOW_TABLESTATUS
@@ -45,8 +41,8 @@ POSTHOOK: type: SHOW_TABLESTATUS
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/partition_test_partitioned/dt=100
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned/dt=100
 inputformat:org.apache.hadoop.mapred.TextInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
 columns:struct columns { string key, string value}
@@ -57,16 +53,16 @@ totalFileSize:216
 maxFileSize:216
 minFileSize:216
 lastAccessTime:0
-lastUpdateTime:1272566036000
+lastUpdateTime:1279737923000
 
 PREHOOK: query: select key from partition_test_partitioned where dt=100
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=100
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-33-56_574_6496604876539248967/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-23_982_7698604976741917428/10000
 POSTHOOK: query: select key from partition_test_partitioned where dt=100
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=100
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-33-56_574_6496604876539248967/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-23_982_7698604976741917428/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 238
@@ -97,11 +93,11 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(s
 PREHOOK: query: select key from partition_test_partitioned
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=100
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-00_180_3082218449482865639/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-26_455_7900841763716004103/10000
 POSTHOOK: query: select key from partition_test_partitioned
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=100
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-00_180_3082218449482865639/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-26_455_7900841763716004103/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 238
@@ -158,8 +154,8 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(s
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/partition_test_partitioned
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned
 inputformat:org.apache.hadoop.hive.ql.io.RCFileInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat
 columns:struct columns { string key, string value}
@@ -170,7 +166,7 @@ totalFileSize:586
 maxFileSize:370
 minFileSize:216
 lastAccessTime:0
-lastUpdateTime:1272566047000
+lastUpdateTime:1279737931000
 
 PREHOOK: query: show table extended like partition_test_partitioned partition(dt=100)
 PREHOOK: type: SHOW_TABLESTATUS
@@ -181,8 +177,8 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(s
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/partition_test_partitioned/dt=100
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned/dt=100
 inputformat:org.apache.hadoop.mapred.TextInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
 columns:struct columns { string key, string value}
@@ -193,7 +189,7 @@ totalFileSize:216
 maxFileSize:216
 minFileSize:216
 lastAccessTime:0
-lastUpdateTime:1272566047000
+lastUpdateTime:1279737931000
 
 PREHOOK: query: show table extended like partition_test_partitioned partition(dt=101)
 PREHOOK: type: SHOW_TABLESTATUS
@@ -204,8 +200,8 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(s
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/partition_test_partitioned/dt=101
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned/dt=101
 inputformat:org.apache.hadoop.hive.ql.io.RCFileInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat
 columns:struct columns { string key, string value}
@@ -216,16 +212,16 @@ totalFileSize:370
 maxFileSize:370
 minFileSize:370
 lastAccessTime:0
-lastUpdateTime:1272566047000
+lastUpdateTime:1279737931000
 
 PREHOOK: query: select key from partition_test_partitioned where dt=100
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=100
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-08_064_4581555677801824990/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-31_748_8367411748103278779/10000
 POSTHOOK: query: select key from partition_test_partitioned where dt=100
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=100
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-08_064_4581555677801824990/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-31_748_8367411748103278779/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -258,11 +254,11 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(s
 PREHOOK: query: select key from partition_test_partitioned where dt=101
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=101
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-11_564_6764711767035125433/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-34_225_2802204195313608860/10000
 POSTHOOK: query: select key from partition_test_partitioned where dt=101
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=101
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-11_564_6764711767035125433/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-34_225_2802204195313608860/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -296,12 +292,12 @@ PREHOOK: query: select key from partition_test_partitioned
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=100
 PREHOOK: Input: default@partition_test_partitioned@dt=101
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-15_008_8542909916045321913/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-36_701_4909943953812237318/10000
 POSTHOOK: query: select key from partition_test_partitioned
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=100
 POSTHOOK: Input: default@partition_test_partitioned@dt=101
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-15_008_8542909916045321913/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-36_701_4909943953812237318/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -391,8 +387,8 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(s
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/partition_test_partitioned
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned
 inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
 columns:struct columns { string key, string value}
@@ -403,7 +399,7 @@ totalFileSize:1474
 maxFileSize:888
 minFileSize:216
 lastAccessTime:0
-lastUpdateTime:1272566061000
+lastUpdateTime:1279737941000
 
 PREHOOK: query: show table extended like partition_test_partitioned partition(dt=100)
 PREHOOK: type: SHOW_TABLESTATUS
@@ -416,8 +412,8 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(s
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/partition_test_partitioned/dt=100
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned/dt=100
 inputformat:org.apache.hadoop.mapred.TextInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
 columns:struct columns { string key, string value}
@@ -428,7 +424,7 @@ totalFileSize:216
 maxFileSize:216
 minFileSize:216
 lastAccessTime:0
-lastUpdateTime:1272566061000
+lastUpdateTime:1279737941000
 
 PREHOOK: query: show table extended like partition_test_partitioned partition(dt=101)
 PREHOOK: type: SHOW_TABLESTATUS
@@ -441,8 +437,8 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(s
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/partition_test_partitioned/dt=101
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned/dt=101
 inputformat:org.apache.hadoop.hive.ql.io.RCFileInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat
 columns:struct columns { string key, string value}
@@ -453,7 +449,7 @@ totalFileSize:370
 maxFileSize:370
 minFileSize:370
 lastAccessTime:0
-lastUpdateTime:1272566061000
+lastUpdateTime:1279737941000
 
 PREHOOK: query: show table extended like partition_test_partitioned partition(dt=102)
 PREHOOK: type: SHOW_TABLESTATUS
@@ -466,8 +462,8 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(s
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive1/hive1/build/ql/test/data/warehouse/partition_test_partitioned/dt=102
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned/dt=102
 inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
 columns:struct columns { string key, string value}
@@ -478,16 +474,16 @@ totalFileSize:888
 maxFileSize:888
 minFileSize:888
 lastAccessTime:0
-lastUpdateTime:1272566061000
+lastUpdateTime:1279737941000
 
 PREHOOK: query: select key from partition_test_partitioned where dt=100
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=100
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-22_555_533033657582480400/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-42_155_7749641338163247267/10000
 POSTHOOK: query: select key from partition_test_partitioned where dt=100
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=100
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-22_555_533033657582480400/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-42_155_7749641338163247267/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -522,11 +518,11 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(s
 PREHOOK: query: select key from partition_test_partitioned where dt=101
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=101
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-26_057_6588328457143833363/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-44_640_267727809536101133/10000
 POSTHOOK: query: select key from partition_test_partitioned where dt=101
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=101
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-26_057_6588328457143833363/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-44_640_267727809536101133/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -561,11 +557,11 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(s
 PREHOOK: query: select key from partition_test_partitioned where dt=102
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=102
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-29_653_6126813740681060035/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-47_122_5836450210515977525/10000
 POSTHOOK: query: select key from partition_test_partitioned where dt=102
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=102
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-29_653_6126813740681060035/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-47_122_5836450210515977525/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -602,13 +598,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=100
 PREHOOK: Input: default@partition_test_partitioned@dt=101
 PREHOOK: Input: default@partition_test_partitioned@dt=102
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-33_143_8176565926876989902/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-49_604_111610574124667/10000
 POSTHOOK: query: select key from partition_test_partitioned
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=100
 POSTHOOK: Input: default@partition_test_partitioned@dt=101
 POSTHOOK: Input: default@partition_test_partitioned@dt=102
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-33_143_8176565926876989902/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-49_604_111610574124667/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -695,13 +691,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=100
 PREHOOK: Input: default@partition_test_partitioned@dt=101
 PREHOOK: Input: default@partition_test_partitioned@dt=102
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-36_758_6312401358649427279/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-52_135_418677121289533314/10000
 POSTHOOK: query: select key from partition_test_partitioned where dt >=100 and dt <= 102
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=100
 POSTHOOK: Input: default@partition_test_partitioned@dt=101
 POSTHOOK: Input: default@partition_test_partitioned@dt=102
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-34-36_758_6312401358649427279/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-45-52_135_418677121289533314/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -783,14 +779,3 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(s
 
 
 
-PREHOOK: query: drop table partition_test_partitioned
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table partition_test_partitioned
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@partition_test_partitioned
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/partition_wise_fileformat2.q.out b/ql/src/test/results/clientpositive/partition_wise_fileformat2.q.out
index c39fcca8c7..af22468119 100644
--- a/ql/src/test/results/clientpositive/partition_wise_fileformat2.q.out
+++ b/ql/src/test/results/clientpositive/partition_wise_fileformat2.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table partition_test_partitioned
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table partition_test_partitioned
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table partition_test_partitioned(key string, value string) partitioned by (dt string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table partition_test_partitioned(key string, value string) partitioned by (dt string)
@@ -66,13 +62,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=100
 PREHOOK: Input: default@partition_test_partitioned@dt=101
 PREHOOK: Input: default@partition_test_partitioned@dt=102
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-29-41_458_4880493720639558189/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-02_669_7182591152833006574/10000
 POSTHOOK: query: select * from partition_test_partitioned where dt >=100 and dt <= 102
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=100
 POSTHOOK: Input: default@partition_test_partitioned@dt=101
 POSTHOOK: Input: default@partition_test_partitioned@dt=102
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/scratchdir/hive_2010-04-29_11-29-41_458_4880493720639558189/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-02_669_7182591152833006574/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -154,14 +150,3 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(s
 		102
 		102
 		102
-PREHOOK: query: drop table partition_test_partitioned
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table partition_test_partitioned
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@partition_test_partitioned
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=100).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/partition_wise_fileformat3.q.out b/ql/src/test/results/clientpositive/partition_wise_fileformat3.q.out
index 0f661591e6..521f117997 100644
--- a/ql/src/test/results/clientpositive/partition_wise_fileformat3.q.out
+++ b/ql/src/test/results/clientpositive/partition_wise_fileformat3.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table partition_test_partitioned
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table partition_test_partitioned
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table partition_test_partitioned(key string, value string) partitioned by (dt string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table partition_test_partitioned(key string, value string) partitioned by (dt string)
@@ -30,8 +26,8 @@ POSTHOOK: type: SHOW_TABLESTATUS
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/partition_test_partitioned/dt=101
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned/dt=101
 inputformat:org.apache.hadoop.hive.ql.io.RCFileInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat
 columns:struct columns { string key, string value}
@@ -42,7 +38,7 @@ totalFileSize:370
 maxFileSize:370
 minFileSize:370
 lastAccessTime:0
-lastUpdateTime:1274314890000
+lastUpdateTime:1279737965000
 
 PREHOOK: query: alter table partition_test_partitioned set fileformat Sequencefile
 PREHOOK: type: null
@@ -73,8 +69,8 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(s
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/partition_test_partitioned/dt=102
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned/dt=102
 inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
 columns:struct columns { string key, string value}
@@ -85,16 +81,16 @@ totalFileSize:888
 maxFileSize:888
 minFileSize:888
 lastAccessTime:0
-lastUpdateTime:1274314895000
+lastUpdateTime:1279737968000
 
 PREHOOK: query: select key from partition_test_partitioned where dt=102
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=102
-PREHOOK: Output: file:/data/users/njain/hive3/hive3/build/ql/scratchdir/hive_2010-05-19_17-21-36_438_6571635211032352917/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-08_379_9044568484329597052/10000
 POSTHOOK: query: select key from partition_test_partitioned where dt=102
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=102
-POSTHOOK: Output: file:/data/users/njain/hive3/hive3/build/ql/scratchdir/hive_2010-05-19_17-21-36_438_6571635211032352917/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-08_379_9044568484329597052/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -149,8 +145,8 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(s
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 tableName:partition_test_partitioned
-owner:njain
-location:file:/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/partition_test_partitioned/dt=101
+owner:jssarma
+location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/partition_test_partitioned/dt=101
 inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
 outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
 columns:struct columns { string key, string value}
@@ -161,16 +157,16 @@ totalFileSize:888
 maxFileSize:888
 minFileSize:888
 lastAccessTime:0
-lastUpdateTime:1274314904000
+lastUpdateTime:1279737973000
 
 PREHOOK: query: select key from partition_test_partitioned where dt=101
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_test_partitioned@dt=101
-PREHOOK: Output: file:/data/users/njain/hive3/hive3/build/ql/scratchdir/hive_2010-05-19_17-21-45_060_1172142624101768381/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-13_527_1591714627720505462/10000
 POSTHOOK: query: select key from partition_test_partitioned where dt=101
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_test_partitioned@dt=101
-POSTHOOK: Output: file:/data/users/njain/hive3/hive3/build/ql/scratchdir/hive_2010-05-19_17-21-45_060_1172142624101768381/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-13_527_1591714627720505462/10000
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
@@ -202,14 +198,3 @@ POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(s
 
 
 
-PREHOOK: query: drop table partition_test_partitioned
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table partition_test_partitioned
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@partition_test_partitioned
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=101).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: partition_test_partitioned PARTITION(dt=102).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/ppd_constant_expr.q.out b/ql/src/test/results/clientpositive/ppd_constant_expr.q.out
index 4d6dcf9dcd..fbd0dccb37 100644
--- a/ql/src/test/results/clientpositive/ppd_constant_expr.q.out
+++ b/ql/src/test/results/clientpositive/ppd_constant_expr.q.out
@@ -62,7 +62,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-25-25_765_7258606383966097370/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-46-27_316_7397975593428086505/10000
 
   Stage: Stage-0
     Move Operator
@@ -77,7 +77,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-25-25_765_7258606383966097370/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-46-27_316_7397975593428086505/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -119,11 +119,11 @@ POSTHOOK: Lineage: ppd_constant_expr.c3 EXPRESSION []
 PREHOOK: query: SELECT ppd_constant_expr.* FROM ppd_constant_expr
 PREHOOK: type: QUERY
 PREHOOK: Input: default@ppd_constant_expr
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-25-31_117_7150325701898782164/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-29_875_7776179823012704510/10000
 POSTHOOK: query: SELECT ppd_constant_expr.* FROM ppd_constant_expr
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@ppd_constant_expr
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-25-31_117_7150325701898782164/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-29_875_7776179823012704510/10000
 POSTHOOK: Lineage: ppd_constant_expr.c1 EXPRESSION []
 POSTHOOK: Lineage: ppd_constant_expr.c2 EXPRESSION [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: ppd_constant_expr.c3 EXPRESSION []
@@ -152,11 +152,3 @@ NULL	NULL	NULL
 NULL	NULL	NULL
 NULL	NULL	NULL
 NULL	NULL	NULL
-PREHOOK: query: DROP TABLE ppd_constant_expr
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE ppd_constant_expr
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@ppd_constant_expr
-POSTHOOK: Lineage: ppd_constant_expr.c1 EXPRESSION []
-POSTHOOK: Lineage: ppd_constant_expr.c2 EXPRESSION [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ppd_constant_expr.c3 EXPRESSION []
diff --git a/ql/src/test/results/clientpositive/ppd_multi_insert.q.out b/ql/src/test/results/clientpositive/ppd_multi_insert.q.out
index 947e1407a2..dd487cb51b 100644
--- a/ql/src/test/results/clientpositive/ppd_multi_insert.q.out
+++ b/ql/src/test/results/clientpositive/ppd_multi_insert.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: DROP TABLE mi1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE mi1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE mi2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE mi2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE mi3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE mi3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE mi1(key INT, value STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE mi1(key INT, value STRING) STORED AS TEXTFILE
@@ -251,11 +239,11 @@ POSTHOOK: Lineage: mi3 PARTITION(ds=2008-04-08,hr=12).key EXPRESSION [(src)a.Fie
 PREHOOK: query: SELECT mi1.* FROM mi1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@mi1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-23-53_508_4738071529393947065/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-54_953_7860817338533878265/10000
 POSTHOOK: query: SELECT mi1.* FROM mi1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@mi1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-23-53_508_4738071529393947065/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-54_953_7860817338533878265/10000
 POSTHOOK: Lineage: mi1.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: mi1.value SIMPLE [(src)a.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: mi2.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
@@ -412,11 +400,11 @@ POSTHOOK: Lineage: mi3 PARTITION(ds=2008-04-08,hr=12).key EXPRESSION [(src)a.Fie
 PREHOOK: query: SELECT mi2.* FROM mi2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@mi2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-23-53_557_935780475427392042/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-55_006_6229034219349396892/10000
 POSTHOOK: query: SELECT mi2.* FROM mi2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@mi2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-23-53_557_935780475427392042/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-55_006_6229034219349396892/10000
 POSTHOOK: Lineage: mi1.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: mi1.value SIMPLE [(src)a.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: mi2.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
@@ -636,11 +624,11 @@ POSTHOOK: Lineage: mi3 PARTITION(ds=2008-04-08,hr=12).key EXPRESSION [(src)a.Fie
 PREHOOK: query: SELECT mi3.* FROM mi3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@mi3@ds=2008-04-08/hr=12
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-23-53_654_6607833934992134433/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-55_057_3846144917217046584/10000
 POSTHOOK: query: SELECT mi3.* FROM mi3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@mi3@ds=2008-04-08/hr=12
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-23-53_654_6607833934992134433/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-46-55_057_3846144917217046584/10000
 POSTHOOK: Lineage: mi1.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: mi1.value SIMPLE [(src)a.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: mi2.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
@@ -1315,33 +1303,3 @@ val_498
 val_498
 val_498
 val_498
-PREHOOK: query: DROP TABLE mi1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE mi1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@mi1
-POSTHOOK: Lineage: mi1.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: mi1.value SIMPLE [(src)a.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: mi2.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: mi2.value SIMPLE [(src)a.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: mi3 PARTITION(ds=2008-04-08,hr=12).key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE mi2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE mi2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@mi2
-POSTHOOK: Lineage: mi1.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: mi1.value SIMPLE [(src)a.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: mi2.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: mi2.value SIMPLE [(src)a.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: mi3 PARTITION(ds=2008-04-08,hr=12).key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE mi3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE mi3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@mi3
-POSTHOOK: Lineage: mi1.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: mi1.value SIMPLE [(src)a.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: mi2.key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: mi2.value SIMPLE [(src)a.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: mi3 PARTITION(ds=2008-04-08,hr=12).key EXPRESSION [(src)a.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/progress_1.q.out b/ql/src/test/results/clientpositive/progress_1.q.out
index b8cbf3062f..b34c83d8bb 100644
--- a/ql/src/test/results/clientpositive/progress_1.q.out
+++ b/ql/src/test/results/clientpositive/progress_1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE PROGRESS_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE PROGRESS_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE PROGRESS_1(key int, value string) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE PROGRESS_1(key int, value string) STORED AS TEXTFILE
@@ -15,14 +11,9 @@ POSTHOOK: Output: default@progress_1
 PREHOOK: query: select count(1) from PROGRESS_1 t1 join PROGRESS_1 t2 on t1.key=t2.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@progress_1
-PREHOOK: Output: file:/tmp/sdong/hive_2010-07-08_16-37-28_683_2518541288555731352/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-47-20_302_6301008962928693970/10000
 POSTHOOK: query: select count(1) from PROGRESS_1 t1 join PROGRESS_1 t2 on t1.key=t2.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@progress_1
-POSTHOOK: Output: file:/tmp/sdong/hive_2010-07-08_16-37-28_683_2518541288555731352/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-47-20_302_6301008962928693970/10000
 5000
-PREHOOK: query: DROP TABLE PROGRESS_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE PROGRESS_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@progress_1
diff --git a/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out b/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out
index 97949df1d1..e0eac84d32 100644
--- a/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out
+++ b/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out
@@ -1,12 +1,10 @@
 PREHOOK: query: -- scanning partitioned data
-drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: -- scanning partitioned data
-drop table tmptable
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: create table tmptable(key string, value string, hr string, ds string)
+
+create table tmptable(key string, value string, hr string, ds string)
 PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table tmptable(key string, value string, hr string, ds string)
+POSTHOOK: query: -- scanning partitioned data
+
+create table tmptable(key string, value string, hr string, ds string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tmptable
 PREHOOK: query: explain extended 
@@ -53,7 +51,7 @@ STAGE PLANS:
                 File Output Operator
                   compressed: false
                   GlobalTableId: 1
-                  directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10002
+                  directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10002
                   NumFilesPerFileSink: 1
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
@@ -64,22 +62,22 @@ STAGE PLANS:
                         columns.types string:string:string:string
                         file.inputformat org.apache.hadoop.mapred.TextInputFormat
                         file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/tmptable
+                        location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/tmptable
                         name tmptable
                         serialization.ddl struct tmptable { string key, string value, string hr, string ds}
                         serialization.format 1
                         serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                        transient_lastDdlTime 1270517178
+                        transient_lastDdlTime 1279738064
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: tmptable
                   TotalFiles: 1
                   MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [a]
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 [a]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 [a]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 [a]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
             base file name: hr=11
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -93,13 +91,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517177
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -110,17 +108,17 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517177
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
           Partition
             base file name: hr=12
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -134,13 +132,13 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
               name srcpart
               partition_columns ds/hr
               serialization.ddl struct srcpart { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517177
+              transient_lastDdlTime 1279735681
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -151,13 +149,13 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcpart
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcpart
                 name srcpart
                 partition_columns ds/hr
                 serialization.ddl struct srcpart { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517177
+                transient_lastDdlTime 1279735681
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
             name: srcpart
@@ -169,14 +167,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10002
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -186,20 +184,20 @@ STAGE PLANS:
                 columns.types string:string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/tmptable
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/tmptable
                 name tmptable
                 serialization.ddl struct tmptable { string key, string value, string hr, string ds}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517178
+                transient_lastDdlTime 1279738064
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: tmptable
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -217,9 +215,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10002 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10002]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -230,12 +228,12 @@ STAGE PLANS:
               columns.types string:string:string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/tmptable
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/tmptable
               name tmptable
               serialization.ddl struct tmptable { string key, string value, string hr, string ds}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517178
+              transient_lastDdlTime 1279738064
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -246,12 +244,12 @@ STAGE PLANS:
                 columns.types string:string:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/tmptable
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/tmptable
                 name tmptable
                 serialization.ddl struct tmptable { string key, string value, string hr, string ds}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517178
+                transient_lastDdlTime 1279738064
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: tmptable
             name: tmptable
@@ -260,7 +258,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-18_926_5456051031002963685/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-47-44_657_3286892654529398603/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -271,12 +269,12 @@ STAGE PLANS:
                   columns.types string:string:string:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/tmptable
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/tmptable
                   name tmptable
                   serialization.ddl struct tmptable { string key, string value, string hr, string ds}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270517178
+                  transient_lastDdlTime 1279738064
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: tmptable
             TotalFiles: 1
@@ -302,11 +300,11 @@ POSTHOOK: Lineage: tmptable.value SIMPLE [(srcpart)a.FieldSchema(name:hr, type:s
 PREHOOK: query: select * from tmptable x sort by x.key,x.value,x.ds,x.hr
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmptable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-27_677_7004058534773978474/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-47-49_754_5902540397001817121/10000
 POSTHOOK: query: select * from tmptable x sort by x.key,x.value,x.ds,x.hr
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmptable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-27_677_7004058534773978474/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-47-49_754_5902540397001817121/10000
 POSTHOOK: Lineage: tmptable.ds SIMPLE [(srcpart)a.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: tmptable.hr SIMPLE [(srcpart)a.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmptable.key SIMPLE [(srcpart)a.FieldSchema(name:ds, type:string, comment:null), ]
@@ -423,12 +421,3 @@ POSTHOOK: Lineage: tmptable.value SIMPLE [(srcpart)a.FieldSchema(name:hr, type:s
 77	val_77	2008-04-08	12
 78	val_78	2008-04-08	11
 78	val_78	2008-04-08	12
-PREHOOK: query: drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmptable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmptable
-POSTHOOK: Lineage: tmptable.ds SIMPLE [(srcpart)a.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: tmptable.hr SIMPLE [(srcpart)a.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmptable.key SIMPLE [(srcpart)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: tmptable.value SIMPLE [(srcpart)a.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/rcfile_bigdata.q.out b/ql/src/test/results/clientpositive/rcfile_bigdata.q.out
index 9b04dcb44a..ea42db5286 100644
--- a/ql/src/test/results/clientpositive/rcfile_bigdata.q.out
+++ b/ql/src/test/results/clientpositive/rcfile_bigdata.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE columnTable_Bigdata
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE columnTable_Bigdata
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE table columnTable_Bigdata (key STRING, value STRING)
 ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
@@ -40,18 +36,11 @@ value	string	from deserializer
 PREHOOK: query: select count(columnTable_Bigdata.key) from columnTable_Bigdata
 PREHOOK: type: QUERY
 PREHOOK: Input: default@columntable_bigdata
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-24-35_314_4030817414359065552/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-04_694_763065487220152863/10000
 POSTHOOK: query: select count(columnTable_Bigdata.key) from columnTable_Bigdata
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@columntable_bigdata
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-24-35_314_4030817414359065552/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-04_694_763065487220152863/10000
 POSTHOOK: Lineage: columntable_bigdata.key SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: columntable_bigdata.value SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
 5005500
-PREHOOK: query: DROP TABLE columnTable_Bigdata
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE columnTable_Bigdata
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@columntable_bigdata
-POSTHOOK: Lineage: columntable_bigdata.key SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: columntable_bigdata.value SCRIPT [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/rcfile_columnar.q.out b/ql/src/test/results/clientpositive/rcfile_columnar.q.out
index 9871f4b20f..a1c47a84fe 100644
--- a/ql/src/test/results/clientpositive/rcfile_columnar.q.out
+++ b/ql/src/test/results/clientpositive/rcfile_columnar.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE columnTable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE columnTable
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE table columnTable (key STRING, value STRING)
 ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
@@ -40,11 +36,11 @@ value	string	from deserializer
 PREHOOK: query: SELECT columnTable.* FROM columnTable
 PREHOOK: type: QUERY
 PREHOOK: Input: default@columntable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-39_201_8408576030937070945/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-15_409_223543948857251615/10000
 POSTHOOK: query: SELECT columnTable.* FROM columnTable
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@columntable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-39_201_8408576030937070945/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-15_409_223543948857251615/10000
 POSTHOOK: Lineage: columntable.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: columntable.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 238	val_238
@@ -57,10 +53,3 @@ POSTHOOK: Lineage: columntable.value SIMPLE [(src)src.FieldSchema(name:value, ty
 278	val_278
 98	val_98
 484	val_484
-PREHOOK: query: DROP TABLE columnTable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE columnTable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@columntable
-POSTHOOK: Lineage: columntable.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: columntable.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/rcfile_default_format.q.out b/ql/src/test/results/clientpositive/rcfile_default_format.q.out
index b63d1d3dff..e92b4df563 100644
--- a/ql/src/test/results/clientpositive/rcfile_default_format.q.out
+++ b/ql/src/test/results/clientpositive/rcfile_default_format.q.out
@@ -9,7 +9,7 @@ POSTHOOK: query: DESCRIBE EXTENDED rcfile_default_format
 POSTHOOK: type: DESCTABLE
 key	string	from deserializer
 	 	 
-Detailed Table Information	Table(tableName:rcfile_default_format, dbName:default, owner:athusoo, createTime:1270516950, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/rcfile_default_format, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516950}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:rcfile_default_format, dbName:default, owner:jssarma, createTime:1279738095, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/rcfile_default_format, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279738095}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: CREATE TABLE rcfile_default_format_ctas AS SELECT key,value FROM src
 PREHOOK: type: CREATETABLE
 PREHOOK: Input: default@src
@@ -24,7 +24,7 @@ POSTHOOK: type: DESCTABLE
 key	string	from deserializer
 value	string	from deserializer
 	 	 
-Detailed Table Information	Table(tableName:rcfile_default_format_ctas, dbName:default, owner:athusoo, createTime:1270516954, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/rcfile_default_format_ctas, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516954}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:rcfile_default_format_ctas, dbName:default, owner:jssarma, createTime:1279738098, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/rcfile_default_format_ctas, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279738098}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: CREATE TABLE rcfile_default_format_txtfile (key STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE rcfile_default_format_txtfile (key STRING) STORED AS TEXTFILE
@@ -46,7 +46,7 @@ POSTHOOK: type: DESCTABLE
 POSTHOOK: Lineage: rcfile_default_format_txtfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 key	string	
 	 	 
-Detailed Table Information	Table(tableName:rcfile_default_format_txtfile, dbName:default, owner:athusoo, createTime:1270516954, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/rcfile_default_format_txtfile, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516954}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
+Detailed Table Information	Table(tableName:rcfile_default_format_txtfile, dbName:default, owner:jssarma, createTime:1279738098, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/rcfile_default_format_txtfile, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279738100}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: CREATE TABLE textfile_default_format_ctas AS SELECT key,value FROM rcfile_default_format_ctas
 PREHOOK: type: CREATETABLE
 PREHOOK: Input: default@rcfile_default_format_ctas
@@ -63,28 +63,4 @@ POSTHOOK: Lineage: rcfile_default_format_txtfile.key SIMPLE [(src)src.FieldSchem
 key	string	
 value	string	
 	 	 
-Detailed Table Information	Table(tableName:textfile_default_format_ctas, dbName:default, owner:athusoo, createTime:1270516964, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/test/data/warehouse/textfile_default_format_ctas, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1270516964}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
-PREHOOK: query: DROP TABLE  rcfile_default_format
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE  rcfile_default_format
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@rcfile_default_format
-POSTHOOK: Lineage: rcfile_default_format_txtfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE  rcfile_default_format_ctas
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE  rcfile_default_format_ctas
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@rcfile_default_format_ctas
-POSTHOOK: Lineage: rcfile_default_format_txtfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE rcfile_default_format_txtfile
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE rcfile_default_format_txtfile
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@rcfile_default_format_txtfile
-POSTHOOK: Lineage: rcfile_default_format_txtfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE textfile_default_format_ctas
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE textfile_default_format_ctas
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@textfile_default_format_ctas
-POSTHOOK: Lineage: rcfile_default_format_txtfile.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+Detailed Table Information	Table(tableName:textfile_default_format_ctas, dbName:default, owner:jssarma, createTime:1279738103, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/textfile_default_format_ctas, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{transient_lastDdlTime=1279738103}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
diff --git a/ql/src/test/results/clientpositive/rcfile_lazydecompress.q.out b/ql/src/test/results/clientpositive/rcfile_lazydecompress.q.out
index b3d08047d0..91e1472c71 100644
--- a/ql/src/test/results/clientpositive/rcfile_lazydecompress.q.out
+++ b/ql/src/test/results/clientpositive/rcfile_lazydecompress.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE rcfileTableLazyDecompress
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE rcfileTableLazyDecompress
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE table rcfileTableLazyDecompress (key STRING, value STRING) STORED AS RCFile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE table rcfileTableLazyDecompress (key STRING, value STRING) STORED AS RCFile
@@ -22,11 +18,11 @@ POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(
 PREHOOK: query: SELECT key, value FROM rcfileTableLazyDecompress where key > 238
 PREHOOK: type: QUERY
 PREHOOK: Input: default@rcfiletablelazydecompress
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-08_126_1758618048562072306/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-26_308_5035766417306203353/10000
 POSTHOOK: query: SELECT key, value FROM rcfileTableLazyDecompress where key > 238
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@rcfiletablelazydecompress
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-08_126_1758618048562072306/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-26_308_5035766417306203353/10000
 POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 311	val_311
@@ -37,11 +33,11 @@ POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(
 PREHOOK: query: SELECT key, value FROM rcfileTableLazyDecompress where key > 238 and key < 400
 PREHOOK: type: QUERY
 PREHOOK: Input: default@rcfiletablelazydecompress
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-12_756_7047055078902721683/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-28_826_2664555248442666518/10000
 POSTHOOK: query: SELECT key, value FROM rcfileTableLazyDecompress where key > 238 and key < 400
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@rcfiletablelazydecompress
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-12_756_7047055078902721683/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-28_826_2664555248442666518/10000
 POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 311	val_311
@@ -50,11 +46,11 @@ POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(
 PREHOOK: query: SELECT key, count(1) FROM rcfileTableLazyDecompress where key > 238 group by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@rcfiletablelazydecompress
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-16_962_6661530342559954352/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-31_347_4980542755724589562/10000
 POSTHOOK: query: SELECT key, count(1) FROM rcfileTableLazyDecompress where key > 238 group by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@rcfiletablelazydecompress
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-16_962_6661530342559954352/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-31_347_4980542755724589562/10000
 POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 255	1
@@ -79,11 +75,11 @@ POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(
 PREHOOK: query: SELECT key, value FROM rcfileTableLazyDecompress where key > 238
 PREHOOK: type: QUERY
 PREHOOK: Input: default@rcfiletablelazydecompress
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-24_793_7750074014093688381/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-36_558_7043959496739147343/10000
 POSTHOOK: query: SELECT key, value FROM rcfileTableLazyDecompress where key > 238
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@rcfiletablelazydecompress
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-24_793_7750074014093688381/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-36_558_7043959496739147343/10000
 POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -96,11 +92,11 @@ POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(
 PREHOOK: query: SELECT key, value FROM rcfileTableLazyDecompress where key > 238 and key < 400
 PREHOOK: type: QUERY
 PREHOOK: Input: default@rcfiletablelazydecompress
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-28_589_3116278472072779381/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-39_022_510504722430321903/10000
 POSTHOOK: query: SELECT key, value FROM rcfileTableLazyDecompress where key > 238 and key < 400
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@rcfiletablelazydecompress
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-28_589_3116278472072779381/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-39_022_510504722430321903/10000
 POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -111,11 +107,11 @@ POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(
 PREHOOK: query: SELECT key, count(1) FROM rcfileTableLazyDecompress where key > 238 group by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@rcfiletablelazydecompress
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-33_415_4146404104589934880/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-41_564_865533805813240026/10000
 POSTHOOK: query: SELECT key, count(1) FROM rcfileTableLazyDecompress where key > 238 group by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@rcfiletablelazydecompress
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-25-33_415_4146404104589934880/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-41_564_865533805813240026/10000
 POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -125,12 +121,3 @@ POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(
 311	1
 409	1
 484	1
-PREHOOK: query: DROP TABLE rcfileTableLazyDecompress
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE rcfileTableLazyDecompress
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@rcfiletablelazydecompress
-POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: rcfiletablelazydecompress.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: rcfiletablelazydecompress.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/rcfile_null_value.q.out b/ql/src/test/results/clientpositive/rcfile_null_value.q.out
index 4228da8f05..31d7298a5e 100644
--- a/ql/src/test/results/clientpositive/rcfile_null_value.q.out
+++ b/ql/src/test/results/clientpositive/rcfile_null_value.q.out
@@ -16,11 +16,11 @@ POSTHOOK: Lineage: src1_rc.value SIMPLE [(src1)src1.FieldSchema(name:value, type
 PREHOOK: query: SELECT * FROM src1_rc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src1_rc
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-45_304_3014831283453834869/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-47_002_2104750496460432907/10000
 POSTHOOK: query: SELECT * FROM src1_rc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src1_rc
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-45_304_3014831283453834869/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-47_002_2104750496460432907/10000
 POSTHOOK: Lineage: src1_rc.key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: src1_rc.value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 238	val_238
@@ -48,13 +48,6 @@ POSTHOOK: Lineage: src1_rc.value SIMPLE [(src1)src1.FieldSchema(name:value, type
 	
 	
 	
-PREHOOK: query: DROP TABLE src1_rc
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE src1_rc
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@src1_rc
-POSTHOOK: Lineage: src1_rc.key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src1_rc.value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: CREATE TABLE dest1_rc(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1_rc(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS RCFILE
@@ -272,11 +265,11 @@ POSTHOOK: Lineage: src1_rc.value SIMPLE [(src1)src1.FieldSchema(name:value, type
 PREHOOK: query: SELECT dest1_rc.* FROM dest1_rc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1_rc
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-51_035_1392258075462361596/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-50_018_8982453667868960765/10000
 POSTHOOK: query: SELECT dest1_rc.* FROM dest1_rc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1_rc
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-26-51_035_1392258075462361596/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-50_018_8982453667868960765/10000
 POSTHOOK: Lineage: dest1_rc.c1 EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1_rc.c2 SIMPLE [(src)src1.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1_rc.c3 EXPRESSION [(src)src2.FieldSchema(name:key, type:string, comment:default), ]
@@ -292,14 +285,3 @@ POSTHOOK: Lineage: src1_rc.value SIMPLE [(src1)src1.FieldSchema(name:value, type
 NULL	NULL	20	val_20
 NULL	NULL	24	val_24
 NULL	NULL	24	val_24
-PREHOOK: query: DROP TABLE dest1_rc
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1_rc
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1_rc
-POSTHOOK: Lineage: dest1_rc.c1 EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1_rc.c2 SIMPLE [(src)src1.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1_rc.c3 EXPRESSION [(src)src2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1_rc.c4 SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: src1_rc.key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: src1_rc.value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/rcfile_union.q.out b/ql/src/test/results/clientpositive/rcfile_union.q.out
index 61027f526e..e887588ceb 100644
--- a/ql/src/test/results/clientpositive/rcfile_union.q.out
+++ b/ql/src/test/results/clientpositive/rcfile_union.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE rcfile_unionTable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE rcfile_unionTable
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE table rcfile_unionTable (b STRING, c STRING)
 ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
@@ -31,14 +27,14 @@ UNION ALL
 SELECT c AS cola FROM rcfile_unionTable) s
 PREHOOK: type: QUERY
 PREHOOK: Input: default@rcfile_uniontable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-22-51_306_3234597609952199798/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-53_001_5783976192735479261/10000
 POSTHOOK: query: SELECT * FROM (
 SELECT b AS cola FROM rcfile_unionTable
 UNION ALL
 SELECT c AS cola FROM rcfile_unionTable) s
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@rcfile_uniontable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-22-51_306_3234597609952199798/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-53_001_5783976192735479261/10000
 POSTHOOK: Lineage: rcfile_uniontable.b SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: rcfile_uniontable.c SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 val_238
@@ -61,10 +57,3 @@ val_98
 98
 val_484
 484
-PREHOOK: query: DROP TABLE rcfile_unionTable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE rcfile_unionTable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@rcfile_uniontable
-POSTHOOK: Lineage: rcfile_uniontable.b SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: rcfile_uniontable.c SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/reduce_deduplicate.q.out b/ql/src/test/results/clientpositive/reduce_deduplicate.q.out
index bbe2917857..7104cd5760 100644
--- a/ql/src/test/results/clientpositive/reduce_deduplicate.q.out
+++ b/ql/src/test/results/clientpositive/reduce_deduplicate.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table bucket5_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket5_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE bucket5_1(key string, value string) CLUSTERED BY (key) INTO 2 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE bucket5_1(key string, value string) CLUSTERED BY (key) INTO 2 BUCKETS
@@ -52,9 +48,9 @@ STAGE PLANS:
                       type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src [src]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src [src]
       Path -> Partition:
-        file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src 
           Partition
             base file name: src
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -65,12 +61,12 @@ STAGE PLANS:
               columns.types string:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
               name src
               serialization.ddl struct src { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270584980
+              transient_lastDdlTime 1279735685
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -81,12 +77,12 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/src
                 name src
                 serialization.ddl struct src { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584980
+                transient_lastDdlTime 1279735685
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: src
             name: src
@@ -102,7 +98,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 1
-              directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-21_146_8698358277578653299/10000
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-48-55_864_1750582395196822547/10000
               NumFilesPerFileSink: 2
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -114,12 +110,12 @@ STAGE PLANS:
                     columns.types string:string
                     file.inputformat org.apache.hadoop.mapred.TextInputFormat
                     file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucket5_1
+                    location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket5_1
                     name bucket5_1
                     serialization.ddl struct bucket5_1 { string key, string value}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    transient_lastDdlTime 1270584981
+                    transient_lastDdlTime 1279738135
                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   name: bucket5_1
               TotalFiles: 2
@@ -129,7 +125,7 @@ STAGE PLANS:
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-21_146_8698358277578653299/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-48-55_864_1750582395196822547/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -140,15 +136,15 @@ STAGE PLANS:
                 columns.types string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/bucket5_1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/bucket5_1
                 name bucket5_1
                 serialization.ddl struct bucket5_1 { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584981
+                transient_lastDdlTime 1279738135
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: bucket5_1
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-21_146_8698358277578653299/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-48-55_864_1750582395196822547/10001
 
 
 PREHOOK: query: insert overwrite table bucket5_1
@@ -166,31 +162,25 @@ POSTHOOK: Lineage: bucket5_1.value SIMPLE [(src)src.FieldSchema(name:value, type
 PREHOOK: query: select sum(hash(key)),sum(hash(value)) from bucket5_1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@bucket5_1
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-24_929_6291705567058449432/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-58_496_2658667578750181431/10000
 POSTHOOK: query: select sum(hash(key)),sum(hash(value)) from bucket5_1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@bucket5_1
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-24_929_6291705567058449432/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-48-58_496_2658667578750181431/10000
 POSTHOOK: Lineage: bucket5_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: bucket5_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 21025334	36210398070
 PREHOOK: query: select sum(hash(key)),sum(hash(value)) from src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-28_771_1329782713419507109/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-49-01_108_5850998193164493516/10000
 POSTHOOK: query: select sum(hash(key)),sum(hash(value)) from src
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-28_771_1329782713419507109/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-49-01_108_5850998193164493516/10000
 POSTHOOK: Lineage: bucket5_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: bucket5_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 21025334	36210398070
-PREHOOK: query: drop table complex_tbl_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table complex_tbl_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: bucket5_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket5_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: create table complex_tbl_1(aid string, bid string, t int, ctime string, etime bigint, l string, et string) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table complex_tbl_1(aid string, bid string, t int, ctime string, etime bigint, l string, et string) partitioned by (ds string)
@@ -198,12 +188,6 @@ POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@complex_tbl_1
 POSTHOOK: Lineage: bucket5_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: bucket5_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table complex_tbl_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table complex_tbl_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: bucket5_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket5_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: create table complex_tbl_2(aet string, aes string) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table complex_tbl_2(aet string, aes string) partitioned by (ds string)
@@ -360,7 +344,7 @@ STAGE PLANS:
                 File Output Operator
                   compressed: false
                   GlobalTableId: 1
-                  directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-32_432_8813792273595897349/10000
+                  directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-49-03_763_2468601734060096330/10000
                   NumFilesPerFileSink: 1
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
@@ -371,13 +355,13 @@ STAGE PLANS:
                         columns.types string:string:int:string:bigint:string:string
                         file.inputformat org.apache.hadoop.mapred.TextInputFormat
                         file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/complex_tbl_1
+                        location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/complex_tbl_1
                         name complex_tbl_1
                         partition_columns ds
                         serialization.ddl struct complex_tbl_1 { string aid, string bid, i32 t, string ctime, i64 etime, string l, string et}
                         serialization.format 1
                         serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                        transient_lastDdlTime 1270584992
+                        transient_lastDdlTime 1279738143
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: complex_tbl_1
                   TotalFiles: 1
@@ -389,7 +373,7 @@ STAGE PLANS:
           partition:
             ds 2010-03-29
           replace: true
-          source: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-32_432_8813792273595897349/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-49-03_763_2468601734060096330/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -399,36 +383,15 @@ STAGE PLANS:
                 columns.types string:string:int:string:bigint:string:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/complex_tbl_1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/complex_tbl_1
                 name complex_tbl_1
                 partition_columns ds
                 serialization.ddl struct complex_tbl_1 { string aid, string bid, i32 t, string ctime, i64 etime, string l, string et}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270584992
+                transient_lastDdlTime 1279738143
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: complex_tbl_1
-          tmp directory: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-32_432_8813792273595897349/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-49-03_763_2468601734060096330/10001
 
 
-PREHOOK: query: drop table complex_tbl_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table complex_tbl_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@complex_tbl_2
-POSTHOOK: Lineage: bucket5_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket5_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table complex_tbl_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table complex_tbl_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@complex_tbl_1
-POSTHOOK: Lineage: bucket5_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket5_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table bucket5_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table bucket5_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@bucket5_1
-POSTHOOK: Lineage: bucket5_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: bucket5_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/rename_column.q.out b/ql/src/test/results/clientpositive/rename_column.q.out
index eba639dd46..8d5da8de5c 100644
--- a/ql/src/test/results/clientpositive/rename_column.q.out
+++ b/ql/src/test/results/clientpositive/rename_column.q.out
@@ -101,8 +101,3 @@ POSTHOOK: type: DESCTABLE
 b	int	
 a	int	test comment2
 c	int	
-PREHOOK: query: DROP TABLE kv_rename_test
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE kv_rename_test
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@kv_rename_test
diff --git a/ql/src/test/results/clientpositive/repair.q.out b/ql/src/test/results/clientpositive/repair.q.out
index 87ecd668ff..a05726a035 100644
--- a/ql/src/test/results/clientpositive/repair.q.out
+++ b/ql/src/test/results/clientpositive/repair.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE repairtable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE repairtable
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE repairtable(col STRING) PARTITIONED BY (p1 STRING, p2 STRING)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE repairtable(col STRING) PARTITIONED BY (p1 STRING, p2 STRING)
@@ -27,8 +23,3 @@ PREHOOK: query: MSCK TABLE repairtable
 PREHOOK: type: MSCK
 POSTHOOK: query: MSCK TABLE repairtable
 POSTHOOK: type: MSCK
-PREHOOK: query: DROP TABLE repairtable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE repairtable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@repairtable
diff --git a/ql/src/test/results/clientpositive/sample10.q.out b/ql/src/test/results/clientpositive/sample10.q.out
index 97ab32abac..e27769f28e 100644
--- a/ql/src/test/results/clientpositive/sample10.q.out
+++ b/ql/src/test/results/clientpositive/sample10.q.out
@@ -128,7 +128,7 @@ STAGE PLANS:
               serialization.ddl struct srcpartbucket { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-              transient_lastDdlTime 1279517872
+              transient_lastDdlTime 1279738180
             serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
           
               input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
@@ -146,7 +146,7 @@ STAGE PLANS:
                 serialization.ddl struct srcpartbucket { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                transient_lastDdlTime 1279517872
+                transient_lastDdlTime 1279738180
               serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
               name: srcpartbucket
             name: srcpartbucket
@@ -171,7 +171,7 @@ STAGE PLANS:
               serialization.ddl struct srcpartbucket { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-              transient_lastDdlTime 1279517872
+              transient_lastDdlTime 1279738180
             serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
           
               input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
@@ -189,7 +189,7 @@ STAGE PLANS:
                 serialization.ddl struct srcpartbucket { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                transient_lastDdlTime 1279517872
+                transient_lastDdlTime 1279738180
               serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
               name: srcpartbucket
             name: srcpartbucket
@@ -214,7 +214,7 @@ STAGE PLANS:
               serialization.ddl struct srcpartbucket { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-              transient_lastDdlTime 1279517872
+              transient_lastDdlTime 1279738180
             serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
           
               input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
@@ -232,7 +232,7 @@ STAGE PLANS:
                 serialization.ddl struct srcpartbucket { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                transient_lastDdlTime 1279517872
+                transient_lastDdlTime 1279738180
               serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
               name: srcpartbucket
             name: srcpartbucket
@@ -257,7 +257,7 @@ STAGE PLANS:
               serialization.ddl struct srcpartbucket { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-              transient_lastDdlTime 1279517872
+              transient_lastDdlTime 1279738180
             serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
           
               input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
@@ -275,7 +275,7 @@ STAGE PLANS:
                 serialization.ddl struct srcpartbucket { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                transient_lastDdlTime 1279517872
+                transient_lastDdlTime 1279738180
               serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
               name: srcpartbucket
             name: srcpartbucket
@@ -299,7 +299,7 @@ STAGE PLANS:
             File Output Operator
               compressed: false
               GlobalTableId: 0
-              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-18_22-37-59_060_3045357226899710132/10001
+              directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-49-50_698_7661671497801340247/10001
               NumFilesPerFileSink: 1
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -322,14 +322,14 @@ PREHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=11
 PREHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=12
 PREHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=11
 PREHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=12
-PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-18_22-37-59_639_2719302052990534208/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-49-50_833_8314246371963786235/10000
 POSTHOOK: query: select ds, count(1) from srcpartbucket tablesample (bucket 1 out of 4 on key) where ds is not null group by ds
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=12
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=11
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=12
-POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-18_22-37-59_639_2719302052990534208/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-49-50_833_8314246371963786235/10000
 POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -346,14 +346,14 @@ PREHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=11
 PREHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=12
 PREHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=11
 PREHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=12
-PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-18_22-38-03_708_4461107599597555917/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-49-54_664_3335998091673950970/10000
 POSTHOOK: query: select ds, count(1) from srcpartbucket tablesample (bucket 1 out of 2 on key) where ds is not null group by ds
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=12
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=11
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=12
-POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-18_22-38-03_708_4461107599597555917/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-49-54_664_3335998091673950970/10000
 POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -370,14 +370,14 @@ PREHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=11
 PREHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=12
 PREHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=11
 PREHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=12
-PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-18_22-38-07_920_2071428667805216792/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-01_601_3474709675356949178/10000
 POSTHOOK: query: select * from srcpartbucket where ds is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-08/hr=12
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=11
 POSTHOOK: Input: default@srcpartbucket@ds=2008-04-09/hr=12
-POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-18_22-38-07_920_2071428667805216792/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-01_601_3474709675356949178/10000
 POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
 POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
 POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
@@ -426,16 +426,3 @@ POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-09,hr=12).value SIMPLE [(s
 2	val_2	2008-04-09	12
 5	val_5	2008-04-09	12
 9	val_9	2008-04-09	12
-PREHOOK: query: drop table srcpartbucket
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table srcpartbucket
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@srcpartbucket
-POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-09,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-09,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-09,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: srcpartbucket PARTITION(ds=2008-04-09,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:hr, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/sample6.q.out b/ql/src/test/results/clientpositive/sample6.q.out
index b9ebf19c75..df8192ff28 100644
--- a/ql/src/test/results/clientpositive/sample6.q.out
+++ b/ql/src/test/results/clientpositive/sample6.q.out
@@ -50,7 +50,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10002
+                    directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
@@ -61,21 +61,21 @@ STAGE PLANS:
                           columns.types int:string
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+                          location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                           name dest1
                           serialization.ddl struct dest1 { i32 key, string value}
                           serialization.format 1
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          transient_lastDdlTime 1270517262
+                          transient_lastDdlTime 1279738223
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: dest1
                     TotalFiles: 1
                     MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket/srcbucket0.txt [s]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket/srcbucket0.txt [s]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket/srcbucket0.txt 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket/srcbucket0.txt 
           Partition
             base file name: srcbucket0.txt
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -87,12 +87,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
               name srcbucket
               serialization.ddl struct srcbucket { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517262
+              transient_lastDdlTime 1279735683
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -104,12 +104,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
                 name srcbucket
                 serialization.ddl struct srcbucket { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279735683
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket
             name: srcbucket
@@ -121,14 +121,14 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10002
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10000
 
   Stage: Stage-0
     Move Operator
       tables:
           replace: true
-          source: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -138,20 +138,20 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                 name dest1
                 serialization.ddl struct dest1 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279738223
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest1
-          tmp directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10001
 
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -165,9 +165,9 @@ STAGE PLANS:
                     type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10002 [file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10002]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10002 [file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10002]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -178,12 +178,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
               name dest1
               serialization.ddl struct dest1 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517262
+              transient_lastDdlTime 1279738223
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -194,12 +194,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                 name dest1
                 serialization.ddl struct dest1 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279738223
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dest1
             name: dest1
@@ -208,7 +208,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-42_865_1009634073752225966/10000
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-23_864_6498950862836410238/10000
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -219,12 +219,12 @@ STAGE PLANS:
                   columns.types int:string
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/dest1
+                  location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dest1
                   name dest1
                   serialization.ddl struct dest1 { i32 key, string value}
                   serialization.format 1
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  transient_lastDdlTime 1270517262
+                  transient_lastDdlTime 1279738223
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest1
             TotalFiles: 1
@@ -246,11 +246,11 @@ POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-48_233_4207127197417407974/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-26_756_8278972907693157648/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-48_233_4207127197417407974/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-26_756_8278972907693157648/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
 468	val_469
@@ -556,9 +556,9 @@ STAGE PLANS:
                           type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket/srcbucket1.txt [s]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket/srcbucket1.txt [s]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket/srcbucket1.txt 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket/srcbucket1.txt 
           Partition
             base file name: srcbucket1.txt
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -570,12 +570,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
               name srcbucket
               serialization.ddl struct srcbucket { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517262
+              transient_lastDdlTime 1279735683
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -587,12 +587,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
                 name srcbucket
                 serialization.ddl struct srcbucket { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279735683
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket
             name: srcbucket
@@ -601,7 +601,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-48_293_3025122128955595487/10001
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-26_811_1928612617318279401/10001
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -622,12 +622,12 @@ PREHOOK: query: SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 4 OUT OF 4 on key)
 ORDER BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcbucket
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-48_859_1916674344218010974/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-26_951_3796284816579844040/10000
 POSTHOOK: query: SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 4 OUT OF 4 on key) s
 ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcbucket
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-48_859_1916674344218010974/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-26_951_3796284816579844040/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
 3	val_4
@@ -923,9 +923,9 @@ STAGE PLANS:
                           type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket/srcbucket0.txt [s]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket/srcbucket0.txt [s]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket/srcbucket0.txt 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket/srcbucket0.txt 
           Partition
             base file name: srcbucket0.txt
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -937,12 +937,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
               name srcbucket
               serialization.ddl struct srcbucket { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517262
+              transient_lastDdlTime 1279735683
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -954,12 +954,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
                 name srcbucket
                 serialization.ddl struct srcbucket { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279735683
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket
             name: srcbucket
@@ -968,7 +968,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-52_953_5622972320044728401/10001
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-29_674_1154918281660126042/10001
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -989,12 +989,12 @@ PREHOOK: query: SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key)
 ORDER BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcbucket
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-53_176_7927998496249013710/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-29_757_1534268201126929954/10000
 POSTHOOK: query: SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
 ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcbucket
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-53_176_7927998496249013710/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-29_757_1534268201126929954/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
 0	val_0
@@ -1544,9 +1544,9 @@ STAGE PLANS:
                           type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket [s]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket [s]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket 
           Partition
             base file name: srcbucket
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -1558,12 +1558,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
               name srcbucket
               serialization.ddl struct srcbucket { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517262
+              transient_lastDdlTime 1279735683
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -1575,12 +1575,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
                 name srcbucket
                 serialization.ddl struct srcbucket { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279735683
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket
             name: srcbucket
@@ -1589,7 +1589,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-58_142_1626689930102051964/10001
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-32_303_2714309411556888893/10001
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -1610,12 +1610,12 @@ PREHOOK: query: SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 3 on key)
 ORDER BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcbucket
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-58_362_4266314071855648980/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-32_384_5005530860359231914/10000
 POSTHOOK: query: SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 3 on key) s
 ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcbucket
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-27-58_362_4266314071855648980/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-32_384_5005530860359231914/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
 0	val_0
@@ -2008,9 +2008,9 @@ STAGE PLANS:
                           type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket [s]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket [s]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket 
           Partition
             base file name: srcbucket
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2022,12 +2022,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
               name srcbucket
               serialization.ddl struct srcbucket { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517262
+              transient_lastDdlTime 1279735683
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2039,12 +2039,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket
                 name srcbucket
                 serialization.ddl struct srcbucket { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279735683
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket
             name: srcbucket
@@ -2053,7 +2053,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-04_331_7162415974693027105/10001
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-34_944_873466326003191566/10001
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2074,12 +2074,12 @@ PREHOOK: query: SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 2 OUT OF 3 on key)
 ORDER BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcbucket
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-04_552_9058511756861142570/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-35_024_8293881759454756883/10000
 POSTHOOK: query: SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 2 OUT OF 3 on key) s
 ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcbucket
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-04_552_9058511756861142570/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-35_024_8293881759454756883/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
 1	val_2
@@ -2458,10 +2458,10 @@ STAGE PLANS:
                           type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2/srcbucket20.txt [s]
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2/srcbucket22.txt [s]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2/srcbucket20.txt [s]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2/srcbucket22.txt [s]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2/srcbucket20.txt 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2/srcbucket20.txt 
           Partition
             base file name: srcbucket20.txt
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2473,12 +2473,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2
               name srcbucket2
               serialization.ddl struct srcbucket2 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517262
+              transient_lastDdlTime 1279735684
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2490,16 +2490,16 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2
                 name srcbucket2
                 serialization.ddl struct srcbucket2 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279735684
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket2
             name: srcbucket2
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2/srcbucket22.txt 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2/srcbucket22.txt 
           Partition
             base file name: srcbucket22.txt
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2511,12 +2511,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2
               name srcbucket2
               serialization.ddl struct srcbucket2 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517262
+              transient_lastDdlTime 1279735684
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2528,12 +2528,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2
                 name srcbucket2
                 serialization.ddl struct srcbucket2 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279735684
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket2
             name: srcbucket2
@@ -2542,7 +2542,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-09_651_8405875539319678293/10001
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-37_595_7840737516924429210/10001
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2563,12 +2563,12 @@ PREHOOK: query: SELECT s.* FROM srcbucket2 TABLESAMPLE (BUCKET 1 OUT OF 2 on key
 ORDER BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcbucket2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-09_876_6368119363282454745/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-37_683_2497676492834201823/10000
 POSTHOOK: query: SELECT s.* FROM srcbucket2 TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
 ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcbucket2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-09_876_6368119363282454745/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-37_683_2497676492834201823/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
 0	val_0
@@ -2747,9 +2747,9 @@ STAGE PLANS:
                           type: string
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2/srcbucket21.txt [s]
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2/srcbucket21.txt [s]
       Path -> Partition:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2/srcbucket21.txt 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2/srcbucket21.txt 
           Partition
             base file name: srcbucket21.txt
             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2761,12 +2761,12 @@ STAGE PLANS:
               columns.types int:string
               file.inputformat org.apache.hadoop.mapred.TextInputFormat
               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2
+              location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2
               name srcbucket2
               serialization.ddl struct srcbucket2 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1270517262
+              transient_lastDdlTime 1279735684
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2778,12 +2778,12 @@ STAGE PLANS:
                 columns.types int:string
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/test/data/warehouse/srcbucket2
+                location file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/srcbucket2
                 name srcbucket2
                 serialization.ddl struct srcbucket2 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1270517262
+                transient_lastDdlTime 1279735684
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket2
             name: srcbucket2
@@ -2792,7 +2792,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-14_786_1452572188214860997/10001
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-40_255_127228907058394247/10001
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2813,12 +2813,12 @@ PREHOOK: query: SELECT s.* FROM srcbucket2 TABLESAMPLE (BUCKET 2 OUT OF 4 on key
 ORDER BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcbucket2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-15_146_419016964614746518/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-40_337_6873183340723048074/10000
 POSTHOOK: query: SELECT s.* FROM srcbucket2 TABLESAMPLE (BUCKET 2 OUT OF 4 on key) s
 ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcbucket2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-15_146_419016964614746518/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-40_337_6873183340723048074/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
 5	val_5
@@ -2924,7 +2924,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-19_226_8478415062494440717/10001
+            directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_11-50-42_960_7107286155400574015/10001
             NumFilesPerFileSink: 1
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
@@ -2945,25 +2945,11 @@ PREHOOK: query: SELECT s.* FROM empty_bucket TABLESAMPLE (BUCKET 1 OUT OF 2 on k
 ORDER BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@empty_bucket
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-19_434_429721878761864733/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-43_037_4167079342764322409/10000
 POSTHOOK: query: SELECT s.* FROM empty_bucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
 ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@empty_bucket
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-28-19_434_429721878761864733/10000
-POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table empty_bucket
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table empty_bucket
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@empty_bucket
-POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-50-43_037_4167079342764322409/10000
 POSTHOOK: Lineage: dest1.key SIMPLE [(srcbucket)s.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(srcbucket)s.FieldSchema(name:value, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/semijoin.q.out b/ql/src/test/results/clientpositive/semijoin.q.out
index d4afa1bf47..e416380f76 100644
--- a/ql/src/test/results/clientpositive/semijoin.q.out
+++ b/ql/src/test/results/clientpositive/semijoin.q.out
@@ -1,19 +1,3 @@
-PREHOOK: query: drop table t1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table t2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table t3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table t4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t4
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table t1 as select cast(key as int) key, value from src where key <= 10
 PREHOOK: type: CREATETABLE
 PREHOOK: Input: default@src
@@ -24,11 +8,11 @@ POSTHOOK: Output: default@t1
 PREHOOK: query: select * from t1 sort by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-34-55_844_5377426816047644405/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-35_256_7415221356692960639/10000
 POSTHOOK: query: select * from t1 sort by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-34-55_844_5377426816047644405/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-35_256_7415221356692960639/10000
 0	val_0
 0	val_0
 0	val_0
@@ -50,11 +34,11 @@ POSTHOOK: Output: default@t2
 PREHOOK: query: select * from t2 sort by key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-04_090_4392814292183673812/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-40_528_939202288577935919/10000
 POSTHOOK: query: select * from t2 sort by key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-04_090_4392814292183673812/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-40_528_939202288577935919/10000
 0	val_0
 0	val_0
 0	val_0
@@ -78,11 +62,11 @@ POSTHOOK: Output: default@t3
 PREHOOK: query: select * from t3 sort by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-18_920_8144945614399368665/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-47_912_8068355266970079632/10000
 POSTHOOK: query: select * from t3 sort by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-18_920_8144945614399368665/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-47_912_8068355266970079632/10000
 0	val_0
 0	val_0
 0	val_0
@@ -113,11 +97,11 @@ POSTHOOK: Output: default@t4
 PREHOOK: query: select * from t4
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t4
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-23_529_199845268930082152/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-50_455_801375301191193749/10000
 POSTHOOK: query: select * from t4
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t4
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-23_529_199845268930082152/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-50_455_801375301191193749/10000
 PREHOOK: query: explain select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value
 PREHOOK: type: QUERY
 POSTHOOK: query: explain select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value
@@ -201,7 +185,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-23_552_5758342471643844459/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-51-50_501_2275832402557448214/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -233,12 +217,12 @@ PREHOOK: query: select * from t1 a left semi join t2 b on a.key=b.key sort by a.
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-23_609_8429769440251868116/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-50_588_8376275316100037063/10000
 POSTHOOK: query: select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-23_609_8429769440251868116/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-50_588_8376275316100037063/10000
 0	val_0
 0	val_0
 0	val_0
@@ -328,7 +312,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-33_326_10811326247390379/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-51-55_574_539988252801347645/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -360,12 +344,12 @@ PREHOOK: query: select * from t2 a left semi join t1 b on b.key=a.key sort by a.
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-33_389_3029611191663485159/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-55_663_6493556599812017880/10000
 POSTHOOK: query: select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-33_389_3029611191663485159/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-51-55_663_6493556599812017880/10000
 0	val_0
 0	val_0
 0	val_0
@@ -457,7 +441,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-44_263_8841961579095622446/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-00_731_4688799125623187788/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -489,12 +473,12 @@ PREHOOK: query: select * from t1 a left semi join t4 b on b.key=a.key sort by a.
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t4
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-44_323_4702074512053597213/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-00_821_8531941471423951343/10000
 POSTHOOK: query: select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t4
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-44_323_4702074512053597213/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-00_821_8531941471423951343/10000
 PREHOOK: query: explain select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value
 PREHOOK: type: QUERY
 POSTHOOK: query: explain select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value
@@ -584,7 +568,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-55_150_2508651293444068657/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-05_850_5295438124379472928/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -612,12 +596,12 @@ PREHOOK: query: select a.value from t1 a left semi join t3 b on (b.key = a.key a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-55_240_8606996641991993570/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-05_940_1477925378025144213/10000
 POSTHOOK: query: select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-35-55_240_8606996641991993570/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-05_940_1477925378025144213/10000
 val_0
 val_0
 val_0
@@ -724,7 +708,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-03_687_6338764462894002347/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-11_016_6870613802522541542/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -756,12 +740,12 @@ PREHOOK: query: select * from t1 a left semi join t2 b on a.key = b.key and b.va
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-03_797_7630600056988799823/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-11_129_6383903554820825954/10000
 POSTHOOK: query: select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-03_797_7630600056988799823/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-11_129_6383903554820825954/10000
 0	val_0
 0	val_0
 0	val_0
@@ -857,7 +841,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-13_895_8749833987137479467/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-16_235_779399372058035213/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -885,12 +869,12 @@ PREHOOK: query: select a.value from t1 a left semi join (select key from t3 wher
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-13_967_7732951019053712101/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-16_329_2275706273437432925/10000
 POSTHOOK: query: select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-13_967_7732951019053712101/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-16_329_2275706273437432925/10000
 val_10
 val_8
 val_9
@@ -996,7 +980,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-23_930_6336397226727500924/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-21_457_1331498307427022235/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -1024,12 +1008,12 @@ PREHOOK: query: select a.value from t1 a left semi join (select key , value from
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-24_008_5120080643273177462/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-21_555_5905902383917612228/10000
 POSTHOOK: query: select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-24_008_5120080643273177462/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-21_555_5905902383917612228/10000
 PREHOOK: query: explain select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value
 PREHOOK: type: QUERY
 POSTHOOK: query: explain select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value
@@ -1126,7 +1110,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-36_441_8634130304518118269/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-26_723_1440922755946250273/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -1158,12 +1142,12 @@ PREHOOK: query: select * from t2 a left semi join (select key , value from t1 wh
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-36_508_3551604947895396638/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-26_818_30173046506810770/10000
 POSTHOOK: query: select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-36_508_3551604947895396638/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-26_818_30173046506810770/10000
 4	val_2
 8	val_4
 10	val_5
@@ -1250,7 +1234,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-45_883_3501789923939810769/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-32_078_2679043381204546775/10002 
           Select Operator
             expressions:
                   expr: _col0
@@ -1288,12 +1272,12 @@ PREHOOK: query: select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-45_948_364129600871726553/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-32_166_7467327752440689603/10000
 POSTHOOK: query: select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-45_948_364129600871726553/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-32_166_7467327752440689603/10000
 0
 0
 0
@@ -1396,7 +1380,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-55_575_4599378053603033966/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-37_167_3913040878592306902/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -1428,12 +1412,12 @@ PREHOOK: query: select * from t1 a left semi join t2 b on a.key = 2*b.key sort b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-55_643_3871759699151868689/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-37_255_8499123111273265654/10000
 POSTHOOK: query: select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-36-55_643_3871759699151868689/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-37_255_8499123111273265654/10000
 0	val_0
 0	val_0
 0	val_0
@@ -1544,7 +1528,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-06_509_1266627034604561120/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-42_392_8203492842374000703/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -1581,13 +1565,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-06_996_7821060985422843977/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-42_498_7477642768461117554/10000
 POSTHOOK: query: select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-06_996_7821060985422843977/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-42_498_7477642768461117554/10000
 0	val_0	0	val_0
 0	val_0	0	val_0
 0	val_0	0	val_0
@@ -1697,7 +1681,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-19_651_1485752586517967264/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-47_638_2312914691946155301/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -1729,12 +1713,12 @@ PREHOOK: query: select * from t3 a left semi join t1 b on a.key = b.key and a.va
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-19_724_3010359613753099590/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-47_729_1185838697277763263/10000
 POSTHOOK: query: select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-19_724_3010359613753099590/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-47_729_1185838697277763263/10000
 0	val_0
 0	val_0
 0	val_0
@@ -1875,7 +1859,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-31_865_8041672431165280246/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-53_202_3850672920021608450/10002 
           Select Operator
             expressions:
                   expr: _col0
@@ -1914,13 +1898,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-31_958_2097477130102804854/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-53_303_2583889326927676663/10000
 POSTHOOK: query: select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-31_958_2097477130102804854/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-53_303_2583889326927676663/10000
 0
 0
 0
@@ -2028,7 +2012,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-39_609_6038505499338215694/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-52-58_405_4765969463451941200/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -2057,13 +2041,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-39_689_2219387084268760247/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-58_504_5713820003651410015/10000
 POSTHOOK: query: select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-39_689_2219387084268760247/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-52-58_504_5713820003651410015/10000
 0
 0
 0
@@ -2183,7 +2167,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-48_727_6848151951679507683/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-53-03_590_5882424135335798699/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -2212,13 +2196,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-48_802_2920426240777860433/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-03_688_7110160714226284605/10000
 POSTHOOK: query: select a.key from t1 a right outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-48_802_2920426240777860433/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-03_688_7110160714226284605/10000
 NULL
 NULL
 NULL
@@ -2341,7 +2325,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-57_921_1430015658693544867/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-53-08_783_8469421069250274879/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -2370,13 +2354,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-57_995_3374323430955047812/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-08_882_8562747020920066272/10000
 POSTHOOK: query: select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-37-57_995_3374323430955047812/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-08_882_8562747020920066272/10000
 NULL
 NULL
 NULL
@@ -2499,7 +2483,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-06_288_8990291848979148784/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-53-14_002_4363333810736619868/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -2528,13 +2512,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-06_381_4851737602793432516/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-14_106_6347833426207703036/10000
 POSTHOOK: query: select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-06_381_4851737602793432516/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-14_106_6347833426207703036/10000
 0
 0
 0
@@ -2657,7 +2641,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-14_800_4127574476527513/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-53-19_255_649175280232405937/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -2686,13 +2670,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-14_885_5696666954446618715/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-19_353_2282659306314528737/10000
 POSTHOOK: query: select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-14_885_5696666954446618715/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-19_353_2282659306314528737/10000
 NULL
 NULL
 NULL
@@ -2817,7 +2801,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-23_064_2361112395389588613/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-53-24_533_3457531430693293506/10002 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -2846,13 +2830,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-23_146_7021023119933807907/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-24_631_7396802807404398252/10000
 POSTHOOK: query: select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-23_146_7021023119933807907/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-24_631_7396802807404398252/10000
 NULL
 NULL
 NULL
@@ -3021,7 +3005,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-31_328_3801807852346700415/10003 
+        file:/tmp/jssarma/hive_2010-07-21_11-53-29_673_2449127370572583191/10003 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -3050,13 +3034,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t3
-PREHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-31_406_3452916753684373079/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-29_775_6410083097422203082/10000
 POSTHOOK: query: select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t3
-POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/scratchdir/hive_2010-02-12_22-38-31_406_3452916753684373079/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-53-29_775_6410083097422203082/10000
 0
 0
 0
@@ -3092,23 +3076,3 @@ POSTHOOK: Output: file:/data/users/zshao/hadoop_hive_trunk2/.ptest_2/build/ql/sc
 16
 18
 20
-PREHOOK: query: drop table t1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-PREHOOK: query: drop table t2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-PREHOOK: query: drop table t3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t3
-PREHOOK: query: drop table t4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table t4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t4
diff --git a/ql/src/test/results/clientpositive/show_tables.q.out b/ql/src/test/results/clientpositive/show_tables.q.out
index 77f98b5660..0bbd81b934 100644
--- a/ql/src/test/results/clientpositive/show_tables.q.out
+++ b/ql/src/test/results/clientpositive/show_tables.q.out
@@ -68,13 +68,3 @@ POSTHOOK: query: SHOW TABLES 'shtb_test1|shtb_test2'
 POSTHOOK: type: SHOWTABLES
 shtb_test1
 shtb_test2
-PREHOOK: query: DROP TABLE shtb_test1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE shtb_test1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@shtb_test1
-PREHOOK: query: DROP TABLE shtb_test2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE shtb_test2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@shtb_test2
diff --git a/ql/src/test/results/clientpositive/skewjoin.q.out b/ql/src/test/results/clientpositive/skewjoin.q.out
index b9baa4de3e..42b80c15ec 100644
--- a/ql/src/test/results/clientpositive/skewjoin.q.out
+++ b/ql/src/test/results/clientpositive/skewjoin.q.out
@@ -1,23 +1,3 @@
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T4
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest_j1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
@@ -256,11 +236,11 @@ POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:
 PREHOOK: query: SELECT sum(hash(key)), sum(hash(value)) FROM dest_j1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest_j1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-28-18_221_756482606592619495/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-54-34_773_8974429205684276500/10000
 POSTHOOK: query: SELECT sum(hash(key)), sum(hash(value)) FROM dest_j1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest_j1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-28-18_221_756482606592619495/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-54-34_773_8974429205684276500/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 278697	101852390308
@@ -410,7 +390,7 @@ PREHOOK: Input: default@t4
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-28-46_933_3748785950132645671/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-54-58_628_3356378875993632263/10000
 POSTHOOK: query: SELECT /*+ STREAMTABLE(a) */ *
 FROM T1 a JOIN T2 b ON a.key = b.key
           JOIN T3 c ON b.key = c.key
@@ -420,7 +400,7 @@ POSTHOOK: Input: default@t4
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-28-46_933_3748785950132645671/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-54-58_628_3356378875993632263/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 2	12	2	22	2	12	2	12
@@ -570,7 +550,7 @@ PREHOOK: Input: default@t4
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-28-54_591_7735680493194698737/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-55-01_582_1289945692817010031/10000
 POSTHOOK: query: SELECT /*+ STREAMTABLE(a,c) */ *
 FROM T1 a JOIN T2 b ON a.key = b.key
           JOIN T3 c ON b.key = c.key
@@ -580,7 +560,7 @@ POSTHOOK: Input: default@t4
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-28-54_591_7735680493194698737/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-55-01_582_1289945692817010031/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 2	12	2	22	2	12	2	12
@@ -670,7 +650,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-29-01_179_7724911428408855420/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-55-04_590_6073131371882078309/10002 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -715,12 +695,12 @@ PREHOOK: query: FROM T1 a JOIN src c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a)
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-29-01_540_565341542651646006/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-55-04_686_5498328271539894923/10000
 POSTHOOK: query: FROM T1 a JOIN src c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key))
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-29-01_540_565341542651646006/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-55-04_686_5498328271539894923/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 198	6274	194
@@ -908,7 +888,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-29-10_811_4042443695658644025/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-55-10_006_6998240996689349171/10002 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -952,7 +932,7 @@ ON (x.key = Y.key)
 SELECT sum(hash(Y.key)), sum(hash(Y.value))
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-29-11_447_1334505274424294054/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-55-10_210_3789281218627696460/10000
 POSTHOOK: query: FROM 
 (SELECT src.* FROM src) x
 JOIN 
@@ -961,7 +941,7 @@ ON (x.key = Y.key)
 SELECT sum(hash(Y.key)), sum(hash(Y.value))
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-29-11_447_1334505274424294054/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-55-10_210_3789281218627696460/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 44481300	101852390308
@@ -1159,7 +1139,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-30-31_216_4251844864071091426/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-56-24_288_8833007000539087866/10002 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -1203,7 +1183,7 @@ ON (x.key = Y.key and substring(x.value, 5)=substring(y.value, 5)+1)
 SELECT sum(hash(Y.key)), sum(hash(Y.value))
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-30-32_014_2966662653258024270/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-56-24_510_9175473615433048274/10000
 POSTHOOK: query: FROM 
 (SELECT src.* FROM src) x
 JOIN 
@@ -1212,7 +1192,7 @@ ON (x.key = Y.key and substring(x.value, 5)=substring(y.value, 5)+1)
 SELECT sum(hash(Y.key)), sum(hash(Y.value))
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-30-32_014_2966662653258024270/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-56-24_510_9175473615433048274/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 NULL	NULL
@@ -1490,7 +1470,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-04_750_6194673445329677427/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-57-04_081_7395777836875573715/10002 
             Reduce Output Operator
               sort order: 
               tag: -1
@@ -1659,7 +1639,7 @@ JOIN
 ON src1.c1 = src3.c5 AND src3.c5 < 80
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-05_886_658859308132239366/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-57-04_482_4874015887141984879/10000
 POSTHOOK: query: SELECT sum(hash(src1.c1)), sum(hash(src2.c4))
 FROM
 (SELECT src.key as c1, src.value as c2 from src) src1
@@ -1671,7 +1651,7 @@ JOIN
 ON src1.c1 = src3.c5 AND src3.c5 < 80
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-05_886_658859308132239366/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-57-04_482_4874015887141984879/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 293143	-136853010385
@@ -1748,7 +1728,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-45_414_6561971598792572097/10002 
+        file:/tmp/jssarma/hive_2010-07-21_11-57-50_557_7888570900112459597/10002 
           Select Operator
             expressions:
                   expr: _col0
@@ -1808,55 +1788,55 @@ STAGE PLANS:
 PREHOOK: query: SELECT /*+ mapjoin(v)*/ sum(hash(k.key)), sum(hash(v.val)) FROM T1 k LEFT OUTER JOIN T1 v ON k.key+1=v.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-45_819_8893484307159075646/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-57-50_652_7524559661828781918/10000
 POSTHOOK: query: SELECT /*+ mapjoin(v)*/ sum(hash(k.key)), sum(hash(v.val)) FROM T1 k LEFT OUTER JOIN T1 v ON k.key+1=v.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-45_819_8893484307159075646/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-57-50_652_7524559661828781918/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 372	6320
 PREHOOK: query: select /*+ mapjoin(k)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.val
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-52_646_8054489073820291796/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-57-55_680_7454971209844095002/10000
 POSTHOOK: query: select /*+ mapjoin(k)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.val
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-52_646_8054489073820291796/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-57-55_680_7454971209844095002/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 NULL	NULL
 PREHOOK: query: select /*+ mapjoin(k)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-59_404_6324512370718311890/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-00_759_2503525506382756482/10000
 POSTHOOK: query: select /*+ mapjoin(k)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-31-59_404_6324512370718311890/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-00_759_2503525506382756482/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 429	12643
 PREHOOK: query: select sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-06_304_4816645829569851115/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-05_940_2644358672516865932/10000
 POSTHOOK: query: select sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-06_304_4816645829569851115/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-05_940_2644358672516865932/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 429	12643
 PREHOOK: query: select count(1) from  T1 a join T1 b on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-16_483_8042226294549297199/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-13_601_8769607871191941131/10000
 POSTHOOK: query: select count(1) from  T1 a join T1 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-16_483_8042226294549297199/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-13_601_8769607871191941131/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 8
@@ -1864,12 +1844,12 @@ PREHOOK: query: FROM T1 a LEFT OUTER JOIN T2 c ON c.key+1=a.key SELECT sum(hash(
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-27_501_4841981538239651087/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-21_418_5101143786482013059/10000
 POSTHOOK: query: FROM T1 a LEFT OUTER JOIN T2 c ON c.key+1=a.key SELECT sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key))
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-27_501_4841981538239651087/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-21_418_5101143786482013059/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 317	9462	50
@@ -1877,12 +1857,12 @@ PREHOOK: query: FROM T1 a RIGHT OUTER JOIN T2 c ON c.key+1=a.key SELECT /*+ STRE
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-34_947_2689434768638553625/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-26_607_2103870217105546568/10000
 POSTHOOK: query: FROM T1 a RIGHT OUTER JOIN T2 c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key))
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-34_947_2689434768638553625/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-26_607_2103870217105546568/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 51	1570	318
@@ -1890,12 +1870,12 @@ PREHOOK: query: FROM T1 a FULL OUTER JOIN T2 c ON c.key+1=a.key SELECT /*+ STREA
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-42_009_8260289531826672082/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-31_727_8374135807047103504/10000
 POSTHOOK: query: FROM T1 a FULL OUTER JOIN T2 c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key))
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-42_009_8260289531826672082/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-31_727_8374135807047103504/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 317	9462	318
@@ -1903,12 +1883,12 @@ PREHOOK: query: SELECT sum(hash(src1.key)), sum(hash(src1.val)), sum(hash(src2.k
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-48_929_687856413641903607/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-36_776_8115766802135436631/10000
 POSTHOOK: query: SELECT sum(hash(src1.key)), sum(hash(src1.val)), sum(hash(src2.key)) FROM T1 src1 LEFT OUTER JOIN T2 src2 ON src1.key+1 = src2.key RIGHT OUTER JOIN T2 src3 ON src2.key = src3.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-48_929_687856413641903607/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-36_776_8115766802135436631/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 370	11003	377
@@ -1916,58 +1896,23 @@ PREHOOK: query: SELECT sum(hash(src1.key)), sum(hash(src1.val)), sum(hash(src2.k
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-55_994_3719817742027708223/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-42_007_1359296702764686796/10000
 POSTHOOK: query: SELECT sum(hash(src1.key)), sum(hash(src1.val)), sum(hash(src2.key)) FROM T1 src1 JOIN T2 src2 ON src1.key+1 = src2.key JOIN T2 src3 ON src2.key = src3.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-32-55_994_3719817742027708223/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-42_007_1359296702764686796/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 370	11003	377
 PREHOOK: query: select /*+ mapjoin(v)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k left outer join T1 v on k.key+1=v.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-33-10_325_5236716677181376533/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-52_321_6570425935975725957/10000
 POSTHOOK: query: select /*+ mapjoin(v)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k left outer join T1 v on k.key+1=v.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-33-10_325_5236716677181376533/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-52_321_6570425935975725957/10000
 POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
 372	6320
-PREHOOK: query: DROP TABLE dest_j1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest_j1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest_j1
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t3
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE T4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t4
-POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src)src1.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest_j1.value SIMPLE [(src)src2.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_1.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_1.q.out
index ba5f56a036..f49d7e9d08 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_1.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_1.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
@@ -110,12 +98,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-23_016_6641092569125426318/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-58_160_2147647945746405898/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-23_016_6641092569125426318/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-58-58_160_2147647945746405898/10000
 PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
 PREHOOK: type: QUERY
@@ -186,12 +174,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-27_563_2034910935092261085/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-00_868_8045473741395817195/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-27_563_2034910935092261085/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-00_868_8045473741395817195/10000
 1	val_1	NULL	NULL
 3	val_3	NULL	NULL
 4	val_4	NULL	NULL
@@ -267,12 +255,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-31_419_6704266785015748074/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-03_575_2296154727598489186/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-31_419_6704266785015748074/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-03_575_2296154727598489186/10000
 NULL	NULL	20	val_20
 NULL	NULL	23	val_23
 NULL	NULL	25	val_25
@@ -347,12 +335,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-35_757_2482613004751266452/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-06_230_3764138320854595509/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-35_757_2482613004751266452/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-06_230_3764138320854595509/10000
 1	val_1	NULL	NULL
 3	val_3	NULL	NULL
 4	val_4	NULL	NULL
@@ -432,12 +420,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-40_043_2203303013426679035/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-08_885_5981278369954590271/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-40_043_2203303013426679035/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-08_885_5981278369954590271/10000
 PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
 PREHOOK: type: QUERY
@@ -508,12 +496,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-44_729_4570053663662247806/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-11_538_4757194244114259808/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-44_729_4570053663662247806/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-11_538_4757194244114259808/10000
 1	val_1	NULL	NULL
 3	val_3	NULL	NULL
 4	val_4	NULL	NULL
@@ -589,12 +577,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-49_658_4830763295823673239/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-14_194_1923984070343940268/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-49_658_4830763295823673239/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-14_194_1923984070343940268/10000
 NULL	NULL	20	val_20
 NULL	NULL	23	val_23
 NULL	NULL	25	val_25
@@ -669,12 +657,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-53_844_552634372790856258/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-16_789_1314118076102856939/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-24-53_844_552634372790856258/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-16_789_1314118076102856939/10000
 1	val_1	NULL	NULL
 3	val_3	NULL	NULL
 4	val_4	NULL	NULL
@@ -684,18 +672,3 @@ NULL	NULL	20	val_20
 NULL	NULL	23	val_23
 NULL	NULL	25	val_25
 NULL	NULL	30	val_30
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_3
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_2
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_1
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_2.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_2.q.out
index 3668e08bbc..f92cff9eb1 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_2.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_2.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
@@ -110,12 +98,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-12_974_8125076326452015201/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-20_114_6729049322478705958/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-12_974_8125076326452015201/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-20_114_6729049322478705958/10000
 4	val_4	4	val_4
 10	val_10	10	val_10
 PREHOOK: query: explain
@@ -188,12 +176,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-17_315_263973989687054916/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-22_746_1628423413210127007/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-17_315_263973989687054916/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-22_746_1628423413210127007/10000
 1	val_1	NULL	NULL
 3	val_3	NULL	NULL
 4	val_4	4	val_4
@@ -269,12 +257,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-21_146_6176650628602536007/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-25_414_5347137080834424806/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-21_146_6176650628602536007/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-25_414_5347137080834424806/10000
 4	val_4	4	val_4
 10	val_10	10	val_10
 NULL	NULL	17	val_17
@@ -351,12 +339,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-25_346_6914946660741597957/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-28_070_5426906800620762025/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-25_346_6914946660741597957/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-28_070_5426906800620762025/10000
 1	val_1	NULL	NULL
 3	val_3	NULL	NULL
 4	val_4	4	val_4
@@ -436,12 +424,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-29_569_7469092976690151810/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-30_739_59573784104638805/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-29_569_7469092976690151810/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-30_739_59573784104638805/10000
 4	val_4	4	val_4
 10	val_10	10	val_10
 PREHOOK: query: explain
@@ -514,12 +502,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-34_270_3218310252140217265/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-33_409_6668885682210635497/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-34_270_3218310252140217265/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-33_409_6668885682210635497/10000
 1	val_1	NULL	NULL
 3	val_3	NULL	NULL
 4	val_4	4	val_4
@@ -595,12 +583,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-39_271_4438844521663002310/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-36_171_6013991795978323279/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-39_271_4438844521663002310/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-36_171_6013991795978323279/10000
 4	val_4	4	val_4
 10	val_10	10	val_10
 NULL	NULL	17	val_17
@@ -677,12 +665,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-44_008_7608838401063726842/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-38_942_2715973452861735765/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-29-44_008_7608838401063726842/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-38_942_2715973452861735765/10000
 1	val_1	NULL	NULL
 3	val_3	NULL	NULL
 4	val_4	4	val_4
@@ -692,18 +680,3 @@ NULL	NULL	17	val_17
 NULL	NULL	19	val_19
 NULL	NULL	20	val_20
 NULL	NULL	23	val_23
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_3
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_2
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_1
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_3.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_3.q.out
index 9b27c9da49..e51b2a204f 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_3.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_3.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
@@ -110,12 +98,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-30_890_4326381724412032730/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-42_230_3901951310502370670/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-30_890_4326381724412032730/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-42_230_3901951310502370670/10000
 20	val_20	20	val_20
 23	val_23	23	val_23
 PREHOOK: query: explain
@@ -188,12 +176,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-34_802_1123185207360865240/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-44_878_1636786032063672327/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-34_802_1123185207360865240/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-44_878_1636786032063672327/10000
 20	val_20	20	val_20
 23	val_23	23	val_23
 25	val_25	NULL	NULL
@@ -268,12 +256,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-38_515_6615795308131459187/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-47_542_136511562565391235/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-38_515_6615795308131459187/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-47_542_136511562565391235/10000
 NULL	NULL	4	val_4
 NULL	NULL	10	val_10
 NULL	NULL	17	val_17
@@ -350,12 +338,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-42_386_5906604295023744125/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-50_137_5584442750772813211/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-42_386_5906604295023744125/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-50_137_5584442750772813211/10000
 NULL	NULL	4	val_4
 NULL	NULL	10	val_10
 NULL	NULL	17	val_17
@@ -434,12 +422,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-46_265_3116250152047550346/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-52_734_8865231468690927845/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-46_265_3116250152047550346/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-52_734_8865231468690927845/10000
 20	val_20	20	val_20
 23	val_23	23	val_23
 PREHOOK: query: explain
@@ -512,12 +500,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-50_307_6279346462102566646/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-55_437_3419031084272250181/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-50_307_6279346462102566646/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-55_437_3419031084272250181/10000
 20	val_20	20	val_20
 23	val_23	23	val_23
 25	val_25	NULL	NULL
@@ -592,12 +580,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-54_229_7460761162272268456/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-58_113_8843462871528159229/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-54_229_7460761162272268456/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_11-59-58_113_8843462871528159229/10000
 NULL	NULL	4	val_4
 NULL	NULL	10	val_10
 NULL	NULL	17	val_17
@@ -674,12 +662,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_2
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-58_806_7833473465102023484/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-00_903_4320870631966344212/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_2
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-31-58_806_7833473465102023484/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-00_903_4320870631966344212/10000
 NULL	NULL	4	val_4
 NULL	NULL	10	val_10
 NULL	NULL	17	val_17
@@ -688,18 +676,3 @@ NULL	NULL	19	val_19
 23	val_23	23	val_23
 25	val_25	NULL	NULL
 30	val_30	NULL	NULL
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_3
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_2
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_1
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_4.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_4.q.out
index 845eb1305f..b33f0af145 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_4.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_4.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
@@ -122,13 +110,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-39_594_4785333669695524791/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-04_369_6894437003845124922/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-39_594_4785333669695524791/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-04_369_6894437003845124922/10000
 PREHOOK: query: explain
 select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 PREHOOK: type: QUERY
@@ -211,13 +199,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-43_909_3797740741379315653/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-07_477_1024337567507880023/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-43_909_3797740741379315653/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-07_477_1024337567507880023/10000
 PREHOOK: query: explain
 select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key
 PREHOOK: type: QUERY
@@ -300,13 +288,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-47_754_7214057582413971326/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-10_477_2598270851351649787/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-47_754_7214057582413971326/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-10_477_2598270851351649787/10000
 1	val_1	NULL	NULL	NULL	NULL
 3	val_3	NULL	NULL	NULL	NULL
 4	val_4	NULL	NULL	NULL	NULL
@@ -394,13 +382,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-51_983_9068539452903561807/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-13_272_3162263331814942921/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-51_983_9068539452903561807/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-13_272_3162263331814942921/10000
 NULL	NULL	NULL	NULL	4	val_4
 NULL	NULL	NULL	NULL	10	val_10
 NULL	NULL	NULL	NULL	17	val_17
@@ -489,13 +477,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-56_291_3462476089219064943/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-15_985_2079439490852572040/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-33-56_291_3462476089219064943/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-15_985_2079439490852572040/10000
 1	val_1	NULL	NULL	NULL	NULL
 3	val_3	NULL	NULL	NULL	NULL
 4	val_4	NULL	NULL	NULL	NULL
@@ -589,13 +577,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-00_348_6130164448020992768/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-18_694_6706221629562778220/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-00_348_6130164448020992768/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-18_694_6706221629562778220/10000
 NULL	NULL	20	val_20	20	val_20
 NULL	NULL	23	val_23	23	val_23
 PREHOOK: query: explain
@@ -680,13 +668,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-04_708_6078968725082023203/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-21_507_7272100397679559384/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-04_708_6078968725082023203/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-21_507_7272100397679559384/10000
 NULL	NULL	20	val_20	20	val_20
 NULL	NULL	23	val_23	23	val_23
 NULL	NULL	25	val_25	NULL	NULL
@@ -773,13 +761,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-09_394_3283017820422337878/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-24_251_4640595451549814065/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-09_394_3283017820422337878/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-24_251_4640595451549814065/10000
 NULL	NULL	NULL	NULL	4	val_4
 NULL	NULL	NULL	NULL	10	val_10
 NULL	NULL	NULL	NULL	17	val_17
@@ -868,13 +856,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-14_134_3541355833837353833/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-27_092_4533249428280289957/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-14_134_3541355833837353833/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-27_092_4533249428280289957/10000
 NULL	NULL	NULL	NULL	4	val_4
 NULL	NULL	NULL	NULL	10	val_10
 NULL	NULL	NULL	NULL	17	val_17
@@ -965,13 +953,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-18_619_4821896421115004375/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-29_832_2145197883545130418/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-18_619_4821896421115004375/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-29_832_2145197883545130418/10000
 NULL	NULL	20	val_20	20	val_20
 NULL	NULL	23	val_23	23	val_23
 PREHOOK: query: explain
@@ -1056,13 +1044,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-23_002_6722154955469787745/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-32_567_8429095297894742947/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-23_002_6722154955469787745/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-32_567_8429095297894742947/10000
 1	val_1	NULL	NULL	NULL	NULL
 3	val_3	NULL	NULL	NULL	NULL
 4	val_4	NULL	NULL	NULL	NULL
@@ -1154,13 +1142,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-27_431_3646775244587101332/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-35_425_7494156323664202716/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-27_431_3646775244587101332/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-35_425_7494156323664202716/10000
 NULL	NULL	NULL	NULL	4	val_4
 NULL	NULL	NULL	NULL	10	val_10
 NULL	NULL	NULL	NULL	17	val_17
@@ -1249,13 +1237,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-31_968_5586945031082623359/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-38_154_5107590626190601504/10000
 POSTHOOK: query: select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-34-31_968_5586945031082623359/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-38_154_5107590626190601504/10000
 1	val_1	NULL	NULL	NULL	NULL
 3	val_3	NULL	NULL	NULL	NULL
 4	val_4	NULL	NULL	NULL	NULL
@@ -1269,18 +1257,3 @@ NULL	NULL	20	val_20	20	val_20
 NULL	NULL	23	val_23	23	val_23
 NULL	NULL	25	val_25	NULL	NULL
 NULL	NULL	30	val_30	NULL	NULL
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_3
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_2
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_1
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_5.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_5.q.out
index 864ea0b7ff..67e137968b 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_5.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_5.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
@@ -122,13 +110,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-22_865_4487686662397360125/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-41_524_6364697581539120113/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-22_865_4487686662397360125/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-41_524_6364697581539120113/10000
 PREHOOK: query: explain
 select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 PREHOOK: type: QUERY
@@ -211,13 +199,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-27_181_2689448944799681233/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-44_285_5430902360454396405/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-27_181_2689448944799681233/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-44_285_5430902360454396405/10000
 PREHOOK: query: explain
 select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key
 PREHOOK: type: QUERY
@@ -300,13 +288,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-30_958_2862069044325639274/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-47_123_2017928206991935005/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-30_958_2862069044325639274/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-47_123_2017928206991935005/10000
 1	val_1	NULL	NULL	NULL	NULL
 3	val_3	NULL	NULL	NULL	NULL
 4	val_4	NULL	NULL	NULL	NULL
@@ -394,13 +382,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-35_461_5426046396711146559/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-49_934_7244598924132392571/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-35_461_5426046396711146559/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-49_934_7244598924132392571/10000
 NULL	NULL	NULL	NULL	4	val_4
 NULL	NULL	NULL	NULL	10	val_10
 NULL	NULL	NULL	NULL	17	val_17
@@ -489,13 +477,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-39_715_1766943698430352843/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-52_668_3083976054304451937/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-39_715_1766943698430352843/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-52_668_3083976054304451937/10000
 1	val_1	NULL	NULL	NULL	NULL
 3	val_3	NULL	NULL	NULL	NULL
 4	val_4	NULL	NULL	NULL	NULL
@@ -589,13 +577,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-43_973_8806162188078780005/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-55_443_5909131157897439506/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-43_973_8806162188078780005/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-55_443_5909131157897439506/10000
 NULL	NULL	20	val_20	20	val_20
 NULL	NULL	23	val_23	23	val_23
 PREHOOK: query: explain
@@ -680,13 +668,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-48_619_2725384740684408102/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-58_148_4762824573436827785/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-48_619_2725384740684408102/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-00-58_148_4762824573436827785/10000
 NULL	NULL	20	val_20	20	val_20
 NULL	NULL	23	val_23	23	val_23
 NULL	NULL	25	val_25	NULL	NULL
@@ -773,13 +761,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-53_557_849175655418581108/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-00_906_6750483002470270430/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-53_557_849175655418581108/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-00_906_6750483002470270430/10000
 NULL	NULL	NULL	NULL	4	val_4
 NULL	NULL	NULL	NULL	10	val_10
 NULL	NULL	NULL	NULL	17	val_17
@@ -868,13 +856,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-58_093_7448714045296241567/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-03_722_2114361761413441528/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-36-58_093_7448714045296241567/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-03_722_2114361761413441528/10000
 NULL	NULL	NULL	NULL	4	val_4
 NULL	NULL	NULL	NULL	10	val_10
 NULL	NULL	NULL	NULL	17	val_17
@@ -965,13 +953,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-02_568_4148492212234051327/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-06_447_2677961588327359326/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-02_568_4148492212234051327/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-06_447_2677961588327359326/10000
 NULL	NULL	20	val_20	20	val_20
 NULL	NULL	23	val_23	23	val_23
 PREHOOK: query: explain
@@ -1056,13 +1044,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-06_625_1217755236567536175/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-09_243_6046930209513279147/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-06_625_1217755236567536175/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-09_243_6046930209513279147/10000
 1	val_1	NULL	NULL	NULL	NULL
 3	val_3	NULL	NULL	NULL	NULL
 4	val_4	NULL	NULL	NULL	NULL
@@ -1154,13 +1142,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-11_033_6560347088229684794/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-12_084_8336441249717148411/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-11_033_6560347088229684794/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-12_084_8336441249717148411/10000
 NULL	NULL	NULL	NULL	4	val_4
 NULL	NULL	NULL	NULL	10	val_10
 NULL	NULL	NULL	NULL	17	val_17
@@ -1249,13 +1237,13 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_2
 PREHOOK: Input: default@smb_bucket_3
 PREHOOK: Input: default@smb_bucket_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-15_593_2231203586411799562/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-14_792_2190079985184779302/10000
 POSTHOOK: query: select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_2
 POSTHOOK: Input: default@smb_bucket_3
 POSTHOOK: Input: default@smb_bucket_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-15_593_2231203586411799562/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-14_792_2190079985184779302/10000
 1	val_1	NULL	NULL	NULL	NULL
 3	val_3	NULL	NULL	NULL	NULL
 4	val_4	NULL	NULL	NULL	NULL
@@ -1269,18 +1257,3 @@ NULL	NULL	20	val_20	20	val_20
 NULL	NULL	23	val_23	23	val_23
 NULL	NULL	25	val_25	NULL	NULL
 NULL	NULL	30	val_30	NULL	NULL
-PREHOOK: query: drop table smb_bucket_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_3
-PREHOOK: query: drop table smb_bucket_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_2
-PREHOOK: query: drop table smb_bucket_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_1
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_6.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_6.q.out
index 640f4dade4..261500d8d5 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_6.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_6.q.out
@@ -1,16 +1,8 @@
-PREHOOK: query: drop table smb_bucket4_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS RCFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@smb_bucket4_1
-PREHOOK: query: drop table smb_bucket4_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS RCFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS RCFILE
@@ -155,11 +147,11 @@ POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name
 PREHOOK: query: select * from smb_join_results order by k1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_join_results
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-37_290_404970238717165532/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-25_651_5833528843838324780/10000
 POSTHOOK: query: select * from smb_join_results order by k1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_join_results
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-37_290_404970238717165532/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-25_651_5833528843838324780/10000
 POSTHOOK: Lineage: smb_bucket4_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1221,11 +1213,11 @@ POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name
 PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
 PREHOOK: type: QUERY
 PREHOOK: Input: default@normal_join_results
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-47_786_921558350057549374/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-31_903_6139272899107061780/10000
 POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@normal_join_results
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-47_786_921558350057549374/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-31_903_6139272899107061780/10000
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
@@ -1242,11 +1234,11 @@ POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name
 PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_join_results
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-52_381_5035665633282282085/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-34_469_4507749468485572228/10000
 POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_join_results
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-37-52_381_5035665633282282085/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-34_469_4507749468485572228/10000
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
@@ -1411,11 +1403,11 @@ POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name
 PREHOOK: query: select * from smb_join_results order by k1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_join_results
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-38-05_908_7509522223168963029/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-42_397_14549692505951984/10000
 POSTHOOK: query: select * from smb_join_results order by k1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_join_results
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-38-05_908_7509522223168963029/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-42_397_14549692505951984/10000
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
@@ -2501,11 +2493,11 @@ POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name
 PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
 PREHOOK: type: QUERY
 PREHOOK: Input: default@normal_join_results
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-38-16_247_9045003684131152373/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-48_586_6079549206607200839/10000
 POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@normal_join_results
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-38-16_247_9045003684131152373/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-48_586_6079549206607200839/10000
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -2534,11 +2526,11 @@ POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name
 PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_join_results
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-38-20_757_9079491273031478705/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-51_149_3714210585214726063/10000
 POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_join_results
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-38-20_757_9079491273031478705/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-51_149_3714210585214726063/10000
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -2985,160 +2977,12 @@ PREHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a join smb_bucket4
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-38-33_816_1444042793831514393/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-59_249_5855564907499917088/10000
 POSTHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key join smb_bucket4_2 c on b.key = c.key where a.key>1000
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-38-33_816_1444042793831514393/10000
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-PREHOOK: query: drop table smb_join_results
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_join_results
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_join_results
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-PREHOOK: query: drop table normal_join_results
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table normal_join_results
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@normal_join_results
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-PREHOOK: query: drop table smb_bucket4_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket4_1
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:from deserializer), ]
-PREHOOK: query: drop table smb_bucket4_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket4_2
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-01-59_249_5855564907499917088/10000
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:from deserializer), ]
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_7.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_7.q.out
index e28a168871..33460556d1 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_7.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_7.q.out
@@ -1,33 +1,13 @@
-PREHOOK: query: drop table smb_bucket4_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@smb_bucket4_1
-PREHOOK: query: drop table smb_bucket4_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@smb_bucket4_2
-PREHOOK: query: drop table smb_join_results
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_join_results
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table smb_join_results_empty_bigtable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_join_results_empty_bigtable
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table normal_join_results
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table normal_join_results
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table smb_join_results(k1 int, v1 string, k2 int, v2 string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table smb_join_results(k1 int, v1 string, k2 int, v2 string)
@@ -108,11 +88,11 @@ POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.F
 PREHOOK: query: select * from smb_join_results_empty_bigtable order by k1, v1, k2, v2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_join_results_empty_bigtable
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-48_526_6972277574358054550/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-10_165_8236782972191051128/10000
 POSTHOOK: query: select * from smb_join_results_empty_bigtable order by k1, v1, k2, v2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_join_results_empty_bigtable
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-48_526_6972277574358054550/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-10_165_8236782972191051128/10000
 POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
@@ -738,11 +718,11 @@ POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.F
 PREHOOK: query: select * from smb_join_results order by k1, v1, k2, v2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_join_results
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-56_214_5633138583101239302/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-15_380_722747470286660046/10000
 POSTHOOK: query: select * from smb_join_results order by k1, v1, k2, v2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_join_results
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-16-56_214_5633138583101239302/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-15_380_722747470286660046/10000
 POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
@@ -1288,11 +1268,11 @@ POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.F
 PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
 PREHOOK: type: QUERY
 PREHOOK: Input: default@normal_join_results
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-17-05_584_7093435845128150848/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-20_549_3444585223992316265/10000
 POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@normal_join_results
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-17-05_584_7093435845128150848/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-20_549_3444585223992316265/10000
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
@@ -1315,11 +1295,11 @@ POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.F
 PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_join_results
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-17-09_400_774289249409782973/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-23_110_6679411607337610857/10000
 POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_join_results
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-17-09_400_774289249409782973/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-23_110_6679411607337610857/10000
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
@@ -1342,11 +1322,11 @@ POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.F
 PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_join_results_empty_bigtable
-PREHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-17-13_031_4148210738549073193/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-25_737_1237303922670794948/10000
 POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_join_results_empty_bigtable
-POSTHOOK: Output: file:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-04-06_13-17-13_031_4148210738549073193/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-25_737_1237303922670794948/10000
 POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
 POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
@@ -1366,118 +1346,3 @@ POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.F
 POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
 POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
 0	130091	0	36210398070
-PREHOOK: query: drop table smb_join_results
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_join_results
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_join_results
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table smb_join_results_empty_bigtable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_join_results_empty_bigtable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_join_results_empty_bigtable
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table normal_join_results
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table normal_join_results
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@normal_join_results
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table smb_bucket4_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket4_1
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-PREHOOK: query: drop table smb_bucket4_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket4_2
-POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_8.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_8.q.out
index be6bca9794..c122eb4499 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_8.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_8.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table smb_bucket_input
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_input
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table smb_bucket_input (key int, value string) stored as rcfile
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table smb_bucket_input (key int, value string) stored as rcfile
@@ -12,28 +8,16 @@ PREHOOK: type: LOAD
 POSTHOOK: query: load data local inpath '../data/files/smb_bucket_input.rc' into table smb_bucket_input
 POSTHOOK: type: LOAD
 POSTHOOK: Output: default@smb_bucket_input
-PREHOOK: query: drop table smb_bucket4_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@smb_bucket4_1
-PREHOOK: query: drop table smb_bucket4_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@smb_bucket4_2
-PREHOOK: query: drop table smb_bucket4_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE smb_bucket4_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE smb_bucket4_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS
@@ -65,12 +49,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-17_294_4701111753866603821/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-34_137_8141051139723931378/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-17_294_4701111753866603821/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-34_137_8141051139723931378/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -85,12 +69,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-20_869_3270207023691398039/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-36_691_9046607359163451591/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-20_869_3270207023691398039/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-36_691_9046607359163451591/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -135,12 +119,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-33_719_8439312857406410646/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-44_394_7557765393788088271/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-33_719_8439312857406410646/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-44_394_7557765393788088271/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
@@ -157,12 +141,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-37_711_6306791296707583726/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-46_918_7232119579754498838/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-37_711_6306791296707583726/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-46_918_7232119579754498838/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
@@ -217,12 +201,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-49_594_9071000043419801849/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-54_681_4952863565750203661/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-49_594_9071000043419801849/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-54_681_4952863565750203661/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -241,12 +225,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-53_241_4070002271822115643/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-57_249_6696803477494534544/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-41-53_241_4070002271822115643/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-02-57_249_6696803477494534544/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -311,12 +295,12 @@ PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-42-06_331_7435853405274714131/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-05_070_1157762852634375607/10000
 POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-42-06_331_7435853405274714131/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-05_070_1157762852634375607/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -340,12 +324,12 @@ PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join sm
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-42-09_999_7766443637433911014/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-07_631_8246437502603272669/10000
 POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-42-09_999_7766443637433911014/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-07_631_8246437502603272669/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -455,14 +439,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_3
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-42-27_366_3890877843061940573/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-18_035_8194510363795325400/10000
 POSTHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 full outer join smb_bucket4_3 c on a.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_3
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-42-27_366_3890877843061940573/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-18_035_8194510363795325400/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -597,14 +581,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_3
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-42-44_318_1144149231454696397/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-28_546_3828350776248819685/10000
 POSTHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 full outer join smb_bucket4_3 c on a.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_3
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-42-44_318_1144149231454696397/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-28_546_3828350776248819685/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -762,14 +746,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_3
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-43-03_722_518907510658738492/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-38_980_3157892661865975232/10000
 POSTHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 full outer join smb_bucket4_3 c on a.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_3
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-43-03_722_518907510658738492/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-38_980_3157892661865975232/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -950,14 +934,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_3
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-43-22_444_7512096506570194566/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-49_348_5700728228920265228/10000
 POSTHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 full outer join smb_bucket4_3 c on a.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_3
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-43-22_444_7512096506570194566/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-49_348_5700728228920265228/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -1162,14 +1146,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket4_2
 PREHOOK: Input: default@smb_bucket4_3
 PREHOOK: Input: default@smb_bucket4_1
-PREHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-43-38_318_5250228255491103376/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-59_852_8218593296814549763/10000
 POSTHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 full outer join smb_bucket4_3 c on a.key=c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket4_2
 POSTHOOK: Input: default@smb_bucket4_3
 POSTHOOK: Input: default@smb_bucket4_1
-POSTHOOK: Output: file:/var/folders/6g/6grtCwPMEf4sqHUPpy6xQG9ByHg/-Tmp-/heyongqiang/hive_2010-06-25_22-43-38_318_5250228255491103376/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-03-59_852_8218593296814549763/10000
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
 POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
@@ -1219,207 +1203,3 @@ POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_inpu
 1000	val_1000	NULL	NULL	NULL	NULL
 NULL	NULL	4000	val_125	NULL	NULL
 NULL	NULL	NULL	NULL	5000	val_125
-PREHOOK: query: drop table smb_bucket4_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket4_1
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-PREHOOK: query: drop table smb_bucket4_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket4_2
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-PREHOOK: query: drop table smb_bucket4_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket4_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket4_3
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-PREHOOK: query: drop table smb_bucket_input
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table smb_bucket_input
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@smb_bucket_input
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
-POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:from deserializer), ]
diff --git a/ql/src/test/results/clientpositive/symlink_text_input_format.q.out b/ql/src/test/results/clientpositive/symlink_text_input_format.q.out
index 792fa53444..55972dbd4c 100644
--- a/ql/src/test/results/clientpositive/symlink_text_input_format.q.out
+++ b/ql/src/test/results/clientpositive/symlink_text_input_format.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE symlink_text_input_format
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE symlink_text_input_format
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: EXPLAIN
 CREATE TABLE symlink_text_input_format (key STRING, value STRING) STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
 PREHOOK: type: CREATETABLE
@@ -87,11 +83,11 @@ STAGE PLANS:
 PREHOOK: query: SELECT * FROM symlink_text_input_format order by key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@symlink_text_input_format
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-06_15-29-50_024_8543560563248210858/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-11_531_6628783367593217570/10000
 POSTHOOK: query: SELECT * FROM symlink_text_input_format order by key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@symlink_text_input_format
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-06_15-29-50_024_8543560563248210858/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-11_531_6628783367593217570/10000
 1	11
 2	12
 2	12
@@ -157,11 +153,11 @@ STAGE PLANS:
 PREHOOK: query: SELECT value FROM symlink_text_input_format order by value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@symlink_text_input_format
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-06_15-29-54_036_4918019810802046490/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-14_067_9110589625279027226/10000
 POSTHOOK: query: SELECT value FROM symlink_text_input_format order by value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@symlink_text_input_format
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-06_15-29-54_036_4918019810802046490/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-14_067_9110589625279027226/10000
 11
 12
 12
@@ -236,14 +232,9 @@ STAGE PLANS:
 PREHOOK: query: SELECT count(1) FROM symlink_text_input_format
 PREHOOK: type: QUERY
 PREHOOK: Input: default@symlink_text_input_format
-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-06_15-29-57_782_3642968004654844824/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-16_606_6863105097181523081/10000
 POSTHOOK: query: SELECT count(1) FROM symlink_text_input_format
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@symlink_text_input_format
-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/scratchdir/hive_2010-04-06_15-29-57_782_3642968004654844824/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-16_606_6863105097181523081/10000
 16
-PREHOOK: query: DROP TABLE symlink_text_input_format
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE symlink_text_input_format
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@symlink_text_input_format
diff --git a/ql/src/test/results/clientpositive/tablename_with_select.q.out b/ql/src/test/results/clientpositive/tablename_with_select.q.out
index 251a57dba7..f3dd7931ec 100644
--- a/ql/src/test/results/clientpositive/tablename_with_select.q.out
+++ b/ql/src/test/results/clientpositive/tablename_with_select.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: DROP TABLE tmp_select
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tmp_select
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE tmp_select(a INT, b STRING)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE tmp_select(a INT, b STRING)
@@ -26,11 +22,11 @@ POSTHOOK: Lineage: tmp_select.b SIMPLE [(src)src.FieldSchema(name:value, type:st
 PREHOOK: query: SELECT a, b FROM tmp_select ORDER BY a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmp_select
-PREHOOK: Output: file:/var/folders/rF/rFg7A9swER0pyf9VBov+VU+++TM/-Tmp-/arvind/hive_2010-06-23_16-31-21_482_1534986163090777807/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-21_919_7549876551032160144/10000
 POSTHOOK: query: SELECT a, b FROM tmp_select ORDER BY a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmp_select
-POSTHOOK: Output: file:/var/folders/rF/rFg7A9swER0pyf9VBov+VU+++TM/-Tmp-/arvind/hive_2010-06-23_16-31-21_482_1534986163090777807/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-21_919_7549876551032160144/10000
 POSTHOOK: Lineage: tmp_select.a EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmp_select.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 0	val_0
@@ -533,10 +529,3 @@ POSTHOOK: Lineage: tmp_select.b SIMPLE [(src)src.FieldSchema(name:value, type:st
 498	val_498
 498	val_498
 498	val_498
-PREHOOK: query: DROP TABLE tmp_select
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE tmp_select
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmp_select
-POSTHOOK: Lineage: tmp_select.a EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmp_select.b SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/transform1.q.out b/ql/src/test/results/clientpositive/transform1.q.out
index e3146f0e05..ef4a297dac 100644
--- a/ql/src/test/results/clientpositive/transform1.q.out
+++ b/ql/src/test/results/clientpositive/transform1.q.out
@@ -1,7 +1,3 @@
-PREHOOK: query: drop table transform1_t1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table transform1_t1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table transform1_t1(a string, b string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table transform1_t1(a string, b string)
@@ -54,20 +50,11 @@ STAGE PLANS:
 PREHOOK: query: SELECT transform(*) USING 'cat' AS (col array<bigint>) FROM transform1_t1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@transform1_t1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-29-51_819_3356279871494035336/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-25_045_4717689685607853398/10000
 POSTHOOK: query: SELECT transform(*) USING 'cat' AS (col array<bigint>) FROM transform1_t1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@transform1_t1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-29-51_819_3356279871494035336/10000
-PREHOOK: query: drop table transform1_t1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table transform1_t1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@transform1_t1
-PREHOOK: query: drop table transform1_t2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table transform1_t2
-POSTHOOK: type: DROPTABLE
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-25_045_4717689685607853398/10000
 PREHOOK: query: create table transform1_t2(col array<int>)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table transform1_t2(col array<int>)
@@ -130,16 +117,10 @@ STAGE PLANS:
 PREHOOK: query: SELECT transform('0\0021\0022') USING 'cat' AS (col array<int>) FROM transform1_t2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@transform1_t2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-29-58_961_100614649774829326/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-30_090_4501252940661071566/10000
 POSTHOOK: query: SELECT transform('0\0021\0022') USING 'cat' AS (col array<int>) FROM transform1_t2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@transform1_t2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-29-58_961_100614649774829326/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-04-30_090_4501252940661071566/10000
 POSTHOOK: Lineage: transform1_t2.col EXPRESSION []
 [0,1,2]
-PREHOOK: query: drop table transform1_t2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table transform1_t2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@transform1_t2
-POSTHOOK: Lineage: transform1_t2.col EXPRESSION []
diff --git a/ql/src/test/results/clientpositive/udf_concat_insert1.q.out b/ql/src/test/results/clientpositive/udf_concat_insert1.q.out
index 84188d4c7e..09cedf5003 100644
--- a/ql/src/test/results/clientpositive/udf_concat_insert1.q.out
+++ b/ql/src/test/results/clientpositive/udf_concat_insert1.q.out
@@ -18,11 +18,11 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:key, type:s
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-31-35_680_1507154475454913300/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-07-54_012_5077477385110287333/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-31-35_680_1507154475454913300/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-07-54_012_5077477385110287333/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION []
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 1234	0
@@ -82,10 +82,3 @@ POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:key, type:s
 1234	96
 1234	97
 1234	98
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION []
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/udf_concat_insert2.q.out b/ql/src/test/results/clientpositive/udf_concat_insert2.q.out
index 0dc7220caf..083313a453 100644
--- a/ql/src/test/results/clientpositive/udf_concat_insert2.q.out
+++ b/ql/src/test/results/clientpositive/udf_concat_insert2.q.out
@@ -18,11 +18,11 @@ POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:str
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-36-24_874_4043008974062287810/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-07-56_820_551394885237380510/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-36-24_874_4043008974062287810/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-07-56_820_551394885237380510/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION []
 POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 1234abcextra argument	val_86
@@ -109,10 +109,3 @@ POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:str
 1234abcextra argument	val_37
 1234abcextra argument	val_90
 1234abcextra argument	val_97
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION []
-POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/udf_field.q.out b/ql/src/test/results/clientpositive/udf_field.q.out
index e437e613f7..f3914d2826 100644
--- a/ql/src/test/results/clientpositive/udf_field.q.out
+++ b/ql/src/test/results/clientpositive/udf_field.q.out
@@ -16,7 +16,7 @@ PREHOOK: query: SELECT
 FROM src LIMIT 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-03-53_853_4058708356956086462/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-12_715_3200523781855195298/10000
 POSTHOOK: query: SELECT
   field("x", "a", "b", "c", "d"),
   field(NULL, "a", "b", "c", "d"),
@@ -24,7 +24,7 @@ POSTHOOK: query: SELECT
 FROM src LIMIT 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-03-53_853_4058708356956086462/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-12_715_3200523781855195298/10000
 0	0	0
 PREHOOK: query: SELECT
   field("a", "a", "b", "c", "d"),
@@ -35,7 +35,7 @@ PREHOOK: query: SELECT
 FROM src LIMIT 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-03-57_114_3619904766510787355/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-15_174_5120124527305231631/10000
 POSTHOOK: query: SELECT
   field("a", "a", "b", "c", "d"),
   field("b", "a", "b", "c", "d"),
@@ -45,7 +45,7 @@ POSTHOOK: query: SELECT
 FROM src LIMIT 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-03-57_114_3619904766510787355/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-15_174_5120124527305231631/10000
 1	2	3	4	4
 PREHOOK: query: SELECT
   field(1, 1, 2, 3, 4),
@@ -56,7 +56,7 @@ PREHOOK: query: SELECT
 FROM src LIMIT 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-04-00_022_7855241436646966982/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-17_642_1728575695911963493/10000
 POSTHOOK: query: SELECT
   field(1, 1, 2, 3, 4),
   field(2, 1, 2, 3, 4),
@@ -66,12 +66,8 @@ POSTHOOK: query: SELECT
 FROM src LIMIT 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-04-00_022_7855241436646966982/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-17_642_1728575695911963493/10000
 1	2	3	4	4
-PREHOOK: query: DROP TABLE test_table
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE test_table
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE test_table(col1 STRING, col2 STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE test_table(col1 STRING, col2 STRING) STORED AS TEXTFILE
@@ -96,7 +92,7 @@ PREHOOK: query: select col1,col2,
 from test_table where col1="86" or col1="66"
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table
-PREHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-04-03_113_1342575242967564464/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-20_226_722121447925099635/10000
 POSTHOOK: query: select col1,col2,
   field("66",col1),
   field("66",col1, col2),
@@ -111,13 +107,9 @@ POSTHOOK: query: select col1,col2,
 from test_table where col1="86" or col1="66"
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table
-POSTHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-04-03_113_1342575242967564464/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-20_226_722121447925099635/10000
 86	val_86	0	0	2	0	0	0	0	0	2	0
 66	val_66	1	1	0	0	0	1	0	0	2	2
-PREHOOK: query: DROP TABLE test_table1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE test_table1
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE test_table1(col1 int, col2 string) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE test_table1(col1 int, col2 string) STORED AS TEXTFILE
@@ -139,7 +131,7 @@ PREHOOK: query: select col1,col2,
 from (select col1, col2, NULL as n from test_table1 where col1=86 or col1=66) t
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table1
-PREHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-04-06_325_6448537514873636635/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-22_921_8146473053376578748/10000
 POSTHOOK: query: select col1,col2,
   field(66,col1),
   field(66,col1, col2),
@@ -151,6 +143,6 @@ POSTHOOK: query: select col1,col2,
 from (select col1, col2, NULL as n from test_table1 where col1=86 or col1=66) t
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table1
-POSTHOOK: Output: file:/tmp/sdong/hive_2010-07-14_18-04-06_325_6448537514873636635/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_12-09-22_921_8146473053376578748/10000
 86	val_86	0	0	2	1	1	0	0
 66	val_66	1	1	0	0	0	0	0
diff --git a/ql/src/test/results/clientpositive/udf_length.q.out b/ql/src/test/results/clientpositive/udf_length.q.out
index 4066ea6416..297c211649 100644
--- a/ql/src/test/results/clientpositive/udf_length.q.out
+++ b/ql/src/test/results/clientpositive/udf_length.q.out
@@ -58,7 +58,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-34-10_571_4239242748530895229/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-31-41_802_6557956853108160258/10000
 
   Stage: Stage-0
     Move Operator
@@ -73,7 +73,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-34-10_571_4239242748530895229/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-31-41_802_6557956853108160258/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -107,11 +107,11 @@ POSTHOOK: Lineage: dest1.len EXPRESSION [(src1)src1.FieldSchema(name:value, type
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-34-14_517_5305435669510064896/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-31-44_358_3550153861453553323/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-34-14_517_5305435669510064896/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-31-44_358_3550153861453553323/10000
 POSTHOOK: Lineage: dest1.len EXPRESSION [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 7
 0
@@ -197,16 +197,10 @@ STAGE PLANS:
 PREHOOK: query: SELECT length(dest1.name) FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-34-14_837_6262252962822172351/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-31-44_626_9093591696777142802/10000
 POSTHOOK: query: SELECT length(dest1.name) FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-34-14_837_6262252962822172351/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-31-44_626_9093591696777142802/10000
 POSTHOOK: Lineage: dest1.len EXPRESSION [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 2
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.len EXPRESSION [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/udf_reverse.q.out b/ql/src/test/results/clientpositive/udf_reverse.q.out
index 8688d403eb..97b2f407a2 100644
--- a/ql/src/test/results/clientpositive/udf_reverse.q.out
+++ b/ql/src/test/results/clientpositive/udf_reverse.q.out
@@ -58,7 +58,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-34-26_713_7153404554728375659/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-33-53_988_9126910952478207979/10000
 
   Stage: Stage-0
     Move Operator
@@ -73,7 +73,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-34-26_713_7153404554728375659/10002 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-33-53_988_9126910952478207979/10002 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -107,11 +107,11 @@ POSTHOOK: Lineage: dest1.len EXPRESSION [(src1)src1.FieldSchema(name:value, type
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-34-30_312_4824860817457749674/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-33-56_476_2425632697021785832/10000
 POSTHOOK: query: SELECT dest1.* FROM dest1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-34-30_312_4824860817457749674/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-33-56_476_2425632697021785832/10000
 POSTHOOK: Lineage: dest1.len EXPRESSION [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 832_lav
 
@@ -165,16 +165,10 @@ POSTHOOK: Lineage: dest1.len EXPRESSION [(src1)src1.FieldSchema(name:value, type
 PREHOOK: query: SELECT count(1) FROM dest1 WHERE reverse(dest1.name) = _UTF-8 0xE993AEE982B5
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-34-30_502_4981122021200605739/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-33-56_662_1561780003827857387/10000
 POSTHOOK: query: SELECT count(1) FROM dest1 WHERE reverse(dest1.name) = _UTF-8 0xE993AEE982B5
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-34-30_502_4981122021200605739/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-33-56_662_1561780003827857387/10000
 POSTHOOK: Lineage: dest1.len EXPRESSION [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
 1
-PREHOOK: query: DROP TABLE dest1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE dest1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.len EXPRESSION [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/udf_sentences.q.out b/ql/src/test/results/clientpositive/udf_sentences.q.out
index b31e722989..92a115c223 100644
--- a/ql/src/test/results/clientpositive/udf_sentences.q.out
+++ b/ql/src/test/results/clientpositive/udf_sentences.q.out
@@ -34,11 +34,11 @@ POSTHOOK: Lineage: sent_tmp2.val SCRIPT [(sent_tmp)sent_tmp.FieldSchema(name:val
 PREHOOK: query: SELECT hex(val) FROM sent_tmp2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@sent_tmp2
-PREHOOK: Output: file:/var/folders/7i/7iCDbWRkGHOcgJgX0zscimPXXts/-Tmp-/mlahiri/hive_2010-07-12_14-58-19_526_2637797515524714813/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-34-24_836_5713772058297109062/10000
 POSTHOOK: query: SELECT hex(val) FROM sent_tmp2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@sent_tmp2
-POSTHOOK: Output: file:/var/folders/7i/7iCDbWRkGHOcgJgX0zscimPXXts/-Tmp-/mlahiri/hive_2010-07-12_14-58-19_526_2637797515524714813/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-34-24_836_5713772058297109062/10000
 POSTHOOK: Lineage: sent_tmp.val SCRIPT []
 POSTHOOK: Lineage: sent_tmp2.val SCRIPT [(sent_tmp)sent_tmp.FieldSchema(name:val, type:array<string>, comment:null), ]
 48697665
@@ -149,11 +149,11 @@ POSTHOOK: Lineage: sent_tmp2.val SCRIPT [(sent_tmp)sent_tmp.FieldSchema(name:val
 PREHOOK: query: SELECT hex(val) FROM sent_tmp2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@sent_tmp2
-PREHOOK: Output: file:/var/folders/7i/7iCDbWRkGHOcgJgX0zscimPXXts/-Tmp-/mlahiri/hive_2010-07-12_14-58-31_739_6627979306575702865/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-34-32_599_1161655294457683255/10000
 POSTHOOK: query: SELECT hex(val) FROM sent_tmp2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@sent_tmp2
-POSTHOOK: Output: file:/var/folders/7i/7iCDbWRkGHOcgJgX0zscimPXXts/-Tmp-/mlahiri/hive_2010-07-12_14-58-31_739_6627979306575702865/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-34-32_599_1161655294457683255/10000
 POSTHOOK: Lineage: sent_tmp.val SCRIPT []
 POSTHOOK: Lineage: sent_tmp.val SCRIPT []
 POSTHOOK: Lineage: sent_tmp2.val SCRIPT [(sent_tmp)sent_tmp.FieldSchema(name:val, type:array<string>, comment:null), ]
@@ -189,32 +189,14 @@ C39C6265727365747A756E67
 66756E6B74696F6E69657274
 696D6D6572
 6E6F6368
-PREHOOK: query: DROP TABLE sent_tmp
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE sent_tmp
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@sent_tmp
-POSTHOOK: Lineage: sent_tmp.val SCRIPT []
-POSTHOOK: Lineage: sent_tmp.val SCRIPT []
-POSTHOOK: Lineage: sent_tmp2.val SCRIPT [(sent_tmp)sent_tmp.FieldSchema(name:val, type:array<string>, comment:null), ]
-POSTHOOK: Lineage: sent_tmp2.val SCRIPT [(sent_tmp)sent_tmp.FieldSchema(name:val, type:array<string>, comment:null), ]
-PREHOOK: query: DROP TABLE sent_tmp2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE sent_tmp2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@sent_tmp2
-POSTHOOK: Lineage: sent_tmp.val SCRIPT []
-POSTHOOK: Lineage: sent_tmp.val SCRIPT []
-POSTHOOK: Lineage: sent_tmp2.val SCRIPT [(sent_tmp)sent_tmp.FieldSchema(name:val, type:array<string>, comment:null), ]
-POSTHOOK: Lineage: sent_tmp2.val SCRIPT [(sent_tmp)sent_tmp.FieldSchema(name:val, type:array<string>, comment:null), ]
 PREHOOK: query: SELECT sentences("Hive is an excellent tool for data querying; and perhaps more versatile than machine translation!! Multiple, ill-formed sentences...confounding punctuation--and yet this UDF still works!!!!") FROM src LIMIT 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
-PREHOOK: Output: file:/var/folders/7i/7iCDbWRkGHOcgJgX0zscimPXXts/-Tmp-/mlahiri/hive_2010-07-12_14-58-35_364_8686350823159923121/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-34-35_087_8096865744921319071/10000
 POSTHOOK: query: SELECT sentences("Hive is an excellent tool for data querying; and perhaps more versatile than machine translation!! Multiple, ill-formed sentences...confounding punctuation--and yet this UDF still works!!!!") FROM src LIMIT 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
-POSTHOOK: Output: file:/var/folders/7i/7iCDbWRkGHOcgJgX0zscimPXXts/-Tmp-/mlahiri/hive_2010-07-12_14-58-35_364_8686350823159923121/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-34-35_087_8096865744921319071/10000
 POSTHOOK: Lineage: sent_tmp.val SCRIPT []
 POSTHOOK: Lineage: sent_tmp.val SCRIPT []
 POSTHOOK: Lineage: sent_tmp2.val SCRIPT [(sent_tmp)sent_tmp.FieldSchema(name:val, type:array<string>, comment:null), ]
diff --git a/ql/src/test/results/clientpositive/union10.q.out b/ql/src/test/results/clientpositive/union10.q.out
index 4b486ae9cf..b7f4684b37 100644
--- a/ql/src/test/results/clientpositive/union10.q.out
+++ b/ql/src/test/results/clientpositive/union10.q.out
@@ -1,14 +1,10 @@
 PREHOOK: query: -- union case: all subqueries are a map-reduce jobs, 3 way union, same input for all sub-queries, followed by filesink
 
-drop table tmptable
-PREHOOK: type: DROPTABLE
+create table tmptable(key string, value int)
+PREHOOK: type: CREATETABLE
 POSTHOOK: query: -- union case: all subqueries are a map-reduce jobs, 3 way union, same input for all sub-queries, followed by filesink
 
-drop table tmptable
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: create table tmptable(key string, value int)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table tmptable(key string, value int)
+create table tmptable(key string, value int)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tmptable
 PREHOOK: query: explain 
@@ -84,7 +80,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-36-31_606_1651420338045270107/10002 
+        file:/tmp/jssarma/hive_2010-07-21_13-38-46_962_8522402125517331101/10002 
           Union
             Select Operator
               expressions:
@@ -108,7 +104,7 @@ STAGE PLANS:
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: tmptable
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-36-31_606_1651420338045270107/10004 
+        file:/tmp/jssarma/hive_2010-07-21_13-38-46_962_8522402125517331101/10004 
           Union
             Select Operator
               expressions:
@@ -132,7 +128,7 @@ STAGE PLANS:
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: tmptable
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-36-31_606_1651420338045270107/10005 
+        file:/tmp/jssarma/hive_2010-07-21_13-38-46_962_8522402125517331101/10005 
           Union
             Select Operator
               expressions:
@@ -164,7 +160,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-36-31_606_1651420338045270107/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-38-46_962_8522402125517331101/10000
 
   Stage: Stage-0
     Move Operator
@@ -179,7 +175,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-36-31_606_1651420338045270107/10003 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-38-46_962_8522402125517331101/10003 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -306,20 +302,13 @@ POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src)s2.null, (src)s
 PREHOOK: query: select * from tmptable x sort by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmptable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-36-51_233_7277943575493517663/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-38-59_753_3964700298494974564/10000
 POSTHOOK: query: select * from tmptable x sort by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmptable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-36-51_233_7277943575493517663/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-38-59_753_3964700298494974564/10000
 POSTHOOK: Lineage: tmptable.key EXPRESSION []
 POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src)s2.null, (src)s3.null, ]
 tst1	500
 tst2	500
 tst3	500
-PREHOOK: query: drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmptable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmptable
-POSTHOOK: Lineage: tmptable.key EXPRESSION []
-POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src)s2.null, (src)s3.null, ]
diff --git a/ql/src/test/results/clientpositive/union12.q.out b/ql/src/test/results/clientpositive/union12.q.out
index 0b9296ca99..83a8acf596 100644
--- a/ql/src/test/results/clientpositive/union12.q.out
+++ b/ql/src/test/results/clientpositive/union12.q.out
@@ -1,14 +1,10 @@
 PREHOOK: query: -- union case: all subqueries are a map-reduce jobs, 3 way union, different inputs for all sub-queries, followed by filesink
 
-drop table tmptable
-PREHOOK: type: DROPTABLE
+create table tmptable(key string, value int)
+PREHOOK: type: CREATETABLE
 POSTHOOK: query: -- union case: all subqueries are a map-reduce jobs, 3 way union, different inputs for all sub-queries, followed by filesink
 
-drop table tmptable
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: create table tmptable(key string, value int)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table tmptable(key string, value int)
+create table tmptable(key string, value int)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tmptable
 PREHOOK: query: explain 
@@ -84,7 +80,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-37-42_214_5006661846869960005/10002 
+        file:/tmp/jssarma/hive_2010-07-21_13-39-13_146_6578228270380339492/10002 
           Union
             Select Operator
               expressions:
@@ -108,7 +104,7 @@ STAGE PLANS:
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: tmptable
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-37-42_214_5006661846869960005/10004 
+        file:/tmp/jssarma/hive_2010-07-21_13-39-13_146_6578228270380339492/10004 
           Union
             Select Operator
               expressions:
@@ -132,7 +128,7 @@ STAGE PLANS:
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: tmptable
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-37-42_214_5006661846869960005/10005 
+        file:/tmp/jssarma/hive_2010-07-21_13-39-13_146_6578228270380339492/10005 
           Union
             Select Operator
               expressions:
@@ -164,7 +160,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-37-42_214_5006661846869960005/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-39-13_146_6578228270380339492/10000
 
   Stage: Stage-0
     Move Operator
@@ -179,7 +175,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-37-42_214_5006661846869960005/10003 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-39-13_146_6578228270380339492/10003 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -310,20 +306,13 @@ POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src1)s2.null, (srcb
 PREHOOK: query: select * from tmptable x sort by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmptable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-03_125_7195580015263686359/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-39-25_864_5858813916026518675/10000
 POSTHOOK: query: select * from tmptable x sort by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmptable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-03_125_7195580015263686359/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-39-25_864_5858813916026518675/10000
 POSTHOOK: Lineage: tmptable.key EXPRESSION []
 POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src1)s2.null, (srcbucket)s3.null, ]
 tst1	500
 tst2	25
 tst3	1000
-PREHOOK: query: drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmptable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmptable
-POSTHOOK: Lineage: tmptable.key EXPRESSION []
-POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src1)s2.null, (srcbucket)s3.null, ]
diff --git a/ql/src/test/results/clientpositive/union17.q.out b/ql/src/test/results/clientpositive/union17.q.out
index 19076bfb26..d80c5656de 100644
--- a/ql/src/test/results/clientpositive/union17.q.out
+++ b/ql/src/test/results/clientpositive/union17.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table DEST1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table DEST2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE
@@ -90,7 +82,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-42-17_660_7741915236100620143/10004 
+        file:/tmp/jssarma/hive_2010-07-21_13-39-54_258_7891516569971054774/10004 
           Union
             Reduce Output Operator
               key expressions:
@@ -106,7 +98,7 @@ STAGE PLANS:
                     type: string
                     expr: _col1
                     type: string
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-42-17_660_7741915236100620143/10007 
+        file:/tmp/jssarma/hive_2010-07-21_13-39-54_258_7891516569971054774/10007 
           Union
             Reduce Output Operator
               key expressions:
@@ -160,7 +152,7 @@ STAGE PLANS:
   Stage: Stage-4
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-42-17_660_7741915236100620143/10005 
+        file:/tmp/jssarma/hive_2010-07-21_13-39-54_258_7891516569971054774/10005 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -212,7 +204,7 @@ STAGE PLANS:
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-42-17_660_7741915236100620143/10006 
+        file:/tmp/jssarma/hive_2010-07-21_13-39-54_258_7891516569971054774/10006 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -316,11 +308,11 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name
 PREHOOK: query: SELECT DEST1.* FROM DEST1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-42-33_864_1765842534170190257/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-07_128_4703708380901938283/10000
 POSTHOOK: query: SELECT DEST1.* FROM DEST1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-42-33_864_1765842534170190257/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-07_128_4703708380901938283/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
@@ -639,11 +631,11 @@ tst1	1
 PREHOOK: query: SELECT DEST2.* FROM DEST2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-42-33_912_7645043623474792521/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-07_181_8284196145398809075/10000
 POSTHOOK: query: SELECT DEST2.* FROM DEST2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-42-33_912_7645043623474792521/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-07_181_8284196145398809075/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
@@ -959,23 +951,3 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name
 97	val_97	1
 98	val_98	1
 tst1	500	1
-PREHOOK: query: drop table DEST1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val1 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table DEST2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val1 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/union18.q.out b/ql/src/test/results/clientpositive/union18.q.out
index 598d80edac..e1a0e1582f 100644
--- a/ql/src/test/results/clientpositive/union18.q.out
+++ b/ql/src/test/results/clientpositive/union18.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table DEST1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table DEST2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE
@@ -94,7 +86,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-25_776_1522464923677630354/10004 
+        file:/tmp/jssarma/hive_2010-07-21_13-40-07_550_3826183924486924788/10004 
           Union
             Select Operator
               expressions:
@@ -128,7 +120,7 @@ STAGE PLANS:
                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                     name: dest2
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-25_776_1522464923677630354/10007 
+        file:/tmp/jssarma/hive_2010-07-21_13-40-07_550_3826183924486924788/10007 
           Union
             Select Operator
               expressions:
@@ -170,7 +162,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-25_776_1522464923677630354/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-40-07_550_3826183924486924788/10000
 
   Stage: Stage-0
     Move Operator
@@ -185,7 +177,7 @@ STAGE PLANS:
   Stage: Stage-4
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-25_776_1522464923677630354/10005 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-40-07_550_3826183924486924788/10005 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -215,7 +207,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-25_776_1522464923677630354/10002
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-40-07_550_3826183924486924788/10002
 
   Stage: Stage-1
     Move Operator
@@ -230,7 +222,7 @@ STAGE PLANS:
   Stage: Stage-7
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-25_776_1522464923677630354/10006 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-40-07_550_3826183924486924788/10006 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -302,11 +294,11 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name
 PREHOOK: query: SELECT DEST1.* FROM DEST1 SORT BY DEST1.key, DEST1.value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-44_851_2095736852722337596/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-20_062_6577181830322101134/10000
 POSTHOOK: query: SELECT DEST1.* FROM DEST1 SORT BY DEST1.key, DEST1.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-44_851_2095736852722337596/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-20_062_6577181830322101134/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
@@ -816,11 +808,11 @@ tst1	500
 PREHOOK: query: SELECT DEST2.* FROM DEST2 SORT BY DEST2.key, DEST2.val1, DEST2.val2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-48_466_9015106715428977146/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-22_623_1808569051501536701/10000
 POSTHOOK: query: SELECT DEST2.* FROM DEST2 SORT BY DEST2.key, DEST2.val1, DEST2.val2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_1/build/ql/scratchdir/hive_2010-04-05_18-38-48_466_9015106715428977146/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-22_623_1808569051501536701/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
@@ -1327,23 +1319,3 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name
 98	val_98	val_98
 98	val_98	val_98
 tst1	500	500
-PREHOOK: query: drop table DEST1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val1 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table DEST2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val1 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/union19.q.out b/ql/src/test/results/clientpositive/union19.q.out
index 26eee282cf..cfee137b6c 100644
--- a/ql/src/test/results/clientpositive/union19.q.out
+++ b/ql/src/test/results/clientpositive/union19.q.out
@@ -1,11 +1,3 @@
-PREHOOK: query: drop table DEST1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table DEST2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST2
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE
@@ -88,7 +80,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-10_956_8209072529967031935/10004 
+        file:/tmp/jssarma/hive_2010-07-21_13-40-25_638_6209859465363547113/10004 
           Union
             Select Operator
               expressions:
@@ -135,7 +127,7 @@ STAGE PLANS:
                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                     name: dest2
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-10_956_8209072529967031935/10005 
+        file:/tmp/jssarma/hive_2010-07-21_13-40-25_638_6209859465363547113/10005 
           Union
             Select Operator
               expressions:
@@ -275,11 +267,11 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name
 PREHOOK: query: SELECT DEST1.* FROM DEST1 SORT BY DEST1.key, DEST1.value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-21_983_2907581159747186721/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-33_529_5679096718849362367/10000
 POSTHOOK: query: SELECT DEST1.* FROM DEST1 SORT BY DEST1.key, DEST1.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest1
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-21_983_2907581159747186721/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-33_529_5679096718849362367/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
@@ -598,11 +590,11 @@ tst1	1
 PREHOOK: query: SELECT DEST2.* FROM DEST2 SORT BY DEST2.key, DEST2.val1, DEST2.val2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest2
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-25_452_88250716044597875/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-36_079_8511341470250811242/10000
 POSTHOOK: query: SELECT DEST2.* FROM DEST2 SORT BY DEST2.key, DEST2.val1, DEST2.val2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dest2
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-25_452_88250716044597875/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-40-36_079_8511341470250811242/10000
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
@@ -1109,23 +1101,3 @@ POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name
 98	val_98	val_98
 98	val_98	val_98
 tst1	500	500
-PREHOOK: query: drop table DEST1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val1 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table DEST2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table DEST2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val1 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/union22.q.out b/ql/src/test/results/clientpositive/union22.q.out
index 88e42adf46..97ed5a2f01 100644
--- a/ql/src/test/results/clientpositive/union22.q.out
+++ b/ql/src/test/results/clientpositive/union22.q.out
@@ -1,16 +1,8 @@
-PREHOOK: query: drop table dst_union22
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dst_union22
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table dst_union22(k1 string, k2 string, k3 string, k4 string) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dst_union22(k1 string, k2 string, k3 string, k4 string) partitioned by (ds string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dst_union22
-PREHOOK: query: drop table dst_union22_delta
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dst_union22_delta
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: create table dst_union22_delta(k0 string, k1 string, k2 string, k3 string, k4 string, k5 string) partitioned by (ds string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dst_union22_delta(k0 string, k1 string, k2 string, k3 string, k4 string, k5 string) partitioned by (ds string)
@@ -126,7 +118,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 0
-                    directory: file:/tmp/jssarma/hive_2010-06-10_16-21-01_038_1201658161362559492/10002
+                    directory: file:/tmp/jssarma/hive_2010-07-21_13-41-06_041_3726366700139911207/10002
                     NumFilesPerFileSink: 1
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -181,7 +173,7 @@ STAGE PLANS:
                         File Output Operator
                           compressed: false
                           GlobalTableId: 0
-                          directory: file:/tmp/jssarma/hive_2010-06-10_16-21-01_038_1201658161362559492/10002
+                          directory: file:/tmp/jssarma/hive_2010-07-21_13-41-06_041_3726366700139911207/10002
                           NumFilesPerFileSink: 1
                           table:
                               input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -215,7 +207,7 @@ STAGE PLANS:
               serialization.ddl struct dst_union22 { string k1, string k2, string k3, string k4}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1276212055
+              transient_lastDdlTime 1279744860
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -232,7 +224,7 @@ STAGE PLANS:
                 serialization.ddl struct dst_union22 { string k1, string k2, string k3, string k4}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1276212055
+                transient_lastDdlTime 1279744860
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dst_union22
             name: dst_union22
@@ -240,7 +232,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/tmp/jssarma/hive_2010-06-10_16-21-01_038_1201658161362559492/10002 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-06_041_3726366700139911207/10002 
           Select Operator
             expressions:
                   expr: _col0
@@ -283,7 +275,7 @@ STAGE PLANS:
                     File Output Operator
                       compressed: false
                       GlobalTableId: 1
-                      directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-06-10_16-21-01_038_1201658161362559492/10000
+                      directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-41-06_041_3726366700139911207/10000
                       NumFilesPerFileSink: 1
                       table:
                           input format: org.apache.hadoop.mapred.TextInputFormat
@@ -300,7 +292,7 @@ STAGE PLANS:
                             serialization.ddl struct dst_union22 { string k1, string k2, string k3, string k4}
                             serialization.format 1
                             serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                            transient_lastDdlTime 1276212055
+                            transient_lastDdlTime 1279744860
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                           name: dst_union22
                       TotalFiles: 1
@@ -344,7 +336,7 @@ STAGE PLANS:
                       File Output Operator
                         compressed: false
                         GlobalTableId: 1
-                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-06-10_16-21-01_038_1201658161362559492/10000
+                        directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-41-06_041_3726366700139911207/10000
                         NumFilesPerFileSink: 1
                         table:
                             input format: org.apache.hadoop.mapred.TextInputFormat
@@ -361,7 +353,7 @@ STAGE PLANS:
                               serialization.ddl struct dst_union22 { string k1, string k2, string k3, string k4}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              transient_lastDdlTime 1276212055
+                              transient_lastDdlTime 1279744860
                             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             name: dst_union22
                         TotalFiles: 1
@@ -369,7 +361,7 @@ STAGE PLANS:
       Needs Tagging: false
       Path -> Alias:
         file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dst_union22_delta/ds=1 [null-subquery1:subq-subquery1:dst_union22_delta]
-        file:/tmp/jssarma/hive_2010-06-10_16-21-01_038_1201658161362559492/10002 [file:/tmp/jssarma/hive_2010-06-10_16-21-01_038_1201658161362559492/10002]
+        file:/tmp/jssarma/hive_2010-07-21_13-41-06_041_3726366700139911207/10002 [file:/tmp/jssarma/hive_2010-07-21_13-41-06_041_3726366700139911207/10002]
       Path -> Partition:
         file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/test/data/warehouse/dst_union22_delta/ds=1 
           Partition
@@ -390,7 +382,7 @@ STAGE PLANS:
               serialization.ddl struct dst_union22_delta { string k0, string k1, string k2, string k3, string k4, string k5}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              transient_lastDdlTime 1276212055
+              transient_lastDdlTime 1279744861
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -407,11 +399,11 @@ STAGE PLANS:
                 serialization.ddl struct dst_union22_delta { string k0, string k1, string k2, string k3, string k4, string k5}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1276212055
+                transient_lastDdlTime 1279744861
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dst_union22_delta
             name: dst_union22_delta
-        file:/tmp/jssarma/hive_2010-06-10_16-21-01_038_1201658161362559492/10002 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-06_041_3726366700139911207/10002 
           Partition
             base file name: 10002
             input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -434,7 +426,7 @@ STAGE PLANS:
           partition:
             ds 2
           replace: true
-          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-06-10_16-21-01_038_1201658161362559492/10000
+          source: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-41-06_041_3726366700139911207/10000
           table:
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -450,10 +442,10 @@ STAGE PLANS:
                 serialization.ddl struct dst_union22 { string k1, string k2, string k3, string k4}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                transient_lastDdlTime 1276212055
+                transient_lastDdlTime 1279744860
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: dst_union22
-          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-06-10_16-21-01_038_1201658161362559492/10001
+          tmp directory: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-41-06_041_3726366700139911207/10001
 
 
 PREHOOK: query: insert overwrite table dst_union22 partition (ds='2')
@@ -503,11 +495,11 @@ POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k5 SIMPLE [(src)src.FieldSc
 PREHOOK: query: select * from dst_union22 where ds = '2' order by k1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dst_union22@ds=2
-PREHOOK: Output: file:/tmp/jssarma/hive_2010-06-10_16-21-06_919_5619601517950726872/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-41-11_581_4870502644081141175/10000
 POSTHOOK: query: select * from dst_union22 where ds = '2' order by k1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@dst_union22@ds=2
-POSTHOOK: Output: file:/tmp/jssarma/hive_2010-06-10_16-21-06_919_5619601517950726872/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-41-11_581_4870502644081141175/10000
 POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k3 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -1538,41 +1530,3 @@ POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k5 SIMPLE [(src)src.FieldSc
 98	val_98	98	val_98	2
 98	val_98	98	val_98	2
 98	val_98	98	val_98	2
-PREHOOK: query: drop table dst_union22
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dst_union22
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dst_union22
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k3 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k4 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=2).k1 EXPRESSION [(dst_union22_delta)dst_union22_delta.FieldSchema(name:k0, type:string, comment:null), (dst_union22)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=2).k2 EXPRESSION [(dst_union22_delta)dst_union22_delta.FieldSchema(name:k1, type:string, comment:null), (dst_union22)a.FieldSchema(name:k1, type:string, comment:null), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=2).k3 EXPRESSION [(dst_union22_delta)dst_union22_delta.FieldSchema(name:k2, type:string, comment:null), (dst_union22_delta)dst_union22_delta.FieldSchema(name:k2, type:string, comment:null), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=2).k4 EXPRESSION [(dst_union22_delta)dst_union22_delta.FieldSchema(name:k3, type:string, comment:null), (dst_union22_delta)dst_union22_delta.FieldSchema(name:k3, type:string, comment:null), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k0 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k3 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k4 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k5 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table dst_union22_delta
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table dst_union22_delta
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@dst_union22_delta
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k3 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=1).k4 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=2).k1 EXPRESSION [(dst_union22_delta)dst_union22_delta.FieldSchema(name:k0, type:string, comment:null), (dst_union22)a.FieldSchema(name:ds, type:string, comment:null), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=2).k2 EXPRESSION [(dst_union22_delta)dst_union22_delta.FieldSchema(name:k1, type:string, comment:null), (dst_union22)a.FieldSchema(name:k1, type:string, comment:null), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=2).k3 EXPRESSION [(dst_union22_delta)dst_union22_delta.FieldSchema(name:k2, type:string, comment:null), (dst_union22_delta)dst_union22_delta.FieldSchema(name:k2, type:string, comment:null), ]
-POSTHOOK: Lineage: dst_union22 PARTITION(ds=2).k4 EXPRESSION [(dst_union22_delta)dst_union22_delta.FieldSchema(name:k3, type:string, comment:null), (dst_union22_delta)dst_union22_delta.FieldSchema(name:k3, type:string, comment:null), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k0 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k3 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k4 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dst_union22_delta PARTITION(ds=1).k5 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/union3.q.out b/ql/src/test/results/clientpositive/union3.q.out
index 5eddea6f5e..d593034a99 100644
--- a/ql/src/test/results/clientpositive/union3.q.out
+++ b/ql/src/test/results/clientpositive/union3.q.out
@@ -88,7 +88,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-51_721_130702566941386493/10002 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-17_607_6172436954215829293/10002 
           Union
             Select Operator
               expressions:
@@ -101,7 +101,7 @@ STAGE PLANS:
                 table:
                     input format: org.apache.hadoop.mapred.TextInputFormat
                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-51_721_130702566941386493/10003 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-17_607_6172436954215829293/10003 
           Union
             Select Operator
               expressions:
@@ -114,7 +114,7 @@ STAGE PLANS:
                 table:
                     input format: org.apache.hadoop.mapred.TextInputFormat
                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-51_721_130702566941386493/10005 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-17_607_6172436954215829293/10005 
           Union
             Select Operator
               expressions:
@@ -127,7 +127,7 @@ STAGE PLANS:
                 table:
                     input format: org.apache.hadoop.mapred.TextInputFormat
                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-51_721_130702566941386493/10007 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-17_607_6172436954215829293/10007 
           Union
             Select Operator
               expressions:
@@ -218,7 +218,7 @@ STAGE PLANS:
   Stage: Stage-5
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-51_721_130702566941386493/10004 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-17_607_6172436954215829293/10004 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -280,7 +280,7 @@ STAGE PLANS:
   Stage: Stage-7
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-37-51_721_130702566941386493/10006 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-17_607_6172436954215829293/10006 
             Reduce Output Operator
               key expressions:
                     expr: _col0
@@ -307,10 +307,6 @@ STAGE PLANS:
       limit: -1
 
 
-PREHOOK: query: DROP TABLE union_out
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE union_out
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE union_out (id int)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE union_out (id int)
@@ -360,11 +356,11 @@ POSTHOOK: Lineage: union_out.id EXPRESSION []
 PREHOOK: query: select * from union_out cluster by id
 PREHOOK: type: QUERY
 PREHOOK: Input: default@union_out
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-38-22_961_8891676490817477492/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-41-37_750_8903242081505469720/10000
 POSTHOOK: query: select * from union_out cluster by id
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@union_out
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-38-22_961_8891676490817477492/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-41-37_750_8903242081505469720/10000
 POSTHOOK: Lineage: union_out.id EXPRESSION []
 1
 2
diff --git a/ql/src/test/results/clientpositive/union4.q.out b/ql/src/test/results/clientpositive/union4.q.out
index c5d29c7214..e119883e6f 100644
--- a/ql/src/test/results/clientpositive/union4.q.out
+++ b/ql/src/test/results/clientpositive/union4.q.out
@@ -1,14 +1,12 @@
 PREHOOK: query: -- union case: both subqueries are map-reduce jobs on same input, followed by filesink
 
-drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: -- union case: both subqueries are map-reduce jobs on same input, followed by filesink
 
-drop table tmptable
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: create table tmptable(key string, value int)
+create table tmptable(key string, value int)
 PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table tmptable(key string, value int)
+POSTHOOK: query: -- union case: both subqueries are map-reduce jobs on same input, followed by filesink
+
+
+create table tmptable(key string, value int)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tmptable
 PREHOOK: query: explain 
@@ -79,7 +77,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-43-02_028_7405143269986564685/10002 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-40_681_7920974237145003540/10002 
           Union
             Select Operator
               expressions:
@@ -103,7 +101,7 @@ STAGE PLANS:
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                       name: tmptable
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-43-02_028_7405143269986564685/10004 
+        file:/tmp/jssarma/hive_2010-07-21_13-41-40_681_7920974237145003540/10004 
           Union
             Select Operator
               expressions:
@@ -135,7 +133,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-43-02_028_7405143269986564685/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-41-40_681_7920974237145003540/10000
 
   Stage: Stage-0
     Move Operator
@@ -150,7 +148,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-43-02_028_7405143269986564685/10003 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-41-40_681_7920974237145003540/10003 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -233,19 +231,12 @@ POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src)s2.null, ]
 PREHOOK: query: select * from tmptable x sort by x.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmptable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-43-14_790_3647521945659456151/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-41-50_925_942224749030960902/10000
 POSTHOOK: query: select * from tmptable x sort by x.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmptable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_0/build/ql/scratchdir/hive_2010-04-05_18-43-14_790_3647521945659456151/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-41-50_925_942224749030960902/10000
 POSTHOOK: Lineage: tmptable.key EXPRESSION []
 POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src)s2.null, ]
 tst1	500
 tst2	500
-PREHOOK: query: drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmptable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmptable
-POSTHOOK: Lineage: tmptable.key EXPRESSION []
-POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src)s2.null, ]
diff --git a/ql/src/test/results/clientpositive/union6.q.out b/ql/src/test/results/clientpositive/union6.q.out
index 4440c4cd14..a6fde81064 100644
--- a/ql/src/test/results/clientpositive/union6.q.out
+++ b/ql/src/test/results/clientpositive/union6.q.out
@@ -1,14 +1,12 @@
 PREHOOK: query: -- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by filesink
 
-drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: -- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by filesink
 
-drop table tmptable
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: create table tmptable(key string, value string)
+create table tmptable(key string, value string)
 PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table tmptable(key string, value string)
+POSTHOOK: query: -- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by filesink
+
+
+create table tmptable(key string, value string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tmptable
 PREHOOK: query: explain 
@@ -79,7 +77,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-38-27_929_6398976384008990165/10002 
+        file:/tmp/jssarma/hive_2010-07-21_13-42-01_703_4701345677859889091/10002 
           Union
             Select Operator
               expressions:
@@ -96,7 +94,7 @@ STAGE PLANS:
                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                     name: tmptable
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-38-27_929_6398976384008990165/10004 
+        file:/tmp/jssarma/hive_2010-07-21_13-42-01_703_4701345677859889091/10004 
           Union
             Select Operator
               expressions:
@@ -121,7 +119,7 @@ STAGE PLANS:
     Move Operator
       files:
           hdfs directory: true
-          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-38-27_929_6398976384008990165/10000
+          destination: file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-42-01_703_4701345677859889091/10000
 
   Stage: Stage-0
     Move Operator
@@ -136,7 +134,7 @@ STAGE PLANS:
   Stage: Stage-3
     Map Reduce
       Alias -> Map Operator Tree:
-        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-38-27_929_6398976384008990165/10003 
+        file:/mnt/vol/devrs004.snc1/jssarma/projects/hive_trunk/build/ql/scratchdir/hive_2010-07-21_13-42-01_703_4701345677859889091/10003 
             Reduce Output Operator
               sort order: 
               Map-reduce partition columns:
@@ -201,11 +199,11 @@ POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src1)s2.FieldSchema
 PREHOOK: query: select * from tmptable x sort by x.key, x.value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tmptable
-PREHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-38-42_744_5479394346398398184/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-11_839_6864063023544125357/10000
 POSTHOOK: query: select * from tmptable x sort by x.key, x.value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tmptable
-POSTHOOK: Output: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/.ptest_2/build/ql/scratchdir/hive_2010-04-05_18-38-42_744_5479394346398398184/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-11_839_6864063023544125357/10000
 POSTHOOK: Lineage: tmptable.key EXPRESSION [(src1)s2.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src1)s2.FieldSchema(name:value, type:string, comment:default), ]
 	
@@ -234,10 +232,3 @@ POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src1)s2.FieldSchema
 66	val_66
 98	val_98
 tst1	500
-PREHOOK: query: drop table tmptable
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table tmptable
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@tmptable
-POSTHOOK: Lineage: tmptable.key EXPRESSION [(src1)s2.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: tmptable.value EXPRESSION [(src)s1.null, (src1)s2.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/ql/src/test/results/clientpositive/uniquejoin.q.out b/ql/src/test/results/clientpositive/uniquejoin.q.out
index 61b188d35e..a5b24d4eef 100644
--- a/ql/src/test/results/clientpositive/uniquejoin.q.out
+++ b/ql/src/test/results/clientpositive/uniquejoin.q.out
@@ -1,15 +1,3 @@
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE
@@ -46,14 +34,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/656661211/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-37_498_6289754437459240853/10000
 POSTHOOK: query: FROM UNIQUEJOIN PRESERVE T1 a (a.key), PRESERVE T2 b (b.key), PRESERVE T3 c (c.key)
 SELECT a.key, b.key, c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/656661211/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-37_498_6289754437459240853/10000
 1	NULL	NULL
 2	2	2
 3	3	NULL
@@ -71,14 +59,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/952293765/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-40_127_7250085494294549177/10000
 POSTHOOK: query: FROM UNIQUEJOIN T1 a (a.key), T2 b (b.key), T3 c (c.key)
 SELECT a.key, b.key, c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/952293765/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-40_127_7250085494294549177/10000
 2	2	2
 PREHOOK: query: FROM UNIQUEJOIN T1 a (a.key), T2 b (b.key-1), T3 c (c.key)
 SELECT a.key, b.key, c.key
@@ -86,14 +74,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/1319731262/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-43_825_1567908835853076632/10000
 POSTHOOK: query: FROM UNIQUEJOIN T1 a (a.key), T2 b (b.key-1), T3 c (c.key)
 SELECT a.key, b.key, c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/1319731262/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-43_825_1567908835853076632/10000
 2	3	2
 7	8	7
 7	8	7
@@ -103,14 +91,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/466973827/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-46_450_7673639263976981471/10000
 POSTHOOK: query: FROM UNIQUEJOIN PRESERVE T1 a (a.key, a.val), PRESERVE T2 b (b.key, b.val), PRESERVE T3 c (c.key, c.val)
 SELECT a.key, a.val, b.key, b.val, c.key, c.val
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/466973827/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-46_450_7673639263976981471/10000
 1	11	NULL	NULL	NULL	NULL
 2	12	NULL	NULL	2	12
 NULL	NULL	2	22	NULL	NULL
@@ -128,14 +116,14 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/1186780299/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-49_123_8832279231761811264/10000
 POSTHOOK: query: FROM UNIQUEJOIN PRESERVE T1 a (a.key), T2 b (b.key), PRESERVE T3 c (c.key)
 SELECT a.key, b.key, c.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/1186780299/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-49_123_8832279231761811264/10000
 1	NULL	NULL
 2	2	2
 3	3	NULL
@@ -151,13 +139,13 @@ SELECT a.key, b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t1
-PREHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/306350125/10000
+PREHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-51_850_1196382022472758858/10000
 POSTHOOK: query: FROM UNIQUEJOIN PRESERVE T1 a (a.key), T2 b(b.key)
 SELECT a.key, b.key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t1
-POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/306350125/10000
+POSTHOOK: Output: file:/tmp/jssarma/hive_2010-07-21_13-42-51_850_1196382022472758858/10000
 1	NULL
 2	2
 3	3
@@ -166,18 +154,3 @@ POSTHOOK: Output: file:/data/users/njain/hive1/hive1/build/ql/tmp/306350125/1000
 8	8
 8	8
 8	8
-PREHOOK: query: DROP TABLE T1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t1
-PREHOOK: query: DROP TABLE T2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t2
-PREHOOK: query: DROP TABLE T3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE T3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Output: default@t3
diff --git a/service/build.xml b/service/build.xml
index 889371a2f7..ffd1854fcb 100644
--- a/service/build.xml
+++ b/service/build.xml
@@ -33,7 +33,7 @@
   <path id="test.classpath">
     <pathelement location="${test.build.classes}" />
     <pathelement location="" />
-    <pathelement location="${test.data.dir}/conf"/>
+    <pathelement location="${test.src.data.dir}/conf"/>
     <pathelement location="${hive.conf.dir}"/>
     <fileset dir="${test.src.data.dir}" includes="files/*.jar"/>
     <fileset dir="${hive.root}" includes="testlibs/*.jar"/>
