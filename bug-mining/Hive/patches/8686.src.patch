diff --git a/iceberg/iceberg-handler/src/test/queries/positive/merge_iceberg_copy_on_write_unpartitioned.q b/iceberg/iceberg-handler/src/test/queries/positive/merge_iceberg_copy_on_write_unpartitioned.q
index 3e77025ed6..5c58bf8bc2 100644
--- a/iceberg/iceberg-handler/src/test/queries/positive/merge_iceberg_copy_on_write_unpartitioned.q
+++ b/iceberg/iceberg-handler/src/test/queries/positive/merge_iceberg_copy_on_write_unpartitioned.q
@@ -15,18 +15,18 @@ explain
 merge into target_ice as t using source src ON t.a = src.a
 when matched and t.a > 100 THEN DELETE
 when matched then update set b = 'Merged', c = t.c + 10
-when not matched then insert values (src.a, src.b, src.c);
+when not matched then insert values (src.a, concat(src.b, '-merge new'), src.c);
 
 -- insert clause with a column list
 explain
 merge into target_ice as t using source src ON t.a = src.a
 when matched and t.a > 100 THEN DELETE
-when not matched then insert (a, b) values (src.a, src.b);
+when not matched then insert (a, b) values (src.a, concat(src.b, '-merge new 2'));
 
 merge into target_ice as t using source src ON t.a = src.a
 when matched and t.a > 100 THEN DELETE
 when matched then update set b = 'Merged', c = t.c + 10
-when not matched then insert values (src.a, src.b, src.c);
+when not matched then insert values (src.a, concat(src.b, '-merge new'), src.c);
 
 select * from target_ice;
 
diff --git a/iceberg/iceberg-handler/src/test/results/positive/merge_iceberg_copy_on_write_unpartitioned.q.out b/iceberg/iceberg-handler/src/test/results/positive/merge_iceberg_copy_on_write_unpartitioned.q.out
index 9fd120e133..96cb974b97 100644
--- a/iceberg/iceberg-handler/src/test/results/positive/merge_iceberg_copy_on_write_unpartitioned.q.out
+++ b/iceberg/iceberg-handler/src/test/results/positive/merge_iceberg_copy_on_write_unpartitioned.q.out
@@ -49,7 +49,7 @@ PREHOOK: query: explain
 merge into target_ice as t using source src ON t.a = src.a
 when matched and t.a > 100 THEN DELETE
 when matched then update set b = 'Merged', c = t.c + 10
-when not matched then insert values (src.a, src.b, src.c)
+when not matched then insert values (src.a, concat(src.b, '-merge new'), src.c)
 PREHOOK: type: QUERY
 PREHOOK: Input: default@source
 PREHOOK: Input: default@target_ice
@@ -58,7 +58,7 @@ POSTHOOK: query: explain
 merge into target_ice as t using source src ON t.a = src.a
 when matched and t.a > 100 THEN DELETE
 when matched then update set b = 'Merged', c = t.c + 10
-when not matched then insert values (src.a, src.b, src.c)
+when not matched then insert values (src.a, concat(src.b, '-merge new'), src.c)
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@source
 POSTHOOK: Input: default@target_ice
@@ -239,12 +239,12 @@ STAGE PLANS:
                   predicate: _col4 is null (type: boolean)
                   Statistics: Num rows: 10 Data size: 1216 Basic stats: COMPLETE Column stats: COMPLETE
                   Select Operator
-                    expressions: _col0 (type: int), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col5 (type: int), _col6 (type: string), _col7 (type: int)
+                    expressions: _col0 (type: int), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col5 (type: int), concat(_col6, '-merge new') (type: string), _col7 (type: int)
                     outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
-                    Statistics: Num rows: 10 Data size: 1200 Basic stats: COMPLETE Column stats: COMPLETE
+                    Statistics: Num rows: 10 Data size: 2688 Basic stats: COMPLETE Column stats: COMPLETE
                     File Output Operator
                       compressed: false
-                      Statistics: Num rows: 17 Data size: 3296 Basic stats: COMPLETE Column stats: COMPLETE
+                      Statistics: Num rows: 17 Data size: 4784 Basic stats: COMPLETE Column stats: COMPLETE
                       table:
                           input format: org.apache.iceberg.mr.hive.HiveIcebergInputFormat
                           output format: org.apache.iceberg.mr.hive.HiveIcebergOutputFormat
@@ -266,7 +266,7 @@ STAGE PLANS:
                   Statistics: Num rows: 1 Data size: 302 Basic stats: COMPLETE Column stats: COMPLETE
                   File Output Operator
                     compressed: false
-                    Statistics: Num rows: 17 Data size: 3296 Basic stats: COMPLETE Column stats: COMPLETE
+                    Statistics: Num rows: 17 Data size: 4784 Basic stats: COMPLETE Column stats: COMPLETE
                     table:
                         input format: org.apache.iceberg.mr.hive.HiveIcebergInputFormat
                         output format: org.apache.iceberg.mr.hive.HiveIcebergOutputFormat
@@ -308,7 +308,7 @@ STAGE PLANS:
                 Statistics: Num rows: 4 Data size: 1196 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 17 Data size: 3296 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 17 Data size: 4784 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.iceberg.mr.hive.HiveIcebergInputFormat
                       output format: org.apache.iceberg.mr.hive.HiveIcebergOutputFormat
@@ -428,7 +428,7 @@ STAGE PLANS:
                       Statistics: Num rows: 2 Data size: 598 Basic stats: COMPLETE Column stats: COMPLETE
                       File Output Operator
                         compressed: false
-                        Statistics: Num rows: 17 Data size: 3296 Basic stats: COMPLETE Column stats: COMPLETE
+                        Statistics: Num rows: 17 Data size: 4784 Basic stats: COMPLETE Column stats: COMPLETE
                         table:
                             input format: org.apache.iceberg.mr.hive.HiveIcebergInputFormat
                             output format: org.apache.iceberg.mr.hive.HiveIcebergOutputFormat
@@ -457,7 +457,7 @@ STAGE PLANS:
 PREHOOK: query: explain
 merge into target_ice as t using source src ON t.a = src.a
 when matched and t.a > 100 THEN DELETE
-when not matched then insert (a, b) values (src.a, src.b)
+when not matched then insert (a, b) values (src.a, concat(src.b, '-merge new 2'))
 PREHOOK: type: QUERY
 PREHOOK: Input: default@source
 PREHOOK: Input: default@target_ice
@@ -465,7 +465,7 @@ PREHOOK: Output: default@target_ice
 POSTHOOK: query: explain
 merge into target_ice as t using source src ON t.a = src.a
 when matched and t.a > 100 THEN DELETE
-when not matched then insert (a, b) values (src.a, src.b)
+when not matched then insert (a, b) values (src.a, concat(src.b, '-merge new 2'))
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@source
 POSTHOOK: Input: default@target_ice
@@ -612,12 +612,12 @@ STAGE PLANS:
                   predicate: _col4 is null (type: boolean)
                   Statistics: Num rows: 10 Data size: 1200 Basic stats: COMPLETE Column stats: COMPLETE
                   Select Operator
-                    expressions: _col0 (type: int), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col5 (type: int), _col6 (type: string), null (type: int)
+                    expressions: _col0 (type: int), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col5 (type: int), concat(_col6, '-merge new 2') (type: string), null (type: int)
                     outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
-                    Statistics: Num rows: 10 Data size: 1188 Basic stats: COMPLETE Column stats: COMPLETE
+                    Statistics: Num rows: 10 Data size: 2676 Basic stats: COMPLETE Column stats: COMPLETE
                     File Output Operator
                       compressed: false
-                      Statistics: Num rows: 13 Data size: 2085 Basic stats: COMPLETE Column stats: COMPLETE
+                      Statistics: Num rows: 13 Data size: 3573 Basic stats: COMPLETE Column stats: COMPLETE
                       table:
                           input format: org.apache.iceberg.mr.hive.HiveIcebergInputFormat
                           output format: org.apache.iceberg.mr.hive.HiveIcebergOutputFormat
@@ -659,7 +659,7 @@ STAGE PLANS:
                 Statistics: Num rows: 2 Data size: 598 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 13 Data size: 2085 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 13 Data size: 3573 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.iceberg.mr.hive.HiveIcebergInputFormat
                       output format: org.apache.iceberg.mr.hive.HiveIcebergOutputFormat
@@ -717,7 +717,7 @@ STAGE PLANS:
                       Statistics: Num rows: 1 Data size: 299 Basic stats: COMPLETE Column stats: COMPLETE
                       File Output Operator
                         compressed: false
-                        Statistics: Num rows: 13 Data size: 2085 Basic stats: COMPLETE Column stats: COMPLETE
+                        Statistics: Num rows: 13 Data size: 3573 Basic stats: COMPLETE Column stats: COMPLETE
                         table:
                             input format: org.apache.iceberg.mr.hive.HiveIcebergInputFormat
                             output format: org.apache.iceberg.mr.hive.HiveIcebergOutputFormat
@@ -808,7 +808,7 @@ STAGE PLANS:
 PREHOOK: query: merge into target_ice as t using source src ON t.a = src.a
 when matched and t.a > 100 THEN DELETE
 when matched then update set b = 'Merged', c = t.c + 10
-when not matched then insert values (src.a, src.b, src.c)
+when not matched then insert values (src.a, concat(src.b, '-merge new'), src.c)
 PREHOOK: type: QUERY
 PREHOOK: Input: default@source
 PREHOOK: Input: default@target_ice
@@ -816,7 +816,7 @@ PREHOOK: Output: default@target_ice
 POSTHOOK: query: merge into target_ice as t using source src ON t.a = src.a
 when matched and t.a > 100 THEN DELETE
 when matched then update set b = 'Merged', c = t.c + 10
-when not matched then insert values (src.a, src.b, src.c)
+when not matched then insert values (src.a, concat(src.b, '-merge new'), src.c)
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@source
 POSTHOOK: Input: default@target_ice
@@ -831,10 +831,10 @@ POSTHOOK: Input: default@target_ice
 POSTHOOK: Output: hdfs://### HDFS PATH ###
 1	Merged	60
 2	Merged	61
-3	three	52
+3	three-merge new	52
 333	two	56
-4	four	53
-5	five	54
+4	four-merge new	53
+5	five-merge new	54
 PREHOOK: query: explain
 merge into target_ice as t using source src ON t.a = src.a
 when matched then update set b = 'Merged', c = t.c - 10
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/ConstraintsUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/ConstraintsUtils.java
index 3291e3daef..ad1515d4f9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/ConstraintsUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/ConstraintsUtils.java
@@ -19,13 +19,9 @@
 package org.apache.hadoop.hive.ql.ddl.table.constraint;
 
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Stack;
 
-import com.google.common.collect.HashMultimap;
-import com.google.common.collect.SetMultimap;
 import org.antlr.runtime.TokenRewriteStream;
 import org.antlr.runtime.tree.Tree;
 import org.apache.hadoop.conf.Configuration;
@@ -40,17 +36,11 @@
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
-import org.apache.hadoop.hive.ql.lib.CostLessRuleDispatcher;
-import org.apache.hadoop.hive.ql.lib.ExpressionWalker;
-import org.apache.hadoop.hive.ql.lib.Node;
-import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
-import org.apache.hadoop.hive.ql.lib.SemanticGraphWalker;
-import org.apache.hadoop.hive.ql.lib.SemanticNodeProcessor;
+import org.apache.hadoop.hive.ql.metadata.HiveUtils;
 import org.apache.hadoop.hive.ql.parse.ASTNode;
 import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.HiveParser;
 import org.apache.hadoop.hive.ql.parse.ParseDriver;
-import org.apache.hadoop.hive.ql.parse.Quotation;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
@@ -255,7 +245,7 @@ private static List<ConstraintInfo> generateConstraintInfos(ASTNode child, List<
         // try to get default value only if this is DEFAULT constraint
         checkOrDefaultValue = getDefaultValue(grandChild, typeChildForDefault, tokenRewriteStream);
       } else if (childType == HiveParser.TOK_CHECK_CONSTRAINT) {
-        UnparseTranslator unparseTranslator = collectUnescapeIdentifierTranslations(grandChild);
+        UnparseTranslator unparseTranslator = HiveUtils.collectUnescapeIdentifierTranslations(grandChild);
         unparseTranslator.applyTranslations(tokenRewriteStream, CHECK_CONSTRAINT_PROGRAM);
         checkOrDefaultValue = tokenRewriteStream.toString(CHECK_CONSTRAINT_PROGRAM, grandChild.getTokenStartIndex(),
             grandChild.getTokenStopIndex());
@@ -298,52 +288,6 @@ private static List<ConstraintInfo> generateConstraintInfos(ASTNode child, List<
     return constraintInfos;
   }
 
-  static class ConstraintExpressionContext implements NodeProcessorCtx {
-    private UnparseTranslator unparseTranslator;
-
-    public ConstraintExpressionContext(UnparseTranslator unparseTranslator) {
-      this.unparseTranslator = unparseTranslator;
-    }
-
-    public UnparseTranslator getUnparseTranslator() {
-      return unparseTranslator;
-    }
-  }
-
-  private static UnparseTranslator collectUnescapeIdentifierTranslations(ASTNode node)
-      throws SemanticException {
-    UnparseTranslator unparseTranslator = new UnparseTranslator(Quotation.BACKTICKS);
-    unparseTranslator.enable();
-
-    SetMultimap<Integer, SemanticNodeProcessor> astNodeToProcessor = HashMultimap.create();
-    astNodeToProcessor.put(HiveParser.TOK_TABLE_OR_COL, new ColumnExprProcessor());
-    astNodeToProcessor.put(HiveParser.DOT, new ColumnExprProcessor());
-    NodeProcessorCtx nodeProcessorCtx = new ConstraintExpressionContext(unparseTranslator);
-
-    CostLessRuleDispatcher costLessRuleDispatcher = new CostLessRuleDispatcher(
-        (nd, stack, procCtx, nodeOutputs) -> null, astNodeToProcessor, nodeProcessorCtx);
-    SemanticGraphWalker walker = new ExpressionWalker(costLessRuleDispatcher);
-    walker.startWalking(Collections.singletonList(node), null);
-    return unparseTranslator;
-  }
-
-  static class ColumnExprProcessor implements SemanticNodeProcessor {
-
-    @Override
-    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)
-        throws SemanticException {
-      UnparseTranslator unparseTranslator = ((ConstraintExpressionContext)procCtx).getUnparseTranslator();
-      ASTNode tokTableOrColNode = (ASTNode) nd;
-      for (int i = 0; i < tokTableOrColNode.getChildCount(); ++i) {
-        ASTNode child = (ASTNode) tokTableOrColNode.getChild(i);
-        if (child.getType() == HiveParser.Identifier) {
-          unparseTranslator.addIdentifierTranslation(child);
-        }
-      }
-      return null;
-    }
-  }
-
   private static final int DEFAULT_MAX_LEN = 255;
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java
index ae8849b852..5343a1bb3b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java
@@ -19,12 +19,27 @@
 package org.apache.hadoop.hive.ql.metadata;
 
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.List;
 
+import java.util.Stack;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
+
+import com.google.common.collect.HashMultimap;
+import com.google.common.collect.SetMultimap;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.lib.CostLessRuleDispatcher;
+import org.apache.hadoop.hive.ql.lib.ExpressionWalker;
+import org.apache.hadoop.hive.ql.lib.Node;
+import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
+import org.apache.hadoop.hive.ql.lib.SemanticGraphWalker;
+import org.apache.hadoop.hive.ql.lib.SemanticNodeProcessor;
+import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.HiveParser;
 import org.apache.hadoop.hive.ql.parse.Quotation;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.parse.UnparseTranslator;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -302,6 +317,52 @@ public static String unparseIdentifier(String identifier) {
     return unparseIdentifier(identifier, Quotation.BACKTICKS);
   }
 
+  public static UnparseTranslator collectUnescapeIdentifierTranslations(ASTNode node)
+      throws SemanticException {
+    UnparseTranslator unparseTranslator = new UnparseTranslator(Quotation.BACKTICKS);
+    unparseTranslator.enable();
+
+    SetMultimap<Integer, SemanticNodeProcessor> astNodeToProcessor = HashMultimap.create();
+    astNodeToProcessor.put(HiveParser.TOK_TABLE_OR_COL, new ColumnExprProcessor());
+    astNodeToProcessor.put(HiveParser.DOT, new ColumnExprProcessor());
+    NodeProcessorCtx nodeProcessorCtx = new QuotedIdExpressionContext(unparseTranslator);
+
+    CostLessRuleDispatcher costLessRuleDispatcher = new CostLessRuleDispatcher(
+        (nd, stack, procCtx, nodeOutputs) -> null, astNodeToProcessor, nodeProcessorCtx);
+    SemanticGraphWalker walker = new ExpressionWalker(costLessRuleDispatcher);
+    walker.startWalking(Collections.singletonList(node), null);
+    return unparseTranslator;
+  }
+
+  static class ColumnExprProcessor implements SemanticNodeProcessor {
+
+    @Override
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)
+        throws SemanticException {
+      UnparseTranslator unparseTranslator = ((QuotedIdExpressionContext)procCtx).getUnparseTranslator();
+      ASTNode tokTableOrColNode = (ASTNode) nd;
+      for (int i = 0; i < tokTableOrColNode.getChildCount(); ++i) {
+        ASTNode child = (ASTNode) tokTableOrColNode.getChild(i);
+        if (child.getType() == HiveParser.Identifier) {
+          unparseTranslator.addIdentifierTranslation(child);
+        }
+      }
+      return null;
+    }
+  }
+
+  static class QuotedIdExpressionContext implements NodeProcessorCtx {
+    private final UnparseTranslator unparseTranslator;
+
+    public QuotedIdExpressionContext(UnparseTranslator unparseTranslator) {
+      this.unparseTranslator = unparseTranslator;
+    }
+
+    public UnparseTranslator getUnparseTranslator() {
+      return unparseTranslator;
+    }
+  }
+
   public static HiveStorageHandler getStorageHandler(
     Configuration conf, String className) throws HiveException {
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/MergeSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/MergeSemanticAnalyzer.java
index 94fb812a30..495c1f3057 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/MergeSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/MergeSemanticAnalyzer.java
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.parse;
 
 import org.antlr.runtime.TokenRewriteStream;
+import org.apache.commons.lang3.ObjectUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.ErrorMsg;
@@ -43,6 +44,8 @@
  */
 public class MergeSemanticAnalyzer extends RewriteSemanticAnalyzer<MergeStatement> {
 
+  private static final String MERGE_INSERT_VALUES_PROGRAM = "MERGE_INSERT_VALUES_PROGRAM";
+
   private int numWhenMatchedUpdateClauses;
   private int numWhenMatchedDeleteClauses;
   private IdentifierQuoter quotedIdentifierHelper;
@@ -346,30 +349,52 @@ private MergeStatement.InsertClause handleInsert(ASTNode whenNotMatchedClause, S
 
     // if column list is specified, then it has to have the same number of elements as the values
     // valuesNode has a child for struct, the rest are the columns
-    if (columnListNode != null && columnListNode.getChildCount() != (valuesNode.getChildCount() - 1)) {
-      throw new SemanticException(String.format("Column schema must have the same length as values (%d vs %d)",
-          columnListNode.getChildCount(), valuesNode.getChildCount() - 1));
-    }
-    UnparseTranslator defaultValuesTranslator = new UnparseTranslator(conf);
-    defaultValuesTranslator.enable();
-    List<String> targetSchema = processTableColumnNames(columnListNode, targetTable.getFullyQualifiedName());
-    collectDefaultValues(valuesNode, targetTable, targetSchema, defaultValuesTranslator);
-    defaultValuesTranslator.applyTranslations(ctx.getTokenRewriteStream());
-    String valuesClause = getMatchedText(valuesNode);
-    valuesClause = valuesClause.substring(1, valuesClause.length() - 1); //strip '(' and ')'
+    List<String> columnNames;
+    if (columnListNode != null) {
+      if (columnListNode.getChildCount() != (valuesNode.getChildCount() - 1)) {
+        throw new SemanticException(String.format("Column schema must have the same length as values (%d vs %d)",
+            columnListNode.getChildCount(), valuesNode.getChildCount() - 1));
+      }
 
-    String extraPredicate = getWhenClausePredicate(whenNotMatchedClause);
-    return new MergeStatement.InsertClause(
-        getMatchedText(columnListNode), valuesClause, onClausePredicate, extraPredicate);
-  }
+      columnNames = new ArrayList<>(valuesNode.getChildCount());
+      for (int i = 0; i < columnListNode.getChildCount(); ++i) {
+        ASTNode columnNameNode = (ASTNode) columnListNode.getChild(i);
+        String columnName = ctx.getTokenRewriteStream().toString(columnNameNode.getTokenStartIndex(),
+            columnNameNode.getTokenStopIndex()).trim();
+        columnNames.add(columnName);
+      }
+    } else {
+      columnNames = null;
+    }
 
-  private void collectDefaultValues(
-          ASTNode valueClause, Table targetTable, List<String> targetSchema, UnparseTranslator unparseTranslator)
-          throws SemanticException {
+    List<String> values = new ArrayList<>(valuesNode.getChildCount());
+    UnparseTranslator unparseTranslator = HiveUtils.collectUnescapeIdentifierTranslations(valuesNode);
+    unparseTranslator.applyTranslations(ctx.getTokenRewriteStream(), MERGE_INSERT_VALUES_PROGRAM);
+    List<String> targetSchema = processTableColumnNames(columnListNode, targetTable.getFullyQualifiedName());
     List<String> defaultConstraints = getDefaultConstraints(targetTable, targetSchema);
-    for (int j = 0; j < defaultConstraints.size(); j++) {
-      unparseTranslator.addDefaultValueTranslation((ASTNode) valueClause.getChild(j + 1), defaultConstraints.get(j));
+    // First child is 'struct', the rest are the value expressions
+    // TOK_FUNCTION
+    //    struct
+    //    .
+    //       TOK_TABLE_OR_COL
+    //          any_alias
+    //       any_column_name
+    //    3
+    for (int i = 1; i < valuesNode.getChildCount(); ++i) {
+      ASTNode valueNode = (ASTNode) valuesNode.getChild(i);
+      String value;
+      if (valueNode.getType() == HiveParser.TOK_TABLE_OR_COL
+          && valueNode.getChild(0).getType() == HiveParser.TOK_DEFAULT_VALUE) {
+        value = ObjectUtils.defaultIfNull(defaultConstraints.get(i - 1), "NULL");
+      } else {
+        value = ctx.getTokenRewriteStream().toString(MERGE_INSERT_VALUES_PROGRAM,
+            valueNode.getTokenStartIndex(), valueNode.getTokenStopIndex()).trim();
+      }
+      values.add(value);
     }
+
+    String extraPredicate = getWhenClausePredicate(whenNotMatchedClause);
+    return new MergeStatement.InsertClause(columnNames, values, onClausePredicate, extraPredicate);
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/CopyOnWriteMergeRewriter.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/CopyOnWriteMergeRewriter.java
index a8c47811c9..d43adacf0f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/CopyOnWriteMergeRewriter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/CopyOnWriteMergeRewriter.java
@@ -154,18 +154,17 @@ public void appendWhenNotMatchedInsertClause(MergeStatement.InsertClause insertC
       }
       List<String> values = sqlGenerator.getDeleteValues(Context.Operation.MERGE);
       
-      if (insertClause.getColumnListText() != null) {
-        String[] columnNames = insertClause.getColumnListText()
-            .substring(1, insertClause.getColumnListText().length() - 1).split(",");
-        String[] columnValues = insertClause.getValuesClause().split(",");
+      if (insertClause.getColumnList() != null) {
+        List<String> columnNames = insertClause.getColumnList();
+        List<String> columnValues = insertClause.getValuesClause();
         
-        Map<String, String> columnMap = IntStream.range(0, columnNames.length).boxed().collect(
-            Collectors.toMap(i -> ParseUtils.stripIdentifierQuotes(columnNames[i].trim()), i -> columnValues[i]));
+        Map<String, String> columnMap = IntStream.range(0, columnNames.size()).boxed().collect(
+            Collectors.toMap(columnNames::get, columnValues::get));
         for (FieldSchema col : mergeStatement.getTargetTable().getAllCols()) {
           values.add(columnMap.getOrDefault(col.getName(), "null"));
         }
       } else {
-        values.add(insertClause.getValuesClause());
+        values.addAll(insertClause.getValuesClause());
       }
       sqlGenerator.append(StringUtils.join(values, ","));
       sqlGenerator.append("\nFROM " + mergeStatement.getSourceName());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/MergeRewriter.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/MergeRewriter.java
index d97f71a641..bee52fbc9b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/MergeRewriter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/MergeRewriter.java
@@ -178,8 +178,10 @@ protected static class MergeWhenClauseSqlGenerator implements MergeStatement.Mer
     @Override
     public void appendWhenNotMatchedInsertClause(MergeStatement.InsertClause insertClause) {
       sqlGenerator.append("INSERT INTO ").append(mergeStatement.getTargetName());
-      if (insertClause.getColumnListText() != null) {
-        sqlGenerator.append(' ').append(insertClause.getColumnListText());
+      if (insertClause.getColumnList() != null) {
+        sqlGenerator.append(" (");
+        sqlGenerator.append(String.join(",", insertClause.getColumnList()));
+        sqlGenerator.append(')');
       }
 
       sqlGenerator.append("    -- insert clause\n  SELECT ");
@@ -188,7 +190,8 @@ public void appendWhenNotMatchedInsertClause(MergeStatement.InsertClause insertC
         hintStr = null;
       }
 
-      sqlGenerator.append(insertClause.getValuesClause()).append("\n   WHERE ").append(insertClause.getPredicate());
+      sqlGenerator.append(String.join(", ", insertClause.getValuesClause()));
+      sqlGenerator.append("\n   WHERE ").append(insertClause.getPredicate());
 
       if (insertClause.getExtraPredicate() != null) {
         //we have WHEN NOT MATCHED AND <boolean expr> THEN INSERT
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/MergeStatement.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/MergeStatement.java
index 341f2a8853..13826c2607 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/MergeStatement.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/MergeStatement.java
@@ -192,23 +192,23 @@ public abstract int addDestNamePrefixOfInsert(
   }
 
   public static class InsertClause extends WhenClause {
-    private final String columnListText;
-    private final String valuesClause;
+    private final List<String> columnList;
+    private final List<String> valuesClause;
     private final String predicate;
 
 
-    public InsertClause(String columnListText, String valuesClause, String predicate, String extraPredicate) {
+    public InsertClause(List<String> columnList, List<String> valuesClause, String predicate, String extraPredicate) {
       super(extraPredicate);
       this.predicate = predicate;
-      this.columnListText = columnListText;
+      this.columnList = columnList;
       this.valuesClause = valuesClause;
     }
 
-    public String getColumnListText() {
-      return columnListText;
+    public List<String> getColumnList() {
+      return columnList;
     }
 
-    public String getValuesClause() {
+    public List<String> getValuesClause() {
       return valuesClause;
     }
 
