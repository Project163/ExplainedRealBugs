diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java
index d31a202261..49b761450c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java
@@ -229,7 +229,7 @@ private SparkJobRef submit(final DriverContext driverContext, final SparkWork sp
     return new RemoteSparkJobRef(hiveConf, jobHandle, sparkJobStatus);
   }
 
-  private void refreshLocalResources(SparkWork sparkWork, HiveConf conf) throws IOException {
+  private synchronized void refreshLocalResources(SparkWork sparkWork, HiveConf conf) throws IOException {
     // add hive-exec jar
     addJars((new JobConf(this.getClass())).getJar());
 
@@ -264,6 +264,7 @@ private void refreshLocalResources(SparkWork sparkWork, HiveConf conf) throws IO
     addResources(addedArchives);
   }
 
+  //This method is not thread safe
   private void addResources(String addedFiles) throws IOException {
     for (String addedFile : CSV_SPLITTER.split(Strings.nullToEmpty(addedFiles))) {
       try {
@@ -281,6 +282,7 @@ private void addResources(String addedFiles) throws IOException {
     }
   }
 
+  //This method is not thread safe
   private void addJars(String addedJars) throws IOException {
     for (String addedJar : CSV_SPLITTER.split(Strings.nullToEmpty(addedJars))) {
       try {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java
index fdc5361989..d384ed6db6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java
@@ -78,6 +78,7 @@ public static BytesWritable copyBytesWritable(BytesWritable bw) {
 
   /**
    * Uploads a local file to HDFS
+   * This method is not thread safe
    *
    * @param source
    * @param conf
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/session/SparkSessionImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/session/SparkSessionImpl.java
index 6a8b42e926..bb50129518 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/session/SparkSessionImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/session/SparkSessionImpl.java
@@ -98,7 +98,6 @@ public class SparkSessionImpl implements SparkSession {
   private final String sessionId;
   private volatile HiveSparkClient hiveSparkClient;
   private volatile Path scratchDir;
-  private final Object dirLock = new Object();
 
   /**
    * The timestamp of the last completed Spark job.
@@ -317,6 +316,7 @@ private boolean matches(String input, String regex, StringBuilder matchedString)
     return result;
   }
 
+  //This method is not thread safe
   private void cleanScratchDir() throws IOException {
     if (scratchDir != null) {
       FileSystem fs = scratchDir.getFileSystem(conf);
@@ -324,15 +324,16 @@ private void cleanScratchDir() throws IOException {
       scratchDir = null;
     }
   }
-
+  /**
+   * Create scratch directory for spark session if it does not exist.
+   * This method is not thread safe.
+   * @return Path to Spark session scratch directory.
+   * @throws IOException
+   */
   @Override
   public Path getHDFSSessionDir() throws IOException {
     if (scratchDir == null) {
-      synchronized (dirLock) {
-        if (scratchDir == null) {
-          scratchDir = createScratchDir();
-        }
-      }
+      scratchDir = createScratchDir();
     }
     return scratchDir;
   }
