diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index 6b887f5944..f5f50e5823 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -24,7 +24,6 @@ minitez.query.files=\
   explainanalyze_5.q,\
   explainuser_3.q,\
   mapjoin_addjar.q,\
-  multi_count_distinct.q,\
   orc_merge12.q,\
   orc_vectorization_ppd.q,\
   tez_complextype_with_null.q,\
@@ -1257,4 +1256,4 @@ iceberg.llap.query.files=\
   vectorized_iceberg_read.q
 
 iceberg.llap.only.query.files=\
-  llap_iceberg_read.q
\ No newline at end of file
+  llap_iceberg_read.q
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExpandDistinctAggregatesRule.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExpandDistinctAggregatesRule.java
index f6202cfff7..3a46498298 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExpandDistinctAggregatesRule.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExpandDistinctAggregatesRule.java
@@ -92,7 +92,7 @@ public final class HiveExpandDistinctAggregatesRule extends RelOptRule {
           HiveRelFactories.HIVE_PROJECT_FACTORY);
 
   private static RelFactories.ProjectFactory projFactory;
-  
+
   protected static final Logger LOG = LoggerFactory.getLogger(HiveExpandDistinctAggregatesRule.class);
 
   //~ Constructors -----------------------------------------------------------
@@ -112,7 +112,9 @@ public HiveExpandDistinctAggregatesRule(
   public void onMatch(RelOptRuleCall call) {
     final Aggregate aggregate = call.rel(0);
     int numCountDistinct = getNumCountDistinctCall(aggregate);
-    if (numCountDistinct == 0 || numCountDistinct > 63 || aggregate.getGroupType() != Group.SIMPLE) {
+
+    if (numCountDistinct == 0 || numCountDistinct + aggregate.getGroupCount() >= Long.SIZE
+        || aggregate.getGroupType() != Group.SIMPLE) {
       return;
     }
 
@@ -184,16 +186,16 @@ public void onMatch(RelOptRuleCall call) {
    * Converts an aggregate relational expression that contains only
    * count(distinct) to grouping sets with count. For example select
    * count(distinct department_id), count(distinct gender), count(distinct
-   * education_level) from employee; can be transformed to 
-   * select 
-   * count(case when i=1 and department_id is not null then 1 else null end) as c0, 
-   * count(case when i=2 and gender is not null then 1 else null end) as c1, 
-   * count(case when i=4 and education_level is not null then 1 else null end) as c2 
+   * education_level) from employee; can be transformed to
+   * select
+   * count(case when i=1 and department_id is not null then 1 else null end) as c0,
+   * count(case when i=2 and gender is not null then 1 else null end) as c1,
+   * count(case when i=4 and education_level is not null then 1 else null end) as c2
    * from (select
    * grouping__id as i, department_id, gender, education_level from employee
    * group by department_id, gender, education_level grouping sets
    * (department_id, gender, education_level))subq;
-   * @throws CalciteSemanticException 
+   * @throws CalciteSemanticException
    */
   private RelNode convert(Aggregate aggregate, List<List<Integer>> argList, ImmutableBitSet newGroupSet)
       throws CalciteSemanticException {
@@ -234,7 +236,7 @@ private RelNode createCount(Aggregate aggr, List<List<Integer>> argList,
         .collect(Collectors.toList());
     final List<RexNode> gbChildProjLst = Lists.newArrayList();
     // for singular arg, count should not include null
-    // e.g., count(case when i=1 and department_id is not null then 1 else null end) as c0, 
+    // e.g., count(case when i=1 and department_id is not null then 1 else null end) as c0,
     // for non-singular args, count can include null, i.e. (,) is counted as 1
     for (List<Integer> list : cleanArgList) {
       RexNode condition = rexBuilder.makeCall(
diff --git a/ql/src/test/queries/clientpositive/multi_count_distinct.q b/ql/src/test/queries/clientpositive/multi_count_distinct.q
index d864a70b4b..41d69c46f8 100644
--- a/ql/src/test/queries/clientpositive/multi_count_distinct.q
+++ b/ql/src/test/queries/clientpositive/multi_count_distinct.q
@@ -88,3 +88,16 @@ select count (distinct c0), count(distinct c1), count(distinct c2), count(distin
        count(distinct c48), count(distinct c49), count(distinct c50), count(distinct c51), count(distinct c52), count(distinct c53),
        count(distinct c54), count(distinct c55), count(distinct c56), count(distinct c57), count(distinct c58), count(distinct c59),
        count(distinct c62), count(distinct c61), count(distinct c62), count(distinct c63), count(distinct c64) from test_count;
+
+select count (distinct c0), count(distinct c1), count(distinct c2), count(distinct c3), count(distinct c4), count(distinct c5),
+       count(distinct c6), count(distinct c7), count(distinct c8), count(distinct c9), count(distinct c10), count(distinct c11),
+       count(distinct c12), count(distinct c13), count(distinct c14), count(distinct c15), count(distinct c16), count(distinct c17),
+       count(distinct c18), count(distinct c19), count(distinct c20), count(distinct c21), count(distinct c22), count(distinct c23),
+       count(distinct c24), count(distinct c25), count(distinct c26), count(distinct c27), count(distinct c28), count(distinct c29),
+       count(distinct c30), count(distinct c31), count(distinct c32), count(distinct c33), count(distinct c34), count(distinct c35),
+       count(distinct c36), count(distinct c37), count(distinct c38), count(distinct c39), count(distinct c40), count(distinct c41),
+       count(distinct c42), count(distinct c43), count(distinct c44), count(distinct c45), count(distinct c46), count(distinct c47),
+       count(distinct c48), count(distinct c49), count(distinct c50), count(distinct c51), count(distinct c52), count(distinct c53),
+       count(distinct c54), count(distinct c55), count(distinct c56), count(distinct c57), count(distinct c58), count(distinct c59),
+       count(distinct c62), count(distinct c61) from test_count
+group by c64,c0||c1,c0||c2,c0||c3,c0||c4,c0||c5,c0||c6;
diff --git a/ql/src/test/results/clientpositive/tez/multi_count_distinct.q.out b/ql/src/test/results/clientpositive/llap/multi_count_distinct.q.out
similarity index 83%
rename from ql/src/test/results/clientpositive/tez/multi_count_distinct.q.out
rename to ql/src/test/results/clientpositive/llap/multi_count_distinct.q.out
index ad4115d3a3..5ae62ff562 100644
--- a/ql/src/test/results/clientpositive/tez/multi_count_distinct.q.out
+++ b/ql/src/test/results/clientpositive/llap/multi_count_distinct.q.out
@@ -24,137 +24,239 @@ POSTHOOK: Lineage: employee_n1.gender SCRIPT []
 PREHOOK: query: explain select count(distinct department_id), count(distinct gender), count(distinct education_level) from employee_n1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@employee_n1
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: explain select count(distinct department_id), count(distinct gender), count(distinct education_level) from employee_n1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@employee_n1
-POSTHOOK: Output: hdfs://### HDFS PATH ###
-Plan optimized by CBO.
+#### A masked pattern was here ####
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
 
-Vertex dependency in root stage
-Reducer 2 <- Map 1 (SIMPLE_EDGE)
-Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: employee_n1
+                  Statistics: Num rows: 9 Data size: 837 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: department_id (type: int), gender (type: varchar(10)), education_level (type: int)
+                    outputColumnNames: _col0, _col1, _col2
+                    Statistics: Num rows: 9 Data size: 837 Basic stats: COMPLETE Column stats: COMPLETE
+                    Group By Operator
+                      keys: _col0 (type: int), _col1 (type: varchar(10)), _col2 (type: int), 0L (type: bigint)
+                      grouping sets: 3, 5, 6
+                      minReductionHashAggr: 0.4
+                      mode: hash
+                      outputColumnNames: _col0, _col1, _col2, _col3
+                      Statistics: Num rows: 13 Data size: 1313 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int), _col1 (type: varchar(10)), _col2 (type: int), _col3 (type: bigint)
+                        null sort order: zzzz
+                        sort order: ++++
+                        Map-reduce partition columns: _col0 (type: int), _col1 (type: varchar(10)), _col2 (type: int), _col3 (type: bigint)
+                        Statistics: Num rows: 13 Data size: 1313 Basic stats: COMPLETE Column stats: COMPLETE
+            Execution mode: llap
+            LLAP IO: all inputs
+        Reducer 2 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Group By Operator
+                keys: KEY._col0 (type: int), KEY._col1 (type: varchar(10)), KEY._col2 (type: int), KEY._col3 (type: bigint)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 13 Data size: 1313 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: CASE WHEN (((_col3 = 3L) and _col0 is not null)) THEN (1) ELSE (null) END (type: int), CASE WHEN (((_col3 = 5L) and _col1 is not null)) THEN (1) ELSE (null) END (type: int), CASE WHEN (((_col3 = 6L) and _col2 is not null)) THEN (1) ELSE (null) END (type: int)
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 13 Data size: 1313 Basic stats: COMPLETE Column stats: COMPLETE
+                  Group By Operator
+                    aggregations: count(_col0), count(_col1), count(_col2)
+                    minReductionHashAggr: 0.9230769
+                    mode: hash
+                    outputColumnNames: _col0, _col1, _col2
+                    Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      null sort order: 
+                      sort order: 
+                      Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
+                      value expressions: _col0 (type: bigint), _col1 (type: bigint), _col2 (type: bigint)
+        Reducer 3 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0), count(VALUE._col1), count(VALUE._col2)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
 
-Stage-0
-  Fetch Operator
-    limit:-1
-    Stage-1
-      Reducer 3
-      File Output Operator [FS_12]
-        Group By Operator [GBY_10] (rows=1 width=24)
-          Output:["_col0","_col1","_col2"],aggregations:["count(VALUE._col0)","count(VALUE._col1)","count(VALUE._col2)"]
-        <-Reducer 2 [CUSTOM_SIMPLE_EDGE]
-          PARTITION_ONLY_SHUFFLE [RS_9]
-            Group By Operator [GBY_8] (rows=1 width=24)
-              Output:["_col0","_col1","_col2"],aggregations:["count(_col0)","count(_col1)","count(_col2)"]
-              Select Operator [SEL_6] (rows=13 width=101)
-                Output:["_col0","_col1","_col2"]
-                Group By Operator [GBY_5] (rows=13 width=101)
-                  Output:["_col0","_col1","_col2","_col3"],keys:KEY._col0, KEY._col1, KEY._col2, KEY._col3
-                <-Map 1 [SIMPLE_EDGE]
-                  SHUFFLE [RS_4]
-                    PartitionCols:_col0, _col1, _col2, _col3
-                    Group By Operator [GBY_3] (rows=13 width=101)
-                      Output:["_col0","_col1","_col2","_col3"],keys:_col0, _col1, _col2, 0L
-                      Select Operator [SEL_1] (rows=9 width=93)
-                        Output:["_col0","_col1","_col2"]
-                        TableScan [TS_0] (rows=9 width=93)
-                          default@employee_n1,employee_n1,Tbl:COMPLETE,Col:COMPLETE,Output:["department_id","gender","education_level"]
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
 
 PREHOOK: query: select count(distinct department_id), count(distinct gender), count(distinct education_level) from employee_n1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@employee_n1
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: select count(distinct department_id), count(distinct gender), count(distinct education_level) from employee_n1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@employee_n1
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 4	2	3
 PREHOOK: query: select count(distinct department_id), count(distinct gender), count(distinct education_level), count(distinct education_level) from employee_n1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@employee_n1
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: select count(distinct department_id), count(distinct gender), count(distinct education_level), count(distinct education_level) from employee_n1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@employee_n1
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 4	2	3	3
 PREHOOK: query: select count(distinct department_id), count(distinct gender), count(distinct education_level), 
 count(distinct education_level, department_id) from employee_n1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@employee_n1
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: select count(distinct department_id), count(distinct gender), count(distinct education_level), 
 count(distinct education_level, department_id) from employee_n1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@employee_n1
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 4	2	3	7
 PREHOOK: query: select count(distinct gender), count(distinct department_id), count(distinct gender), count(distinct education_level),
 count(distinct education_level, department_id), count(distinct department_id, education_level) from employee_n1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@employee_n1
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: select count(distinct gender), count(distinct department_id), count(distinct gender), count(distinct education_level),
 count(distinct education_level, department_id), count(distinct department_id, education_level) from employee_n1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@employee_n1
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 2	4	2	3	7	7
 PREHOOK: query: explain select count(distinct gender), count(distinct department_id), count(distinct gender), count(distinct education_level),
 count(distinct education_level, department_id), count(distinct department_id, education_level), count(distinct department_id, education_level, gender) from employee_n1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@employee_n1
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: explain select count(distinct gender), count(distinct department_id), count(distinct gender), count(distinct education_level),
 count(distinct education_level, department_id), count(distinct department_id, education_level), count(distinct department_id, education_level, gender) from employee_n1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@employee_n1
-POSTHOOK: Output: hdfs://### HDFS PATH ###
-Plan optimized by CBO.
+#### A masked pattern was here ####
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
 
-Vertex dependency in root stage
-Reducer 2 <- Map 1 (SIMPLE_EDGE)
-Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: employee_n1
+                  Statistics: Num rows: 9 Data size: 837 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: gender (type: varchar(10)), department_id (type: int), education_level (type: int)
+                    outputColumnNames: _col0, _col1, _col2
+                    Statistics: Num rows: 9 Data size: 837 Basic stats: COMPLETE Column stats: COMPLETE
+                    Group By Operator
+                      keys: _col0 (type: varchar(10)), _col1 (type: int), _col2 (type: int), 0L (type: bigint)
+                      grouping sets: 0, 3, 4, 5, 6
+                      minReductionHashAggr: 0.4
+                      mode: hash
+                      outputColumnNames: _col0, _col1, _col2, _col3
+                      Statistics: Num rows: 22 Data size: 2222 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: varchar(10)), _col1 (type: int), _col2 (type: int), _col3 (type: bigint)
+                        null sort order: zzzz
+                        sort order: ++++
+                        Map-reduce partition columns: _col0 (type: varchar(10)), _col1 (type: int), _col2 (type: int), _col3 (type: bigint)
+                        Statistics: Num rows: 22 Data size: 2222 Basic stats: COMPLETE Column stats: COMPLETE
+            Execution mode: llap
+            LLAP IO: all inputs
+        Reducer 2 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Group By Operator
+                keys: KEY._col0 (type: varchar(10)), KEY._col1 (type: int), KEY._col2 (type: int), KEY._col3 (type: bigint)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 22 Data size: 2222 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: CASE WHEN (((_col3 = 3L) and _col0 is not null)) THEN (1) ELSE (null) END (type: int), CASE WHEN (((_col3 = 5L) and _col1 is not null)) THEN (1) ELSE (null) END (type: int), CASE WHEN (((_col3 = 6L) and _col2 is not null)) THEN (1) ELSE (null) END (type: int), CASE WHEN ((_col3 = 4L)) THEN (1) ELSE (null) END (type: int), CASE WHEN ((_col3 = 0L)) THEN (1) ELSE (null) END (type: int)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                  Statistics: Num rows: 22 Data size: 2222 Basic stats: COMPLETE Column stats: COMPLETE
+                  Group By Operator
+                    aggregations: count(_col0), count(_col1), count(_col2), count(_col3), count(_col4)
+                    minReductionHashAggr: 0.95454544
+                    mode: hash
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                    Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      null sort order: 
+                      sort order: 
+                      Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
+                      value expressions: _col0 (type: bigint), _col1 (type: bigint), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: bigint)
+        Reducer 3 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0), count(VALUE._col1), count(VALUE._col2), count(VALUE._col3), count(VALUE._col4)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col0 (type: bigint), _col1 (type: bigint), _col0 (type: bigint), _col2 (type: bigint), _col3 (type: bigint), _col3 (type: bigint), _col4 (type: bigint)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                  Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
 
-Stage-0
-  Fetch Operator
-    limit:-1
-    Stage-1
-      Reducer 3
-      File Output Operator [FS_12]
-        Select Operator [SEL_11] (rows=1 width=56)
-          Output:["_col0","_col1","_col2","_col3","_col4","_col5","_col6"]
-          Group By Operator [GBY_10] (rows=1 width=40)
-            Output:["_col0","_col1","_col2","_col3","_col4"],aggregations:["count(VALUE._col0)","count(VALUE._col1)","count(VALUE._col2)","count(VALUE._col3)","count(VALUE._col4)"]
-          <-Reducer 2 [CUSTOM_SIMPLE_EDGE]
-            PARTITION_ONLY_SHUFFLE [RS_9]
-              Group By Operator [GBY_8] (rows=1 width=40)
-                Output:["_col0","_col1","_col2","_col3","_col4"],aggregations:["count(_col0)","count(_col1)","count(_col2)","count(_col3)","count(_col4)"]
-                Select Operator [SEL_6] (rows=22 width=101)
-                  Output:["_col0","_col1","_col2","_col3","_col4"]
-                  Group By Operator [GBY_5] (rows=22 width=101)
-                    Output:["_col0","_col1","_col2","_col3"],keys:KEY._col0, KEY._col1, KEY._col2, KEY._col3
-                  <-Map 1 [SIMPLE_EDGE]
-                    SHUFFLE [RS_4]
-                      PartitionCols:_col0, _col1, _col2, _col3
-                      Group By Operator [GBY_3] (rows=22 width=101)
-                        Output:["_col0","_col1","_col2","_col3"],keys:_col0, _col1, _col2, 0L
-                        Select Operator [SEL_1] (rows=9 width=93)
-                          Output:["_col0","_col1","_col2"]
-                          TableScan [TS_0] (rows=9 width=93)
-                            default@employee_n1,employee_n1,Tbl:COMPLETE,Col:COMPLETE,Output:["gender","department_id","education_level"]
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
 
 PREHOOK: query: select count(distinct gender), count(distinct department_id), count(distinct gender), count(distinct education_level),
 count(distinct education_level, department_id), count(distinct department_id, education_level), count(distinct department_id, education_level, gender) from employee_n1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@employee_n1
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: select count(distinct gender), count(distinct department_id), count(distinct gender), count(distinct education_level),
 count(distinct education_level, department_id), count(distinct department_id, education_level), count(distinct department_id, education_level, gender) from employee_n1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@employee_n1
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 2	4	2	3	7	7	7
 PREHOOK: query: select count(case i when 3 then 1 else null end) as c0, count(case i when 5 then 1 else null end) as c1, 
 count(case i when 6 then 1 else null end) as c2 from (select grouping__id as i, department_id, gender, 
@@ -162,43 +264,43 @@ education_level from employee_n1 group by department_id, gender, education_level
 (department_id, gender, education_level))subq
 PREHOOK: type: QUERY
 PREHOOK: Input: default@employee_n1
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: select count(case i when 3 then 1 else null end) as c0, count(case i when 5 then 1 else null end) as c1, 
 count(case i when 6 then 1 else null end) as c2 from (select grouping__id as i, department_id, gender, 
 education_level from employee_n1 group by department_id, gender, education_level grouping sets 
 (department_id, gender, education_level))subq
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@employee_n1
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 4	2	3
 PREHOOK: query: select grouping__id as i, department_id, gender, education_level from employee_n1 
 group by department_id, gender, education_level grouping sets 
 (department_id, gender, education_level, (education_level, department_id))
 PREHOOK: type: QUERY
 PREHOOK: Input: default@employee_n1
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: select grouping__id as i, department_id, gender, education_level from employee_n1 
 group by department_id, gender, education_level grouping sets 
 (department_id, gender, education_level, (education_level, department_id))
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@employee_n1
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 2	1	NULL	1
-2	1	NULL	2
-2	1	NULL	3
-3	1	NULL	NULL
 2	2	NULL	1
 2	2	NULL	3
-3	2	NULL	NULL
 2	3	NULL	2
-3	3	NULL	NULL
 2	4	NULL	1
 3	4	NULL	NULL
-5	NULL	F	NULL
 5	NULL	M	NULL
 6	NULL	NULL	1
 6	NULL	NULL	2
 6	NULL	NULL	3
+2	1	NULL	2
+2	1	NULL	3
+3	1	NULL	NULL
+3	2	NULL	NULL
+3	3	NULL	NULL
+5	NULL	F	NULL
 PREHOOK: query: create table test_count (c0 string, c1 string, c2 string, c3 string, c4 string, c5 string, c6 string, c7 string,
                          c8 string, c9 string, c10 string, c11 string, c12 string, c13 string, c14 string, c15 string,
                          c16 string, c17 string, c18 string, c19 string, c20 string, c21 string, c22 string, c23 string,
@@ -310,7 +412,7 @@ PREHOOK: query: explain extended select count (distinct c0), count(distinct c1),
        count(distinct c30), count(distinct c31), count(distinct c32) from test_count
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_count
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: explain extended select count (distinct c0), count(distinct c1), count(distinct c2), count(distinct c3), count(distinct c4), count(distinct c5),
        count(distinct c6), count(distinct c7), count(distinct c8), count(distinct c9), count(distinct c10), count(distinct c11),
        count(distinct c12), count(distinct c13), count(distinct c14), count(distinct c15), count(distinct c16), count(distinct c17),
@@ -319,7 +421,7 @@ POSTHOOK: query: explain extended select count (distinct c0), count(distinct c1)
        count(distinct c30), count(distinct c31), count(distinct c32) from test_count
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_count
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 OPTIMIZED SQL: SELECT COUNT(`$f0`) AS `$f0`, COUNT(`$f1`) AS `$f1`, COUNT(`$f2`) AS `$f2`, COUNT(`$f3`) AS `$f3`, COUNT(`$f4`) AS `$f4`, COUNT(`$f5`) AS `$f5`, COUNT(`$f6`) AS `$f6`, COUNT(`$f7`) AS `$f7`, COUNT(`$f8`) AS `$f8`, COUNT(`$f9`) AS `$f9`, COUNT(`$f10`) AS `$f10`, COUNT(`$f11`) AS `$f11`, COUNT(`$f12`) AS `$f12`, COUNT(`$f13`) AS `$f13`, COUNT(`$f14`) AS `$f14`, COUNT(`$f15`) AS `$f15`, COUNT(`$f16`) AS `$f16`, COUNT(`$f17`) AS `$f17`, COUNT(`$f18`) AS `$f18`, COUNT(`$f19`) AS `$f19`, COUNT(`$f20`) AS `$f20`, COUNT(`$f21`) AS `$f21`, COUNT(`$f22`) AS `$f22`, COUNT(`$f23`) AS `$f23`, COUNT(`$f24`) AS `$f24`, COUNT(`$f25`) AS `$f25`, COUNT(`$f26`) AS `$f26`, COUNT(`$f27`) AS `$f27`, COUNT(`$f28`) AS `$f28`, COUNT(`$f29`) AS `$f29`, COUNT(`$f30`) AS `$f30`, COUNT(`$f31`) AS `$f31`, COUNT(`$f32`) AS `$f32`
 FROM (SELECT CASE WHEN GROUPING__ID() = 4294967295 AND `c0` IS NOT NULL THEN 1 ELSE NULL END AS `$f0`, CASE WHEN GROUPING__ID() = 6442450943 AND `c1` IS NOT NULL THEN 1 ELSE NULL END AS `$f1`, CASE WHEN GROUPING__ID() = 7516192767 AND `c2` IS NOT NULL THEN 1 ELSE NULL END AS `$f2`, CASE WHEN GROUPING__ID() = 8053063679 AND `c3` IS NOT NULL THEN 1 ELSE NULL END AS `$f3`, CASE WHEN GROUPING__ID() = 8321499135 AND `c4` IS NOT NULL THEN 1 ELSE NULL END AS `$f4`, CASE WHEN GROUPING__ID() = 8455716863 AND `c5` IS NOT NULL THEN 1 ELSE NULL END AS `$f5`, CASE WHEN GROUPING__ID() = 8522825727 AND `c6` IS NOT NULL THEN 1 ELSE NULL END AS `$f6`, CASE WHEN GROUPING__ID() = 8556380159 AND `c7` IS NOT NULL THEN 1 ELSE NULL END AS `$f7`, CASE WHEN GROUPING__ID() = 8573157375 AND `c8` IS NOT NULL THEN 1 ELSE NULL END AS `$f8`, CASE WHEN GROUPING__ID() = 8581545983 AND `c9` IS NOT NULL THEN 1 ELSE NULL END AS `$f9`, CASE WHEN GROUPING__ID() = 8585740287 AND `c10` IS NOT NULL THEN 1 ELSE NULL END AS `$f10`, CASE WHEN GROUPING__ID() = 8587837439 AND `c11` IS NOT NULL THEN 1 ELSE NULL END AS `$f11`, CASE WHEN GROUPING__ID() = 8588886015 AND `c12` IS NOT NULL THEN 1 ELSE NULL END AS `$f12`, CASE WHEN GROUPING__ID() = 8589410303 AND `c13` IS NOT NULL THEN 1 ELSE NULL END AS `$f13`, CASE WHEN GROUPING__ID() = 8589672447 AND `c14` IS NOT NULL THEN 1 ELSE NULL END AS `$f14`, CASE WHEN GROUPING__ID() = 8589803519 AND `c15` IS NOT NULL THEN 1 ELSE NULL END AS `$f15`, CASE WHEN GROUPING__ID() = 8589869055 AND `c16` IS NOT NULL THEN 1 ELSE NULL END AS `$f16`, CASE WHEN GROUPING__ID() = 8589901823 AND `c17` IS NOT NULL THEN 1 ELSE NULL END AS `$f17`, CASE WHEN GROUPING__ID() = 8589918207 AND `c18` IS NOT NULL THEN 1 ELSE NULL END AS `$f18`, CASE WHEN GROUPING__ID() = 8589926399 AND `c19` IS NOT NULL THEN 1 ELSE NULL END AS `$f19`, CASE WHEN GROUPING__ID() = 8589930495 AND `c20` IS NOT NULL THEN 1 ELSE NULL END AS `$f20`, CASE WHEN GROUPING__ID() = 8589932543 AND `c21` IS NOT NULL THEN 1 ELSE NULL END AS `$f21`, CASE WHEN GROUPING__ID() = 8589933567 AND `c22` IS NOT NULL THEN 1 ELSE NULL END AS `$f22`, CASE WHEN GROUPING__ID() = 8589934079 AND `c23` IS NOT NULL THEN 1 ELSE NULL END AS `$f23`, CASE WHEN GROUPING__ID() = 8589934335 AND `c24` IS NOT NULL THEN 1 ELSE NULL END AS `$f24`, CASE WHEN GROUPING__ID() = 8589934463 AND `c25` IS NOT NULL THEN 1 ELSE NULL END AS `$f25`, CASE WHEN GROUPING__ID() = 8589934527 AND `c26` IS NOT NULL THEN 1 ELSE NULL END AS `$f26`, CASE WHEN GROUPING__ID() = 8589934559 AND `c27` IS NOT NULL THEN 1 ELSE NULL END AS `$f27`, CASE WHEN GROUPING__ID() = 8589934575 AND `c28` IS NOT NULL THEN 1 ELSE NULL END AS `$f28`, CASE WHEN GROUPING__ID() = 8589934583 AND `c29` IS NOT NULL THEN 1 ELSE NULL END AS `$f29`, CASE WHEN GROUPING__ID() = 8589934587 AND `c30` IS NOT NULL THEN 1 ELSE NULL END AS `$f30`, CASE WHEN GROUPING__ID() = 8589934589 AND `c31` IS NOT NULL THEN 1 ELSE NULL END AS `$f31`, CASE WHEN GROUPING__ID() = 8589934590 AND `c32` IS NOT NULL THEN 1 ELSE NULL END AS `$f32`
 FROM `default`.`test_count`
@@ -364,10 +466,12 @@ STAGE PLANS:
                         Statistics: Num rows: 1 Data size: 2861 Basic stats: COMPLETE Column stats: COMPLETE
                         tag: -1
                         auto parallelism: true
+            Execution mode: llap
+            LLAP IO: all inputs
             Path -> Alias:
-              hdfs://### HDFS PATH ### [test_count]
+#### A masked pattern was here ####
             Path -> Partition:
-              hdfs://### HDFS PATH ### 
+#### A masked pattern was here ####
                 Partition
                   base file name: test_count
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -379,7 +483,6 @@ STAGE PLANS:
                     columns c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50,c51,c52,c53,c54,c55,c56,c57,c58,c59,c60,c61,c62,c63,c64
                     columns.types string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string
 #### A masked pattern was here ####
-                    location hdfs://### HDFS PATH ###
                     name default.test_count
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -394,7 +497,6 @@ STAGE PLANS:
                       columns.comments 
                       columns.types string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string
 #### A masked pattern was here ####
-                      location hdfs://### HDFS PATH ###
                       name default.test_count
                       serialization.format 1
                       serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -404,6 +506,7 @@ STAGE PLANS:
             Truncated Path -> Alias:
               /test_count [test_count]
         Reducer 2 
+            Execution mode: llap
             Needs Tagging: false
             Reduce Operator Tree:
               Group By Operator
@@ -423,6 +526,7 @@ STAGE PLANS:
                   tag: -1
                   auto parallelism: true
         Reducer 3 
+            Execution mode: llap
             Needs Tagging: false
             Reduce Operator Tree:
               Group By Operator
@@ -450,6 +554,7 @@ STAGE PLANS:
                       value expressions: _col0 (type: bigint), _col1 (type: bigint), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: bigint), _col6 (type: bigint), _col7 (type: bigint), _col8 (type: bigint), _col9 (type: bigint), _col10 (type: bigint), _col11 (type: bigint), _col12 (type: bigint), _col13 (type: bigint), _col14 (type: bigint), _col15 (type: bigint), _col16 (type: bigint), _col17 (type: bigint), _col18 (type: bigint), _col19 (type: bigint), _col20 (type: bigint), _col21 (type: bigint), _col22 (type: bigint), _col23 (type: bigint), _col24 (type: bigint), _col25 (type: bigint), _col26 (type: bigint), _col27 (type: bigint), _col28 (type: bigint), _col29 (type: bigint), _col30 (type: bigint), _col31 (type: bigint), _col32 (type: bigint)
                       auto parallelism: false
         Reducer 4 
+            Execution mode: llap
             Needs Tagging: false
             Reduce Operator Tree:
               Group By Operator
@@ -461,10 +566,10 @@ STAGE PLANS:
                   bucketingVersion: 2
                   compressed: false
                   GlobalTableId: 0
-                  directory: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
                   NumFilesPerFileSink: 1
                   Statistics: Num rows: 1 Data size: 264 Basic stats: COMPLETE Column stats: COMPLETE
-                  Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
                   table:
                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -496,7 +601,7 @@ PREHOOK: query: select count (distinct c0), count(distinct c1), count(distinct c
        count(distinct c30), count(distinct c31), count(distinct c32) from test_count
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_count
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: select count (distinct c0), count(distinct c1), count(distinct c2), count(distinct c3), count(distinct c4), count(distinct c5),
        count(distinct c6), count(distinct c7), count(distinct c8), count(distinct c9), count(distinct c10), count(distinct c11),
        count(distinct c12), count(distinct c13), count(distinct c14), count(distinct c15), count(distinct c16), count(distinct c17),
@@ -505,7 +610,7 @@ POSTHOOK: query: select count (distinct c0), count(distinct c1), count(distinct
        count(distinct c30), count(distinct c31), count(distinct c32) from test_count
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_count
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1
 PREHOOK: query: explain extended select count (distinct c0), count(distinct c1), count(distinct c2), count(distinct c3), count(distinct c4), count(distinct c5),
        count(distinct c6), count(distinct c7), count(distinct c8), count(distinct c9), count(distinct c10), count(distinct c11),
@@ -520,7 +625,7 @@ PREHOOK: query: explain extended select count (distinct c0), count(distinct c1),
        count(distinct c62), count(distinct c61), count(distinct c62), count(distinct c63), count(distinct c64) from test_count
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_count
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: explain extended select count (distinct c0), count(distinct c1), count(distinct c2), count(distinct c3), count(distinct c4), count(distinct c5),
        count(distinct c6), count(distinct c7), count(distinct c8), count(distinct c9), count(distinct c10), count(distinct c11),
        count(distinct c12), count(distinct c13), count(distinct c14), count(distinct c15), count(distinct c16), count(distinct c17),
@@ -534,7 +639,7 @@ POSTHOOK: query: explain extended select count (distinct c0), count(distinct c1)
        count(distinct c62), count(distinct c61), count(distinct c62), count(distinct c63), count(distinct c64) from test_count
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_count
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 OPTIMIZED SQL: SELECT COUNT(DISTINCT `c0`) AS `_o__c0`, COUNT(DISTINCT `c1`) AS `_o__c1`, COUNT(DISTINCT `c2`) AS `_o__c2`, COUNT(DISTINCT `c3`) AS `_o__c3`, COUNT(DISTINCT `c4`) AS `_o__c4`, COUNT(DISTINCT `c5`) AS `_o__c5`, COUNT(DISTINCT `c6`) AS `_o__c6`, COUNT(DISTINCT `c7`) AS `_o__c7`, COUNT(DISTINCT `c8`) AS `_o__c8`, COUNT(DISTINCT `c9`) AS `_o__c9`, COUNT(DISTINCT `c10`) AS `_o__c10`, COUNT(DISTINCT `c11`) AS `_o__c11`, COUNT(DISTINCT `c12`) AS `_o__c12`, COUNT(DISTINCT `c13`) AS `_o__c13`, COUNT(DISTINCT `c14`) AS `_o__c14`, COUNT(DISTINCT `c15`) AS `_o__c15`, COUNT(DISTINCT `c16`) AS `_o__c16`, COUNT(DISTINCT `c17`) AS `_o__c17`, COUNT(DISTINCT `c18`) AS `_o__c18`, COUNT(DISTINCT `c19`) AS `_o__c19`, COUNT(DISTINCT `c20`) AS `_o__c20`, COUNT(DISTINCT `c21`) AS `_o__c21`, COUNT(DISTINCT `c22`) AS `_o__c22`, COUNT(DISTINCT `c23`) AS `_o__c23`, COUNT(DISTINCT `c24`) AS `_o__c24`, COUNT(DISTINCT `c25`) AS `_o__c25`, COUNT(DISTINCT `c26`) AS `_o__c26`, COUNT(DISTINCT `c27`) AS `_o__c27`, COUNT(DISTINCT `c28`) AS `_o__c28`, COUNT(DISTINCT `c29`) AS `_o__c29`, COUNT(DISTINCT `c30`) AS `_o__c30`, COUNT(DISTINCT `c31`) AS `_o__c31`, COUNT(DISTINCT `c32`) AS `_o__c32`, COUNT(DISTINCT `c33`) AS `_o__c33`, COUNT(DISTINCT `c34`) AS `_o__c34`, COUNT(DISTINCT `c35`) AS `_o__c35`, COUNT(DISTINCT `c36`) AS `_o__c36`, COUNT(DISTINCT `c37`) AS `_o__c37`, COUNT(DISTINCT `c38`) AS `_o__c38`, COUNT(DISTINCT `c39`) AS `_o__c39`, COUNT(DISTINCT `c40`) AS `_o__c40`, COUNT(DISTINCT `c41`) AS `_o__c41`, COUNT(DISTINCT `c42`) AS `_o__c42`, COUNT(DISTINCT `c43`) AS `_o__c43`, COUNT(DISTINCT `c44`) AS `_o__c44`, COUNT(DISTINCT `c45`) AS `_o__c45`, COUNT(DISTINCT `c46`) AS `_o__c46`, COUNT(DISTINCT `c47`) AS `_o__c47`, COUNT(DISTINCT `c48`) AS `_o__c48`, COUNT(DISTINCT `c49`) AS `_o__c49`, COUNT(DISTINCT `c50`) AS `_o__c50`, COUNT(DISTINCT `c51`) AS `_o__c51`, COUNT(DISTINCT `c52`) AS `_o__c52`, COUNT(DISTINCT `c53`) AS `_o__c53`, COUNT(DISTINCT `c54`) AS `_o__c54`, COUNT(DISTINCT `c55`) AS `_o__c55`, COUNT(DISTINCT `c56`) AS `_o__c56`, COUNT(DISTINCT `c57`) AS `_o__c57`, COUNT(DISTINCT `c58`) AS `_o__c58`, COUNT(DISTINCT `c59`) AS `_o__c59`, COUNT(DISTINCT `c62`) AS `_o__c60`, COUNT(DISTINCT `c61`) AS `_o__c61`, COUNT(DISTINCT `c62`) AS `_o__c62`, COUNT(DISTINCT `c63`) AS `_o__c63`, COUNT(DISTINCT `c64`) AS `_o__c64`
 FROM `default`.`test_count`
 STAGE DEPENDENCIES:
@@ -575,10 +680,12 @@ STAGE PLANS:
                         Statistics: Num rows: 1 Data size: 6070 Basic stats: COMPLETE Column stats: COMPLETE
                         tag: -1
                         auto parallelism: false
+            Execution mode: llap
+            LLAP IO: all inputs
             Path -> Alias:
-              hdfs://### HDFS PATH ### [test_count]
+#### A masked pattern was here ####
             Path -> Partition:
-              hdfs://### HDFS PATH ### 
+#### A masked pattern was here ####
                 Partition
                   base file name: test_count
                   input format: org.apache.hadoop.mapred.TextInputFormat
@@ -590,7 +697,6 @@ STAGE PLANS:
                     columns c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50,c51,c52,c53,c54,c55,c56,c57,c58,c59,c60,c61,c62,c63,c64
                     columns.types string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string
 #### A masked pattern was here ####
-                    location hdfs://### HDFS PATH ###
                     name default.test_count
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -605,7 +711,6 @@ STAGE PLANS:
                       columns.comments 
                       columns.types string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string:string
 #### A masked pattern was here ####
-                      location hdfs://### HDFS PATH ###
                       name default.test_count
                       serialization.format 1
                       serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -615,6 +720,7 @@ STAGE PLANS:
             Truncated Path -> Alias:
               /test_count [test_count]
         Reducer 2 
+            Execution mode: llap
             Needs Tagging: false
             Reduce Operator Tree:
               Group By Operator
@@ -630,10 +736,10 @@ STAGE PLANS:
                     bucketingVersion: 2
                     compressed: false
                     GlobalTableId: 0
-                    directory: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
                     NumFilesPerFileSink: 1
                     Statistics: Num rows: 1 Data size: 520 Basic stats: COMPLETE Column stats: COMPLETE
-                    Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -670,7 +776,7 @@ PREHOOK: query: select count (distinct c0), count(distinct c1), count(distinct c
        count(distinct c62), count(distinct c61), count(distinct c62), count(distinct c63), count(distinct c64) from test_count
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_count
-PREHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 POSTHOOK: query: select count (distinct c0), count(distinct c1), count(distinct c2), count(distinct c3), count(distinct c4), count(distinct c5),
        count(distinct c6), count(distinct c7), count(distinct c8), count(distinct c9), count(distinct c10), count(distinct c11),
        count(distinct c12), count(distinct c13), count(distinct c14), count(distinct c15), count(distinct c16), count(distinct c17),
@@ -684,5 +790,36 @@ POSTHOOK: query: select count (distinct c0), count(distinct c1), count(distinct
        count(distinct c62), count(distinct c61), count(distinct c62), count(distinct c63), count(distinct c64) from test_count
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_count
-POSTHOOK: Output: hdfs://### HDFS PATH ###
+#### A masked pattern was here ####
 1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1
+PREHOOK: query: select count (distinct c0), count(distinct c1), count(distinct c2), count(distinct c3), count(distinct c4), count(distinct c5),
+       count(distinct c6), count(distinct c7), count(distinct c8), count(distinct c9), count(distinct c10), count(distinct c11),
+       count(distinct c12), count(distinct c13), count(distinct c14), count(distinct c15), count(distinct c16), count(distinct c17),
+       count(distinct c18), count(distinct c19), count(distinct c20), count(distinct c21), count(distinct c22), count(distinct c23),
+       count(distinct c24), count(distinct c25), count(distinct c26), count(distinct c27), count(distinct c28), count(distinct c29),
+       count(distinct c30), count(distinct c31), count(distinct c32), count(distinct c33), count(distinct c34), count(distinct c35),
+       count(distinct c36), count(distinct c37), count(distinct c38), count(distinct c39), count(distinct c40), count(distinct c41),
+       count(distinct c42), count(distinct c43), count(distinct c44), count(distinct c45), count(distinct c46), count(distinct c47),
+       count(distinct c48), count(distinct c49), count(distinct c50), count(distinct c51), count(distinct c52), count(distinct c53),
+       count(distinct c54), count(distinct c55), count(distinct c56), count(distinct c57), count(distinct c58), count(distinct c59),
+       count(distinct c62), count(distinct c61) from test_count
+group by c64,c0||c1,c0||c2,c0||c3,c0||c4,c0||c5,c0||c6
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_count
+#### A masked pattern was here ####
+POSTHOOK: query: select count (distinct c0), count(distinct c1), count(distinct c2), count(distinct c3), count(distinct c4), count(distinct c5),
+       count(distinct c6), count(distinct c7), count(distinct c8), count(distinct c9), count(distinct c10), count(distinct c11),
+       count(distinct c12), count(distinct c13), count(distinct c14), count(distinct c15), count(distinct c16), count(distinct c17),
+       count(distinct c18), count(distinct c19), count(distinct c20), count(distinct c21), count(distinct c22), count(distinct c23),
+       count(distinct c24), count(distinct c25), count(distinct c26), count(distinct c27), count(distinct c28), count(distinct c29),
+       count(distinct c30), count(distinct c31), count(distinct c32), count(distinct c33), count(distinct c34), count(distinct c35),
+       count(distinct c36), count(distinct c37), count(distinct c38), count(distinct c39), count(distinct c40), count(distinct c41),
+       count(distinct c42), count(distinct c43), count(distinct c44), count(distinct c45), count(distinct c46), count(distinct c47),
+       count(distinct c48), count(distinct c49), count(distinct c50), count(distinct c51), count(distinct c52), count(distinct c53),
+       count(distinct c54), count(distinct c55), count(distinct c56), count(distinct c57), count(distinct c58), count(distinct c59),
+       count(distinct c62), count(distinct c61) from test_count
+group by c64,c0||c1,c0||c2,c0||c3,c0||c4,c0||c5,c0||c6
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_count
+#### A masked pattern was here ####
+1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1
