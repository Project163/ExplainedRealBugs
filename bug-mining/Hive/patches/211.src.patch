diff --git a/CHANGES.txt b/CHANGES.txt
index 2e75cce39b..f26f1655bb 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -384,6 +384,9 @@ Trunk - Unreleased
     HIVE-643. recognize escaped strings
     (Emil Ibrishimov via namit)
 
+    HIVE-592. Renaming internal table should rename HDFS.
+    (Prasad Chakka via zshao)
+
 Release 0.3.1 - Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/metastore/build.xml b/metastore/build.xml
index b5e42cdb72..50bbf93f47 100755
--- a/metastore/build.xml
+++ b/metastore/build.xml
@@ -77,7 +77,6 @@
 	<jvmarg line="-Dlog4j.configuration=${basedir}/../conf/hive-log4j.properties"/>
     </datanucleusenhancer>
   </target>
-
   <target name="model-jar" depends="model-enhance">
     <echo message="Jar: ${name}"/>
     <jar
diff --git a/metastore/if/hive_metastore.thrift b/metastore/if/hive_metastore.thrift
index 99e94045f3..ba668eec85 100755
--- a/metastore/if/hive_metastore.thrift
+++ b/metastore/if/hive_metastore.thrift
@@ -15,7 +15,7 @@ struct Version {
 
 struct FieldSchema {
   1: string name, // name of the field
-  2: string type, // type of the field. primitive types defined above, specify list<TYPE_NAME>, map<TYPE_NAME, TYPE_NAME> for lists & maps 
+  2: string type, // type of the field. primitive types defined above, specify list<TYPE_NAME>, map<TYPE_NAME, TYPE_NAME> for lists & maps
   3: string comment
 }
 
@@ -90,6 +90,7 @@ struct Index {
   3: string       tableName,
   4: string       dbName,
   5: list<string> colNames,  // for now columns will be sorted in the ascending order
+  6: string       partName   // partition name
 }
 
 // schema of the table/query results etc.
@@ -137,7 +138,7 @@ exception InvalidOperationException {
 */
 service ThriftHiveMetastore extends fb303.FacebookService
 {
-  bool create_database(1:string name, 2:string description) 
+  bool create_database(1:string name, 2:string description)
                                        throws(1:AlreadyExistsException o1, 2:MetaException o2)
   Database get_database(1:string name) throws(1:NoSuchObjectException o1, 2:MetaException o2)
   bool drop_database(1:string name)    throws(2:MetaException o2)
@@ -147,7 +148,7 @@ service ThriftHiveMetastore extends fb303.FacebookService
   Type get_type(1:string name)  throws(1:MetaException o2)
   bool create_type(1:Type type) throws(1:AlreadyExistsException o1, 2:InvalidObjectException o2, 3:MetaException o3)
   bool drop_type(1:string type) throws(1:MetaException o2)
-  map<string, Type> get_type_all(1:string name) 
+  map<string, Type> get_type_all(1:string name)
                                 throws(1:MetaException o2)
 
   list<FieldSchema> get_fields(1: string db_name, 2: string table_name) throws (1: MetaException o1, 2: UnknownTableException o2, 3: UnknownDBException o3),
@@ -163,31 +164,31 @@ service ThriftHiveMetastore extends fb303.FacebookService
   void create_table(1:Table tbl) throws(1:AlreadyExistsException o1, 2:InvalidObjectException o2, 3:MetaException o3, 4:NoSuchObjectException o4)
   // drops the table and all the partitions associated with it if the table has partitions
   // delete data (including partitions) if deleteData is set to true
-  void drop_table(1:string dbname, 2:string name, 3:bool deleteData) 
+  void drop_table(1:string dbname, 2:string name, 3:bool deleteData)
                        throws(1:NoSuchObjectException o1, 2:MetaException o3)
-  list<string> get_tables(1: string db_name, 2: string pattern) 
+  list<string> get_tables(1: string db_name, 2: string pattern)
                        throws (1: MetaException o1)
 
-  Table get_table(1:string dbname, 2:string tbl_name) 
+  Table get_table(1:string dbname, 2:string tbl_name)
                        throws (1:MetaException o1, 2:NoSuchObjectException o2)
   // alter table applies to only future partitions not for existing partitions
-  void alter_table(1:string dbname, 2:string tbl_name, 3:Table new_tbl) 
+  void alter_table(1:string dbname, 2:string tbl_name, 3:Table new_tbl)
                        throws (1:InvalidOperationException o1, 2:MetaException o2)
 
   // the following applies to only tables that have partitions
-  Partition add_partition(1:Partition new_part) 
+  Partition add_partition(1:Partition new_part)
                        throws(1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)
-  Partition append_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals) 
+  Partition append_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals)
                        throws (1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)
-  bool drop_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals, 4:bool deleteData) 
+  bool drop_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals, 4:bool deleteData)
                        throws(1:NoSuchObjectException o1, 2:MetaException o2)
-  Partition get_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals) 
+  Partition get_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals)
                        throws(1:MetaException o1)
-  // returns all the partitions for this table in reverse chronological order. 
+  // returns all the partitions for this table in reverse chronological order.
   // if max parts is given then it will return only that many
-  list<Partition> get_partitions(1:string db_name, 2:string tbl_name, 3:i16 max_parts=-1) 
+  list<Partition> get_partitions(1:string db_name, 2:string tbl_name, 3:i16 max_parts=-1)
                        throws(1:NoSuchObjectException o1, 2:MetaException o2)
-  list<string> get_partition_names(1:string db_name, 2:string tbl_name, 3:i16 max_parts=-1) 
+  list<string> get_partition_names(1:string db_name, 2:string tbl_name, 3:i16 max_parts=-1)
                        throws(1:MetaException o2)
 
   // changes the partition to the new partition object. partition is identified from the part values
diff --git a/metastore/src/gen-php/hive_metastore_types.php b/metastore/src/gen-php/hive_metastore_types.php
index f7c85fe644..d917f39952 100644
--- a/metastore/src/gen-php/hive_metastore_types.php
+++ b/metastore/src/gen-php/hive_metastore_types.php
@@ -1639,6 +1639,7 @@ class metastore_Index {
   public $tableName = null;
   public $dbName = null;
   public $colNames = null;
+  public $partName = null;
 
   public function __construct($vals=null) {
     if (!isset(self::$_TSPEC)) {
@@ -1667,6 +1668,10 @@ class metastore_Index {
             'type' => TType::STRING,
             ),
           ),
+        6 => array(
+          'var' => 'partName',
+          'type' => TType::STRING,
+          ),
         );
     }
     if (is_array($vals)) {
@@ -1685,6 +1690,9 @@ class metastore_Index {
       if (isset($vals['colNames'])) {
         $this->colNames = $vals['colNames'];
       }
+      if (isset($vals['partName'])) {
+        $this->partName = $vals['partName'];
+      }
     }
   }
 
@@ -1752,6 +1760,13 @@ class metastore_Index {
             $xfer += $input->skip($ftype);
           }
           break;
+        case 6:
+          if ($ftype == TType::STRING) {
+            $xfer += $input->readString($this->partName);
+          } else {
+            $xfer += $input->skip($ftype);
+          }
+          break;
         default:
           $xfer += $input->skip($ftype);
           break;
@@ -1802,6 +1817,11 @@ class metastore_Index {
       }
       $xfer += $output->writeFieldEnd();
     }
+    if ($this->partName !== null) {
+      $xfer += $output->writeFieldBegin('partName', TType::STRING, 6);
+      $xfer += $output->writeString($this->partName);
+      $xfer += $output->writeFieldEnd();
+    }
     $xfer += $output->writeFieldStop();
     $xfer += $output->writeStructEnd();
     return $xfer;
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java b/metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java
new file mode 100644
index 0000000000..3dba654910
--- /dev/null
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.metastore;
+
+import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.hive.metastore.api.InvalidOperationException;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.Table;
+
+/**
+ * Interface for Alter Table and Alter Partition code 
+ */
+public interface AlterHandler extends Configurable {
+
+  /**
+   * handles alter table
+   * @param msdb      object to get metadata
+   * @param wh TODO
+   * @param dbname    database of the table being altered
+   * @param name      original name of the table being altered. same as 
+   *                  <i>newTable.tableName</i> if alter op is not a rename.
+   * @param newTable  new table object
+   * @throws InvalidOperationException  thrown if the newTable object is invalid
+   * @throws MetaException              thrown if there is any other erro 
+   */
+   public abstract void alterTable(RawStore msdb,
+        Warehouse wh, String dbname,
+        String name, Table newTable) throws InvalidOperationException, MetaException;
+}
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
new file mode 100644
index 0000000000..2dcfed49d7
--- /dev/null
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
@@ -0,0 +1,170 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.metastore;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.metastore.api.InvalidObjectException;
+import org.apache.hadoop.hive.metastore.api.InvalidOperationException;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.Table;
+
+/**
+ * Hive specific implementation of alter 
+ */
+public class HiveAlterHandler  implements AlterHandler {
+
+  private Configuration hiveConf;
+  private static final Log LOG = LogFactory.getLog(HiveAlterHandler.class.getName());
+
+  public Configuration getConf() { 
+    return hiveConf;
+  }
+
+  @SuppressWarnings("nls")
+  public void setConf(Configuration conf) {
+    this.hiveConf = conf;
+  }
+
+  public void alterTable(RawStore msdb, Warehouse wh, String dbname, String name, Table newt)
+  throws InvalidOperationException, MetaException {
+    if (newt == null) {
+      throw new InvalidOperationException("New table is invalid: " + newt);
+    }
+    
+    if(!MetaStoreUtils.validateName(newt.getTableName()) ||
+        !MetaStoreUtils.validateColNames(newt.getSd().getCols())) {
+      throw new InvalidOperationException(newt.getTableName() + " is not a valid object name");
+    }
+
+    Path srcPath = null;
+    FileSystem srcFs = null;
+    Path destPath = null;
+    FileSystem destFs = null;
+
+    boolean success = false;
+    String oldTblLoc = null;
+    String newTblLoc = null;
+    boolean moveData = false;
+    boolean rename = false;
+    try {
+      msdb.openTransaction();
+      name = name.toLowerCase();
+      dbname = dbname.toLowerCase();
+
+      // check if table with the new name already exists
+      if (!newt.getTableName().equalsIgnoreCase(name)
+          || !newt.getDbName().equalsIgnoreCase(dbname)) {
+        if(msdb.getTable(newt.getDbName(), newt.getTableName()) != null) {
+          throw new InvalidOperationException("new table " + newt.getDbName() 
+              + "." + newt.getTableName() + " already exists");
+        }
+        rename = true; 
+      }
+
+      // get old table
+      Table oldt = msdb.getTable(dbname, name);
+      if(oldt == null) {
+        throw new InvalidOperationException("table " + newt.getDbName() 
+            + "." + newt.getTableName() + " doesn't exist");
+      }
+      
+      // check that partition keys have not changed
+      if( oldt.getPartitionKeys().size() != newt.getPartitionKeys().size()
+          || !oldt.getPartitionKeys().containsAll(newt.getPartitionKeys())) {
+        throw new InvalidOperationException("partition keys can not be changed.");
+      }
+      
+      if (rename  // if this alter is a rename
+          && (oldt.getSd().getLocation().compareTo(newt.getSd().getLocation()) == 0 // and user didn't change the default location
+              || StringUtils.isEmpty(newt.getSd().getLocation())) // or new location is empty
+          && !oldt.getParameters().containsKey("EXTERNAL")) { // and table is not an external table
+        // that means user is asking metastore to move data to new location corresponding to the new name
+        // get new location
+        newTblLoc = wh.getDefaultTablePath(newt.getDbName(), newt.getTableName()).toString();
+        newt.getSd().setLocation(newTblLoc);
+        oldTblLoc = oldt.getSd().getLocation();
+        moveData = true;
+        // check that destination does not exist otherwise we will be overwriting data
+        srcPath = new Path(oldTblLoc);
+        srcFs = wh.getFs(srcPath);
+        destPath = new Path(newTblLoc);
+        destFs = wh.getFs(destPath);
+        // check that src and dest are on the same file system
+        if (srcFs != destFs) {
+          throw new InvalidOperationException("table new location " + destPath 
+              + " is on a different file system than the old location " + srcPath
+              + ". This operation is not supported");
+        }
+        try {
+          srcFs.exists(srcPath); // check that src exists and also checks permissions necessary
+          if(destFs.exists(destPath)) {
+            throw new InvalidOperationException("New location for this table "+ newt.getDbName() 
+                + "." + newt.getTableName() + " already exists : " +  destPath); 
+          }
+        } catch (IOException e) {
+          throw new InvalidOperationException("Unable to access new location " + destPath + " for table "
+              + newt.getDbName() + "." + newt.getTableName() );
+        }
+        // also the location field in partition
+        List<Partition> parts = msdb.getPartitions(dbname, name, 0);
+        for (Partition part : parts) {
+          String oldPartLoc = part.getSd().getLocation();
+          if (oldPartLoc.contains(oldTblLoc)) {
+            part.getSd().setLocation(part.getSd().getLocation().replace(oldTblLoc, newTblLoc));
+            msdb.alterPartition(dbname, name, part);
+          }
+        }
+      }
+      // now finally call alter table
+      msdb.alterTable(dbname, name, newt);
+      // commit the changes
+      success = msdb.commitTransaction();
+    } catch (InvalidObjectException e) {
+      LOG.debug(e);
+      throw new InvalidOperationException("Unable to change partition or table." +
+      		" Check metastore logs for detailed stack." + e.getMessage());
+    } finally {
+      if(!success) {
+        msdb.rollbackTransaction();
+      }
+      if(success && moveData) {
+        // change the file name in hdfs
+        // check that src exists otherwise there is no need to copy the data
+        try {
+          if (srcFs.exists(srcPath)) {
+            // rename the src to destination
+            srcFs.rename(srcPath, destPath);
+          }
+        } catch (IOException e) {
+          throw new InvalidOperationException("Unable to access old location " + srcPath + " for table "
+              + dbname + "." + name );
+        }
+      }
+    }
+
+  }
+}
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
index 7885ff3e77..82ce82ee8a 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
@@ -100,6 +100,7 @@ public HMSHandler(String name, HiveConf conf) throws MetaException {
       }
 
       private ClassLoader classLoader;
+      private AlterHandler alterHandler;
       {
         classLoader = Thread.currentThread().getContextClassLoader();
         if (classLoader == null) {
@@ -110,6 +111,8 @@ public HMSHandler(String name, HiveConf conf) throws MetaException {
       private boolean init() throws MetaException {
         rawStoreClassName = hiveConf.get("hive.metastore.rawstore.impl");
         checkForDefaultDb = hiveConf.getBoolean("hive.metastore.checkForDefaultDb", true);
+        String alterHandlerName = hiveConf.get("hive.metastore.alter.impl", HiveAlterHandler.class.getName());
+        alterHandler = (AlterHandler) ReflectionUtils.newInstance(getClass(alterHandlerName, AlterHandler.class), hiveConf);
         wh = new Warehouse(hiveConf);
         createDefaultDB();
         return true;
@@ -147,7 +150,7 @@ private void createDefaultDB() throws MetaException {
         HMSHandler.createDefaultDB = true;
       }
 
-      private Class<?> getClass(String rawStoreClassName, Class<RawStore> class1) throws MetaException {
+      private Class<?> getClass(String rawStoreClassName, Class<?> class1) throws MetaException {
         try {
           return Class.forName(rawStoreClassName, true, classLoader);
         } catch (ClassNotFoundException e) {
@@ -333,6 +336,7 @@ public void drop_table(String dbname, String name, boolean deleteData) throws No
         boolean isExternal = false;
         Path tblPath = null;
         Table tbl = null;
+        isExternal = false;
         try {
           getMS().openTransaction();
           // drop any partitions
@@ -613,16 +617,7 @@ public void alter_table(String dbname, String name, Table newTable) throws Inval
           MetaException {
         this.incrementCounter("alter_table");
         logStartFunction("truncate_table: db=" + dbname + " tbl=" + name + " newtbl=" + newTable.getTableName());
-        if(!MetaStoreUtils.validateName(newTable.getTableName()) ||
-            !MetaStoreUtils.validateColNames(newTable.getSd().getCols())) {
-          throw new InvalidOperationException(newTable.getTableName() + " is not a valid object name");
-        }
-        try {
-          getMS().alterTable(dbname, name, newTable);
-        } catch (InvalidObjectException e) {
-          LOG.error(StringUtils.stringifyException(e));
-          throw new InvalidOperationException("alter is not possible");
-        }
+        alterHandler.alterTable(getMS(), wh, dbname, name, newTable);
       }
 
       public List<String> get_tables(String dbname, String pattern) throws MetaException {
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index 10997d6858..e8031a9cfd 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -854,14 +854,7 @@ public void alterTable(String dbname, String name, Table newTable) throws Invali
       openTransaction();
       name = name.toLowerCase();
       dbname = dbname.toLowerCase();
-      MTable newt = this.getMTable(newTable.getDbName(), newTable.getTableName());
-      if(newt != null) {
-        if(!newTable.getTableName().equals(name) || !newTable.getDbName().equals(dbname)) {
-          // if the old table and new table aren't the same
-          throw new InvalidObjectException("new table " + newTable.getDbName() +" already exists");
-        }
-      }
-      newt = convertToMTable(newTable);
+      MTable newt = convertToMTable(newTable);
       if(newt == null) {
         throw new InvalidObjectException("new table is invalid");
       }
@@ -878,7 +871,7 @@ public void alterTable(String dbname, String name, Table newTable) throws Invali
       oldt.setSd(newt.getSd());
       oldt.setDatabase(newt.getDatabase());
       oldt.setRetention(newt.getRetention());
-      //oldt.setPartitionKeys(newt.getPartitionKeys()); //this should never be changed for hive 
+      oldt.setPartitionKeys(newt.getPartitionKeys()); 
       
       // commit the changes
       success = commitTransaction();
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
index a2024c982c..eb7661d3d8 100755
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
@@ -62,7 +62,7 @@ public Warehouse(Configuration conf) throws MetaException {
   /**
    * Helper function to convert IOException to MetaException
    */
-  private FileSystem getFs(Path f) throws MetaException {
+  public FileSystem getFs(Path f) throws MetaException {
     try {
       return f.getFileSystem(conf);
     } catch (IOException e) {
diff --git a/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java b/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
index f592ce47a2..a67785269a 100644
--- a/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
+++ b/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
@@ -518,12 +518,19 @@ public void testAlterTable() throws Exception {
         assertTrue("Able to rename table with invalid name: " + invTblName, false);
       }
       // try a valid alter table
-      tbl2.setTableName(tblName);
+      tbl2.setTableName(tblName+"_renamed");
       tbl2.getSd().setCols(cols);
       tbl2.getSd().setNumBuckets(32);
       client.alter_table(dbName, tblName, tbl2);
-      Table tbl3 = client.getTable(dbName, tblName);
-      assertEquals("Alter table didn't succeed. Num buckets ", tbl2.getSd().getNumBuckets(), tbl3.getSd().getNumBuckets());
+      Table tbl3 = client.getTable(dbName, tbl2.getTableName());
+      assertEquals("Alter table didn't succeed. Num buckets is different ",
+          tbl2.getSd().getNumBuckets(), tbl3.getSd().getNumBuckets());
+      // check that data has moved
+      FileSystem fs = FileSystem.get(hiveConf);
+      assertFalse("old table location still exists", fs.exists(new Path(tbl.getSd().getLocation())));
+      assertTrue("data did not move to new location", fs.exists(new Path(tbl3.getSd().getLocation())));
+      assertEquals("alter table didn't move data correct location", tbl3.getSd().getLocation(),
+          tbl2.getSd().getLocation());
     } catch (Exception e) {
       System.err.println(StringUtils.stringifyException(e));
       System.err.println("testSimpleTable() failed.");
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 277a997012..71160d8c65 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -536,7 +536,7 @@ private void analyzeDescribeTable(ASTNode ast)
       for (int i = 0; i < partspec.getChildCount(); ++i) {
         ASTNode partspec_val = (ASTNode) partspec.getChild(i);
         String val = stripQuotes(partspec_val.getChild(1).getText());
-        partSpec.put(partspec_val.getChild(0).getText(), val);
+        partSpec.put(partspec_val.getChild(0).getText().toLowerCase(), val);
       }
     }
     
diff --git a/ql/src/test/queries/clientpositive/alter3.q b/ql/src/test/queries/clientpositive/alter3.q
new file mode 100644
index 0000000000..d8eb469825
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/alter3.q
@@ -0,0 +1,19 @@
+drop table alter3_src;
+drop table alter3;
+
+create table alter3_src ( col1 string ) stored as textfile ;
+load data local inpath '../data/files/test.dat' overwrite into table alter3_src ;
+
+create table alter3 ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
+
+insert overwrite table alter3 partition (pCol1='test_part', pcol2='test_part') select col1 from alter3_src ;
+select * from alter3 where pcol1='test_part' and pcol2='test_part';
+
+alter table alter3 rename to alter3_renamed;
+describe extended alter3_renamed;
+describe extended alter3_renamed partition (pCol1='test_part', pcol2='test_part');
+select * from alter3_renamed where pcol1='test_part' and pcol2='test_part';
+
+drop table alter3_src;
+drop table alter3;
+drop table alter3_renamed;
diff --git a/ql/src/test/results/clientpositive/alter3.q.out b/ql/src/test/results/clientpositive/alter3.q.out
new file mode 100644
index 0000000000..e96fabd32a
--- /dev/null
+++ b/ql/src/test/results/clientpositive/alter3.q.out
@@ -0,0 +1,42 @@
+query: drop table alter3_src
+query: drop table alter3
+query: create table alter3_src ( col1 string ) stored as textfile
+query: load data local inpath '../data/files/test.dat' overwrite into table alter3_src
+query: create table alter3 ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile
+query: insert overwrite table alter3 partition (pCol1='test_part', pcol2='test_part') select col1 from alter3_src
+Input: default/alter3_src
+Output: default/alter3/pcol1=test_part/pcol2=test_part
+query: select * from alter3 where pcol1='test_part' and pcol2='test_part'
+Input: default/alter3/pcol1=test_part/pcol2=test_part
+Output: file:/data/users/pchakka/workspace/oshive2/build/ql/tmp/1140751313/10000
+1	test_part	test_part
+2	test_part	test_part
+3	test_part	test_part
+4	test_part	test_part
+5	test_part	test_part
+6	test_part	test_part
+query: alter table alter3 rename to alter3_renamed
+query: describe extended alter3_renamed
+col1	string	
+pcol1	string	
+pcol2	string	
+	 	 
+Detailed Table Information	Table(tableName:alter3_renamed, dbName:default, owner:pchakka, createTime:1247544316, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:string, comment:null)], location:file:/data/users/pchakka/workspace/oshive2/build/ql/test/data/warehouse/alter3_renamed, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:pcol1, type:string, comment:null), FieldSchema(name:pcol2, type:string, comment:null)], parameters:{last_modified_by=pchakka,last_modified_time=1247544320})	
+query: describe extended alter3_renamed partition (pCol1='test_part', pcol2='test_part')
+col1	string	
+pcol1	string	
+pcol2	string	
+	 	 
+Detailed Partition Information	Partition(values:[test_part, test_part], dbName:default, tableName:alter3_renamed, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:string, comment:null)], location:file:/data/users/pchakka/workspace/oshive2/build/ql/test/data/warehouse/alter3_renamed/pcol1=test_part/pcol2=test_part, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{})	
+query: select * from alter3_renamed where pcol1='test_part' and pcol2='test_part'
+Input: default/alter3_renamed/pcol1=test_part/pcol2=test_part
+Output: file:/data/users/pchakka/workspace/oshive2/build/ql/tmp/862867654/10000
+1	test_part	test_part
+2	test_part	test_part
+3	test_part	test_part
+4	test_part	test_part
+5	test_part	test_part
+6	test_part	test_part
+query: drop table alter3_src
+query: drop table alter3
+query: drop table alter3_renamed
