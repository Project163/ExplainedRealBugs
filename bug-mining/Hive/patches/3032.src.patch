diff --git a/itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java b/itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java
index 4fa8aef137..1f3e4848c7 100644
--- a/itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java
+++ b/itests/hive-unit/src/test/java/org/apache/hive/beeline/TestBeeLineWithArgs.java
@@ -533,6 +533,20 @@ public void testQueryProgress() throws Throwable {
     testScriptFile(SCRIPT_TEXT, EXPECTED_PATTERN, true, getBaseArgs(miniHS2.getBaseJdbcURL()));
   }
 
+  /**
+   * Test Beeline could show the query progress for time-consuming query when hive.exec.parallel
+   * is true
+   * @throws Throwable
+   */
+  @Test
+  public void testQueryProgressParallel() throws Throwable {
+    final String SCRIPT_TEXT = "set hive.support.concurrency = false;\n" +
+        "set hive.exec.parallel = true;\n" +
+        "select count(*) from " + tableName + ";\n";
+    final String EXPECTED_PATTERN = "number of splits";
+    testScriptFile(SCRIPT_TEXT, EXPECTED_PATTERN, true, getBaseArgs(miniHS2.getBaseJdbcURL()));
+  }
+
   /**
    * Test Beeline will hide the query progress when silent option is set.
    * @throws Throwable
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 1355230ad3..5d98beb0ce 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -111,6 +111,7 @@
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivObjectActionType;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObjectType;
+import org.apache.hadoop.hive.ql.session.OperationLog;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
 import org.apache.hadoop.hive.serde2.ByteStream;
@@ -1623,6 +1624,7 @@ private TaskRunner launchTask(Task<? extends Serializable> tsk, String queryId,
       if (LOG.isInfoEnabled()){
         LOG.info("Starting task [" + tsk + "] in parallel");
       }
+      tskRun.setOperationLog(OperationLog.getCurrentOperationLog());
       tskRun.start();
     } else {
       if (LOG.isInfoEnabled()){
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskRunner.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskRunner.java
index 99adf2e8d7..94ce0d042d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskRunner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskRunner.java
@@ -21,6 +21,7 @@
 import java.io.Serializable;
 import java.util.concurrent.atomic.AtomicLong;
 
+import org.apache.hadoop.hive.ql.session.OperationLog;
 import org.apache.hadoop.hive.ql.session.SessionState;
 
 /**
@@ -32,6 +33,7 @@ public class TaskRunner extends Thread {
   protected Task<? extends Serializable> tsk;
   protected TaskResult result;
   protected SessionState ss;
+  private OperationLog operationLog;
   private static AtomicLong taskCounter = new AtomicLong(0);
   private static ThreadLocal<Long> taskRunnerID = new ThreadLocal<Long>() {
     @Override
@@ -68,6 +70,7 @@ public boolean isRunning() {
   public void run() {
     runner = Thread.currentThread();
     try {
+      OperationLog.setCurrentOperationLog(operationLog);
       SessionState.start(ss);
       runSequential();
     } finally {
@@ -95,4 +98,8 @@ public void runSequential() {
   public static long getTaskRunnerID () {
     return taskRunnerID.get();
   }
+
+  public void setOperationLog(OperationLog operationLog) {
+    this.operationLog = operationLog;
+  }
 }
diff --git a/service/src/java/org/apache/hive/service/cli/operation/OperationLog.java b/ql/src/java/org/apache/hadoop/hive/ql/session/OperationLog.java
similarity index 80%
rename from service/src/java/org/apache/hive/service/cli/operation/OperationLog.java
rename to ql/src/java/org/apache/hadoop/hive/ql/session/OperationLog.java
index 1c28745f8a..571bbbceae 100644
--- a/service/src/java/org/apache/hive/service/cli/operation/OperationLog.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/OperationLog.java
@@ -15,18 +15,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hive.service.cli.operation;
+package org.apache.hadoop.hive.ql.session;
 
-import com.google.common.base.Charsets;
 import org.apache.commons.io.FileUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.io.IOUtils;
-import org.apache.hive.service.cli.FetchOrientation;
-import org.apache.hive.service.cli.HiveSQLException;
-import org.apache.hive.service.cli.OperationHandle;
 
 import java.io.*;
+import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -78,14 +75,14 @@ public void writeOperationLog(String operationLogMessage) {
 
   /**
    * Read operation execution logs from log file
-   * @param fetchOrientation one of Enum FetchOrientation values
+   * @param isFetchFirst true if the Enum FetchOrientation value is Fetch_First
    * @param maxRows the max number of fetched lines from log
    * @return
-   * @throws HiveSQLException
+   * @throws java.sql.SQLException
    */
-  public List<String> readOperationLog(FetchOrientation fetchOrientation, long maxRows)
-      throws HiveSQLException{
-    return logFile.read(fetchOrientation, maxRows);
+  public List<String> readOperationLog(boolean isFetchFirst, long maxRows)
+      throws SQLException{
+    return logFile.read(isFetchFirst, maxRows);
   }
 
   /**
@@ -116,10 +113,10 @@ synchronized void write(String msg) {
       out.print(msg);
     }
 
-    synchronized List<String> read(FetchOrientation fetchOrientation, long maxRows)
-        throws HiveSQLException{
+    synchronized List<String> read(boolean isFetchFirst, long maxRows)
+        throws SQLException{
       // reset the BufferReader, if fetching from the beginning of the file
-      if (fetchOrientation.equals(FetchOrientation.FETCH_FIRST)) {
+      if (isFetchFirst) {
         resetIn();
       }
 
@@ -148,16 +145,16 @@ private void resetIn() {
       }
     }
 
-    private List<String> readResults(long nLines) throws HiveSQLException {
+    private List<String> readResults(long nLines) throws SQLException {
       if (in == null) {
         try {
           in = new BufferedReader(new InputStreamReader(new FileInputStream(file)));
         } catch (FileNotFoundException e) {
           if (isRemoved) {
-            throw new HiveSQLException("The operation has been closed and its log file " +
+            throw new SQLException("The operation has been closed and its log file " +
                 file.getAbsolutePath() + " has been removed.", e);
           } else {
-            throw new HiveSQLException("Operation Log file " + file.getAbsolutePath() +
+            throw new SQLException("Operation Log file " + file.getAbsolutePath() +
                 " is not found.", e);
           }
         }
@@ -176,10 +173,10 @@ private List<String> readResults(long nLines) throws HiveSQLException {
           }
         } catch (IOException e) {
           if (isRemoved) {
-            throw new HiveSQLException("The operation has been closed and its log file " +
+            throw new SQLException("The operation has been closed and its log file " +
                 file.getAbsolutePath() + " has been removed.", e);
           } else {
-            throw new HiveSQLException("Reading operation log file encountered an exception: ", e);
+            throw new SQLException("Reading operation log file encountered an exception: ", e);
           }
         }
       }
diff --git a/service/src/java/org/apache/hive/service/cli/operation/LogDivertAppender.java b/service/src/java/org/apache/hive/service/cli/operation/LogDivertAppender.java
index 3bf296024e..f7ba8af619 100644
--- a/service/src/java/org/apache/hive/service/cli/operation/LogDivertAppender.java
+++ b/service/src/java/org/apache/hive/service/cli/operation/LogDivertAppender.java
@@ -21,6 +21,7 @@
 import java.util.regex.Pattern;
 
 import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.session.OperationLog;
 import org.apache.log4j.Layout;
 import org.apache.log4j.Logger;
 import org.apache.log4j.WriterAppender;
diff --git a/service/src/java/org/apache/hive/service/cli/operation/Operation.java b/service/src/java/org/apache/hive/service/cli/operation/Operation.java
index acb95cb015..d85db8a0ee 100644
--- a/service/src/java/org/apache/hive/service/cli/operation/Operation.java
+++ b/service/src/java/org/apache/hive/service/cli/operation/Operation.java
@@ -27,6 +27,7 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
+import org.apache.hadoop.hive.ql.session.OperationLog;
 import org.apache.hive.service.cli.FetchOrientation;
 import org.apache.hive.service.cli.HiveSQLException;
 import org.apache.hive.service.cli.OperationHandle;
diff --git a/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java b/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java
index 76be71363f..6cae8a8db1 100644
--- a/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java
+++ b/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java
@@ -18,6 +18,7 @@
 
 package org.apache.hive.service.cli.operation;
 
+import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.Enumeration;
 import java.util.HashMap;
@@ -29,6 +30,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Schema;
+import org.apache.hadoop.hive.ql.session.OperationLog;
 import org.apache.hive.service.AbstractService;
 import org.apache.hive.service.cli.FetchOrientation;
 import org.apache.hive.service.cli.HiveSQLException;
@@ -258,7 +260,13 @@ public RowSet getOperationLogRowSet(OperationHandle opHandle,
     }
 
     // read logs
-    List<String> logs = operationLog.readOperationLog(orientation, maxRows);
+    List<String> logs;
+    try {
+      logs = operationLog.readOperationLog(isFetchFirst(orientation), maxRows);
+    } catch (SQLException e) {
+      throw new HiveSQLException(e.getMessage(), e.getCause());
+    }
+
 
     // convert logs to RowSet
     TableSchema tableSchema = new TableSchema(getLogSchema());
@@ -270,6 +278,15 @@ public RowSet getOperationLogRowSet(OperationHandle opHandle,
     return rowSet;
   }
 
+  private boolean isFetchFirst(FetchOrientation fetchOrientation) {
+    //TODO: Since OperationLog is moved to package o.a.h.h.ql.session,
+    // we may add a Enum there and map FetchOrientation to it.
+    if (fetchOrientation.equals(FetchOrientation.FETCH_FIRST)) {
+      return true;
+    }
+    return false;
+  }
+
   private Schema getLogSchema() {
     Schema schema = new Schema();
     FieldSchema fieldSchema = new FieldSchema();
diff --git a/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java b/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
index 74d7d530f5..684ed7c84f 100644
--- a/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
+++ b/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
@@ -41,6 +41,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.parse.VariableSubstitution;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
+import org.apache.hadoop.hive.ql.session.OperationLog;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.SerDe;
@@ -50,7 +51,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.hive.shims.Utils;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.security.UserGroupInformation;
