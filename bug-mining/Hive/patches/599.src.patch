diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index a21f5899a0..68c81320dd 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -85,6 +85,7 @@ public class HiveConf extends Configuration {
       HiveConf.ConfVars.METASTORE_CACHE_LEVEL2_TYPE,
       HiveConf.ConfVars.METASTORE_IDENTIFIER_FACTORY,
       HiveConf.ConfVars.METASTORE_PLUGIN_REGISTRY_BUNDLE_CHECK,
+      HiveConf.ConfVars.METASTORE_AUTHORIZATION_STORAGE_AUTH_CHECKS,
       };
 
   /**
@@ -214,6 +215,8 @@ public static enum ConfVars {
     METASTORE_IDENTIFIER_FACTORY("datanucleus.identifierFactory", "datanucleus"),
     METASTORE_PLUGIN_REGISTRY_BUNDLE_CHECK("datanucleus.plugin.pluginRegistryBundleCheck", "LOG"),
     METASTORE_BATCH_RETRIEVE_MAX("hive.metastore.batch.retrieve.max", 300),
+    // should we do checks against the storage (usually hdfs) for operations like drop_partition
+    METASTORE_AUTHORIZATION_STORAGE_AUTH_CHECKS("hive.metastore.authorization.storage.checks", false),
 
 
     // Default parameters for creating tables
diff --git a/conf/hive-default.xml b/conf/hive-default.xml
index c42197f137..bb95cb8a9b 100644
--- a/conf/hive-default.xml
+++ b/conf/hive-default.xml
@@ -970,6 +970,15 @@
    An example like "select,drop" will grant select and drop privilege to the owner of the table</description>
 </property>
 
+<property>
+  <name>hive.metastore.authorization.storage.checks</name>
+  <value>false</value>
+  <description>Should the metastore do authorization checks against the underlying storage
+  for operations like drop-partition (disallow the drop-partition if the user in 
+  question doesn't have permissions to delete the corresponding directory
+  on the storage).</description>
+</property>
+
 <property>
   <name>hive.error.on.empty.partition</name>
   <value>false</value>
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
index 6adfc1b4ae..1c36c652fb 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
@@ -527,7 +527,8 @@ public void shutdown() {
     }
 
     private void create_database_core(RawStore ms, final Database db)
-        throws AlreadyExistsException, InvalidObjectException, MetaException {
+        throws AlreadyExistsException, InvalidObjectException, MetaException,
+        IOException {
       if (!validateName(db.getName())) {
         throw new InvalidObjectException(db.getName() + " is not a valid database name");
       }
@@ -633,7 +634,8 @@ Boolean run(RawStore ms) throws Exception {
 
     private void drop_database_core(RawStore ms,
         final String name, final boolean deleteData)
-        throws NoSuchObjectException, InvalidOperationException, MetaException {
+        throws NoSuchObjectException, InvalidOperationException, MetaException,
+        IOException {
       boolean success = false;
       Database db = null;
       try {
@@ -642,6 +644,12 @@ private void drop_database_core(RawStore ms,
         if (!get_all_tables(db.getName()).isEmpty()) {
           throw new InvalidOperationException("Database " + db.getName() + " is not empty");
         }
+        Path path = new Path(db.getLocationUri()).getParent();
+        if (!wh.isWritable(path)) {
+          throw new MetaException("Database not dropped since " +
+              path + " is not writable by " + 
+              hiveConf.getUser());
+        }
         if (ms.dropDatabase(name)) {
           success = ms.commitTransaction();
         }
@@ -963,7 +971,7 @@ private boolean is_table_exists(RawStore ms, String dbname, String name)
 
     private void drop_table_core(final RawStore ms, final String dbname,
         final String name, final boolean deleteData)
-        throws NoSuchObjectException, MetaException {
+        throws NoSuchObjectException, MetaException, IOException {
 
       boolean success = false;
       boolean isExternal = false;
@@ -1004,6 +1012,11 @@ private void drop_table_core(final RawStore ms, final String dbname,
         isExternal = isExternal(tbl);
         if (tbl.getSd().getLocation() != null) {
           tblPath = new Path(tbl.getSd().getLocation());
+          if (!wh.isWritable(tblPath.getParent())) {
+            throw new MetaException("Table metadata not deleted since " +
+                tblPath.getParent() + " is not writable by " + 
+                hiveConf.getUser());
+          }
         }
 
         if (!ms.dropTable(dbname, name)) {
@@ -1361,7 +1374,7 @@ Partition run(RawStore ms) throws Exception {
 
     private boolean drop_partition_common(RawStore ms, String db_name, String tbl_name,
         List<String> part_vals, final boolean deleteData)
-    throws MetaException, NoSuchObjectException {
+    throws MetaException, NoSuchObjectException, IOException {
 
       boolean success = false;
       Path partPath = null;
@@ -1382,6 +1395,11 @@ private boolean drop_partition_common(RawStore ms, String db_name, String tbl_na
         isArchived = MetaStoreUtils.isArchived(part);
         if (isArchived) {
           archiveParentDir = MetaStoreUtils.getOriginalLocation(part);
+          if (!wh.isWritable(archiveParentDir.getParent())) {
+            throw new MetaException("Table partition not deleted since " +
+                archiveParentDir.getParent() + " is not writable by " + 
+                hiveConf.getUser());
+          }
         }
         if (!ms.dropPartition(db_name, tbl_name, part_vals)) {
           throw new MetaException("Unable to drop partition");
@@ -1389,6 +1407,11 @@ private boolean drop_partition_common(RawStore ms, String db_name, String tbl_na
         success = ms.commitTransaction();
         if ((part.getSd() != null) && (part.getSd().getLocation() != null)) {
           partPath = new Path(part.getSd().getLocation());
+          if (!wh.isWritable(partPath.getParent())) {
+            throw new MetaException("Table partition not deleted since " +
+                partPath.getParent() + " is not writable by " + 
+                hiveConf.getUser());
+          }
         }
         tbl = get_table(db_name, tbl_name);
       } finally {
@@ -1962,7 +1985,7 @@ Partition run(RawStore ms) throws Exception {
     private boolean drop_partition_by_name_core(final RawStore ms,
         final String db_name, final String tbl_name, final String part_name,
         final boolean deleteData) throws NoSuchObjectException,
-        MetaException, TException {
+        MetaException, TException, IOException {
 
       List<String> partVals = null;
       try {
@@ -2231,7 +2254,7 @@ Boolean run(RawStore ms) throws Exception {
     private boolean drop_index_by_name_core(final RawStore ms,
         final String dbName, final String tblName,
         final String indexName, final boolean deleteData) throws NoSuchObjectException,
-        MetaException, TException {
+        MetaException, TException, IOException {
 
       boolean success = false;
       Path tblPath = null;
@@ -2255,6 +2278,11 @@ private boolean drop_index_by_name_core(final RawStore ms,
 
           if (tbl.getSd().getLocation() != null) {
             tblPath = new Path(tbl.getSd().getLocation());
+            if (!wh.isWritable(tblPath.getParent())) {
+              throw new MetaException("Index table metadata not deleted since " +
+                  tblPath.getParent() + " is not writable by " + 
+                  hiveConf.getUser());
+            }
           }
           if (!ms.dropTable(dbName, idxTblName)) {
             throw new MetaException("Unable to drop underlying data table "
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
index 4e8c0459fd..5218149a66 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
@@ -20,6 +20,8 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.net.InetSocketAddress;
+import java.net.Socket;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Iterator;
@@ -53,6 +55,8 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;
 import org.apache.hadoop.util.StringUtils;
 
 public class MetaStoreUtils {
@@ -696,6 +700,45 @@ public static void makeDir(Path path, HiveConf hiveConf) throws MetaException {
     }
 
   }
+  
+  public static void startMetaStore(final int port, 
+      final HadoopThriftAuthBridge bridge) throws Exception {
+    Thread thread = new Thread(new Runnable() {
+      public void run() {
+        try {
+          HiveMetaStore.startMetaStore(port, bridge);
+        } catch (Throwable e) {
+          System.exit(1);
+        }
+      }
+    });
+    thread.setDaemon(true);
+    thread.start();
+    loopUntilHMSReady(port);
+  }
+  /**
+   * A simple connect test to make sure that the metastore is up
+   * @throws Exception
+   */
+  private static void loopUntilHMSReady(int port) throws Exception {
+    int retries = 0;
+    Exception exc = null;
+    while (true) {
+      try {
+        Socket socket = new Socket();
+        socket.connect(new InetSocketAddress(port), 5000);
+        socket.close();
+        return;
+      } catch (Exception e) {
+        if (retries++ > 6) { //give up
+          exc = e;
+          break;
+        }
+        Thread.sleep(10000);
+      }
+    }
+    throw exc;
+  }
 
   /**
    * Catches exceptions that can't be handled and bundles them to MetaException
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
index d1817474ff..8814618854 100755
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
@@ -30,6 +30,9 @@
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import javax.security.auth.login.LoginException;
+
+import org.apache.commons.lang.ArrayUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -37,11 +40,14 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /**
@@ -57,6 +63,7 @@ public class Warehouse {
   public static final Log LOG = LogFactory.getLog("hive.metastore.warehouse");
   
   private MetaStoreFS fsHandler = null;
+  private boolean storageAuthCheck = false;
 
   public Warehouse(Configuration conf) throws MetaException {
     this.conf = conf;
@@ -66,6 +73,8 @@ public Warehouse(Configuration conf) throws MetaException {
           + " is not set in the config or blank");
     }
     fsHandler = getMetaStoreFsHandler(conf);
+    storageAuthCheck = HiveConf.getBoolVar(conf, 
+        HiveConf.ConfVars.METASTORE_AUTHORIZATION_STORAGE_AUTH_CHECKS);
   }
   
   private MetaStoreFS getMetaStoreFsHandler(Configuration conf)
@@ -171,6 +180,50 @@ public boolean deleteDir(Path f, boolean recursive) throws MetaException {
     return fsHandler.deleteDir(fs, f, recursive, conf);
   }
 
+  public boolean isWritable(Path path) throws IOException {
+    if (!storageAuthCheck) {
+      // no checks for non-secure hadoop installations
+      return true;
+    }
+    if (path == null) { //what??!!
+      return false;
+    }
+    final FileStatus stat;
+    try {
+      stat = getFs(path).getFileStatus(path);
+    } catch (FileNotFoundException fnfe){
+      // File named by path doesn't exist; nothing to validate.
+      return true;
+    } catch (Exception e) {
+      // all other exceptions are considered as emanating from
+      // unauthorized accesses
+      return false;
+    }
+    final UserGroupInformation ugi;
+    try {
+      ugi = ShimLoader.getHadoopShims().getUGIForConf(conf);
+    } catch (LoginException le) {
+      throw new IOException(le);
+    }
+    String user = ShimLoader.getHadoopShims().getShortUserName(ugi);
+    //check whether owner can delete
+    if (stat.getOwner().equals(user) && 
+        stat.getPermission().getUserAction().implies(FsAction.WRITE)) {
+      return true;
+    }
+    //check whether group of the user can delete
+    if (stat.getPermission().getGroupAction().implies(FsAction.WRITE)) {
+      String[] groups = ugi.getGroupNames();
+      if (ArrayUtils.contains(groups, stat.getGroup())) {
+        return true;
+      }  
+    }
+    //check whether others can delete (uncommon case!!)
+    if (stat.getPermission().getOtherAction().implies(FsAction.WRITE)) {
+      return true;
+    }
+    return false;
+  }
   /*
   // NOTE: This is for generating the internal path name for partitions. Users
   // should always use the MetaStore API to get the path name for a partition.
diff --git a/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java b/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java
new file mode 100644
index 0000000000..935956c269
--- /dev/null
+++ b/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreAuthorization.java
@@ -0,0 +1,115 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+
+public class TestMetaStoreAuthorization extends TestCase {
+  protected HiveConf conf = new HiveConf();
+
+  private final int port = 10000;
+
+  public void setup() throws Exception {
+    System.setProperty(HiveConf.ConfVars.METASTORE_AUTHORIZATION_STORAGE_AUTH_CHECKS.varname,
+        "true");
+    conf.set("hive.metastore.local", "false");
+    conf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    conf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTRETRIES, 3);
+    conf.setIntVar(ConfVars.METASTORE_CLIENT_CONNECT_RETRY_DELAY, 60);
+  }
+
+  public void testIsWritable() throws Exception {
+    setup();
+    conf = new HiveConf(this.getClass());
+    String testDir = System.getProperty("test.warehouse.dir", "/tmp");
+    Path testDirPath = new Path(testDir);
+    FileSystem fs = testDirPath.getFileSystem(conf);
+    Path top = new Path(testDirPath, "_foobarbaz12_");
+    try {
+      fs.mkdirs(top);
+
+      Warehouse wh = new Warehouse(conf);
+      FsPermission writePerm = FsPermission.createImmutable((short)0777);
+      FsPermission noWritePerm = FsPermission.createImmutable((short)0555);
+
+      fs.setPermission(top, writePerm);
+      assertTrue("Expected " + top + " to be writable", wh.isWritable(top));
+
+      fs.setPermission(top, noWritePerm);
+      assertTrue("Expected " + top + " to be not writable", !wh.isWritable(top));
+    } finally {
+      fs.delete(top, true);
+    }
+  }
+
+  public void testMetaStoreAuthorization() throws Exception {
+    setup();
+    MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
+    HiveMetaStoreClient client = new HiveMetaStoreClient(conf);
+
+    FileSystem fs = null;
+    String dbName = "simpdb";
+    Database db1 = null;
+    Path p = null;
+    try {
+      try {
+        db1 = client.getDatabase(dbName);
+        client.dropDatabase(dbName);
+      } catch (NoSuchObjectException noe) {}
+      if (db1 != null) {
+        p = new Path(db1.getLocationUri());
+        fs = p.getFileSystem(conf);
+        fs.delete(p, true);
+      }
+      db1 = new Database();
+      db1.setName(dbName);
+      client.createDatabase(db1);
+      Database db = client.getDatabase(dbName);
+
+      assertTrue("Databases do not match", db1.getName().equals(db.getName()));
+      p = new Path(db.getLocationUri());
+      if (fs == null) {
+        fs = p.getFileSystem(conf);
+      }
+      fs.setPermission(p.getParent(), FsPermission.createImmutable((short)0555));
+      try {
+        client.dropDatabase(dbName);
+        throw new Exception("Expected dropDatabase call to fail");
+      } catch (MetaException me) {
+      }
+      fs.setPermission(p.getParent(), FsPermission.createImmutable((short)0755));
+      client.dropDatabase(dbName);
+    } finally {
+      if (p != null) {
+        fs.delete(p, true);
+      }
+    }
+  }
+}
diff --git a/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java b/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
index 20e9ec4483..763a1fb833 100644
--- a/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
+++ b/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
@@ -429,6 +429,11 @@ public UserGroupInformation getUGIForConf(Configuration conf) throws LoginExcept
   public boolean isSecureShimImpl() {
     return false;
   }
+  
+  @Override
+  public String getShortUserName(UserGroupInformation ugi) {
+    return ugi.getUserName();
+  }
 
   @Override
   public String getTokenStrForm(String tokenSignature) throws IOException {
diff --git a/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java b/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
index e626c75341..0d7d83923b 100644
--- a/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
+++ b/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
@@ -426,6 +426,11 @@ public UserGroupInformation getUGIForConf(Configuration conf) throws IOException
   public boolean isSecureShimImpl() {
     return true;
   }
+  
+  @Override
+  public String getShortUserName(UserGroupInformation ugi) {
+    return ugi.getShortUserName();
+  }
 
   @Override
   public String getTokenStrForm(String tokenSignature) throws IOException {
diff --git a/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java b/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
index bcf230f4c7..63e5874902 100644
--- a/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
+++ b/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
@@ -157,6 +157,15 @@ int createHadoopArchive(Configuration conf, Path parentDir, Path destDir,
    * access control context's user, ignoring the configuration.
    */
   public UserGroupInformation getUGIForConf(Configuration conf) throws LoginException, IOException;
+  
+  /**
+   * Get the short name corresponding to the subject in the passed UGI
+   *
+   * In secure versions of Hadoop, this returns the short name (after
+   * undergoing the translation in the kerberos name rule mapping).
+   * In unsecure versions of Hadoop, this returns the name of the subject
+   */
+  public String getShortUserName(UserGroupInformation ugi);
 
   /**
    * Return true if the Shim is based on Hadoop Security APIs.
diff --git a/shims/src/test/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java b/shims/src/test/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
index 719acca9df..5d0b26a2fa 100644
--- a/shims/src/test/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
+++ b/shims/src/test/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStore;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.io.Text;
@@ -117,18 +118,7 @@ public void setup(final int port) throws Exception {
         System.getProperty("test.build.data", "/tmp")).toString());
     conf = new HiveConf(TestHadoop20SAuthBridge.class);
     conf.setBoolean("hive.metastore.local", false);
-    Thread thread = new Thread(new Runnable() {
-      public void run() {
-        try {
-          HiveMetaStore.startMetaStore(port,new MyHadoopThriftAuthBridge20S());
-        } catch (Throwable e) {
-          System.exit(1);
-        }
-      }
-    });
-    thread.setDaemon(true);
-    thread.start();
-    loopUntilHMSReady(port);
+    MetaStoreUtils.startMetaStore(port, new MyHadoopThriftAuthBridge20S());
   }
 
   public void testSaslWithHiveMetaStore() throws Exception {
@@ -288,30 +278,6 @@ public HiveMetaStoreClient run() {
     assertTrue("Expected metastore operations to fail", hiveClient == null);
   }
 
-  /**
-   * A simple connect test to make sure that the metastore is up
-   * @throws Exception
-   */
-  private void loopUntilHMSReady(int port) throws Exception {
-    int retries = 0;
-    Exception exc = null;
-    while (true) {
-      try {
-        Socket socket = new Socket();
-        socket.connect(new InetSocketAddress(port), 5000);
-        socket.close();
-        return;
-      } catch (Exception e) {
-        if (retries++ > 6) { //give up
-          exc = e;
-          break;
-        }
-        Thread.sleep(10000);
-      }
-    }
-    throw exc;
-  }
-
   private void createDBAndVerifyExistence(HiveMetaStoreClient client)
   throws Exception {
     String dbName = "simpdb";
