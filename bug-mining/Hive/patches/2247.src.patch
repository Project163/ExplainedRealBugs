diff --git a/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java b/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java
index 3e666e5402..df73c7876e 100644
--- a/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java
+++ b/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.common.type;
 
 import java.math.BigDecimal;
+import java.math.BigInteger;
 
 import org.junit.Assert;
 import org.junit.Test;
@@ -118,4 +119,17 @@ public void testException() {
     Assert.assertNull(dec);
   }
 
+  @Test
+  public void testBinaryConversion() {
+    HiveDecimal dec = HiveDecimal.create("234.79");
+    int scale = 2;
+    byte[] d = dec.setScale(2).unscaledValue().toByteArray();
+    Assert.assertEquals(dec, HiveDecimal.create(new BigInteger(d), scale));
+    int prec = 5;
+    int len =  (int) (Math.ceil((Math.log(Math.pow(10, prec)) - 1)/Math.log(2) + 1) / 8);
+    byte[] res = new byte[len];
+    System.arraycopy(d, 0, res, len-d.length, d.length); // Padding leading zeros.
+    Assert.assertEquals(dec, HiveDecimal.create(new BigInteger(res), scale));
+  }
+
 }
diff --git a/data/files/dec_comp.txt b/data/files/dec_comp.txt
new file mode 100644
index 0000000000..008ebfc43d
--- /dev/null
+++ b/data/files/dec_comp.txt
@@ -0,0 +1,2 @@
+3.14,6.28,7.30|k1:92.77,k2:29.39|5,9.03
+12.4,1.33,0.34|k2:2.79,k4:29.09|11,0.0314
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
index 1243585a54..29f7e1133f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
@@ -83,8 +83,11 @@ private static Type convertType(final String name, final TypeInfo typeInfo, fina
         throw new UnsupportedOperationException("Void type not implemented");
       } else if (typeInfo instanceof DecimalTypeInfo) {
         DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo) typeInfo;
-        return Types.primitive(PrimitiveTypeName.BINARY, repetition).as(OriginalType.DECIMAL).scale(decimalTypeInfo.scale()).
-            precision(decimalTypeInfo.precision()).named(name);
+        int prec = decimalTypeInfo.precision();
+        int scale = decimalTypeInfo.scale();
+        int bytes = ParquetHiveSerDe.PRECISION_TO_BYTE_COUNT[prec - 1];
+        return Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(bytes).as(OriginalType.DECIMAL).
+        		scale(scale).precision(prec).named(name);
       } else if (typeInfo.equals(TypeInfoFactory.unknownTypeInfo)) {
         throw new UnsupportedOperationException("Unknown type not implemented");
       } else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
index 6b23fbe6bc..fefa8712a1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
@@ -37,7 +37,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
@@ -67,12 +66,21 @@
  *
  */
 public class ParquetHiveSerDe extends AbstractSerDe {
-
   public static final Text MAP_KEY = new Text("key");
   public static final Text MAP_VALUE = new Text("value");
   public static final Text MAP = new Text("map");
   public static final Text ARRAY = new Text("bag");
 
+  // Map precision to the number bytes needed for binary conversion.
+  public static final int PRECISION_TO_BYTE_COUNT[] = new int[38];
+  static {
+    for (int prec = 1; prec <= 38; prec++) {
+      // Estimated number of bytes needed.
+      PRECISION_TO_BYTE_COUNT[prec - 1] = (int)
+          Math.ceil((Math.log(Math.pow(10, prec) - 1) / Math.log(2) + 1) / 8);
+    }
+  }
+
   private SerDeStats stats;
   private ObjectInspector objInspector;
 
@@ -246,7 +254,18 @@ private Writable createPrimitive(final Object obj, final PrimitiveObjectInspecto
     case DECIMAL:
       HiveDecimal hd = (HiveDecimal)inspector.getPrimitiveJavaObject(obj);
       DecimalTypeInfo decTypeInfo = (DecimalTypeInfo) inspector.getTypeInfo();
-      return new BinaryWritable(Binary.fromByteArray(hd.setScale(decTypeInfo.scale()).unscaledValue().toByteArray()));
+      int prec = decTypeInfo.precision();
+      int scale = decTypeInfo.scale();
+      byte[] src = hd.setScale(scale).unscaledValue().toByteArray();
+      // Estimated number of bytes needed.
+      int bytes =  PRECISION_TO_BYTE_COUNT[prec - 1];
+      if (bytes == src.length) {
+        // No padding needed.
+        return new BinaryWritable(Binary.fromByteArray(src));
+      }
+      byte[] tgt = new byte[bytes];
+      System.arraycopy(src, 0, tgt, bytes - src.length, src.length); // Padding leading zeroes.
+      return new BinaryWritable(Binary.fromByteArray(tgt));
     default:
       throw new SerDeException("Unknown primitive : " + inspector.getPrimitiveCategory());
     }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
index ff604ab467..b87cf74496 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
@@ -20,6 +20,7 @@
 import java.util.List;
 
 import org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter;
+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.junit.Test;
@@ -27,6 +28,8 @@
 import parquet.schema.MessageType;
 import parquet.schema.MessageTypeParser;
 import parquet.schema.OriginalType;
+import parquet.schema.Types;
+import parquet.schema.PrimitiveType.PrimitiveTypeName;
 import parquet.schema.Type.Repetition;
 
 public class TestHiveSchemaConverter {
@@ -80,7 +83,7 @@ public void testDecimalType() throws Exception {
             "a",
             "decimal(5,2)",
             "message hive_schema {\n"
-            + "  optional binary a (DECIMAL(5,2));\n"
+            + "  optional fixed_len_byte_array(3) a (DECIMAL(5,2));\n"
             + "}\n");
   }
 
@@ -104,7 +107,7 @@ public void testArrayDecimal() throws Exception {
             "message hive_schema {\n"
             + "  optional group arrayCol (LIST) {\n"
             + "    repeated group bag {\n"
-            + "      optional binary array_element (DECIMAL(5,2));\n"
+            + "      optional fixed_len_byte_array(3) array_element (DECIMAL(5,2));\n"
             + "    }\n"
             + "  }\n"
             + "}\n");
@@ -119,7 +122,7 @@ public void testStruct() throws Exception {
             + "    optional int32 a;\n"
             + "    optional double b;\n"
             + "    optional boolean c;\n"
-            + "    optional binary d (DECIMAL(5,2));\n"
+            + "    optional fixed_len_byte_array(3) d (DECIMAL(5,2));\n"
             + "  }\n"
             + "}\n");
   }
@@ -146,7 +149,7 @@ public void testMapDecimal() throws Exception {
             + "  optional group mapCol (MAP) {\n"
             + "    repeated group map (MAP_KEY_VALUE) {\n"
             + "      required binary key;\n"
-            + "      optional binary value (DECIMAL(5,2));\n"
+            + "      optional fixed_len_byte_array(3) value (DECIMAL(5,2));\n"
             + "    }\n"
             + "  }\n"
             + "}\n");
diff --git a/ql/src/test/results/clientpositive/parquet_decimal1.q.out b/ql/src/test/results/clientpositive/parquet_decimal1.q.out
index de1145b5cd..9ff09504fe 100644
--- a/ql/src/test/results/clientpositive/parquet_decimal1.q.out
+++ b/ql/src/test/results/clientpositive/parquet_decimal1.q.out
@@ -70,9 +70,6 @@ POSTHOOK: query: SELECT * FROM parq_dec_comp
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@parq_dec_comp
 #### A masked pattern was here ####
-POSTHOOK: Lineage: parq_dec_comp.arr SIMPLE [(dec_comp)dec_comp.FieldSchema(name:arr, type:array<decimal(5,2)>, comment:null), ]
-POSTHOOK: Lineage: parq_dec_comp.m SIMPLE [(dec_comp)dec_comp.FieldSchema(name:m, type:map<string,decimal(5,2)>, comment:null), ]
-POSTHOOK: Lineage: parq_dec_comp.s SIMPLE [(dec_comp)dec_comp.FieldSchema(name:s, type:struct<i:int,d:decimal(5,2)>, comment:null), ]
 [3.14,6.28,7.3]	{"k2":29.39,"k1":92.77}	{"i":5,"d":9.03}
 [12.4,1.33,0.34]	{"k4":29.09,"k2":2.79}	{"i":11,"d":0.03}
 PREHOOK: query: DROP TABLE dec_comp
@@ -83,9 +80,6 @@ POSTHOOK: query: DROP TABLE dec_comp
 POSTHOOK: type: DROPTABLE
 POSTHOOK: Input: default@dec_comp
 POSTHOOK: Output: default@dec_comp
-POSTHOOK: Lineage: parq_dec_comp.arr SIMPLE [(dec_comp)dec_comp.FieldSchema(name:arr, type:array<decimal(5,2)>, comment:null), ]
-POSTHOOK: Lineage: parq_dec_comp.m SIMPLE [(dec_comp)dec_comp.FieldSchema(name:m, type:map<string,decimal(5,2)>, comment:null), ]
-POSTHOOK: Lineage: parq_dec_comp.s SIMPLE [(dec_comp)dec_comp.FieldSchema(name:s, type:struct<i:int,d:decimal(5,2)>, comment:null), ]
 PREHOOK: query: DROP TABLE parq_dec_comp
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@parq_dec_comp
@@ -94,6 +88,3 @@ POSTHOOK: query: DROP TABLE parq_dec_comp
 POSTHOOK: type: DROPTABLE
 POSTHOOK: Input: default@parq_dec_comp
 POSTHOOK: Output: default@parq_dec_comp
-POSTHOOK: Lineage: parq_dec_comp.arr SIMPLE [(dec_comp)dec_comp.FieldSchema(name:arr, type:array<decimal(5,2)>, comment:null), ]
-POSTHOOK: Lineage: parq_dec_comp.m SIMPLE [(dec_comp)dec_comp.FieldSchema(name:m, type:map<string,decimal(5,2)>, comment:null), ]
-POSTHOOK: Lineage: parq_dec_comp.s SIMPLE [(dec_comp)dec_comp.FieldSchema(name:s, type:struct<i:int,d:decimal(5,2)>, comment:null), ]
