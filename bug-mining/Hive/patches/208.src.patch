diff --git a/CHANGES.txt b/CHANGES.txt
index 440032d5d5..1a0feefb95 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -137,6 +137,8 @@ Trunk - Unreleased
     HIVE-644. Change default size for merging files to 256MB.
     (Namit Jain via zshao)
 
+    HIVE-405. Cleanup operator initialization. (Prasad Chakka via zshao)
+
   OPTIMIZATIONS
 
     HIVE-279. Predicate Pushdown support (Prasad Chakka via athusoo).
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
index 457de6b2de..23e3ea3cc7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
@@ -26,7 +26,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
-import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.conf.Configuration;
 
 /**
@@ -39,15 +38,16 @@ public class CollectOperator extends Operator <collectDesc> implements Serializa
   transient protected ObjectInspector standardRowInspector;
   transient int maxSize;
 
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+  protected void initializeOp(Configuration hconf) throws HiveException {
+    super.initializeOp(hconf);
     rowList = new ArrayList<Object> ();
     maxSize = conf.getBufferSize().intValue();
-    initializeChildren(hconf, reporter, inputObjInspector);
   }
 
   boolean firstRow = true;
-  public void process(Object row, ObjectInspector rowInspector, int tag)
+  public void process(Object row, int tag)
       throws HiveException {
+    ObjectInspector rowInspector = inputObjInspectors[tag];
     if (firstRow) {
       firstRow = false;
       // Get the standard ObjectInspector of the row
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
index 0fd7fd439c..4613ad6a76 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
@@ -41,7 +41,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
-import org.apache.hadoop.mapred.Reporter;
 
 /**
  * Join operator implementation.
@@ -99,7 +98,6 @@ public void popObj() {
   transient private ArrayList<ArrayList<Object>>[] dummyObjVectors;
   transient private Stack<Iterator<ArrayList<Object>>> iterators;
   transient protected int totalSz; // total size of the composite object
-  transient ObjectInspector joinOutputObjectInspector;  // The OI for the output row 
   
   // keys are the column names. basically this maps the position of the column in 
   // the output of the CommonJoinOperator to the input columnInfo.
@@ -181,8 +179,8 @@ protected static <T extends joinDesc> ObjectInspector getJoinOutputObjectInspect
     return joinOutputObjectInspector;
   }
   
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-    LOG.info("COMMONJOIN " + ((StructObjectInspector)inputObjInspector[0]).getTypeName());   
+  protected void initializeOp(Configuration hconf) throws HiveException {
+    LOG.info("COMMONJOIN " + ((StructObjectInspector)inputObjInspectors[0]).getTypeName());   
     totalSz = 0;
     // Map that contains the rows for each alias
     storage = new HashMap<Byte, ArrayList<ArrayList<Object>>>();
@@ -201,7 +199,7 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
 
     totalSz = populateJoinKeyValue(joinValues, conf.getExprs());
 
-    joinValuesObjectInspectors = getObjectInspectorsFromEvaluators(joinValues, inputObjInspector);
+    joinValuesObjectInspectors = getObjectInspectorsFromEvaluators(joinValues, inputObjInspectors);
     joinValuesStandardObjectInspectors = getStandardObjectInspectors(joinValuesObjectInspectors);
       
     dummyObj = new Object[numAliases];
@@ -227,8 +225,8 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
     
     forwardCache = new Object[totalSz];
     
-    joinOutputObjectInspector = getJoinOutputObjectInspector(order, joinValuesStandardObjectInspectors, conf);
-    LOG.info("JOIN " + ((StructObjectInspector)joinOutputObjectInspector).getTypeName() + " totalsz = " + totalSz);
+    outputObjInspector = getJoinOutputObjectInspector(order, joinValuesStandardObjectInspectors, conf);
+    LOG.info("JOIN " + ((StructObjectInspector)outputObjInspector).getTypeName() + " totalsz = " + totalSz);
   }
 
   public void startGroup() throws HiveException {
@@ -286,7 +284,7 @@ private void createForwardJoinObject(IntermediateObject intObj,
         }
       }
     }
-    forward(forwardCache, joinOutputObjectInspector);
+    forward(forwardCache, outputObjInspector);
   }
 
   private void copyOldArray(boolean[] src, boolean[] dest) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
index e2b99cd006..6e720c33ed 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
@@ -18,22 +18,26 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
+import java.io.IOException;
+import java.io.Serializable;
 import java.net.URLClassLoader;
-import java.util.*;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
 
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.mapred.*;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.hive.ql.plan.mapredWork;
-import org.apache.hadoop.hive.ql.plan.mapredLocalWork;
 import org.apache.hadoop.hive.ql.plan.fetchWork;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.hive.ql.plan.mapredLocalWork;
+import org.apache.hadoop.hive.ql.plan.mapredWork;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reporter;
 
 public class ExecMapper extends MapReduceBase implements Mapper {
 
@@ -46,15 +50,6 @@ public class ExecMapper extends MapReduceBase implements Mapper {
   public static final Log l4j = LogFactory.getLog("ExecMapper");
   private static boolean done;
 
-  private void init() {
-    mo = null;
-    fetchOperators = null;
-    oc = null;
-    jc = null;
-    abort = false;
-    rp = null;
-  }
-  
   public void configure(JobConf job) {
     try {
       l4j.info("conf classpath = " 
@@ -65,58 +60,61 @@ public void configure(JobConf job) {
       l4j.info("cannot get classpath: " + e.getMessage());
     }
     try {
-      init();
       jc = job;
+      // create map and fetch operators
       mapredWork mrwork = Utilities.getMapRedWork(job);
       mo = new MapOperator();
       mo.setConf(mrwork);
-      mapredLocalWork mlo = mrwork.getMapLocalWork();
-      if (mlo != null) {
-        fetchOperators = new HashMap<String, FetchOperator>();
-        Map<String, fetchWork> aliasToFetchWork = mlo.getAliasToFetchWork();
-        Iterator<Map.Entry<String, fetchWork>> fetchWorkSet = aliasToFetchWork
-            .entrySet().iterator();
-        while (fetchWorkSet.hasNext()) {
-          Map.Entry<String, fetchWork> entry = fetchWorkSet.next();
-          String alias = entry.getKey();
-          fetchWork fWork = entry.getValue();
-          fetchOperators.put(alias, new FetchOperator(fWork, job));
-          l4j.info("fetchoperator for " + alias + " initialized");
-        }
+      // initialize map operator
+      mo.setChildren(job);
+      l4j.info(mo.dump(0));
+      mo.initialize(jc, null);
+
+      // initialize map local work
+      mapredLocalWork localWork = mrwork.getMapLocalWork();
+      if (localWork == null) {
+        return;
+      }
+      fetchOperators = new HashMap<String, FetchOperator>();
+      // create map local operators
+      for (Map.Entry<String, fetchWork> entry : localWork.getAliasToFetchWork().entrySet()) {
+        fetchOperators.put(entry.getKey(), new FetchOperator(entry.getValue(), job));
+        l4j.info("fetchoperator for " + entry.getKey() + " created");
+      }
+      // initialize map local operators
+      for (Map.Entry<String, FetchOperator> entry : fetchOperators.entrySet()) {
+        Operator<? extends Serializable> forwardOp = localWork.getAliasToWork().get(entry.getKey()); 
+        // All the operators need to be initialized before process
+        forwardOp.initialize(jc, new ObjectInspector[]{entry.getValue().getOutputObjectInspector()});
+        l4j.info("fetchoperator for " + entry.getKey() + " initialized");
+      }
+      // defer processing of map local operators to first row if in case there is no input (??)
+    } catch (Throwable e) {
+      abort = true;
+      if (e instanceof OutOfMemoryError) {
+        // will this be true here?
+        // Don't create a new object if we are already out of memory 
+        throw (OutOfMemoryError) e; 
+      } else {
+        throw new RuntimeException ("Map operator initialization failed", e);
       }
-
-      // we don't initialize the operator until we have set the output collector
-    } catch (Exception e) {
-      // Bail out ungracefully - we should never hit
-      // this here - but would have hit it in SemanticAnalyzer
-      throw new RuntimeException(e);
     }
+
   }
 
   public void map(Object key, Object value,
                   OutputCollector output,
                   Reporter reporter) throws IOException {
     if(oc == null) {
-      try {
-        oc = output;
-        mo.setOutputCollector(oc);
-        mo.initialize(jc, reporter, null);
-        if (fetchOperators != null) {
-          mapredWork mrwork = Utilities.getMapRedWork(jc);
-          mapredLocalWork localWork = mrwork.getMapLocalWork();
-          Iterator<Map.Entry<String, FetchOperator>> fetchOps = fetchOperators.entrySet().iterator();
-          while (fetchOps.hasNext()) {
-            Map.Entry<String, FetchOperator> entry = fetchOps.next();
-            String alias = entry.getKey();
-            FetchOperator fetchOp = entry.getValue();
-            Operator<? extends Serializable> forwardOp = localWork.getAliasToWork().get(alias); 
-            // All the operators need to be initialized before process
-            forwardOp.initialize(jc, reporter, new ObjectInspector[]{fetchOp.getOutputObjectInspector()});
-          }
-
-          fetchOps = fetchOperators.entrySet().iterator();
-          while (fetchOps.hasNext()) {
-            Map.Entry<String, FetchOperator> entry = fetchOps.next();
+      oc = output;
+      rp = reporter;
+      mo.setOutputCollector(oc);
+      mo.setReporter(rp);
+      // process map local operators
+      if (fetchOperators != null) {
+        try {
+          mapredLocalWork localWork = mo.getConf().getMapLocalWork();
+          for (Map.Entry<String, FetchOperator> entry : fetchOperators.entrySet()) {
             String alias = entry.getKey();
             FetchOperator fetchOp = entry.getValue();
             Operator<? extends Serializable> forwardOp = localWork.getAliasToWork().get(alias); 
@@ -127,20 +125,17 @@ public void map(Object key, Object value,
                 break;
               }
 
-              forwardOp.process(row.o, row.oi, 0);
+              forwardOp.process(row.o, 0);
             }
           }
-        }
-
-        rp = reporter;
-      } catch (Throwable e) {
-        abort = true;
-        e.printStackTrace();
-        if (e instanceof OutOfMemoryError) {
-          // Don't create a new object if we are already out of memory 
-          throw (OutOfMemoryError) e; 
-        } else {
-          throw new RuntimeException ("Map operator initialization failed", e);
+        } catch (Throwable e) {
+          abort = true;
+          if (e instanceof OutOfMemoryError) {
+            // Don't create a new object if we are already out of memory 
+            throw (OutOfMemoryError) e; 
+          } else {
+            throw new RuntimeException ("Map local work failed", e);
+          }
         }
       }
     }
@@ -166,20 +161,7 @@ public void map(Object key, Object value,
   public void close() {
     // No row was processed
     if(oc == null) {
-      try {
-        l4j.trace("Close called no row");
-        mo.initialize(jc, null, null);
-        rp = null;
-      } catch (Throwable e) {
-        abort = true;
-        e.printStackTrace();
-        if (e instanceof OutOfMemoryError) {
-          // Don't create a new object if we are already out of memory 
-          throw (OutOfMemoryError) e; 
-        } else {
-          throw new RuntimeException ("Map operator close failed during initialize", e);
-        }
-      }
+      l4j.trace("Close called. no row processed by map.");
     }
 
     // detecting failed executions by exceptions thrown by the operator tree
@@ -187,13 +169,9 @@ public void close() {
     try {
       mo.close(abort);
       if (fetchOperators != null) {
-        mapredWork mrwork = Utilities.getMapRedWork(jc);
-        mapredLocalWork localWork = mrwork.getMapLocalWork();
-        Iterator<Map.Entry<String, FetchOperator>> fetchOps = fetchOperators.entrySet().iterator();
-        while (fetchOps.hasNext()) {
-          Map.Entry<String, FetchOperator> entry = fetchOps.next();
-          String alias = entry.getKey();
-          Operator<? extends Serializable> forwardOp = localWork.getAliasToWork().get(alias); 
+        mapredLocalWork localWork = mo.getConf().getMapLocalWork();
+        for (Map.Entry<String, FetchOperator> entry : fetchOperators.entrySet()) {
+          Operator<? extends Serializable> forwardOp = localWork.getAliasToWork().get(entry.getKey()); 
           forwardOp.close(abort);
         }
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
index 700e378f99..499f90993d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
@@ -70,6 +70,9 @@ public class ExecReducer extends MapReduceBase implements Reducer {
   }
 
   public void configure(JobConf job) {
+    ObjectInspector[] rowObjectInspector = new ObjectInspector[Byte.MAX_VALUE];
+    ObjectInspector[] valueObjectInspector = new ObjectInspector[Byte.MAX_VALUE];
+    ObjectInspector keyObjectInspector;
     try {
       l4j.info("conf classpath = " 
           + Arrays.asList(((URLClassLoader)job.getClassLoader()).getURLs()));
@@ -81,7 +84,7 @@ public void configure(JobConf job) {
     jc = job;
     mapredWork gWork = Utilities.getMapRedWork(job);
     reducer = gWork.getReducer();
-    reducer.setMapredWork(gWork);
+    reducer.setParentOperators(null); // clear out any parents as reducer is the root
     isTagged = gWork.getNeedsTagging();
     try {
       tableDesc keyTableDesc = gWork.getKeyDesc();
@@ -104,14 +107,25 @@ public void configure(JobConf job) {
       }
     } catch (Exception e) {
       throw new RuntimeException(e);
-    }    
+    }
+    
+    //initialize reduce operator tree
+    try {
+      l4j.info(reducer.dump(0));
+      reducer.initialize(jc, rowObjectInspector);
+    } catch (Throwable e) {
+      abort = true;
+      if (e instanceof OutOfMemoryError) {
+        // Don't create a new object if we are already out of memory 
+        throw (OutOfMemoryError) e; 
+      } else {
+        throw new RuntimeException ("Reduce operator initialization failed", e);
+      }
+    }
   }
 
   private Object keyObject;
-  private ObjectInspector keyObjectInspector;
   private Object[] valueObject = new Object[Byte.MAX_VALUE];
-  private ObjectInspector[] valueObjectInspector = new ObjectInspector[Byte.MAX_VALUE];
-  private ObjectInspector[] rowObjectInspector = new ObjectInspector[Byte.MAX_VALUE];
   
   private BytesWritable groupKey;
   
@@ -122,21 +136,11 @@ public void reduce(Object key, Iterator values,
                      Reporter reporter) throws IOException {
 
     if(oc == null) {
-      try {
-        oc = output;
-        reducer.setOutputCollector(oc);
-        reducer.initialize(jc, reporter, rowObjectInspector);
-        rp = reporter;
-      } catch (Throwable e) {
-        abort = true;
-        e.printStackTrace();
-        if (e instanceof OutOfMemoryError) {
-          // Don't create a new object if we are already out of memory 
-          throw (OutOfMemoryError) e; 
-        } else {
-          throw new RuntimeException ("Reduce operator initialization failed");
-        }
-      }
+      // propagete reporter and output collector to all operators
+      oc = output;
+      rp = reporter;
+      reducer.setOutputCollector(oc);
+      reducer.setReporter(rp);
     }
 
     try {
@@ -186,7 +190,7 @@ public void reduce(Object key, Iterator values,
           l4j.info("ExecReducer: processing " + cntr + " rows");
           nextCntr = getNextCntr(cntr);
         }
-        reducer.process(row, rowObjectInspector[tag.get()], tag.get());
+        reducer.process(row, tag.get());
       }
 
     } catch (Throwable e) {
@@ -213,20 +217,7 @@ public void close() {
 
     // No row was processed
     if(oc == null) {
-      try {
-        l4j.trace("Close called no row");
-        reducer.initialize(jc, null, rowObjectInspector);
-        rp = null;
-      } catch (Throwable e) {
-        abort = true;
-        e.printStackTrace();
-        if (e instanceof OutOfMemoryError) {
-          // Don't create a new object if we are already out of memory 
-          throw (OutOfMemoryError) e; 
-        } else {
-          throw new RuntimeException ("Reduce operator close failed during initialize", e);
-        }
-      }
+      l4j.trace("Close called no row");
     }
 
     try {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
index 064daf748e..2b00398c62 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
@@ -18,14 +18,11 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
+import java.io.Serializable;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.extractDesc;
-import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.conf.Configuration;
 
 /**
  * Extract operator implementation
@@ -35,15 +32,14 @@ public class ExtractOperator extends Operator<extractDesc> implements Serializab
   private static final long serialVersionUID = 1L;
   transient protected ExprNodeEvaluator eval;
 
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+  protected void initializeOp(Configuration hconf) throws HiveException {
     eval = ExprNodeEvaluatorFactory.get(conf.getCol());
-    outputRowInspector = eval.initialize(inputObjInspector[0]);
-    initializeChildren(hconf, reporter, new ObjectInspector[]{outputRowInspector});
+    outputObjInspector = eval.initialize(inputObjInspectors[0]);
+    initializeChildren(hconf);
   }
 
-  ObjectInspector outputRowInspector;
-  public void process(Object row, ObjectInspector rowInspector, int tag) throws HiveException {
-    forward(eval.evaluate(row), outputRowInspector);
+  public void process(Object row, int tag) throws HiveException {
+    forward(eval.evaluate(row), outputObjInspector);
   }
 
   
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index 14aa3eb49d..bbf747f0e1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -18,26 +18,25 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
+import java.io.IOException;
+import java.io.Serializable;
 import java.lang.reflect.Method;
 import java.util.Properties;
 
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.mapred.*;
-import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.Configuration;
-
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
+import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.fileSinkDesc;
 import org.apache.hadoop.hive.ql.plan.tableDesc;
-import org.apache.hadoop.hive.ql.exec.FilterOperator.Counter;
-import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
-import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
-import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
-import org.apache.hadoop.hive.serde.Constants;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.Serializer;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.JobConf;
 
 /**
  * File Sink operator implementation
@@ -72,34 +71,7 @@ private void commit() throws IOException {
     LOG.info("Committed to output file: " + finalPath);
   }
 
-  public void close(boolean abort) throws HiveException {
-    if (state == state.CLOSE) 
-      return;
-
-    state = state.CLOSE;
-    if (!abort) {
-      if (outWriter != null) {
-        try {
-          outWriter.close(abort);
-          commit();
-        } catch (IOException e) {
-          throw new HiveException(e);
-        }
-      }
-    } else {
-      // Will come here if an Exception was thrown in map() or reduce().
-      // Hadoop always call close() even if an Exception was thrown in map() or reduce(). 
-      try {
-        outWriter.close(abort);
-        if(!autoDelete)
-          fs.delete(outPath, true);
-      } catch (Exception e) {
-        e.printStackTrace();
-      }
-    }
-  }
-
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+  protected void initializeOp(Configuration hconf) throws HiveException {
     try {
       serializer = (Serializer)conf.getTableInfo().getDeserializerClass().newInstance();
       serializer.initialize(null, conf.getTableInfo().getProperties());
@@ -128,7 +100,7 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
       finalPath = new Path(tmpPath, taskId);
       outPath = new Path(tmpPath, Utilities.toTempPath(taskId));
 
-      LOG.info("Writing to temp file: " + outPath);
+      LOG.info("Writing to temp file: FS " + outPath);
 
       HiveOutputFormat<?, ?> hiveOutputFormat = conf.getTableInfo().getOutputFileFormatClass().newInstance();
       final Class<? extends Writable> outputClass = serializer.getSerializedClass();
@@ -142,7 +114,7 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
       finalPath = HiveFileFormatUtils.getOutputFormatFinalPath(parent, jc, hiveOutputFormat, isCompressed, finalPath);
       tableDesc tableInfo = conf.getTableInfo();
 
-      this.outWriter = getRecordWriter(jc, hiveOutputFormat, outputClass, isCompressed, tableInfo.getProperties(), outPath);
+      outWriter = getRecordWriter(jc, hiveOutputFormat, outputClass, isCompressed, tableInfo.getProperties(), outPath);
 
       // in recent hadoop versions, use deleteOnExit to clean tmp files.
       try {
@@ -152,14 +124,13 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
         autoDelete = true;
       } catch (Exception e) {}
 
+      initializeChildren(hconf);
     } catch (HiveException e) {
       throw e;
     } catch (Exception e) {
       e.printStackTrace();
       throw new HiveException(e);
     }
-    
-    initializeChildren(hconf, reporter, inputObjInspector);
   }
 
   public static RecordWriter getRecordWriter(JobConf jc, HiveOutputFormat<?, ?> hiveOutputFormat,
@@ -172,12 +143,12 @@ public static RecordWriter getRecordWriter(JobConf jc, HiveOutputFormat<?, ?> hi
   }
 
   Writable recordValue; 
-  public void process(Object row, ObjectInspector rowInspector, int tag) throws HiveException {
+  public void process(Object row, int tag) throws HiveException {
     try {
       if (reporter != null)
         reporter.progress();
       // user SerDe to serialize r, and write it out
-      recordValue = serializer.serialize(row, rowInspector);
+      recordValue = serializer.serialize(row, inputObjInspectors[tag]);
       if (row_count != null){
         row_count.set(row_count.get()+ 1);
       }
@@ -190,6 +161,33 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
     }
   }
 
+  public void close(boolean abort) throws HiveException {
+    if (state == State.CLOSE) 
+      return;
+  
+    state = State.CLOSE;
+    if (!abort) {
+      if (outWriter != null) {
+        try {
+          outWriter.close(abort);
+          commit();
+        } catch (IOException e) {
+          throw new HiveException(e);
+        }
+      }
+    } else {
+      // Will come here if an Exception was thrown in map() or reduce().
+      // Hadoop always call close() even if an Exception was thrown in map() or reduce(). 
+      try {
+        outWriter.close(abort);
+        if(!autoDelete)
+          fs.delete(outPath, true);
+      } catch (Exception e) {
+        e.printStackTrace();
+      }
+    }
+  }
+
   /**
    * @return the name of the operator
    */
@@ -220,7 +218,7 @@ public void jobClose(Configuration hconf, boolean success) throws HiveException
             Utilities.renameOrMoveFiles(fs, intermediatePath, finalPath);
           }
         } else {
-          fs.delete(tmpPath);
+          fs.delete(tmpPath, true);
         }
       }
     } catch (IOException e) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
index 4d06cd1fc3..2e3f985fab 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
@@ -20,16 +20,13 @@
 
 import java.io.Serializable;
 
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.filterDesc;
-import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.io.BooleanWritable;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.mapred.Reporter;
 
 /**
  * Filter operator implementation
@@ -51,23 +48,21 @@ public FilterOperator () {
     consecutiveFails = 0;
   }
 
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-    this.reporter = reporter;
-
+  protected void initializeOp(Configuration hconf) throws HiveException {
     try {
       heartbeatInterval = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVESENDHEARTBEAT);
       this.conditionEvaluator = ExprNodeEvaluatorFactory.get(conf.getPredicate());
       statsMap.put(Counter.FILTERED, filtered_count);
       statsMap.put(Counter.PASSED, passed_count);
       conditionInspector = null;
-      initializeChildren(hconf, reporter, inputObjInspector);
     } catch (Throwable e) {
-      e.printStackTrace();
-      throw new RuntimeException(e);
+      throw new HiveException(e);
     }
+    initializeChildren(hconf);
   }
 
-  public void process(Object row, ObjectInspector rowInspector, int tag) throws HiveException {
+  public void process(Object row, int tag) throws HiveException {
+    ObjectInspector rowInspector = inputObjInspectors[tag];
     if (conditionInspector == null) {
       conditionInspector = (PrimitiveObjectInspector)conditionEvaluator.initialize(rowInspector);
     }
@@ -82,7 +77,7 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
       consecutiveFails++;
       
       // In case of a lot of consecutive failures, send a heartbeat in order to avoid timeout
-      if ((consecutiveFails % heartbeatInterval) == 0)
+      if (((consecutiveFails % heartbeatInterval) == 0) && (reporter != null))
         reporter.progress();
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
index 6284c96d02..642f10bb0f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
@@ -18,13 +18,10 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
+import java.io.Serializable;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.forwardDesc;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.conf.Configuration;
 
 /**
  * Forward Operator
@@ -32,14 +29,10 @@
  **/
 public class ForwardOperator extends  Operator<forwardDesc>  implements Serializable {
   private static final long serialVersionUID = 1L;
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-    initializeChildren(hconf, reporter, inputObjInspector);
-    // nothing to do really ..
-  }
 
   @Override
-  public void process(Object row, ObjectInspector rowInspector, int tag)
+  public void process(Object row, int tag)
       throws HiveException {
-    forward(row, rowInspector);    
+    forward(row, inputObjInspectors[tag]);    
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
index 0541d5a10e..f52c1db067 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
@@ -46,7 +46,6 @@
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.Reporter;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 
@@ -75,7 +74,6 @@ public class GroupByOperator extends Operator <groupByDesc> implements Serializa
   transient GenericUDAFEvaluator[] aggregationEvaluators;
   
   transient protected ArrayList<ObjectInspector> objectInspectors;
-  transient protected ObjectInspector outputObjectInspector;
   transient ArrayList<String> fieldNames;
 
   // Used by sort-based GroupBy: Mode = COMPLETE, PARTIAL1, PARTIAL2, MERGEPARTIAL
@@ -134,14 +132,13 @@ List<Field> getFields() {
   transient int           numEntriesVarSize;
   transient int           numEntriesHashTable;
   
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-
+  protected void initializeOp(Configuration hconf) throws HiveException {
     totalMemory = Runtime.getRuntime().totalMemory();
     numRowsInput = 0;
     numRowsHashTbl = 0;
 
-    assert(inputObjInspector.length == 1);
-    ObjectInspector rowInspector = inputObjInspector[0];
+    assert(inputObjInspectors.length == 1);
+    ObjectInspector rowInspector = inputObjInspectors[0];
 
     // init keyFields
     keyFields = new ExprNodeEvaluator[conf.getKeys().size()];
@@ -235,7 +232,7 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
     currentKeyObjectInspector = 
         ObjectInspectorFactory.getStandardStructObjectInspector(keyNames, Arrays.asList(currentKeyObjectInspectors));
     
-    outputObjectInspector = 
+    outputObjInspector = 
       ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, objectInspectors);
 	
     firstRow = true;
@@ -243,8 +240,7 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
     // is not known, estimate that based on the number of entries
     if (conf.getMode() == groupByDesc.Mode.HASH)
       computeMaxEntriesHashAggr(hconf);
-    
-    initializeChildren(hconf, reporter, new ObjectInspector[]{outputObjectInspector});
+    initializeChildren(hconf);
   }
 
   /**
@@ -428,8 +424,9 @@ protected void updateAggregations(AggregationBuffer[] aggs, Object row, ObjectIn
     }
   }
   
-  public void process(Object row, ObjectInspector rowInspector, int tag) throws HiveException {
+  public void process(Object row, int tag) throws HiveException {
     firstRow = false;
+    ObjectInspector rowInspector = inputObjInspectors[tag];
     // Total number of input rows is needed for hash aggregation only
     if (hashAggr) {
       numRowsInput++;
@@ -654,7 +651,7 @@ protected void forward(ArrayList<Object> keys, AggregationBuffer[] aggs) throws
     for(int i=0; i<aggs.length; i++) {
       forwardCache[keys.size() + i] = aggregationEvaluators[i].evaluate(aggs[i]);
     }
-    forward(forwardCache, outputObjectInspector);
+    forward(forwardCache, outputObjInspector);
   }
   
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
index 0c9293d8b3..4e923b603a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
@@ -24,10 +24,8 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.joinDesc;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.mapred.Reporter;
 
 
 /**
@@ -37,13 +35,12 @@ public class JoinOperator extends CommonJoinOperator<joinDesc> implements Serial
   private static final long serialVersionUID = 1L;
   
   @Override
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-    super.initializeOp(hconf, reporter, inputObjInspector);
-
-    initializeChildren(hconf, reporter, new ObjectInspector[]{joinOutputObjectInspector});
+  protected void initializeOp(Configuration hconf) throws HiveException {
+    super.initializeOp(hconf);
+    initializeChildren(hconf);
   }
   
-  public void process(Object row, ObjectInspector rowInspector, int tag)
+  public void process(Object row, int tag)
       throws HiveException {
     try {
       // get alias
@@ -72,7 +69,7 @@ public void process(Object row, ObjectInspector rowInspector, int tag)
           // Output a warning if we reached at least 1000 rows for a join operand
           // We won't output a warning for the last join operand since the size
           // will never goes to joinEmitInterval.
-          StructObjectInspector soi = (StructObjectInspector)rowInspector;
+          StructObjectInspector soi = (StructObjectInspector)inputObjInspectors[tag];
           StructField sf = soi.getStructFieldRef(Utilities.ReduceField.KEY.toString());
           Object keyObject = soi.getStructFieldData(row, sf);
           LOG.warn("table " + alias + " has " + sz + " rows for join key " + keyObject);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
index 833c473eb7..5ff5b8951d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
@@ -18,16 +18,11 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
-import java.util.HashMap;
+import java.io.Serializable;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.parse.OpParseContext;
-import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.plan.limitDesc;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.conf.Configuration;
 
 /**
  * Limit operator implementation
@@ -39,15 +34,15 @@ public class LimitOperator extends Operator<limitDesc> implements Serializable {
   transient protected int limit;
   transient protected int currCount;
 
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+  protected void initializeOp(Configuration hconf) throws HiveException {
+    super.initializeOp(hconf);
     limit = conf.getLimit();
     currCount = 0;
-    initializeChildren(hconf, reporter, inputObjInspector);
   }
 
-  public void process(Object row, ObjectInspector rowInspector, int tag) throws HiveException {
+  public void process(Object row, int tag) throws HiveException {
     if (currCount < limit) {
-      forward(row, rowInspector);
+      forward(row, inputObjInspectors[tag]);
       currCount++;
     }
     else
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
index f5ab40c6d2..7831c03ae5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
@@ -20,23 +20,25 @@
 
 import java.io.File;
 import java.io.IOException;
-import java.lang.Exception;
 import java.io.Serializable;
+import java.util.ArrayList;
 import java.util.HashMap;
-import java.util.Map;
 import java.util.List;
-import java.util.Random;
-import java.util.ArrayList;
-import java.util.ArrayList;
+import java.util.Map;
 import java.util.Properties;
+import java.util.Random;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.mapJoinDesc;
 import org.apache.hadoop.hive.ql.plan.tableDesc;
-import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.util.jdbm.RecordManager;
+import org.apache.hadoop.hive.ql.util.jdbm.RecordManagerFactory;
+import org.apache.hadoop.hive.ql.util.jdbm.RecordManagerOptions;
+import org.apache.hadoop.hive.ql.util.jdbm.htree.HTree;
 import org.apache.hadoop.hive.serde2.SerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -45,13 +47,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
-import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hadoop.hive.ql.util.jdbm.htree.HTree;
-import org.apache.hadoop.hive.ql.util.jdbm.helper.FastIterator;
-import org.apache.hadoop.hive.ql.util.jdbm.RecordManager;
-import org.apache.hadoop.hive.ql.util.jdbm.RecordManagerFactory;
-import org.apache.hadoop.hive.ql.util.jdbm.RecordManagerOptions;
 
 /**
  * Map side Join operator implementation.
@@ -123,9 +119,8 @@ static public Map<Integer, MapJoinObjectCtx> getMapMetadata() {
   transient int      heartbeatInterval;
   
   @Override
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-    super.initializeOp(hconf, reporter, inputObjInspector);
-    this.reporter=reporter;
+  protected void initializeOp(Configuration hconf) throws HiveException {
+    super.initializeOp(hconf);
     numMapRowsRead = 0;
   
     firstRow = true;
@@ -135,7 +130,7 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
       joinKeys  = new HashMap<Byte, List<ExprNodeEvaluator>>();
       
       populateJoinKeyValue(joinKeys, conf.getKeys());
-      joinKeysObjectInspectors = getObjectInspectorsFromEvaluators(joinKeys, inputObjInspector);
+      joinKeysObjectInspectors = getObjectInspectorsFromEvaluators(joinKeys, inputObjInspectors);
       joinKeysStandardObjectInspectors = getStandardObjectInspectors(joinKeysObjectInspectors); 
         
       // all other tables are small, and are cached in the hash table
@@ -179,7 +174,7 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
       
       mapJoinRowsKey = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVEMAPJOINROWSIZE);
       
-      List<? extends StructField> structFields = ((StructObjectInspector)joinOutputObjectInspector).getAllStructFieldRefs();
+      List<? extends StructField> structFields = ((StructObjectInspector)outputObjInspector).getAllStructFieldRefs();
       if (conf.getOutputColumnNames().size() < structFields.size()) {
         List<ObjectInspector> structFieldObjectInspectors = new ArrayList<ObjectInspector>();
         for (Byte alias : order) {
@@ -191,20 +186,18 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
                 .getFieldObjectInspector());
           }
         }
-        joinOutputObjectInspector = ObjectInspectorFactory
+        outputObjInspector = ObjectInspectorFactory
             .getStandardStructObjectInspector(conf.getOutputColumnNames(),
                 structFieldObjectInspectors);
       }
-      
-      initializeChildren(hconf, reporter, new ObjectInspector[]{joinOutputObjectInspector});
+      initializeChildren(hconf);
     } catch (IOException e) {
-      e.printStackTrace();
       throw new HiveException(e);
     }
   }
 
   @Override
-  public void process(Object row, ObjectInspector rowInspector, int tag) throws HiveException {
+  public void process(Object row, int tag) throws HiveException {
     try {
 
       // get alias
@@ -237,7 +230,7 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
         
         // Send some status perodically 
         numMapRowsRead++;
-        if ((numMapRowsRead % heartbeatInterval) == 0)
+        if (((numMapRowsRead % heartbeatInterval) == 0) && (reporter != null))
           reporter.progress();
 
         HTree hashTable = mapJoinTables.get(alias);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
index 7d64ad2f59..5025241a4c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
@@ -18,28 +18,32 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.util.*;
-import java.io.*;
-import java.net.URLClassLoader;
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import java.util.Map.Entry;
 
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
-
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.mapredWork;
-import org.apache.hadoop.hive.ql.plan.tableDesc;
 import org.apache.hadoop.hive.ql.plan.partitionDesc;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.plan.tableDesc;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
 
 /**
  * Map operator. This triggers overall map side processing.
@@ -57,6 +61,7 @@ public static enum Counter {DESERIALIZE_ERRORS}
   transient private Object[] rowWithPart;
   transient private StructObjectInspector rowObjectInspector;
   transient private boolean isPartitioned;
+  private Map<MapInputPath, MapOpCtx> opCtxMap;
   
   private static class MapInputPath {
     String path;
@@ -96,6 +101,8 @@ private static class MapOpCtx {
     StructObjectInspector rowObjectInspector;
     Object[]              rowWithPart;
     Deserializer          deserializer;
+    public String tableName;
+    public String partName;
     
     /**
      * @param isPartitioned
@@ -139,32 +146,50 @@ public Deserializer getDeserializer() {
     }
   }
   
-  private MapOpCtx initObjectInspector(Configuration hconf, String onefile) throws HiveException, ClassNotFoundException, InstantiationException, IllegalAccessException, SerDeException {
+  /**
+   * Initializes this map op as the root of the tree. It sets JobConf & MapRedWork
+   * and starts initialization of the operator tree rooted at this op.
+   * @param hconf
+   * @param mrwork
+   * @throws HiveException
+   */
+  public void initializeAsRoot(Configuration hconf, mapredWork mrwork) throws HiveException {
+    setConf(mrwork);
+    setChildren(hconf);
+    initialize(hconf, null);
+  }
+  
+  private static MapOpCtx initObjectInspector(mapredWork conf, Configuration hconf, String onefile) 
+    throws HiveException, ClassNotFoundException, InstantiationException, IllegalAccessException, SerDeException {
     partitionDesc pd = conf.getPathToPartitionInfo().get(onefile);
     LinkedHashMap<String, String> partSpec = pd.getPartSpec();
     tableDesc td = pd.getTableDesc();
-    Properties p = td.getProperties();
+    Properties tblProps = td.getProperties();
 
-    // Add alias, table name, and partitions to hadoop conf
-    HiveConf.setVar(hconf, HiveConf.ConfVars.HIVETABLENAME, String.valueOf(p.getProperty("name")));
-    HiveConf.setVar(hconf, HiveConf.ConfVars.HIVEPARTITIONNAME, String.valueOf(partSpec));
     Class sdclass = td.getDeserializerClass();
     if(sdclass == null) {
       String className = td.getSerdeClassName();
       if ((className == "") || (className == null)) {
-        throw new HiveException("SerDe class or the SerDe class name is not set for table: " + td.getProperties().getProperty("name"));
+        throw new HiveException("SerDe class or the SerDe class name is not set for table: " 
+            + td.getProperties().getProperty("name"));
       }
       sdclass = hconf.getClassByName(className);
     }
     
-    deserializer = (Deserializer) sdclass.newInstance();
-    deserializer.initialize(hconf, p);
-    rowObjectInspector = (StructObjectInspector)deserializer.getObjectInspector();
-    
+    String tableName = String.valueOf(tblProps.getProperty("name"));
+    String partName = String.valueOf(partSpec);
+    //HiveConf.setVar(hconf, HiveConf.ConfVars.HIVETABLENAME, tableName);
+    //HiveConf.setVar(hconf, HiveConf.ConfVars.HIVEPARTITIONNAME, partName);
+    Deserializer deserializer = (Deserializer) sdclass.newInstance();
+    deserializer.initialize(hconf, tblProps);
+    StructObjectInspector rowObjectInspector = (StructObjectInspector)deserializer.getObjectInspector();
+
+    MapOpCtx opCtx = null;
     // Next check if this table has partitions and if so
     // get the list of partition names as well as allocate
     // the serdes for the partition columns
-    String pcols = p.getProperty(org.apache.hadoop.hive.metastore.api.Constants.META_TABLE_PARTITION_COLUMNS);
+    String pcols = tblProps.getProperty(org.apache.hadoop.hive.metastore.api.Constants.META_TABLE_PARTITION_COLUMNS);
+    //Log LOG = LogFactory.getLog(MapOperator.class.getName());
     if (pcols != null && pcols.length() > 0) {
       String[] partKeys = pcols.trim().split("/");
       List<String> partNames = new ArrayList<String>(partKeys.length);
@@ -176,79 +201,67 @@ private MapOpCtx initObjectInspector(Configuration hconf, String onefile) throws
         partValues[i] = new Text(partSpec.get(key));
         partObjectInspectors.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);
       }
-      StructObjectInspector partObjectInspector = ObjectInspectorFactory.getStandardStructObjectInspector(partNames, partObjectInspectors);
+      StructObjectInspector partObjectInspector = ObjectInspectorFactory
+                  .getStandardStructObjectInspector(partNames, partObjectInspectors);
       
-      rowWithPart = new Object[2];
+      Object[] rowWithPart = new Object[2];
       rowWithPart[1] = partValues;
-      rowObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(Arrays.asList(new StructObjectInspector[]{
-                                                                                                rowObjectInspector, 
-                                                                                                partObjectInspector}));
-      return new MapOpCtx(true, rowObjectInspector, rowWithPart, deserializer);
+      rowObjectInspector = ObjectInspectorFactory
+                                .getUnionStructObjectInspector(
+                                    Arrays.asList(new StructObjectInspector[]{
+                                                    rowObjectInspector, 
+                                                    partObjectInspector}));
+      //LOG.info("dump " + tableName + " " + partName + " " + rowObjectInspector.getTypeName());
+      opCtx = new MapOpCtx(true, rowObjectInspector, rowWithPart, deserializer);
     }
     else {
-      return new MapOpCtx(false, rowObjectInspector, null, deserializer);
+      //LOG.info("dump2 " + tableName + " " + partName + " " + rowObjectInspector.getTypeName());
+      opCtx = new MapOpCtx(false, rowObjectInspector, null, deserializer);
     }
+    opCtx.tableName = tableName;
+    opCtx.partName = partName;
+    return opCtx;
   }  
-
-  public void initializeOp(Configuration hconf, Reporter reporter,
-      ObjectInspector[] inputObjInspector) throws HiveException {
+  
+  public void setChildren(Configuration hconf) throws HiveException {
+    
     Path fpath = new Path((new Path(HiveConf.getVar(hconf,
         HiveConf.ConfVars.HADOOPMAPFILENAME))).toUri().getPath());
-    ArrayList<Operator<? extends Serializable>> todo = new ArrayList<Operator<? extends Serializable>>();
-    Map<MapInputPath, MapOpCtx> opCtx = new HashMap<MapInputPath, MapOpCtx>();
+    ArrayList<Operator<? extends Serializable>> children = 
+        new ArrayList<Operator<? extends Serializable>>();
+    opCtxMap = new HashMap<MapInputPath, MapOpCtx>();
     statsMap.put(Counter.DESERIALIZE_ERRORS, deserialize_error_count);
 
     try {
-      // initialize the complete subtree
+      boolean done = false;
       for (String onefile : conf.getPathToAliases().keySet()) {
-        MapOpCtx ctx = initObjectInspector(hconf, onefile);
-
+        MapOpCtx opCtx = initObjectInspector(conf, hconf, onefile);
+        Path onepath = new Path(new Path(onefile).toUri().getPath());
         List<String> aliases = conf.getPathToAliases().get(onefile);
         for (String onealias : aliases) {
           Operator<? extends Serializable> op = conf.getAliasToWork().get(
               onealias);
-          opCtx.put(new MapInputPath(onefile, onealias, op), ctx);
-        }
-      }
-
-      boolean done = false;
-      // for each configuration path that fpath can be relativized against ..
-      for (String onefile : conf.getPathToAliases().keySet()) {
-        Path onepath = new Path(new Path(onefile).toUri().getPath());
-        if (!onepath.toUri().relativize(fpath.toUri()).equals(fpath.toUri())) {
-
-          // pick up work corresponding to this configuration path
-          List<String> aliases = conf.getPathToAliases().get(onefile);
-          for (String onealias : aliases) {
-            LOG.info("Adding alias " + onealias + " to work list for file "
-                + fpath.toUri().getPath());
-            Operator<? extends Serializable> op = conf.getAliasToWork().get(
-                onealias);
-            List<Operator<? extends Serializable>> parents = new ArrayList<Operator<? extends Serializable>>();
-            parents.add(this);
-            op.setParentOperators(parents);
-            todo.add(op);
-            MapInputPath inp = new MapInputPath(onefile, onealias, op);
-            LOG.info("dump " + opCtx.get(inp).getRowObjectInspector().getTypeName());
-            op.initialize(hconf, reporter, new ObjectInspector[] { opCtx.get(inp).getRowObjectInspector() });
-
+          LOG.info("Adding alias " + onealias + " to work list for file "
+              + fpath.toUri().getPath());
+          MapInputPath inp = new MapInputPath(onefile, onealias, op);
+          opCtxMap.put(inp, opCtx);
+          op.setParentOperators(new ArrayList<Operator<? extends Serializable>>());
+          op.getParentOperators().add(this);
+          // check for the operators who will process rows coming to this Map Operator
+          if (!onepath.toUri().relativize(fpath.toUri()).equals(fpath.toUri())) {
+            children.add(op);
+            LOG.info("dump " + op.getName() + " " + opCtxMap.get(inp).getRowObjectInspector().getTypeName());
             if (!done) {
-              deserializer = opCtx.get(inp).getDeserializer();
-              isPartitioned = opCtx.get(inp).isPartitioned();
-              rowWithPart = opCtx.get(inp).getRowWithPart();
-              rowObjectInspector = opCtx.get(inp).getRowObjectInspector();
+              deserializer = opCtxMap.get(inp).getDeserializer();
+              isPartitioned = opCtxMap.get(inp).isPartitioned();
+              rowWithPart = opCtxMap.get(inp).getRowWithPart();
+              rowObjectInspector = opCtxMap.get(inp).getRowObjectInspector();
               done = true;
             }
           }
         }
       }
-
-      for (MapInputPath input : opCtx.keySet()) {
-        Operator<? extends Serializable> op = input.op;
-        op.initialize(hconf, reporter, new ObjectInspector[] { opCtx.get(input).getRowObjectInspector() });
-      }
-
-      if (todo.size() == 0) {
+      if (children.size() == 0) {
         // didn't find match for input file path in configuration!
         // serious problem ..
         LOG.error("Configuration does not have any alias for path: "
@@ -256,26 +269,26 @@ public void initializeOp(Configuration hconf, Reporter reporter,
         throw new HiveException("Configuration and input path are inconsistent");
       }
 
-      // we found all the operators that we are supposed to process. now
-      // bootstrap
-      this.setChildOperators(todo);
-      // the child operators may need the global mr configuration. set it now so
-      // that they can get access during initiaize.
-      this.setMapredWork(conf);
-      // way hacky - need to inform child operators about output collector
-      this.setOutputCollector(out);
-
-    } catch (SerDeException e) {
-      e.printStackTrace();
-      throw new HiveException(e);
-    } catch (InstantiationException e) {
-      throw new HiveException(e);
-    } catch (IllegalAccessException e) {
-      throw new HiveException(e);
-    } catch (ClassNotFoundException e) {
+      // we found all the operators that we are supposed to process.
+      setChildOperators(children);      
+    } catch (Exception e) {
       throw new HiveException(e);
     }
   }
+  
+
+  public void initializeOp(Configuration hconf) throws HiveException {
+    // set that parent initialization is done and call initialize on children
+    state = State.INIT;
+    for (Entry<MapInputPath, MapOpCtx> entry : opCtxMap.entrySet()) {
+      // Add alias, table name, and partitions to hadoop conf so that their children will
+      // inherit these
+      HiveConf.setVar(hconf, HiveConf.ConfVars.HIVETABLENAME, entry.getValue().tableName);
+      HiveConf.setVar(hconf, HiveConf.ConfVars.HIVEPARTITIONNAME, entry.getValue().partName);
+      Operator<? extends Serializable> op = entry.getKey().op;
+      op.initialize(hconf, new ObjectInspector[]{entry.getValue().getRowObjectInspector()});
+    }
+  }
 
   public void process(Writable value) throws HiveException {
     try {
@@ -293,8 +306,12 @@ public void process(Writable value) throws HiveException {
     }
   }
 
-  public void process(Object row, ObjectInspector rowInspector, int tag)
+  public void process(Object row, int tag)
       throws HiveException {
     throw new HiveException("Hive 2 Internal error: should not be called!");
   }
+
+  public String getName() {
+    return "MAP";
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
index 8d8d670d7b..7905941520 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
@@ -22,21 +22,18 @@
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
-import java.util.Vector;
 import java.util.Map;
-import org.apache.hadoop.hive.ql.lib.Node;
+import java.util.Vector;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.explain;
 import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
-import org.apache.hadoop.hive.ql.plan.mapredWork;
-import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.mapred.OutputCollector;
@@ -112,16 +109,6 @@ public List<Operator<? extends Serializable>> getParentOperators() {
     return parentOperators;
   }
 
-  public void allocateParentOperatorsInitArray() {
-    if ((parentOperators == null) || (parentsObjectInspector != null))
-      return;
-    parentsObjectInspector = new ParentInit[parentOperators.size()];
-    for (int pos = 0; pos < parentOperators.size(); pos++) {
-      parentsObjectInspector[pos] = new ParentInit();
-      parentsObjectInspector[pos].done = false;
-    }
-  }
-  
   protected T conf;
   protected boolean done;
 
@@ -157,12 +144,14 @@ public RowSchema getSchema() {
 
   transient protected HashMap<Enum<?>, LongWritable> statsMap = new HashMap<Enum<?>, LongWritable> ();
   transient protected OutputCollector out;
-  transient protected Log LOG = LogFactory.getLog(this.getClass().getName());;
-  transient protected mapredWork gWork;
+  transient protected Log LOG = LogFactory.getLog(this.getClass().getName());
   transient protected String alias;
-  transient protected String joinAlias;
   transient protected Reporter reporter;
   transient protected String id;
+  // object inspectors for input rows
+  transient protected ObjectInspector[] inputObjInspectors = new ObjectInspector[Byte.MAX_VALUE];
+  // for output rows of this operator
+  transient protected ObjectInspector outputObjInspector; 
 
   /**
    * A map of output column name to input expression map. This is used by optimizer
@@ -183,32 +172,27 @@ public void setId(String id) {
    */
   public String getIdentifier() { return id; }
   
-  public void setOutputCollector(OutputCollector out) {
-    this.out = out;
+  public void setReporter(Reporter rep) {
+    reporter = rep;
 
     // the collector is same across all operators
     if(childOperators == null)
       return;
 
     for(Operator<? extends Serializable> op: childOperators) {
-      op.setOutputCollector(out);
+      op.setReporter(rep);
     }
   }
+  
+  public void setOutputCollector(OutputCollector out) {
+    this.out = out;
 
-  /**
-   * Operators often need access to global variables. This allows
-   * us to put global config information in the root configuration
-   * object and have that be accessible to all the operators in the
-   * tree.
-   */
-  public void setMapredWork(mapredWork gWork) {
-    this.gWork = gWork;
-
+    // the collector is same across all operators
     if(childOperators == null)
       return;
 
     for(Operator<? extends Serializable> op: childOperators) {
-      op.setMapredWork(gWork);
+      op.setOutputCollector(out);
     }
   }
 
@@ -226,22 +210,6 @@ public void setAlias(String alias) {
     }
   }
 
-  /**
-   * Store the join alias this operator is working on behalf of
-   */
-  public void setJoinAlias(String joinAlias) {
-    this.joinAlias = joinAlias;
-
-    if(childOperators == null)
-      return;
-
-    for(Operator<? extends Serializable> op: childOperators) {
-      op.setJoinAlias(joinAlias);
-    }
-  }
-
-
-
   public Map<Enum<?>, Long> getStats() {
     HashMap<Enum<?>, Long> ret = new HashMap<Enum<?>, Long> ();
     for(Enum<?> one: statsMap.keySet()) {
@@ -250,44 +218,55 @@ public Map<Enum<?>, Long> getStats() {
     return(ret);
   }
 
-  public abstract void initializeOp (Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException;
-
-  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-    if (state == state.INIT) {
-      LOG.info("Already Initialized");
-      return;
+  /**
+   * checks whether all parent operators are initialized or not
+   * @return true if there are no parents or all parents are initialized. false otherwise
+   */
+  protected boolean areAllParentsInitialized() {
+    if (parentOperators == null) {
+      return true;
     }
-
-    LOG.info("Initializing Self " + id);
-    this.reporter = reporter;
-    
-    initializeOp(hconf, reporter, inputObjInspector);
-    state = State.INIT;
-
-    LOG.info("Initialization Done " + id);
+    for(Operator<? extends Serializable> parent: parentOperators) {
+      if (parent.state != State.INIT) {
+        return false;
+      }
+    }
+    return true;
   }
 
-  /** 
-   * The default implementation assumes that the first inspector in the array is the output inspector as well. Specific operators can override this
-   * to pass their own output object inspectors. 
+  /**
+   * Initializes operators only if all parents have been initialized.
+   * Calls operator specific initializer which then initializes child ops.
+   * 
+   * @param hconf
+   * @param inputOIs input object inspector array indexes by tag id. null value is ignored.
+   * @throws HiveException
    */
-  public void initializeChildren (Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-    if (childOperators == null) {
+  public void initialize(Configuration hconf, ObjectInspector[] inputOIs) throws HiveException {
+    if (state == State.INIT) {
       return;
     }
 
-    LOG.info("Initializing children:");
-    // Copy operators from List to Array for faster access
-    if (childOperatorsArray == null && childOperators != null) {
+    if(!areAllParentsInitialized()) {
+      return;
+    }
+    LOG.info("Initializing Self " + id + " " + getName());
+    
+    if (inputOIs != null) {
+      inputObjInspectors = inputOIs;
+    }
+    
+    // initialize structure to maintain child op info. operator tree changes while
+    // initializing so this need to be done here instead of initialize() method
+    if (childOperators != null) {
       childOperatorsArray = new Operator[childOperators.size()];
       for (int i=0; i<childOperatorsArray.length; i++) {
         childOperatorsArray[i] = childOperators.get(i); 
-        childOperatorsArray[i].allocateParentOperatorsInitArray();
       }
       childOperatorsTag = new int[childOperatorsArray.length];
       for (int i=0; i<childOperatorsArray.length; i++) {
         List<Operator<? extends Serializable>> parentOperators = 
-            childOperatorsArray[i].getParentOperators();
+          childOperatorsArray[i].getParentOperators();
         if (parentOperators == null) {
           throw new HiveException("Hive internal error: parent is null in " 
               + childOperatorsArray[i].getClass() + "!");
@@ -299,47 +278,58 @@ public void initializeChildren (Configuration hconf, Reporter reporter, ObjectIn
       }
     }
     
-    for (int i = 0; i < childOperatorsArray.length; i++) {
-      Operator<? extends Serializable> op = childOperatorsArray[i];
-      op.initialize(hconf, reporter, inputObjInspector == null ? null : inputObjInspector[0], childOperatorsTag[i]);
-    }    
+    if (inputObjInspectors.length == 0) {
+      throw new HiveException("Internal Error during operator initialization.");
+    }
+    // derived classes can set this to different object if needed
+    outputObjInspector = inputObjInspectors[0];
+    initializeOp(hconf);
+    LOG.info("Initialization Done " + id + " " + getName());
   }
 
-  static private class ParentInit {
-    boolean           done;
-    ObjectInspector   parIns;
+  /**
+   * Operator specific initialization.
+   */
+  protected void initializeOp(Configuration hconf) throws HiveException {
+    initializeChildren(hconf);
   }
-  
-  transient protected ParentInit[] parentsObjectInspector = null; 
-
-  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector inputObjInspector, int parentId) throws HiveException {
-    parentsObjectInspector[parentId].parIns = inputObjInspector;
-    parentsObjectInspector[parentId].done = true;
-
-    LOG.info("parent " + parentId + " initialized");
-    
-    // If all the parents have been initialied, go ahead
-    for (ParentInit par : parentsObjectInspector)
-      if (par.done == false)
-        return;
-
-    LOG.info("start Initializing " + id);
-    
-    ObjectInspector[] par = new ObjectInspector[parentsObjectInspector.length];
-    for (int pos = 0; pos < par.length; pos++)
-      par[pos] = parentsObjectInspector[pos].parIns;
-    initialize(hconf, reporter, par);    
-    LOG.info("done Initializing " + id);
+ 
+  /**
+   * Calls initialize on each of the children with outputObjetInspector as the output row format
+   */
+  protected void initializeChildren(Configuration hconf) throws HiveException {
+    state = State.INIT;
+    LOG.info("Operator " + id + " " + getName() + " initialized");
+    if (childOperators == null) {
+      return;
+    }
+    LOG.info("Initializing children of " + id + " " + getName());
+    for (int i = 0; i < childOperatorsArray.length; i++) {
+      childOperatorsArray[i].initialize(hconf, outputObjInspector, childOperatorsTag[i]);
+    }
   }
 
   /**
+   * Collects all the parent's output object inspectors and calls actual initialization method
+   * @param hconf
+   * @param inputOI OI of the row that this parent will pass to this op
+   * @param parentId parent operator id
+   * @throws HiveException
+   */
+  private void initialize(Configuration hconf, ObjectInspector inputOI, int parentId) throws HiveException {
+    LOG.info("Initializing child " + id + " " + getName());
+    inputObjInspectors[parentId] = inputOI;
+    // call the actual operator initialization function
+    initialize(hconf, null);    
+  }
+
+   /**
    * Process the row.
    * @param row  The object representing the row.
-   * @param rowInspector  The inspector for the row object, will be deprecated soon.
    * @param tag  The tag of the row usually means which parent this row comes from.
    *             Rows with the same tag should have exactly the same rowInspector all the time.
    */
-  public abstract void process(Object row, ObjectInspector rowInspector, int tag) throws HiveException;
+  public abstract void process(Object row, int tag) throws HiveException;
  
   // If a operator wants to do some work at the beginning of a group
   public void startGroup() throws HiveException {
@@ -356,8 +346,7 @@ public void startGroup() throws HiveException {
   }  
   
   // If a operator wants to do some work at the beginning of a group
-  public void endGroup() throws HiveException
-  {
+  public void endGroup() throws HiveException {
     LOG.debug("Ending group");
     
     if (childOperators == null)
@@ -371,7 +360,7 @@ public void endGroup() throws HiveException
   }
 
   public void close(boolean abort) throws HiveException {
-    if (state == state.CLOSE) 
+    if (state == State.CLOSE) 
       return;
 
     try {
@@ -411,7 +400,7 @@ public void jobClose(Configuration conf, boolean success) throws HiveException {
    *  Cache childOperators in an array for faster access. childOperatorsArray is accessed
    *  per row, so it's important to make the access efficient.
    */
-  transient protected Operator<? extends Serializable>[] childOperatorsArray;
+  transient protected Operator<? extends Serializable>[] childOperatorsArray = null;
   transient protected int[] childOperatorsTag; 
 
    /**
@@ -457,26 +446,9 @@ protected void forward(Object row, ObjectInspector rowInspector) throws HiveExce
     // For debugging purposes:
     // System.out.println("" + this.getClass() + ": " + SerDeUtils.getJSONString(row, rowInspector));
     // System.out.println("" + this.getClass() + ">> " + ObjectInspectorUtils.getObjectInspectorName(rowInspector));
-    
-    // Copy operators from List to Array for faster access
+ 
     if (childOperatorsArray == null && childOperators != null) {
-      childOperatorsArray = new Operator[childOperators.size()];
-      for (int i=0; i<childOperatorsArray.length; i++) {
-        childOperatorsArray[i] = childOperators.get(i); 
-      }
-      childOperatorsTag = new int[childOperatorsArray.length];
-      for (int i=0; i<childOperatorsArray.length; i++) {
-        List<Operator<? extends Serializable>> parentOperators = 
-            childOperatorsArray[i].getParentOperators();
-        if (parentOperators == null) {
-          throw new HiveException("Hive internal error: parent is null in " 
-              + childOperatorsArray[i].getClass() + "!");
-        }
-        childOperatorsTag[i] = parentOperators.indexOf(this);
-        if (childOperatorsTag[i] == -1) {
-          throw new HiveException("Hive internal error: cannot find parent in the child operator!");
-        }
-      }
+      throw new HiveException("Internal Hive error during operator initialization.");
     }
     
     if((childOperatorsArray == null) || (getDone())) {
@@ -489,7 +461,7 @@ protected void forward(Object row, ObjectInspector rowInspector) throws HiveExce
       if (o.getDone()) {
         childrenDone ++;
       } else {
-        o.process(row, rowInspector, childOperatorsTag[i]);
+        o.process(row, childOperatorsTag[i]);
       }
     }
     
@@ -545,26 +517,45 @@ public void setColumnExprMap(Map<String, exprNodeDesc> colExprMap) {
     this.colExprMap = colExprMap;
   }
   
-  public String dump() {
+  private String getLevelString(int level) {
+    if (level == 0) {
+      return "\n";
+    }
+    StringBuilder s = new StringBuilder();
+    s.append("\n");
+    while(level > 0) {
+      s.append("  ");
+      level--;
+    }
+    return s.toString();
+  }
+  
+  public String dump(int level) {
     StringBuilder s = new StringBuilder();
+    String ls = getLevelString(level);
+    s.append(ls);
     s.append("<" + getName() + ">");
     s.append("Id =" + id);
     if (childOperators != null) {
-      s.append("<Children>");
+      s.append(ls);
+      s.append("  <Children>");
       for (Operator<? extends Serializable> o : childOperators) {
-        s.append(o.dump());
+        s.append(o.dump(level+2));
       }
-      s.append("<\\Children>");
+      s.append(ls);
+      s.append("  <\\Children>");
     }
 
     if (parentOperators != null) {
-      s.append("<Parent>");
+      s.append(ls);
+      s.append("  <Parent>");
       for (Operator<? extends Serializable> o : parentOperators) {
         s.append("Id = " + o.id + " ");
       }
       s.append("<\\Parent>");
     }
 
+    s.append(ls);
     s.append("<\\" + getName() + ">");
     return s.toString();
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
index f011ca20ad..8653804452 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
@@ -29,16 +29,14 @@
 import org.apache.hadoop.hive.ql.plan.reduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.tableDesc;
 import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 
 /**
  * Reduce Sink Operator sends output to the reduce stage
@@ -72,8 +70,7 @@ public class ReduceSinkOperator extends TerminalOperator <reduceSinkDesc> implem
   transient int tag;
   transient byte[] tagByte = new byte[1];
   
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-    LOG.info("Initializing Self");
+  protected void initializeOp(Configuration hconf) throws HiveException {
 
     try {
       keyEval = new ExprNodeEvaluator[conf.getKeyCols().size()];
@@ -108,8 +105,7 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
       valueSerializer.initialize(null, valueTableDesc.getProperties());
       
       firstRow = true;
-      initializeChildren(hconf, reporter, inputObjInspector);
-      LOG.info("Initialization Done");
+      initializeChildren(hconf);
     } catch (Exception e) {
       e.printStackTrace();
       throw new RuntimeException(e);
@@ -130,9 +126,9 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
   boolean firstRow;
   
   transient Random random;
-  
-  public void process(Object row, ObjectInspector rowInspector, int tag) throws HiveException {
+  public void process(Object row, int tag) throws HiveException {
     try {
+      ObjectInspector rowInspector = inputObjInspectors[tag];
       if (firstRow) {
         firstRow = false;
         keyObjectInspector = initEvaluatorsAndReturnStruct(keyEval, conf.getOutputKeyColumnNames(), rowInspector);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
index 13d1aa146c..2c54221a71 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
@@ -18,24 +18,34 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.util.*;
-import java.io.*;
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Timer;
+import java.util.TimerTask;
 
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.plan.scriptDesc;
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.plan.scriptDesc;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.SerDe;
 import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.LineRecordReader.LineReader;
 import org.apache.hadoop.util.StringUtils;
-import org.apache.hadoop.fs.FileUtil;
 
 
 public class ScriptOperator extends Operator<scriptDesc> implements Serializable {
@@ -166,7 +176,7 @@ public File getAbsolutePath(String filename)
     }
   }
 
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+  protected void initializeOp(Configuration hconf) throws HiveException {
 
     statsMap.put(Counter.DESERIALIZE_ERRORS, deserialize_error_count);
     statsMap.put(Counter.SERIALIZE_ERRORS, serialize_error_count);
@@ -180,7 +190,7 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
       scriptInputSerializer = (Serializer)conf.getScriptInputInfo().getDeserializerClass().newInstance();
       scriptInputSerializer.initialize(hconf, conf.getScriptInputInfo().getProperties());
 
-      initializeChildren(hconf, reporter, new ObjectInspector[]{scriptOutputDeserializer.getObjectInspector()});
+      outputObjInspector = scriptOutputDeserializer.getObjectInspector();
 
       String [] cmdArgs = splitArgs(conf.getScriptCmd());
 
@@ -214,12 +224,10 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
       scriptErr = new DataInputStream(new BufferedInputStream(scriptPid.getErrorStream()));
       outThread = new StreamThread(scriptIn, new OutputStreamProcessor(
           scriptOutputDeserializer.getObjectInspector()), "OutputProcessor");
-      outThread.start();
       errThread = new StreamThread(scriptErr,
                                    new ErrorStreamProcessor
                                    (HiveConf.getIntVar(hconf, HiveConf.ConfVars.SCRIPTERRORLIMIT)),
                                    "ErrorProcessor");
-      errThread.start();
       
       /* Timer that reports every 5 minutes to the jobtracker. This ensures that even if
          the user script is not returning rows for greater than that duration, a progress
@@ -236,19 +244,24 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
 
       rpTimer = new Timer(true);
       rpTimer.scheduleAtFixedRate(new ReporterTask(reporter), 0, exp_interval);
+
+      // initialize all children before starting the script
+      initializeChildren(hconf);
+      outThread.start();
+      errThread.start();
     } catch (Exception e) {
-      e.printStackTrace();
       throw new HiveException ("Cannot initialize ScriptOperator", e);
     }
   }
 
   Text text = new Text();
-  public void process(Object row, ObjectInspector rowInspector, int tag) throws HiveException {
+  public void process(Object row, int tag) throws HiveException {
+
     if(scriptError != null) {
       throw new HiveException(scriptError);
     }
     try {
-      text = (Text) scriptInputSerializer.serialize(row, rowInspector);
+      text = (Text) scriptInputSerializer.serialize(row, inputObjInspectors[tag]);
       scriptOut.write(text.getBytes(), 0, text.getLength());
       scriptOut.write(Utilities.newLineCode);
     } catch (SerDeException e) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
index a5db74c69b..6a3b8432ed 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
@@ -25,9 +25,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.selectDesc;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.mapred.Reporter;
 
 /**
  * Select operator implementation
@@ -38,12 +36,11 @@ public class SelectOperator extends Operator <selectDesc> implements Serializabl
   transient protected ExprNodeEvaluator[] eval;
 
   transient Object[] output;
-  transient ObjectInspector outputObjectInspector;
   
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+  protected void initializeOp(Configuration hconf) throws HiveException {
     // Just forward the row as is
     if (conf.isSelStarNoCompute()) {
-      initializeChildren(hconf, reporter, inputObjInspector);
+    	initializeChildren(hconf);
       return;
     }
     
@@ -54,20 +51,19 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
       eval[i] = ExprNodeEvaluatorFactory.get(colList.get(i));
     }
    
-    assert inputObjInspector.length == 1;
     output = new Object[eval.length];
-    LOG.info("SELECT " + ((StructObjectInspector)inputObjInspector[0]).getTypeName());
-    outputObjectInspector = initEvaluatorsAndReturnStruct(eval, conf
-          .getOutputColumnNames(), inputObjInspector[0]);
-    initializeChildren(hconf, reporter, new ObjectInspector[]{outputObjectInspector});
+    LOG.info("SELECT " + ((StructObjectInspector)inputObjInspectors[0]).getTypeName());
+    outputObjInspector = initEvaluatorsAndReturnStruct(eval, conf
+          .getOutputColumnNames(), inputObjInspectors[0]);
+    initializeChildren(hconf);
   }
 
-  public void process(Object row, ObjectInspector rowInspector, int tag)
+  public void process(Object row, int tag)
       throws HiveException {
 
     // Just forward the row as is
     if (conf.isSelStarNoCompute()) {
-      forward(row, rowInspector);
+      forward(row, inputObjInspectors[tag]);
       return;
     }
     
@@ -81,7 +77,7 @@ public void process(Object row, ObjectInspector rowInspector, int tag)
             + conf.getColList().get(i).getExprString(), e);
       }
     }
-    forward(output, outputObjectInspector);
+    forward(output, outputObjInspector);
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
index cb6102d521..84b8c3be63 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
@@ -18,13 +18,10 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
+import java.io.Serializable;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.tableScanDesc;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.conf.Configuration;
 
 /**
  * Table Scan Operator
@@ -33,10 +30,6 @@
  **/
 public class TableScanOperator extends Operator<tableScanDesc> implements Serializable {
   private static final long serialVersionUID = 1L;
-  public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
-    initializeChildren(hconf, reporter, inputObjInspector);
-    // nothing to do really ..
-  }
 
   /**
    * Currently, the table scan operator does not do anything special other than just forwarding the row. Since the 
@@ -44,9 +37,9 @@ public void initializeOp(Configuration hconf, Reporter reporter, ObjectInspector
    * i.e table data is not only read by the mapper, this operator will be enhanced to read the table.
    **/
   @Override
-  public void process(Object row, ObjectInspector rowInspector, int tag)
+  public void process(Object row, int tag)
       throws HiveException {
-    forward(row, rowInspector);    
+    forward(row, inputObjInspectors[tag]);    
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java
index 972d015317..0bd1aade65 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java
@@ -18,10 +18,11 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import java.io.*;
+import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.List;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.unionDesc;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ReturnObjectInspectorResolver;
@@ -29,8 +30,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.conf.Configuration;
 
 /**
  * Union Operator
@@ -46,8 +45,6 @@ public class UnionOperator extends  Operator<unionDesc>  implements Serializable
   boolean[] needsTransform;
   
   ArrayList<Object> outputRow;
-  ObjectInspector outputOI;
-
 
   /** UnionOperator will transform the input rows if the inputObjInspectors
    *  from different parents are different.
@@ -55,15 +52,13 @@ public class UnionOperator extends  Operator<unionDesc>  implements Serializable
    *  ObjectInspector, then we don't need to do transformation for that parent.
    *  This information is recorded in needsTransform[].
    */
-  @Override
-  public void initializeOp(Configuration hconf, Reporter reporter,
-      ObjectInspector[] inputObjInspector) throws HiveException {
+  protected void initializeOp(Configuration hconf) throws HiveException {
     
-    int parents = inputObjInspector.length;
+    int parents = parentOperators.size();
     parentObjInspectors = new StructObjectInspector[parents];
     parentFields = new List[parents];
     for (int p = 0; p < parents; p++) {
-      parentObjInspectors[p] = (StructObjectInspector)inputObjInspector[p];
+      parentObjInspectors[p] = (StructObjectInspector)inputObjInspectors[p];
       parentFields[p] = parentObjInspectors[p].getAllStructFieldRefs();
     }
     
@@ -93,7 +88,7 @@ public void initializeOp(Configuration hconf, Reporter reporter,
     }
     
     // create output row ObjectInspector
-    outputOI = ObjectInspectorFactory.getStandardStructObjectInspector(
+    outputObjInspector = ObjectInspectorFactory.getStandardStructObjectInspector(
         columnNames, outputFieldOIs);
     outputRow = new ArrayList<Object>(columns);
     for (int c = 0; c < columns; c++) {
@@ -105,34 +100,31 @@ public void initializeOp(Configuration hconf, Reporter reporter,
     for (int p = 0; p < parents; p++) {
       // Testing using != is good enough, because we use ObjectInspectorFactory to
       // create ObjectInspectors.
-      needsTransform[p] = (inputObjInspector[p] != outputOI);
+      needsTransform[p] = (inputObjInspectors[p] != outputObjInspector);
       if (needsTransform[p]) {
         LOG.info("Union Operator needs to transform row from parent[" + p + "] from "
-            + inputObjInspector[p] + " to " + outputOI);
+            + inputObjInspectors[p] + " to " + outputObjInspector);
       }
     }
-
-    // initialize the children
-    initializeChildren(hconf, reporter, new ObjectInspector[] {outputOI});
+    initializeChildren(hconf);
   }
   
   @Override
-  public void process(Object row, ObjectInspector rowInspector, int tag)
-      throws HiveException {
+  public void process(Object row, int tag) throws HiveException {
 
-    if (needsTransform[tag]) {
-      StructObjectInspector soi = parentObjInspectors[tag];
+    StructObjectInspector soi = parentObjInspectors[tag];
       List<? extends StructField> fields = parentFields[tag];
-    
-      for (int c = 0; c < fields.size(); c++) {
-        outputRow.set(c, columnTypeResolvers[c].convertIfNecessary(
-            soi.getStructFieldData(row, fields.get(c)),
-            fields.get(c).getFieldObjectInspector()));
+
+      if (needsTransform[tag]) {
+        for (int c = 0; c < fields.size(); c++) {
+          outputRow.set(c, columnTypeResolvers[c].convertIfNecessary(
+              soi.getStructFieldData(row, fields.get(c)),
+              fields.get(c).getFieldObjectInspector()));
+        }
+        forward(outputRow, outputObjInspector);
+      } else {
+        forward(row, inputObjInspectors[tag]);
       }
-      forward(outputRow, outputOI);
-    } else {
-      forward(row, rowInspector);
-    }
   }
 
   /**
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
index 3a8b11b746..7e7ba91d3b 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
@@ -18,21 +18,26 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import junit.framework.TestCase;
-import java.io.*;
-import java.util.*;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.Map;
 
+import junit.framework.TestCase;
 
-import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
-import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
-import org.apache.hadoop.hive.ql.plan.*;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
+import org.apache.hadoop.hive.ql.plan.PlanUtils;
+import org.apache.hadoop.hive.ql.plan.collectDesc;
+import org.apache.hadoop.hive.ql.plan.exprNodeConstantDesc;
+import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.filterDesc;
+import org.apache.hadoop.hive.ql.plan.mapredWork;
+import org.apache.hadoop.hive.ql.plan.partitionDesc;
+import org.apache.hadoop.hive.ql.plan.scriptDesc;
+import org.apache.hadoop.hive.ql.plan.selectDesc;
+import org.apache.hadoop.hive.ql.plan.tableDesc;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -40,6 +45,9 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.JobConf;
 
 public class TestOperators extends TestCase {
 
@@ -89,10 +97,10 @@ public void testBaseFilterOperator() throws Throwable {
       op.setConf(filterCtx);
 
       // runtime initialization
-      op.initialize(new JobConf(TestOperators.class), null, new ObjectInspector[]{r[0].oi});
+      op.initialize(new JobConf(TestOperators.class), new ObjectInspector[]{r[0].oi});
 
       for(InspectableObject oner: r) {
-        op.process(oner.o, oner.oi, 0);
+        op.process(oner.o, 0);
       }
 
       Map<Enum<?>, Long> results = op.getStats();
@@ -121,7 +129,6 @@ public void testFileSinkOperator() throws Throwable {
       exprNodeDesc exprDesc1 = TestExecDriver.getStringColumn("col1");
 
       // col2
-      ArrayList<exprNodeDesc> exprDesc2children = new ArrayList<exprNodeDesc>();
       exprNodeDesc expr1 = TestExecDriver.getStringColumn("col0");
       exprNodeDesc expr2 = new exprNodeConstantDesc("1");
       exprNodeDesc exprDesc2 = TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("concat", expr1, expr2);
@@ -138,15 +145,15 @@ public void testFileSinkOperator() throws Throwable {
       op.setConf(selectCtx);
 
       // fileSinkOperator to dump the output of the select
-      fileSinkDesc fsd = new fileSinkDesc ("file:///tmp" + File.separator + System.getProperty("user.name") + File.separator + "TestFileSinkOperator",
-                                           Utilities.defaultTd, false);
-      Operator<fileSinkDesc> flop = OperatorFactory.getAndMakeChild(fsd, op);
+      //fileSinkDesc fsd = new fileSinkDesc ("file:///tmp" + File.separator + System.getProperty("user.name") + File.separator + "TestFileSinkOperator",
+      //                                     Utilities.defaultTd, false);
+      //Operator<fileSinkDesc> flop = OperatorFactory.getAndMakeChild(fsd, op);
       
-      op.initialize(new JobConf(TestOperators.class), Reporter.NULL, new ObjectInspector[]{r[0].oi});
+      op.initialize(new JobConf(TestOperators.class), new ObjectInspector[]{r[0].oi});
 
       // evaluate on row
       for(int i=0; i<5; i++) {
-        op.process(r[i].o, r[i].oi, 0);
+        op.process(r[i].o, 0);
       }
       op.close(false);
 
@@ -191,11 +198,11 @@ public void testScriptOperator() throws Throwable {
       collectDesc cd = new collectDesc (Integer.valueOf(10));
       CollectOperator cdop = (CollectOperator) OperatorFactory.getAndMakeChild(cd, sop);
 
-      op.initialize(new JobConf(TestOperators.class), null, new ObjectInspector[]{r[0].oi});
+      op.initialize(new JobConf(TestOperators.class), new ObjectInspector[]{r[0].oi});
 
       // evaluate on row
       for(int i=0; i<5; i++) {
-        op.process(r[i].o, r[i].oi, 0);
+        op.process(r[i].o, 0);
       }
       op.close(false);
 
@@ -262,8 +269,7 @@ public void testMapOperator() throws Throwable {
 
       // get map operator and initialize it
       MapOperator mo = new MapOperator();
-      mo.setConf(mrwork);
-      mo.initialize(hconf, null, null);
+      mo.initializeAsRoot(hconf, mrwork);
 
       Text tw = new Text();
       InspectableObject io1 = new InspectableObject();
