diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 090854b30b..f3878ef973 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -4987,7 +4987,8 @@ public static enum ConfVars {
         "comma separated list of plugin can be used:\n"
             + "  overlay: hiveconf subtree 'reexec.overlay' is used as an overlay in case of an execution errors out\n"
             + "  reoptimize: collects operator statistics during execution and recompile the query after a failure\n"
-            + "  reexecute_lost_am: reexecutes query if it failed due to tez am node gets decommissioned"),
+            + "  reexecute_lost_am: reexecutes query if it failed due to tez am node gets decommissioned\n"
+            + "  The retrylock strategy is always enabled: recompiles the query if snapshot becomes outdated before lock acquisition"),
     HIVE_QUERY_REEXECUTION_STATS_PERSISTENCE("hive.query.reexecution.stats.persist.scope", "metastore",
         new StringSet("query", "hiveserver", "metastore"),
         "Sets the persistence scope of runtime statistics\n"
@@ -4997,7 +4998,11 @@ public static enum ConfVars {
 
 
     HIVE_QUERY_MAX_REEXECUTION_COUNT("hive.query.reexecution.max.count", 1,
-        "Maximum number of re-executions for a single query."),
+        "Maximum number of re-executions for a single query."
+            + " The maximum re-execution retry is limited at 10"),
+    HIVE_QUERY_MAX_REEXECUTION_RETRYLOCK_COUNT("hive.query.reexecution.retrylock.max.count", 5,
+        "Maximum number of re-executions with retrylock strategy for a single query."
+            + " The maximum re-execution retry is limited at 10"),
     HIVE_QUERY_REEXECUTION_ALWAYS_COLLECT_OPERATOR_STATS("hive.query.reexecution.always.collect.operator.stats", false,
         "If sessionstats are enabled; this option can be used to collect statistics all the time"),
     HIVE_QUERY_REEXECUTION_STATS_CACHE_BATCH_SIZE("hive.query.reexecution.stats.cache.batch.size", -1,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 9fab7c5a49..9fbd53794c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -89,6 +89,9 @@ public class Driver implements IDriver {
   private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);
   private static final LogHelper CONSOLE = new LogHelper(LOG);
   private static final int SHUTDOWN_HOOK_PRIORITY = 0;
+  // Exception message that ReExecutionRetryLockPlugin will recognize
+  public static final String SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED =
+      "snapshot was outdated when locks were acquired";
   private Runnable shutdownRunner = null;
 
   private int maxRows = 100;
@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command
 
       try {
         if (!validTxnManager.isValidTxnListState()) {
-          LOG.info("Compiling after acquiring locks");
+          LOG.warn("Reexecuting after acquiring locks, since snapshot was outdated.");
           // Snapshot was outdated when locks were acquired, hence regenerate context,
-          // txn list and retry
-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.
-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,
-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.
-          // But if snapshot is not valid, we recompile the query.
-          if (driverContext.isOutdatedTxn()) {
-            driverContext.getTxnManager().rollbackTxn();
-
-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);
-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());
-            lockAndRespond();
-          }
-          driverContext.setRetrial(true);
-          driverContext.getBackupContext().addSubContext(context);
-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());
-          context = driverContext.getBackupContext();
-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,
-            driverContext.getTxnManager().getValidTxns().toString());
-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {
-            validTxnManager.recordValidWriteIds();
-          }
-
-          if (!alreadyCompiled) {
-            // compile internal will automatically reset the perf logger
-            compileInternal(command, true);
-          } else {
-            // Since we're reusing the compiled plan, we need to update its start time for current run
-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());
-          }
-
-          if (!validTxnManager.isValidTxnListState()) {
-            // Throw exception
-            throw handleHiveException(new HiveException("Operation could not be executed"), 14);
+          // txn list and retry (see ReExecutionRetryLockPlugin)
+          try {
+            releaseLocksAndCommitOrRollback(false);
+          } catch (LockException e) {
+            handleHiveException(e, 12);
           }
-
-          //Reset the PerfLogger
-          perfLogger = SessionState.getPerfLogger(true);
-
-          // the reason that we set the txn manager for the cxt here is because each
-          // query has its own ctx object. The txn mgr is shared across the
-          // same instance of Driver, which can run multiple queries.
-          context.setHiveTxnManager(driverContext.getTxnManager());
+          throw handleHiveException(
+              new HiveException(
+                  "Operation could not be executed, " + SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED + "."),
+              14);
         }
       } catch (LockException e) {
         throw handleHiveException(e, 13);
@@ -787,8 +758,6 @@ else if(driverContext.getPlan().getOperation() == HiveOperation.ROLLBACK) {
   }
 
   private void rollback(CommandProcessorException cpe) throws CommandProcessorException {
-
-    //console.printError(cpr.toString());
     try {
       releaseLocksAndCommitOrRollback(false);
     } catch (LockException e) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java b/ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java
index 0afa657ccb..a8c83fc504 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java
@@ -59,7 +59,6 @@ public class DriverContext {
   // either initTxnMgr or from the SessionState, in that order.
   private HiveTxnManager txnManager;
   private TxnType txnType = TxnType.DEFAULT;
-  private boolean outdatedTxn;
   private StatsSource statsSource;
 
   // Boolean to store information about whether valid txn list was generated
@@ -156,14 +155,6 @@ public TxnType getTxnType() {
     return txnType;
   }
 
-  public void setOutdatedTxn(boolean outdated) {
-    this.outdatedTxn = outdated;
-  }
-
-  public boolean isOutdatedTxn() {
-    return outdatedTxn;
-  }
-
   public void setTxnType(TxnType txnType) {
     this.txnType = txnType;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java
index 87bb34d2a9..78ace0bfe3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.reexec.IReExecutionPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecDriver;
+import org.apache.hadoop.hive.ql.reexec.ReExecutionRetryLockPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecuteLostAMQueryPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecutionOverlayPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReOptimizePlugin;
@@ -54,6 +55,8 @@ public static IDriver newDriver(QueryState queryState, QueryInfo queryInfo) {
       }
       plugins.add(buildReExecPlugin(string));
     }
+    // The retrylock plugin is always enabled
+    plugins.add(new ReExecutionRetryLockPlugin());
 
     return new ReExecDriver(queryState, queryInfo, plugins);
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ValidTxnManager.java b/ql/src/java/org/apache/hadoop/hive/ql/ValidTxnManager.java
index e5f8ce005f..5fac442b72 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ValidTxnManager.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ValidTxnManager.java
@@ -158,10 +158,8 @@ private boolean checkWriteIds(String currentTxnString, Set<String> nonSharedLock
           ValidWriteIdList writeIdList = txnWriteIdList.getTableValidWriteIdList(tableInfo.getKey());
           ValidWriteIdList currentWriteIdList = currentTxnWriteIds.getTableValidWriteIdList(tableInfo.getKey());
           // Check if there was a conflicting write between current SNAPSHOT generation and locking.
-          // If yes, mark current transaction as outdated.
           if (currentWriteIdList.isWriteIdRangeValid(writeIdList.getHighWatermark() + 1,
               currentWriteIdList.getHighWatermark()) != ValidWriteIdList.RangeResponse.NONE) {
-            driverContext.setOutdatedTxn(true);
             return false;
           }
           // Check that write id is still valid
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/IReExecutionPlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/IReExecutionPlugin.java
index be62fc0075..c6d848c2f0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/IReExecutionPlugin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/IReExecutionPlugin.java
@@ -22,6 +22,7 @@
 import org.apache.hadoop.hive.common.classification.InterfaceStability;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 
 /**
  * Defines an interface for re-execution logics.
@@ -47,7 +48,7 @@ public interface IReExecutionPlugin {
   /**
    * The query have failed, does this plugin advises to re-execute it again?
    */
-  boolean shouldReExecute(int executionNum);
+  boolean shouldReExecute(int executionNum, CommandProcessorException ex);
 
   /**
    * The plugin should prepare for the re-compilaton of the query.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java
index c307085366..42dac46960 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java
@@ -57,6 +57,10 @@
  */
 public class ReExecDriver implements IDriver {
 
+  // Every plugin should check for execution limit in shouldReexecute
+  // But just in case, we don't want an infinite loop
+  private final static int MAX_EXECUTION = 10;
+
   private class HandleReOptimizationExplain implements HiveSemanticAnalyzerHook {
 
     @Override
@@ -148,8 +152,6 @@ public void setOperationId(String operationId) {
   @Override
   public CommandProcessorResponse run() throws CommandProcessorException {
     executionIndex = 0;
-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);
-
 
     while (true) {
       executionIndex++;
@@ -170,9 +172,9 @@ public CommandProcessorResponse run() throws CommandProcessorException {
       afterExecute(oldPlanMapper, cpr != null);
 
       boolean shouldReExecute = explainReOptimization && executionIndex==1;
-      shouldReExecute |= cpr == null && shouldReExecute();
+      shouldReExecute |= cpr == null && shouldReExecute(cpe);
 
-      if (executionIndex >= maxExecutuions || !shouldReExecute) {
+      if (executionIndex >= MAX_EXECUTION || !shouldReExecute) {
         if (cpr != null) {
           return cpr;
         } else {
@@ -214,10 +216,10 @@ private boolean shouldReExecuteAfterCompile(PlanMapper oldPlanMapper, PlanMapper
     return ret;
   }
 
-  private boolean shouldReExecute() {
+  private boolean shouldReExecute(CommandProcessorException ex) {
     boolean ret = false;
     for (IReExecutionPlugin p : plugins) {
-      boolean shouldReExecute = p.shouldReExecute(executionIndex);
+      boolean shouldReExecute = p.shouldReExecute(executionIndex, ex);
       LOG.debug("{}.shouldReExecute = {}", p, shouldReExecute);
       ret |= shouldReExecute;
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecuteLostAMQueryPlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecuteLostAMQueryPlugin.java
index 5467cd8003..da4b470b9b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecuteLostAMQueryPlugin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecuteLostAMQueryPlugin.java
@@ -18,15 +18,18 @@
 
 package org.apache.hadoop.hive.ql.reexec;
 
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;
 import org.apache.hadoop.hive.ql.hooks.HookContext;
 import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 
 import java.util.regex.Pattern;
 
 public class ReExecuteLostAMQueryPlugin implements IReExecutionPlugin {
   private boolean retryPossible;
+  private int maxExecutions = 1;
 
   // Lost am container have exit code -100, due to node failures.
   private Pattern lostAMContainerErrorPattern = Pattern.compile(".*AM Container for .* exited .* exitCode: -100.*");
@@ -49,6 +52,7 @@ public void run(HookContext hookContext) throws Exception {
   @Override
   public void initialize(Driver driver) {
     driver.getHookRunner().addOnFailureHook(new LocalHook());
+    maxExecutions = 1 + driver.getConf().getIntVar(HiveConf.ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);
   }
 
   @Override
@@ -56,8 +60,8 @@ public void beforeExecute(int executionIndex, boolean explainReOptimization) {
   }
 
   @Override
-  public boolean shouldReExecute(int executionNum) {
-    return retryPossible;
+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {
+    return (executionNum < maxExecutions) && retryPossible;
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java
index 83df334931..84b5960057 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.ql.hooks.HookContext;
 import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
 import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 import org.apache.tez.dag.api.TezConfiguration;
 
 /**
@@ -74,7 +75,8 @@ public void prepareToReExecute() {
   }
 
   @Override
-  public boolean shouldReExecute(int executionNum) {
+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {
+
     return executionNum == 1 && !subtree.isEmpty() && retryPossible;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java
new file mode 100644
index 0000000000..6366c4ed93
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.reexec;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
+
+public class ReExecutionRetryLockPlugin implements IReExecutionPlugin {
+
+  private Driver coreDriver;
+  private int maxRetryLockExecutions = 1;
+
+  @Override
+  public void initialize(Driver driver) {
+    coreDriver = driver;
+    maxRetryLockExecutions = 1 + coreDriver.getConf().getIntVar(HiveConf.ConfVars.HIVE_QUERY_MAX_REEXECUTION_RETRYLOCK_COUNT);
+  }
+
+  @Override
+  public void beforeExecute(int executionIndex, boolean explainReOptimization) {
+  }
+
+  @Override
+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {
+    return executionNum < maxRetryLockExecutions && ex != null &&
+        ex.getMessage().contains(Driver.SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED);
+  }
+
+  @Override
+  public void prepareToReExecute() {
+  }
+
+  @Override
+  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {
+    return executionNum < maxRetryLockExecutions;
+  }
+
+  @Override
+  public void afterExecute(PlanMapper planMapper, boolean successfull) {
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReOptimizePlugin.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReOptimizePlugin.java
index 09045af4bc..ecef30308f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReOptimizePlugin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReOptimizePlugin.java
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
 import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;
 import org.apache.hadoop.hive.ql.plan.mapper.StatsSources;
+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 import org.apache.hadoop.hive.ql.stats.OperatorStatsReaderHook;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -49,6 +50,8 @@ public class ReOptimizePlugin implements IReExecutionPlugin {
 
   private boolean alwaysCollectStats;
 
+  private int maxExecutions = 1;
+
   class LocalHook implements ExecuteWithHookContext {
 
     @Override
@@ -80,14 +83,15 @@ public void initialize(Driver driver) {
     coreDriver.getHookRunner().addOnFailureHook(statsReaderHook);
     coreDriver.getHookRunner().addPostHook(statsReaderHook);
     alwaysCollectStats = driver.getConf().getBoolVar(ConfVars.HIVE_QUERY_REEXECUTION_ALWAYS_COLLECT_OPERATOR_STATS);
+    maxExecutions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);
     statsReaderHook.setCollectOnSuccess(alwaysCollectStats);
 
     coreDriver.setStatsSource(StatsSources.getStatsSource(driver.getConf()));
   }
 
   @Override
-  public boolean shouldReExecute(int executionNum) {
-    return retryPossible;
+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {
+    return (executionNum < maxExecutions) && retryPossible;
   }
 
   @Override
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java
index c7b41862ec..5bd456abb1 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java
@@ -276,8 +276,7 @@ public void run() {
       } catch (HiveException e) {
         throw new RuntimeException(e);
       }
-      QueryState qs = new QueryState.Builder().withHiveConf(hiveConf).nonIsolated().build();
-      try (Driver d = new Driver(qs)) {
+      try (IDriver d = DriverFactory.newDriver(hiveConf)) {
         LOG.info("Ready to run the query: " + query);
         syncThreadStart(cdlIn, cdlOut);
         try {
@@ -427,7 +426,12 @@ public void testParallelTruncateAnalyzeStats() throws Exception {
     stats = getTxnTableStats(msClient, tableName);
     boolean hasStats = 0 != stats.size();
     if (hasStats) {
-      verifyLongStats(0, 0, 0, stats);
+      // Either the truncate run before or the analyze
+      if (stats.get(0).getStatsData().getLongStats().getNumDVs() > 0) {
+        verifyLongStats(1, 0, 0, stats);
+      } else {
+        verifyLongStats(0, 0, 0, stats);
+      }
     }
 
     // Stats should be valid after analyze.
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/DbTxnManagerEndToEndTestBase.java b/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/DbTxnManagerEndToEndTestBase.java
index a5c783101e..1979a1974c 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/DbTxnManagerEndToEndTestBase.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/DbTxnManagerEndToEndTestBase.java
@@ -23,6 +23,8 @@
 import org.apache.hadoop.hive.metastore.txn.TxnUtils;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.DriverFactory;
+import org.apache.hadoop.hive.ql.IDriver;
 import org.apache.hadoop.hive.ql.QueryState;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.junit.After;
@@ -38,7 +40,8 @@ public abstract class DbTxnManagerEndToEndTestBase {
   protected static HiveConf conf = new HiveConf(Driver.class);
   protected HiveTxnManager txnMgr;
   protected Context ctx;
-  protected Driver driver, driver2;
+  protected Driver driver;
+  protected IDriver driver2;
   protected TxnStore txnHandler;
 
   public DbTxnManagerEndToEndTestBase() {
@@ -57,7 +60,7 @@ public void setUp() throws Exception {
     SessionState.start(conf);
     ctx = new Context(conf);
     driver = new Driver(new QueryState.Builder().withHiveConf(conf).nonIsolated().build());
-    driver2 = new Driver(new QueryState.Builder().withHiveConf(conf).build());
+    driver2 = DriverFactory.newDriver(conf);
     conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);
     TxnDbUtil.cleanDb(conf);
     SessionState ss = SessionState.get();
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java b/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java
index ba2e11ba75..32f99321fa 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java
@@ -32,7 +32,10 @@
 import org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.txn.AcidHouseKeeperService;
+import org.apache.hadoop.hive.ql.DriverFactory;
+import org.apache.hadoop.hive.ql.IDriver;
 import org.apache.hadoop.hive.ql.TestTxnCommands2;
+import org.apache.hadoop.hive.ql.reexec.ReExecDriver;
 import org.junit.Assert;
 import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -845,6 +848,8 @@ public void testLockingOnInsertOverwriteNonNativeTables() throws Exception {
     List<ShowLocksResponseElement> locks = getLocks(txnMgr);
     Assert.assertEquals("Unexpected lock count", 1, locks.size());
     checkLock(LockType.EXCLUSIVE, LockState.ACQUIRED, "default", "tab_not_acid", null, locks);
+    txnMgr.rollbackTxn();
+    dropTable(new String[] {"tab_not_acid"});
   }
 
   /** The list is small, and the object is generated, so we don't use sets/equals/etc. */
@@ -1751,6 +1756,7 @@ public void testMultiInsertOnDynamicallyPartitionedMmTable() throws Exception {
     Assert.assertEquals(completedTxnComponentsContents,
         2, TxnDbUtil.countQueryAgent(conf, "select count(*) from \"COMPLETED_TXN_COMPONENTS\" where \"CTC_TXNID\"=6 "
             + "and \"CTC_TABLE\"='tabmmdp' and \"CTC_UPDATE_DELETE\"='N'"));
+    dropTable(new String[] {"tabMmDp", "tab_not_acid"});
   }
 
   private List<ShowLocksResponseElement> getLocksWithFilterOptions(HiveTxnManager txnMgr,
@@ -2048,6 +2054,7 @@ private void testMerge3Way(boolean causeConflict, boolean sharedWrite) throws Ex
           TxnDbUtil.countQueryAgent(conf, "select count(*) from \"WRITE_SET\" where \"WS_TXNID\"=" + txnId2 +
               " and \"WS_OPERATION_TYPE\"='d'"));
     }
+    dropTable(new String[]{"target", "source", "source2"});
   }
 
   @Test
@@ -2242,26 +2249,28 @@ private void testConcurrentMergeInsertSnapshotInvalidate(String query, boolean s
     driver.run("create table source (a int, b int)");
     driver.run("insert into source values (5,6), (7,8)");
 
-    driver.compileAndRespond("merge into target t using source s on t.a = s.a " +
-      "when not matched then insert values (s.a, s.b)");
-
     DbTxnManager txnMgr2 = (DbTxnManager) TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf);
     swapTxnManager(txnMgr2);
-    driver2.run(query);
-    driver2.run("select * from target");
+    driver2.compileAndRespond("merge into target t using source s on t.a = s.a " +
+      "when not matched then insert values (s.a, s.b)");
 
     swapTxnManager(txnMgr);
+    driver.run(query);
+    driver.run("select * from target");
+
+    swapTxnManager(txnMgr2);
     try {
-      driver.run();
+      driver2.run();
     } catch (Exception ex ){
       Assert.assertTrue(ex.getCause().getMessage().contains("due to a write conflict"));
     }
 
-    swapTxnManager(txnMgr2);
-    driver2.run("select * from target");
+    swapTxnManager(txnMgr);
+    driver.run("select * from target");
     List res = new ArrayList();
-    driver2.getFetchTask().fetch(res);
+    driver.getFetchTask().fetch(res);
     Assert.assertEquals("Duplicate records found", 4, res.size());
+    dropTable(new String[]{"target", "source"});
   }
 
   @Test
@@ -2313,6 +2322,142 @@ private void testConcurrentMergeInsertNoDuplicates(String query, boolean sharedW
     List res = new ArrayList();
     driver.getFetchTask().fetch(res);
     Assert.assertEquals("Duplicate records found", 4, res.size());
+    dropTable(new String[]{"target", "source"});
+  }
+
+  /**
+   * ValidTxnManager.isValidTxnListState can invalidate a snapshot if a relevant write transaction was committed
+   * between a query compilation and lock acquisition. When this happens we have to recompile the given query,
+   * otherwise we can miss reading partitions created between. The following three cases test these scenarios.
+   * @throws Exception ex
+   */
+  @Test
+  public void testMergeInsertDynamicPartitioningSequential() throws Exception {
+
+    dropTable(new String[]{"target", "source"});
+    conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);
+
+    // Create partition c=1
+    driver.run("create table target (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')");
+    driver.run("insert into target values (1,1,1), (2,2,1)");
+    //Create partition c=2
+    driver.run("create table source (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')");
+    driver.run("insert into source values (3,3,2), (4,4,2)");
+
+    // txn 1 inserts data to an old and a new partition
+    driver.run("insert into source values (5,5,2), (6,6,3)");
+
+    // txn 2 inserts into the target table into a new partition ( and a duplicate considering the source table)
+    driver.run("insert into target values (3, 3, 2)");
+
+    // txn3 merge
+    driver.run("merge into target t using source s on t.a = s.a " +
+        "when not matched then insert values (s.a, s.b, s.c)");
+    driver.run("select * from target");
+    List res = new ArrayList();
+    driver.getFetchTask().fetch(res);
+    // The merge should see all three partition and not create duplicates
+    Assert.assertEquals("Duplicate records found", 6, res.size());
+    Assert.assertTrue("Partition 3 was skipped", res.contains("6\t6\t3"));
+    dropTable(new String[]{"target", "source"});
+  }
+
+  @Test
+  public void testMergeInsertDynamicPartitioningSnapshotInvalidatedWithOldCommit() throws Exception {
+
+    // By creating the driver with the factory, we should have a ReExecDriver
+    IDriver driver3 = DriverFactory.newDriver(conf);
+    Assert.assertTrue("ReExecDriver was expected", driver3 instanceof ReExecDriver);
+
+    dropTable(new String[]{"target", "source"});
+    conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);
+
+    // Create partition c=1
+    driver.run("create table target (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')");
+    driver.run("insert into target values (1,1,1), (2,2,1)");
+    //Create partition c=2
+    driver.run("create table source (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')");
+    driver.run("insert into source values (3,3,2), (4,4,2)");
+
+    // txn 1 insert data to an old and a new partition
+    driver.compileAndRespond("insert into source values (5,5,2), (6,6,3)");
+
+    DbTxnManager txnMgr2 = (DbTxnManager) TxnManagerFactory.getTxnManagerFactory().getTxnManager(new HiveConf(conf));
+    swapTxnManager(txnMgr2);
+
+    // txn 2 insert into the target table into a new partition ( and a duplicate considering the source table)
+    driver2.compileAndRespond("insert into target values (3, 3, 2)");
+
+    DbTxnManager txnMgr3 = (DbTxnManager) TxnManagerFactory.getTxnManagerFactory().getTxnManager(new HiveConf(conf));
+    swapTxnManager(txnMgr3);
+
+    // Compile txn 3 with only 1 known partition
+    driver3.compileAndRespond("merge into target t using source s on t.a = s.a " +
+        "when not matched then insert values (s.a, s.b, s.c)");
+
+    swapTxnManager(txnMgr);
+    driver.run();
+
+    swapTxnManager(txnMgr2);
+    driver2.run();
+    // Since txn2 was committed and it is part of txn3 snapshot, the snapshot should be invalidated
+    // txn3 should be rolled back and the query reexecuted
+    swapTxnManager(txnMgr3);
+    driver3.run();
+
+    swapTxnManager(txnMgr);
+    driver.run("select * from target");
+    List res = new ArrayList();
+    driver.getFetchTask().fetch(res);
+    // The merge should see all three partition and not create duplicates
+    Assert.assertEquals("Duplicate records found", 6, res.size());
+    Assert.assertTrue("Partition 3 was skipped", res.contains("6\t6\t3"));
+    dropTable(new String[]{"target", "source"});
+  }
+
+
+  @Test
+  public void testMergeInsertDynamicPartitioningSnapshotInvalidatedWithNewCommit() throws Exception {
+    // By creating the driver with the factory, we should have a ReExecDriver
+    IDriver driver3 = DriverFactory.newDriver(conf);
+    Assert.assertTrue("ReExecDriver was expected", driver3 instanceof ReExecDriver);
+
+    dropTable(new String[]{"target", "source"});
+    conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);
+
+    // Create partition c=1
+    driver.run("create table target (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')");
+    driver.run("insert into target values (1,1,1), (2,2,1)");
+    //Create partition c=2
+    driver.run("create table source (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')");
+    driver.run("insert into source values (3,3,2), (4,4,2)");
+
+    DbTxnManager txnMgr3 = (DbTxnManager) TxnManagerFactory.getTxnManagerFactory().getTxnManager(new HiveConf(conf));
+    swapTxnManager(txnMgr3);
+    // Compile txn 1 merge with only 1 known partition
+    driver3.compileAndRespond("merge into target t using source s on t.a = s.a " +
+        "when not matched then insert values (s.a, s.b, s.c)");
+
+    swapTxnManager(txnMgr);
+    // txn 2 insert data to an old and a new partition
+    driver.run("insert into source values (5,5,2), (6,6,3)");
+
+    // txn 3 insert into the target table into a new partition ( and a duplicate considering the source table)
+    driver.run("insert into target values (3, 3, 2)");
+
+    // Since we were writing in the target table, txn 3 should break txn 1 snapshot regardless that it was opened later
+    swapTxnManager(txnMgr3);
+    driver3.run();
+
+
+    swapTxnManager(txnMgr);
+    driver.run("select * from target");
+    List res = new ArrayList();
+    driver.getFetchTask().fetch(res);
+    // The merge should see all three partition and not create duplicates
+    Assert.assertEquals("Duplicate records found", 6, res.size());
+    Assert.assertTrue("Partition 3 was skipped", res.contains("6\t6\t3"));
+    dropTable(new String[]{"target", "source"});
   }
 
   /**
@@ -2561,6 +2706,7 @@ private void testMergePartitioned(boolean causeConflict, boolean sharedWrite) th
           1, //1 partitions updated (and no other entries)
           TxnDbUtil.countQueryAgent(conf, "select count(*) from \"WRITE_SET\" where \"WS_TXNID\"=" + txnid2));
     }
+    dropTable(new String[] {"target","source"});
   }
 
   /**
@@ -2752,7 +2898,11 @@ private void testFairness2(boolean zeroWaitRead) throws Exception {
       Assert.assertEquals("Unexpected lock count", 2, locks.size());
       checkLock(LockType.SHARED_READ, LockState.ACQUIRED, "default", "T7", "p=1", locks);
       checkLock(LockType.SHARED_READ, LockState.ACQUIRED, "default", "T7", "p=2", locks);
+    } else {
+      txnMgr2.rollbackTxn();
     }
+    txnMgr3.rollbackTxn();
+    dropTable(new String[]{"T7"});
   }
 
   @Test
