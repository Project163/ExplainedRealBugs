diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
index 6346dc5a77..fabceab9fa 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
@@ -858,7 +858,6 @@ public void majorCompactAfterAbort() throws Exception {
             Lists.newArrayList(5, 6), 1);
   }
 
-  @Ignore("PR #1618")
   @Test
   public void testCleanAbortCompactAfter2ndCommitAbort() throws Exception {
     String dbName = "default";
@@ -872,15 +871,14 @@ public void testCleanAbortCompactAfter2ndCommitAbort() throws Exception {
     connection.commitTransaction();
 
     connection.beginTransaction();
-    connection.write("2,3".getBytes());
+    connection.write("3,2".getBytes());
     connection.write("3,3".getBytes());
     connection.abortTransaction();
 
-    assertAndCompactCleanAbort(dbName, tblName, false);
+    assertAndCompactCleanAbort(dbName, tblName, true, true);
     connection.close();
   }
 
-  @Ignore("PR #1618")
   @Test
   public void testCleanAbortCompactAfter1stCommitAbort() throws Exception {
     String dbName = "default";
@@ -894,11 +892,11 @@ public void testCleanAbortCompactAfter1stCommitAbort() throws Exception {
     connection.abortTransaction();
 
     connection.beginTransaction();
-    connection.write("2,3".getBytes());
+    connection.write("3,2".getBytes());
     connection.write("3,3".getBytes());
     connection.commitTransaction();
 
-    assertAndCompactCleanAbort(dbName, tblName, false);
+    assertAndCompactCleanAbort(dbName, tblName, true, true);
     connection.close();
   }
 
@@ -924,7 +922,7 @@ public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {
     connection2.write("3,3".getBytes());
     connection2.abortTransaction();
 
-    assertAndCompactCleanAbort(dbName, tblName, true);
+    assertAndCompactCleanAbort(dbName, tblName, false, false);
 
     connection1.close();
     connection2.close();
@@ -953,13 +951,13 @@ public void testCleanAbortCompactAfterAbort() throws Exception {
     connection2.write("3,3".getBytes());
     connection2.abortTransaction();
 
-    assertAndCompactCleanAbort(dbName, tblName, true);
+    assertAndCompactCleanAbort(dbName, tblName, false, false);
 
     connection1.close();
     connection2.close();
   }
 
-  private void assertAndCompactCleanAbort(String dbName, String tblName, boolean allAborted) throws Exception {
+  private void assertAndCompactCleanAbort(String dbName, String tblName, boolean partialAbort, boolean singleSession) throws Exception {
     IMetaStoreClient msClient = new HiveMetaStoreClient(conf);
     TxnStore txnHandler = TxnUtils.getTxnStore(conf);
     Table table = msClient.getTable(dbName, tblName);
@@ -972,7 +970,7 @@ private void assertAndCompactCleanAbort(String dbName, String tblName, boolean a
 
     int count = TxnDbUtil.countQueryAgent(conf, "select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='i'");
     // We should have two rows corresponding to the two aborted transactions
-    Assert.assertEquals(TxnDbUtil.queryToString(conf, "select * from TXN_COMPONENTS"), allAborted ? 2 : 1, count);
+    Assert.assertEquals(TxnDbUtil.queryToString(conf, "select * from TXN_COMPONENTS"), partialAbort ? 1 : 2, count);
 
     runInitiator(conf);
     count = TxnDbUtil.countQueryAgent(conf, "select count(*) from COMPACTION_QUEUE");
@@ -991,14 +989,14 @@ private void assertAndCompactCleanAbort(String dbName, String tblName, boolean a
 
     // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.
     count = TxnDbUtil.countQueryAgent(conf, "select count(*) from TXN_COMPONENTS");
-    Assert.assertEquals(TxnDbUtil.queryToString(conf, "select * from TXN_COMPONENTS"), 0, count);
+    Assert.assertEquals(TxnDbUtil.queryToString(conf, "select * from TXN_COMPONENTS"), (singleSession && partialAbort) ? 1 : 0, count);
 
     count = TxnDbUtil.countQueryAgent(conf, "select count(*) from COMPACTION_QUEUE");
     Assert.assertEquals(TxnDbUtil.queryToString(conf, "select * from COMPACTION_QUEUE"), 0, count);
 
     RemoteIterator it =
         fs.listFiles(new Path(table.getSd().getLocation()), true);
-    if (it.hasNext() && allAborted) {
+    if (it.hasNext() && !partialAbort) {
       Assert.fail("Expected cleaner to drop aborted delta & base directories, FileStatus[] stat " + Arrays.toString(stat));
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
index 4153f015c7..6f03fd5bdd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
@@ -31,6 +31,7 @@
 import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
@@ -112,6 +113,7 @@
 import javax.annotation.concurrent.Immutable;
 import java.nio.charset.Charset;
 import java.util.stream.Collectors;
+import java.util.stream.LongStream;
 import java.util.stream.Stream;
 
 /**
@@ -563,6 +565,7 @@ else if (filename.startsWith(BUCKET_PREFIX)) {
 
   public static final class DirectoryImpl implements Directory {
     private final List<Path> abortedDirectories;
+    private final Set<Long> abortedWriteIds;
     private final boolean isBaseInRawFormat;
     private final List<HdfsFileStatusWithId> original;
     private final List<Path> obsolete;
@@ -570,11 +573,13 @@ public static final class DirectoryImpl implements Directory {
     private final Path base;
     private List<HdfsFileStatusWithId> baseFiles;
 
-    public DirectoryImpl(List<Path> abortedDirectories,
+    public DirectoryImpl(List<Path> abortedDirectories, Set<Long> abortedWriteIds,
         boolean isBaseInRawFormat, List<HdfsFileStatusWithId> original,
         List<Path> obsolete, List<ParsedDelta> deltas, Path base) {
       this.abortedDirectories = abortedDirectories == null ?
           Collections.emptyList() : abortedDirectories;
+      this.abortedWriteIds = abortedWriteIds == null ?
+        Collections.emptySet() : abortedWriteIds;
       this.isBaseInRawFormat = isBaseInRawFormat;
       this.original = original == null ? Collections.emptyList() : original;
       this.obsolete = obsolete == null ? Collections.emptyList() : obsolete;
@@ -620,6 +625,11 @@ public List<Path> getAbortedDirectories() {
       return abortedDirectories;
     }
 
+    @Override
+    public Set<Long> getAbortedWriteIds() {
+      return abortedWriteIds;
+    }
+
     @Override
     public String toString() {
       return "Aborted Directories: " + abortedDirectories + "; isBaseInRawFormat: " + isBaseInRawFormat + "; original: "
@@ -907,6 +917,12 @@ public interface Directory {
      * @return the list of aborted directories
      */
     List<Path> getAbortedDirectories();
+
+    /**
+     * Get the list of writeIds that belong to the aborted transactions.
+     * @return the list of aborted writeIds
+     */
+    Set<Long> getAbortedWriteIds();
   }
   /**
    * Since version 3 but prior to version 4, format of a base is "base_X" where X is a writeId.
@@ -1344,6 +1360,7 @@ private static Directory getAcidState(FileSystem fileSystem, Path candidateDirec
     List<Path> originalDirectories = new ArrayList<>();
     final List<Path> obsolete = new ArrayList<>();
     final List<Path> abortedDirectories = new ArrayList<>();
+    final Set<Long> abortedWriteIds = new HashSet<>();
     TxnBase bestBase = new TxnBase();
     final List<HdfsFileStatusWithId> original = new ArrayList<>();
 
@@ -1352,14 +1369,14 @@ private static Directory getAcidState(FileSystem fileSystem, Path candidateDirec
     if (childrenWithId != null) {
       for (HdfsFileStatusWithId child : childrenWithId) {
         getChildState(child, writeIdList, working, originalDirectories, original, obsolete,
-            bestBase, ignoreEmptyFiles, abortedDirectories, fs, validTxnList);
+            bestBase, ignoreEmptyFiles, abortedDirectories, abortedWriteIds, fs, validTxnList);
       }
     } else {
       if (dirSnapshots == null) {
         dirSnapshots = getHdfsDirSnapshots(fs, candidateDirectory);
       }
       getChildState(candidateDirectory, dirSnapshots, writeIdList, working, originalDirectories, original, obsolete,
-          bestBase, ignoreEmptyFiles, abortedDirectories, fs, validTxnList);
+          bestBase, ignoreEmptyFiles, abortedDirectories, abortedWriteIds, fs, validTxnList);
     }
     // If we have a base, the original files are obsolete.
     if (bestBase.basePath != null) {
@@ -1469,7 +1486,7 @@ else if (prev != null && next.maxWriteId == prev.maxWriteId
      */
     // this does "Path.uri.compareTo(that.uri)"
     original.sort(Comparator.comparing(HdfsFileStatusWithId::getFileStatus));
-    return new DirectoryImpl(abortedDirectories, isBaseInRawFormat, original, obsolete, deltas, base);
+    return new DirectoryImpl(abortedDirectories, abortedWriteIds, isBaseInRawFormat, original, obsolete, deltas, base);
   }
 
   public static Map<Path, HdfsDirSnapshot> getHdfsDirSnapshots(final FileSystem fs, final Path path)
@@ -1753,7 +1770,7 @@ private static boolean isCompactedBase(ParsedBase parsedBase, FileSystem fs,
 
   private static void getChildState(HdfsFileStatusWithId childWithId, ValidWriteIdList writeIdList,
       List<ParsedDelta> working, List<Path> originalDirectories, List<HdfsFileStatusWithId> original,
-      List<Path> obsolete, TxnBase bestBase, boolean ignoreEmptyFiles, List<Path> aborted, FileSystem fs,
+      List<Path> obsolete, TxnBase bestBase, boolean ignoreEmptyFiles, List<Path> aborted, Set<Long> abortedWriteIds, FileSystem fs,
       ValidTxnList validTxnList) throws IOException {
     Path childPath = childWithId.getFileStatus().getPath();
     String fn = childPath.getName();
@@ -1762,9 +1779,9 @@ private static void getChildState(HdfsFileStatusWithId childWithId, ValidWriteId
         original.add(childWithId);
       }
     } else if (fn.startsWith(BASE_PREFIX)) {
-      processBaseDir(childPath, writeIdList, obsolete, bestBase, aborted, fs, validTxnList, null);
+      processBaseDir(childPath, writeIdList, obsolete, bestBase, aborted, abortedWriteIds, fs, validTxnList, null);
     } else if (fn.startsWith(DELTA_PREFIX) || fn.startsWith(DELETE_DELTA_PREFIX)) {
-      processDeltaDir(childPath, writeIdList, working, aborted, fs, validTxnList, null);
+      processDeltaDir(childPath, writeIdList, working, aborted, abortedWriteIds, fs, validTxnList, null);
     } else {
       // This is just the directory.  We need to recurse and find the actual files.  But don't
       // do this until we have determined there is no base.  This saves time.  Plus,
@@ -1777,7 +1794,7 @@ private static void getChildState(HdfsFileStatusWithId childWithId, ValidWriteId
   private static void getChildState(Path candidateDirectory, Map<Path, HdfsDirSnapshot> dirSnapshots,
       ValidWriteIdList writeIdList, List<ParsedDelta> working, List<Path> originalDirectories,
       List<HdfsFileStatusWithId> original, List<Path> obsolete, TxnBase bestBase, boolean ignoreEmptyFiles,
-      List<Path> aborted, FileSystem fs, ValidTxnList validTxnList) throws IOException {
+      List<Path> aborted, Set<Long> abortedWriteIds, FileSystem fs, ValidTxnList validTxnList) throws IOException {
     for (HdfsDirSnapshot dirSnapshot : dirSnapshots.values()) {
       Path dirPath = dirSnapshot.getPath();
       String dirName = dirPath.getName();
@@ -1791,9 +1808,9 @@ private static void getChildState(Path candidateDirectory, Map<Path, HdfsDirSnap
           }
         }
       } else if (dirName.startsWith(BASE_PREFIX)) {
-        processBaseDir(dirPath, writeIdList, obsolete, bestBase, aborted, fs, validTxnList, dirSnapshot);
+        processBaseDir(dirPath, writeIdList, obsolete, bestBase, aborted, abortedWriteIds, fs, validTxnList, dirSnapshot);
       } else if (dirName.startsWith(DELTA_PREFIX) || dirName.startsWith(DELETE_DELTA_PREFIX)) {
-        processDeltaDir(dirPath, writeIdList, working, aborted, fs, validTxnList, dirSnapshot);
+        processDeltaDir(dirPath, writeIdList, working, aborted, abortedWriteIds, fs, validTxnList, dirSnapshot);
       } else {
         originalDirectories.add(dirPath);
         for (FileStatus stat : dirSnapshot.getFiles()) {
@@ -1806,7 +1823,7 @@ private static void getChildState(Path candidateDirectory, Map<Path, HdfsDirSnap
   }
 
   private static void processBaseDir(Path baseDir, ValidWriteIdList writeIdList, List<Path> obsolete, TxnBase bestBase,
-      List<Path> aborted, FileSystem fs, ValidTxnList validTxnList, AcidUtils.HdfsDirSnapshot dirSnapshot)
+      List<Path> aborted, Set<Long> abortedWriteIds, FileSystem fs, ValidTxnList validTxnList, AcidUtils.HdfsDirSnapshot dirSnapshot)
       throws IOException {
     ParsedBase parsedBase = ParsedBase.parseBase(baseDir);
     if (!isDirUsable(baseDir, parsedBase.getVisibilityTxnId(), aborted, validTxnList)) {
@@ -1821,6 +1838,7 @@ private static void processBaseDir(Path baseDir, ValidWriteIdList writeIdList, L
     // Handle aborted IOW base.
     if (writeIdList.isWriteIdAborted(writeId) && !isCompactedBase(parsedBase, fs, dirSnapshot)) {
       aborted.add(baseDir);
+      abortedWriteIds.add(parsedBase.writeId);
       return;
     }
     if (bestBase.basePath == null) {
@@ -1840,7 +1858,7 @@ private static void processBaseDir(Path baseDir, ValidWriteIdList writeIdList, L
   }
 
   private static void processDeltaDir(Path deltadir, ValidWriteIdList writeIdList, List<ParsedDelta> working,
-      List<Path> aborted, FileSystem fs, ValidTxnList validTxnList, AcidUtils.HdfsDirSnapshot dirSnapshot)
+      List<Path> aborted, Set<Long> abortedWriteIds, FileSystem fs, ValidTxnList validTxnList, AcidUtils.HdfsDirSnapshot dirSnapshot)
       throws IOException {
     String dirName = deltadir.getName();
     String deltaPrefix = dirName.startsWith(DELTA_PREFIX) ? DELTA_PREFIX : DELETE_DELTA_PREFIX;
@@ -1850,6 +1868,8 @@ private static void processDeltaDir(Path deltadir, ValidWriteIdList writeIdList,
     }
     if (ValidWriteIdList.RangeResponse.ALL == writeIdList.isWriteIdRangeAborted(delta.minWriteId, delta.maxWriteId)) {
       aborted.add(deltadir);
+      abortedWriteIds.addAll(LongStream.rangeClosed(delta.minWriteId, delta.maxWriteId)
+          .boxed().collect(Collectors.toList()));
     } else if (writeIdList.isWriteIdRangeValid(delta.minWriteId, delta.maxWriteId)
         != ValidWriteIdList.RangeResponse.NONE) {
       working.add(delta);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
index b76f797f01..7fa2302c6f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.ql.io.orc;
 
+import com.google.common.collect.Sets;
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus;
 import org.apache.hadoop.hive.common.BlobStorageUtils;
@@ -1331,7 +1332,7 @@ private AcidDirInfo callInternal() throws IOException {
         for (HdfsFileStatusWithId fileId : originals) {
           baseFiles.add(new AcidBaseFileInfo(fileId, AcidUtils.AcidBaseFileType.ORIGINAL_BASE));
         }
-        return new AcidDirInfo(fs.get(), dir, new AcidUtils.DirectoryImpl(Lists.newArrayList(), true, originals,
+        return new AcidDirInfo(fs.get(), dir, new AcidUtils.DirectoryImpl(Lists.newArrayList(), Sets.newHashSet(), true, originals,
             Lists.newArrayList(), Lists.newArrayList(), null), baseFiles, new ArrayList<>());
       }
       //todo: shouldn't ignoreEmptyFiles be set based on ExecutionEngine?
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java
index c8c11fa7d6..083f275528 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java
@@ -218,7 +218,7 @@ private void clean(CompactionInfo ci, long minOpenTxnGLB) throws MetaException {
               ci.getFullPartitionName() + idWatermark(ci), exception);
         }
       }
-      if (removedFiles.value) {
+      if (removedFiles.value || isDynPartAbort(t, ci)) {
         txnHandler.markCleaned(ci);
       } else {
         LOG.warn("No files were removed. Leaving queue entry " + ci + " in ready for cleaning state.");
@@ -255,6 +255,11 @@ private ValidReaderWriteIdList getValidCleanerWriteIdList(CompactionInfo ci, Tab
     return validWriteIdList;
   }
 
+  private static boolean isDynPartAbort(Table t, CompactionInfo ci) {
+    return t.getPartitionKeys() != null && t.getPartitionKeys().size() > 0
+        && ci.partName == null;
+  }
+
   private static String idWatermark(CompactionInfo ci) {
     return " id=" + ci.id;
   }
@@ -277,13 +282,18 @@ private boolean removeFiles(String location, ValidWriteIdList writeIdList, Compa
      * txns with write IDs > {@link CompactionInfo#highestWriteId}.
      * See {@link TxnStore#markCleaned(CompactionInfo)}
      */
+    Table table = getMSForConf(conf).getTable(getDefaultCatalog(conf), ci.dbname, ci.tableName);
+    if (isDynPartAbort(table, ci)) {
+      ci.setWriteIds(dir.getAbortedWriteIds());
+    }
     obsoleteDirs.addAll(dir.getAbortedDirectories());
     List<Path> filesToDelete = new ArrayList<>(obsoleteDirs.size());
+
     StringBuilder extraDebugInfo = new StringBuilder("[");
     for (Path stat : obsoleteDirs) {
       filesToDelete.add(stat);
       extraDebugInfo.append(stat.getName()).append(",");
-      if(!FileUtils.isPathWithinSubtree(stat, locPath)) {
+      if (!FileUtils.isPathWithinSubtree(stat, locPath)) {
         LOG.info(idWatermark(ci) + " found unexpected file: " + stat);
       }
     }
@@ -298,7 +308,6 @@ private boolean removeFiles(String location, ValidWriteIdList writeIdList, Compa
 
     FileSystem fs = filesToDelete.get(0).getFileSystem(conf);
     Database db = getMSForConf(conf).getDatabase(getDefaultCatalog(conf), ci.dbname);
-    Table table = getMSForConf(conf).getTable(getDefaultCatalog(conf), ci.dbname, ci.tableName);
 
     for (Path dead : filesToDelete) {
       LOG.debug("Going to delete path " + dead.toString());
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionInfo.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionInfo.java
index 80793c9158..36a867dfb6 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionInfo.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionInfo.java
@@ -27,6 +27,7 @@
 import java.sql.PreparedStatement;
 import java.sql.ResultSet;
 import java.sql.SQLException;
+import java.util.Set;
 
 /**
  * Information on a possible or running compaction.
@@ -60,6 +61,9 @@ public class CompactionInfo implements Comparable<CompactionInfo> {
    * {@link ValidCompactorWriteIdList#highWatermark}.
    */
   public long highestWriteId;
+  public Set<Long> writeIds;
+  public boolean isSetWriteIds;
+
   byte[] metaInfo;
   String hadoopJobId;
   public String errorMessage;
@@ -259,4 +263,9 @@ public static CompactionInfo optionalCompactionInfoStructToInfo(OptionalCompacti
     }
     return null;
   }
+
+  public void setWriteIds(Set<Long> writeIds) {
+    this.writeIds = writeIds;
+    isSetWriteIds = true;
+  }
 }
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java
index b7a92edfb5..1d916706ce 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java
@@ -35,6 +35,7 @@
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 
@@ -347,21 +348,21 @@ public void markCleaned(CompactionInfo info) throws MetaException {
       ResultSet rs = null;
       try {
         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);
-        pStmt = dbConn.prepareStatement("SELECT \"CQ_ID\", \"CQ_DATABASE\", \"CQ_TABLE\", \"CQ_PARTITION\", "
-            + "\"CQ_STATE\", \"CQ_TYPE\", \"CQ_TBLPROPERTIES\", \"CQ_WORKER_ID\", \"CQ_START\", \"CQ_RUN_AS\", "
-            + "\"CQ_HIGHEST_WRITE_ID\", \"CQ_META_INFO\", \"CQ_HADOOP_JOB_ID\", \"CQ_ERROR_MESSAGE\", "
-            + "\"CQ_ENQUEUE_TIME\" "
-            + "FROM \"COMPACTION_QUEUE\" WHERE \"CQ_ID\" = ?");
+        String s = "INSERT INTO \"COMPLETED_COMPACTIONS\"(\"CC_ID\", \"CC_DATABASE\", "
+            + "\"CC_TABLE\", \"CC_PARTITION\", \"CC_STATE\", \"CC_TYPE\", \"CC_TBLPROPERTIES\", \"CC_WORKER_ID\", "
+            + "\"CC_START\", \"CC_END\", \"CC_RUN_AS\", \"CC_HIGHEST_WRITE_ID\", \"CC_META_INFO\", "
+            + "\"CC_HADOOP_JOB_ID\", \"CC_ERROR_MESSAGE\", \"CC_ENQUEUE_TIME\") "
+          + "SELECT \"CQ_ID\", \"CQ_DATABASE\", \"CQ_TABLE\", \"CQ_PARTITION\", "
+            + quoteChar(SUCCEEDED_STATE) + ", \"CQ_TYPE\", \"CQ_TBLPROPERTIES\", \"CQ_WORKER_ID\", \"CQ_START\", "
+            + TxnDbUtil.getEpochFn(dbProduct) + ", \"CQ_RUN_AS\", \"CQ_HIGHEST_WRITE_ID\", \"CQ_META_INFO\", "
+            + "\"CQ_HADOOP_JOB_ID\", \"CQ_ERROR_MESSAGE\", \"CQ_ENQUEUE_TIME\" "
+            + "FROM \"COMPACTION_QUEUE\" WHERE \"CQ_ID\" = ?";
+        pStmt = dbConn.prepareStatement(s);
         pStmt.setLong(1, info.id);
-        rs = pStmt.executeQuery();
-        if (rs.next()) {
-          info = CompactionInfo.loadFullFromCompactionQueue(rs);
-        }
-        else {
-          throw new IllegalStateException("No record with CQ_ID=" + info.id + " found in COMPACTION_QUEUE");
-        }
-        close(rs);
-        String s = "DELETE FROM \"COMPACTION_QUEUE\" WHERE \"CQ_ID\" = ?";
+        LOG.debug("Going to execute update <" + s + "> for CQ_ID=" + info.id);
+        pStmt.executeUpdate();
+
+        s = "DELETE FROM \"COMPACTION_QUEUE\" WHERE \"CQ_ID\" = ?";
         pStmt = dbConn.prepareStatement(s);
         pStmt.setLong(1, info.id);
         LOG.debug("Going to execute update <" + s + ">");
@@ -371,18 +372,6 @@ public void markCleaned(CompactionInfo info) throws MetaException {
           LOG.debug("Going to rollback");
           dbConn.rollback();
         }
-        s = "INSERT INTO \"COMPLETED_COMPACTIONS\"(\"CC_ID\", \"CC_DATABASE\", "
-            + "\"CC_TABLE\", \"CC_PARTITION\", \"CC_STATE\", \"CC_TYPE\", \"CC_TBLPROPERTIES\", \"CC_WORKER_ID\", "
-            + "\"CC_START\", \"CC_END\", \"CC_RUN_AS\", \"CC_HIGHEST_WRITE_ID\", \"CC_META_INFO\", "
-            + "\"CC_HADOOP_JOB_ID\", \"CC_ERROR_MESSAGE\", \"CC_ENQUEUE_TIME\")"
-            + " VALUES(?,?,?,?,?, ?,?,?,?,?, ?,?,?,?,?,?)";
-        pStmt = dbConn.prepareStatement(s);
-        info.state = SUCCEEDED_STATE;
-        long enqueueTime = getDbTime(dbConn);
-        CompactionInfo.insertIntoCompletedCompactions(pStmt, info, enqueueTime);
-        LOG.debug("Going to execute update <" + s + "> with parameter enqueue time: " + enqueueTime);
-        updCount = pStmt.executeUpdate();
-
         // Remove entries from completed_txn_components as well, so we don't start looking there
         // again but only up to the highest write ID include in this compaction job.
         //highestWriteId will be NULL in upgrade scenarios
@@ -415,31 +404,56 @@ public void markCleaned(CompactionInfo info) throws MetaException {
          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).
          * See {@link ql.txn.compactor.Cleaner.removeFiles()}
          */
-        s = "DELETE FROM \"TXN_COMPONENTS\" WHERE \"TC_TXNID\" IN (" +
-            "   SELECT \"TXN_ID\" FROM \"TXNS\" WHERE \"TXN_STATE\" = " + TxnStatus.ABORTED + ") " +
-            "AND \"TC_DATABASE\" = ? AND \"TC_TABLE\" = ? " +
-            "AND \"TC_PARTITION\" "+ (info.partName != null ? "= ?" : "IS NULL");
-        if (info.highestWriteId != 0) {
-          s += " AND \"TC_WRITEID\" <= ?";
-        }
-        LOG.debug("Going to execute update <" + s + ">");
-        pStmt = dbConn.prepareStatement(s);
-        paramCount = 1;
+        s = "DELETE FROM \"TXN_COMPONENTS\" WHERE \"TC_TXNID\" IN ( "
+              + "SELECT \"TXN_ID\" FROM \"TXNS\" WHERE \"TXN_STATE\" = " + TxnStatus.ABORTED + ") "
+            + "AND \"TC_DATABASE\" = ? AND \"TC_TABLE\" = ? "
+            + "AND \"TC_PARTITION\" "+ (info.partName != null ? "= ?" : "IS NULL");
 
-        pStmt.setString(paramCount++, info.dbname);
-        pStmt.setString(paramCount++, info.tableName);
-        if (info.partName != null) {
-          pStmt.setString(paramCount++, info.partName);
-        }
-        if (info.highestWriteId != 0) {
-          pStmt.setLong(paramCount, info.highestWriteId);
+        List<String> queries = new ArrayList<>();
+        Iterator<Long> writeIdsIter = null;
+        List<Integer> counts = null;
+
+        if (info.isSetWriteIds && !info.writeIds.isEmpty()) {
+          StringBuilder prefix = new StringBuilder(s).append(" AND ");
+          List<String> questions = Collections.nCopies(info.writeIds.size(), "?");
+
+          counts = TxnUtils.buildQueryWithINClauseStrings(conf, queries, prefix,
+            new StringBuilder(), questions, "\"TC_WRITEID\"", false, false);
+          writeIdsIter = info.writeIds.iterator();
+        } else if (!info.isSetWriteIds){
+          if (info.highestWriteId != 0) {
+            s += " AND \"TC_WRITEID\" <= ?";
+          }
+          queries.add(s);
         }
-        int rc = pStmt.executeUpdate();
-        LOG.debug("Removed " + rc + " records from txn_components");
-        // Don't bother cleaning from the txns table.  A separate call will do that.  We don't
-        // know here which txns still have components from other tables or partitions in the
-        // table, so we don't know which ones we can and cannot clean.
 
+        for (int i = 0; i < queries.size(); i++) {
+          String query = queries.get(i);
+          int writeIdCount = (counts != null) ? counts.get(i) : 0;
+
+          LOG.debug("Going to execute update <" + query + ">");
+          pStmt = dbConn.prepareStatement(query);
+          paramCount = 1;
+
+          pStmt.setString(paramCount++, info.dbname);
+          pStmt.setString(paramCount++, info.tableName);
+          if (info.partName != null) {
+            pStmt.setString(paramCount++, info.partName);
+          }
+          if (info.highestWriteId != 0 && writeIdCount == 0) {
+            pStmt.setLong(paramCount, info.highestWriteId);
+          }
+          for (int j = 0; j < writeIdCount; j++) {
+            if (writeIdsIter.hasNext()) {
+              pStmt.setLong(paramCount + j, writeIdsIter.next());
+            }
+          }
+          int rc = pStmt.executeUpdate();
+          LOG.debug("Removed " + rc + " records from txn_components");
+          // Don't bother cleaning from the txns table.  A separate call will do that.  We don't
+          // know here which txns still have components from other tables or partitions in the
+          // table, so we don't know which ones we can and cannot clean.
+        }
         LOG.debug("Going to commit");
         dbConn.commit();
       } catch (SQLException e) {
@@ -1037,7 +1051,7 @@ public void markFailed(CompactionInfo ci) throws MetaException {//todo: this sho
             + "FROM \"COMPACTION_QUEUE\" WHERE \"CQ_ID\" = ?");
         pStmt.setLong(1, ci.id);
         rs = pStmt.executeQuery();
-        if(rs.next()) {
+        if (rs.next()) {
           ci = CompactionInfo.loadFullFromCompactionQueue(rs);
           String s = "DELETE FROM \"COMPACTION_QUEUE\" WHERE \"CQ_ID\" = ?";
           pStmt = dbConn.prepareStatement(s);
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
index 037132a67d..5d76508db7 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
@@ -291,7 +291,7 @@ abstract class TxnHandler implements TxnStore, TxnStore.MutexAPI {
   private int deadlockCnt;
   private long deadlockRetryInterval;
   protected Configuration conf;
-  protected static DatabaseProduct dbProduct;
+  static DatabaseProduct dbProduct;
   private static SQLGenerator sqlGenerator;
   private static long openTxnTimeOutMillis;
 
