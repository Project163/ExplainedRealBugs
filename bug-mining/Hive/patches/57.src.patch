diff --git a/CHANGES.txt b/CHANGES.txt
index e32e679c08..80f47579ff 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -219,3 +219,6 @@ Trunk (unreleased changes)
 
     HIVE-25. Enable Table aliases in cluster by, distribute by and sort
     by clauses (Prasad Chakka via athusoo)
+
+    HIVE-217. Report progress during FileSinkOperator in order to avoid
+    Stream closes exceptions (Johan Oskarsson via athusoo)
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
index ca5e412924..c32a726fd5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.conf.Configuration;
 
 /**
@@ -38,8 +39,8 @@ public class CollectOperator extends Operator <collectDesc> implements Serializa
   transient protected ArrayList<ObjectInspector> rowInspectorList;
   transient int maxSize;
 
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     rowList = new ArrayList<Object> ();
     rowInspectorList = new ArrayList<ObjectInspector> ();
     maxSize = conf.getBufferSize().intValue();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
index c084fcb22c..cdbfe8cf09 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
@@ -54,7 +54,7 @@ public void map(Object key, Object value,
       try {
         oc = output;
         mo.setOutputCollector(oc);
-        mo.initialize(jc);
+        mo.initialize(jc, reporter);
         rp = reporter;
       } catch (HiveException e) {
         abort = true;
@@ -81,7 +81,7 @@ public void close() {
     if(oc == null) {
       try {
         l4j.trace("Close called no row");
-        mo.initialize(jc);
+        mo.initialize(jc, null);
         rp = null;
       } catch (HiveException e) {
         abort = true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
index 5f0bd712e5..1030f17946 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
@@ -111,7 +111,7 @@ public void reduce(Object key, Iterator values,
       try {
         oc = output;
         reducer.setOutputCollector(oc);
-        reducer.initialize(jc);
+        reducer.initialize(jc, reporter);
         rp = reporter;
       } catch (HiveException e) {
         abort = true;
@@ -176,7 +176,7 @@ public void close() {
     if(oc == null) {
       try {
         l4j.trace("Close called no row");
-        reducer.initialize(jc);
+        reducer.initialize(jc, null);
         rp = null;
       } catch (HiveException e) {
         abort = true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
index 6b57114a78..17c3f1bb37 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.ql.plan.extractDesc;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.conf.Configuration;
 
 /**
@@ -35,8 +36,8 @@ public class ExtractOperator extends Operator<extractDesc> implements Serializab
   transient protected ExprNodeEvaluator eval;
   transient protected InspectableObject result = new InspectableObject();
 
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     eval = ExprNodeEvaluatorFactory.get(conf.getCol());
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index 4b843008e9..7fe645fdc6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -75,8 +75,9 @@ public void close(boolean abort) throws HiveException {
     }
   }
 
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
+    
     try {
       serializer = (Serializer)conf.getTableInfo().getDeserializerClass().newInstance();
       serializer.initialize(null, conf.getTableInfo().getProperties());
@@ -155,6 +156,7 @@ public void close(boolean abort) throws IOException {
   Writable recordValue; 
   public void process(Object row, ObjectInspector rowInspector) throws HiveException {
     try {
+      reporter.progress();
       // user SerDe to serialize r, and write it out
       recordValue = serializer.serialize(row, rowInspector);
       outWriter.write(recordValue);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
index 696ab33530..e35db2b5fd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.Reporter;
 
 /**
  * Filter operator implementation
@@ -45,8 +46,8 @@ public FilterOperator () {
     conditionInspectableObject = new InspectableObject();
   }
 
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     try {
       this.conditionEvaluator = ExprNodeEvaluatorFactory.get(conf.getPredicate());
       statsMap.put(Counter.FILTERED, filtered_count);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
index 4277f94f0f..ca54dad49c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
@@ -23,6 +23,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.forwardDesc;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.conf.Configuration;
 
 /**
@@ -31,8 +32,8 @@
  **/
 public class ForwardOperator extends  Operator<forwardDesc>  implements Serializable {
   private static final long serialVersionUID = 1L;
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     // nothing to do really ..
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
index 08dd3adf12..f7cc55c57e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
@@ -42,6 +42,7 @@
 import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.ql.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 
@@ -112,8 +113,8 @@ List<Field> getFields() {
   transient int           numEntriesVarSize;
   transient int           numEntriesHashTable;
 
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     totalMemory = Runtime.getRuntime().totalMemory();
 
     // init keyFields
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
index 254ce5e3ad..5e5c816fc8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
@@ -39,6 +39,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.mapred.Reporter;
 
 /**
  * Join operator implementation.
@@ -113,8 +114,9 @@ public void popObj() {
   HashMap<Byte, Vector<ArrayList<Object>>> storage;
   int joinEmitInterval = -1;
   
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
+    
     totalSz = 0;
     // Map that contains the rows for each alias
     storage = new HashMap<Byte, Vector<ArrayList<Object>>>();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
index a72e0c7a33..dc8cb3f1eb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.plan.limitDesc;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.conf.Configuration;
 
 /**
@@ -38,8 +39,8 @@ public class LimitOperator extends Operator<limitDesc> implements Serializable {
   transient protected int limit;
   transient protected int currCount;
 
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     limit = conf.getLimit();
     currCount = 0;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
index 403bd55617..361b98d980 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
@@ -23,6 +23,7 @@
 
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 
@@ -61,8 +62,8 @@ public static enum Counter {DESERIALIZE_ERRORS}
   transient private List<ObjectInspector> partObjectInspectors;
   
 
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     Path fpath = new Path((new Path (HiveConf.getVar(hconf, HiveConf.ConfVars.HADOOPMAPFILENAME))).toUri().getPath());
     ArrayList<Operator<? extends Serializable>> todo = new ArrayList<Operator<? extends Serializable>> ();
     statsMap.put(Counter.DESERIALIZE_ERRORS, deserialize_error_count);
@@ -163,7 +164,7 @@ public void initialize(Configuration hconf) throws HiveException {
     this.setOutputCollector(out);
 
     for(Operator op: todo) {
-      op.initialize(hconf);
+      op.initialize(hconf, reporter);
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
index 92a14724f7..e5883203a3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reporter;
 
 /**
  * Base operator implementation
@@ -48,6 +49,14 @@ public abstract class Operator <T extends Serializable> implements Serializable,
   protected List<Operator<? extends Serializable>> parentOperators;
 
   public Operator() {}
+  
+  /**
+   * Create an operator with a reporter.
+   * @param reporter Used to report progress of certain operators.
+   */
+  public Operator(Reporter reporter) {
+    this.reporter = reporter;
+  }
 
   public void setChildOperators(List<Operator<? extends Serializable>> childOperators) {
     this.childOperators = childOperators;
@@ -130,6 +139,7 @@ public RowSchema getSchema() {
   transient protected mapredWork gWork;
   transient protected String alias;
   transient protected String joinAlias;
+  transient protected Reporter reporter;
 
   public void setOutputCollector(OutputCollector out) {
     this.out = out;
@@ -198,15 +208,16 @@ public Map<Enum<?>, Long> getStats() {
     return(ret);
   }
 
-  public void initialize (Configuration hconf) throws HiveException {
+  public void initialize (Configuration hconf, Reporter reporter) throws HiveException {
     LOG.info("Initializing Self");
+    this.reporter = reporter;
     
     if(childOperators == null) {
       return;
     }
     LOG.info("Initializing children:");
     for(Operator<? extends Serializable> op: childOperators) {
-      op.initialize(hconf);
+      op.initialize(hconf, reporter);
     }    
     LOG.info("Initialization Done");
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
index e2d6b51df3..579a10b757 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
@@ -37,6 +37,7 @@
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.Reporter;
 
 /**
  * Reduce Sink Operator sends output to the reduce stage
@@ -70,8 +71,8 @@ public class ReduceSinkOperator extends TerminalOperator <reduceSinkDesc> implem
   transient int tag;
   transient byte[] tagByte = new byte[1];
   
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     try {
       keyEval = new ExprNodeEvaluator[conf.getKeyCols().size()];
       int i=0;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
index eddc734cb2..6e4c7364b7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
@@ -32,6 +32,7 @@
 import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.LineRecordReader.LineReader;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hadoop.fs.FileUtil;
@@ -161,8 +162,8 @@ public File getAbsolutePath(String filename)
     }
   }
 
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     statsMap.put(Counter.DESERIALIZE_ERRORS, deserialize_error_count);
     statsMap.put(Counter.SERIALIZE_ERRORS, serialize_error_count);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
index d17bd53ba4..53da741f2f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
@@ -28,6 +28,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.mapred.Reporter;
 
 /**
  * Select operator implementation
@@ -44,8 +45,8 @@ public class SelectOperator extends Operator <selectDesc> implements Serializabl
   
   boolean firstRow;
   
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     try {
       ArrayList<exprNodeDesc> colList = conf.getColList();
       eval = new ExprNodeEvaluator[colList.size()];
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
index a6406a1d6f..309d4f791e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
@@ -23,6 +23,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.tableScanDesc;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.conf.Configuration;
 
 /**
@@ -32,8 +33,8 @@
  **/
 public class TableScanOperator extends Operator<tableScanDesc> implements Serializable {
   private static final long serialVersionUID = 1L;
-  public void initialize(Configuration hconf) throws HiveException {
-    super.initialize(hconf);
+  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
+    super.initialize(hconf, reporter);
     // nothing to do really ..
   }
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
index 70e6735d70..c7601ac93c 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
@@ -22,6 +22,7 @@
 import java.io.*;
 import java.util.*;
 import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
@@ -85,7 +86,7 @@ public void testBaseFilterOperator() throws Throwable {
       op.setConf(filterCtx);
 
       // runtime initialization
-      op.initialize(null);
+      op.initialize(null, null);
 
       for(InspectableObject oner: r) {
         op.process(oner.o, oner.oi);
@@ -140,7 +141,7 @@ public void testFileSinkOperator() throws Throwable {
       nextOp.add(flop);
 
       op.setChildOperators(nextOp);
-      op.initialize(new JobConf(TestOperators.class));
+      op.initialize(new JobConf(TestOperators.class), Reporter.NULL);
 
       // evaluate on row
       for(int i=0; i<5; i++) {
@@ -200,7 +201,7 @@ public void testScriptOperator() throws Throwable {
       sop.setChildOperators(nextCollectOp);
 
 
-      op.initialize(new JobConf(TestOperators.class));
+      op.initialize(new JobConf(TestOperators.class), null);
 
       // evaluate on row
       for(int i=0; i<5; i++) {
@@ -270,7 +271,7 @@ public void testMapOperator() throws Throwable {
       // get map operator and initialize it
       MapOperator mo = new MapOperator();
       mo.setConf(mrwork);
-      mo.initialize(hconf);
+      mo.initialize(hconf, null);
 
       Text tw = new Text();
       InspectableObject io1 = new InspectableObject();
