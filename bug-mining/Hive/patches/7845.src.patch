diff --git a/Jenkinsfile b/Jenkinsfile
index fe016418f7..7f733c88de 100644
--- a/Jenkinsfile
+++ b/Jenkinsfile
@@ -174,6 +174,17 @@ def loadWS() {
     tar -xf archive.tar'''
 }
 
+def saveFile(name) {
+  sh """#!/bin/bash -e
+    rsync -rltDq --stats ${name} rsync://rsync/data/$LOCKED_RESOURCE.${name}"""
+}
+
+def loadFile(name) {
+  sh """#!/bin/bash -e
+    rsync -rltDq --stats rsync://rsync/data/$LOCKED_RESOURCE.${name} ${name}"""
+}
+
+
 jobWrappers {
 
   def splits
@@ -207,19 +218,18 @@ jobWrappers {
     }
   }
 
-  stage('Testing') {
-    def branches = [:]
-    for (def d in ['derby','postgres']) {
-      def dbType=d
-      def splitName = "init@$dbType"
-      branches[splitName] = {
-        executorNode {
-          stage('Prepare') {
-              loadWS();
-          }
-          stage('init-metastore') {
-             withEnv(["dbType=$dbType"]) {
-               sh '''#!/bin/bash -e
+  def branches = [:]
+  for (def d in ['derby','postgres']) {
+    def dbType=d
+    def splitName = "init@$dbType"
+    branches[splitName] = {
+      executorNode {
+        stage('Prepare') {
+            loadWS();
+        }
+        stage('init-metastore') {
+           withEnv(["dbType=$dbType"]) {
+             sh '''#!/bin/bash -e
 set -x
 echo 127.0.0.1 dev_$dbType | sudo tee -a /etc/hosts
 . /etc/profile.d/confs.sh
@@ -228,35 +238,69 @@ ping -c2 dev_$dbType
 export DOCKER_NETWORK=host
 reinit_metastore $dbType
 '''
-            }
           }
         }
       }
     }
-    for (int i = 0; i < splits.size(); i++) {
-      def num = i
-      def split = splits[num]
-      def splitName=String.format("split-%02d",num+1)
-      branches[splitName] = {
-        executorNode {
-          stage('Prepare') {
-              loadWS();
-              writeFile file: (split.includes ? "inclusions.txt" : "exclusions.txt"), text: split.list.join("\n")
-              writeFile file: (split.includes ? "exclusions.txt" : "inclusions.txt"), text: ''
-              sh '''echo "@INC";cat inclusions.txt;echo "@EXC";cat exclusions.txt;echo "@END"'''
+  }
+  for (int i = 0; i < splits.size(); i++) {
+    def num = i
+    def split = splits[num]
+    def splitName=String.format("split-%02d",num+1)
+    branches[splitName] = {
+      executorNode {
+        stage('Prepare') {
+            loadWS();
+            writeFile file: (split.includes ? "inclusions.txt" : "exclusions.txt"), text: split.list.join("\n")
+            writeFile file: (split.includes ? "exclusions.txt" : "inclusions.txt"), text: ''
+            sh '''echo "@INC";cat inclusions.txt;echo "@EXC";cat exclusions.txt;echo "@END"'''
+        }
+        try {
+          stage('Test') {
+            buildHive("org.apache.maven.plugins:maven-antrun-plugin:run@{define-classpath,setup-test-dirs,setup-metastore-scripts} org.apache.maven.plugins:maven-surefire-plugin:test -q")
           }
-          try {
-            stage('Test') {
-              buildHive("org.apache.maven.plugins:maven-antrun-plugin:run@{define-classpath,setup-test-dirs,setup-metastore-scripts} org.apache.maven.plugins:maven-surefire-plugin:test -q")
-            }
-          } finally {
-            stage('Archive') {
+        } finally {
+          stage('PostProcess') {
+            try {
+              sh """#!/bin/bash -e
+                # removes all stdout and err for passed tests
+                xmlstarlet ed -L -d 'testsuite/testcase/system-out[count(../failure)=0]' -d 'testsuite/testcase/system-err[count(../failure)=0]' `find . -name 'TEST*xml' -path '*/surefire-reports/*'`
+                # remove all output.txt files
+                find . -name '*output.txt' -path '*/surefire-reports/*' -exec unlink "{}" \\;
+              """
+            } finally {
+              def fn="${splitName}.tgz"
+              sh """#!/bin/bash -e
+              tar -czf ${fn} --files-from  <(find . -path '*/surefire-reports/*')"""
+              saveFile(fn)
               junit '**/TEST-*.xml'
             }
           }
         }
       }
     }
-    parallel branches
+  }
+  try {
+    stage('Testing') {
+      parallel branches
+    }
+  } finally {
+    stage('Archive') {
+      executorNode {
+        for (int i = 0; i < splits.size(); i++) {
+          def num = i
+          def splitName=String.format("split-%02d",num+1)
+          def fn="${splitName}.tgz"
+          loadFile(fn)
+          sh("""#!/bin/bash -e
+              mkdir ${splitName}
+              tar xzf ${fn} -C ${splitName}
+              unlink ${fn}""")
+        }
+        sh("""#!/bin/bash -e
+        tar czf test-results.tgz split*""")
+        archiveArtifacts artifacts: "**/test-results.tgz"
+      }
+    }
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/hooks/RuntimeStatsPersistenceCheckerHook.java b/ql/src/java/org/apache/hadoop/hive/ql/hooks/RuntimeStatsPersistenceCheckerHook.java
index b0bdad3007..cc754eebc4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/hooks/RuntimeStatsPersistenceCheckerHook.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/hooks/RuntimeStatsPersistenceCheckerHook.java
@@ -59,7 +59,7 @@ public void run(HookContext hookContext) throws Exception {
         throw new RuntimeException("while checking the signature of: " + sig.getSig(), e);
       }
     }
-    LOG.info("signature checked: " + sigs.size());
+    LOG.debug("signature checked: " + sigs.size());
   }
 
   private <T> T persistenceLoop(T sig, Class<T> clazz) throws IOException {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketVersionPopulator.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketVersionPopulator.java
index 421e4e14ee..fa8df98923 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketVersionPopulator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketVersionPopulator.java
@@ -181,7 +181,7 @@ List<OperatorBucketingVersionInfo> getBucketingVersions() {
           if (numBuckets > 1) {
             ret.add(new OperatorBucketingVersionInfo(operator, bucketingVersion));
           } else {
-            LOG.info("not considering bucketingVersion for: %s because it has %d<2 buckets ", tso, numBuckets);
+            LOG.info("not considering bucketingVersion for: {} because it has {}<2 buckets ", tso, numBuckets);
           }
         }
         if (operator instanceof FileSinkOperator) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java
index 8250900510..10faf4012f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java
@@ -164,8 +164,7 @@ public static class ScriptPPD extends DefaultPPD implements SemanticNodeProcesso
     @Override
     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
         Object... nodeOutputs) throws SemanticException {
-      LOG.debug("Processing for " + nd.getName() + "("
-          + ((Operator) nd).getIdentifier() + ")");
+      LOG.debug("Processing for {}", nd.toString());
       // script operator is a black-box to hive so no optimization here
       // assuming that nothing can be pushed above the script op
       // same with LIMIT op
@@ -198,8 +197,7 @@ public static class PTFPPD extends ScriptPPD {
     @Override
     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
         Object... nodeOutputs) throws SemanticException {
-      LOG.info("Processing for " + nd.getName() + "("
-          + ((Operator) nd).getIdentifier() + ")");
+      LOG.debug("Processing for {}", nd.toString());
       OpWalkerInfo owi = (OpWalkerInfo) procCtx;
       PTFOperator ptfOp = (PTFOperator) nd;
 
@@ -390,8 +388,7 @@ public static class LateralViewForwardPPD extends DefaultPPD implements Semantic
     @Override
     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
         Object... nodeOutputs) throws SemanticException {
-      LOG.info("Processing for " + nd.getName() + "("
-          + ((Operator) nd).getIdentifier() + ")");
+      LOG.debug("Processing for {}", nd.toString());
       OpWalkerInfo owi = (OpWalkerInfo) procCtx;
 
       // The lateral view forward operator has 2 children, a SELECT(*) and
@@ -417,8 +414,7 @@ public static class TableScanPPD extends DefaultPPD implements SemanticNodeProce
     @Override
     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
         Object... nodeOutputs) throws SemanticException {
-      LOG.info("Processing for " + nd.getName() + "("
-          + ((Operator) nd).getIdentifier() + ")");
+      LOG.debug("Processing for {}", nd.toString());
       OpWalkerInfo owi = (OpWalkerInfo) procCtx;
       TableScanOperator tsOp = (TableScanOperator) nd;
       mergeWithChildrenPred(tsOp, owi, null, null);
@@ -450,8 +446,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
 
     Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
             boolean onlySyntheticJoinPredicate, Object... nodeOutputs) throws SemanticException {
-      LOG.info("Processing for " + nd.getName() + "("
-          + ((Operator) nd).getIdentifier() + ")");
+      LOG.debug("Processing for {}", nd.toString());
 
       OpWalkerInfo owi = (OpWalkerInfo) procCtx;
       Operator<? extends OperatorDesc> op = (Operator<? extends OperatorDesc>) nd;
@@ -539,8 +534,7 @@ public static class JoinerPPD extends DefaultPPD implements SemanticNodeProcesso
     @Override
     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
         Object... nodeOutputs) throws SemanticException {
-      LOG.info("Processing for " + nd.getName() + "("
-          + ((Operator) nd).getIdentifier() + ")");
+      LOG.debug("Processing for {}", nd.toString());
       OpWalkerInfo owi = (OpWalkerInfo) procCtx;
       Set<String> aliases = getAliases(nd);
       // we pass null for aliases here because mergeWithChildrenPred filters
@@ -837,8 +831,7 @@ public static class DefaultPPD implements SemanticNodeProcessor {
     @Override
     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
         Object... nodeOutputs) throws SemanticException {
-      LOG.info("Processing for " + nd.getName() + "("
-          + ((Operator) nd).getIdentifier() + ")");
+      LOG.debug("Processing for {}", nd.toString());
       OpWalkerInfo owi = (OpWalkerInfo) procCtx;
 
       Set<String> includes = getQualifiedAliases((Operator<?>) nd, owi);
