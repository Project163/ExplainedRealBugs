diff --git a/data/files/hive_26612.parquet b/data/files/hive_26612.parquet
new file mode 100644
index 0000000000..9a42d90680
Binary files /dev/null and b/data/files/hive_26612.parquet differ
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
index 28207714e3..4c3ab70958 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
@@ -749,18 +749,21 @@ PrimitiveConverter getConverter(final PrimitiveType type, final int index, final
         TypeInfo hiveTypeInfo) {
       if (hiveTypeInfo != null) {
         String typeName = TypeInfoUtils.getBaseName(hiveTypeInfo.getTypeName());
+        final long min = getMinValue(type, typeName, Long.MIN_VALUE);
+        final long max = getMaxValue(typeName, Long.MAX_VALUE);
+
         switch (typeName) {
-          case serdeConstants.BIGINT_TYPE_NAME:
-            return new BinaryConverter<LongWritable>(type, parent, index) {
-              @Override
-              protected LongWritable convert(Binary binary) {
-                Preconditions.checkArgument(binary.length() == 8, "Must be 8 bytes");
-                ByteBuffer buf = binary.toByteBuffer();
-                buf.order(ByteOrder.LITTLE_ENDIAN);
-                long longVal = buf.getLong();
-                return new LongWritable(longVal);
+        case serdeConstants.BIGINT_TYPE_NAME:
+          return new PrimitiveConverter() {
+            @Override
+            public void addLong(long value) {
+              if ((value >= min) && (value <= max)) {
+                parent.set(index, new LongWritable(value));
+              } else {
+                parent.set(index, null);
               }
-            };
+            }
+          };
         }
       }
       return new PrimitiveConverter() {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/convert/TestETypeConverter.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/convert/TestETypeConverter.java
index fcfb5c7782..cf6444c9c0 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/convert/TestETypeConverter.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/convert/TestETypeConverter.java
@@ -116,23 +116,19 @@ public void testGetDecimalConverterDoubleHiveType() throws Exception {
   }
 
   @Test
-  public void testGetSmallBigIntConverter() {
+  public void testGetInt64TimestampConverterBigIntHiveType() {
     Timestamp timestamp = Timestamp.valueOf("1998-10-03 09:58:31.231");
     long msTime = timestamp.toEpochMilli();
-    ByteBuffer buf = ByteBuffer.allocate(12);
-    buf.order(ByteOrder.LITTLE_ENDIAN);
-    buf.putLong(msTime);
-    buf.flip();
     // Need TimeStamp logicalType annotation here
     PrimitiveType primitiveType = createInt64TimestampType(false, TimeUnit.MILLIS);
-    Writable writable = getWritableFromBinaryConverter(createHiveTypeInfo("bigint"), primitiveType, Binary.fromByteBuffer(buf));
+    Writable writable = getWritableFromPrimitiveConverter(createHiveTypeInfo("bigint"), primitiveType, msTime);
     // Retrieve as BigInt
     LongWritable longWritable = (LongWritable) writable;
     assertEquals(msTime, longWritable.get());
   }
 
   @Test
-  public void testGetBigIntConverter() {
+  public void testGetInt96TimestampConverterBigIntHiveType() {
     Timestamp timestamp = Timestamp.valueOf("1998-10-03 09:58:31.231");
     NanoTime nanoTime = NanoTimeUtils.getNanoTime(timestamp, ZoneOffset.UTC, false);
     PrimitiveType primitiveType = Types.optional(PrimitiveTypeName.INT96).named("value");
diff --git a/ql/src/test/queries/clientpositive/parquet_int64_timestamp_to_bigint.q b/ql/src/test/queries/clientpositive/parquet_int64_timestamp_to_bigint.q
new file mode 100644
index 0000000000..62b18dc314
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/parquet_int64_timestamp_to_bigint.q
@@ -0,0 +1,22 @@
+-- The file hive_26612.parquet has the following schema:
+-- {
+--   "type" : "record",
+--   "name" : "spark_schema",
+--   "fields" : [ {
+--     "name" : "typeid",
+--     "type" : "int"
+--   }, {
+--     "name" : "eventtime",
+--     "type" : [ "null", {
+--       "type" : "long",
+--       "logicalType" : "timestamp-millis"
+--     } ],
+--     "default" : null
+--   } ]
+-- }
+-- It was created from Spark with the steps documented in HIVE-26612
+CREATE TABLE ts_as_bigint_pq (typeid int, eventtime BIGINT) STORED AS PARQUET;
+
+LOAD DATA LOCAL INPATH '../../data/files/hive_26612.parquet' into table ts_as_bigint_pq;
+
+SELECT * FROM ts_as_bigint_pq;
diff --git a/ql/src/test/results/clientpositive/llap/parquet_int64_timestamp_to_bigint.q.out b/ql/src/test/results/clientpositive/llap/parquet_int64_timestamp_to_bigint.q.out
new file mode 100644
index 0000000000..22ffd1bee8
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/parquet_int64_timestamp_to_bigint.q.out
@@ -0,0 +1,26 @@
+PREHOOK: query: CREATE TABLE ts_as_bigint_pq (typeid int, eventtime BIGINT) STORED AS PARQUET
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@ts_as_bigint_pq
+POSTHOOK: query: CREATE TABLE ts_as_bigint_pq (typeid int, eventtime BIGINT) STORED AS PARQUET
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@ts_as_bigint_pq
+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/hive_26612.parquet' into table ts_as_bigint_pq
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@ts_as_bigint_pq
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/hive_26612.parquet' into table ts_as_bigint_pq
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@ts_as_bigint_pq
+PREHOOK: query: SELECT * FROM ts_as_bigint_pq
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ts_as_bigint_pq
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM ts_as_bigint_pq
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ts_as_bigint_pq
+#### A masked pattern was here ####
+1	1388617201000
+1	1417351232000
