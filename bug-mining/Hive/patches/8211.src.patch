diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
index 66a8475d36..daf864210d 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
@@ -24,8 +24,16 @@
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.hadoop.hive.metastore.IMetaStoreClient;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.metastore.utils.TestTxnDbUtil;
 import org.apache.hadoop.hive.ql.QTestMiniClusters.MiniClusterType;
 
+import org.junit.Assert;
 import org.junit.Test;
 import static org.junit.Assert.fail;
 
@@ -134,4 +142,45 @@ public void testAlterTablePartitionLocation_alter5() throws Exception {
       fail("One or more queries failed");
     }
   }
+
+  /**
+   * Verify the delta directory name of the load data inpath command for MM acid tables.
+   */
+  @Test
+  public void testAcidLoadDataLocation() throws Exception {
+    String[] testNames = new String[]{"acid_load_data.q"};
+
+    File[] qfiles = setupQFiles(testNames);
+
+    QTestUtil qt = new QTestUtil(QTestArguments.QTestArgumentsBuilder.instance()
+            .withOutDir(resDir + "/llap")
+            .withLogDir(logDir)
+            .withClusterType(MiniClusterType.LLAP_LOCAL)
+            .withConfDir(null)
+            .withInitScript("")
+            .withCleanupScript("")
+            .withLlapIo(false)
+            .build());
+
+    HiveConf hiveConf = qt.getConf();
+    TestTxnDbUtil.setConfValues(hiveConf);
+    TestTxnDbUtil.cleanDb(hiveConf);
+    TestTxnDbUtil.prepDb(hiveConf);
+    qt.postInit();
+    qt.newSession();
+    qt.setInputFile(qfiles[0]);
+    qt.clearTestSideEffects();
+
+    boolean success = QTestRunnerUtils.queryListRunnerSingleThreaded(qfiles, new QTestUtil[]{qt});
+    if (success) {
+      IMetaStoreClient hmsClient = new HiveMetaStoreClient(hiveConf);
+      Table table = hmsClient.getTable("default", "kv_mm");
+      FileSystem fs = FileSystem.get(hiveConf);
+      String location = table.getSd().getLocation();
+      Path delta = fs.listStatus(new Path(location))[0].getPath();
+      Assert.assertEquals("Delta directory name mismatch!", "delta_0000001_0000001_0000", delta.getName());
+    } else {
+      fail("One or more queries failed");
+    }
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java b/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java
index 24ab354ebf..21f591ba92 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java
@@ -210,8 +210,10 @@ Set<FileSinkDesc> getAcidSinks() {
    * @param path
    * @return The statementId from the FileSinkOperator with the given writeId, moveTaskId, operation and path.
    * -1 if there are multiple FileSinkOperators with the same value of these parameters.
+   * The original statement id if there were no matching acid sinks.
    */
-  public Integer getStatementIdForAcidWriteType(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {
+  public Integer getStatementIdForAcidWriteType(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path,
+                                                int originalStatementId) {
     FileSinkDesc result = null;
     for (FileSinkDesc acidSink : acidSinks) {
       if (acidOperation.equals(acidSink.getAcidOperation()) && path.equals(acidSink.getDestPath())
@@ -226,7 +228,9 @@ public Integer getStatementIdForAcidWriteType(long writeId, String moveTaskId, A
     if (result != null) {
       return result.getStatementId();
     } else {
-      return -1;
+      // If there were no matching acid sinks proceed with the original statement id. This can happen, if we used the
+      // load data inpath command on an insert only table.
+      return originalStatementId;
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index 19fcdd3f88..f0e895091e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -422,7 +422,7 @@ public int execute() {
           int statementId = tbd.getStmtId();
           if (tbd.isDirectInsert() || tbd.isMmTable()) {
             statementId = queryPlan.getStatementIdForAcidWriteType(work.getLoadTableWork().getWriteId(),
-                tbd.getMoveTaskId(), work.getLoadTableWork().getWriteType(), tbd.getSourcePath());
+                tbd.getMoveTaskId(), work.getLoadTableWork().getWriteType(), tbd.getSourcePath(), statementId);
             LOG.debug("The statementId used when loading the dynamic partitions is " + statementId);
           }
 
@@ -566,7 +566,7 @@ private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,
     int statementId = tbd.getStmtId();
     if (tbd.isDirectInsert() || tbd.isMmTable()) {
       statementId = queryPlan.getStatementIdForAcidWriteType(work.getLoadTableWork().getWriteId(),
-          tbd.getMoveTaskId(), work.getLoadTableWork().getWriteType(), tbd.getSourcePath());
+          tbd.getMoveTaskId(), work.getLoadTableWork().getWriteType(), tbd.getSourcePath(), statementId);
       LOG.debug("The statementId used when loading the dynamic partitions is " + statementId);
     }
     Map<String, List<Path>> dynamicPartitionSpecs = null;
diff --git a/ql/src/test/queries/clientpositive/acid_load_data.q b/ql/src/test/queries/clientpositive/acid_load_data.q
new file mode 100644
index 0000000000..d162c80ce0
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/acid_load_data.q
@@ -0,0 +1,6 @@
+set hive.mapred.mode=nonstrict;
+set hive.support.concurrency=true;
+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
+
+CREATE TABLE kv_mm(key int, val string) tblproperties ("transactional"="true", "transactional_properties"="insert_only");
+LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE kv_mm;
\ No newline at end of file
diff --git a/ql/src/test/results/clientpositive/llap/acid_load_data.q.out b/ql/src/test/results/clientpositive/llap/acid_load_data.q.out
new file mode 100644
index 0000000000..6b1c70e9b0
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/acid_load_data.q.out
@@ -0,0 +1,16 @@
+PREHOOK: query: CREATE TABLE kv_mm(key int, val string) tblproperties ("transactional"="true", "transactional_properties"="insert_only")
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@kv_mm
+POSTHOOK: query: CREATE TABLE kv_mm(key int, val string) tblproperties ("transactional"="true", "transactional_properties"="insert_only")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@kv_mm
+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE kv_mm
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@kv_mm
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE kv_mm
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@kv_mm
