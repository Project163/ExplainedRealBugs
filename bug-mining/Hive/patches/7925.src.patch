diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
index b5c3424730..b2e5f414a8 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
@@ -131,14 +131,15 @@ public void testCreateFunctionOnHDFSIncrementalReplication() throws Throwable {
     Path identityUdfLocalPath = new Path("../../data/files/identity_udf.jar");
     Path identityUdf1HdfsPath = new Path(primary.functionsRoot, "idFunc1" + File.separator + "identity_udf1.jar");
     Path identityUdf2HdfsPath = new Path(primary.functionsRoot, "idFunc2" + File.separator + "identity_udf2.jar");
+    List<String> withClause = Arrays.asList("'" + HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname + "'='false'");
     setupUDFJarOnHDFS(identityUdfLocalPath, identityUdf1HdfsPath);
     setupUDFJarOnHDFS(identityUdfLocalPath, identityUdf2HdfsPath);
 
     primary.run("CREATE FUNCTION " + primaryDbName
             + ".idFunc1 as 'IdentityStringUDF' "
             + "using jar  '" + identityUdf1HdfsPath.toString() + "'");
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
-    replica.load(replicatedDbName, primaryDbName)
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, withClause);
+    replica.load(replicatedDbName, primaryDbName, withClause)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId)
             .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
@@ -146,13 +147,14 @@ public void testCreateFunctionOnHDFSIncrementalReplication() throws Throwable {
             .run("SELECT " + replicatedDbName + ".idFunc1('MyName')")
             .verifyResults(new String[] { "MyName"});
 
+    assertFunctionJarsOnTarget("idFunc1", Arrays.asList("identity_udf1.jar"));
     primary.run("CREATE FUNCTION " + primaryDbName
             + ".idFunc2 as 'IdentityStringUDF' "
             + "using jar  '" + identityUdf2HdfsPath.toString() + "'");
 
     WarehouseInstance.Tuple incrementalDump =
-            primary.dump(primaryDbName);
-    replica.load(replicatedDbName, primaryDbName)
+            primary.dump(primaryDbName, withClause);
+    replica.load(replicatedDbName, primaryDbName, withClause)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(incrementalDump.lastReplicationId)
             .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
@@ -160,6 +162,9 @@ public void testCreateFunctionOnHDFSIncrementalReplication() throws Throwable {
                     replicatedDbName + ".idFunc2" })
             .run("SELECT " + replicatedDbName + ".idFunc2('YourName')")
             .verifyResults(new String[] { "YourName"});
+
+    assertFunctionJarsOnTarget("idFunc1", Arrays.asList("identity_udf1.jar"));
+    assertFunctionJarsOnTarget("idFunc2", Arrays.asList("identity_udf2.jar"));
   }
 
   @Test
@@ -169,13 +174,13 @@ public void testCreateFunctionOnHDFSIncrementalReplicationLazyCopy() throws Thro
     Path identityUdf2HdfsPath = new Path(primary.functionsRoot, "idFunc2" + File.separator + "identity_udf2.jar");
     setupUDFJarOnHDFS(identityUdfLocalPath, identityUdf1HdfsPath);
     setupUDFJarOnHDFS(identityUdfLocalPath, identityUdf2HdfsPath);
-    List<String> withClasuse = Arrays.asList("'" + HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname + "'='true'");
+    List<String> withClause = Arrays.asList("'" + HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname + "'='true'");
 
     primary.run("CREATE FUNCTION " + primaryDbName
             + ".idFunc1 as 'IdentityStringUDF' "
             + "using jar  '" + identityUdf1HdfsPath.toString() + "'");
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, withClasuse);
-    replica.load(replicatedDbName, primaryDbName, withClasuse)
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, withClause);
+    replica.load(replicatedDbName, primaryDbName, withClause)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId)
             .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
@@ -183,13 +188,15 @@ public void testCreateFunctionOnHDFSIncrementalReplicationLazyCopy() throws Thro
             .run("SELECT " + replicatedDbName + ".idFunc1('MyName')")
             .verifyResults(new String[] { "MyName"});
 
+    assertFunctionJarsOnTarget("idFunc1", Arrays.asList("identity_udf1.jar"));
+
     primary.run("CREATE FUNCTION " + primaryDbName
             + ".idFunc2 as 'IdentityStringUDF' "
             + "using jar  '" + identityUdf2HdfsPath.toString() + "'");
 
     WarehouseInstance.Tuple incrementalDump =
-            primary.dump(primaryDbName, withClasuse);
-    replica.load(replicatedDbName, primaryDbName, withClasuse)
+            primary.dump(primaryDbName, withClause);
+    replica.load(replicatedDbName, primaryDbName, withClause)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(incrementalDump.lastReplicationId)
             .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
@@ -197,6 +204,9 @@ public void testCreateFunctionOnHDFSIncrementalReplicationLazyCopy() throws Thro
                     replicatedDbName + ".idFunc2" })
             .run("SELECT " + replicatedDbName + ".idFunc2('YourName')")
             .verifyResults(new String[] { "YourName"});
+
+    assertFunctionJarsOnTarget("idFunc1", Arrays.asList("identity_udf1.jar"));
+    assertFunctionJarsOnTarget("idFunc2", Arrays.asList("identity_udf2.jar"));
   }
 
   @Test
@@ -328,29 +338,36 @@ public void testBootstrapFunctionReplication() throws Throwable {
   public void testCreateFunctionWithFunctionBinaryJarsOnHDFS() throws Throwable {
     Dependencies dependencies = dependencies("ivy://io.github.myui:hivemall:0.4.0-2", primary);
     String jarSubString = dependencies.toJarSubSql();
+    List<String> withClause = Arrays.asList("'" + HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname + "'='false'");
 
     primary.run("CREATE FUNCTION " + primaryDbName
         + ".anotherFunction as 'hivemall.tools.string.StopwordUDF' "
         + "using " + jarSubString);
 
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, withClause);
 
-    replica.load(replicatedDbName, primaryDbName)
+    replica.load(replicatedDbName, primaryDbName, withClause)
         .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
         .verifyResult(replicatedDbName + ".anotherFunction");
+    assertFunctionJarsOnTarget("anotherFunction", dependencies.jarNames());
+  }
+
+  @Test
+  public void testBootstrapFunctionOnHDFSLazyCopy() throws Throwable {
+    Dependencies dependencies = dependencies("ivy://io.github.myui:hivemall:0.4.0-2", primary);
+    String jarSubString = dependencies.toJarSubSql();
+    List<String> withClause = Arrays.asList("'" + HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname + "'='true'");
 
-    FileStatus[] fileStatuses = replica.miniDFSCluster.getFileSystem().globStatus(
-        new Path(
-            replica.functionsRoot + "/" + replicatedDbName.toLowerCase() + "/anotherfunction/*/*")
-        , path -> path.toString().endsWith("jar"));
-    List<String> expectedDependenciesNames = dependencies.jarNames();
-    assertThat(fileStatuses.length, is(equalTo(expectedDependenciesNames.size())));
-    List<String> jars = Arrays.stream(fileStatuses).map(f -> {
-      String[] splits = f.getPath().toString().split("/");
-      return splits[splits.length - 1];
-    }).collect(Collectors.toList());
-
-    assertThat(jars, containsInAnyOrder(expectedDependenciesNames.toArray()));
+    primary.run("CREATE FUNCTION " + primaryDbName
+            + ".anotherFunction as 'hivemall.tools.string.StopwordUDF' "
+            + "using " + jarSubString);
+
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, withClause);
+
+    replica.load(replicatedDbName, primaryDbName, withClause)
+            .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
+            .verifyResult(replicatedDbName + ".anotherFunction");
+    assertFunctionJarsOnTarget("anotherFunction", dependencies.jarNames());
   }
 
   @Test
@@ -372,19 +389,7 @@ public void testIncrementalCreateFunctionWithFunctionBinaryJarsOnHDFS() throws T
     replica.load(replicatedDbName, primaryDbName)
             .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
             .verifyResult(replicatedDbName + ".anotherFunction");
-
-    FileStatus[] fileStatuses = replica.miniDFSCluster.getFileSystem().globStatus(
-            new Path(
-                    replica.functionsRoot + "/" + replicatedDbName.toLowerCase() + "/anotherfunction/*/*")
-            , path -> path.toString().endsWith("jar"));
-    List<String> expectedDependenciesNames = dependencies.jarNames();
-    assertThat(fileStatuses.length, is(equalTo(expectedDependenciesNames.size())));
-    List<String> jars = Arrays.stream(fileStatuses).map(f -> {
-        String[] splits = f.getPath().toString().split("/");
-        return splits[splits.length - 1];
-    }).collect(Collectors.toList());
-
-    assertThat(jars, containsInAnyOrder(expectedDependenciesNames.toArray()));
+    assertFunctionJarsOnTarget("anotherFunction", dependencies.jarNames());
   }
 
   static class Dependencies {
@@ -2277,4 +2282,23 @@ private List<String> getHdfsNameserviceClause() {
             + NS_REMOTE + "'");
     return withClause;
   }
+
+  private void assertFunctionJarsOnTarget(String functionName, List<String> expectedJars) throws IOException {
+    //correct location of jars on target is functionRoot/dbName/funcName/nanoTs/jarFile
+    FileStatus[] fileStatuses = replica.miniDFSCluster.getFileSystem()
+            .globStatus(new Path(replica.functionsRoot + "/" +
+                    replicatedDbName.toLowerCase() + "/" + functionName.toLowerCase() + "/*/*")
+            );
+    assertEquals(fileStatuses.length, expectedJars.size());
+    List<String> jars = new ArrayList<>();
+    for (FileStatus fileStatus : fileStatuses) {
+      jars.add(fileStatus.getPath().getName());
+    }
+    assertThat(jars, containsInAnyOrder(expectedJars.toArray()));
+
+    //confirm no jars created as directories
+    for (FileStatus jarStatus : fileStatuses) {
+      assert(!jarStatus.isDirectory());
+    }
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/show/ShowCreateTableOperation.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/show/ShowCreateTableOperation.java
index 33277a3f61..665a801c8d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/show/ShowCreateTableOperation.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/show/ShowCreateTableOperation.java
@@ -232,8 +232,9 @@ private String getPartitionsForView(Table table) {
       return "";
     }
     List<String> partitionCols = new ArrayList<String>();
-    for(String col:table.getPartColNames())
-      partitionCols.add('`' + col +'`');
+    for(String col:table.getPartColNames()) {
+      partitionCols.add('`' + col + '`');
+    }
     return " PARTITIONED ON (" + StringUtils.join(partitionCols, ", ") + ")";
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java
index b934ca4af4..d009541478 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java
@@ -206,7 +206,8 @@ private Task<?> getCopyTask(String sourceUri, Path dest) {
         return ReplCopyTask.getLoadCopyTask(metadata.getReplicationSpec(), new Path(sourceUri), dest, context.hiveConf,
                 context.getDumpDirectory(), context.getMetricCollector());
       } else {
-        return TaskFactory.get(new CopyWork(new Path(sourceUri), dest, true, false,
+        //CopyTask expects the destination directory, hence we pass the parent of the actual destination path
+        return TaskFactory.get(new CopyWork(new Path(sourceUri), dest.getParent(), true, false,
                 context.getDumpDirectory(), context.getMetricCollector(), true), context.hiveConf);
       }
     }
