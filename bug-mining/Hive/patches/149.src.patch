diff --git a/CHANGES.txt b/CHANGES.txt
index 1420cb794a..3783e1d026 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -132,6 +132,9 @@ Trunk - Unreleased
     HIVE-486. Fix concurrent modification exception in case of empty partitions
     (Namit Jain via prasadc)
 
+    HIVE-485. Fix join not to assume all columns are strings.
+    (Namit Jain via zshao)
+
 Release 0.3.1 - Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
index fcd2276771..4543a0fea5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
@@ -39,8 +39,8 @@ public class CollectOperator extends Operator <collectDesc> implements Serializa
   transient protected ObjectInspector standardRowInspector;
   transient int maxSize;
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     rowList = new ArrayList<Object> ();
     maxSize = conf.getBufferSize().intValue();
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
index cdbfe8cf09..7a6d472aaf 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
@@ -54,7 +54,7 @@ public void map(Object key, Object value,
       try {
         oc = output;
         mo.setOutputCollector(oc);
-        mo.initialize(jc, reporter);
+        mo.initialize(jc, reporter, null);
         rp = reporter;
       } catch (HiveException e) {
         abort = true;
@@ -81,7 +81,7 @@ public void close() {
     if(oc == null) {
       try {
         l4j.trace("Close called no row");
-        mo.initialize(jc, null);
+        mo.initialize(jc, null, null);
         rp = null;
       } catch (HiveException e) {
         abort = true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
index cbc3cbd1c3..9ed8798839 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
@@ -116,7 +116,7 @@ public void reduce(Object key, Iterator values,
       try {
         oc = output;
         reducer.setOutputCollector(oc);
-        reducer.initialize(jc, reporter);
+        reducer.initialize(jc, reporter, rowObjectInspector);
         rp = reporter;
       } catch (HiveException e) {
         abort = true;
@@ -196,7 +196,7 @@ public void close() {
     if(oc == null) {
       try {
         l4j.trace("Close called no row");
-        reducer.initialize(jc, null);
+        reducer.initialize(jc, null, rowObjectInspector);
         rp = null;
       } catch (HiveException e) {
         abort = true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
index 2dfbd47e6d..3f6a6ae5f3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
@@ -35,8 +35,8 @@ public class ExtractOperator extends Operator<extractDesc> implements Serializab
   private static final long serialVersionUID = 1L;
   transient protected ExprNodeEvaluator eval;
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     eval = ExprNodeEvaluatorFactory.get(conf.getCol());
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index 8532c7cfc6..0e6f51f756 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -39,6 +39,8 @@
 import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 
+import com.sun.corba.se.pept.encoding.InputObject;
+
 /**
  * File Sink operator implementation
  **/
@@ -94,8 +96,8 @@ public void close(boolean abort) throws HiveException {
     }
   }
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     
     try {
       serializer = (Serializer)conf.getTableInfo().getDeserializerClass().newInstance();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
index 2adbeed8dd..d49e551800 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
@@ -47,8 +47,8 @@ public FilterOperator () {
     passed_count = new LongWritable();
   }
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     try {
       this.conditionEvaluator = ExprNodeEvaluatorFactory.get(conf.getPredicate());
       statsMap.put(Counter.FILTERED, filtered_count);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
index 6d1eef476f..5b7fb79cce 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
@@ -32,8 +32,8 @@
  **/
 public class ForwardOperator extends  Operator<forwardDesc>  implements Serializable {
   private static final long serialVersionUID = 1L;
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     // nothing to do really ..
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
index 090e6a8183..6d4436a839 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
@@ -131,8 +131,8 @@ List<Field> getFields() {
   transient int           numEntriesVarSize;
   transient int           numEntriesHashTable;
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     totalMemory = Runtime.getRuntime().totalMemory();
     numRowsInput = 0;
     numRowsHashTbl = 0;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
index 1a9c653987..1228a67ff9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
@@ -41,6 +41,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -120,8 +121,8 @@ public void popObj() {
   int nextSz = 0;
   transient Byte lastAlias = null;
   
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     
     totalSz = 0;
     // Map that contains the rows for each alias
@@ -156,10 +157,14 @@ public void initialize(Configuration hconf, Reporter reporter) throws HiveExcept
 
     ArrayList<ObjectInspector> structFieldObjectInspectors = new ArrayList<ObjectInspector>(
         totalSz);
-    for (int i = 0; i < totalSz; i++) {
-      structFieldObjectInspectors.add(PrimitiveObjectInspectorFactory
-          .writableStringObjectInspector);
+
+    for (Byte alias : order) {
+      int sz = map.get(alias).size();
+      StructObjectInspector fldObjIns = (StructObjectInspector)((StructObjectInspector)inputObjInspector[alias.intValue()]).getStructFieldRef("VALUE").getFieldObjectInspector();
+      for (int i = 0; i < sz; i++)
+        structFieldObjectInspectors.add(fldObjIns.getAllStructFieldRefs().get(i).getFieldObjectInspector());
     }
+    
     joinOutputObjectInspector = ObjectInspectorFactory
         .getStandardStructObjectInspector(ObjectInspectorUtils
             .getIntegerArray(totalSz), structFieldObjectInspectors);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
index d041e932f3..4d22cc7c2a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
@@ -39,8 +39,8 @@ public class LimitOperator extends Operator<limitDesc> implements Serializable {
   transient protected int limit;
   transient protected int currCount;
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     limit = conf.getLimit();
     currCount = 0;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
index bffd72524d..b8a8d03c11 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
@@ -64,8 +64,8 @@ public static enum Counter {DESERIALIZE_ERRORS}
   transient private List<ObjectInspector> partObjectInspectors;
   
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     Path fpath = new Path((new Path (HiveConf.getVar(hconf, HiveConf.ConfVars.HADOOPMAPFILENAME))).toUri().getPath());
     ArrayList<Operator<? extends Serializable>> todo = new ArrayList<Operator<? extends Serializable>> ();
     statsMap.put(Counter.DESERIALIZE_ERRORS, deserialize_error_count);
@@ -171,7 +171,7 @@ public void initialize(Configuration hconf, Reporter reporter) throws HiveExcept
     this.setOutputCollector(out);
 
     for(Operator op: todo) {
-      op.initialize(hconf, reporter);
+      op.initialize(hconf, reporter, inputObjInspector);
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
index db80032488..c9fae8a0af 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
@@ -240,7 +240,7 @@ public Map<Enum<?>, Long> getStats() {
     return(ret);
   }
 
-  public void initialize (Configuration hconf, Reporter reporter) throws HiveException {
+  public void initialize (Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
     if (state == state.INIT) {
       LOG.info("Already Initialized");
       return;
@@ -254,7 +254,7 @@ public void initialize (Configuration hconf, Reporter reporter) throws HiveExcep
     }
     LOG.info("Initializing children:");
     for(Operator<? extends Serializable> op: childOperators) {
-      op.initialize(hconf, reporter);
+      op.initialize(hconf, reporter, inputObjInspector);
     }    
 
     state = State.INIT;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
index 85b7b43cd0..8d093c236b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
@@ -69,8 +69,8 @@ public class ReduceSinkOperator extends TerminalOperator <reduceSinkDesc> implem
   transient int tag;
   transient byte[] tagByte = new byte[1];
   
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     try {
       keyEval = new ExprNodeEvaluator[conf.getKeyCols().size()];
       int i=0;
@@ -182,7 +182,7 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
         for(ExprNodeEvaluator e: partitionEval) {
           Object o = e.evaluate(row);
           keyHashCode = keyHashCode * 31 
-            + (o == null ? 0 : o.hashCode());
+            + (o == null ? 0 : Utilities.hashCode(o, keyObjectInspector));
         }
       }
       keyWritable.setHashCode(keyHashCode);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
index 933f440b60..a8e68b3caf 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
@@ -162,8 +162,8 @@ public File getAbsolutePath(String filename)
     }
   }
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     statsMap.put(Counter.DESERIALIZE_ERRORS, deserialize_error_count);
     statsMap.put(Counter.SERIALIZE_ERRORS, serialize_error_count);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
index 8fb35b17d6..255fea8930 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
@@ -43,8 +43,8 @@ public class SelectOperator extends Operator <selectDesc> implements Serializabl
   
   boolean firstRow;
   
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
 
     ArrayList<exprNodeDesc> colList = conf.getColList();
     eval = new ExprNodeEvaluator[colList.size()];
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
index 11897c3dd7..411520a3df 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
@@ -33,8 +33,8 @@
  **/
 public class TableScanOperator extends Operator<tableScanDesc> implements Serializable {
   private static final long serialVersionUID = 1L;
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     // nothing to do really ..
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 1819328956..41da193317 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -35,7 +35,12 @@
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.io.compress.CompressionCodec;
 import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.BooleanWritable;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.compress.DefaultCodec;
 import org.apache.hadoop.util.ReflectionUtils;
 
@@ -45,6 +50,12 @@
 import org.apache.hadoop.hive.ql.io.RCFile;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.serde2.io.ByteWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.mapred.FileOutputFormat;
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
@@ -615,4 +626,35 @@ public static String getNameMessage(Exception e) {
     return e.getClass().getName() + "(" +  e.getMessage() + ")";
   }
 
+  public static int hashCode(Object o, ObjectInspector objIns) {
+    if ((objIns.getCategory() == Category.PRIMITIVE) &&
+        ((PrimitiveObjectInspector)objIns).isWritable())
+      return o.hashCode();
+        
+    if (o instanceof String)
+      return (new Text((String)o)).hashCode();
+    
+    if (o instanceof Integer) 
+      return (new IntWritable(((Integer)o).intValue())).hashCode();
+   
+    if (o instanceof Short)
+      return (new ShortWritable(((Short)o).shortValue())).hashCode();
+    
+    if (o instanceof Float)
+      return (new FloatWritable(((Float)o).floatValue())).hashCode();
+    
+    if (o instanceof Long)
+      return (new LongWritable(((Long)o).longValue())).hashCode();
+ 
+    if (o instanceof Boolean)
+      return (new BooleanWritable(((Boolean)o).booleanValue())).hashCode();
+   
+    if (o instanceof Double)
+      return (new DoubleWritable(((Double)o).doubleValue())).hashCode();
+    
+    if (o instanceof Byte)
+      return (new ByteWritable(((Byte)o).byteValue())).hashCode();
+    
+    return o.hashCode();
+  }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
index 2ee39eaead..8c1858a159 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
@@ -89,7 +89,7 @@ public void testBaseFilterOperator() throws Throwable {
       op.setConf(filterCtx);
 
       // runtime initialization
-      op.initialize(null, null);
+      op.initialize(null, null, null);
 
       for(InspectableObject oner: r) {
         op.process(oner.o, oner.oi, 0);
@@ -139,7 +139,7 @@ public void testFileSinkOperator() throws Throwable {
                                            Utilities.defaultTd, false);
       Operator<fileSinkDesc> flop = OperatorFactory.getAndMakeChild(fsd, op);
       
-      op.initialize(new JobConf(TestOperators.class), Reporter.NULL);
+      op.initialize(new JobConf(TestOperators.class), Reporter.NULL, null);
 
       // evaluate on row
       for(int i=0; i<5; i++) {
@@ -185,7 +185,7 @@ public void testScriptOperator() throws Throwable {
       collectDesc cd = new collectDesc (Integer.valueOf(10));
       CollectOperator cdop = (CollectOperator) OperatorFactory.getAndMakeChild(cd, sop);
 
-      op.initialize(new JobConf(TestOperators.class), null);
+      op.initialize(new JobConf(TestOperators.class), null, null);
 
       // evaluate on row
       for(int i=0; i<5; i++) {
@@ -257,7 +257,7 @@ public void testMapOperator() throws Throwable {
       // get map operator and initialize it
       MapOperator mo = new MapOperator();
       mo.setConf(mrwork);
-      mo.initialize(hconf, null);
+      mo.initialize(hconf, null, null);
 
       Text tw = new Text();
       InspectableObject io1 = new InspectableObject();
diff --git a/ql/src/test/queries/clientpositive/join24.q b/ql/src/test/queries/clientpositive/join24.q
new file mode 100644
index 0000000000..2e5477a9ce
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/join24.q
@@ -0,0 +1,8 @@
+create table tst1(key STRING, cnt INT);
+
+INSERT OVERWRITE TABLE tst1
+SELECT a.key, count(1) FROM src a group by a.key;
+
+SELECT sum(a.cnt)  FROM tst1 a JOIN tst1 b ON a.key = b.key;
+
+drop table tst1;
diff --git a/ql/src/test/results/clientpositive/join24.q.out b/ql/src/test/results/clientpositive/join24.q.out
new file mode 100644
index 0000000000..ca93d7279e
--- /dev/null
+++ b/ql/src/test/results/clientpositive/join24.q.out
@@ -0,0 +1,10 @@
+query: create table tst1(key STRING, cnt INT)
+query: INSERT OVERWRITE TABLE tst1
+SELECT a.key, count(1) FROM src a group by a.key
+Input: default/src
+Output: default/tst1
+query: SELECT sum(a.cnt)  FROM tst1 a JOIN tst1 b ON a.key = b.key
+Input: default/tst1
+Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/148610721/785047285.10000
+500.0
+query: drop table tst1
