diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeGenericFuncEvaluator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeGenericFuncEvaluator.java
index bb5f4f3ae4..97ee9760ac 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeGenericFuncEvaluator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeGenericFuncEvaluator.java
@@ -45,6 +45,7 @@ public class ExprNodeGenericFuncEvaluator extends ExprNodeEvaluator<ExprNodeGene
   transient ExprNodeEvaluator[] children;
   transient GenericUDF.DeferredObject[] deferredChildren;
   transient boolean isEager;
+  transient boolean isConstant = false;
 
   /**
    * Class to allow deferred evaluation for GenericUDF.
@@ -124,7 +125,10 @@ public ObjectInspector initialize(ObjectInspector rowInspector) throws HiveExcep
     if (context != null) {
       context.setup(genericUDF);
     }
-    return outputOI = genericUDF.initializeAndFoldConstants(childrenOIs);
+    outputOI = genericUDF.initializeAndFoldConstants(childrenOIs);
+    isConstant = ObjectInspectorUtils.isConstantObjectInspector(outputOI)
+        && isDeterministic();
+    return outputOI;
   }
 
   @Override
@@ -154,12 +158,11 @@ public boolean isStateful() {
 
   @Override
   protected Object _evaluate(Object row, int version) throws HiveException {
-    rowObject = row;
-    if (ObjectInspectorUtils.isConstantObjectInspector(outputOI) &&
-        isDeterministic()) {
+    if (isConstant) {
       // The output of this UDF is constant, so don't even bother evaluating.
-      return ((ConstantObjectInspector)outputOI).getWritableConstantValue();
+      return ((ConstantObjectInspector) outputOI).getWritableConstantValue();
     }
+    rowObject = row;
     for (int i = 0; i < deferredChildren.length; i++) {
       deferredChildren[i].prepare(version);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
index 792d87fc1c..4eb95477c8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
@@ -77,6 +77,7 @@ public class GroupByOperator extends Operator<GroupByDesc> implements
 
   private static final Log LOG = LogFactory.getLog(GroupByOperator.class
       .getName());
+  private static final boolean isTraceEnabled = LOG.isTraceEnabled();
   private static final long serialVersionUID = 1L;
   private static final int NUMROWSESTIMATESIZE = 1000;
 
@@ -101,6 +102,7 @@ public class GroupByOperator extends Operator<GroupByDesc> implements
   transient ExprNodeEvaluator unionExprEval = null;
 
   transient GenericUDAFEvaluator[] aggregationEvaluators;
+  transient boolean[] estimableAggregationEvaluators;
 
   protected transient ArrayList<ObjectInspector> objectInspectors;
   transient ArrayList<String> fieldNames;
@@ -557,11 +559,13 @@ private void estimateRowSize() throws HiveException {
     // Go over all the aggregation classes and and get the size of the fields of
     // fixed length. Keep track of the variable length
     // fields in these aggregation classes.
+    estimableAggregationEvaluators = new boolean[aggregationEvaluators.length];
     for (int i = 0; i < aggregationEvaluators.length; i++) {
 
       fixedRowSize += javaObjectOverHead;
       AggregationBuffer agg = aggregationEvaluators[i].getNewAggregationBuffer();
       if (GenericUDAFEvaluator.isEstimable(agg)) {
+        estimableAggregationEvaluators[i] = true;
         continue;
       }
       Field[] fArr = ObjectInspectorUtils.getDeclaredNonStaticFields(agg.getClass());
@@ -765,10 +769,12 @@ public void processOp(Object row, int tag) throws HiveException {
           flushHashTable(true);
           hashAggr = false;
         } else {
-          LOG.trace("Hash Aggr Enabled: #hash table = " + numRowsHashTbl
-              + " #total = " + numRowsInput + " reduction = " + 1.0
-              * (numRowsHashTbl / numRowsInput) + " minReduction = "
-              + minReductionHashAggr);
+          if (isTraceEnabled) {
+            LOG.trace("Hash Aggr Enabled: #hash table = " + numRowsHashTbl
+                + " #total = " + numRowsInput + " reduction = " + 1.0
+                * (numRowsHashTbl / numRowsInput) + " minReduction = "
+                + minReductionHashAggr);
+          }
         }
       }
     }
@@ -952,7 +958,7 @@ private boolean shouldBeFlushed(KeyWrapper newKeys) {
       AggregationBuffer[] aggs = hashAggregations.get(newKeys);
       for (int i = 0; i < aggs.length; i++) {
         AggregationBuffer agg = aggs[i];
-        if (GenericUDAFEvaluator.isEstimable(agg)) {
+        if (estimableAggregationEvaluators[i]) {
           totalVariableSize += ((GenericUDAFEvaluator.AbstractAggregationBuffer)agg).estimate();
           continue;
         }
@@ -966,8 +972,10 @@ private boolean shouldBeFlushed(KeyWrapper newKeys) {
       // Update the number of entries that can fit in the hash table
       numEntriesHashTable =
           (int) (maxHashTblMemory / (fixedRowSize + (totalVariableSize / numEntriesVarSize)));
-      LOG.trace("Hash Aggr: #hash table = " + numEntries
-          + " #max in hash table = " + numEntriesHashTable);
+      if (isTraceEnabled) {
+        LOG.trace("Hash Aggr: #hash table = " + numEntries
+            + " #max in hash table = " + numEntriesHashTable);
+      }
     }
 
     // flush if necessary
