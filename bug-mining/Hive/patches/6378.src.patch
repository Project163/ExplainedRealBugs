diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidGroupByQueryRecordReader.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidGroupByQueryRecordReader.java
index 765f1cb7e7..00a4b72a34 100644
--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidGroupByQueryRecordReader.java
+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidGroupByQueryRecordReader.java
@@ -17,31 +17,29 @@
  */
 package org.apache.hadoop.hive.druid.serde;
 
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.stream.Collectors;
-
+import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.JavaType;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import com.metamx.http.client.HttpClient;
 import io.druid.data.input.MapBasedRow;
+import io.druid.data.input.Row;
 import io.druid.query.dimension.DimensionSpec;
 import io.druid.query.dimension.ExtractionDimensionSpec;
 import io.druid.query.extraction.TimeFormatExtractionFn;
+import io.druid.query.groupby.GroupByQuery;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.druid.DruidStorageHandlerUtils;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.mapreduce.InputSplit;
-
-import com.fasterxml.jackson.core.type.TypeReference;
-
-import io.druid.data.input.Row;
-import io.druid.query.groupby.GroupByQuery;
 import org.joda.time.format.ISODateTimeFormat;
 
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
 import static org.apache.hadoop.hive.druid.serde.DruidSerDeUtils.ISO_TIME_FORMAT;
 
 /**
@@ -133,7 +131,7 @@ public DruidWritable getCurrentValue() throws IOException, InterruptedException
     DruidWritable value = new DruidWritable();
     // 1) The timestamp column
     value.getValue().put(DruidStorageHandlerUtils.EVENT_TIMESTAMP_COLUMN,
-        currentRow.getTimestamp().getMillis()
+        currentRow.getTimestamp() == null ? null : currentRow.getTimestamp().getMillis()
     );
     // 2) The dimension columns
     value.getValue().putAll(currentEvent);
@@ -147,7 +145,7 @@ public boolean next(NullWritable key, DruidWritable value) {
       value.getValue().clear();
       // 1) The timestamp column
       value.getValue().put(DruidStorageHandlerUtils.EVENT_TIMESTAMP_COLUMN,
-          currentRow.getTimestamp().getMillis()
+          currentRow.getTimestamp() == null ? null : currentRow.getTimestamp().getMillis()
       );
       // 2) The dimension columns
       value.getValue().putAll(currentEvent);
diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidTimeseriesQueryRecordReader.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidTimeseriesQueryRecordReader.java
index f07f212b14..d72624800d 100644
--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidTimeseriesQueryRecordReader.java
+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidTimeseriesQueryRecordReader.java
@@ -17,17 +17,15 @@
  */
 package org.apache.hadoop.hive.druid.serde;
 
-import java.io.IOException;
-
-import com.fasterxml.jackson.databind.JavaType;
-import org.apache.hadoop.hive.druid.DruidStorageHandlerUtils;
-import org.apache.hadoop.io.NullWritable;
-
 import com.fasterxml.jackson.core.type.TypeReference;
-
+import com.fasterxml.jackson.databind.JavaType;
 import io.druid.query.Result;
 import io.druid.query.timeseries.TimeseriesQuery;
 import io.druid.query.timeseries.TimeseriesResultValue;
+import org.apache.hadoop.hive.druid.DruidStorageHandlerUtils;
+import org.apache.hadoop.io.NullWritable;
+
+import java.io.IOException;
 
 /**
  * Record reader for results for Druid TimeseriesQuery.
@@ -63,7 +61,7 @@ public DruidWritable getCurrentValue() throws IOException, InterruptedException
     // Create new value
     DruidWritable value = new DruidWritable();
     value.getValue().put(DruidStorageHandlerUtils.EVENT_TIMESTAMP_COLUMN,
-        current.getTimestamp().getMillis()
+        current.getTimestamp() == null ? null : current.getTimestamp().getMillis()
     );
     value.getValue().putAll(current.getValue().getBaseObject());
     return value;
@@ -75,7 +73,7 @@ public boolean next(NullWritable key, DruidWritable value) {
       // Update value
       value.getValue().clear();
       value.getValue().put(DruidStorageHandlerUtils.EVENT_TIMESTAMP_COLUMN,
-          current.getTimestamp().getMillis()
+          current.getTimestamp() == null ? null : current.getTimestamp().getMillis()
       );
       value.getValue().putAll(current.getValue().getBaseObject());
       return true;
