diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 30c24d90bb..907d0641f2 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -1407,7 +1407,7 @@ public static enum ConfVars {
         "while writing a table with ORC file format, enabling this config will do stripe-level\n" +
         "fast merge for small ORC files. Note that enabling this config will not honor the\n" +
         "padding tolerance config (hive.exec.orc.block.padding.tolerance)."),
-    HIVE_ORC_CODEC_POOL("hive.use.orc.codec.pool", true,
+    HIVE_ORC_CODEC_POOL("hive.use.orc.codec.pool", false,
         "Whether to use codec pool in ORC. Disable if there are bugs with codec reuse."),
 
     HIVEUSEEXPLICITRCFILEHEADER("hive.exec.rcfile.use.explicit.header", true,
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
index 458d9269cb..89df943693 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
@@ -22,7 +22,6 @@
 import java.security.PrivilegedExceptionAction;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 
 import org.apache.hadoop.hive.llap.counters.LlapIOCounters;
@@ -81,9 +80,7 @@
 import org.apache.orc.DataReader;
 import org.apache.hadoop.hive.ql.io.orc.OrcFile;
 import org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions;
-import org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater;
 import org.apache.orc.OrcConf;
-import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.io.orc.OrcSplit;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader;
 import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl;
@@ -913,20 +910,17 @@ public void determineStripesToRead() {
   }
 
   private class DataWrapperForOrc implements DataReader, DataCache, BufferObjectFactory {
-    private final DataReader orcDataReader;
-
-    private DataWrapperForOrc(DataWrapperForOrc other) {
-      orcDataReader = other.orcDataReader.clone();
-    }
+    /** A reference to parent DataReader not owned by this object. */
+    private final DataReader orcDataReaderRef;
 
     public DataWrapperForOrc() throws IOException {
       ensureRawDataReader(false);
-      this.orcDataReader = rawDataReader.clone();
+      this.orcDataReaderRef = rawDataReader;
     }
 
     @Override
     public CompressionCodec getCompressionCodec() {
-      return orcDataReader.getCompressionCodec();
+      return orcDataReaderRef.getCompressionCodec();
     }
 
     @Override
@@ -974,14 +968,14 @@ public Allocator getAllocator() {
 
     @Override
     public void close() throws IOException {
-      orcDataReader.close();
+      // Noop: orcDataReaderRef is owned by the parent object
     }
 
     @Override
     public DiskRangeList readFileData(DiskRangeList range, long baseOffset,
         boolean doForceDirect) throws IOException {
       long startTime = counters.startTimeCounter();
-      DiskRangeList result = orcDataReader.readFileData(range, baseOffset, doForceDirect);
+      DiskRangeList result = orcDataReaderRef.readFileData(range, baseOffset, doForceDirect);
       counters.recordHdfsTime(startTime);
       if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
         LlapIoImpl.ORC_LOGGER.trace("Disk ranges after disk read (file {}, base offset {}): {}",
@@ -993,23 +987,23 @@ public DiskRangeList readFileData(DiskRangeList range, long baseOffset,
 
     @Override
     public boolean isTrackingDiskRanges() {
-      return orcDataReader.isTrackingDiskRanges();
+      return orcDataReaderRef.isTrackingDiskRanges();
     }
 
     @Override
     public void releaseBuffer(ByteBuffer buffer) {
-      orcDataReader.releaseBuffer(buffer);
+      orcDataReaderRef.releaseBuffer(buffer);
     }
 
     @Override
     public DataWrapperForOrc clone() {
-      return new DataWrapperForOrc(this);
+      throw new AssertionError("Clone not supported");
     }
 
     @Override
     public void open() throws IOException {
       long startTime = counters.startTimeCounter();
-      orcDataReader.open();
+      orcDataReaderRef.open();
       counters.recordHdfsTime(startTime);
     }
 
@@ -1025,14 +1019,14 @@ public OrcIndex readRowIndex(StripeInformation stripe,
                                  OrcProto.Stream.Kind[] bloomFilterKinds,
                                  OrcProto.BloomFilterIndex[] bloomFilterIndices
                                  ) throws IOException {
-      return orcDataReader.readRowIndex(stripe, fileSchema, footer,
+      return orcDataReaderRef.readRowIndex(stripe, fileSchema, footer,
           ignoreNonUtf8BloomFilter, included, indexes,
           sargColumns, version, bloomFilterKinds, bloomFilterIndices);
     }
 
     @Override
     public OrcProto.StripeFooter readStripeFooter(StripeInformation stripe) throws IOException {
-      return orcDataReader.readStripeFooter(stripe);
+      return orcDataReaderRef.readStripeFooter(stripe);
     }
 
     @Override
