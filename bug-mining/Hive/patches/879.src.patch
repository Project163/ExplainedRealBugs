diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 8d4cf72c5c..e0a4e01770 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -3891,6 +3891,7 @@ private Operator genBucketingSortingDest(String dest, Operator input, QB qb, Tab
     ArrayList<ExprNodeDesc> partnCols = new ArrayList<ExprNodeDesc>();
     ArrayList<ExprNodeDesc> partnColsNoConvert = new ArrayList<ExprNodeDesc>();
     ArrayList<ExprNodeDesc> sortCols  = new ArrayList<ExprNodeDesc>();
+    ArrayList<Integer> sortOrders = new ArrayList<Integer>();
     boolean multiFileSpray = false;
     int     numFiles = 1;
     int     totalFiles = 1;
@@ -3907,6 +3908,7 @@ private Operator genBucketingSortingDest(String dest, Operator input, QB qb, Tab
         (conf.getBoolVar(HiveConf.ConfVars.HIVEENFORCESORTING))) {
       enforceSorting = true;
       sortCols = getSortCols(dest, qb, dest_tab, table_desc, input, true);
+      sortOrders = getSortOrders(dest, qb, dest_tab, input);
       if (!enforceBucketing) {
         partnCols = sortCols;
         partnColsNoConvert = getSortCols(dest, qb, dest_tab, table_desc, input, false);
@@ -3932,7 +3934,8 @@ private Operator genBucketingSortingDest(String dest, Operator input, QB qb, Tab
         maxReducers = numBuckets;
       }
 
-      input = genReduceSinkPlanForSortingBucketing(dest_tab, input, sortCols, partnCols, maxReducers);
+      input = genReduceSinkPlanForSortingBucketing(dest_tab, input,
+          sortCols, sortOrders, partnCols, maxReducers);
       ctx.setMultiFileSpray(multiFileSpray);
       ctx.setNumFiles(numFiles);
       ctx.setPartnCols(partnColsNoConvert);
@@ -4703,9 +4706,28 @@ private ArrayList<ExprNodeDesc> getSortCols(String dest, QB qb, Table tab, Table
     return genConvertCol(dest, qb, tab, table_desc, input, posns, convert);
   }
 
+  private ArrayList<Integer> getSortOrders(String dest, QB qb, Table tab, Operator input)
+    throws SemanticException {
+    RowResolver inputRR = opParseCtx.get(input).getRowResolver();
+    List<Order> tabSortCols = tab.getSortCols();
+    List<FieldSchema> tabCols  = tab.getCols();
+
+    ArrayList<Integer> orders = new ArrayList<Integer>();
+    for (Order sortCol : tabSortCols) {
+      for (FieldSchema tabCol : tabCols) {
+        if (sortCol.getCol().equals(tabCol.getName())) {
+          orders.add(sortCol.getOrder());
+          break;
+        }
+      }
+    }
+    return orders;
+  }
+
   @SuppressWarnings("nls")
   private Operator genReduceSinkPlanForSortingBucketing(Table tab, Operator input,
                                                         ArrayList<ExprNodeDesc> sortCols,
+                                                        List<Integer> sortOrders,
                                                         ArrayList<ExprNodeDesc> partitionCols,
                                                         int numReducers)
     throws SemanticException {
@@ -4729,8 +4751,8 @@ private Operator genReduceSinkPlanForSortingBucketing(Table tab, Operator input,
     }
 
     StringBuilder order = new StringBuilder();
-    for (int i = 0; i < sortCols.size(); i++) {
-      order.append("+");
+    for (int sortOrder : sortOrders) {
+      order.append(sortOrder == BaseSemanticAnalyzer.HIVE_COLUMN_ORDER_ASC ? '+' :'-');
     }
 
     Operator interim = putOpInsertMap(OperatorFactory.getAndMakeChild(PlanUtils
diff --git a/ql/src/test/queries/clientpositive/enforce_order.q b/ql/src/test/queries/clientpositive/enforce_order.q
new file mode 100644
index 0000000000..6a303c3ad2
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/enforce_order.q
@@ -0,0 +1,13 @@
+drop table table_asc;
+drop table table_desc;
+
+set hive.enforce.sorting = true;
+
+create table table_asc(key string, value string) clustered by (key) sorted by (key ASC) into 1 BUCKETS;
+create table table_desc(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS;
+
+insert overwrite table table_asc select key, value from src;
+insert overwrite table table_desc select key, value from src;
+
+select * from table_asc limit 10;
+select * from table_desc limit 10;
diff --git a/ql/src/test/results/clientpositive/enforce_order.q.out b/ql/src/test/results/clientpositive/enforce_order.q.out
new file mode 100644
index 0000000000..e87083751a
--- /dev/null
+++ b/ql/src/test/results/clientpositive/enforce_order.q.out
@@ -0,0 +1,84 @@
+PREHOOK: query: drop table table_asc
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table table_asc
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table table_desc
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table table_desc
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table table_asc(key string, value string) clustered by (key) sorted by (key ASC) into 1 BUCKETS
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table table_asc(key string, value string) clustered by (key) sorted by (key ASC) into 1 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@table_asc
+PREHOOK: query: create table table_desc(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table table_desc(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@table_desc
+PREHOOK: query: insert overwrite table table_asc select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@table_asc
+POSTHOOK: query: insert overwrite table table_asc select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@table_asc
+POSTHOOK: Lineage: table_asc.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: table_asc.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table table_desc select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@table_desc
+POSTHOOK: query: insert overwrite table table_desc select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@table_desc
+POSTHOOK: Lineage: table_asc.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: table_asc.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: table_desc.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: table_desc.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from table_asc limit 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@table_asc
+#### A masked pattern was here ####
+POSTHOOK: query: select * from table_asc limit 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@table_asc
+#### A masked pattern was here ####
+POSTHOOK: Lineage: table_asc.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: table_asc.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: table_desc.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: table_desc.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+0	val_0
+0	val_0
+0	val_0
+10	val_10
+100	val_100
+100	val_100
+103	val_103
+103	val_103
+104	val_104
+104	val_104
+PREHOOK: query: select * from table_desc limit 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@table_desc
+#### A masked pattern was here ####
+POSTHOOK: query: select * from table_desc limit 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@table_desc
+#### A masked pattern was here ####
+POSTHOOK: Lineage: table_asc.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: table_asc.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: table_desc.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: table_desc.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+98	val_98
+98	val_98
+97	val_97
+97	val_97
+96	val_96
+95	val_95
+95	val_95
+92	val_92
+90	val_90
+90	val_90
