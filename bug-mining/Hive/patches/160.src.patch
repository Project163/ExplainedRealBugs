diff --git a/CHANGES.txt b/CHANGES.txt
index f1c90ce9e3..8d888ff929 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -50,6 +50,9 @@ Trunk - Unreleased
     HIVE-204. Provide option to run tests with MiniMRCluster.
     (Namit Jain via athusoo)
 
+    HIVE-511. Make DoubleWritable's hashCode more uniform.
+    (Zheng Shao via rmurthy)
+
   OPTIMIZATIONS
 
     HIVE-279. Predicate Pushdown support (Prasad Chakka via athusoo).
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
index 68de87e4f5..d73440efc3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
@@ -498,7 +498,7 @@ public static void main(String[] args) throws IOException, HiveException {
 
     // If started from main(), and isSilent is on, we should not output
     // any logs.
-    if (isSilent) {
+    if (!isSilent) {
       BasicConfigurator.resetConfiguration();
       BasicConfigurator.configure(new NullAppender());
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
index c0b34e0a9a..acdf158388 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
@@ -49,8 +49,6 @@ public class FunctionRegistry {
   static HashMap<String, FunctionInfo> mFunctions;
   static {
     mFunctions = new HashMap<String, FunctionInfo>();
-    registerUDF("default_sample_hashfn", UDFDefaultSampleHashFn.class,
-                OperatorType.PREFIX, false);
     registerUDF("concat", UDFConcat.class, OperatorType.PREFIX, false);
     registerUDF("substr", UDFSubstr.class, OperatorType.PREFIX, false);
 
@@ -163,7 +161,7 @@ public class FunctionRegistry {
     // Generic UDFs
     registerGenericUDF("case", GenericUDFCase.class);
     registerGenericUDF("when", GenericUDFWhen.class);
-    
+    registerGenericUDF("hash", GenericUDFHash.class);
   }
 
   public static FunctionInfo getInfo(Class<?> fClass) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
index 8d093c236b..e97296a7ff 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
@@ -32,10 +32,12 @@
 import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 
 /**
  * Reduce Sink Operator sends output to the reduce stage
@@ -114,9 +116,9 @@ public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[]
   transient HiveKey keyWritable = new HiveKey();
   transient Writable value;
   
-  transient ObjectInspector keyObjectInspector;
-  transient ObjectInspector valueObjectInspector;
-  transient ObjectInspector partitionObjectInspector;
+  transient StructObjectInspector keyObjectInspector;
+  transient StructObjectInspector valueObjectInspector;
+  transient ObjectInspector[] partitionObjectInspectors;
 
   transient Object[] cachedKeys;
   transient Object[] cachedValues;
@@ -131,7 +133,7 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
         firstRow = false;
         keyObjectInspector = initEvaluatorsAndReturnStruct(keyEval, rowInspector);
         valueObjectInspector = initEvaluatorsAndReturnStruct(valueEval, rowInspector);
-        partitionObjectInspector = initEvaluatorsAndReturnStruct(partitionEval, rowInspector);
+        partitionObjectInspectors = initEvaluators(partitionEval, rowInspector);
 
         cachedKeys = new Object[keyEval.length];
         cachedValues = new Object[valueEval.length];
@@ -179,10 +181,10 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
         }
         keyHashCode = random.nextInt();
       } else {
-        for(ExprNodeEvaluator e: partitionEval) {
-          Object o = e.evaluate(row);
+        for (int i = 0; i < partitionEval.length; i++) {
+          Object o = partitionEval[i].evaluate(row);
           keyHashCode = keyHashCode * 31 
-            + (o == null ? 0 : Utilities.hashCode(o, keyObjectInspector));
+            + ObjectInspectorUtils.hashCode(o, partitionObjectInspectors[i]);
         }
       }
       keyWritable.setHashCode(keyHashCode);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 41da193317..8f25acfc80 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -56,6 +56,14 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.mapred.FileOutputFormat;
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
@@ -626,35 +634,4 @@ public static String getNameMessage(Exception e) {
     return e.getClass().getName() + "(" +  e.getMessage() + ")";
   }
 
-  public static int hashCode(Object o, ObjectInspector objIns) {
-    if ((objIns.getCategory() == Category.PRIMITIVE) &&
-        ((PrimitiveObjectInspector)objIns).isWritable())
-      return o.hashCode();
-        
-    if (o instanceof String)
-      return (new Text((String)o)).hashCode();
-    
-    if (o instanceof Integer) 
-      return (new IntWritable(((Integer)o).intValue())).hashCode();
-   
-    if (o instanceof Short)
-      return (new ShortWritable(((Short)o).shortValue())).hashCode();
-    
-    if (o instanceof Float)
-      return (new FloatWritable(((Float)o).floatValue())).hashCode();
-    
-    if (o instanceof Long)
-      return (new LongWritable(((Long)o).longValue())).hashCode();
- 
-    if (o instanceof Boolean)
-      return (new BooleanWritable(((Boolean)o).booleanValue())).hashCode();
-   
-    if (o instanceof Double)
-      return (new DoubleWritable(((Double)o).doubleValue())).hashCode();
-    
-    if (o instanceof Byte)
-      return (new ByteWritable(((Byte)o).byteValue())).hashCode();
-    
-    return o.hashCode();
-  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index e49a5ab3c1..7ff8ca4918 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -109,6 +109,7 @@
 import org.apache.hadoop.hive.ql.plan.tableScanDesc;
 import org.apache.hadoop.hive.ql.plan.unionDesc;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
@@ -3200,7 +3201,7 @@ private Operator genUnionPlan(String unionalias, String leftalias,
    * clause and the table has clustering columns defined in it's metadata.
    * The predicate created has the following structure:
    * 
-   *     ((default_sample_hashfn(expressions) & Integer.MAX_VALUE) % denominator) == numerator
+   *     ((hash(expressions) & Integer.MAX_VALUE) % denominator) == numerator
    * 
    * @param ts TABLESAMPLE clause information
    * @param bucketCols The clustering columns of the table
@@ -3244,8 +3245,9 @@ private exprNodeDesc genSamplePredicate(TableSample ts, List<String> bucketCols,
     }
 
     exprNodeDesc equalsExpr = null;
-    try {
-      exprNodeDesc hashfnExpr = TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("default_sample_hashfn", args);
+    {
+      exprNodeDesc hashfnExpr = new exprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo,
+          GenericUDFHash.class, args);
       assert(hashfnExpr != null);
       LOG.info("hashfnExpr = " + hashfnExpr);
       exprNodeDesc andExpr = TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("&", hashfnExpr, intMaxExpr);
@@ -3258,8 +3260,6 @@ private exprNodeDesc genSamplePredicate(TableSample ts, List<String> bucketCols,
       equalsExpr = TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("==", modExpr, numeratorExpr);
       LOG.info("equalsExpr = " + equalsExpr);
       assert(equalsExpr != null);
-    } catch (UDFArgumentTypeException e) {
-      throw new RuntimeException("Hive 2 internal exception", e);
     }
     return equalsExpr;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java
index 8d140bbb4b..e69de29bb2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf;
-
-import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-public class UDFDefaultSampleHashFn extends UDF {
-  protected final Log LOG;
-
-  IntWritable result = new IntWritable();
-  
-  public UDFDefaultSampleHashFn() {
-    LOG = LogFactory.getLog(this.getClass().getName());
-  }
-
-  public IntWritable evaluate(Object o) {
-    result.set(o == null ? 0 : o.hashCode());
-    return result;
-  }
-  
-  // TODO: For now, only allow up to two columns on which to sample
-  // Going forward we will allow sampling on an arbitrary number of columns
-  public IntWritable evaluate(Object o1, Object o2) {
-    result.set((o1 == null ? 0 : o1.hashCode()) ^ (o2 == null ? 0 : o2.hashCode()));
-    return result;
-  }
-}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFHash.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFHash.java
new file mode 100644
index 0000000000..c1a538d2cd
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFHash.java
@@ -0,0 +1,72 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.VoidObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
+import org.apache.hadoop.io.IntWritable;
+
+/**
+ * GenericUDF Class for computing hash values.
+ */
+public class GenericUDFHash extends GenericUDF {
+
+  private static Log LOG = LogFactory.getLog(GenericUDFHash.class.getName());
+
+  ObjectInspector[] argumentOIs;
+  
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments)
+      throws UDFArgumentTypeException {
+    
+    this.argumentOIs = arguments;
+    return PrimitiveObjectInspectorFactory.writableIntObjectInspector;
+  }
+
+  IntWritable result = new IntWritable();
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    // See http://java.sun.com/j2se/1.5.0/docs/api/java/util/List.html#hashCode()
+    int r = 0;
+    for(int i = 0; i < arguments.length; i++) {
+      r = r * 31 + ObjectInspectorUtils.hashCode(arguments[i].get(), argumentOIs[i]);
+    }
+    result.set(r);
+    return result;
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    return "hash(" + StringUtils.join(children, ',') + ")";
+  }
+
+}
diff --git a/ql/src/test/queries/clientpositive/udf_hash.q b/ql/src/test/queries/clientpositive/udf_hash.q
new file mode 100644
index 0000000000..d5f354666d
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/udf_hash.q
@@ -0,0 +1,14 @@
+EXPLAIN
+SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),
+       hash(3), hash(CAST('123456789012' AS BIGINT)),
+       hash(CAST(1.25 AS FLOAT)), hash(CAST(16.0 AS DOUBLE)),
+       hash('400'), hash('abc'), hash(TRUE), hash(FALSE),
+       hash(1, 2, 3)
+FROM src LIMIT 1;
+
+SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),
+       hash(3), hash(CAST('123456789012' AS BIGINT)),
+       hash(CAST(1.25 AS FLOAT)), hash(CAST(16.0 AS DOUBLE)),
+       hash('400'), hash('abc'), hash(TRUE), hash(FALSE),
+       hash(1, 2, 3)
+FROM src LIMIT 1;
diff --git a/ql/src/test/results/clientpositive/sample1.q.out b/ql/src/test/results/clientpositive/sample1.q.out
index 61816332fc..2791d7c08d 100644
--- a/ql/src/test/results/clientpositive/sample1.q.out
+++ b/ql/src/test/results/clientpositive/sample1.q.out
@@ -18,7 +18,7 @@ STAGE PLANS:
         s 
             Filter Operator
               predicate:
-                  expr: (((default_sample_hashfn(rand()) & 2147483647) % 1) = 0)
+                  expr: (((hash(rand()) & 2147483647) % 1) = 0)
                   type: boolean
               Filter Operator
                 predicate:
@@ -47,7 +47,7 @@ STAGE PLANS:
                     File Output Operator
                       compressed: false
                       GlobalTableId: 1
-                      directory: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/151429807/960604309.10000.insclause-0
+                      directory: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/51597465/892026021.10000.insclause-0
                       table:
                           input format: org.apache.hadoop.mapred.TextInputFormat
                           output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -61,14 +61,14 @@ STAGE PLANS:
                             serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             file.inputformat org.apache.hadoop.mapred.TextInputFormat
                             file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/dest1
+                            location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                           name: dest1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
       Path -> Partition:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
             partition values:
               ds 2008-04-08
@@ -87,7 +87,7 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
 
@@ -95,7 +95,7 @@ STAGE PLANS:
     Move Operator
       tables:
             replace: true
-            source: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/151429807/960604309.10000.insclause-0
+            source: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/51597465/892026021.10000.insclause-0
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -109,10 +109,10 @@ STAGE PLANS:
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/dest1
+                  location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest1
-            tmp directory: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/151429807/960604309.10001
+            tmp directory: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/51597465/892026021.10001
 
 
 query: INSERT OVERWRITE TABLE dest1 SELECT s.*
@@ -122,7 +122,7 @@ Input: default/srcpart/ds=2008-04-08/hr=11
 Output: default/dest1
 query: SELECT dest1.* FROM dest1
 Input: default/dest1
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/383189915/1659525718.10000
+Output: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/115935535/402166983.10000
 238	val_238	2008-04-08	11
 86	val_86	2008-04-08	11
 311	val_311	2008-04-08	11
diff --git a/ql/src/test/results/clientpositive/sample3.q.out b/ql/src/test/results/clientpositive/sample3.q.out
index aa40c6fc54..6ef607f050 100644
--- a/ql/src/test/results/clientpositive/sample3.q.out
+++ b/ql/src/test/results/clientpositive/sample3.q.out
@@ -20,7 +20,7 @@ STAGE PLANS:
                     type: string
               Filter Operator
                 predicate:
-                    expr: (((default_sample_hashfn(0) & 2147483647) % 5) = 0)
+                    expr: (((hash(0) & 2147483647) % 5) = 0)
                     type: boolean
                 Select Operator
                   expressions:
@@ -41,7 +41,7 @@ STAGE PLANS:
 query: SELECT s.key
 FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 on key) s SORT BY key
 Input: default/srcbucket
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/1055187428/602701590.10000
+Output: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/336730571/201032975.10000
 100
 100
 100
diff --git a/ql/src/test/results/clientpositive/sample5.q.out b/ql/src/test/results/clientpositive/sample5.q.out
index 54ea91bee1..d10852132c 100644
--- a/ql/src/test/results/clientpositive/sample5.q.out
+++ b/ql/src/test/results/clientpositive/sample5.q.out
@@ -17,7 +17,7 @@ STAGE PLANS:
         s 
             Filter Operator
               predicate:
-                  expr: (((default_sample_hashfn(key) & 2147483647) % 5) = 0)
+                  expr: (((hash(key) & 2147483647) % 5) = 0)
                   type: boolean
               Select Operator
                 expressions:
@@ -34,7 +34,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/937153166/420939583.10000.insclause-0
+                    directory: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/61392830/366487994.10000.insclause-0
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -48,14 +48,14 @@ STAGE PLANS:
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/dest1
+                          location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: dest1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcbucket 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket 
       Path -> Partition:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcbucket 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket 
           Partition
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -71,7 +71,7 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcbucket
+                location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket
 
@@ -79,7 +79,7 @@ STAGE PLANS:
     Move Operator
       tables:
             replace: true
-            source: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/937153166/420939583.10000.insclause-0
+            source: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/61392830/366487994.10000.insclause-0
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -93,10 +93,10 @@ STAGE PLANS:
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/dest1
+                  location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest1
-            tmp directory: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/937153166/420939583.10001
+            tmp directory: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/61392830/366487994.10001
 
 
 query: INSERT OVERWRITE TABLE dest1 SELECT s.* -- here's another test
@@ -105,7 +105,7 @@ Input: default/srcbucket
 Output: default/dest1
 query: SELECT dest1.* FROM dest1 SORT BY key
 Input: default/dest1
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/567142156/684303413.10000
+Output: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1162360503/929070733.10000
 2	val_2
 2	val_3
 18	val_18
diff --git a/ql/src/test/results/clientpositive/sample6.q.out b/ql/src/test/results/clientpositive/sample6.q.out
index 22f89f755a..6232128026 100644
--- a/ql/src/test/results/clientpositive/sample6.q.out
+++ b/ql/src/test/results/clientpositive/sample6.q.out
@@ -17,7 +17,7 @@ STAGE PLANS:
         s 
             Filter Operator
               predicate:
-                  expr: (((default_sample_hashfn(key) & 2147483647) % 4) = 0)
+                  expr: (((hash(key) & 2147483647) % 4) = 0)
                   type: boolean
               Select Operator
                 expressions:
@@ -34,7 +34,7 @@ STAGE PLANS:
                   File Output Operator
                     compressed: false
                     GlobalTableId: 1
-                    directory: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/644812951/21988928.10000.insclause-0
+                    directory: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/218973675/62457806.10000.insclause-0
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -48,14 +48,14 @@ STAGE PLANS:
                           serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                           file.inputformat org.apache.hadoop.mapred.TextInputFormat
                           file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                          location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/dest1
+                          location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1
                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                         name: dest1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcbucket/kv1.txt 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt 
       Path -> Partition:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcbucket/kv1.txt 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt 
           Partition
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -71,7 +71,7 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcbucket
+                location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket
 
@@ -79,7 +79,7 @@ STAGE PLANS:
     Move Operator
       tables:
             replace: true
-            source: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/644812951/21988928.10000.insclause-0
+            source: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/218973675/62457806.10000.insclause-0
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -93,10 +93,10 @@ STAGE PLANS:
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/dest1
+                  location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest1
-            tmp directory: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/644812951/21988928.10001
+            tmp directory: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/218973675/62457806.10001
 
 
 query: INSERT OVERWRITE TABLE dest1 SELECT s.* 
@@ -105,7 +105,7 @@ Input: default/srcbucket
 Output: default/dest1
 query: SELECT dest1.* FROM dest1
 Input: default/dest1
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/241022860/87886661.10000
+Output: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/685531102/1290995162.10000
 165	val_165
 484	val_484
 150	val_150
diff --git a/ql/src/test/results/clientpositive/sample7.q.out b/ql/src/test/results/clientpositive/sample7.q.out
index 3b8ca96a6f..4a9fd87c76 100644
--- a/ql/src/test/results/clientpositive/sample7.q.out
+++ b/ql/src/test/results/clientpositive/sample7.q.out
@@ -18,7 +18,7 @@ STAGE PLANS:
         s 
             Filter Operator
               predicate:
-                  expr: (((default_sample_hashfn(key) & 2147483647) % 4) = 0)
+                  expr: (((hash(key) & 2147483647) % 4) = 0)
                   type: boolean
               Filter Operator
                 predicate:
@@ -39,7 +39,7 @@ STAGE PLANS:
                     File Output Operator
                       compressed: false
                       GlobalTableId: 1
-                      directory: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/418752266/1655292388.10000.insclause-0
+                      directory: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1044305705/895861142.10000.insclause-0
                       table:
                           input format: org.apache.hadoop.mapred.TextInputFormat
                           output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -53,14 +53,14 @@ STAGE PLANS:
                             serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                             file.inputformat org.apache.hadoop.mapred.TextInputFormat
                             file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/dest1
+                            location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1
                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                           name: dest1
       Needs Tagging: false
       Path -> Alias:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcbucket/kv1.txt 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt 
       Path -> Partition:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcbucket/kv1.txt 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt 
           Partition
           
               input format: org.apache.hadoop.mapred.TextInputFormat
@@ -76,7 +76,7 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcbucket
+                location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcbucket
 
@@ -84,7 +84,7 @@ STAGE PLANS:
     Move Operator
       tables:
             replace: true
-            source: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/418752266/1655292388.10000.insclause-0
+            source: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1044305705/895861142.10000.insclause-0
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -98,10 +98,10 @@ STAGE PLANS:
                   serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                   file.inputformat org.apache.hadoop.mapred.TextInputFormat
                   file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/dest1
+                  location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1
                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 name: dest1
-            tmp directory: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/418752266/1655292388.10001
+            tmp directory: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1044305705/895861142.10001
 
 
 query: INSERT OVERWRITE TABLE dest1 SELECT s.* 
@@ -111,7 +111,7 @@ Input: default/srcbucket
 Output: default/dest1
 query: SELECT dest1.* FROM dest1
 Input: default/dest1
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/920399475/4091792.10000
+Output: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/578063712/363033982.10000
 165	val_165
 484	val_484
 150	val_150
diff --git a/ql/src/test/results/clientpositive/sample8.q.out b/ql/src/test/results/clientpositive/sample8.q.out
index 762ca472db..7c85076b57 100644
--- a/ql/src/test/results/clientpositive/sample8.q.out
+++ b/ql/src/test/results/clientpositive/sample8.q.out
@@ -27,7 +27,7 @@ STAGE PLANS:
                     type: string
               Filter Operator
                 predicate:
-                    expr: (((default_sample_hashfn(0) & 2147483647) % 10) = 0)
+                    expr: (((hash(0) & 2147483647) % 10) = 0)
                     type: boolean
                 Reduce Output Operator
                   sort order: 
@@ -40,7 +40,7 @@ STAGE PLANS:
         s 
             Filter Operator
               predicate:
-                  expr: (((default_sample_hashfn(key) & 2147483647) % 1) = 0)
+                  expr: (((hash(key) & 2147483647) % 1) = 0)
                   type: boolean
               Reduce Output Operator
                 sort order: 
@@ -56,12 +56,12 @@ STAGE PLANS:
                       type: string
       Needs Tagging: true
       Path -> Alias:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
       Path -> Partition:
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
             partition values:
               ds 2008-04-08
@@ -80,10 +80,10 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
           Partition
             partition values:
               ds 2008-04-08
@@ -102,10 +102,10 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
           Partition
             partition values:
               ds 2008-04-09
@@ -124,10 +124,10 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
-        file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
+        file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
           Partition
             partition values:
               ds 2008-04-09
@@ -146,7 +146,7 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                location file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: srcpart
       Reduce Operator Tree:
@@ -173,7 +173,7 @@ STAGE PLANS:
               File Output Operator
                 compressed: false
                 GlobalTableId: 0
-                directory: /data/users/njain/hive5/hive5/build/ql/tmp/165890068/705170586.10002
+                directory: /data/users/zshao/tools/495-trunk-apache-hive/build/ql/tmp/172500705/271052336.10002
                 table:
                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
@@ -186,7 +186,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /data/users/njain/hive5/hive5/build/ql/tmp/165890068/705170586.10002 
+        /data/users/zshao/tools/495-trunk-apache-hive/build/ql/tmp/172500705/271052336.10002 
           Reduce Output Operator
             key expressions:
                   expr: 0
@@ -211,9 +211,9 @@ STAGE PLANS:
                   type: string
       Needs Tagging: false
       Path -> Alias:
-        /data/users/njain/hive5/hive5/build/ql/tmp/165890068/705170586.10002 
+        /data/users/zshao/tools/495-trunk-apache-hive/build/ql/tmp/172500705/271052336.10002 
       Path -> Partition:
-        /data/users/njain/hive5/hive5/build/ql/tmp/165890068/705170586.10002 
+        /data/users/zshao/tools/495-trunk-apache-hive/build/ql/tmp/172500705/271052336.10002 
           Partition
           
               input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -228,7 +228,7 @@ STAGE PLANS:
           File Output Operator
             compressed: false
             GlobalTableId: 0
-            directory: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/404663379.10001.insclause-0
+            directory: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/55560660.10001.insclause-0
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -251,7 +251,7 @@ Input: default/srcpart/ds=2008-04-08/hr=11
 Input: default/srcpart/ds=2008-04-08/hr=12
 Input: default/srcpart/ds=2008-04-09/hr=11
 Input: default/srcpart/ds=2008-04-09/hr=12
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/68681937/1395083774.10000
+Output: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/314649852/697371331.10000
 0	val_0
 0	val_0
 0	val_0
diff --git a/ql/src/test/results/clientpositive/udf_hash.q.out b/ql/src/test/results/clientpositive/udf_hash.q.out
new file mode 100644
index 0000000000..f2ef082840
--- /dev/null
+++ b/ql/src/test/results/clientpositive/udf_hash.q.out
@@ -0,0 +1,66 @@
+query: EXPLAIN
+SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),
+       hash(3), hash(CAST('123456789012' AS BIGINT)),
+       hash(CAST(1.25 AS FLOAT)), hash(CAST(16.0 AS DOUBLE)),
+       hash('400'), hash('abc'), hash(TRUE), hash(FALSE),
+       hash(1, 2, 3)
+FROM src LIMIT 1
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION hash (TOK_FUNCTION TOK_TINYINT 1))) (TOK_SELEXPR (TOK_FUNCTION hash (TOK_FUNCTION TOK_SMALLINT 2))) (TOK_SELEXPR (TOK_FUNCTION hash 3)) (TOK_SELEXPR (TOK_FUNCTION hash (TOK_FUNCTION TOK_BIGINT '123456789012'))) (TOK_SELEXPR (TOK_FUNCTION hash (TOK_FUNCTION TOK_FLOAT 1.25))) (TOK_SELEXPR (TOK_FUNCTION hash (TOK_FUNCTION TOK_DOUBLE 16.0))) (TOK_SELEXPR (TOK_FUNCTION hash '400')) (TOK_SELEXPR (TOK_FUNCTION hash 'abc')) (TOK_SELEXPR (TOK_FUNCTION hash TRUE)) (TOK_SELEXPR (TOK_FUNCTION hash FALSE)) (TOK_SELEXPR (TOK_FUNCTION hash 1 2 3))) (TOK_LIMIT 1)))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+            Select Operator
+              Select Operator
+                expressions:
+                      expr: hash(UDFToByte(1))
+                      type: int
+                      expr: hash(UDFToShort(2))
+                      type: int
+                      expr: hash(3)
+                      type: int
+                      expr: hash(UDFToLong('123456789012'))
+                      type: int
+                      expr: hash(UDFToFloat(1.25))
+                      type: int
+                      expr: hash(16.0)
+                      type: int
+                      expr: hash('400')
+                      type: int
+                      expr: hash('abc')
+                      type: int
+                      expr: hash(true)
+                      type: int
+                      expr: hash(false)
+                      type: int
+                      expr: hash(1,2,3)
+                      type: int
+                Limit
+                  File Output Operator
+                    compressed: false
+                    GlobalTableId: 0
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 1
+
+
+query: SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),
+       hash(3), hash(CAST('123456789012' AS BIGINT)),
+       hash(CAST(1.25 AS FLOAT)), hash(CAST(16.0 AS DOUBLE)),
+       hash('400'), hash('abc'), hash(TRUE), hash(FALSE),
+       hash(1, 2, 3)
+FROM src LIMIT 1
+Input: default/src
+Output: /data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/347375001/1065831137.10000
+1	2	3	-1097262584	1067450368	1076887552	51508	96354	1	0	1026
diff --git a/ql/src/test/results/compiler/plan/sample1.q.xml b/ql/src/test/results/compiler/plan/sample1.q.xml
index fed2f83426..cd13ff7018 100644
--- a/ql/src/test/results/compiler/plan/sample1.q.xml
+++ b/ql/src/test/results/compiler/plan/sample1.q.xml
@@ -30,7 +30,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/data/users/zshao/tools/deploy-trunk-apache-hive/ql/../build/ql/tmp/699353582/704180056.10001.insclause-0</string> 
+                           <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/876735944/588596206.10001.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -462,20 +462,7 @@
                         <void property="childExprs"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeFuncDesc"> 
-                            <void property="UDFClass"> 
-                             <class>org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn</class> 
-                            </void> 
-                            <void property="UDFMethod"> 
-                             <object class="org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn" method="getMethod"> 
-                              <string>evaluate</string> 
-                              <array class="java.lang.Class" length="1"> 
-                               <void index="0"> 
-                                <class>java.lang.Object</class> 
-                               </void> 
-                              </array> 
-                             </object> 
-                            </void> 
+                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeGenericFuncDesc"> 
                             <void property="childExprs"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -503,6 +490,9 @@
                               </void> 
                              </object> 
                             </void> 
+                            <void property="genericUDFClass"> 
+                             <class>org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash</class> 
+                            </void> 
                             <void property="typeInfo"> 
                              <object id="PrimitiveTypeInfo2" class="org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo"> 
                               <void property="typeName"> 
@@ -590,7 +580,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/hr=11/ds=2008-04-08</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -602,7 +592,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/hr=11/ds=2008-04-08</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"> 
@@ -671,7 +661,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcpart</string> 
+             <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcpart</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample3.q.xml b/ql/src/test/results/compiler/plan/sample3.q.xml
index 417bd7ef51..e585956008 100644
--- a/ql/src/test/results/compiler/plan/sample3.q.xml
+++ b/ql/src/test/results/compiler/plan/sample3.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/data/users/zshao/tools/deploy-trunk-apache-hive/ql/../build/ql/tmp/636268945/66220949.10000.insclause-0</string> 
+             <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1100474640/428587611.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -84,7 +84,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -94,7 +94,7 @@
              </object> 
             </void> 
             <void property="tmpDir"> 
-             <string>/data/users/athusoo/commits/hive_trunk_ws9/ql/../build/ql/tmp/285088828/920051529.10001</string> 
+             <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1100474640/428587611.10001</string> 
             </void> 
            </object> 
           </void> 
@@ -134,7 +134,7 @@
                        <int>1</int> 
                       </void> 
                       <void property="dirName"> 
-                       <string>/data/users/zshao/tools/deploy-trunk-apache-hive/ql/../build/ql/tmp/636268945/66220949.10000.insclause-0</string> 
+                       <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1100474640/428587611.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -308,23 +308,7 @@
                         <void property="childExprs"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeFuncDesc"> 
-                            <void property="UDFClass"> 
-                             <class>org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn</class> 
-                            </void> 
-                            <void property="UDFMethod"> 
-                             <object class="org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn" method="getMethod"> 
-                              <string>evaluate</string> 
-                              <array class="java.lang.Class" length="2"> 
-                               <void index="0"> 
-                                <class>java.lang.Object</class> 
-                               </void> 
-                               <void index="1"> 
-                                <class>java.lang.Object</class> 
-                               </void> 
-                              </array> 
-                             </object> 
-                            </void> 
+                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeGenericFuncDesc"> 
                             <void property="childExprs"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -349,6 +333,9 @@
                               </void> 
                              </object> 
                             </void> 
+                            <void property="genericUDFClass"> 
+                             <class>org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash</class> 
+                            </void> 
                             <void property="typeInfo"> 
                              <object id="PrimitiveTypeInfo1" class="org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo"> 
                               <void property="typeName"> 
@@ -461,7 +448,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -473,7 +460,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -533,7 +520,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample5.q.xml b/ql/src/test/results/compiler/plan/sample5.q.xml
index c299d98535..7edb19b611 100644
--- a/ql/src/test/results/compiler/plan/sample5.q.xml
+++ b/ql/src/test/results/compiler/plan/sample5.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/data/users/zshao/tools/deploy-trunk-apache-hive/ql/../build/ql/tmp/19951575/194654863.10000.insclause-0</string> 
+             <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/375244052/371343677.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -84,7 +84,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -94,7 +94,7 @@
              </object> 
             </void> 
             <void property="tmpDir"> 
-             <string>/data/users/athusoo/commits/hive_trunk_ws9/ql/../build/ql/tmp/83476196/1532819303.10001</string> 
+             <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/375244052/371343677.10001</string> 
             </void> 
            </object> 
           </void> 
@@ -134,7 +134,7 @@
                        <int>1</int> 
                       </void> 
                       <void property="dirName"> 
-                       <string>/data/users/zshao/tools/deploy-trunk-apache-hive/ql/../build/ql/tmp/19951575/194654863.10000.insclause-0</string> 
+                       <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/375244052/371343677.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -308,20 +308,7 @@
                         <void property="childExprs"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeFuncDesc"> 
-                            <void property="UDFClass"> 
-                             <class>org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn</class> 
-                            </void> 
-                            <void property="UDFMethod"> 
-                             <object class="org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn" method="getMethod"> 
-                              <string>evaluate</string> 
-                              <array class="java.lang.Class" length="1"> 
-                               <void index="0"> 
-                                <class>java.lang.Object</class> 
-                               </void> 
-                              </array> 
-                             </object> 
-                            </void> 
+                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeGenericFuncDesc"> 
                             <void property="childExprs"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -336,6 +323,9 @@
                               </void> 
                              </object> 
                             </void> 
+                            <void property="genericUDFClass"> 
+                             <class>org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash</class> 
+                            </void> 
                             <void property="typeInfo"> 
                              <object id="PrimitiveTypeInfo1" class="org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo"> 
                               <void property="typeName"> 
@@ -448,7 +438,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -460,7 +450,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -520,7 +510,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample6.q.xml b/ql/src/test/results/compiler/plan/sample6.q.xml
index c112a2b1be..a80ec06207 100644
--- a/ql/src/test/results/compiler/plan/sample6.q.xml
+++ b/ql/src/test/results/compiler/plan/sample6.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/data/users/zshao/tools/deploy-trunk-apache-hive/ql/../build/ql/tmp/8728732/6227044.10000.insclause-0</string> 
+             <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1200611922/1364220611.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -84,7 +84,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -94,7 +94,7 @@
              </object> 
             </void> 
             <void property="tmpDir"> 
-             <string>/data/users/athusoo/commits/hive_trunk_ws9/ql/../build/ql/tmp/61425565/17389492.10001</string> 
+             <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1200611922/1364220611.10001</string> 
             </void> 
            </object> 
           </void> 
@@ -134,7 +134,7 @@
                        <int>1</int> 
                       </void> 
                       <void property="dirName"> 
-                       <string>/data/users/zshao/tools/deploy-trunk-apache-hive/ql/../build/ql/tmp/8728732/6227044.10000.insclause-0</string> 
+                       <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1200611922/1364220611.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -308,20 +308,7 @@
                         <void property="childExprs"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeFuncDesc"> 
-                            <void property="UDFClass"> 
-                             <class>org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn</class> 
-                            </void> 
-                            <void property="UDFMethod"> 
-                             <object class="org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn" method="getMethod"> 
-                              <string>evaluate</string> 
-                              <array class="java.lang.Class" length="1"> 
-                               <void index="0"> 
-                                <class>java.lang.Object</class> 
-                               </void> 
-                              </array> 
-                             </object> 
-                            </void> 
+                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeGenericFuncDesc"> 
                             <void property="childExprs"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -336,6 +323,9 @@
                               </void> 
                              </object> 
                             </void> 
+                            <void property="genericUDFClass"> 
+                             <class>org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash</class> 
+                            </void> 
                             <void property="typeInfo"> 
                              <object id="PrimitiveTypeInfo1" class="org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo"> 
                               <void property="typeName"> 
@@ -448,7 +438,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -460,7 +450,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -520,7 +510,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample7.q.xml b/ql/src/test/results/compiler/plan/sample7.q.xml
index 5669168452..35ab4a5ad4 100644
--- a/ql/src/test/results/compiler/plan/sample7.q.xml
+++ b/ql/src/test/results/compiler/plan/sample7.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/data/users/zshao/tools/deploy-trunk-apache-hive/ql/../build/ql/tmp/309063600/55379357.10000.insclause-0</string> 
+             <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1120445121/55543577.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -84,7 +84,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -94,7 +94,7 @@
              </object> 
             </void> 
             <void property="tmpDir"> 
-             <string>/data/users/athusoo/commits/hive_trunk_ws9/ql/../build/ql/tmp/1894315848/355605785.10001</string> 
+             <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1120445121/55543577.10001</string> 
             </void> 
            </object> 
           </void> 
@@ -138,7 +138,7 @@
                            <int>1</int> 
                           </void> 
                           <void property="dirName"> 
-                           <string>/data/users/zshao/tools/deploy-trunk-apache-hive/ql/../build/ql/tmp/309063600/55379357.10000.insclause-0</string> 
+                           <string>/data/users/zshao/tools/495-trunk-apache-hive/ql/../build/ql/tmp/1120445121/55543577.10000.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object idref="tableDesc0"/> 
@@ -462,20 +462,7 @@
                         <void property="childExprs"> 
                          <object class="java.util.ArrayList"> 
                           <void method="add"> 
-                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeFuncDesc"> 
-                            <void property="UDFClass"> 
-                             <class>org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn</class> 
-                            </void> 
-                            <void property="UDFMethod"> 
-                             <object class="org.apache.hadoop.hive.ql.udf.UDFDefaultSampleHashFn" method="getMethod"> 
-                              <string>evaluate</string> 
-                              <array class="java.lang.Class" length="1"> 
-                               <void index="0"> 
-                                <class>java.lang.Object</class> 
-                               </void> 
-                              </array> 
-                             </object> 
-                            </void> 
+                           <object class="org.apache.hadoop.hive.ql.plan.exprNodeGenericFuncDesc"> 
                             <void property="childExprs"> 
                              <object class="java.util.ArrayList"> 
                               <void method="add"> 
@@ -490,6 +477,9 @@
                               </void> 
                              </object> 
                             </void> 
+                            <void property="genericUDFClass"> 
+                             <class>org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash</class> 
+                            </void> 
                             <void property="typeInfo"> 
                              <object idref="PrimitiveTypeInfo2"/> 
                             </void> 
@@ -573,7 +563,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -585,7 +575,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
+       <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket/kv1.txt</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -645,7 +635,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/data/users/zshao/tools/deploy-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
+             <string>file:/data/users/zshao/tools/495-trunk-apache-hive/build/ql/test/data/warehouse/srcbucket</string> 
             </void> 
            </object> 
           </void> 
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
index d8adf5087e..3ea5794b2c 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
@@ -29,8 +29,17 @@
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
 import org.apache.hadoop.io.BooleanWritable;
 import org.apache.hadoop.io.FloatWritable;
 import org.apache.hadoop.io.IntWritable;
@@ -311,5 +320,53 @@ public static String getObjectInspectorName(ObjectInspector oi) {
       }
     }
   }
+
+  public static int hashCode(Object o, ObjectInspector objIns) {
+    if (o == null) {
+      return 0;
+    }
+    switch (objIns.getCategory()) {
+      case PRIMITIVE: {
+        PrimitiveObjectInspector poi = ((PrimitiveObjectInspector)objIns);
+        switch (poi.getPrimitiveCategory()) {
+          case VOID: return 0;
+          case BOOLEAN: return ((BooleanObjectInspector)poi).get(o) ? 1 : 0;
+          case BYTE: return ((ByteObjectInspector)poi).get(o);
+          case SHORT: return ((ShortObjectInspector)poi).get(o);
+          case INT: return ((IntObjectInspector)poi).get(o);
+          case LONG: {
+            long a = ((LongObjectInspector)poi).get(o);
+            return (int)((a >>> 32) ^ a);
+          }
+          case FLOAT: return Float.floatToIntBits(((FloatObjectInspector)poi).get(o));
+          case DOUBLE: {
+            // This hash function returns the same result as Double.hashCode()
+            // while DoubleWritable.hashCode returns a different result.
+            long a = Double.doubleToLongBits(((DoubleObjectInspector)poi).get(o));
+            return (int)((a >>> 32) ^ a);
+          }
+          case STRING: {
+            // This hash function returns the same result as String.hashCode() when
+            // all characters are ASCII, while Text.hashCode() always returns a
+            // different result.
+            Text t = ((StringObjectInspector)poi).getPrimitiveWritableObject(o);
+            int r = 0;
+            for (int i=0; i<t.getLength(); i++) {
+              r = r * 31 + (int)t.getBytes()[i];
+            }
+            return r;
+          }
+          default: {
+            throw new RuntimeException("Unknown type: " + poi.getPrimitiveCategory());
+          }
+        }
+      }
+      case STRUCT:
+      case LIST: 
+      case MAP: 
+      default:  
+        throw new RuntimeException("Hash code on complex types not supported yet.");
+    }
+  }
   
 }
