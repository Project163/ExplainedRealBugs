diff --git a/iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java b/iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
index 12ade76a68..9fe617de82 100644
--- a/iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
+++ b/iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
@@ -75,7 +75,8 @@ public class HiveTableOperations extends BaseMetastoreTableOperations {
   private static final BiMap<String, String> ICEBERG_TO_HMS_TRANSLATION = ImmutableBiMap.of(
       // gc.enabled in Iceberg and external.table.purge in Hive are meant to do the same things but with different names
       GC_ENABLED, "external.table.purge",
-      TableProperties.PARQUET_COMPRESSION, ParquetOutputFormat.COMPRESSION);
+      TableProperties.PARQUET_COMPRESSION, ParquetOutputFormat.COMPRESSION,
+      TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES, ParquetOutputFormat.BLOCK_SIZE);
 
   /**
    * Provides key translation where necessary between Iceberg and HMS props. This translation is needed because some
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputFormat.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputFormat.java
index 00fd9db2f0..7c625543b7 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputFormat.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputFormat.java
@@ -33,11 +33,13 @@
 import org.apache.hadoop.mapred.TaskAttemptID;
 import org.apache.hadoop.util.Progressable;
 import org.apache.iceberg.Table;
+import org.apache.iceberg.TableProperties;
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.mr.Catalogs;
 import org.apache.iceberg.mr.hive.writer.HiveIcebergWriter;
 import org.apache.iceberg.mr.hive.writer.WriterBuilder;
 import org.apache.iceberg.mr.mapred.Container;
+import org.apache.parquet.hadoop.ParquetOutputFormat;
 
 public class HiveIcebergOutputFormat<T> implements OutputFormat<NullWritable, Container<Record>>,
     HiveOutputFormat<NullWritable, Container<Record>> {
@@ -68,6 +70,7 @@ private static HiveIcebergWriter writer(JobConf jc) {
     String tableName = jc.get(Catalogs.NAME);
     int poolSize = jc.getInt(DELETE_FILE_THREAD_POOL_SIZE, DELETE_FILE_THREAD_POOL_SIZE_DEFAULT);
 
+    setWriterLevelConfiguration(jc, table);
     return WriterBuilder.builderFor(table)
         .queryId(jc.get(HiveConf.ConfVars.HIVEQUERYID.varname))
         .tableName(tableName)
@@ -76,4 +79,18 @@ private static HiveIcebergWriter writer(JobConf jc) {
         .operation(HiveCustomStorageHandlerUtils.getWriteOperation(jc, tableName))
         .build();
   }
+
+  private static void setWriterLevelConfiguration(JobConf jc, Table table) {
+    final String writeFormat = table.properties().get("write.format.default");
+    if (writeFormat == null || "PARQUET".equalsIgnoreCase(writeFormat)) {
+      if (table.properties().get(TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES) == null &&
+          jc.get(ParquetOutputFormat.BLOCK_SIZE) != null) {
+        table.properties().put(TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES, jc.get(ParquetOutputFormat.BLOCK_SIZE));
+      }
+      if (table.properties().get(TableProperties.PARQUET_COMPRESSION) == null &&
+          jc.get(ParquetOutputFormat.COMPRESSION) != null) {
+        table.properties().put(TableProperties.PARQUET_COMPRESSION, jc.get(ParquetOutputFormat.COMPRESSION));
+      }
+    }
+  }
 }
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerNoScan.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerNoScan.java
index 97c2eb83ef..af18d4c8c7 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerNoScan.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerNoScan.java
@@ -1637,19 +1637,24 @@ public void testCTLTHiveCatalogValidation() throws TException, InterruptedExcept
   @Test
   public void testParquetHiveCatalogValidation() throws TException, InterruptedException, IOException {
 
-    // Create a table with explicitly set parquet.compression
+    // Create a table with explicitly set parquet.compression & parquet.block.size
+    Map<String, String> props = Maps.newHashMap();
+    props.put(ParquetOutputFormat.BLOCK_SIZE, "10000");
+    props.put(ParquetOutputFormat.COMPRESSION, "SNAPPY");
     TableIdentifier target = TableIdentifier.of("default", "target");
     Table table = testTables.createTable(shell, target.name(), HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA,
         PartitionSpec.unpartitioned(), FileFormat.PARQUET, HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS, 1,
-        Collections.singletonMap(ParquetOutputFormat.COMPRESSION, "SNAPPY"));
+        props);
 
     // Check the property got set in the hive table metadata.
     org.apache.hadoop.hive.metastore.api.Table hmsTable = shell.metastore().getTable(target);
     Assert.assertEquals("SNAPPY", hmsTable.getParameters().get(ParquetOutputFormat.COMPRESSION).toUpperCase());
+    Assert.assertEquals("10000", hmsTable.getParameters().get(ParquetOutputFormat.BLOCK_SIZE));
 
     // Check the property got set in the iceberg table metadata.
     Table icebergTable = testTables.loadTable(target);
     Assert.assertEquals("SNAPPY", icebergTable.properties().get(TableProperties.PARQUET_COMPRESSION).toUpperCase());
+    Assert.assertEquals("10000", icebergTable.properties().get(TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES));
   }
 
   @Test
