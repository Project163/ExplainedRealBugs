diff --git a/CHANGES.txt b/CHANGES.txt
index 2529d23c63..b420e1ff9c 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -321,6 +321,9 @@ Trunk -  Unreleased
     HIVE-1277 Failure if the local file system directory for ${hive.user.scratchdir}
     does not exist (Arvind Prabhakar via namit)
 
+    HIVE-1281 Bucketing column names in create table should be case-insensitive
+    (He Yongqiang via namit)
+
 Release 0.5.0 -  Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index 3d99875196..b1d4ebe665 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -336,18 +336,26 @@ public List<FieldSchema> getResultSchema() {
     return null;
   }
 
+  protected List<FieldSchema> getColumns(ASTNode ast) throws SemanticException {
+    return getColumns(ast, true);
+  }
+  
   /**
    * Get the list of FieldSchema out of the ASTNode.
    */
-  protected List<FieldSchema> getColumns(ASTNode ast) throws SemanticException {
+  protected List<FieldSchema> getColumns(ASTNode ast, boolean lowerCase) throws SemanticException {
     List<FieldSchema> colList = new ArrayList<FieldSchema>();
     int numCh = ast.getChildCount();
     for (int i = 0; i < numCh; i++) {
       FieldSchema col = new FieldSchema();
       ASTNode child = (ASTNode) ast.getChild(i);
 
+      String name = child.getChild(0).getText();
+      if(lowerCase) {
+        name = name.toLowerCase();
+      }
       // child 0 is the name of the column
-      col.setName(unescapeIdentifier(child.getChild(0).getText()));
+      col.setName(unescapeIdentifier(name));
       // child 1 is the type of the column
       ASTNode typeChild = (ASTNode) (child.getChild(1));
       col.setType(getTypeStringFromAST(typeChild));
@@ -366,7 +374,7 @@ protected List<String> getColumnNames(ASTNode ast) {
     int numCh = ast.getChildCount();
     for (int i = 0; i < numCh; i++) {
       ASTNode child = (ASTNode) ast.getChild(i);
-      colList.add(unescapeIdentifier(child.getText()));
+      colList.add(unescapeIdentifier(child.getText()).toLowerCase());
     }
     return colList;
   }
@@ -377,10 +385,10 @@ protected List<Order> getColumnNamesOrder(ASTNode ast) {
     for (int i = 0; i < numCh; i++) {
       ASTNode child = (ASTNode) ast.getChild(i);
       if (child.getToken().getType() == HiveParser.TOK_TABSORTCOLNAMEASC) {
-        colList.add(new Order(unescapeIdentifier(child.getChild(0).getText()),
+        colList.add(new Order(unescapeIdentifier(child.getChild(0).getText()).toLowerCase(),
             HIVE_COLUMN_ORDER_ASC));
       } else {
-        colList.add(new Order(unescapeIdentifier(child.getChild(0).getText()),
+        colList.add(new Order(unescapeIdentifier(child.getChild(0).getText()).toLowerCase(),
             HIVE_COLUMN_ORDER_DESC));
       }
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 9673d8208a..bf17d4375f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -6323,7 +6323,7 @@ private ASTNode analyzeCreateTable(ASTNode ast, QB qb)
         comment = unescapeSQLString(child.getChild(0).getText());
         break;
       case HiveParser.TOK_TABLEPARTCOLS:
-        partCols = getColumns((ASTNode) child.getChild(0));
+        partCols = getColumns((ASTNode) child.getChild(0), false);
         break;
       case HiveParser.TOK_TABLEBUCKETS:
         bucketCols = getColumnNames((ASTNode) child.getChild(0));
diff --git a/ql/src/test/queries/clientpositive/ct_case_insensitive.q b/ql/src/test/queries/clientpositive/ct_case_insensitive.q
new file mode 100644
index 0000000000..aafb878e72
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/ct_case_insensitive.q
@@ -0,0 +1,5 @@
+DROP TABLE tmp_pyang_bucket3;
+CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS;
+DROP TABLE tmp_pyang_bucket3;
+CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) SORTED BY (USERID) INTO 32 BUCKETS;
+DROP TABLE tmp_pyang_bucket3;
diff --git a/ql/src/test/results/clientpositive/ct_case_insensitive.q.out b/ql/src/test/results/clientpositive/ct_case_insensitive.q.out
new file mode 100644
index 0000000000..91df2556f2
--- /dev/null
+++ b/ql/src/test/results/clientpositive/ct_case_insensitive.q.out
@@ -0,0 +1,24 @@
+PREHOOK: query: DROP TABLE tmp_pyang_bucket3
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE tmp_pyang_bucket3
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tmp_pyang_bucket3
+PREHOOK: query: DROP TABLE tmp_pyang_bucket3
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE tmp_pyang_bucket3
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Output: default@tmp_pyang_bucket3
+PREHOOK: query: CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) SORTED BY (USERID) INTO 32 BUCKETS
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) SORTED BY (USERID) INTO 32 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tmp_pyang_bucket3
+PREHOOK: query: DROP TABLE tmp_pyang_bucket3
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE tmp_pyang_bucket3
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Output: default@tmp_pyang_bucket3
diff --git a/ql/src/test/results/clientpositive/input3.q.out b/ql/src/test/results/clientpositive/input3.q.out
index d194c3519f..5e1a645b7d 100644
--- a/ql/src/test/results/clientpositive/input3.q.out
+++ b/ql/src/test/results/clientpositive/input3.q.out
@@ -64,7 +64,7 @@ STAGE PLANS:
       Alter Table Operator:
         Alter Table
           type: add columns
-          new columns: X double
+          new columns: x double
           old name: TEST3b
 
 
@@ -148,7 +148,7 @@ STAGE PLANS:
       Alter Table Operator:
         Alter Table
           type: replace columns
-          new columns: R1 int, R2 double
+          new columns: r1 int, r2 double
           old name: TEST3c
 
 
@@ -165,7 +165,7 @@ POSTHOOK: type: DESCTABLE
 r1	int	
 r2	double	
 	 	 
-Detailed Table Information	Table(tableName:test3c, dbName:default, owner:njain, createTime:1261267074, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:r1, type:int, comment:null), FieldSchema(name:r2, type:double, comment:null)], location:file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/test3c, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=njain,last_modified_time=1261267075,transient_lastDdlTime=1261267075})	
+Detailed Table Information	Table(tableName:test3c, dbName:default, owner:njain, createTime:1269906076, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:r1, type:int, comment:null), FieldSchema(name:r2, type:double, comment:null)], location:file:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/test3c, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[], parameters:{last_modified_by=njain,last_modified_time=1269906077,transient_lastDdlTime=1269906077}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)	
 PREHOOK: query: DROP TABLE TEST3a
 PREHOOK: type: DROPTABLE
 POSTHOOK: query: DROP TABLE TEST3a
