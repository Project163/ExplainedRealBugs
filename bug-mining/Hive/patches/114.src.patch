diff --git a/CHANGES.txt b/CHANGES.txt
index 9d045135d0..9d1ea0ee41 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -23,6 +23,9 @@ Trunk - unreleased changes
 
     HIVE-385. Split Driver's run into compile and execute.
     (Neil Conway via namit)
+  
+    HIVE-393. Remove unnecessary checks in movetask for file type.
+    (Zheng Shao via namit)
 
 Release 0.3.0 - Unreleased
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index a8a4c72998..3dded40261 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -99,43 +99,44 @@ public int execute() {
         String mesg_detail = " from " + tbd.getSourceDir();
         console.printInfo(mesg, mesg_detail);
 
-        // Get the file format of the table
-        boolean tableIsSequenceFile = tbd.getTable().getInputFileFormatClass().equals(SequenceFileInputFormat.class);
-        // Get all files from the src directory
-        FileStatus [] dirs;
-        ArrayList<FileStatus> files;
-        try {
-          fs = FileSystem.get(db.getTable(tbd.getTable().getTableName()).getDataLocation(),
-              Hive.get().getConf());
-          dirs = fs.globStatus(new Path(tbd.getSourceDir()));
-          files = new ArrayList<FileStatus>();
-          for (int i=0; (dirs != null && i<dirs.length); i++) {
-            files.addAll(Arrays.asList(fs.listStatus(dirs[i].getPath())));
-            // We only check one file, so exit the loop when we have at least one.
-            if (files.size()>0) break;
-          }
-        } catch (IOException e) {
-          throw new HiveException("addFiles: filesystem error in check phase", e);
-        }
-        // Check if the file format of the file matches that of the table.
-        if (files.size() > 0) {
-          int fileId = 0;
-          boolean fileIsSequenceFile = true;   
+        if (work.getCheckFileFormat()) {
+          // Get the file format of the table
+          boolean tableIsSequenceFile = tbd.getTable().getInputFileFormatClass().equals(SequenceFileInputFormat.class);
+          // Get all files from the src directory
+          FileStatus [] dirs;
+          ArrayList<FileStatus> files;
           try {
-            SequenceFile.Reader reader = new SequenceFile.Reader(
-              fs, files.get(fileId).getPath(), conf);
-            reader.close();
+            fs = FileSystem.get(db.getTable(tbd.getTable().getTableName()).getDataLocation(),
+                Hive.get().getConf());
+            dirs = fs.globStatus(new Path(tbd.getSourceDir()));
+            files = new ArrayList<FileStatus>();
+            for (int i=0; (dirs != null && i<dirs.length); i++) {
+              files.addAll(Arrays.asList(fs.listStatus(dirs[i].getPath())));
+              // We only check one file, so exit the loop when we have at least one.
+              if (files.size()>0) break;
+            }
           } catch (IOException e) {
-            fileIsSequenceFile = false;
+            throw new HiveException("addFiles: filesystem error in check phase", e);
           }
-          if (!fileIsSequenceFile && tableIsSequenceFile) {
-            throw new HiveException("Cannot load text files into a table stored as SequenceFile.");
+          // Check if the file format of the file matches that of the table.
+          if (files.size() > 0) {
+            int fileId = 0;
+            boolean fileIsSequenceFile = true;   
+            try {
+              SequenceFile.Reader reader = new SequenceFile.Reader(
+                fs, files.get(fileId).getPath(), conf);
+              reader.close();
+            } catch (IOException e) {
+              fileIsSequenceFile = false;
+            }
+            if (!fileIsSequenceFile && tableIsSequenceFile) {
+              throw new HiveException("Cannot load text files into a table stored as SequenceFile.");
+            }
+            if (fileIsSequenceFile && !tableIsSequenceFile) {
+              throw new HiveException("Cannot load SequenceFiles into a table stored as TextFile.");
+            }
           }
-          if (fileIsSequenceFile && !tableIsSequenceFile) {
-            throw new HiveException("Cannot load SequenceFiles into a table stored as TextFile.");
-          }
-        }
-         
+        }           
 
         if(tbd.getPartitionSpec().size() == 0) {
           db.loadTable(new Path(tbd.getSourceDir()), tbd.getTable().getTableName(), tbd.getReplace());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
index 60fb4b7054..d4891606fa 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
@@ -213,9 +213,9 @@ public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
                                         isOverWrite));
 
     if(rTask != null) {
-      rTask.addDependentTask(TaskFactory.get(new moveWork(loadTableWork, loadFileWork), this.conf));
+      rTask.addDependentTask(TaskFactory.get(new moveWork(loadTableWork, loadFileWork, true), this.conf));
     } else {
-      rTask = TaskFactory.get(new moveWork(loadTableWork, loadFileWork), this.conf);
+      rTask = TaskFactory.get(new moveWork(loadTableWork, loadFileWork, true), this.conf);
     }
 
     rootTasks.add(rTask);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index adb5475a2d..981506f4fa 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -3437,7 +3437,7 @@ private void genMapRedTasks(QB qb) throws SemanticException {
     else {
       // First we generate the move work as this needs to be made dependent on all
       // the tasks that have a file sink operation
-      mv = new moveWork(loadTableWork, loadFileWork);
+      mv = new moveWork(loadTableWork, loadFileWork, false);
       mvTask = TaskFactory.get(mv, this.conf);
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/moveWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/moveWork.java
index f4849c9ec1..59a9d1891a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/moveWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/moveWork.java
@@ -27,12 +27,16 @@ public class moveWork implements Serializable {
   private List<loadTableDesc> loadTableWork;
   private List<loadFileDesc> loadFileWork;
 
+  private boolean checkFileFormat;
+
   public moveWork() { }
   public moveWork(
     final List<loadTableDesc> loadTableWork,
-    final List<loadFileDesc> loadFileWork) {
+    final List<loadFileDesc> loadFileWork,
+    boolean checkFileFormat) {
     this.loadTableWork = loadTableWork;
     this.loadFileWork = loadFileWork;
+    this.checkFileFormat = checkFileFormat;
   }
   @explain(displayName="tables")
   public List<loadTableDesc> getLoadTableWork() {
@@ -49,4 +53,12 @@ public List<loadFileDesc> getLoadFileWork() {
   public void setLoadFileWork(final List<loadFileDesc> loadFileWork) {
     this.loadFileWork=loadFileWork;
   }
+  
+  public boolean getCheckFileFormat() {
+    return checkFileFormat;
+  }
+  public void setCheckFileFormat(boolean checkFileFormat) {
+    this.checkFileFormat = checkFileFormat;
+  }
+  
 }
