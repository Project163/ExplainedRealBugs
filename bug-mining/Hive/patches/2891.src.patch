diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java
index b074ca97e8..d6067e7a23 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java
@@ -134,6 +134,7 @@ public ResultSet run(PreparedStatement stmt) throws SQLException {
       }
     };
 
+    fileID = JDBCStatsUtils.truncateRowId(fileID);
     String keyPrefix = Utilities.escapeSqlLike(fileID) + "%";
     for (int failures = 0;; failures++) {
       try {
@@ -147,7 +148,7 @@ public ResultSet run(PreparedStatement stmt) throws SQLException {
         if (result.next()) {
           retval = result.getLong(1);
         } else {
-          LOG.warn("Warning. Nothing published. Nothing aggregated.");
+          LOG.warn("Nothing published. Nothing aggregated.");
           return null;
         }
         return Long.toString(retval);
@@ -217,6 +218,7 @@ public Void run(PreparedStatement stmt) throws SQLException {
     };
     try {
 
+      rowID = JDBCStatsUtils.truncateRowId(rowID);
       String keyPrefix = Utilities.escapeSqlLike(rowID) + "%";
 
       PreparedStatement delStmt = Utilities.prepareWithRetry(conn,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java
index 5e317ab9e3..c1621e029b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java
@@ -139,7 +139,11 @@ public boolean publishStat(String fileID, Map<String, String> stats) {
           + " stats: " + JDBCStatsUtils.getSupportedStatistics());
       return false;
     }
-    LOG.info("Stats publishing for key " + fileID);
+    String rowId = JDBCStatsUtils.truncateRowId(fileID);
+    if (LOG.isInfoEnabled()) {
+      String truncateSuffix = (rowId != fileID) ? " (from " + fileID + ")" : ""; // object equality
+      LOG.info("Stats publishing for key " + rowId + truncateSuffix);
+    }
 
     Utilities.SQLCommand<Void> execUpdate = new Utilities.SQLCommand<Void>() {
       @Override
@@ -153,7 +157,7 @@ public Void run(PreparedStatement stmt) throws SQLException {
 
     for (int failures = 0;; failures++) {
       try {
-        insStmt.setString(1, fileID);
+        insStmt.setString(1, rowId);
         for (int i = 0; i < JDBCStatsUtils.getSupportedStatistics().size(); i++) {
           insStmt.setString(i + 2, stats.get(supportedStatistics.get(i)));
         }
@@ -172,10 +176,10 @@ public Void run(PreparedStatement stmt) throws SQLException {
             for (i = 0; i < JDBCStatsUtils.getSupportedStatistics().size(); i++) {
               updStmt.setString(i + 1, stats.get(supportedStatistics.get(i)));
             }
-            updStmt.setString(supportedStatistics.size() + 1, fileID);
+            updStmt.setString(supportedStatistics.size() + 1, rowId);
             updStmt.setString(supportedStatistics.size() + 2,
                 stats.get(JDBCStatsUtils.getBasicStat()));
-            updStmt.setString(supportedStatistics.size() + 3, fileID);
+            updStmt.setString(supportedStatistics.size() + 3, rowId);
             Utilities.executeWithRetry(execUpdate, updStmt, waitWindow, maxRetries);
             return true;
           } catch (SQLRecoverableException ue) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsSetupConstants.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsSetupConstants.java
index 70badf228d..b999f8ae99 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsSetupConstants.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsSetupConstants.java
@@ -34,4 +34,7 @@ public final class JDBCStatsSetupConstants {
 
   public static final String PART_STAT_RAW_DATA_SIZE_COLUMN_NAME = "RAW_DATA_SIZE";
 
+  // 255 is an old value that we will keep for now; it can be increased to 4000; limits are
+  // MySQL - 65535, SQL Server - 8000, Oracle - 4000, Derby - 32762, Postgres - large.
+  public static final int ID_COLUMN_VARCHAR_SIZE = 255;
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsUtils.java
index 4625d277bf..383314bdd9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsUtils.java
@@ -24,6 +24,7 @@
 import java.util.Map;
 
 import org.apache.hadoop.hive.common.StatsSetupConst;
+import org.apache.hadoop.util.hash.MurmurHash;
 
 public class JDBCStatsUtils {
 
@@ -124,9 +125,10 @@ public static String getBasicStat() {
    * Prepares CREATE TABLE query
    */
   public static String getCreate(String comment) {
-    String create = "CREATE TABLE /* " + comment + " */ " + JDBCStatsUtils.getStatTableName() +
-          " (" + getTimestampColumnName() + " TIMESTAMP DEFAULT CURRENT_TIMESTAMP, " +
-          JDBCStatsUtils.getIdColumnName() + " VARCHAR(255) PRIMARY KEY ";
+    String create = "CREATE TABLE /* " + comment + " */ " + JDBCStatsUtils.getStatTableName()
+        + " (" + getTimestampColumnName() + " TIMESTAMP DEFAULT CURRENT_TIMESTAMP, "
+        + JDBCStatsUtils.getIdColumnName() + " VARCHAR("
+        + JDBCStatsSetupConstants.ID_COLUMN_VARCHAR_SIZE + ") PRIMARY KEY ";
     for (int i = 0; i < supportedStats.size(); i++) {
       create += ", " + getStatColumnName(supportedStats.get(i)) + " BIGINT ";
     }
@@ -191,4 +193,13 @@ public static String getDeleteAggr(String rowID, String comment) {
     return delete;
   }
 
+  /**
+   * Make sure the row ID fits into the row ID column in the table.
+   * @param rowId Row ID.
+   * @return Resulting row ID truncated to correct length, if necessary.
+   */
+  public static String truncateRowId(String rowId) {
+    return (rowId.length() <= JDBCStatsSetupConstants.ID_COLUMN_VARCHAR_SIZE)
+        ? rowId : Integer.toHexString(MurmurHash.getInstance().hash(rowId.getBytes()));
+  }
 }
