diff --git a/common/src/java/org/apache/hadoop/hive/common/FileUtils.java b/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
index 9e07c08374..9a0521c275 100644
--- a/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
+++ b/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
@@ -32,7 +32,10 @@
 import java.util.Random;
 import java.util.Set;
 
+import com.google.common.annotations.VisibleForTesting;
+
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.ContentSummary;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
@@ -563,21 +566,38 @@ public static boolean copy(FileSystem srcFS, Path src,
     boolean deleteSource,
     boolean overwrite,
     HiveConf conf) throws IOException {
+    return copy(srcFS, src, dstFS, dst, deleteSource, overwrite, conf, ShimLoader.getHadoopShims());
+  }
 
-    HadoopShims shims = ShimLoader.getHadoopShims();
-    boolean copied;
+  @VisibleForTesting
+  static boolean copy(FileSystem srcFS, Path src,
+    FileSystem dstFS, Path dst,
+    boolean deleteSource,
+    boolean overwrite,
+    HiveConf conf, HadoopShims shims) throws IOException {
+
+    boolean copied = false;
+    boolean triedDistcp = false;
 
     /* Run distcp if source file/dir is too big */
-    if (srcFS.getUri().getScheme().equals("hdfs") &&
-        srcFS.getFileStatus(src).getLen() > conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE)) {
-      LOG.info("Source is " + srcFS.getFileStatus(src).getLen() + " bytes. (MAX: " + conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE) + ")");
-      LOG.info("Launch distributed copy (distcp) job.");
-      HiveConfUtil.updateJobCredentialProviders(conf);
-      copied = shims.runDistCp(src, dst, conf);
-      if (copied && deleteSource) {
-        srcFS.delete(src, true);
+    if (srcFS.getUri().getScheme().equals("hdfs")) {
+      ContentSummary srcContentSummary = srcFS.getContentSummary(src);
+      if (srcContentSummary.getFileCount() > conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXNUMFILES)
+              && srcContentSummary.getLength() > conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE)) {
+
+        LOG.info("Source is " + srcContentSummary.getLength() + " bytes. (MAX: " + conf.getLongVar(
+                HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE) + ")");
+        LOG.info("Source is " + srcContentSummary.getFileCount() + " files. (MAX: " + conf.getLongVar(
+                HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXNUMFILES) + ")");
+        LOG.info("Launch distributed copy (distcp) job.");
+        triedDistcp = true;
+        copied = shims.runDistCp(src, dst, conf);
+        if (copied && deleteSource) {
+          srcFS.delete(src, true);
+        }
       }
-    } else {
+    }
+    if (!triedDistcp) {
       copied = FileUtil.copy(srcFS, src, dstFS, dst, deleteSource, overwrite, conf);
     }
 
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index a479deb7c0..f68bd352b4 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -1162,9 +1162,12 @@ public static enum ConfVars {
     HIVE_GROUPBY_LIMIT_EXTRASTEP("hive.groupby.limit.extrastep", true, "This parameter decides if Hive should \n" +
         "create new MR job for sorting final output"),
 
-    // Max filesize used to do a single copy (after that, distcp is used)
+    // Max file num and size used to do a single copy (after that, distcp is used)
+    HIVE_EXEC_COPYFILE_MAXNUMFILES("hive.exec.copyfile.maxnumfiles", 1L,
+        "Maximum number of files Hive uses to do sequential HDFS copies between directories." +
+        "Distributed copies (distcp) will be used instead for larger numbers of files so that copies can be done faster."),
     HIVE_EXEC_COPYFILE_MAXSIZE("hive.exec.copyfile.maxsize", 32L * 1024 * 1024 /*32M*/,
-        "Maximum file size (in Mb) that Hive uses to do single HDFS copies between directories." +
+        "Maximum file size (in bytes) that Hive uses to do single HDFS copies between directories." +
         "Distributed copies (distcp) will be used instead for bigger files so that copies can be done faster."),
 
     // for hive udtf operator
diff --git a/common/src/test/org/apache/hadoop/hive/common/TestFileUtils.java b/common/src/test/org/apache/hadoop/hive/common/TestFileUtils.java
index 5705028489..03fcaeb802 100644
--- a/common/src/test/org/apache/hadoop/hive/common/TestFileUtils.java
+++ b/common/src/test/org/apache/hadoop/hive/common/TestFileUtils.java
@@ -22,18 +22,28 @@
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
+import static org.mockito.Matchers.any;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
 
 import java.io.File;
 import java.io.IOException;
+import java.net.URI;
 import java.util.ArrayList;
 import java.util.HashSet;
 import java.util.Set;
 
+import org.apache.hadoop.fs.ContentSummary;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.LocalFileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.shims.HadoopShims;
+
 import org.junit.Assert;
 import org.junit.Test;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -197,4 +207,26 @@ private void verifyIfParentsContainPath(Path key, Set<Path> parents, boolean exp
     boolean result = parents.contains(key);
     assertEquals("key=" + key, expected, result);
   }
+
+  @Test
+  public void testCopyWithDistcp() throws IOException {
+    Path copySrc = new Path("copySrc");
+    Path copyDst = new Path("copyDst");
+    HiveConf conf = new HiveConf(TestFileUtils.class);
+    conf.set(HiveConf.ConfVars.HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS.varname, "false");
+
+    FileSystem mockFs = mock(FileSystem.class);
+    when(mockFs.getUri()).thenReturn(URI.create("hdfs:///"));
+
+    ContentSummary mockContentSummary = mock(ContentSummary.class);
+    when(mockContentSummary.getFileCount()).thenReturn(Long.MAX_VALUE);
+    when(mockContentSummary.getLength()).thenReturn(Long.MAX_VALUE);
+    when(mockFs.getContentSummary(any(Path.class))).thenReturn(mockContentSummary);
+
+    HadoopShims shims = mock(HadoopShims.class);
+    when(shims.runDistCp(copySrc, copyDst, conf)).thenReturn(true);
+
+    Assert.assertTrue(FileUtils.copy(mockFs, copySrc, mockFs, copyDst, false, false, conf, shims));
+    verify(shims).runDistCp(copySrc, copyDst, conf);
+  }
 }
diff --git a/itests/hive-unit-hadoop2/pom.xml b/itests/hive-unit-hadoop2/pom.xml
index 44135d62e3..d15bd54317 100644
--- a/itests/hive-unit-hadoop2/pom.xml
+++ b/itests/hive-unit-hadoop2/pom.xml
@@ -150,6 +150,11 @@
       <artifactId>hadoop-common</artifactId>
       <version>${hadoop.version}</version>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-distcp</artifactId>
+      <version>${hadoop.version}</version>
+    </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-hdfs</artifactId>
diff --git a/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/common/TestFileUtils.java b/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/common/TestFileUtils.java
new file mode 100644
index 0000000000..f14331563a
--- /dev/null
+++ b/itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/common/TestFileUtils.java
@@ -0,0 +1,103 @@
+package org.apache.hadoop.hive.common;
+
+import java.io.IOException;
+import java.io.OutputStream;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.shims.HadoopShims;
+import org.apache.hadoop.hive.shims.ShimLoader;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ * Integration tests for {{@link FileUtils}. Tests run against a {@link HadoopShims.MiniDFSShim}.
+ */
+public class TestFileUtils {
+
+  private static final Path basePath = new Path("/tmp/");
+
+  private static HiveConf conf;
+  private static FileSystem fs;
+  private static HadoopShims.MiniDFSShim dfs;
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    conf = new HiveConf(TestFileUtils.class);
+    dfs = ShimLoader.getHadoopShims().getMiniDfs(conf, 4, true, null);
+    fs = dfs.getFileSystem();
+  }
+
+  @Test
+  public void testCopySingleEmptyFile() throws IOException {
+    String file1Name = "file1.txt";
+    Path copySrc = new Path(basePath, "copySrc");
+    Path copyDst = new Path(basePath, "copyDst");
+    try {
+      fs.create(new Path(basePath, new Path(copySrc, file1Name))).close();
+      Assert.assertTrue("FileUtils.copy failed to copy data",
+              FileUtils.copy(fs, copySrc, fs, copyDst, false, false, conf));
+
+      Path dstFileName1 = new Path(copyDst, file1Name);
+      Assert.assertTrue(fs.exists(new Path(copyDst, file1Name)));
+      Assert.assertEquals(fs.getFileStatus(dstFileName1).getLen(), 0);
+    } finally {
+      try {
+        fs.delete(copySrc, true);
+        fs.delete(copyDst, true);
+      } catch (IOException e) {
+        // Do nothing
+      }
+    }
+  }
+
+  @Test
+  public void testCopyWithDistcp() throws IOException {
+    String file1Name = "file1.txt";
+    String file2Name = "file2.txt";
+    Path copySrc = new Path(basePath, "copySrc");
+    Path copyDst = new Path(basePath, "copyDst");
+    Path srcFile1 = new Path(basePath, new Path(copySrc, file1Name));
+    Path srcFile2 = new Path(basePath, new Path(copySrc, file2Name));
+    try {
+      OutputStream os1 = fs.create(srcFile1);
+      os1.write(new byte[]{1, 2, 3});
+      os1.close();
+
+      OutputStream os2 = fs.create(srcFile2);
+      os2.write(new byte[]{1, 2, 3});
+      os2.close();
+
+      conf.set(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXNUMFILES.varname, "1");
+      conf.set(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE.varname, "1");
+      Assert.assertTrue("FileUtils.copy failed to copy data",
+              FileUtils.copy(fs, copySrc, fs, copyDst, false, false, conf));
+
+      Path dstFileName1 = new Path(copyDst, file1Name);
+      Assert.assertTrue(fs.exists(new Path(copyDst, file1Name)));
+      Assert.assertEquals(fs.getFileStatus(dstFileName1).getLen(), 3);
+
+      Path dstFileName2 = new Path(copyDst, file2Name);
+      Assert.assertTrue(fs.exists(new Path(copyDst, file2Name)));
+      Assert.assertEquals(fs.getFileStatus(dstFileName2).getLen(), 3);
+    } finally {
+      try {
+        fs.delete(copySrc, true);
+        fs.delete(copyDst, true);
+      } catch (IOException e) {
+        // Do nothing
+      }
+    }
+  }
+
+  @AfterClass
+  public static void shutdown() throws IOException {
+    fs.close();
+    dfs.shutdown();
+  }
+}
