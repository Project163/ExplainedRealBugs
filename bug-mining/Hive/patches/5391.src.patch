diff --git a/beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java b/beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
index 048d744b64..f85d8a35bb 100644
--- a/beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
+++ b/beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
@@ -62,7 +62,7 @@ class BeeLineOpts implements Completer {
   public static final int DEFAULT_MAX_COLUMN_WIDTH = 50;
   public static final int DEFAULT_INCREMENTAL_BUFFER_ROWS = 1000;
 
-  public static String URL_ENV_PREFIX = "BEELINE_URL_";
+  public static final String URL_ENV_PREFIX = "BEELINE_URL_";
 
   private final BeeLine beeLine;
   private boolean autosave = false;
diff --git a/beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java b/beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java
index 181f0d2824..711f6a8903 100644
--- a/beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java
+++ b/beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java
@@ -292,7 +292,7 @@ protected HiveConf getHiveConf() {
 
   // Derby commandline parser
   public static class DerbyCommandParser extends AbstractCommandParser {
-    private static String DERBY_NESTING_TOKEN = "RUN";
+    private static final String DERBY_NESTING_TOKEN = "RUN";
 
     public DerbyCommandParser(String dbOpts, String msUsername, String msPassword,
         HiveConf hiveConf) {
@@ -380,11 +380,11 @@ public String cleanseCommand(String dbCommand) {
 
   // Postgres specific parser
   public static class PostgresCommandParser extends AbstractCommandParser {
-    private static String POSTGRES_NESTING_TOKEN = "\\i";
+    private static final String POSTGRES_NESTING_TOKEN = "\\i";
     @VisibleForTesting
-    public static String POSTGRES_STANDARD_STRINGS_OPT = "SET standard_conforming_strings";
+    public static final String POSTGRES_STANDARD_STRINGS_OPT = "SET standard_conforming_strings";
     @VisibleForTesting
-    public static String POSTGRES_SKIP_STANDARD_STRINGS_DBOPT = "postgres.filter.81";
+    public static final String POSTGRES_SKIP_STANDARD_STRINGS_DBOPT = "postgres.filter.81";
 
     public PostgresCommandParser(String dbOpts, String msUsername, String msPassword,
         HiveConf hiveConf) {
@@ -427,7 +427,7 @@ public boolean isNonExecCommand(String dbCommand) {
 
   //Oracle specific parser
   public static class OracleCommandParser extends AbstractCommandParser {
-    private static String ORACLE_NESTING_TOKEN = "@";
+    private static final String ORACLE_NESTING_TOKEN = "@";
 
     public OracleCommandParser(String dbOpts, String msUsername, String msPassword,
         HiveConf hiveConf) {
@@ -451,7 +451,7 @@ public boolean isNestedScript(String dbCommand) {
 
   //MSSQL specific parser
   public static class MSSQLCommandParser extends AbstractCommandParser {
-    private static String MSSQL_NESTING_TOKEN = ":r";
+    private static final String MSSQL_NESTING_TOKEN = ":r";
 
     public MSSQLCommandParser(String dbOpts, String msUsername, String msPassword,
         HiveConf hiveConf) {
diff --git a/beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java b/beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java
index 2c088c9cb1..18f0c5e16e 100644
--- a/beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java
+++ b/beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java
@@ -89,7 +89,7 @@ public HiveSchemaTool(String hiveHome, HiveConf hiveConf, String dbType)
     }
     this.hiveConf = hiveConf;
     this.dbType = dbType;
-    this.metaStoreSchemaInfo = new MetaStoreSchemaInfo(hiveHome, hiveConf, dbType);
+    this.metaStoreSchemaInfo = new MetaStoreSchemaInfo(hiveHome, dbType);
     userName = hiveConf.get(ConfVars.METASTORE_CONNECTION_USER_NAME.varname);
     try {
       passWord = ShimLoader.getHadoopShims().getPassword(hiveConf,
diff --git a/cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java b/cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java
index f1806a0d99..24550fae2c 100644
--- a/cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java
+++ b/cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java
@@ -54,7 +54,7 @@ public class RCFileCat implements Tool{
   // In verbose mode, print an update per RECORD_PRINT_INTERVAL records
   private static final int RECORD_PRINT_INTERVAL = (1024*1024);
 
-  protected static boolean test=false;
+  protected boolean test = false;
 
   public RCFileCat() {
     super();
@@ -63,12 +63,12 @@ public RCFileCat() {
       onUnmappableCharacter(CodingErrorAction.REPLACE);
   }
 
-  private static CharsetDecoder decoder;
+  private CharsetDecoder decoder;
 
   Configuration conf = null;
 
-  private static String TAB ="\t";
-  private static String NEWLINE ="\r\n";
+  private static final String TAB ="\t";
+  private static final String NEWLINE ="\r\n";
 
   @Override
   public int run(String[] args) throws Exception {
@@ -243,7 +243,7 @@ public void setConf(Configuration conf) {
     this.conf = conf;
   }
 
-  private static String Usage = "RCFileCat [--start=start_offet] [--length=len] [--verbose] " +
+  private static final String Usage = "RCFileCat [--start=start_offet] [--length=len] [--verbose] " +
       "[--column-sizes | --column-sizes-pretty] [--file-sizes] fileName";
 
   public static void main(String[] args) {
@@ -262,7 +262,7 @@ public static void main(String[] args) {
     }
   }
 
-  private static void setupBufferedOutput() {
+  private void setupBufferedOutput() {
     OutputStream pdataOut;
     if (test) {
       pdataOut = System.out;
@@ -275,6 +275,7 @@ private static void setupBufferedOutput() {
         new PrintStream(bos, false);
     System.setOut(ps);
   }
+
   private static void printUsage(String errorMsg) {
     System.err.println(Usage);
     if(errorMsg != null) {
diff --git a/cli/src/test/org/apache/hadoop/hive/cli/TestRCFileCat.java b/cli/src/test/org/apache/hadoop/hive/cli/TestRCFileCat.java
index 11ceb310ec..4cb4a19e94 100644
--- a/cli/src/test/org/apache/hadoop/hive/cli/TestRCFileCat.java
+++ b/cli/src/test/org/apache/hadoop/hive/cli/TestRCFileCat.java
@@ -25,8 +25,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.io.PrintStream;
-import java.net.URI;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -77,7 +75,7 @@ public void testRCFileCat() throws Exception {
     writer.close();
 
     RCFileCat fileCat = new RCFileCat();
-    RCFileCat.test=true;
+    fileCat.test=true;
     fileCat.setConf(new Configuration());
 
     // set fake input and output streams
diff --git a/common/src/java/org/apache/hadoop/hive/common/LogUtils.java b/common/src/java/org/apache/hadoop/hive/common/LogUtils.java
index c2a0d9a727..01b2e7c2e0 100644
--- a/common/src/java/org/apache/hadoop/hive/common/LogUtils.java
+++ b/common/src/java/org/apache/hadoop/hive/common/LogUtils.java
@@ -25,7 +25,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.core.LoggerContext;
 import org.apache.logging.log4j.core.config.Configurator;
 import org.apache.logging.log4j.core.impl.Log4jContextFactory;
 import org.slf4j.Logger;
@@ -45,8 +44,8 @@ public class LogUtils {
   /**
    * Constants for log masking
    */
-  private static String KEY_TO_MASK_WITH = "password";
-  private static String MASKED_VALUE = "###_MASKED_###";
+  private static final String KEY_TO_MASK_WITH = "password";
+  private static final String MASKED_VALUE = "###_MASKED_###";
 
   @SuppressWarnings("serial")
   public static class LogInitializationException extends Exception {
diff --git a/common/src/java/org/apache/hadoop/hive/common/StatsSetupConst.java b/common/src/java/org/apache/hadoop/hive/common/StatsSetupConst.java
index 926b4a6805..a9e17c240d 100644
--- a/common/src/java/org/apache/hadoop/hive/common/StatsSetupConst.java
+++ b/common/src/java/org/apache/hadoop/hive/common/StatsSetupConst.java
@@ -49,7 +49,7 @@
 
 public class StatsSetupConst {
 
-  protected final static Logger LOG = LoggerFactory.getLogger(StatsSetupConst.class.getName());
+  protected static final Logger LOG = LoggerFactory.getLogger(StatsSetupConst.class.getName());
 
   public enum StatDB {
     fs {
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
index 6d1673d362..7188af62ac 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
@@ -189,13 +189,13 @@ public void testVersionCompatibility () throws Exception {
 
   //  write the given version to metastore
   private String getVersion(HiveConf conf) throws HiveMetaException {
-    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, conf, "derby");
+    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, "derby");
     return getMetaStoreVersion();
   }
 
   //  write the given version to metastore
   private void setVersion(HiveConf conf, String version) throws HiveMetaException {
-    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, conf, "derby");
+    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, "derby");
     setMetaStoreVersion(version, "setVersion test");
   }
 
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreSchemaInfo.java b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreSchemaInfo.java
index 9c30ee7add..320902bed5 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreSchemaInfo.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreSchemaInfo.java
@@ -19,7 +19,6 @@
 
 import java.io.BufferedReader;
 import java.io.File;
-import java.io.FileInputStream;
 import java.io.FileNotFoundException;
 import java.io.FileReader;
 import java.io.IOException;
@@ -27,21 +26,19 @@
 import java.util.List;
 import java.util.Map;
 
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hive.common.util.HiveVersionInfo;
 
 import com.google.common.collect.ImmutableMap;
 
 
 public class MetaStoreSchemaInfo {
-  private static String SQL_FILE_EXTENSION=".sql";
-  private static String UPGRADE_FILE_PREFIX="upgrade-";
-  private static String INIT_FILE_PREFIX="hive-schema-";
-  private static String VERSION_UPGRADE_LIST = "upgrade.order";
-  private static String PRE_UPGRADE_PREFIX = "pre-";
+  private static final String SQL_FILE_EXTENSION = ".sql";
+  private static final String UPGRADE_FILE_PREFIX = "upgrade-";
+  private static final String INIT_FILE_PREFIX = "hive-schema-";
+  private static final String VERSION_UPGRADE_LIST = "upgrade.order";
+  private static final String PRE_UPGRADE_PREFIX = "pre-";
   private final String dbType;
   private final String hiveSchemaVersions[];
-  private final HiveConf hiveConf;
   private final String hiveHome;
 
   // Some version upgrades often don't change schema. So they are equivalent to
@@ -55,10 +52,9 @@ public class MetaStoreSchemaInfo {
           "1.2.1", "1.2.0"
       );
 
-  public MetaStoreSchemaInfo(String hiveHome, HiveConf hiveConf, String dbType) throws HiveMetaException {
+  public MetaStoreSchemaInfo(String hiveHome, String dbType) throws HiveMetaException {
     this.hiveHome = hiveHome;
     this.dbType = dbType;
-    this.hiveConf = hiveConf;
     // load upgrade order for the given dbType
     List<String> upgradeOrderList = new ArrayList<String>();
     String upgradeListFile = getMetaStoreScriptDir() + File.separator +
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java
index 6381a21379..f7fad94bfe 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java
@@ -28,8 +28,6 @@
 import java.util.List;
 import java.util.Map;
 
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
@@ -40,7 +38,6 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.shims.HadoopShims;
 
 /**
  * ArchiveUtils.
@@ -48,9 +45,7 @@
  */
 @SuppressWarnings("nls")
 public final class ArchiveUtils {
-  private static final Logger LOG = LoggerFactory.getLogger(ArchiveUtils.class.getName());
-
-  public static String ARCHIVING_LEVEL = "archiving_level";
+  public static final String ARCHIVING_LEVEL = "archiving_level";
 
   /**
    * PartSpecInfo keeps fields and values extracted from partial partition info
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
index e3ace2a4a0..1653136e73 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
@@ -774,7 +774,7 @@ public static TypeInfo getCommonClassForUnionAll(TypeInfo a, TypeInfo b) {
    *
    * @return null if no common class could be found.
    */
-  public static TypeInfo getCommonClassForComparison(TypeInfo a, TypeInfo b) {
+  public static synchronized TypeInfo getCommonClassForComparison(TypeInfo a, TypeInfo b) {
     // If same return one of them
     if (a.equals(b)) {
       return a;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 6693134341..79955e9ee1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -143,7 +143,6 @@
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
 import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.util.Shell;
 import org.apache.hive.common.util.ACLConfigurationParser;
 import org.apache.hive.common.util.ReflectionUtil;
 import org.slf4j.Logger;
@@ -213,11 +212,11 @@ public final class Utilities {
    * The object in the reducer are composed of these top level fields.
    */
 
-  public static String HADOOP_LOCAL_FS = "file:///";
+  public static final String HADOOP_LOCAL_FS = "file:///";
   public static final String HADOOP_LOCAL_FS_SCHEME = "file";
-  public static String MAP_PLAN_NAME = "map.xml";
-  public static String REDUCE_PLAN_NAME = "reduce.xml";
-  public static String MERGE_PLAN_NAME = "merge.xml";
+  public static final String MAP_PLAN_NAME = "map.xml";
+  public static final String REDUCE_PLAN_NAME = "reduce.xml";
+  public static final String MERGE_PLAN_NAME = "merge.xml";
   public static final String INPUT_NAME = "iocontext.input.name";
   public static final String HAS_MAP_WORK = "has.map.work";
   public static final String HAS_REDUCE_WORK = "has.reduce.work";
@@ -226,11 +225,11 @@ public final class Utilities {
   public static final String HIVE_ADDED_JARS = "hive.added.jars";
   public static final String VECTOR_MODE = "VECTOR_MODE";
   public static final String USE_VECTORIZED_INPUT_FILE_FORMAT = "USE_VECTORIZED_INPUT_FILE_FORMAT";
-  public static String MAPNAME = "Map ";
-  public static String REDUCENAME = "Reducer ";
+  public static final String MAPNAME = "Map ";
+  public static final String REDUCENAME = "Reducer ";
 
   @Deprecated
-  protected static String DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX = "mapred.dfsclient.parallelism.max";
+  protected static final String DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX = "mapred.dfsclient.parallelism.max";
 
   /**
    * ReduceField:
@@ -690,8 +689,8 @@ protected void initialize(Class type, Object oldInstance, Object newInstance, En
   // Note: When DDL supports specifying what string to represent null,
   // we should specify "NULL" to represent null in the temp table, and then
   // we can make the following translation deprecated.
-  public static String nullStringStorage = "\\N";
-  public static String nullStringOutput = "NULL";
+  public static final String nullStringStorage = "\\N";
+  public static final String nullStringOutput = "NULL";
 
   public static Random randGen = new Random();
 
@@ -2538,7 +2537,7 @@ public static void setColumnTypeList(JobConf jobConf, Operator op, boolean exclu
     setColumnTypeList(jobConf, rowSchema, excludeVCs);
   }
 
-  public static String suffix = ".hashtable";
+  public static final String suffix = ".hashtable";
 
   public static Path generatePath(Path basePath, String dumpFilePrefix,
       Byte tag, String bigBucketFileName) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
index 5b0c2bf00a..2e27fd5019 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
@@ -1359,7 +1359,7 @@ private String getNewInstanceArgumentString(Object [] args) {
     return "arguments: " + Arrays.toString(args) + ", argument classes: " + argClasses.toString();
   }
 
-  private static int STACK_LENGTH_LIMIT = 15;
+  private static final int STACK_LENGTH_LIMIT = 15;
 
   public static String getStackTraceAsSingleLine(Throwable e) {
     StringBuilder sb = new StringBuilder();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetBytes.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetBytes.java
index 6383e8a3e9..266365e2d5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetBytes.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetBytes.java
@@ -39,8 +39,8 @@ public class CuckooSetBytes {
   private int salt = 0;
   private Random gen = new Random(676983475);
   private int rehashCount = 0;
-  private static long INT_MASK  = 0x00000000ffffffffL;
-  private static long BYTE_MASK = 0x00000000000000ffL;
+  private static final long INT_MASK  = 0x00000000ffffffffL;
+  private static final long BYTE_MASK = 0x00000000000000ffL;
 
   /**
    * Allocate a new set to hold expectedSize values. Re-allocation to expand
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTable.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTable.java
index 9030e5fb0d..b6db3bcc1d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTable.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTable.java
@@ -40,10 +40,10 @@ public abstract class VectorMapJoinFastHashTable implements VectorMapJoinHashTab
   protected int metricExpands;
 
   // 2^30 (we cannot use Integer.MAX_VALUE which is 2^31-1).
-  public static int HIGHEST_INT_POWER_OF_2 = 1073741824;
+  public static final int HIGHEST_INT_POWER_OF_2 = 1073741824;
 
-  public static int ONE_QUARTER_LIMIT = HIGHEST_INT_POWER_OF_2 / 4;
-  public static int ONE_SIXTH_LIMIT = HIGHEST_INT_POWER_OF_2 / 6;
+  public static final int ONE_QUARTER_LIMIT = HIGHEST_INT_POWER_OF_2 / 4;
+  public static final int ONE_SIXTH_LIMIT = HIGHEST_INT_POWER_OF_2 / 6;
 
   public void throwExpandError(int limit, String dataTypeName) {
     throw new RuntimeException(
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryImpl.java
index 6582cdd2d9..c23d202383 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryImpl.java
@@ -46,13 +46,12 @@
  * Each session uses a new object, which creates a new file.
  */
 public class HiveHistoryImpl implements HiveHistory{
+  private static final Logger LOG = LoggerFactory.getLogger("hive.ql.exec.HiveHistoryImpl");
 
   PrintWriter histStream; // History File stream
 
   String histFileName; // History file name
 
-  private static final Logger LOG = LoggerFactory.getLogger("hive.ql.exec.HiveHistoryImpl");
-
   private static final Random randGen = new Random();
 
   private LogHelper console;
@@ -305,7 +304,7 @@ public void progressTask(String queryId, Task<? extends Serializable> task) {
   /**
    * write out counters.
    */
-  static ThreadLocal<Map<String,String>> ctrMapFactory =
+  static final ThreadLocal<Map<String,String>> ctrMapFactory =
       new ThreadLocal<Map<String, String>>() {
     @Override
     protected Map<String,String> initialValue() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndex.java b/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndex.java
index a1408e9bbe..2c3ba7f9e7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndex.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndex.java
@@ -26,10 +26,8 @@
  * Holds index related constants
  */
 public class HiveIndex {
-
   public static final Logger l4j = LoggerFactory.getLogger("HiveIndex");
-
-  public static String INDEX_TABLE_CREATETIME = "hive.index.basetbl.dfs.lastModifiedTime";
+  public static final String INDEX_TABLE_CREATETIME = "hive.index.basetbl.dfs.lastModifiedTime";
 
   public static enum IndexType {
     AGGREGATE_TABLE("aggregate",  AggregateIndexHandler.class.getName()),
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java
index 772711427a..cc69c7eab8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java
@@ -87,7 +87,7 @@ public final class HiveFileFormatUtils {
   public static class FileChecker {
     // we don't have many file formats that implement InputFormatChecker. We won't be holding
     // multiple instances of such classes
-    private static int MAX_CACHE_SIZE = 16;
+    private static final int MAX_CACHE_SIZE = 16;
 
     // immutable maps
     Map<Class<? extends InputFormat>, Class<? extends InputFormatChecker>> inputFormatCheckerMap;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
index 4995bdfe80..010b88c9b1 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
@@ -83,14 +83,13 @@
  */
 public class HiveInputFormat<K extends WritableComparable, V extends Writable>
     implements InputFormat<K, V>, JobConfigurable {
-
   private static final String CLASS_NAME = HiveInputFormat.class.getName();
   private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);
 
   /**
    * A cache of InputFormat instances.
    */
-  private static Map<Class, InputFormat<WritableComparable, Writable>> inputFormats
+  private static final Map<Class, InputFormat<WritableComparable, Writable>> inputFormats
     = new ConcurrentHashMap<Class, InputFormat<WritableComparable, Writable>>();
 
   private JobConf job;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java b/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
index d391164aab..f41edc43a8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
@@ -839,7 +839,7 @@ public static class Writer {
     // the max size of memory for buffering records before writes them out
     private int columnsBufferSize = 4 * 1024 * 1024; // 4M
     // the conf string for COLUMNS_BUFFER_SIZE
-    public static String COLUMNS_BUFFER_SIZE_CONF_STR = "hive.io.rcfile.record.buffer.size";
+    public static final String COLUMNS_BUFFER_SIZE_CONF_STR = "hive.io.rcfile.record.buffer.size";
 
     // how many records already buffered
     private int bufferedRecords = 0;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
index 59682db7cd..8318a62761 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
@@ -158,7 +158,7 @@ static enum SplitStrategyKind {
   }
 
   private static final Logger LOG = LoggerFactory.getLogger(OrcInputFormat.class);
-  private static boolean isDebugEnabled = LOG.isDebugEnabled();
+  private static final boolean isDebugEnabled = LOG.isDebugEnabled();
   static final HadoopShims SHIMS = ShimLoader.getHadoopShims();
 
   private static final long DEFAULT_MIN_SPLIT_SIZE = 16 * 1024 * 1024;
@@ -1913,10 +1913,6 @@ public float getProgress() throws IOException {
     }
   }
 
-  // The schema type description does not include the ACID fields (i.e. it is the
-  // non-ACID original schema).
-  private static boolean SCHEMA_TYPES_IS_ORIGINAL = true;
-
   @Override
   public RowReader<OrcStruct> getReader(InputSplit inputSplit,
                                         Options options)
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
index 90b1dffbc6..77bce9765e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
@@ -31,7 +31,6 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.common.StatsSetupConst;
-import org.apache.hadoop.hive.common.StatsSetupConst.StatDB;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
 import org.apache.hadoop.hive.ql.Context;
@@ -58,7 +57,6 @@
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.mapred.Counters;
 import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobClient;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RunningJob;
@@ -77,8 +75,6 @@
 @SuppressWarnings( { "deprecation"})
 public class PartialScanTask extends Task<PartialScanWork> implements
     Serializable, HadoopJobExecHook {
-
-
   private static final long serialVersionUID = 1L;
 
   protected transient JobConf job;
@@ -274,7 +270,7 @@ public String getName() {
     return "RCFile Statistics Partial Scan";
   }
 
-  public static String INPUT_SEPERATOR = ":";
+  public static final String INPUT_SEPERATOR = ":";
 
   public static void main(String[] args) {
     String inputPathStr = null;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java
index 044d64c6a3..2435bf1613 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java
@@ -60,7 +60,7 @@ public enum VirtualColumn {
    */
   GROUPINGID("GROUPING__ID", TypeInfoFactory.intTypeInfo);
 
-  public static ImmutableSet<String> VIRTUAL_COLUMN_NAMES =
+  public static final ImmutableSet<String> VIRTUAL_COLUMN_NAMES =
       ImmutableSet.of(FILENAME.getName(), BLOCKOFFSET.getName(), ROWOFFSET.getName(),
           RAWDATASIZE.getName(), GROUPINGID.getName(), ROWID.getName());
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
index 0e67ea64c7..88bf829999 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -130,11 +130,7 @@
  * map-reduce tasks.
  */
 public final class GenMapRedUtils {
-  private static Logger LOG;
-
-  static {
-    LOG = LoggerFactory.getLogger("org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils");
-  }
+  private static final Logger LOG = LoggerFactory.getLogger("org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils");
 
   public static boolean needsTagging(ReduceWork rWork) {
     return rWork != null && (rWork.getReducer().getClass() == JoinOperator.class ||
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/ListBucketingPrunerUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/ListBucketingPrunerUtils.java
index 4d3e74e45f..88b8119a28 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/ListBucketingPrunerUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/ListBucketingPrunerUtils.java
@@ -37,10 +37,10 @@
 public final class ListBucketingPrunerUtils {
 
   /* Default list bucketing directory name. internal use only not for client. */
-  public static String HIVE_LIST_BUCKETING_DEFAULT_DIR_NAME =
+  public static final String HIVE_LIST_BUCKETING_DEFAULT_DIR_NAME =
       "HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME";
   /* Default list bucketing directory key. internal use only not for client. */
-  public static String HIVE_LIST_BUCKETING_DEFAULT_KEY = "HIVE_DEFAULT_LIST_BUCKETING_KEY";
+  public static final String HIVE_LIST_BUCKETING_DEFAULT_KEY = "HIVE_DEFAULT_LIST_BUCKETING_KEY";
 
   /**
    * Decide if pruner skips the skewed directory
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
index 93202c36bd..93b8a5d137 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
@@ -383,11 +383,11 @@ public static boolean skewJoinEnabled(HiveConf conf, JoinOperator joinOp) {
     return true;
   }
 
-  private static String skewJoinPrefix = "hive_skew_join";
-  private static String UNDERLINE = "_";
-  private static String BIGKEYS = "bigkeys";
-  private static String SMALLKEYS = "smallkeys";
-  private static String RESULTS = "results";
+  private static final String skewJoinPrefix = "hive_skew_join";
+  private static final String UNDERLINE = "_";
+  private static final String BIGKEYS = "bigkeys";
+  private static final String SMALLKEYS = "smallkeys";
+  private static final String RESULTS = "results";
 
   static Path getBigKeysDir(Path baseDir, Byte srcTbl) {
     return StringInternUtils.internUriStringsInPath(
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
index 50eda15912..a2a3485769 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
@@ -37,7 +37,6 @@
 import org.apache.commons.lang.ArrayUtils;
 
 import org.apache.calcite.util.Pair;
-import org.apache.commons.lang.ArrayUtils;
 import org.apache.commons.lang3.tuple.ImmutablePair;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -45,8 +44,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.*;
 import org.apache.hadoop.hive.ql.exec.mr.MapRedTask;
-import org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask;
-import org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer;
 import org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey;
 import org.apache.hadoop.hive.ql.exec.spark.SparkTask;
 import org.apache.hadoop.hive.ql.exec.tez.TezTask;
@@ -70,10 +67,8 @@
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector.Type;
 import org.apache.hadoop.hive.ql.exec.vector.VectorColumnOutputMapping;
 import org.apache.hadoop.hive.ql.exec.vector.VectorColumnSourceMapping;
-import org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator;
 import org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator;
-import org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.HiveVectorAdaptorUsageMode;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.InConstantType;
@@ -81,7 +76,6 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;
 import org.apache.hadoop.hive.ql.lib.DefaultGraphWalker;
@@ -107,16 +101,13 @@
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
-import org.apache.hadoop.hive.ql.plan.FetchWork;
 import org.apache.hadoop.hive.ql.plan.FileSinkDesc;
 import org.apache.hadoop.hive.ql.plan.FilterDesc;
 import org.apache.hadoop.hive.ql.plan.GroupByDesc;
-import org.apache.hadoop.hive.ql.plan.HashTableSinkDesc;
 import org.apache.hadoop.hive.ql.plan.JoinDesc;
 import org.apache.hadoop.hive.ql.plan.LimitDesc;
 import org.apache.hadoop.hive.ql.plan.MapJoinDesc;
 import org.apache.hadoop.hive.ql.plan.MapWork;
-import org.apache.hadoop.hive.ql.plan.MapredLocalWork;
 import org.apache.hadoop.hive.ql.plan.MapredWork;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.SelectDesc;
@@ -130,7 +121,6 @@
 import org.apache.hadoop.hive.ql.plan.VectorSparkPartitionPruningSinkDesc;
 import org.apache.hadoop.hive.ql.plan.VectorLimitDesc;
 import org.apache.hadoop.hive.ql.plan.VectorMapJoinInfo;
-import org.apache.hadoop.hive.ql.plan.VectorPartitionConversion;
 import org.apache.hadoop.hive.ql.plan.VectorSMBJoinDesc;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;
@@ -149,7 +139,6 @@
 import org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.HashTableKind;
 import org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.OperatorVariation;
 import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc.VectorDeserializeType;
-import org.apache.hadoop.hive.ql.plan.VectorMapJoinInfo;
 import org.apache.hadoop.hive.ql.plan.VectorReduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.VectorReduceSinkInfo;
 import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc;
@@ -198,14 +187,11 @@
 import org.apache.hadoop.hive.ql.udf.UDFYear;
 import org.apache.hadoop.hive.ql.udf.generic.*;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.NullStructSerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe;
-import org.apache.hadoop.hive.serde2.SerDeException;
-import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -219,8 +205,6 @@
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
 import org.apache.hadoop.mapred.TextInputFormat;
 import org.apache.hive.common.util.AnnotationUtils;
-import org.apache.hive.common.util.HiveStringUtils;
-import org.apache.hive.common.util.ReflectionUtil;
 import org.apache.hadoop.util.ReflectionUtils;
 
 import com.google.common.base.Preconditions;
@@ -264,16 +248,12 @@ public class Vectorizer implements PhysicalPlanResolver {
     supportedDataTypesPattern = Pattern.compile(patternBuilder.toString());
   }
 
-  private List<Task<? extends Serializable>> vectorizableTasks =
-      new ArrayList<Task<? extends Serializable>>();
   private Set<Class<?>> supportedGenericUDFs = new HashSet<Class<?>>();
 
   private Set<String> supportedAggregationUdfs = new HashSet<String>();
 
   private HiveConf hiveConf;
 
-  private boolean isSpark;
-
   private boolean useVectorizedInputFileFormat;
   private boolean useVectorDeserialize;
   private boolean useRowDeserialize;
@@ -456,8 +436,6 @@ private class VectorTaskColumnInfo {
 
     Set<Operator<? extends OperatorDesc>> nonVectorizedOps;
 
-    TableScanOperator tableScanOperator;
-
     VectorTaskColumnInfo() {
       partitionColumnCount = 0;
     }
@@ -499,9 +477,6 @@ public void setUseVectorizedInputFileFormat(boolean useVectorizedInputFileFormat
     public void setNonVectorizedOps(Set<Operator<? extends OperatorDesc>> nonVectorizedOps) {
       this.nonVectorizedOps = nonVectorizedOps;
     }
-    public void setTableScanOperator(TableScanOperator tableScanOperator) {
-      this.tableScanOperator = tableScanOperator;
-    }
 
     public Set<Operator<? extends OperatorDesc>> getNonVectorizedOps() {
       return nonVectorizedOps;
@@ -540,12 +515,6 @@ public void transferToBaseWork(BaseWork baseWork) {
 
   class VectorizationDispatcher implements Dispatcher {
 
-    private final PhysicalContext physicalContext;
-
-    public VectorizationDispatcher(PhysicalContext physicalContext) {
-      this.physicalContext = physicalContext;
-    }
-
     @Override
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
         throws SemanticException {
@@ -1030,9 +999,6 @@ private ImmutablePair<Boolean, Boolean> validateInputFormatAndSchemaEvolution(Ma
       vectorTaskColumnInfo.setPartitionColumnCount(partitionColumnCount);
       vectorTaskColumnInfo.setUseVectorizedInputFileFormat(useVectorizedInputFileFormat);
 
-      // Helps to keep this for debugging.
-      vectorTaskColumnInfo.setTableScanOperator(tableScanOperator);
-
       // Always set these so EXPLAIN can see.
       mapWork.setVectorizationInputFileFormatClassNameSet(inputFileFormatClassNameSet);
       mapWork.setVectorizationEnabledConditionsMet(new ArrayList(enabledConditionsMetSet));
@@ -1519,14 +1485,12 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
 
   class MapWorkVectorizationNodeProcessor extends VectorizationNodeProcessor {
 
-    private final MapWork mWork;
     private final VectorTaskColumnInfo vectorTaskColumnInfo;
     private final boolean isTezOrSpark;
 
     public MapWorkVectorizationNodeProcessor(MapWork mWork, boolean isTezOrSpark,
         VectorTaskColumnInfo vectorTaskColumnInfo) {
       super(vectorTaskColumnInfo, vectorTaskColumnInfo.getNonVectorizedOps());
-      this.mWork = mWork;
       this.vectorTaskColumnInfo = vectorTaskColumnInfo;
       this.isTezOrSpark = isTezOrSpark;
     }
@@ -1684,8 +1648,6 @@ public PhysicalContext resolve(PhysicalContext physicalContext) throws SemanticE
       return physicalContext;
     }
 
-    isSpark = (HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("spark"));
-
     useVectorizedInputFileFormat =
         HiveConf.getBoolVar(hiveConf,
             HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT);
@@ -1710,7 +1672,7 @@ public PhysicalContext resolve(PhysicalContext physicalContext) throws SemanticE
     hiveVectorAdaptorUsageMode = HiveVectorAdaptorUsageMode.getHiveConfValue(hiveConf);
 
     // create dispatcher and graph walker
-    Dispatcher disp = new VectorizationDispatcher(physicalContext);
+    Dispatcher disp = new VectorizationDispatcher();
     TaskGraphWalker ogw = new TaskGraphWalker(disp);
 
     // get all the tasks nodes from root task
@@ -2579,8 +2541,6 @@ private boolean canSpecializeMapJoin(Operator<? extends OperatorDesc> op, MapJoi
     List<ExprNodeDesc> keyDesc = desc.getKeys().get(posBigTable);
     VectorExpression[] allBigTableKeyExpressions = vContext.getVectorExpressions(keyDesc);
     final int allBigTableKeyExpressionsLength = allBigTableKeyExpressions.length;
-    boolean isEmptyKey = (allBigTableKeyExpressionsLength == 0);
-
     boolean supportsKeyTypes = true;  // Assume.
     HashSet<String> notSupportedKeyTypes = new HashSet<String>();
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/VectorizerReason.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/VectorizerReason.java
index e0a6198b98..30fdd3067e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/VectorizerReason.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/VectorizerReason.java
@@ -27,7 +27,7 @@
  */
 public class VectorizerReason  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public static enum VectorizerNodeIssue {
     NONE,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index f762feeb1d..41245c8013 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -119,10 +119,10 @@ public abstract class BaseSemanticAnalyzer {
   // whether any ACID table is involved in a query
   protected boolean acidInQuery;
 
-  public static int HIVE_COLUMN_ORDER_ASC = 1;
-  public static int HIVE_COLUMN_ORDER_DESC = 0;
-  public static int HIVE_COLUMN_NULLS_FIRST = 0;
-  public static int HIVE_COLUMN_NULLS_LAST = 1;
+  public static final int HIVE_COLUMN_ORDER_ASC = 1;
+  public static final int HIVE_COLUMN_ORDER_DESC = 0;
+  public static final int HIVE_COLUMN_NULLS_FIRST = 0;
+  public static final int HIVE_COLUMN_NULLS_LAST = 1;
 
   /**
    * ReadEntities that are passed to the hooks.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index f175663f02..fc132929be 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -1808,7 +1808,7 @@ static HashMap<String, String> getProps(ASTNode prop) {
   static class QualifiedNameUtil {
 
     // delimiter to check DOT delimited qualified names
-    static String delimiter = "\\.";
+    static final String delimiter = "\\.";
 
     /**
      * Get the fully qualified name in the ast. e.g. the ast of the form ^(DOT
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java
index 01b5559236..9b03d05b17 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java
@@ -586,7 +586,7 @@ public static enum WindowType
    */
   public static class BoundarySpec implements Comparable<BoundarySpec>
   {
-    public static int UNBOUNDED_AMOUNT = Integer.MAX_VALUE;
+    public static final int UNBOUNDED_AMOUNT = Integer.MAX_VALUE;
 
     Direction direction;
     int amt;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractVectorDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractVectorDesc.java
index e85a418beb..48b7f92820 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractVectorDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractVectorDesc.java
@@ -18,11 +18,9 @@
 
 package org.apache.hadoop.hive.ql.plan;
 
-import org.apache.hadoop.hive.ql.exec.Operator;
-
 public class AbstractVectorDesc implements VectorDesc {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private Class<?> vectorOpClass;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
index 0b49294fff..38a9ef2af1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
@@ -50,7 +50,7 @@ public class GroupByDesc extends AbstractOperatorDesc {
    * MERGEPARTIAL: FINAL for non-distinct aggregations, COMPLETE for distinct
    * aggregations.
    */
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   /**
    * Mode.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
index ca69697f59..940630c236 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
@@ -22,7 +22,6 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
 import java.util.LinkedHashSet;
@@ -32,13 +31,10 @@
 import java.util.Set;
 
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.vector.VectorColumnOutputMapping;
-import org.apache.hadoop.hive.ql.exec.vector.VectorColumnSourceMapping;
 import org.apache.hadoop.hive.ql.plan.Explain.Level;
 import org.apache.hadoop.hive.ql.plan.Explain.Vectorization;
 import org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.HashTableImplementationType;
 import org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.OperatorVariation;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
 /**
  * Map Join operator Descriptor implementation.
@@ -392,7 +388,7 @@ public void setDynamicPartitionHashJoin(boolean isDistributedHashJoin) {
   }
 
   // Use LinkedHashSet to give predictable display order.
-  private static Set<String> vectorizableMapJoinNativeEngines =
+  private static final Set<String> vectorizableMapJoinNativeEngines =
       new LinkedHashSet<String>(Arrays.asList("tez", "spark"));
 
   public class MapJoinOperatorExplainVectorization extends OperatorExplainVectorization {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java
index 9ae30ab8ba..11e9c20a51 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java
@@ -482,7 +482,7 @@ public void setHasOrderBy(boolean hasOrderBy) {
   }
 
   // Use LinkedHashSet to give predictable display order.
-  private static Set<String> vectorizableReduceSinkNativeEngines =
+  private static final Set<String> vectorizableReduceSinkNativeEngines =
       new LinkedHashSet<String>(Arrays.asList("tez", "spark"));
 
   public class ReduceSinkOperatorExplainVectorization extends OperatorExplainVectorization {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorAppMasterEventDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorAppMasterEventDesc.java
index 2e11321684..5aaf2ea250 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorAppMasterEventDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorAppMasterEventDesc.java
@@ -28,7 +28,7 @@
  */
 public class VectorAppMasterEventDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorAppMasterEventDesc() {
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFileSinkDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFileSinkDesc.java
index 325ac91b7c..5a00e5d6ff 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFileSinkDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFileSinkDesc.java
@@ -28,7 +28,7 @@
  */
 public class VectorFileSinkDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorFileSinkDesc() {
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFilterDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFilterDesc.java
index 6feed84c2d..b457f44914 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFilterDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFilterDesc.java
@@ -30,7 +30,7 @@
  */
 public class VectorFilterDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private VectorExpression predicateExpression;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java
index f8554e2047..f9a87257a4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java
@@ -31,7 +31,7 @@
  */
 public class VectorGroupByDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   /**
    *     GLOBAL         No key.  All rows --> 1 full aggregation on end of input
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorLimitDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorLimitDesc.java
index c9bc45a8e6..4093800d48 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorLimitDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorLimitDesc.java
@@ -28,7 +28,7 @@
  */
 public class VectorLimitDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorLimitDesc() {
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinDesc.java
index 3aa65d3e78..60400de1c5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinDesc.java
@@ -35,7 +35,7 @@
  */
 public class VectorMapJoinDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public static enum HashTableImplementationType {
     NONE,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinInfo.java
index 9429785db2..7432efa275 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinInfo.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinInfo.java
@@ -36,7 +36,7 @@
  */
 public class VectorMapJoinInfo {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private int[] bigTableKeyColumnMap;
   private String[] bigTableKeyColumnNames;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPartitionDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPartitionDesc.java
index 4078c7d2fe..bb8f5e13b3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPartitionDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPartitionDesc.java
@@ -34,7 +34,7 @@
  */
 public class VectorPartitionDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   // Data Type Conversion Needed?
   //
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkDesc.java
index 2eb44b88d6..445dccaec2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkDesc.java
@@ -28,7 +28,7 @@
  */
 public class VectorReduceSinkDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public static enum ReduceSinkKeyType {
     NONE,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java
index 8c35415846..da6e606fa6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java
@@ -33,7 +33,7 @@
  */
 public class VectorReduceSinkInfo {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private int[] reduceSinkKeyColumnMap;
   private TypeInfo[] reduceSinkKeyTypeInfos;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSMBJoinDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSMBJoinDesc.java
index 031f11e6c1..41919b6daf 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSMBJoinDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSMBJoinDesc.java
@@ -28,7 +28,7 @@
  */
 public class VectorSMBJoinDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorSMBJoinDesc() {
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSelectDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSelectDesc.java
index c2c9450209..1f6ef185a6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSelectDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSelectDesc.java
@@ -30,7 +30,7 @@
  */
 public class VectorSelectDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private VectorExpression[] selectExpressions;
   private int[] projectedOutputColumns;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkHashTableSinkDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkHashTableSinkDesc.java
index 7fb59dbf01..e43b0f67c5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkHashTableSinkDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkHashTableSinkDesc.java
@@ -28,7 +28,7 @@
  */
 public class VectorSparkHashTableSinkDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorSparkHashTableSinkDesc() {
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkPartitionPruningSinkDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkPartitionPruningSinkDesc.java
index c0bc7e43b6..6c65f838b5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkPartitionPruningSinkDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkPartitionPruningSinkDesc.java
@@ -28,7 +28,7 @@
  */
 public class VectorSparkPartitionPruningSinkDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorSparkPartitionPruningSinkDesc() {
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorTableScanDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorTableScanDesc.java
index 6e5ebe49d1..84729a59f3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorTableScanDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorTableScanDesc.java
@@ -28,7 +28,7 @@
  */
 public class VectorTableScanDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private int[] projectedOutputColumns;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/processors/HiveCommand.java b/ql/src/java/org/apache/hadoop/hive/ql/processors/HiveCommand.java
index 145808b930..bd1b4f5407 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/processors/HiveCommand.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/processors/HiveCommand.java
@@ -36,7 +36,7 @@ public enum HiveCommand {
   DELETE(),
   COMPILE();
 
-  public static boolean ONLY_FOR_TESTING = true;
+  public static final boolean ONLY_FOR_TESTING = true;
   private boolean usedOnlyForTesting;
 
   HiveCommand() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFInternalInterval.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFInternalInterval.java
index fcf291a006..0b29c6a885 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFInternalInterval.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFInternalInterval.java
@@ -51,7 +51,7 @@
 
 public class GenericUDFInternalInterval extends GenericUDF {
 
-  private static Map<Integer, IntervalProcessor> processorMap;
+  private Map<Integer, IntervalProcessor> processorMap;
 
   private transient IntervalProcessor processor;
   private transient PrimitiveObjectInspector inputOI;
@@ -286,7 +286,7 @@ protected HiveIntervalYearMonth getIntervalYearMonth(String arg) {
     }
   }
 
-  private static Map<Integer, IntervalProcessor> getProcessorMap() {
+  private Map<Integer, IntervalProcessor> getProcessorMap() {
 
     if (processorMap != null) {
       return processorMap;
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CheckFastRowHashMap.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CheckFastRowHashMap.java
index 638ccc527c..72fceb94b7 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CheckFastRowHashMap.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CheckFastRowHashMap.java
@@ -90,16 +90,11 @@ public static void verifyHashMapRows(List<Object[]> rows, int[] actualToValueMap
     }
   }
 
-  private static String debugDetailedReadPositionString;
-
-  private static String debugDetailedHashMapResultPositionString;
-
-  private static String debugExceptionMessage;
-  private static StackTraceElement[] debugStackTrace;
-
   public static void verifyHashMapRowsMore(List<Object[]> rows, int[] actualToValueMap,
       VectorMapJoinHashMapResult hashMapResult, TypeInfo[] typeInfos,
       int clipIndex, boolean useExactBytes) throws IOException {
+    String debugExceptionMessage = null;
+    StackTraceElement[] debugStackTrace = null;
 
     final int count = rows.size();
     final int columnCount = typeInfos.length;
@@ -134,7 +129,6 @@ public static void verifyHashMapRowsMore(List<Object[]> rows, int[] actualToValu
 
       boolean thrown = false;
       Exception saveException = null;
-      boolean notExpected = false;
       int index = 0;
       try {
         for (index = 0; index < columnCount; index++) {
@@ -144,9 +138,9 @@ public static void verifyHashMapRowsMore(List<Object[]> rows, int[] actualToValu
       } catch (Exception e) {
         thrown = true;
         saveException = e;
-        debugDetailedReadPositionString = lazyBinaryDeserializeRead.getDetailedReadPositionString();
+        lazyBinaryDeserializeRead.getDetailedReadPositionString();
 
-        debugDetailedHashMapResultPositionString = hashMapResult.getDetailedHashMapResultPositionString();
+        hashMapResult.getDetailedHashMapResultPositionString();
 
         debugExceptionMessage = saveException.getMessage();
         debugStackTrace = saveException.getStackTrace();
@@ -159,7 +153,6 @@ public static void verifyHashMapRowsMore(List<Object[]> rows, int[] actualToValu
           if (saveException instanceof EOFException) {
             // This is the one we are expecting.
           } else if (saveException instanceof ArrayIndexOutOfBoundsException) {
-            notExpected = true;
           } else {
             TestCase.fail("Expecting an EOFException to be thrown for the clipped case...");
           }
@@ -385,7 +378,7 @@ public void verify(VectorMapJoinFastHashTable map,
     }
   }
 
-  static int STACK_LENGTH_LIMIT = 20;
+  static final int STACK_LENGTH_LIMIT = 20;
   public static String getStackTraceAsSingleLine(StackTraceElement[] stackTrace) {
     StringBuilder sb = new StringBuilder();
     sb.append("Stack trace: ");
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CommonFastHashTable.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CommonFastHashTable.java
index 90e8f331f8..ff518703a9 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CommonFastHashTable.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CommonFastHashTable.java
@@ -31,9 +31,9 @@ public class CommonFastHashTable {
   protected static final int LARGE_CAPACITY = 8388608;
   protected static Random random;
 
-  protected static int MAX_KEY_LENGTH = 100;
+  protected static final int MAX_KEY_LENGTH = 100;
 
-  protected static int MAX_VALUE_LENGTH = 1000;
+  protected static final int MAX_VALUE_LENGTH = 1000;
 
   public static int generateLargeCount() {
     int count = 0;
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
index 6802a059d5..5ecfbca445 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
@@ -65,7 +65,7 @@ public final class SerDeUtils {
   public static final char COMMA = ',';
   // we should use '\0' for COLUMN_NAME_DELIMITER if column name contains COMMA
   // but we should also take care of the backward compatibility
-  public static char COLUMN_COMMENTS_DELIMITER = '\0';
+  public static final char COLUMN_COMMENTS_DELIMITER = '\0';
   public static final String LBRACKET = "[";
   public static final String RBRACKET = "]";
   public static final String LBRACE = "{";
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java b/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
index 88c3da9128..ecfe15f59d 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
@@ -75,7 +75,7 @@ class AvroDeserializer {
    * Flag to print the re-encoding warning message only once. Avoid excessive logging for each
    * record encoding.
    */
-  private static boolean warnedOnce = false;
+  private boolean warnedOnce = false;
   /**
    * When encountering a record with an older schema than the one we're trying
    * to read, it is necessary to re-encode with a reader against the newer schema.
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java
index 3b35baf5c3..606b246b5d 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java
@@ -666,9 +666,7 @@ public void logExceptionMessage(byte[] bytes, int bytesStart, int bytesLength, S
 
   //------------------------------------------------------------------------------------------------
 
-  private static byte[] maxLongBytes = ((Long) Long.MAX_VALUE).toString().getBytes();
-  private static int maxLongDigitsCount = maxLongBytes.length;
-  private static byte[] minLongNoSignBytes = ((Long) Long.MIN_VALUE).toString().substring(1).getBytes();
+  private static final byte[] maxLongBytes = ((Long) Long.MAX_VALUE).toString().getBytes();
 
   public static int byteArrayCompareRanges(byte[] arg1, int start1, byte[] arg2, int start2, int len) {
     for (int i = 0; i < len; i++) {
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java
index f50b4fd50e..c784b69a57 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java
@@ -15,12 +15,12 @@
 import java.nio.charset.StandardCharsets;
 
 public class StringToDouble {
-  static int maxExponent = 511;	/* Largest possible base 10 exponent.  Any
+  static final int maxExponent = 511;	/* Largest possible base 10 exponent.  Any
 				 * exponent larger than this will already
 				 * produce underflow or overflow, so there's
 				 * no need to worry about additional digits.
 				 */
-  static double powersOf10[] = {	/* Table giving binary powers of 10.  Entry */
+  static final double powersOf10[] = {	/* Table giving binary powers of 10.  Entry */
       10.,			/* is 10^2^i.  Used to convert decimal */
       100.,			/* exponents into floating-point numbers. */
       1.0e4,
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java
index f4ac56f9c1..8237b647f1 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java
@@ -402,7 +402,7 @@ public static int writeVLongToByteArray(byte[] bytes, int offset, long l) {
     return 1 + len;
   }
 
-  public static int VLONG_BYTES_LEN = 9;
+  public static final int VLONG_BYTES_LEN = 9;
 
   private static ThreadLocal<byte[]> vLongBytesThreadLocal = new ThreadLocal<byte[]>() {
     @Override
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
index 14349fabca..54964e407d 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
@@ -869,7 +869,7 @@ public static int getCharacterLengthForType(PrimitiveTypeInfo typeInfo) {
     }
   }
 
-  public static void registerNumericType(PrimitiveCategory primitiveCategory, int level) {
+  public static synchronized void registerNumericType(PrimitiveCategory primitiveCategory, int level) {
     numericTypeList.add(primitiveCategory);
     numericTypes.put(primitiveCategory, level);
   }
diff --git a/shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java b/shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java
index 72704261de..277738fac7 100644
--- a/shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java
+++ b/shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java
@@ -49,12 +49,12 @@
 import com.google.common.collect.Iterables;
 
 public class HdfsUtils {
+  private static final Logger LOG = LoggerFactory.getLogger("shims.HdfsUtils");
 
   // TODO: this relies on HDFS not changing the format; we assume if we could get inode ID, this
   //       is still going to work. Otherwise, file IDs can be turned off. Later, we should use
   //       as public utility method in HDFS to obtain the inode-based path.
-  private static String HDFS_ID_PATH_PREFIX = "/.reserved/.inodes/";
-  static Logger LOG = LoggerFactory.getLogger("shims.HdfsUtils");
+  private static final String HDFS_ID_PATH_PREFIX = "/.reserved/.inodes/";
 
   public static Path getFileIdPath(
       FileSystem fileSystem, Path path, long fileId) {
diff --git a/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java b/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
index a58f1f28e0..d937ddfb28 100644
--- a/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
+++ b/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
@@ -35,7 +35,7 @@
  */
 public class HiveIOExceptionHandlerChain {
 
-  public static String HIVE_IO_EXCEPTION_HANDLE_CHAIN = "hive.io.exception.handlers";
+  public static final String HIVE_IO_EXCEPTION_HANDLE_CHAIN = "hive.io.exception.handlers";
 
   @SuppressWarnings("unchecked")
   public static HiveIOExceptionHandlerChain getHiveIOExceptionHandlerChain(
diff --git a/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java b/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
index d972edb20a..6af3c8c1d4 100644
--- a/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
+++ b/shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
@@ -24,10 +24,10 @@
 
 public class HiveIOExceptionHandlerUtil {
 
-  private static ThreadLocal<HiveIOExceptionHandlerChain> handlerChainInstance =
+  private static final ThreadLocal<HiveIOExceptionHandlerChain> handlerChainInstance =
     new ThreadLocal<HiveIOExceptionHandlerChain>();
 
-  private static HiveIOExceptionHandlerChain get(JobConf job) {
+  private static synchronized HiveIOExceptionHandlerChain get(JobConf job) {
     HiveIOExceptionHandlerChain cache = HiveIOExceptionHandlerUtil.handlerChainInstance
         .get();
     if (cache == null) {
diff --git a/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java b/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
index 44f24b244d..f15e7ff53f 100644
--- a/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
+++ b/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
@@ -32,7 +32,7 @@
  */
 public abstract class ShimLoader {
   private static final Logger LOG = LoggerFactory.getLogger(ShimLoader.class);
-  public static String HADOOP23VERSIONNAME = "0.23";
+  public static final String HADOOP23VERSIONNAME = "0.23";
 
   private static volatile HadoopShims hadoopShims;
   private static JettyShims jettyShims;
diff --git a/storage-api/src/java/org/apache/hadoop/hive/common/type/FastHiveDecimalImpl.java b/storage-api/src/java/org/apache/hadoop/hive/common/type/FastHiveDecimalImpl.java
index 7a565dd00d..f733c1ecb5 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/common/type/FastHiveDecimalImpl.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/common/type/FastHiveDecimalImpl.java
@@ -145,7 +145,6 @@ public class FastHiveDecimalImpl extends FastHiveDecimal {
    * Int: 8 decimal digits.  An even number and 1/2 of MAX_LONGWORD_DECIMAL.
    */
   private static final int INTWORD_DECIMAL_DIGITS = 8;
-  private static final int MAX_INTWORD_DECIMAL = (int) powerOfTenTable[INTWORD_DECIMAL_DIGITS] - 1;
   private static final int MULTIPLER_INTWORD_DECIMAL = (int) powerOfTenTable[INTWORD_DECIMAL_DIGITS];
 
   /**
@@ -164,9 +163,6 @@ public class FastHiveDecimalImpl extends FastHiveDecimal {
   private static final long MAX_HIGHWORD_DECIMAL =
       powerOfTenTable[HIGHWORD_DECIMAL_DIGITS] - 1;
 
-  private static long HIGHWORD_DIVIDE_FACTOR = powerOfTenTable[LONGWORD_DECIMAL_DIGITS - HIGHWORD_DECIMAL_DIGITS];
-  private static long HIGHWORD_MULTIPLY_FACTOR = powerOfTenTable[HIGHWORD_DECIMAL_DIGITS];
-
   // 38 * 2 or 76 full decimal maximum - (64 + 8) digits in 4 lower longs (4 digits here).
   private static final long FULL_MAX_HIGHWORD_DECIMAL =
       powerOfTenTable[MAX_DECIMAL_DIGITS * 2 - (FOUR_X_LONGWORD_DECIMAL_DIGITS + INTWORD_DECIMAL_DIGITS)] - 1;
@@ -189,11 +185,6 @@ public class FastHiveDecimalImpl extends FastHiveDecimal {
       BigInteger.ONE.add(BIG_INTEGER_MAX_LONGWORD_DECIMAL);
   private static final BigInteger BIG_INTEGER_LONGWORD_MULTIPLIER_2X =
       BIG_INTEGER_LONGWORD_MULTIPLIER.multiply(BIG_INTEGER_LONGWORD_MULTIPLIER);
-  private static final BigInteger BIG_INTEGER_LONGWORD_MULTIPLIER_3X =
-      BIG_INTEGER_LONGWORD_MULTIPLIER_2X.multiply(BIG_INTEGER_LONGWORD_MULTIPLIER);
-  private static final BigInteger BIG_INTEGER_LONGWORD_MULTIPLIER_4X =
-      BIG_INTEGER_LONGWORD_MULTIPLIER_3X.multiply(BIG_INTEGER_LONGWORD_MULTIPLIER);
-
   private static final BigInteger BIG_INTEGER_MAX_HIGHWORD_DECIMAL =
       BigInteger.valueOf(MAX_HIGHWORD_DECIMAL);
   private static final BigInteger BIG_INTEGER_HIGHWORD_MULTIPLIER =
@@ -203,21 +194,21 @@ public class FastHiveDecimalImpl extends FastHiveDecimal {
   // conversion.
 
   // There is only one blank in UTF-8.
-  private final static byte BYTE_BLANK = (byte) ' ';
+  private static final byte BYTE_BLANK = (byte) ' ';
 
-  private final static byte BYTE_DIGIT_ZERO = (byte) '0';
-  private final static byte BYTE_DIGIT_NINE = (byte) '9';
+  private static final byte BYTE_DIGIT_ZERO = (byte) '0';
+  private static final byte BYTE_DIGIT_NINE = (byte) '9';
 
   // Decimal point.
-  private final static byte BYTE_DOT = (byte) '.';
+  private static final byte BYTE_DOT = (byte) '.';
 
   // Sign.
-  private final static byte BYTE_MINUS = (byte) '-';
-  private final static byte BYTE_PLUS = (byte) '+';
+  private static final byte BYTE_MINUS = (byte) '-';
+  private static final byte BYTE_PLUS = (byte) '+';
 
   // Exponent E or e.
-  private final static byte BYTE_EXPONENT_LOWER = (byte) 'e';
-  private final static byte BYTE_EXPONENT_UPPER = (byte) 'E';
+  private static final byte BYTE_EXPONENT_LOWER = (byte) 'e';
+  private static final byte BYTE_EXPONENT_UPPER = (byte) 'E';
 
   //************************************************************************************************
   // Initialize (fastSetFrom*).
@@ -1758,7 +1749,7 @@ private static boolean doDecimalToBinaryConversion(
    *      4,611,686,018,427,387,904 or
    *      461,1686018427387904 (16 digit comma'd)
    */
-  private static FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_62 =
+  private static final FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_62 =
       new FastHiveDecimal(1, 1686018427387904L, 461L, 0, 19, 0);
 
   /*
@@ -1769,7 +1760,7 @@ private static boolean doDecimalToBinaryConversion(
    *      9,223,372,036,854,775,808 or
    *      922,3372036854775808 (16 digit comma'd)
    */
-  private static FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_63 =
+  private static final FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_63 =
       new FastHiveDecimal(1, 3372036854775808L, 922L, 0, 19, 0);
 
   /*
@@ -1784,7 +1775,7 @@ private static boolean doDecimalToBinaryConversion(
    *      42,535,295,865,117,307,932,921,825,928,971,026,432 or
    *      425352,9586511730793292,1825928971026432  (16 digit comma'd)
    */
-  private static FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_125 =
+  private static final FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_125 =
       new FastHiveDecimal(1, 1825928971026432L, 9586511730793292L, 425352L, 38, 0);
 
   /*
@@ -1797,7 +1788,7 @@ private static boolean doDecimalToBinaryConversion(
    *
    * 3*16 (48) + 15 --> 63 down shift.
    */
-  private static FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_63_INVERSE =
+  private static final FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_63_INVERSE =
       new FastHiveDecimal(1, 6994171142578125L, 5044340074528008L, 1084202172485L, 45, 0);
 
   /*
@@ -2141,7 +2132,7 @@ public static boolean fastSerializationUtilsWrite(OutputStream outputStream,
    *      72,057,594,037,927,936 or
    *      7,2057594037927936  (16 digit comma'd)
    */
-  private static FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_56 =
+  private static final FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_56 =
       new FastHiveDecimal(1, 2057594037927936L, 7L, 0, 17, 0);
 
   /*
@@ -2154,7 +2145,7 @@ public static boolean fastSerializationUtilsWrite(OutputStream outputStream,
    *      5,192,296,858,534,827,628,530,496,329,220,096 or
    *      51,9229685853482762,8530496329220096  (16 digit comma'd)
    */
-  private static FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_112 =
+  private static final FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_112 =
       new FastHiveDecimal(1, 8530496329220096L, 9229685853482762L, 51L, 34, 0);
 
   // Multiply by 1/2^56 or 1.387778780781445675529539585113525390625e-17 to divide by 2^56.
@@ -2164,7 +2155,7 @@ public static boolean fastSerializationUtilsWrite(OutputStream outputStream,
   //
   // 3*16 (48) + 8 --> 56 down shift.
   //
-  private static FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_56_INVERSE =
+  private static final FastHiveDecimal FAST_HIVE_DECIMAL_TWO_POWER_56_INVERSE =
       new FastHiveDecimal(1, 9585113525390625L, 8078144567552953L, 13877787L, 40, 0);
 
   /*
@@ -2175,16 +2166,16 @@ public static boolean fastSerializationUtilsWrite(OutputStream outputStream,
   private static final int BIG_INTEGER_BYTES_QUOTIENT_INTEGER_WORD_NUM = 3;
   private static final int BIG_INTEGER_BYTES_QUOTIENT_INTEGER_DIGIT_NUM = 8;
 
-  private static int INITIAL_SHIFT = 48;   // 56 bits minus 1 byte.
+  private static final int INITIAL_SHIFT = 48;   // 56 bits minus 1 byte.
 
   // Long masks and values.
-  private static long LONG_56_BIT_MASK = 0xFFFFFFFFFFFFFFL;
-  private static long LONG_TWO_TO_56_POWER = LONG_56_BIT_MASK + 1L;
-  private static long LONG_BYTE_MASK = 0xFFL;
-  private static long LONG_BYTE_HIGH_BIT_MASK = 0x80L;
+  private static final long LONG_56_BIT_MASK = 0xFFFFFFFFFFFFFFL;
+  private static final long LONG_TWO_TO_56_POWER = LONG_56_BIT_MASK + 1L;
+  private static final long LONG_BYTE_MASK = 0xFFL;
+  private static final long LONG_BYTE_HIGH_BIT_MASK = 0x80L;
 
   // Byte values.
-  private static byte BYTE_ALL_BITS = (byte) 0xFF;
+  private static final byte BYTE_ALL_BITS = (byte) 0xFF;
 
   /**
    * Convert bytes in the format used by BigInteger's toByteArray format (and accepted by its
@@ -2838,32 +2829,32 @@ public static int fastBigIntegerBytesScaled(
   // Decimal to Integer conversion.
 
   private static final int MAX_BYTE_DIGITS = 3;
-  private static FastHiveDecimal FASTHIVEDECIMAL_MIN_BYTE_VALUE_MINUS_ONE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MIN_BYTE_VALUE_MINUS_ONE =
       new FastHiveDecimal((long) Byte.MIN_VALUE - 1L);
-  private static FastHiveDecimal FASTHIVEDECIMAL_MAX_BYTE_VALUE_PLUS_ONE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MAX_BYTE_VALUE_PLUS_ONE =
       new FastHiveDecimal((long) Byte.MAX_VALUE + 1L);
 
   private static final int MAX_SHORT_DIGITS = 5;
-  private static FastHiveDecimal FASTHIVEDECIMAL_MIN_SHORT_VALUE_MINUS_ONE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MIN_SHORT_VALUE_MINUS_ONE =
       new FastHiveDecimal((long) Short.MIN_VALUE - 1L);
-  private static FastHiveDecimal FASTHIVEDECIMAL_MAX_SHORT_VALUE_PLUS_ONE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MAX_SHORT_VALUE_PLUS_ONE =
       new FastHiveDecimal((long) Short.MAX_VALUE + 1L);
 
   private static final int MAX_INT_DIGITS = 10;
-  private static FastHiveDecimal FASTHIVEDECIMAL_MIN_INT_VALUE_MINUS_ONE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MIN_INT_VALUE_MINUS_ONE =
       new FastHiveDecimal((long) Integer.MIN_VALUE - 1L);
-  private static FastHiveDecimal FASTHIVEDECIMAL_MAX_INT_VALUE_PLUS_ONE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MAX_INT_VALUE_PLUS_ONE =
       new FastHiveDecimal((long) Integer.MAX_VALUE + 1L);
 
-  private static FastHiveDecimal FASTHIVEDECIMAL_MIN_LONG_VALUE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MIN_LONG_VALUE =
       new FastHiveDecimal(Long.MIN_VALUE);
-  private static FastHiveDecimal FASTHIVEDECIMAL_MAX_LONG_VALUE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MAX_LONG_VALUE =
       new FastHiveDecimal(Long.MAX_VALUE);
   private static final int MAX_LONG_DIGITS =
       FASTHIVEDECIMAL_MAX_LONG_VALUE.fastIntegerDigitCount;
-  private static FastHiveDecimal FASTHIVEDECIMAL_MIN_LONG_VALUE_MINUS_ONE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MIN_LONG_VALUE_MINUS_ONE =
       new FastHiveDecimal("-9223372036854775809");
-  private static FastHiveDecimal FASTHIVEDECIMAL_MAX_LONG_VALUE_PLUS_ONE =
+  private static final FastHiveDecimal FASTHIVEDECIMAL_MAX_LONG_VALUE_PLUS_ONE =
       new FastHiveDecimal("9223372036854775808");
 
   private static final BigInteger BIG_INTEGER_UNSIGNED_BYTE_MAX_VALUE = BIG_INTEGER_TWO.pow(Byte.SIZE).subtract(BigInteger.ONE);
@@ -9355,7 +9346,7 @@ public static void fastRaiseInvalidException(
   //************************************************************************************************
   // Decimal Debugging.
 
-  static int STACK_LENGTH_LIMIT = 20;
+  static final int STACK_LENGTH_LIMIT = 20;
   public static String getStackTraceAsSingleLine(StackTraceElement[] stackTrace) {
     StringBuilder sb = new StringBuilder();
     sb.append("Stack trace: ");
diff --git a/storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java b/storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java
index 8d950a2e26..eeb3359b3f 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java
@@ -102,7 +102,7 @@ public static Date getRandDate(Random r) {
   public static final long MILLISECONDS_PER_SECOND = TimeUnit.SECONDS.toMillis(1);
   public static final long NANOSECONDS_PER_MILLISSECOND = TimeUnit.MILLISECONDS.toNanos(1);
 
-  private static ThreadLocal<DateFormat> DATE_FORMAT =
+  private static final ThreadLocal<DateFormat> DATE_FORMAT =
       new ThreadLocal<DateFormat>() {
         @Override
         protected DateFormat initialValue() {
@@ -111,10 +111,10 @@ protected DateFormat initialValue() {
       };
 
   // We've switched to Joda/Java Calendar which has a more limited time range....
-  public static int MIN_YEAR = 1900;
-  public static int MAX_YEAR = 3000;
-  private static long MIN_FOUR_DIGIT_YEAR_MILLIS = parseToMillis("1900-01-01 00:00:00");
-  private static long MAX_FOUR_DIGIT_YEAR_MILLIS = parseToMillis("3000-01-01 00:00:00");
+  public static final int MIN_YEAR = 1900;
+  public static final int MAX_YEAR = 3000;
+  private static final long MIN_FOUR_DIGIT_YEAR_MILLIS = parseToMillis("1900-01-01 00:00:00");
+  private static final long MAX_FOUR_DIGIT_YEAR_MILLIS = parseToMillis("3000-01-01 00:00:00");
 
   private static long parseToMillis(String s) {
     try {
diff --git a/testutils/src/java/org/apache/hive/testutils/jdbc/HiveBurnInClient.java b/testutils/src/java/org/apache/hive/testutils/jdbc/HiveBurnInClient.java
index 41ade5fae3..140c198440 100644
--- a/testutils/src/java/org/apache/hive/testutils/jdbc/HiveBurnInClient.java
+++ b/testutils/src/java/org/apache/hive/testutils/jdbc/HiveBurnInClient.java
@@ -26,10 +26,10 @@
 
 
 public class HiveBurnInClient {
-  private static String driverName = "org.apache.hive.jdbc.HiveDriver";
+  private static final String driverName = "org.apache.hive.jdbc.HiveDriver";
 
   //default 80k (runs slightly over 1 day long)
-  private final static int NUM_QUERY_ITERATIONS = 80000;
+  private static final int NUM_QUERY_ITERATIONS = 80000;
 
   /**
    * Creates 2 tables to query from
