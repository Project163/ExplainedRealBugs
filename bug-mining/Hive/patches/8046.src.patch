diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveFilterProjectTransposeRule.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveFilterProjectTransposeRule.java
index 8a806029bc..02b4a539ab 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveFilterProjectTransposeRule.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveFilterProjectTransposeRule.java
@@ -18,26 +18,43 @@
 package org.apache.hadoop.hive.ql.optimizer.calcite.rules;
 
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
+import java.util.Map;
 import java.util.Set;
 
+import org.apache.calcite.plan.RelOptCluster;
+import org.apache.calcite.plan.RelOptPredicateList;
 import org.apache.calcite.plan.RelOptRuleCall;
 import org.apache.calcite.plan.RelOptRuleOperand;
 import org.apache.calcite.plan.RelOptUtil;
+import org.apache.calcite.plan.hep.HepRelVertex;
 import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.rel.RelVisitor;
 import org.apache.calcite.rel.core.Filter;
 import org.apache.calcite.rel.core.Join;
 import org.apache.calcite.rel.core.Project;
 import org.apache.calcite.rel.rules.FilterProjectTransposeRule;
 import org.apache.calcite.rel.type.RelDataTypeFactory;
+import org.apache.calcite.rex.RexBuilder;
 import org.apache.calcite.rex.RexCall;
+import org.apache.calcite.rex.RexCorrelVariable;
+import org.apache.calcite.rex.RexDynamicParam;
+import org.apache.calcite.rex.RexFieldAccess;
 import org.apache.calcite.rex.RexInputRef;
+import org.apache.calcite.rex.RexLiteral;
 import org.apache.calcite.rex.RexNode;
 import org.apache.calcite.rex.RexOver;
+import org.apache.calcite.rex.RexRangeRef;
+import org.apache.calcite.rex.RexSimplify;
 import org.apache.calcite.rex.RexUtil;
+import org.apache.calcite.rex.RexVisitorImpl;
+import org.apache.calcite.sql.SqlKind;
+import org.apache.calcite.sql.fun.SqlStdOperatorTable;
 import org.apache.calcite.tools.RelBuilder;
 import org.apache.calcite.tools.RelBuilderFactory;
+import org.apache.calcite.util.Util;
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil;
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories;
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil;
@@ -132,8 +149,8 @@ public void onMatch(RelOptRuleCall call) {
       filterCondToPushBelowProj = null;
       if (pushThroughWindowing) {
         Set<Integer> commonPartitionKeys = getCommonPartitionCols(origproject.getProjects());
-        List<RexNode> newPartKeyFilConds = new ArrayList<RexNode>();
-        List<RexNode> unpushedFilConds = new ArrayList<RexNode>();
+        List<RexNode> newPartKeyFilConds = new ArrayList<>();
+        List<RexNode> unpushedFilConds = new ArrayList<>();
 
         // TODO:
         // 1) Handle compound partition keys (partition by k1+k2)
@@ -170,9 +187,12 @@ public void onMatch(RelOptRuleCall call) {
       }
     }
 
-    if (filterCondToPushBelowProj != null) {
+    if (filterCondToPushBelowProj != null
+        && !isRedundantIsNotNull(origproject, filterCondToPushBelowProj)) {
+
       RelNode newProjRel = getNewProject(filterCondToPushBelowProj, unPushedFilCondAboveProj, origproject, filter.getCluster()
           .getTypeFactory(), call.builder());
+
       call.transformTo(newProjRel);
     }
   }
@@ -211,7 +231,7 @@ private static RelNode getNewProject(RexNode filterCondToPushBelowProj, RexNode
   private static Set<Integer> getCommonPartitionCols(List<RexNode> projections) {
     RexOver overClause;
     boolean firstOverClause = true;
-    Set<Integer> commonPartitionKeys = new HashSet<Integer>();
+    Set<Integer> commonPartitionKeys = new HashSet<>();
 
     for (RexNode expr : projections) {
       if (expr instanceof RexOver) {
@@ -230,7 +250,7 @@ private static Set<Integer> getCommonPartitionCols(List<RexNode> projections) {
   }
 
   private static List<Integer> getPartitionCols(List<RexNode> partitionKeys) {
-    List<Integer> pCols = new ArrayList<Integer>();
+    List<Integer> pCols = new ArrayList<>();
     for (RexNode key : partitionKeys) {
       if (key instanceof RexInputRef) {
         pCols.add(((RexInputRef) key).getIndex());
@@ -238,4 +258,145 @@ private static List<Integer> getPartitionCols(List<RexNode> partitionKeys) {
     }
     return pCols;
   }
+
+  // in cases when a filter using IS NOT NULL is applied to an input $i down the subtree,
+  // creating another filter IS NOT NULL(FUNC_CALL+($i)) is of a doubtful usefulness and
+  // might lead to infinite loops in predicate pull-up and push-down, like in HIVE-25275
+  private static boolean isRedundantIsNotNull(Project project, RexNode newCondition) {
+    if (!newCondition.isA(SqlKind.IS_NOT_NULL)) {
+      return false;
+    }
+
+    // we handle expressions over a single input ref
+    if (HiveCalciteUtil.getInputRefs(newCondition).size() != 1) {
+      return false;
+    }
+
+    RedundancyChecker redundancyChecker = new RedundancyChecker(newCondition);
+    redundancyChecker.go(project);
+    return redundancyChecker.isRedundant;
+  }
+
+  private static class RedundancyChecker extends RelVisitor {
+    private boolean isRedundant;
+
+    final RexNode newCondition;
+    final Map<RelNode, RexNode> filter2newConditionMap = new HashMap<>();
+
+    protected RedundancyChecker(RexNode newCondition) {
+      this.newCondition = newCondition;
+    }
+
+    @Override
+    public void visit(RelNode node, int ordinal, RelNode parent) {
+      RexNode filterCondition =
+          filter2newConditionMap.isEmpty() ? newCondition : filter2newConditionMap.get(node);
+
+      if (isRedundant) {
+        return;
+      }
+
+      if (node instanceof HepRelVertex) {
+        // unwrap HepRelVertex and replace it with the associated RelNode
+        RelNode currNode = ((HepRelVertex) node).getCurrentRel();
+        filter2newConditionMap.put(currNode, filter2newConditionMap.remove(node));
+        visit(currNode, ordinal, parent);
+      } else {
+        if (node instanceof Filter) {
+          check((Filter) node);
+        } else if (node instanceof Project) {
+          filterCondition = RelOptUtil.pushPastProject(filterCondition, (Project) node);
+        } else {
+          // we do not support other operators for now
+          return;
+        }
+        RexNode finalFilterCondition = filterCondition;
+        node.getInputs().forEach(i -> filter2newConditionMap.put(i, finalFilterCondition));
+
+        super.visit(node, ordinal, parent);
+      }
+    }
+
+    private void check(Filter filter) {
+      final RelOptCluster cluster = filter.getCluster();
+      final RexBuilder rexBuilder = cluster.getRexBuilder();
+
+      final RexSimplify simplify = new RexSimplify(rexBuilder, RelOptPredicateList.EMPTY,
+          Util.first(cluster.getPlanner().getExecutor(), RexUtil.EXECUTOR));
+      final RexNode newCondition = simplify.simplify(filter2newConditionMap.get(filter));
+
+      // if the condition simplifies to a literal, bail out
+      if (RexUtil.isLiteral(newCondition, true)) {
+        return;
+      }
+
+      // IS NOT NULL($i) is always safe, it is either needed, or existing already
+      if (newCondition instanceof RexCall
+          && ((RexCall) newCondition).getOperands().get(0).isA(SqlKind.INPUT_REF)) {
+        return;
+      }
+
+      final RexNode filterCondition = simplify.simplify(filter.getCondition());
+
+      final Set<Integer> inputRefs = HiveCalciteUtil.getInputRefs(newCondition);
+      final RexInputRef rexInputRef =
+          rexBuilder.makeInputRef(filter.getInput(), inputRefs.iterator().next());
+      final RexNode baseCondition = rexBuilder.makeCall(SqlStdOperatorTable.IS_NOT_NULL, rexInputRef);
+
+      // if they are identical, pushing it down is harmless and can remove upper (redundant) filters
+      if (newCondition.toString().equals(filterCondition.toString())) {
+        return;
+      }
+
+      isRedundant = !isPredicateIncluded(newCondition, filterCondition)
+          && isPredicateIncluded(baseCondition, filterCondition);
+    }
+  }
+
+  private static boolean isPredicateIncluded(RexNode includedPred, RexNode containingPred) {
+    SubsumptionChecker inclusionChecker = new SubsumptionChecker(includedPred);
+    return containingPred.accept(inclusionChecker);
+  }
+
+  private static class SubsumptionChecker extends RexVisitorImpl<Boolean> {
+    private final String includedPredDigest;
+
+    protected SubsumptionChecker(RexNode includedPred) {
+      super(true);
+      this.includedPredDigest = includedPred.toString();
+    }
+
+    public Boolean visitInputRef(RexInputRef inputRef) {
+      return false;
+    }
+
+    public Boolean visitLiteral(RexLiteral literal) {
+      return false;
+    }
+
+    public Boolean visitCorrelVariable(RexCorrelVariable correlVariable) {
+      return false;
+    }
+
+    public Boolean visitCall(RexCall call) {
+      if (call.isA(SqlKind.AND)) {
+        return call.getOperands().stream().anyMatch(o -> o.accept(this));
+      } else if (call.isA(SqlKind.OR)) {
+        return call.getOperands().stream().allMatch(o -> o.accept(this));
+      }
+      return includedPredDigest.equals(call.toString());
+    }
+
+    public Boolean visitDynamicParam(RexDynamicParam dynamicParam) {
+      return false;
+    }
+
+    public Boolean visitRangeRef(RexRangeRef rangeRef) {
+      return false;
+    }
+
+    public Boolean visitFieldAccess(RexFieldAccess fieldAccess) {
+      return false;
+    }
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java
index 3090294b50..d1c9f7501e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java
@@ -20,6 +20,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Set;
+import java.util.stream.Collectors;
 
 import org.apache.calcite.plan.RelOptRule;
 import org.apache.calcite.plan.RelOptRuleCall;
@@ -29,6 +30,7 @@
 import org.apache.calcite.rel.core.RelFactories;
 import org.apache.calcite.rel.core.RelFactories.FilterFactory;
 import org.apache.calcite.rex.RexBuilder;
+import org.apache.calcite.rex.RexInputRef;
 import org.apache.calcite.rex.RexNode;
 import org.apache.calcite.rex.RexUtil;
 import org.apache.calcite.sql.SqlKind;
@@ -42,6 +44,7 @@
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSemiJoin;
 
+import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
 
@@ -81,7 +84,7 @@ public void onMatch(RelOptRuleCall call) {
 
     // For anti join case add the not null on right side even if the condition is always true.
     // This is done because during execution, anti join expect the right side to be empty and
-    // if we dont put null check on right, for null only right side table and condition always
+    // if we don't put null check on right, for null only right side table and condition always
     // true, execution will produce 0 records as the post processing to filter out null value
     // is not done for always true conditions during execution.
     // eg  select * from left_tbl where (select 1 from all_null_right limit 1) is null
@@ -113,8 +116,8 @@ public void onMatch(RelOptRuleCall call) {
       return;
     }
 
-    RelNode lChild = getNewChild(call, join, join.getLeft(), newLeftPredicate);
-    RelNode rChild = getNewChild(call, join, join.getRight(), newRightPredicate);
+    RelNode lChild = getNewChild(call, join.getLeft(), newLeftPredicate);
+    RelNode rChild = getNewChild(call, join.getRight(), newRightPredicate);
 
     Join newJoin = join.copy(join.getTraitSet(), join.getCondition(), lChild, rChild, join.getJoinType(),
         join.isSemiJoinDone());
@@ -153,7 +156,19 @@ private static List<RexNode> getNotNullConditions(RexBuilder rexBuilder, List<Re
       Set<String> pushedPredicates) {
     List<RexNode> newConditions = Lists.newArrayList();
 
+    Set<Integer> joinExprInputRefs = inputJoinExprs.stream()
+        .filter(n -> n.isA(SqlKind.INPUT_REF))
+        .map(RexInputRef.class::cast)
+        .map(RexInputRef::getIndex)
+        .collect(Collectors.toSet());
+
     for (RexNode rexNode : inputJoinExprs) {
+      Set<Integer> rexNodeInputRefs = HiveCalciteUtil.getInputRefs(rexNode);
+      // if we have both $0 and EXPR($0), then create only IS NOT NULL($0)
+      if (!rexNode.isA(SqlKind.INPUT_REF) && rexNodeInputRefs.size() == 1
+          && joinExprInputRefs.contains(rexNodeInputRefs.iterator().next())) {
+        continue;
+      }
       RexNode cond = rexBuilder.makeCall(SqlStdOperatorTable.IS_NOT_NULL, rexNode);
       String digest = cond.toString();
       if (pushedPredicates.add(digest)) {
@@ -163,9 +178,9 @@ private static List<RexNode> getNotNullConditions(RexBuilder rexBuilder, List<Re
     return newConditions;
   }
 
-  private RelNode getNewChild(RelOptRuleCall call, Join join, RelNode child, RexNode newPredicate) {
+  private RelNode getNewChild(RelOptRuleCall call, RelNode child, RexNode newPredicate) {
     if (!newPredicate.isAlwaysTrue()) {
-      RelNode newChild = filterFactory.createFilter(child, newPredicate);
+      RelNode newChild = filterFactory.createFilter(child, newPredicate, ImmutableSet.of());
       call.getPlanner().onCopy(child, newChild);
       return newChild;
     }
diff --git a/ql/src/test/queries/clientpositive/cbo_join_transitive_pred_loop.q b/ql/src/test/queries/clientpositive/cbo_join_transitive_pred_loop.q
new file mode 100644
index 0000000000..ce70a9d472
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/cbo_join_transitive_pred_loop.q
@@ -0,0 +1,7 @@
+CREATE TABLE A (`value_date` date) STORED AS ORC;
+CREATE TABLE B (`business_date` date) STORED AS ORC;
+
+EXPLAIN SELECT A.VALUE_DATE
+FROM A, B
+WHERE A.VALUE_DATE = BUSINESS_DATE
+  AND A.VALUE_DATE = TRUNC(BUSINESS_DATE, 'MONTH');
\ No newline at end of file
diff --git a/ql/src/test/results/clientpositive/llap/cbo_join_transitive_pred_loop.q.out b/ql/src/test/results/clientpositive/llap/cbo_join_transitive_pred_loop.q.out
new file mode 100644
index 0000000000..c2a4a6567f
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/cbo_join_transitive_pred_loop.q.out
@@ -0,0 +1,114 @@
+PREHOOK: query: CREATE TABLE A (`value_date` date) STORED AS ORC
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@A
+POSTHOOK: query: CREATE TABLE A (`value_date` date) STORED AS ORC
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@A
+PREHOOK: query: CREATE TABLE B (`business_date` date) STORED AS ORC
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@B
+POSTHOOK: query: CREATE TABLE B (`business_date` date) STORED AS ORC
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@B
+PREHOOK: query: EXPLAIN SELECT A.VALUE_DATE
+FROM A, B
+WHERE A.VALUE_DATE = BUSINESS_DATE
+  AND A.VALUE_DATE = TRUNC(BUSINESS_DATE, 'MONTH')
+PREHOOK: type: QUERY
+PREHOOK: Input: default@a
+PREHOOK: Input: default@b
+#### A masked pattern was here ####
+POSTHOOK: query: EXPLAIN SELECT A.VALUE_DATE
+FROM A, B
+WHERE A.VALUE_DATE = BUSINESS_DATE
+  AND A.VALUE_DATE = TRUNC(BUSINESS_DATE, 'MONTH')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@a
+POSTHOOK: Input: default@b
+#### A masked pattern was here ####
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 3 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  filterExpr: value_date is not null (type: boolean)
+                  Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: value_date is not null (type: boolean)
+                    Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: value_date (type: date)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: date), _col0 (type: date)
+                        null sort order: zz
+                        sort order: ++
+                        Map-reduce partition columns: _col0 (type: date), _col0 (type: date)
+                        Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+        Map 3 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  filterExpr: business_date is not null (type: boolean)
+                  Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: business_date is not null (type: boolean)
+                    Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: business_date (type: date), CAST( trunc(business_date, 'MONTH') AS DATE) (type: date)
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: NONE
+                      Filter Operator
+                        predicate: _col1 is not null (type: boolean)
+                        Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          key expressions: _col1 (type: date), _col0 (type: date)
+                          null sort order: zz
+                          sort order: ++
+                          Map-reduce partition columns: _col1 (type: date), _col0 (type: date)
+                          Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+        Reducer 2 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 _col0 (type: date), _col0 (type: date)
+                  1 _col1 (type: date), _col0 (type: date)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 61 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 61 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
