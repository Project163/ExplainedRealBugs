diff --git a/data/files/table1_delim.txt b/data/files/table1_delim.txt
new file mode 100644
index 0000000000..60a592dfd3
--- /dev/null
+++ b/data/files/table1_delim.txt
@@ -0,0 +1,5 @@
+1	Acura	4
+2	Toyota	3
+3	Tesla	5
+4	Honda	5
+11	Mazda	2
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 1d013aeb82..3c1741f525 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -7463,7 +7463,7 @@ protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
         acidOp = getAcidType(tableDescriptor.getOutputFileFormatClass(), dest, isMmTable);
         checkAcidConstraints();
       } else {
-        // Acid tables can't be list bucketed or have skewed cols
+        // Transactional tables can't be list bucketed or have skewed cols
         lbCtx = constructListBucketingCtx(destinationPartition.getSkewedColNames(),
             destinationPartition.getSkewedColValues(), destinationPartition.getSkewedColValueLocationMaps(),
             destinationPartition.isStoredAsSubDirectories());
diff --git a/ql/src/test/queries/clientpositive/load_micromanaged_delim.q b/ql/src/test/queries/clientpositive/load_micromanaged_delim.q
new file mode 100644
index 0000000000..00ba262085
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/load_micromanaged_delim.q
@@ -0,0 +1,29 @@
+set hive.support.concurrency=true;
+set hive.exec.dynamic.partition.mode=nonstrict;
+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
+
+
+dfs -mkdir ${system:test.tmp.dir}/delim_table;
+dfs -mkdir ${system:test.tmp.dir}/delim_table_ext;
+dfs -mkdir ${system:test.tmp.dir}/delim_table_trans;
+dfs -cp ${system:hive.root}/data/files/table1_delim.txt ${system:test.tmp.dir}/delim_table/;
+dfs -cp ${system:hive.root}/data/files/table1_delim.txt ${system:test.tmp.dir}/delim_table_ext/;
+dfs -cp ${system:hive.root}/data/files/table1_delim.txt ${system:test.tmp.dir}/delim_table_trans/;
+
+-- Checking that MicroManged and External tables have the same behaviour with delimited input files
+-- External table
+CREATE EXTERNAL TABLE delim_table_ext(id INT, name STRING, safety INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE LOCATION '${system:test.tmp.dir}/delim_table_ext/';
+describe formatted delim_table_ext;
+SELECT * FROM delim_table_ext;
+
+-- MicroManaged insert_only table
+CREATE TABLE delim_table_micro(id INT, name STRING, safety INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE TBLPROPERTIES('transactional'='true', "transactional_properties"="insert_only");
+LOAD DATA INPATH '${system:test.tmp.dir}/delim_table/table1_delim.txt' OVERWRITE INTO TABLE delim_table_micro;
+describe formatted delim_table_micro;
+SELECT * FROM delim_table_micro;
+
+-- Same as above with different syntax
+CREATE TRANSACTIONAL TABLE delim_table_trans(id INT, name STRING, safety INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;
+LOAD DATA INPATH '${system:test.tmp.dir}/delim_table_trans/table1_delim.txt' OVERWRITE INTO TABLE delim_table_trans;
+describe formatted delim_table_trans;
+SELECT * FROM delim_table_trans;
diff --git a/ql/src/test/results/clientpositive/llap/acid_stats5.q.out b/ql/src/test/results/clientpositive/llap/acid_stats5.q.out
index 3c6f3fb4eb..8a891eb027 100644
--- a/ql/src/test/results/clientpositive/llap/acid_stats5.q.out
+++ b/ql/src/test/results/clientpositive/llap/acid_stats5.q.out
@@ -106,6 +106,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted stats2 key
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@stats2
@@ -314,6 +316,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted stats2 key
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@stats2
@@ -460,6 +464,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted stats2 key
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@stats2
@@ -556,6 +562,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted stats2 key
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@stats2
@@ -721,6 +729,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted stats2 key
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@stats2
@@ -867,6 +877,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted stats2 key
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@stats2
diff --git a/ql/src/test/results/clientpositive/llap/check_constraint.q.out b/ql/src/test/results/clientpositive/llap/check_constraint.q.out
index 553937fc58..e2410f30ab 100644
--- a/ql/src/test/results/clientpositive/llap/check_constraint.q.out
+++ b/ql/src/test/results/clientpositive/llap/check_constraint.q.out
@@ -3710,6 +3710,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 	 	 
 # Constraints	 	 
 	 	 
@@ -3764,6 +3766,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 	 	 
 # Constraints	 	 
 	 	 
@@ -3892,6 +3896,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 	 	 
 # Constraints	 	 
 	 	 
diff --git a/ql/src/test/results/clientpositive/llap/create_transactional_insert_only.q.out b/ql/src/test/results/clientpositive/llap/create_transactional_insert_only.q.out
index c4daffb377..d005422159 100644
--- a/ql/src/test/results/clientpositive/llap/create_transactional_insert_only.q.out
+++ b/ql/src/test/results/clientpositive/llap/create_transactional_insert_only.q.out
@@ -50,6 +50,8 @@ Compressed:         	No
 Num Buckets:        	2                   	 
 Bucket Columns:     	[key]               	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: insert into table transactional_insert_only_table partition(ds)  select key,value,ds from srcpart
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart
diff --git a/ql/src/test/results/clientpositive/llap/load_micromanaged_delim.q.out b/ql/src/test/results/clientpositive/llap/load_micromanaged_delim.q.out
new file mode 100644
index 0000000000..85bb1d0022
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/load_micromanaged_delim.q.out
@@ -0,0 +1,192 @@
+#### A masked pattern was here ####
+PREHOOK: type: CREATETABLE
+#### A masked pattern was here ####
+PREHOOK: Output: database:default
+PREHOOK: Output: default@delim_table_ext
+#### A masked pattern was here ####
+POSTHOOK: type: CREATETABLE
+#### A masked pattern was here ####
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@delim_table_ext
+PREHOOK: query: describe formatted delim_table_ext
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@delim_table_ext
+POSTHOOK: query: describe formatted delim_table_ext
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@delim_table_ext
+# col_name            	data_type           	comment             
+id                  	int                 	                    
+name                	string              	                    
+safety              	int                 	                    
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	EXTERNAL_TABLE      	 
+Table Parameters:	 	 
+	EXTERNAL            	TRUE                
+	bucketing_version   	2                   
+	numFiles            	1                   
+	totalSize           	52                  
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	field.delim         	\t                  
+	serialization.format	\t                  
+PREHOOK: query: SELECT * FROM delim_table_ext
+PREHOOK: type: QUERY
+PREHOOK: Input: default@delim_table_ext
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM delim_table_ext
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@delim_table_ext
+#### A masked pattern was here ####
+1	Acura	4
+2	Toyota	3
+3	Tesla	5
+4	Honda	5
+11	Mazda	2
+PREHOOK: query: CREATE TABLE delim_table_micro(id INT, name STRING, safety INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE TBLPROPERTIES('transactional'='true', "transactional_properties"="insert_only")
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@delim_table_micro
+POSTHOOK: query: CREATE TABLE delim_table_micro(id INT, name STRING, safety INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE TBLPROPERTIES('transactional'='true', "transactional_properties"="insert_only")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@delim_table_micro
+#### A masked pattern was here ####
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@delim_table_micro
+#### A masked pattern was here ####
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@delim_table_micro
+PREHOOK: query: describe formatted delim_table_micro
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@delim_table_micro
+POSTHOOK: query: describe formatted delim_table_micro
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@delim_table_micro
+# col_name            	data_type           	comment             
+id                  	int                 	                    
+name                	string              	                    
+safety              	int                 	                    
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	bucketing_version   	2                   
+	numFiles            	1                   
+	numRows             	0                   
+	rawDataSize         	0                   
+	totalSize           	52                  
+	transactional       	true                
+	transactional_properties	insert_only         
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	field.delim         	\t                  
+	serialization.format	\t                  
+PREHOOK: query: SELECT * FROM delim_table_micro
+PREHOOK: type: QUERY
+PREHOOK: Input: default@delim_table_micro
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM delim_table_micro
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@delim_table_micro
+#### A masked pattern was here ####
+1	Acura	4
+2	Toyota	3
+3	Tesla	5
+4	Honda	5
+11	Mazda	2
+PREHOOK: query: CREATE TRANSACTIONAL TABLE delim_table_trans(id INT, name STRING, safety INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@delim_table_trans
+POSTHOOK: query: CREATE TRANSACTIONAL TABLE delim_table_trans(id INT, name STRING, safety INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@delim_table_trans
+#### A masked pattern was here ####
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@delim_table_trans
+#### A masked pattern was here ####
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@delim_table_trans
+PREHOOK: query: describe formatted delim_table_trans
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@delim_table_trans
+POSTHOOK: query: describe formatted delim_table_trans
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@delim_table_trans
+# col_name            	data_type           	comment             
+id                  	int                 	                    
+name                	string              	                    
+safety              	int                 	                    
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	bucketing_version   	2                   
+	numFiles            	1                   
+	numRows             	0                   
+	rawDataSize         	0                   
+	totalSize           	52                  
+	transactional       	true                
+	transactional_properties	insert_only         
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	field.delim         	\t                  
+	serialization.format	\t                  
+PREHOOK: query: SELECT * FROM delim_table_trans
+PREHOOK: type: QUERY
+PREHOOK: Input: default@delim_table_trans
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM delim_table_trans
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@delim_table_trans
+#### A masked pattern was here ####
+1	Acura	4
+2	Toyota	3
+3	Tesla	5
+4	Honda	5
+11	Mazda	2
diff --git a/ql/src/test/results/clientpositive/llap/mm_all.q.out b/ql/src/test/results/clientpositive/llap/mm_all.q.out
index 576f3ac902..7d69cc98d8 100644
--- a/ql/src/test/results/clientpositive/llap/mm_all.q.out
+++ b/ql/src/test/results/clientpositive/llap/mm_all.q.out
@@ -1867,6 +1867,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: insert into table stats_mm  select key from intermediate_n0
 PREHOOK: type: QUERY
 PREHOOK: Input: default@intermediate_n0
@@ -1916,6 +1918,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: drop table stats_mm
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@stats_mm
@@ -1973,6 +1977,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: drop table stats2_mm
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@stats2_mm
diff --git a/ql/src/test/results/clientpositive/llap/mm_bhif.q.out b/ql/src/test/results/clientpositive/llap/mm_bhif.q.out
index 225f08de9b..cd908657a5 100644
--- a/ql/src/test/results/clientpositive/llap/mm_bhif.q.out
+++ b/ql/src/test/results/clientpositive/llap/mm_bhif.q.out
@@ -66,8 +66,7 @@ STAGE PLANS:
     Tez
 #### A masked pattern was here ####
       Edges:
-        Reducer 2 <- Map 1 (SIMPLE_EDGE)
-        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
 #### A masked pattern was here ####
       Vertices:
         Map 1 
@@ -81,38 +80,23 @@ STAGE PLANS:
                     Statistics: Num rows: 6 Data size: 510 Basic stats: COMPLETE Column stats: COMPLETE
                     Group By Operator
                       keys: key (type: string)
-                      minReductionHashAggr: 0.5
-                      mode: hash
+                      mode: final
                       outputColumnNames: _col0
-                      Statistics: Num rows: 3 Data size: 255 Basic stats: COMPLETE Column stats: COMPLETE
-                      Reduce Output Operator
-                        key expressions: _col0 (type: string)
-                        null sort order: z
-                        sort order: +
-                        Map-reduce partition columns: _col0 (type: string)
-                        Statistics: Num rows: 3 Data size: 255 Basic stats: COMPLETE Column stats: COMPLETE
+                      Statistics: Num rows: 5 Data size: 425 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: count(_col0)
+                        minReductionHashAggr: 0.8
+                        mode: hash
+                        outputColumnNames: _col0
+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          null sort order: 
+                          sort order: 
+                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: bigint)
             Execution mode: vectorized, llap
             LLAP IO: no inputs
         Reducer 2 
-            Execution mode: vectorized, llap
-            Reduce Operator Tree:
-              Group By Operator
-                keys: KEY._col0 (type: string)
-                mode: mergepartial
-                outputColumnNames: _col0
-                Statistics: Num rows: 3 Data size: 255 Basic stats: COMPLETE Column stats: COMPLETE
-                Group By Operator
-                  aggregations: count(_col0)
-                  minReductionHashAggr: 0.6666666
-                  mode: hash
-                  outputColumnNames: _col0
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                  Reduce Output Operator
-                    null sort order: 
-                    sort order: 
-                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                    value expressions: _col0 (type: bigint)
-        Reducer 3 
             Execution mode: vectorized, llap
             Reduce Operator Tree:
               Group By Operator
diff --git a/ql/src/test/results/clientpositive/llap/mm_default.q.out b/ql/src/test/results/clientpositive/llap/mm_default.q.out
index 2bc6a01528..1758baaf65 100644
--- a/ql/src/test/results/clientpositive/llap/mm_default.q.out
+++ b/ql/src/test/results/clientpositive/llap/mm_default.q.out
@@ -128,6 +128,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted mm2
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@mm2
@@ -162,6 +164,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted mm3
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@mm3
@@ -196,6 +200,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted mm4
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@mm4
@@ -230,6 +236,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: desc formatted non_mm1
 PREHOOK: type: DESCTABLE
 PREHOOK: Input: default@non_mm1
diff --git a/ql/src/test/results/clientpositive/llap/mm_exim.q.out b/ql/src/test/results/clientpositive/llap/mm_exim.q.out
index 191c5bb809..c23d711534 100644
--- a/ql/src/test/results/clientpositive/llap/mm_exim.q.out
+++ b/ql/src/test/results/clientpositive/llap/mm_exim.q.out
@@ -327,6 +327,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: select * from import2_mm order by key, p
 PREHOOK: type: QUERY
 PREHOOK: Input: default@import2_mm
@@ -403,6 +405,8 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
 PREHOOK: query: select * from import3_mm order by key, p
 PREHOOK: type: QUERY
 PREHOOK: Input: default@import3_mm
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
index d573a668d8..f959ac75be 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
@@ -661,10 +661,11 @@ public List<Partition> run(List<String> input) throws MetaException {
    * Gets partitions by using direct SQL queries.
    * @param filter The filter.
    * @param max The maximum number of partitions to return.
+   * @param isAcidTable True if the table is ACID
    * @return List of partitions.
    */
   public List<Partition> getPartitionsViaSqlFilter(String catName, String dbName, String tableName,
-      SqlFilterForPushdown filter, Integer max, boolean isTxnTable) throws MetaException {
+      SqlFilterForPushdown filter, Integer max, boolean isAcidTable) throws MetaException {
     List<Long> partitionIds = getPartitionIdsViaSqlFilter(catName,
         dbName, tableName, filter.filter, filter.params,
         filter.joins, max);
@@ -675,7 +676,7 @@ public List<Partition> getPartitionsViaSqlFilter(String catName, String dbName,
       @Override
       public List<Partition> run(List<Long> input) throws MetaException {
         return getPartitionsFromPartitionIds(catName, dbName,
-            tableName, null, input, Collections.emptyList(), isTxnTable);
+            tableName, null, input, Collections.emptyList(), isAcidTable);
       }
     });
   }
@@ -926,7 +927,7 @@ private List<Partition> getPartitionsFromPartitionIds(String catName, String dbN
   /** Should be called with the list short enough to not trip up Oracle/etc. */
   private List<Partition> getPartitionsFromPartitionIds(String catName, String dbName, String tblName,
       Boolean isView, List<Long> partIdList, List<String> projectionFields,
-      boolean isTxnTable) throws MetaException {
+      boolean isAcidTable) throws MetaException {
 
     boolean doTrace = LOG.isDebugEnabled();
 
@@ -1062,19 +1063,19 @@ private List<Partition> getPartitionsFromPartitionIds(String catName, String dbN
     String serdeIds = trimCommaList(serdeSb);
     String colIds = trimCommaList(colsSb);
 
-    if (!isTxnTable) {
+    if (!isAcidTable) {
       // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.
       MetastoreDirectSqlUtils.setSDParameters(SD_PARAMS, convertMapNullsToEmptyStrings, pm, sds, sdIds);
     }
 
     boolean hasSkewedColumns = false;
-    if (!isTxnTable) {
+    if (!isAcidTable) {
       MetastoreDirectSqlUtils.setSDSortCols(SORT_COLS, pm, sds, sdIds);
     }
 
     MetastoreDirectSqlUtils.setSDBucketCols(BUCKETING_COLS, pm, sds, sdIds);
 
-    if (!isTxnTable) {
+    if (!isAcidTable) {
       // Skewed columns stuff.
       hasSkewedColumns = MetastoreDirectSqlUtils.setSkewedColNames(SKEWED_COL_NAMES, pm, sds, sdIds);
     }
@@ -1097,7 +1098,7 @@ private List<Partition> getPartitionsFromPartitionIds(String catName, String dbN
     }
 
     // Finally, get all the stuff for serdes - just the params.
-    if (!isTxnTable) {
+    if (!isAcidTable) {
       MetastoreDirectSqlUtils.setSerdeParams(SERDE_PARAMS, convertMapNullsToEmptyStrings, pm, serdes, serdeIds);
     }
 
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index 57a5f02c0a..673303e575 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -1925,10 +1925,10 @@ private Table convertToTable(MTable mtbl) throws MetaException {
       }
     }
     Map<String, String> parameters = convertMap(mtbl.getParameters());
-    boolean isTxnTable = TxnUtils.isTransactionalTable(parameters);
+    boolean isAcidTable = TxnUtils.isAcidTable(parameters);
     final Table t = new Table(mtbl.getTableName(), mtbl.getDatabase().getName(), mtbl
         .getOwner(), mtbl.getCreateTime(), mtbl.getLastAccessTime(), mtbl
-        .getRetention(), convertToStorageDescriptor(mtbl.getSd(), false, isTxnTable),
+        .getRetention(), convertToStorageDescriptor(mtbl.getSd(), false, isAcidTable),
         convertToFieldSchemas(mtbl.getPartitionKeys()), parameters,
         mtbl.getViewOriginalText(), mtbl.getViewExpandedText(), tableType);
 
@@ -2083,24 +2083,24 @@ private MColumnDescriptor createNewMColumnDescriptor(List<MFieldSchema> cols) {
   }
 
   private StorageDescriptor convertToStorageDescriptor(
-      MStorageDescriptor msd, boolean noFS, boolean isTxnTable) throws MetaException {
+      MStorageDescriptor msd, boolean noFS, boolean isAcidTable) throws MetaException {
     if (msd == null) {
       return null;
     }
     List<MFieldSchema> mFieldSchemas = msd.getCD() == null ? null : msd.getCD().getCols();
 
-    List<Order> orderList = (isTxnTable) ? Collections.emptyList() : convertToOrders(msd.getSortCols());
+    List<Order> orderList = (isAcidTable) ? Collections.emptyList() : convertToOrders(msd.getSortCols());
     List<String> bucList = convertList(msd.getBucketCols());
     SkewedInfo skewedInfo = null;
 
-    Map<String, String> sdParams = isTxnTable ? Collections.emptyMap() : convertMap(msd.getParameters());
+    Map<String, String> sdParams = isAcidTable ? Collections.emptyMap() : convertMap(msd.getParameters());
     StorageDescriptor sd = new StorageDescriptor(noFS ? null : convertToFieldSchemas(mFieldSchemas),
         msd.getLocation(), msd.getInputFormat(), msd.getOutputFormat(), msd
         .isCompressed(), msd.getNumBuckets(),
-        (!isTxnTable) ? convertToSerDeInfo(msd.getSerDeInfo(), true)
+        (!isAcidTable) ? convertToSerDeInfo(msd.getSerDeInfo(), true)
             : new SerDeInfo(msd.getSerDeInfo().getName(), msd.getSerDeInfo().getSerializationLib(), Collections.emptyMap()),
         bucList , orderList, sdParams);
-    if (!isTxnTable) {
+    if (!isAcidTable) {
       skewedInfo = new SkewedInfo(convertList(msd.getSkewedColNames()),
           convertToSkewedValues(msd.getSkewedColValues()),
           covertToSkewedMap(msd.getSkewedColValueLocationMaps()));
@@ -2606,7 +2606,7 @@ private MPartition convertToMPart(Partition part, MTable mt, boolean useTableCD)
         msd, part.getParameters());
   }
 
-  private Partition convertToPart(MPartition mpart, boolean isTxnTable) throws MetaException {
+  private Partition convertToPart(MPartition mpart, boolean isAcidTable) throws MetaException {
     if (mpart == null) {
       return null;
     }
@@ -2619,7 +2619,7 @@ private Partition convertToPart(MPartition mpart, boolean isTxnTable) throws Met
         table.getDatabase() == null ? null : table.getDatabase().getCatalogName();
     Map<String,String> params = convertMap(mpart.getParameters());
     Partition p = new Partition(convertList(mpart.getValues()), dbName, tableName, mpart.getCreateTime(),
-        mpart.getLastAccessTime(), convertToStorageDescriptor(mpart.getSd(), false, isTxnTable),
+        mpart.getLastAccessTime(), convertToStorageDescriptor(mpart.getSd(), false, isAcidTable),
         params);
     p.setCatName(catName);
     p.setWriteId(mpart.getWriteId());
@@ -2627,7 +2627,7 @@ private Partition convertToPart(MPartition mpart, boolean isTxnTable) throws Met
   }
 
   private Partition convertToPart(String catName, String dbName, String tblName,
-      MPartition mpart, boolean isTxnTable)
+      MPartition mpart, boolean isAcidTable)
       throws MetaException {
     if (mpart == null) {
       return null;
@@ -2635,7 +2635,7 @@ private Partition convertToPart(String catName, String dbName, String tblName,
     Map<String,String> params = convertMap(mpart.getParameters());
     Partition p = new Partition(convertList(mpart.getValues()), dbName, tblName,
         mpart.getCreateTime(), mpart.getLastAccessTime(),
-        convertToStorageDescriptor(mpart.getSd(), false, isTxnTable), params);
+        convertToStorageDescriptor(mpart.getSd(), false, isAcidTable), params);
     p.setCatName(catName);
     p.setWriteId(mpart.getWriteId());
     return p;
@@ -2953,11 +2953,11 @@ private List<Partition> convertToParts(List<MPartition> src, List<Partition> des
   }
 
   private List<Partition> convertToParts(String catName, String dbName, String tblName,
-      List<MPartition> mparts, boolean isTxnTable)
+      List<MPartition> mparts, boolean isAcidTable)
       throws MetaException {
     List<Partition> parts = new ArrayList<>(mparts.size());
     for (MPartition mp : mparts) {
-      parts.add(convertToPart(catName, dbName, tblName, mp, isTxnTable));
+      parts.add(convertToPart(catName, dbName, tblName, mp, isAcidTable));
       Deadline.checkTimeout();
     }
     return parts;
@@ -3600,7 +3600,7 @@ protected boolean getPartitionsByExprInternal(String catName, String dbName, Str
 
     MTable mTable = ensureGetMTable(catName, dbName, tblName);
     List<FieldSchema> partitionKeys = convertToFieldSchemas(mTable.getPartitionKeys());
-    boolean isTxnTbl = TxnUtils.isTransactionalTable(convertMap(mTable.getParameters()));
+    boolean isAcidTable = TxnUtils.isAcidTable(mTable.getParameters());
     result.addAll(new GetListHelper<Partition>(catName, dbName, tblName, allowSql, allowJdo) {
       @Override
       protected List<Partition> getSqlResult(GetHelper<List<Partition>> ctx) throws MetaException {
@@ -3610,7 +3610,7 @@ protected List<Partition> getSqlResult(GetHelper<List<Partition>> ctx) throws Me
           if (directSql.generateSqlFilterForPushdown(catName, dbName, tblName, partitionKeys,
               exprTree, defaultPartitionName, filter)) {
             String catalogName = (catName != null) ? catName : DEFAULT_CATALOG_NAME;
-            return directSql.getPartitionsViaSqlFilter(catalogName, dbName, tblName, filter, null, isTxnTbl);
+            return directSql.getPartitionsViaSqlFilter(catalogName, dbName, tblName, filter, null, isAcidTable);
           }
         }
         // We couldn't do SQL filter pushdown. Get names via normal means.
@@ -3633,7 +3633,7 @@ protected List<Partition> getJdoResult(
           List<String> partNames = new ArrayList<>();
           hasUnknownPartitions.set(getPartitionNamesPrunedByExprNoTxn(
                   catName, dbName, tblName, partitionKeys, expr, defaultPartitionName, maxParts, partNames));
-          result = getPartitionsViaOrmFilter(catName, dbName, tblName, partNames, isTxnTbl);
+          result = getPartitionsViaOrmFilter(catName, dbName, tblName, partNames, isAcidTable);
         }
         return result;
       }
@@ -3739,10 +3739,11 @@ private Integer getNumPartitionsViaOrmFilter(String catName, String dbName, Stri
    * @param dbName Database name.
    * @param tblName Table name.
    * @param partNames Partition names to get the objects for.
+   * @param isAcidTable True if the table is ACID
    * @return Resulting partitions.
    */
   private List<Partition> getPartitionsViaOrmFilter(String catName, String dbName, String tblName,
-      List<String> partNames, boolean isTxnTable) throws MetaException {
+      List<String> partNames, boolean isAcidTable) throws MetaException {
 
     if (partNames.isEmpty()) {
       return Collections.emptyList();
@@ -3759,7 +3760,7 @@ public List<Partition> run(List<String> input) throws MetaException {
         query.setOrdering("partitionName ascending");
 
         List<MPartition> mparts = (List<MPartition>) query.executeWithMap(queryWithParams.getRight());
-        List<Partition> partitions = convertToParts(catName, dbName, tblName, mparts, isTxnTable);
+        List<Partition> partitions = convertToParts(catName, dbName, tblName, mparts, isAcidTable);
         query.closeAll();
 
         return partitions;
@@ -4110,7 +4111,7 @@ public int getNumPartitionsByFilter(String catName, String dbName, String tblNam
     tblName = normalizeIdentifier(tblName);
     MTable mTable = ensureGetMTable(catName, dbName, tblName);
     List<FieldSchema> partitionKeys = convertToFieldSchemas(mTable.getPartitionKeys());
-    Map<String, String> parameters = mTable.getParameters();
+
     return new GetHelper<Integer>(catName, dbName, tblName, true, true) {
       private final SqlFilterForPushdown filter = new SqlFilterForPushdown();
 
@@ -4201,7 +4202,7 @@ protected List<Partition> getPartitionsByFilterInternal(
 
     MTable mTable = ensureGetMTable(catName, dbName, tblName);
     List<FieldSchema> partitionKeys = convertToFieldSchemas(mTable.getPartitionKeys());
-    boolean isTxnTable = TxnUtils.isTransactionalTable(convertMap(mTable.getParameters()));
+    boolean isAcidTable = TxnUtils.isAcidTable(mTable.getParameters());
     final ExpressionTree tree = (filter != null && !filter.isEmpty())
         ? PartFilterExprUtil.getFilterParser(filter).tree : ExpressionTree.EMPTY_TREE;
     return new GetListHelper<Partition>(catName, dbName, tblName, allowSql, allowJdo) {
@@ -4214,7 +4215,7 @@ protected boolean canUseDirectSql(GetHelper<List<Partition>> ctx) throws MetaExc
 
       @Override
       protected List<Partition> getSqlResult(GetHelper<List<Partition>> ctx) throws MetaException {
-        return directSql.getPartitionsViaSqlFilter(catName, dbName, tblName, filter, (maxParts < 0) ? null : (int)maxParts, isTxnTable);
+        return directSql.getPartitionsViaSqlFilter(catName, dbName, tblName, filter, (maxParts < 0) ? null : (int)maxParts, isAcidTable);
       }
 
       @Override
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java
index 4ee1a45aae..2a0a5c0c5e 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java
@@ -157,6 +157,12 @@ public static boolean isAcidTable(Table table) {
       .get(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES));
   }
 
+  public static boolean isAcidTable(Map<String, String> parameters) {
+    return isTransactionalTable(parameters) &&
+            TransactionalValidationListener.DEFAULT_TRANSACTIONAL_PROPERTY.
+                    equals(parameters.get(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES));
+  }
+
   /**
    * Should produce the result as &lt;dbName&gt;.&lt;tableName&gt;.
    */
