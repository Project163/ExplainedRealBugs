diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java
index b85970bea7..6b11ad7579 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java
@@ -22,9 +22,11 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.ValidCompactorWriteIdList;
 import org.apache.hadoop.hive.common.ValidTxnList;
+import org.apache.hadoop.hive.common.ValidWriteIdList;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreUtils;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.MetaStoreThread;
+import org.apache.hadoop.hive.metastore.api.CompactionType;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
@@ -128,7 +130,8 @@ public void run() {
     }
   }
 
-  private void verifyTableIdHasNotChanged(CompactionInfo ci, Table originalTable) throws HiveException, MetaException {
+  @VisibleForTesting
+  public void verifyTableIdHasNotChanged(CompactionInfo ci, Table originalTable) throws HiveException, MetaException {
     Table currentTable = resolveTable(ci);
     if (originalTable.getId() != currentTable.getId()) {
       throw new HiveException("Table " + originalTable.getDbName() + "." + originalTable.getTableName()
@@ -532,7 +535,6 @@ protected Boolean findNextCompactionAndExecute(boolean computeStats) {
           LOG.info("Will compact id: " + ci.id + " via MR job");
           runCompactionViaMrJob(ci, t, p, sd, tblValidWriteIds, jobName, dir, su);
         }
-
         heartbeater.cancel();
 
         verifyTableIdHasNotChanged(ci, t1);
@@ -543,8 +545,27 @@ protected Boolean findNextCompactionAndExecute(boolean computeStats) {
         compactionTxn.wasSuccessful();
       } catch (Throwable e) {
         LOG.error("Caught exception while trying to compact " + ci +
-            ".  Marking failed to avoid repeated failures", e);
+            ". Marking failed to avoid repeated failures", e);
+        final CompactionType ctype = ci.type;
         markFailed(ci, e);
+
+        if (runJobAsSelf(ci.runAs)) {
+          cleanupResultDirs(sd, tblValidWriteIds, ctype, dir);
+        } else {
+          LOG.info("Cleaning as user " + ci.runAs);
+          UserGroupInformation ugi = UserGroupInformation.createProxyUser(ci.runAs,
+              UserGroupInformation.getLoginUser());
+
+          ugi.doAs((PrivilegedExceptionAction<Void>) () -> {
+            cleanupResultDirs(sd, tblValidWriteIds, ctype, dir);
+            return null;
+          });
+          try {
+            FileSystem.closeAllForUGI(ugi);
+          } catch (IOException ex) {
+            LOG.error("Could not clean up file-system handles for UGI: " + ugi, e);
+          }
+        }
       }
     } catch (TException | IOException t) {
       LOG.error("Caught an exception in the main loop of compactor worker " + workerName, t);
@@ -591,6 +612,28 @@ private AcidDirectory getAcidStateForWorker(CompactionInfo ci, StorageDescriptor
     }
   }
 
+  private void cleanupResultDirs(StorageDescriptor sd, ValidWriteIdList writeIds, CompactionType ctype, AcidDirectory dir) {
+    // result directory for compactor to write new files
+    Path resultDir = QueryCompactor.Util.getCompactionResultDir(sd, writeIds, conf,
+        ctype == CompactionType.MAJOR, false, false, dir);
+    LOG.info("Deleting result directories created by the compactor:\n");
+    try {
+      FileSystem fs = resultDir.getFileSystem(conf);
+      LOG.info(resultDir.toString());
+      fs.delete(resultDir, true);
+
+      if (ctype == CompactionType.MINOR) {
+        Path deleteDeltaDir = QueryCompactor.Util.getCompactionResultDir(sd, writeIds, conf,
+            false, true, false, dir);
+
+        LOG.info(deleteDeltaDir.toString());
+        fs.delete(deleteDeltaDir, true);
+      }
+    } catch (IOException ex) {
+      LOG.error("Caught exception while cleaning result directories:", ex);
+    }
+  }
+
   private void failCompactionIfSetForTest() {
     if(conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST) && conf.getBoolVar(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION)) {
       throw new RuntimeException(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION.name() + "=true");
@@ -599,7 +642,7 @@ private void failCompactionIfSetForTest() {
 
   private void runCompactionViaMrJob(CompactionInfo ci, Table t, Partition p, StorageDescriptor sd,
       ValidCompactorWriteIdList tblValidWriteIds, StringBuilder jobName, AcidDirectory dir, StatsUpdater su)
-      throws IOException, HiveException, InterruptedException {
+      throws IOException, InterruptedException {
     final CompactorMR mr = new CompactorMR();
     if (runJobAsSelf(ci.runAs)) {
       mr.run(conf, jobName.toString(), t, p, sd, tblValidWriteIds, ci, su, msc, dir);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands3.java b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands3.java
index 5a0fe68d19..489123a711 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands3.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands3.java
@@ -33,8 +33,10 @@
 import org.apache.hadoop.hive.ql.io.orc.TestVectorizedOrcAcidRowBatchReader;
 import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;
 import org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory;
+import org.apache.hadoop.hive.ql.txn.compactor.Worker;
 import org.junit.Assert;
 import org.junit.Test;
+import org.mockito.Mockito;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -43,8 +45,10 @@
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
+import java.util.concurrent.atomic.AtomicBoolean;
 
 import static org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.swapTxnManager;
+import static org.mockito.Matchers.any;
 
 public class TestTxnCommands3 extends TxnCommandsBaseForTests {
   static final private Logger LOG = LoggerFactory.getLogger(TestTxnCommands3.class);
@@ -463,6 +467,101 @@ public void testCompactionAbort() throws Exception {
     runCleaner(hiveConf);
   }
 
+  @Test
+  public void testMajorCompactionAbortLeftoverFiles() throws Exception {
+    MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID, true);
+
+    dropTable(new String[] {"T"});
+    //note: transaction names T1, T2, etc below, are logical, the actual txnid will be different
+    runStatementOnDriver("create table T (a int, b int) stored as orc");
+    runStatementOnDriver("insert into T values(0,2)"); //makes delta_1_1 in T1
+    runStatementOnDriver("insert into T values(1,4)"); //makes delta_2_2 in T2
+
+    runStatementOnDriver("alter table T compact 'minor'");
+    //create failed compaction attempt so that compactor txn is aborted
+    Worker worker = Mockito.spy(new Worker());
+    worker.setConf(hiveConf);
+    worker.init(new AtomicBoolean(true));
+    Mockito.doThrow(new RuntimeException(
+      "Will cause CompactorMR to fail all opening txn and creating directories for compaction."))
+      .when(worker).verifyTableIdHasNotChanged(any(), any());
+    worker.run();
+
+    TxnStore txnHandler = TxnUtils.getTxnStore(hiveConf);
+    ShowCompactResponse resp = txnHandler.showCompact(new ShowCompactRequest());
+    Assert.assertEquals("Unexpected number of compactions in history",
+        1, resp.getCompactsSize());
+    Assert.assertEquals("Unexpected 0th compaction state",
+        TxnStore.FAILED_RESPONSE, resp.getCompacts().get(0).getState());
+    GetOpenTxnsResponse openResp =  txnHandler.getOpenTxns();
+    Assert.assertEquals(openResp.toString(), 1, openResp.getOpen_txnsSize());
+    //check that the compactor txn is aborted
+    Assert.assertTrue(openResp.toString(), BitSet.valueOf(openResp.getAbortedBits()).get(0));
+
+    FileSystem fs = FileSystem.get(hiveConf);
+    Path warehousePath = new Path(getWarehouseDir());
+    FileStatus[] actualList = fs.listStatus(new Path(warehousePath + "/t"),
+        FileUtils.HIDDEN_FILES_PATH_FILTER);
+
+    // we expect all the t/base_* files to be removed by the compactor failure
+    String[] expectedList = new String[] {
+        "/t/delta_0000001_0000001_0000",
+        "/t/delta_0000002_0000002_0000",
+    };
+    checkExpectedFiles(actualList, expectedList, warehousePath.toString());
+    //delete metadata about aborted txn from txn_components and files (if any)
+    runCleaner(hiveConf);
+  }
+
+  @Test
+  public void testMinorCompactionAbortLeftoverFiles() throws Exception {
+    MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID, true);
+
+    dropTable(new String[] {"T"});
+    //note: transaction names T1, T2, etc below, are logical, the actual txnid will be different
+    runStatementOnDriver("create table T (a int, b int) stored as orc");
+    runStatementOnDriver("insert into T values(0,2)"); //makes delta_1_1 in T1
+    runStatementOnDriver("insert into T values(1,4)"); //makes delta_2_2 in T2
+    runStatementOnDriver("update T set a=3 where b=2"); //makes delta/(delete_delta)_3_3 in T3
+
+    runStatementOnDriver("alter table T compact 'minor'");
+    //create failed compaction attempt so that compactor txn is aborted
+    Worker worker = Mockito.spy(new Worker());
+    worker.setConf(hiveConf);
+    worker.init(new AtomicBoolean(true));
+    Mockito.doThrow(new RuntimeException(
+      "Will cause CompactorMR to fail all opening txn and creating directories for compaction."))
+      .when(worker).verifyTableIdHasNotChanged(any(), any());
+    worker.run();
+
+    TxnStore txnHandler = TxnUtils.getTxnStore(hiveConf);
+    ShowCompactResponse resp = txnHandler.showCompact(new ShowCompactRequest());
+    Assert.assertEquals("Unexpected number of compactions in history",
+        1, resp.getCompactsSize());
+    Assert.assertEquals("Unexpected 0th compaction state",
+        TxnStore.FAILED_RESPONSE, resp.getCompacts().get(0).getState());
+    GetOpenTxnsResponse openResp =  txnHandler.getOpenTxns();
+    Assert.assertEquals(openResp.toString(), 1, openResp.getOpen_txnsSize());
+    //check that the compactor txn is aborted
+    Assert.assertTrue(openResp.toString(), BitSet.valueOf(openResp.getAbortedBits()).get(0));
+
+    FileSystem fs = FileSystem.get(hiveConf);
+    Path warehousePath = new Path(getWarehouseDir());
+    FileStatus[] actualList = fs.listStatus(new Path(warehousePath + "/t"),
+        FileUtils.HIDDEN_FILES_PATH_FILTER);
+
+    // we expect all the t/base_* files to be removed by the compactor failure
+    String[] expectedList = new String[] {
+        "/t/delta_0000001_0000001_0000",
+        "/t/delta_0000002_0000002_0000",
+        "/t/delete_delta_0000003_0000003_0000",
+        "/t/delta_0000003_0000003_0000",
+    };
+    checkExpectedFiles(actualList, expectedList, warehousePath.toString());
+    //delete metadata about aborted txn from txn_components and files (if any)
+    runCleaner(hiveConf);
+  }
+
   /**
    * Not enough deltas to compact, no need to clean: there is absolutely nothing to do.
    */
