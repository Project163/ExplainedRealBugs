diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index ab7e3c6950..f670973c5d 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -967,7 +967,7 @@ public static enum ConfVars {
         "than this threshold, it will try to convert the common join into map join"),
 
 
-    HIVE_SCHEMA_EVOLUTION("hive.exec.schema.evolution", true,
+    HIVE_SCHEMA_EVOLUTION("hive.exec.schema.evolution", false,
         "Use schema evolution to convert self-describing file format's data to the schema desired by the reader."),
 
     HIVESAMPLERANDOMNUM("hive.sample.seednumber", 0,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
index ac0ecd9c31..deec8bba45 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
@@ -72,6 +72,7 @@
 import org.apache.hadoop.hive.ql.exec.tez.TezTask;
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
+import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.io.RCFileInputFormat;
 import org.apache.hadoop.hive.ql.io.merge.MergeFileTask;
 import org.apache.hadoop.hive.ql.io.merge.MergeFileWork;
@@ -3253,6 +3254,14 @@ private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {
     return 0;
   }
 
+  private boolean isSchemaEvolutionEnabled(Table tbl) {
+    boolean isAcid = AcidUtils.isTablePropertyTransactional(tbl.getMetadata());
+    if (isAcid || HiveConf.getBoolVar(conf, ConfVars.HIVE_SCHEMA_EVOLUTION)) {
+      return true;
+    }
+    return false;
+  }
+
   private int alterTableOrSinglePartition(AlterTableDesc alterTbl, Table tbl, Partition part)
       throws HiveException {
 
@@ -3301,8 +3310,10 @@ private int alterTableOrSinglePartition(AlterTableDesc alterTbl, Table tbl, Part
       boolean first = alterTbl.getFirst();
       String afterCol = alterTbl.getAfterCol();
       // if orc table, restrict reordering columns as it will break schema evolution
-      boolean isOrc = sd.getInputFormat().equals(OrcInputFormat.class.getName());
-      if (isOrc && (first || (afterCol != null && !afterCol.trim().isEmpty()))) {
+      boolean isOrcSchemaEvolution =
+          sd.getInputFormat().equals(OrcInputFormat.class.getName()) &&
+          isSchemaEvolutionEnabled(tbl);
+      if (isOrcSchemaEvolution && (first || (afterCol != null && !afterCol.trim().isEmpty()))) {
         throw new HiveException(ErrorMsg.CANNOT_REORDER_COLUMNS, alterTbl.getOldName());
       }
       FieldSchema column = null;
@@ -3323,7 +3334,7 @@ private int alterTableOrSinglePartition(AlterTableDesc alterTbl, Table tbl, Part
         } else if (oldColName.equalsIgnoreCase(oldName)) {
           // if orc table, restrict changing column types. Only integer type promotion is supported.
           // smallint -> int -> bigint
-          if (isOrc && !isSupportedTypeChange(col.getType(), type)) {
+          if (isOrcSchemaEvolution && !isSupportedTypeChange(col.getType(), type)) {
             throw new HiveException(ErrorMsg.CANNOT_CHANGE_COLUMN_TYPE, col.getType(), type,
                 newName);
           }
@@ -3382,9 +3393,11 @@ private int alterTableOrSinglePartition(AlterTableDesc alterTbl, Table tbl, Part
           && !serializationLib.equals(OrcSerde.class.getName())) {
         throw new HiveException(ErrorMsg.CANNOT_REPLACE_COLUMNS, alterTbl.getOldName());
       }
-      final boolean isOrc = serializationLib.equals(OrcSerde.class.getName());
-      // adding columns and limited integer type promotion is supported for ORC
-      if (isOrc) {
+      final boolean isOrcSchemaEvolution =
+          serializationLib.equals(OrcSerde.class.getName()) &&
+          isSchemaEvolutionEnabled(tbl);
+      // adding columns and limited integer type promotion is supported for ORC schema evolution
+      if (isOrcSchemaEvolution) {
         final List<FieldSchema> existingCols = sd.getCols();
         final List<FieldSchema> replaceCols = alterTbl.getNewCols();
 
@@ -3417,7 +3430,8 @@ private int alterTableOrSinglePartition(AlterTableDesc alterTbl, Table tbl, Part
       String serdeName = alterTbl.getSerdeName();
       String oldSerdeName = sd.getSerdeInfo().getSerializationLib();
       // if orc table, restrict changing the serde as it can break schema evolution
-      if (oldSerdeName.equalsIgnoreCase(OrcSerde.class.getName()) &&
+      if (isSchemaEvolutionEnabled(tbl) &&
+          oldSerdeName.equalsIgnoreCase(OrcSerde.class.getName()) &&
           !serdeName.equalsIgnoreCase(OrcSerde.class.getName())) {
         throw new HiveException(ErrorMsg.CANNOT_CHANGE_SERDE, OrcSerde.class.getSimpleName(),
             alterTbl.getOldName());
@@ -3447,7 +3461,8 @@ private int alterTableOrSinglePartition(AlterTableDesc alterTbl, Table tbl, Part
     } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDFILEFORMAT) {
       StorageDescriptor sd = (part == null ? tbl.getTTable().getSd() : part.getTPartition().getSd());
       // if orc table, restrict changing the file format as it can break schema evolution
-      if (sd.getInputFormat().equals(OrcInputFormat.class.getName())
+      if (isSchemaEvolutionEnabled(tbl) &&
+          sd.getInputFormat().equals(OrcInputFormat.class.getName())
           && !alterTbl.getInputFormat().equals(OrcInputFormat.class.getName())) {
         throw new HiveException(ErrorMsg.CANNOT_CHANGE_FILEFORMAT, "ORC", alterTbl.getOldName());
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
index 3aafe8921b..a5489a9b13 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
@@ -604,7 +604,8 @@ private StructObjectInspector getPartitionedRowOI(StructObjectInspector valueOI)
   }
 
   private boolean needConversion(PartitionDesc partitionDesc) {
-    if (Utilities.isInputFileFormatSelfDescribing(partitionDesc)) {
+    boolean isAcid = AcidUtils.isTablePropertyTransactional(partitionDesc.getTableDesc().getProperties());
+    if (Utilities.isSchemaEvolutionEnabled(job, isAcid) && Utilities.isInputFileFormatSelfDescribing(partitionDesc)) {
       return false;
     }
     return needConversion(partitionDesc.getTableDesc(), Arrays.asList(partitionDesc));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
index 99724c1dfb..ec0d95ce7e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
@@ -39,6 +39,7 @@
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.exec.MapOperator.MapOpCtx;
 import org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext;
+import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.io.RecordIdentifier;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -205,7 +206,8 @@ private MapOpCtx initObjectInspector(Configuration hconf, MapOpCtx opCtx,
     opCtx.deserializer = pd.getDeserializer(hconf);
 
     StructObjectInspector partRawRowObjectInspector;
-    if (Utilities.isInputFileFormatSelfDescribing(pd)) {
+    boolean isAcid = AcidUtils.isTablePropertyTransactional(td.getProperties());
+    if (Utilities.isSchemaEvolutionEnabled(hconf, isAcid) && Utilities.isInputFileFormatSelfDescribing(pd)) {
       partRawRowObjectInspector = tableRowOI;
     } else {
       partRawRowObjectInspector =
@@ -313,7 +315,8 @@ private Map<TableDesc, StructObjectInspector> getConvertedOI(Configuration hconf
         Deserializer partDeserializer = pd.getDeserializer(hconf);
 
         StructObjectInspector partRawRowObjectInspector;
-        if (Utilities.isInputFileFormatSelfDescribing(pd)) {
+        boolean isAcid = AcidUtils.isTablePropertyTransactional(tableDesc.getProperties());
+        if (Utilities.isSchemaEvolutionEnabled(hconf, isAcid) && Utilities.isInputFileFormatSelfDescribing(pd)) {
           Deserializer tblDeserializer = tableDesc.getDeserializer(hconf);
           partRawRowObjectInspector = (StructObjectInspector) tblDeserializer.getObjectInspector();
         } else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 2d317a04f4..5e0553dc65 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -3526,6 +3526,10 @@ public static List<String> getStatsTmpDirs(BaseWork work, Configuration conf) {
     return statsTmpDirs;
   }
 
+  public static boolean isSchemaEvolutionEnabled(Configuration conf, boolean isAcid) {
+    return isAcid || HiveConf.getBoolVar(conf, ConfVars.HIVE_SCHEMA_EVOLUTION);
+  }
+
   public static boolean isInputFileFormatSelfDescribing(PartitionDesc pd) {
     Class<?> inputFormatClass = pd.getInputFileFormatClass();
     return SelfDescribingInputFormatInterface.class.isAssignableFrom(inputFormatClass);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
index 359cbf72e3..eae281c4b1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
@@ -39,6 +39,7 @@
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.commons.codec.binary.Hex;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.io.IOConstants;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo;
@@ -249,8 +250,11 @@ public static RecordReader createReaderFromFile(Reader file,
 
     /**
      * Do we have schema on read in the configuration variables?
+     *
+     * NOTE: This code path is NOT used by ACID.  OrcInputFormat.getRecordReader intercepts for
+     * ACID tables creates raw record merger, etc.
      */
-    TypeDescription schema = getDesiredRowTypeDescr(conf);
+    TypeDescription schema = getDesiredRowTypeDescr(conf, /* isAcid */ false);
 
     Reader.Options options = new Reader.Options().range(offset, length);
     options.schema(schema);
@@ -1523,7 +1527,10 @@ public RowReader<OrcStruct> getReader(InputSplit inputSplit,
     /**
      * Do we have schema on read in the configuration variables?
      */
-    TypeDescription schema = getDesiredRowTypeDescr(conf);
+    TypeDescription schema = getDesiredRowTypeDescr(conf, /* isAcid */ true);
+    if (schema == null) {
+      throw new IOException(ErrorMsg.SCHEMA_REQUIRED_TO_READ_ACID_TABLES.getErrorCodedMsg());
+    }
 
     final Reader reader;
     final int bucket;
@@ -2051,7 +2058,7 @@ public static TypeDescription convertTypeInfo(TypeInfo info) {
   }
 
 
-  public static TypeDescription getDesiredRowTypeDescr(Configuration conf) {
+  public static TypeDescription getDesiredRowTypeDescr(Configuration conf, boolean isAcid) {
 
     String columnNameProperty = null;
     String columnTypeProperty = null;
@@ -2060,7 +2067,7 @@ public static TypeDescription getDesiredRowTypeDescr(Configuration conf) {
     ArrayList<TypeDescription> schemaEvolutionTypeDescrs = null;
 
     boolean haveSchemaEvolutionProperties = false;
-    if (HiveConf.getBoolVar(conf, ConfVars.HIVE_SCHEMA_EVOLUTION)) {
+    if (isAcid || HiveConf.getBoolVar(conf, ConfVars.HIVE_SCHEMA_EVOLUTION) ) {
 
       columnNameProperty = conf.get(IOConstants.SCHEMA_EVOLUTION_COLUMNS);
       columnTypeProperty = conf.get(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES);
@@ -2082,7 +2089,14 @@ public static TypeDescription getDesiredRowTypeDescr(Configuration conf) {
       }
     }
 
-    if (!haveSchemaEvolutionProperties) {
+    if (haveSchemaEvolutionProperties) {
+      LOG.info("Using schema evolution configuration variables schema.evolution.columns " +
+          schemaEvolutionColumnNames.toString() +
+          " / schema.evolution.columns.types " +
+          schemaEvolutionTypeDescrs.toString() +
+          " (isAcid " + isAcid + ")");
+
+    } else {
 
       // Try regular properties;
       columnNameProperty = conf.get(serdeConstants.LIST_COLUMNS);
@@ -2100,6 +2114,11 @@ public static TypeDescription getDesiredRowTypeDescr(Configuration conf) {
       if (schemaEvolutionTypeDescrs.size() != schemaEvolutionColumnNames.size()) {
         return null;
       }
+      LOG.info("Using column configuration variables columns " +
+              schemaEvolutionColumnNames.toString() +
+              " / columns.types " +
+              schemaEvolutionTypeDescrs.toString() +
+              " (isAcid " + isAcid + ")");
     }
 
     // Desired schema does not include virtual columns or partition columns.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java
index e5f9786cb0..2c8dae286c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java
@@ -447,7 +447,7 @@ static Reader.Options createEventOptions(Reader.Options options) {
     this.length = options.getLength();
     this.validTxnList = validTxnList;
 
-    TypeDescription typeDescr = OrcInputFormat.getDesiredRowTypeDescr(conf);
+    TypeDescription typeDescr = OrcInputFormat.getDesiredRowTypeDescr(conf, /* isAcid */ true);
     if (typeDescr == null) {
       throw new IOException(ErrorMsg.SCHEMA_REQUIRED_TO_READ_ACID_TABLES.getErrorCodedMsg());
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
index e08aaf358b..14f275f45b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
@@ -63,10 +63,18 @@ static class VectorizedOrcRecordReader
     VectorizedOrcRecordReader(Reader file, Configuration conf,
         FileSplit fileSplit) throws IOException {
 
+      // if HiveCombineInputFormat gives us FileSplits instead of OrcSplits,
+      // we know it is not ACID. (see a check in CombineHiveInputFormat.getSplits() that assures this).
+      //
+      // Why would an ACID table reach here instead of VectorizedOrcAcidRowReader?
+      // OrcInputFormat.getRecordReader will use this reader for original files that have no deltas.
+      //
+      boolean isAcid = (fileSplit instanceof OrcSplit);
+
       /**
        * Do we have schema on read in the configuration variables?
        */
-      TypeDescription schema = OrcInputFormat.getDesiredRowTypeDescr(conf);
+      TypeDescription schema = OrcInputFormat.getDesiredRowTypeDescr(conf, isAcid);
 
       List<OrcProto.Type> types = file.getTypes();
       Reader.Options options = new Reader.Options();
diff --git a/ql/src/test/queries/clientnegative/orc_change_fileformat.q b/ql/src/test/queries/clientnegative/orc_change_fileformat.q
index 5b2a7e6b92..a0f89d9c17 100644
--- a/ql/src/test/queries/clientnegative/orc_change_fileformat.q
+++ b/ql/src/test/queries/clientnegative/orc_change_fileformat.q
@@ -1,2 +1,3 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key tinyint, val string) stored as orc;
 alter table src_orc set fileformat textfile;
diff --git a/ql/src/test/queries/clientnegative/orc_change_fileformat_acid.q b/ql/src/test/queries/clientnegative/orc_change_fileformat_acid.q
new file mode 100644
index 0000000000..0fd287b9b9
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_change_fileformat_acid.q
@@ -0,0 +1,3 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+alter table src_orc set fileformat textfile;
diff --git a/ql/src/test/queries/clientnegative/orc_change_serde.q b/ql/src/test/queries/clientnegative/orc_change_serde.q
index e7b70fd193..49d56bdffc 100644
--- a/ql/src/test/queries/clientnegative/orc_change_serde.q
+++ b/ql/src/test/queries/clientnegative/orc_change_serde.q
@@ -1,2 +1,3 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key tinyint, val string) stored as orc;
 alter table src_orc set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';
diff --git a/ql/src/test/queries/clientnegative/orc_change_serde_acid.q b/ql/src/test/queries/clientnegative/orc_change_serde_acid.q
new file mode 100644
index 0000000000..d317a2845c
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_change_serde_acid.q
@@ -0,0 +1,3 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+alter table src_orc set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';
diff --git a/ql/src/test/queries/clientnegative/orc_reorder_columns1.q b/ql/src/test/queries/clientnegative/orc_reorder_columns1.q
index 2f43ddb2cb..516c170430 100644
--- a/ql/src/test/queries/clientnegative/orc_reorder_columns1.q
+++ b/ql/src/test/queries/clientnegative/orc_reorder_columns1.q
@@ -1,2 +1,3 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key tinyint, val string) stored as orc;
 alter table src_orc change key k tinyint first;
diff --git a/ql/src/test/queries/clientnegative/orc_reorder_columns1_acid.q b/ql/src/test/queries/clientnegative/orc_reorder_columns1_acid.q
new file mode 100644
index 0000000000..2c6cc9fcf3
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_reorder_columns1_acid.q
@@ -0,0 +1,3 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+alter table src_orc change key k tinyint first;
diff --git a/ql/src/test/queries/clientnegative/orc_reorder_columns2.q b/ql/src/test/queries/clientnegative/orc_reorder_columns2.q
index 3634d2d3f1..2acabdf5a7 100644
--- a/ql/src/test/queries/clientnegative/orc_reorder_columns2.q
+++ b/ql/src/test/queries/clientnegative/orc_reorder_columns2.q
@@ -1,2 +1,3 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key tinyint, val string) stored as orc;
 alter table src_orc change key k tinyint after val;
diff --git a/ql/src/test/queries/clientnegative/orc_reorder_columns2_acid.q b/ql/src/test/queries/clientnegative/orc_reorder_columns2_acid.q
new file mode 100644
index 0000000000..938a0bca50
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_reorder_columns2_acid.q
@@ -0,0 +1,3 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+alter table src_orc change key k tinyint after val;
diff --git a/ql/src/test/queries/clientnegative/orc_replace_columns1.q b/ql/src/test/queries/clientnegative/orc_replace_columns1.q
index e5f944961e..f6b1c0657b 100644
--- a/ql/src/test/queries/clientnegative/orc_replace_columns1.q
+++ b/ql/src/test/queries/clientnegative/orc_replace_columns1.q
@@ -1,2 +1,3 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key tinyint, val string) stored as orc;
 alter table src_orc replace columns (k int);
diff --git a/ql/src/test/queries/clientnegative/orc_replace_columns1_acid.q b/ql/src/test/queries/clientnegative/orc_replace_columns1_acid.q
new file mode 100644
index 0000000000..68a8127d6d
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_replace_columns1_acid.q
@@ -0,0 +1,3 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+alter table src_orc replace columns (k int);
diff --git a/ql/src/test/queries/clientnegative/orc_replace_columns2.q b/ql/src/test/queries/clientnegative/orc_replace_columns2.q
index cc6076ddf3..2a50b94ad7 100644
--- a/ql/src/test/queries/clientnegative/orc_replace_columns2.q
+++ b/ql/src/test/queries/clientnegative/orc_replace_columns2.q
@@ -1,2 +1,3 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key tinyint, val string) stored as orc;
 alter table src_orc replace columns (k smallint, val string);
diff --git a/ql/src/test/queries/clientnegative/orc_replace_columns2_acid.q b/ql/src/test/queries/clientnegative/orc_replace_columns2_acid.q
new file mode 100644
index 0000000000..417a5de57c
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_replace_columns2_acid.q
@@ -0,0 +1,3 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+alter table src_orc replace columns (k smallint, val string);
diff --git a/ql/src/test/queries/clientnegative/orc_replace_columns3.q b/ql/src/test/queries/clientnegative/orc_replace_columns3.q
index 57d3c9bb01..b7b527f742 100644
--- a/ql/src/test/queries/clientnegative/orc_replace_columns3.q
+++ b/ql/src/test/queries/clientnegative/orc_replace_columns3.q
@@ -1,3 +1,4 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key smallint, val string) stored as orc;
 alter table src_orc replace columns (k int, val string, z smallint);
 alter table src_orc replace columns (k int, val string, z tinyint);
diff --git a/ql/src/test/queries/clientnegative/orc_replace_columns3_acid.q b/ql/src/test/queries/clientnegative/orc_replace_columns3_acid.q
new file mode 100644
index 0000000000..b09eb37970
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_replace_columns3_acid.q
@@ -0,0 +1,4 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key smallint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+alter table src_orc replace columns (k int, val string, z smallint);
+alter table src_orc replace columns (k int, val string, z tinyint);
diff --git a/ql/src/test/queries/clientnegative/orc_type_promotion1.q b/ql/src/test/queries/clientnegative/orc_type_promotion1.q
index e465b2a241..d7facc389a 100644
--- a/ql/src/test/queries/clientnegative/orc_type_promotion1.q
+++ b/ql/src/test/queries/clientnegative/orc_type_promotion1.q
@@ -1,2 +1,3 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key tinyint, val string) stored as orc;
 alter table src_orc change key key float;
diff --git a/ql/src/test/queries/clientnegative/orc_type_promotion1_acid.q b/ql/src/test/queries/clientnegative/orc_type_promotion1_acid.q
new file mode 100644
index 0000000000..26e67e573e
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_type_promotion1_acid.q
@@ -0,0 +1,3 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+alter table src_orc change key key float;
diff --git a/ql/src/test/queries/clientnegative/orc_type_promotion2.q b/ql/src/test/queries/clientnegative/orc_type_promotion2.q
index a294beeb23..c4ee1b5164 100644
--- a/ql/src/test/queries/clientnegative/orc_type_promotion2.q
+++ b/ql/src/test/queries/clientnegative/orc_type_promotion2.q
@@ -1,3 +1,4 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key smallint, val string) stored as orc;
 desc src_orc;
 alter table src_orc change key key smallint;
diff --git a/ql/src/test/queries/clientnegative/orc_type_promotion2_acid.q b/ql/src/test/queries/clientnegative/orc_type_promotion2_acid.q
new file mode 100644
index 0000000000..e076d2ba62
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_type_promotion2_acid.q
@@ -0,0 +1,10 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key smallint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+desc src_orc;
+alter table src_orc change key key smallint;
+desc src_orc;
+alter table src_orc change key key int;
+desc src_orc;
+alter table src_orc change key key bigint;
+desc src_orc;
+alter table src_orc change val val char(100);
diff --git a/ql/src/test/queries/clientnegative/orc_type_promotion3.q b/ql/src/test/queries/clientnegative/orc_type_promotion3.q
index 3d85bceece..3ee99ec4fe 100644
--- a/ql/src/test/queries/clientnegative/orc_type_promotion3.q
+++ b/ql/src/test/queries/clientnegative/orc_type_promotion3.q
@@ -1,2 +1,3 @@
+SET hive.exec.schema.evolution=true;
 create table src_orc (key tinyint, val string) stored as orc;
 alter table src_orc change key key smallint;
diff --git a/ql/src/test/queries/clientnegative/orc_type_promotion3_acid.q b/ql/src/test/queries/clientnegative/orc_type_promotion3_acid.q
new file mode 100644
index 0000000000..3b7c28b609
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/orc_type_promotion3_acid.q
@@ -0,0 +1,3 @@
+SET hive.exec.schema.evolution=false;
+create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true');
+alter table src_orc change key key smallint;
diff --git a/ql/src/test/queries/clientpositive/schema_evol_orc_acid_mapwork_part.q b/ql/src/test/queries/clientpositive/schema_evol_orc_acid_mapwork_part.q
index 6fe9d45e67..800c5f8b1d 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_orc_acid_mapwork_part.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_orc_acid_mapwork_part.q
@@ -2,7 +2,7 @@ set hive.mapred.mode=nonstrict;
 set hive.cli.print.header=true;
 set hive.support.concurrency=true;
 set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
-
+SET hive.exec.schema.evolution=false;
 SET hive.vectorized.execution.enabled=false;
 set hive.fetch.task.conversion=none;
 set hive.exec.dynamic.partition.mode=nonstrict;
@@ -11,6 +11,7 @@ set hive.exec.dynamic.partition.mode=nonstrict;
 -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/queries/clientpositive/schema_evol_orc_acid_mapwork_table.q b/ql/src/test/queries/clientpositive/schema_evol_orc_acid_mapwork_table.q
index 8c933e1de6..b006acbe3a 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_orc_acid_mapwork_table.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_orc_acid_mapwork_table.q
@@ -1,7 +1,7 @@
 set hive.cli.print.header=true;
 set hive.support.concurrency=true;
 set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
-
+SET hive.exec.schema.evolution=false;
 SET hive.vectorized.execution.enabled=false;
 set hive.fetch.task.conversion=none;
 set hive.exec.dynamic.partition.mode=nonstrict;
@@ -10,6 +10,7 @@ set hive.exec.dynamic.partition.mode=nonstrict;
 -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/queries/clientpositive/schema_evol_orc_acidvec_mapwork_part.q b/ql/src/test/queries/clientpositive/schema_evol_orc_acidvec_mapwork_part.q
index 1581192839..fc935d5a2c 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_orc_acidvec_mapwork_part.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_orc_acidvec_mapwork_part.q
@@ -2,7 +2,7 @@ set hive.mapred.mode=nonstrict;
 set hive.cli.print.header=true;
 set hive.support.concurrency=true;
 set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
-
+SET hive.exec.schema.evolution=false;
 SET hive.vectorized.execution.enabled=true;
 set hive.fetch.task.conversion=none;
 set hive.exec.dynamic.partition.mode=nonstrict;
@@ -11,6 +11,7 @@ set hive.exec.dynamic.partition.mode=nonstrict;
 -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/queries/clientpositive/schema_evol_orc_acidvec_mapwork_table.q b/ql/src/test/queries/clientpositive/schema_evol_orc_acidvec_mapwork_table.q
index d6e82f50a1..e49a0f3bb5 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_orc_acidvec_mapwork_table.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_orc_acidvec_mapwork_table.q
@@ -1,7 +1,7 @@
 set hive.cli.print.header=true;
 set hive.support.concurrency=true;
 set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
-
+SET hive.exec.schema.evolution=false;
 SET hive.vectorized.execution.enabled=true;
 set hive.fetch.task.conversion=none;
 set hive.exec.dynamic.partition.mode=nonstrict;
@@ -10,6 +10,7 @@ set hive.exec.dynamic.partition.mode=nonstrict;
 -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_fetchwork_part.q b/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_fetchwork_part.q
index be3e4dac55..9c66243ba2 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_fetchwork_part.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_fetchwork_part.q
@@ -1,5 +1,6 @@
 set hive.mapred.mode=nonstrict;
 set hive.cli.print.header=true;
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=false;
 set hive.fetch.task.conversion=more;
 set hive.exec.dynamic.partition.mode=nonstrict;
diff --git a/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_fetchwork_table.q b/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_fetchwork_table.q
index 8e7e373d3a..5bdd620015 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_fetchwork_table.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_fetchwork_table.q
@@ -1,6 +1,7 @@
 set hive.cli.print.header=true;
 set hive.support.concurrency=true;
 set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=false;
 set hive.fetch.task.conversion=more;
 set hive.exec.dynamic.partition.mode=nonstrict;
diff --git a/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_mapwork_part.q b/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_mapwork_part.q
index d1b0d9777d..0fbcadba1e 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_mapwork_part.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_orc_nonvec_mapwork_part.q
@@ -1,5 +1,6 @@
 set hive.mapred.mode=nonstrict;
 set hive.cli.print.header=true;
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=false;
 set hive.fetch.task.conversion=none;
 set hive.exec.dynamic.partition.mode=nonstrict;
diff --git a/ql/src/test/queries/clientpositive/schema_evol_orc_vec_mapwork_part.q b/ql/src/test/queries/clientpositive/schema_evol_orc_vec_mapwork_part.q
index 676756d1fd..30b19bb353 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_orc_vec_mapwork_part.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_orc_vec_mapwork_part.q
@@ -1,5 +1,6 @@
 set hive.mapred.mode=nonstrict;
 set hive.cli.print.header=true;
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=true;
 set hive.fetch.task.conversion=more;
 set hive.exec.dynamic.partition.mode=nonstrict;
diff --git a/ql/src/test/queries/clientpositive/schema_evol_text_fetchwork_table.q b/ql/src/test/queries/clientpositive/schema_evol_text_fetchwork_table.q
index 7de536711c..44f7264f29 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_text_fetchwork_table.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_text_fetchwork_table.q
@@ -1,7 +1,7 @@
 set hive.cli.print.header=true;
 set hive.support.concurrency=true;
 set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
-
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=false;
 set hive.fetch.task.conversion=none;
 set hive.exec.dynamic.partition.mode=nonstrict;
diff --git a/ql/src/test/queries/clientpositive/schema_evol_text_mapwork_table.q b/ql/src/test/queries/clientpositive/schema_evol_text_mapwork_table.q
index 7de536711c..44f7264f29 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_text_mapwork_table.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_text_mapwork_table.q
@@ -1,7 +1,7 @@
 set hive.cli.print.header=true;
 set hive.support.concurrency=true;
 set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
-
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=false;
 set hive.fetch.task.conversion=none;
 set hive.exec.dynamic.partition.mode=nonstrict;
diff --git a/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_fetchwork_part.q b/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_fetchwork_part.q
index eaa3dc35b6..4d78642baa 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_fetchwork_part.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_fetchwork_part.q
@@ -1,5 +1,6 @@
 set hive.mapred.mode=nonstrict;
 set hive.cli.print.header=true;
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=false;
 set hive.fetch.task.conversion=more;
 set hive.exec.dynamic.partition.mode=nonstrict;
diff --git a/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_fetchwork_table.q b/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_fetchwork_table.q
index 67c2fc3ab0..0834351216 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_fetchwork_table.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_fetchwork_table.q
@@ -1,4 +1,5 @@
 set hive.cli.print.header=true;
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=true;
 set hive.fetch.task.conversion=more;
 
diff --git a/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_mapwork_part.q b/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_mapwork_part.q
index aa2cd5e32e..173e41738c 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_mapwork_part.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_mapwork_part.q
@@ -1,5 +1,6 @@
 set hive.mapred.mode=nonstrict;
 set hive.cli.print.header=true;
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=false;
 set hive.fetch.task.conversion=none;
 set hive.exec.dynamic.partition.mode=nonstrict;
diff --git a/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_mapwork_table.q b/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_mapwork_table.q
index 499d36dd7f..83cab14132 100644
--- a/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_mapwork_table.q
+++ b/ql/src/test/queries/clientpositive/schema_evol_text_nonvec_mapwork_table.q
@@ -1,4 +1,5 @@
 set hive.cli.print.header=true;
+SET hive.exec.schema.evolution=true;
 SET hive.vectorized.execution.enabled=true;
 set hive.fetch.task.conversion=none;
 
diff --git a/ql/src/test/results/clientnegative/orc_change_fileformat_acid.q.out b/ql/src/test/results/clientnegative/orc_change_fileformat_acid.q.out
new file mode 100644
index 0000000000..c29fe79fc0
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_change_fileformat_acid.q.out
@@ -0,0 +1,13 @@
+PREHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc set fileformat textfile
+PREHOOK: type: ALTERTABLE_FILEFORMAT
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Changing file format (from ORC) is not supported for table default.src_orc
diff --git a/ql/src/test/results/clientnegative/orc_change_serde_acid.q.out b/ql/src/test/results/clientnegative/orc_change_serde_acid.q.out
new file mode 100644
index 0000000000..01fb8700b4
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_change_serde_acid.q.out
@@ -0,0 +1,13 @@
+PREHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
+PREHOOK: type: ALTERTABLE_SERIALIZER
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Changing SerDe (from OrcSerde) is not supported for table default.src_orc. File format may be incompatible
diff --git a/ql/src/test/results/clientnegative/orc_reorder_columns1_acid.q.out b/ql/src/test/results/clientnegative/orc_reorder_columns1_acid.q.out
new file mode 100644
index 0000000000..5186081350
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_reorder_columns1_acid.q.out
@@ -0,0 +1,13 @@
+PREHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc change key k tinyint first
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Reordering columns is not supported for table default.src_orc. SerDe may be incompatible
diff --git a/ql/src/test/results/clientnegative/orc_reorder_columns2_acid.q.out b/ql/src/test/results/clientnegative/orc_reorder_columns2_acid.q.out
new file mode 100644
index 0000000000..7b65d7cc2d
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_reorder_columns2_acid.q.out
@@ -0,0 +1,13 @@
+PREHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc change key k tinyint after val
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Reordering columns is not supported for table default.src_orc. SerDe may be incompatible
diff --git a/ql/src/test/results/clientnegative/orc_replace_columns1_acid.q.out b/ql/src/test/results/clientnegative/orc_replace_columns1_acid.q.out
new file mode 100644
index 0000000000..ec09d4b4e6
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_replace_columns1_acid.q.out
@@ -0,0 +1,13 @@
+PREHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc replace columns (k int)
+PREHOOK: type: ALTERTABLE_REPLACECOLS
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Replacing columns cannot drop columns for table default.src_orc. SerDe may be incompatible
diff --git a/ql/src/test/results/clientnegative/orc_replace_columns2_acid.q.out b/ql/src/test/results/clientnegative/orc_replace_columns2_acid.q.out
new file mode 100644
index 0000000000..ae373d238f
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_replace_columns2_acid.q.out
@@ -0,0 +1,13 @@
+PREHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc replace columns (k smallint, val string)
+PREHOOK: type: ALTERTABLE_REPLACECOLS
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Replacing columns with unsupported type conversion (from tinyint to smallint) for column k. SerDe may be incompatible
diff --git a/ql/src/test/results/clientnegative/orc_replace_columns3_acid.q.out b/ql/src/test/results/clientnegative/orc_replace_columns3_acid.q.out
new file mode 100644
index 0000000000..90ab2fd202
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_replace_columns3_acid.q.out
@@ -0,0 +1,21 @@
+PREHOOK: query: create table src_orc (key smallint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key smallint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc replace columns (k int, val string, z smallint)
+PREHOOK: type: ALTERTABLE_REPLACECOLS
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: alter table src_orc replace columns (k int, val string, z smallint)
+POSTHOOK: type: ALTERTABLE_REPLACECOLS
+POSTHOOK: Input: default@src_orc
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc replace columns (k int, val string, z tinyint)
+PREHOOK: type: ALTERTABLE_REPLACECOLS
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Replacing columns with unsupported type conversion (from smallint to tinyint) for column z. SerDe may be incompatible
diff --git a/ql/src/test/results/clientnegative/orc_type_promotion1_acid.q.out b/ql/src/test/results/clientnegative/orc_type_promotion1_acid.q.out
new file mode 100644
index 0000000000..5357fd169d
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_type_promotion1_acid.q.out
@@ -0,0 +1,13 @@
+PREHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc change key key float
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Changing from type tinyint to float is not supported for column key. SerDe may be incompatible
diff --git a/ql/src/test/results/clientnegative/orc_type_promotion2_acid.q.out b/ql/src/test/results/clientnegative/orc_type_promotion2_acid.q.out
new file mode 100644
index 0000000000..74e75cfdbd
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_type_promotion2_acid.q.out
@@ -0,0 +1,69 @@
+PREHOOK: query: create table src_orc (key smallint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key smallint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: desc src_orc
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@src_orc
+POSTHOOK: query: desc src_orc
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@src_orc
+key                 	smallint            	                    
+val                 	string              	                    
+PREHOOK: query: alter table src_orc change key key smallint
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: alter table src_orc change key key smallint
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@src_orc
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: desc src_orc
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@src_orc
+POSTHOOK: query: desc src_orc
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@src_orc
+key                 	smallint            	                    
+val                 	string              	                    
+PREHOOK: query: alter table src_orc change key key int
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: alter table src_orc change key key int
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@src_orc
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: desc src_orc
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@src_orc
+POSTHOOK: query: desc src_orc
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@src_orc
+key                 	int                 	                    
+val                 	string              	                    
+PREHOOK: query: alter table src_orc change key key bigint
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: alter table src_orc change key key bigint
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@src_orc
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: desc src_orc
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@src_orc
+POSTHOOK: query: desc src_orc
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@src_orc
+key                 	bigint              	                    
+val                 	string              	                    
+PREHOOK: query: alter table src_orc change val val char(100)
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Changing from type string to char(100) is not supported for column val. SerDe may be incompatible
diff --git a/ql/src/test/results/clientnegative/orc_type_promotion3_acid.q.out b/ql/src/test/results/clientnegative/orc_type_promotion3_acid.q.out
new file mode 100644
index 0000000000..15e87fad0a
--- /dev/null
+++ b/ql/src/test/results/clientnegative/orc_type_promotion3_acid.q.out
@@ -0,0 +1,13 @@
+PREHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@src_orc
+POSTHOOK: query: create table src_orc (key tinyint, val string) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@src_orc
+PREHOOK: query: alter table src_orc change key key smallint
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@src_orc
+PREHOOK: Output: default@src_orc
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Changing from type tinyint to smallint is not supported for column key. SerDe may be incompatible
diff --git a/ql/src/test/results/clientpositive/schema_evol_orc_acid_mapwork_part.q.out b/ql/src/test/results/clientpositive/schema_evol_orc_acid_mapwork_part.q.out
index 92d8b01885..a922175e67 100644
--- a/ql/src/test/results/clientpositive/schema_evol_orc_acid_mapwork_part.q.out
+++ b/ql/src/test/results/clientpositive/schema_evol_orc_acid_mapwork_part.q.out
@@ -1,6 +1,7 @@
 PREHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
@@ -12,6 +13,7 @@ PREHOOK: Output: default@partitioned1
 POSTHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/results/clientpositive/schema_evol_orc_acid_mapwork_table.q.out b/ql/src/test/results/clientpositive/schema_evol_orc_acid_mapwork_table.q.out
index 0317a9937a..4885aebdd6 100644
--- a/ql/src/test/results/clientpositive/schema_evol_orc_acid_mapwork_table.q.out
+++ b/ql/src/test/results/clientpositive/schema_evol_orc_acid_mapwork_table.q.out
@@ -1,6 +1,7 @@
 PREHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
@@ -12,6 +13,7 @@ PREHOOK: Output: default@table1
 POSTHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/results/clientpositive/schema_evol_orc_acidvec_mapwork_part.q.out b/ql/src/test/results/clientpositive/schema_evol_orc_acidvec_mapwork_part.q.out
index babac1a750..c5af165296 100644
--- a/ql/src/test/results/clientpositive/schema_evol_orc_acidvec_mapwork_part.q.out
+++ b/ql/src/test/results/clientpositive/schema_evol_orc_acidvec_mapwork_part.q.out
@@ -1,6 +1,7 @@
 PREHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
@@ -12,6 +13,7 @@ PREHOOK: Output: default@partitioned1
 POSTHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/results/clientpositive/schema_evol_orc_acidvec_mapwork_table.q.out b/ql/src/test/results/clientpositive/schema_evol_orc_acidvec_mapwork_table.q.out
index 3edaff0d69..2b1e5c3a04 100644
--- a/ql/src/test/results/clientpositive/schema_evol_orc_acidvec_mapwork_table.q.out
+++ b/ql/src/test/results/clientpositive/schema_evol_orc_acidvec_mapwork_table.q.out
@@ -1,6 +1,7 @@
 PREHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
@@ -12,6 +13,7 @@ PREHOOK: Output: default@table1
 POSTHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/results/clientpositive/tez/schema_evol_orc_acid_mapwork_part.q.out b/ql/src/test/results/clientpositive/tez/schema_evol_orc_acid_mapwork_part.q.out
index 92d8b01885..a922175e67 100644
--- a/ql/src/test/results/clientpositive/tez/schema_evol_orc_acid_mapwork_part.q.out
+++ b/ql/src/test/results/clientpositive/tez/schema_evol_orc_acid_mapwork_part.q.out
@@ -1,6 +1,7 @@
 PREHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
@@ -12,6 +13,7 @@ PREHOOK: Output: default@partitioned1
 POSTHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/results/clientpositive/tez/schema_evol_orc_acid_mapwork_table.q.out b/ql/src/test/results/clientpositive/tez/schema_evol_orc_acid_mapwork_table.q.out
index 0317a9937a..4885aebdd6 100644
--- a/ql/src/test/results/clientpositive/tez/schema_evol_orc_acid_mapwork_table.q.out
+++ b/ql/src/test/results/clientpositive/tez/schema_evol_orc_acid_mapwork_table.q.out
@@ -1,6 +1,7 @@
 PREHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
@@ -12,6 +13,7 @@ PREHOOK: Output: default@table1
 POSTHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/results/clientpositive/tez/schema_evol_orc_acidvec_mapwork_part.q.out b/ql/src/test/results/clientpositive/tez/schema_evol_orc_acidvec_mapwork_part.q.out
index babac1a750..c5af165296 100644
--- a/ql/src/test/results/clientpositive/tez/schema_evol_orc_acidvec_mapwork_part.q.out
+++ b/ql/src/test/results/clientpositive/tez/schema_evol_orc_acidvec_mapwork_part.q.out
@@ -1,6 +1,7 @@
 PREHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
@@ -12,6 +13,7 @@ PREHOOK: Output: default@partitioned1
 POSTHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Partitioned
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
diff --git a/ql/src/test/results/clientpositive/tez/schema_evol_orc_acidvec_mapwork_table.q.out b/ql/src/test/results/clientpositive/tez/schema_evol_orc_acidvec_mapwork_table.q.out
index 3edaff0d69..2b1e5c3a04 100644
--- a/ql/src/test/results/clientpositive/tez/schema_evol_orc_acidvec_mapwork_table.q.out
+++ b/ql/src/test/results/clientpositive/tez/schema_evol_orc_acidvec_mapwork_table.q.out
@@ -1,6 +1,7 @@
 PREHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
@@ -12,6 +13,7 @@ PREHOOK: Output: default@table1
 POSTHOOK: query: -- SORT_QUERY_RESULTS
 --
 -- FILE VARIATION: ORC, ACID Vectorized, MapWork, Table
+-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
 --
 --
 -- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
