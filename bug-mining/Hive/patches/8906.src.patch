diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/FilesForCommit.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/FilesForCommit.java
index 1ca3872b42..b395f4c038 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/FilesForCommit.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/FilesForCommit.java
@@ -22,6 +22,7 @@
 import java.io.Serializable;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.List;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 import org.apache.hadoop.fs.Path;
@@ -35,6 +36,7 @@ public class FilesForCommit implements Serializable {
   private final Collection<DataFile> dataFiles;
   private final Collection<DeleteFile> deleteFiles;
   private final Collection<DataFile> replacedDataFiles;
+  private final Collection<DeleteFile> rewrittenDeleteFiles;
   private final Collection<CharSequence> referencedDataFiles;
   private final Collection<Path> mergedAndDeletedFiles;
 
@@ -44,23 +46,25 @@ public FilesForCommit(Collection<DataFile> dataFiles, Collection<DeleteFile> del
 
   public FilesForCommit(Collection<DataFile> dataFiles, Collection<DeleteFile> deleteFiles,
       Collection<DataFile> replacedDataFiles, Collection<CharSequence> referencedDataFiles,
-      Collection<Path> mergedAndDeletedFiles) {
+      Collection<DeleteFile> rewrittenDeleteFiles, Collection<Path> mergedAndDeletedFiles) {
     this.dataFiles = dataFiles;
     this.deleteFiles = deleteFiles;
     this.replacedDataFiles = replacedDataFiles;
     this.referencedDataFiles = referencedDataFiles;
+    this.rewrittenDeleteFiles = rewrittenDeleteFiles;
     this.mergedAndDeletedFiles = mergedAndDeletedFiles;
   }
 
   public FilesForCommit(Collection<DataFile> dataFiles, Collection<DeleteFile> deleteFiles,
       Collection<DataFile> replacedDataFiles) {
-    this(dataFiles, deleteFiles, replacedDataFiles, Collections.emptySet(), Collections.emptySet());
+    this(dataFiles, deleteFiles, replacedDataFiles, Collections.emptySet(), Collections.emptySet(),
+        Collections.emptySet());
   }
 
   public static FilesForCommit onlyDelete(Collection<DeleteFile> deleteFiles,
-      Collection<CharSequence> referencedDataFiles) {
-    return new FilesForCommit(Collections.emptyList(), deleteFiles, Collections.emptyList(),
-            referencedDataFiles, Collections.emptySet());
+      Collection<CharSequence> referencedDataFiles, List<DeleteFile> rewrittenDeleteFiles) {
+    return new FilesForCommit(Collections.emptyList(), deleteFiles, Collections.emptyList(), referencedDataFiles,
+        rewrittenDeleteFiles, Collections.emptySet());
   }
 
   public static FilesForCommit onlyData(Collection<DataFile> dataFiles) {
@@ -87,6 +91,10 @@ public Collection<DataFile> replacedDataFiles() {
     return replacedDataFiles;
   }
 
+  public Collection<DeleteFile> rewrittenDeleteFiles() {
+    return rewrittenDeleteFiles;
+  }
+
   public Collection<CharSequence> referencedDataFiles() {
     return referencedDataFiles;
   }
@@ -110,8 +118,8 @@ public String toString() {
         .add("deleteFiles", deleteFiles.toString())
         .add("replacedDataFiles", replacedDataFiles.toString())
         .add("referencedDataFiles", referencedDataFiles.toString())
+        .add("rewrittenDeleteFiles", rewrittenDeleteFiles.toString())
         .add("mergedAndDeletedFiles", mergedAndDeletedFiles.toString())
         .toString();
   }
-
 }
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java
index 489ac013df..5bb9390d86 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java
@@ -165,6 +165,7 @@ public void commitTask(TaskAttemptContext originalContext) throws IOException {
                 List<DataFile> dataFiles = Lists.newArrayList();
                 List<DeleteFile> deleteFiles = Lists.newArrayList();
                 List<DataFile> replacedDataFiles = Lists.newArrayList();
+                List<DeleteFile> rewrittenDeleteFiles = Lists.newArrayList();
                 Set<CharSequence> referencedDataFiles = Sets.newHashSet();
 
                 for (HiveIcebergWriter writer : writers.get(output)) {
@@ -173,10 +174,11 @@ public void commitTask(TaskAttemptContext originalContext) throws IOException {
                   deleteFiles.addAll(files.deleteFiles());
                   replacedDataFiles.addAll(files.replacedDataFiles());
                   referencedDataFiles.addAll(files.referencedDataFiles());
+                  rewrittenDeleteFiles.addAll(files.rewrittenDeleteFiles());
                 }
                 createFileForCommit(
-                    new FilesForCommit(dataFiles, deleteFiles, replacedDataFiles, referencedDataFiles, mergedPaths),
-                    fileForCommitLocation, table.io());
+                    new FilesForCommit(dataFiles, deleteFiles, replacedDataFiles, referencedDataFiles,
+                        rewrittenDeleteFiles, mergedPaths), fileForCommitLocation, table.io());
               } else {
                 LOG.info("CommitTask found no writer for specific table: {}, attemptID: {}", output, attemptID);
                 createFileForCommit(FilesForCommit.empty(), fileForCommitLocation, table.io());
@@ -426,6 +428,7 @@ private void commitTable(FileIO io, ExecutorService executor, OutputTable output
     List<DataFile> dataFiles = Lists.newArrayList();
     List<DeleteFile> deleteFiles = Lists.newArrayList();
     List<DataFile> replacedDataFiles = Lists.newArrayList();
+    List<DeleteFile> rewrittenDeleteFiles = Lists.newArrayList();
     Set<CharSequence> referencedDataFiles = Sets.newHashSet();
     Set<Path> mergedAndDeletedFiles = Sets.newHashSet();
 
@@ -467,6 +470,7 @@ private void commitTable(FileIO io, ExecutorService executor, OutputTable output
       deleteFiles.addAll(writeResults.deleteFiles());
       replacedDataFiles.addAll(writeResults.replacedDataFiles());
       referencedDataFiles.addAll(writeResults.referencedDataFiles());
+      rewrittenDeleteFiles.addAll(writeResults.rewrittenDeleteFiles());
 
       mergedAndDeletedFiles.addAll(writeResults.mergedAndDeletedFiles());
     }
@@ -474,8 +478,9 @@ private void commitTable(FileIO io, ExecutorService executor, OutputTable output
     dataFiles.removeIf(dataFile -> mergedAndDeletedFiles.contains(new Path(dataFile.location())));
     deleteFiles.removeIf(deleteFile -> mergedAndDeletedFiles.contains(new Path(deleteFile.location())));
 
-    FilesForCommit filesForCommit = new FilesForCommit(dataFiles, deleteFiles, replacedDataFiles, referencedDataFiles,
-        Collections.emptySet());
+    FilesForCommit filesForCommit =
+        new FilesForCommit(dataFiles, deleteFiles, replacedDataFiles, referencedDataFiles, rewrittenDeleteFiles,
+            Collections.emptySet());
     long startTime = System.currentTimeMillis();
 
     if (Operation.IOW != operation) {
@@ -562,6 +567,7 @@ private void commitWrite(Table table, String branchName, Long snapshotId, long s
       RowDelta write = table.newRowDelta();
       results.dataFiles().forEach(write::addRows);
       results.deleteFiles().forEach(write::addDeletes);
+      results.rewrittenDeleteFiles().forEach(write::removeDeletes);
 
       if (StringUtils.isNotEmpty(branchName)) {
         write.toBranch(HiveUtils.getTableSnapshotRef(branchName));
@@ -757,6 +763,7 @@ private static FilesForCommit collectResults(int numTasks, ExecutorService execu
     Collection<DataFile> dataFiles = new ConcurrentLinkedQueue<>();
     Collection<DeleteFile> deleteFiles = new ConcurrentLinkedQueue<>();
     Collection<DataFile> replacedDataFiles = new ConcurrentLinkedQueue<>();
+    Collection<DeleteFile> rewrittenDeleteFiles = new ConcurrentLinkedQueue<>();
     Collection<CharSequence> referencedDataFiles = new ConcurrentLinkedQueue<>();
     Collection<Path> mergedAndDeletedFiles = new ConcurrentLinkedQueue<>();
     Tasks.range(numTasks)
@@ -771,11 +778,13 @@ private static FilesForCommit collectResults(int numTasks, ExecutorService execu
           dataFiles.addAll(files.dataFiles());
           deleteFiles.addAll(files.deleteFiles());
           replacedDataFiles.addAll(files.replacedDataFiles());
+          rewrittenDeleteFiles.addAll(files.rewrittenDeleteFiles());
           referencedDataFiles.addAll(files.referencedDataFiles());
           mergedAndDeletedFiles.addAll(files.mergedAndDeletedFiles());
         });
 
-    return new FilesForCommit(dataFiles, deleteFiles, replacedDataFiles, referencedDataFiles, mergedAndDeletedFiles);
+    return new FilesForCommit(dataFiles, deleteFiles, replacedDataFiles, referencedDataFiles, rewrittenDeleteFiles,
+        mergedAndDeletedFiles);
   }
 
   /**
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/writer/HiveIcebergDeleteWriter.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/writer/HiveIcebergDeleteWriter.java
index aa387bf436..9365f9834a 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/writer/HiveIcebergDeleteWriter.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/writer/HiveIcebergDeleteWriter.java
@@ -71,8 +71,10 @@ public void write(Writable row) throws IOException {
 
   @Override
   public FilesForCommit files() {
-    List<DeleteFile> deleteFiles = ((DeleteWriteResult) writer.result()).deleteFiles();
-    Set<CharSequence> referencedDataFiles = ((DeleteWriteResult) writer.result()).referencedDataFiles();
-    return FilesForCommit.onlyDelete(deleteFiles, referencedDataFiles);
+    DeleteWriteResult result = (DeleteWriteResult) writer.result();
+    List<DeleteFile> deleteFiles = result.deleteFiles();
+    Set<CharSequence> referencedDataFiles = result.referencedDataFiles();
+    List<DeleteFile> rewrittenDeleteFiles = result.rewrittenDeleteFiles();
+    return FilesForCommit.onlyDelete(deleteFiles, referencedDataFiles, rewrittenDeleteFiles);
   }
 }
diff --git a/iceberg/iceberg-handler/src/test/queries/positive/iceberg_v3_deletion_vectors.q b/iceberg/iceberg-handler/src/test/queries/positive/iceberg_v3_deletion_vectors.q
index 60a0d066d2..8b4aaecd3a 100644
--- a/iceberg/iceberg-handler/src/test/queries/positive/iceberg_v3_deletion_vectors.q
+++ b/iceberg/iceberg-handler/src/test/queries/positive/iceberg_v3_deletion_vectors.q
@@ -25,9 +25,12 @@ insert into ice01 values (1),(2),(3),(4);
 select * from ice01;
 
 -- delete some values
-delete from ice01 where id>2;
+delete from ice01 where id=3;
 select content, file_format, spec_id, record_count, content_offset from default.ice01.delete_files;
+-- delete one more time to trigger one more delete file for the same data file
+delete from ice01 where id=4;
 
+select content, file_format, spec_id, record_count, content_offset from default.ice01.delete_files;
 -- check the values, the delete value should be there
 select * from ice01 order by id;
 
@@ -44,6 +47,14 @@ select content, file_format, spec_id, record_count, content_offset from default.
 -- check the entries, the deleted entries shouldn't be there.
 select * from ice01 order by id;
 
+-- update one value part of the same data file where delete happened
+update ice01 set id=20 where id=8;
+
+select content, file_format, spec_id, record_count, content_offset from default.ice01.delete_files;
+
+-- check the entries, the deleted entries shouldn't be there.
+select * from ice01 order by id;
+
 -- create a partitioned table
  create table icepart01 (id int) partitioned by (part int) Stored by Iceberg stored as ORC
  TBLPROPERTIES('format-version'='3');
diff --git a/iceberg/iceberg-handler/src/test/results/positive/iceberg_v3_deletion_vectors.q.out b/iceberg/iceberg-handler/src/test/results/positive/iceberg_v3_deletion_vectors.q.out
index e6bcbdadc2..d6246c0d5a 100644
--- a/iceberg/iceberg-handler/src/test/results/positive/iceberg_v3_deletion_vectors.q.out
+++ b/iceberg/iceberg-handler/src/test/results/positive/iceberg_v3_deletion_vectors.q.out
@@ -59,11 +59,28 @@ POSTHOOK: Output: hdfs://### HDFS PATH ###
 2
 3
 4
-PREHOOK: query: delete from ice01 where id>2
+PREHOOK: query: delete from ice01 where id=3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@ice01
 PREHOOK: Output: default@ice01
-POSTHOOK: query: delete from ice01 where id>2
+POSTHOOK: query: delete from ice01 where id=3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ice01
+POSTHOOK: Output: default@ice01
+PREHOOK: query: select content, file_format, spec_id, record_count, content_offset from default.ice01.delete_files
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ice01
+PREHOOK: Output: hdfs://### HDFS PATH ###
+POSTHOOK: query: select content, file_format, spec_id, record_count, content_offset from default.ice01.delete_files
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ice01
+POSTHOOK: Output: hdfs://### HDFS PATH ###
+1	PUFFIN	0	1	4
+PREHOOK: query: delete from ice01 where id=4
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ice01
+PREHOOK: Output: default@ice01
+POSTHOOK: query: delete from ice01 where id=4
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@ice01
 POSTHOOK: Output: default@ice01
@@ -139,6 +156,39 @@ POSTHOOK: Output: hdfs://### HDFS PATH ###
 5
 6
 8
+PREHOOK: query: update ice01 set id=20 where id=8
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ice01
+PREHOOK: Output: default@ice01
+PREHOOK: Output: default@ice01
+POSTHOOK: query: update ice01 set id=20 where id=8
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ice01
+POSTHOOK: Output: default@ice01
+POSTHOOK: Output: default@ice01
+PREHOOK: query: select content, file_format, spec_id, record_count, content_offset from default.ice01.delete_files
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ice01
+PREHOOK: Output: hdfs://### HDFS PATH ###
+POSTHOOK: query: select content, file_format, spec_id, record_count, content_offset from default.ice01.delete_files
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ice01
+POSTHOOK: Output: hdfs://### HDFS PATH ###
+1	PUFFIN	0	2	4
+1	PUFFIN	0	2	4
+PREHOOK: query: select * from ice01 order by id
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ice01
+PREHOOK: Output: hdfs://### HDFS PATH ###
+POSTHOOK: query: select * from ice01 order by id
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ice01
+POSTHOOK: Output: hdfs://### HDFS PATH ###
+1
+2
+5
+6
+20
 PREHOOK: query: create table icepart01 (id int) partitioned by (part int) Stored by Iceberg stored as ORC
  TBLPROPERTIES('format-version'='3')
 PREHOOK: type: CREATETABLE
