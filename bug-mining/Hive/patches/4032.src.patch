diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/parser/Filter.g b/metastore/src/java/org/apache/hadoop/hive/metastore/parser/Filter.g
index 8aef5bf631..81111a0c60 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/parser/Filter.g
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/parser/Filter.g
@@ -122,6 +122,10 @@ operatorExpression
     :
     betweenExpression
     |
+    inExpression
+    |
+    multiColInExpression
+    |
     binOpExpression
     ;
 
@@ -203,16 +207,229 @@ betweenExpression
         tree.addIntermediateNode(isPositive ? LogicalOperator.AND : LogicalOperator.OR);
     };
 
+inExpression
+@init {
+    List constants = new ArrayList();
+    Object constantV = null;
+    boolean isPositive = true;
+}
+    :
+    (
+        LPAREN key = Identifier RPAREN ( KW_NOT { isPositive = false; } )? IN LPAREN
+        (
+            (
+                constant = DateLiteral
+                {
+                    constantV = FilterLexer.ExtractDate(constant.getText());
+                    constants.add(constantV);
+                }
+                (
+                    COMMA constant = DateLiteral
+                    {
+                        constantV = FilterLexer.ExtractDate(constant.getText());
+                        constants.add(constantV);
+                    }
+                )*
+            )
+            |
+            (
+                constant = StringLiteral
+                {
+                    constantV = TrimQuotes(constant.getText());
+                    constants.add(constantV);
+                }
+                (
+                    COMMA constant = StringLiteral
+                    {
+                        constantV = TrimQuotes(constant.getText());
+                        constants.add(constantV);
+                    }
+                )*
+            )
+            |
+            (
+                constant = IntegralLiteral
+                {
+                    constantV = Long.parseLong(constant.getText());
+                    constants.add(constantV);
+                }
+                (
+                    COMMA constant = IntegralLiteral
+                    {
+                        constantV = Long.parseLong(constant.getText());
+                        constants.add(constantV);
+                    }
+                )*
+            )
+        ) RPAREN
+    )
+    {
+        for (int i = 0; i < constants.size(); i++) {
+            Object value = constants.get(i);
+            LeafNode leaf = new LeafNode();
+            leaf.keyName = key.getText();
+            leaf.value = value;
+            leaf.operator = isPositive ? Operator.EQUALS : Operator.NOTEQUALS2;
+            tree.addLeafNode(leaf);
+            if (i != 0) {
+                tree.addIntermediateNode(isPositive ? LogicalOperator.OR : LogicalOperator.AND);
+            }
+        }
+    };
+
+multiColInExpression
+@init {
+    List<String> keyNames = new ArrayList<String>();
+    List constants = new ArrayList();
+    List partialConstants;
+    String keyV = null;
+    Object constantV = null;
+    boolean isPositive = true;
+}
+    :
+    (
+        LPAREN
+        (
+            KW_STRUCT LPAREN key = Identifier
+            {
+                keyV = key.getText();
+                keyNames.add(keyV);
+            }
+            (
+                COMMA key = Identifier
+                {
+                    keyV = key.getText();
+                    keyNames.add(keyV);
+                }
+            )* RPAREN
+        ) RPAREN ( KW_NOT { isPositive = false; } )? IN LPAREN KW_CONST KW_STRUCT LPAREN
+        {
+            partialConstants = new ArrayList();
+        }
+        (
+            constant = DateLiteral
+            {
+                constantV = FilterLexer.ExtractDate(constant.getText());
+                partialConstants.add(constantV);
+            }
+            | constant = StringLiteral
+            {
+                constantV = TrimQuotes(constant.getText());
+                partialConstants.add(constantV);
+            }
+            | constant = IntegralLiteral
+            {
+                constantV = Long.parseLong(constant.getText());
+                partialConstants.add(constantV);
+            }
+        )
+        (
+            COMMA
+            (
+                constant = DateLiteral
+                {
+                    constantV = FilterLexer.ExtractDate(constant.getText());
+                    partialConstants.add(constantV);
+                }
+                | constant = StringLiteral
+                {
+                    constantV = TrimQuotes(constant.getText());
+                    partialConstants.add(constantV);
+                }
+                | constant = IntegralLiteral
+                {
+                    constantV = Long.parseLong(constant.getText());
+                    partialConstants.add(constantV);
+                }
+            )
+        )*
+        {
+            constants.add(partialConstants);
+        }
+        RPAREN
+        (
+            COMMA KW_CONST KW_STRUCT LPAREN
+            {
+                partialConstants = new ArrayList();
+            }
+            (
+                constant = DateLiteral
+                {
+                    constantV = FilterLexer.ExtractDate(constant.getText());
+                    partialConstants.add(constantV);
+                }
+                | constant = StringLiteral
+                {
+                    constantV = TrimQuotes(constant.getText());
+                    partialConstants.add(constantV);
+                }
+                | constant = IntegralLiteral
+                {
+                    constantV = Long.parseLong(constant.getText());
+                    partialConstants.add(constantV);
+                }
+            )
+            (
+                COMMA
+                (
+                    constant = DateLiteral
+                    {
+                        constantV = FilterLexer.ExtractDate(constant.getText());
+                        partialConstants.add(constantV);
+                    }
+                    | constant = StringLiteral
+                    {
+                        constantV = TrimQuotes(constant.getText());
+                        partialConstants.add(constantV);
+                    }
+                    | constant = IntegralLiteral
+                    {
+                        constantV = Long.parseLong(constant.getText());
+                        partialConstants.add(constantV);
+                    }
+                )
+            )*
+            {
+                constants.add(partialConstants);
+            }
+            RPAREN
+        )* RPAREN
+    )
+    {
+        for (int i = 0; i < constants.size(); i++) {
+            List list = (List) constants.get(i);
+            assert keyNames.size() == list.size();
+            for (int j=0; j < list.size(); j++) {
+                String keyName = keyNames.get(j);
+                Object value = list.get(j);
+                LeafNode leaf = new LeafNode();
+                leaf.keyName = keyName;
+                leaf.value = value;
+                leaf.operator = isPositive ? Operator.EQUALS : Operator.NOTEQUALS2;
+                tree.addLeafNode(leaf);
+                if (j != 0) {
+                    tree.addIntermediateNode(isPositive ? LogicalOperator.AND : LogicalOperator.OR);
+                }
+            }
+            if (i != 0) {
+                tree.addIntermediateNode(isPositive ? LogicalOperator.OR : LogicalOperator.AND);
+            }
+        }
+    };
+
 // Keywords
 KW_NOT : 'NOT';
 KW_AND : 'AND';
 KW_OR : 'OR';
 KW_LIKE : 'LIKE';
 KW_DATE : 'date';
+KW_CONST : 'CONST';
+KW_STRUCT : 'STRUCT';
 
 // Operators
 LPAREN : '(' ;
 RPAREN : ')' ;
+COMMA : ',' ;
 EQUAL : '=';
 NOTEQUAL : '<>' | '!=';
 LESSTHANOREQUALTO : '<=';
@@ -220,6 +437,7 @@ LESSTHAN : '<';
 GREATERTHANOREQUALTO : '>=';
 GREATERTHAN : '>';
 BETWEEN : 'BETWEEN';
+IN : 'IN';
 
 // LITERALS
 fragment
diff --git a/ql/src/test/queries/clientpositive/pointlookup4.q b/ql/src/test/queries/clientpositive/pointlookup4.q
new file mode 100644
index 0000000000..e0bf5a6f81
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/pointlookup4.q
@@ -0,0 +1,27 @@
+drop table pcr_t1;
+
+create table pcr_t1 (key int, value string) partitioned by (ds1 string, ds2 string);
+insert overwrite table pcr_t1 partition (ds1='2000-04-08', ds2='2001-04-08') select * from src where key < 20 order by key;
+insert overwrite table pcr_t1 partition (ds1='2000-04-09', ds2='2001-04-09') select * from src where key < 20 order by key;
+insert overwrite table pcr_t1 partition (ds1='2000-04-10', ds2='2001-04-10') select * from src where key < 20 order by key;
+
+set hive.optimize.point.lookup=false;
+set hive.optimize.partition.columns.separate=false;
+
+explain extended
+select key, value, ds1, ds2
+from pcr_t1
+where (ds1='2000-04-08' and ds2='2001-04-08' and key=1) or (ds1='2000-04-09' and ds2='2001-04-09' and key=2)
+order by key, value, ds1, ds2;
+
+set hive.optimize.point.lookup=true;
+set hive.optimize.point.lookup.min=0;
+set hive.optimize.partition.columns.separate=true;
+
+explain extended
+select key, value, ds1, ds2
+from pcr_t1
+where (ds1='2000-04-08' and ds2='2001-04-08' and key=1) or (ds1='2000-04-09' and ds2='2001-04-09' and key=2)
+order by key, value, ds1, ds2;
+
+drop table pcr_t1;
diff --git a/ql/src/test/results/clientpositive/pointlookup4.q.out b/ql/src/test/results/clientpositive/pointlookup4.q.out
new file mode 100644
index 0000000000..157aea6577
--- /dev/null
+++ b/ql/src/test/results/clientpositive/pointlookup4.q.out
@@ -0,0 +1,530 @@
+PREHOOK: query: drop table pcr_t1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table pcr_t1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table pcr_t1 (key int, value string) partitioned by (ds1 string, ds2 string)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@pcr_t1
+POSTHOOK: query: create table pcr_t1 (key int, value string) partitioned by (ds1 string, ds2 string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@pcr_t1
+PREHOOK: query: insert overwrite table pcr_t1 partition (ds1='2000-04-08', ds2='2001-04-08') select * from src where key < 20 order by key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@pcr_t1@ds1=2000-04-08/ds2=2001-04-08
+POSTHOOK: query: insert overwrite table pcr_t1 partition (ds1='2000-04-08', ds2='2001-04-08') select * from src where key < 20 order by key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@pcr_t1@ds1=2000-04-08/ds2=2001-04-08
+POSTHOOK: Lineage: pcr_t1 PARTITION(ds1=2000-04-08,ds2=2001-04-08).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: pcr_t1 PARTITION(ds1=2000-04-08,ds2=2001-04-08).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table pcr_t1 partition (ds1='2000-04-09', ds2='2001-04-09') select * from src where key < 20 order by key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@pcr_t1@ds1=2000-04-09/ds2=2001-04-09
+POSTHOOK: query: insert overwrite table pcr_t1 partition (ds1='2000-04-09', ds2='2001-04-09') select * from src where key < 20 order by key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@pcr_t1@ds1=2000-04-09/ds2=2001-04-09
+POSTHOOK: Lineage: pcr_t1 PARTITION(ds1=2000-04-09,ds2=2001-04-09).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: pcr_t1 PARTITION(ds1=2000-04-09,ds2=2001-04-09).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table pcr_t1 partition (ds1='2000-04-10', ds2='2001-04-10') select * from src where key < 20 order by key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@pcr_t1@ds1=2000-04-10/ds2=2001-04-10
+POSTHOOK: query: insert overwrite table pcr_t1 partition (ds1='2000-04-10', ds2='2001-04-10') select * from src where key < 20 order by key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@pcr_t1@ds1=2000-04-10/ds2=2001-04-10
+POSTHOOK: Lineage: pcr_t1 PARTITION(ds1=2000-04-10,ds2=2001-04-10).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: pcr_t1 PARTITION(ds1=2000-04-10,ds2=2001-04-10).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: explain extended
+select key, value, ds1, ds2
+from pcr_t1
+where (ds1='2000-04-08' and ds2='2001-04-08' and key=1) or (ds1='2000-04-09' and ds2='2001-04-09' and key=2)
+order by key, value, ds1, ds2
+PREHOOK: type: QUERY
+POSTHOOK: query: explain extended
+select key, value, ds1, ds2
+from pcr_t1
+where (ds1='2000-04-08' and ds2='2001-04-08' and key=1) or (ds1='2000-04-09' and ds2='2001-04-09' and key=2)
+order by key, value, ds1, ds2
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  
+TOK_QUERY
+   TOK_FROM
+      TOK_TABREF
+         TOK_TABNAME
+            pcr_t1
+   TOK_INSERT
+      TOK_DESTINATION
+         TOK_DIR
+            TOK_TMP_FILE
+      TOK_SELECT
+         TOK_SELEXPR
+            TOK_TABLE_OR_COL
+               key
+         TOK_SELEXPR
+            TOK_TABLE_OR_COL
+               value
+         TOK_SELEXPR
+            TOK_TABLE_OR_COL
+               ds1
+         TOK_SELEXPR
+            TOK_TABLE_OR_COL
+               ds2
+      TOK_WHERE
+         or
+            and
+               and
+                  =
+                     TOK_TABLE_OR_COL
+                        ds1
+                     '2000-04-08'
+                  =
+                     TOK_TABLE_OR_COL
+                        ds2
+                     '2001-04-08'
+               =
+                  TOK_TABLE_OR_COL
+                     key
+                  1
+            and
+               and
+                  =
+                     TOK_TABLE_OR_COL
+                        ds1
+                     '2000-04-09'
+                  =
+                     TOK_TABLE_OR_COL
+                        ds2
+                     '2001-04-09'
+               =
+                  TOK_TABLE_OR_COL
+                     key
+                  2
+      TOK_ORDERBY
+         TOK_TABSORTCOLNAMEASC
+            TOK_TABLE_OR_COL
+               key
+         TOK_TABSORTCOLNAMEASC
+            TOK_TABLE_OR_COL
+               value
+         TOK_TABSORTCOLNAMEASC
+            TOK_TABLE_OR_COL
+               ds1
+         TOK_TABSORTCOLNAMEASC
+            TOK_TABLE_OR_COL
+               ds2
+
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: pcr_t1
+            Statistics: Num rows: 40 Data size: 320 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate: (((ds1 = '2000-04-08') and (ds2 = '2001-04-08') and (key = 1)) or ((ds1 = '2000-04-09') and (ds2 = '2001-04-09') and (key = 2))) (type: boolean)
+              Statistics: Num rows: 40 Data size: 320 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: key (type: int), value (type: string), ds1 (type: string), ds2 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 40 Data size: 320 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int), _col1 (type: string), _col2 (type: string), _col3 (type: string)
+                  sort order: ++++
+                  Statistics: Num rows: 40 Data size: 320 Basic stats: COMPLETE Column stats: NONE
+                  tag: -1
+                  auto parallelism: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: ds2=2001-04-08
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds1 2000-04-08
+              ds2 2001-04-08
+            properties:
+              COLUMN_STATS_ACCURATE true
+              bucket_count -1
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.pcr_t1
+              numFiles 1
+              numRows 20
+              partition_columns ds1/ds2
+              partition_columns.types string:string
+              rawDataSize 160
+              serialization.ddl struct pcr_t1 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 180
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count -1
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.pcr_t1
+                partition_columns ds1/ds2
+                partition_columns.types string:string
+                serialization.ddl struct pcr_t1 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.pcr_t1
+            name: default.pcr_t1
+#### A masked pattern was here ####
+          Partition
+            base file name: ds2=2001-04-09
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds1 2000-04-09
+              ds2 2001-04-09
+            properties:
+              COLUMN_STATS_ACCURATE true
+              bucket_count -1
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.pcr_t1
+              numFiles 1
+              numRows 20
+              partition_columns ds1/ds2
+              partition_columns.types string:string
+              rawDataSize 160
+              serialization.ddl struct pcr_t1 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 180
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count -1
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.pcr_t1
+                partition_columns ds1/ds2
+                partition_columns.types string:string
+                serialization.ddl struct pcr_t1 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.pcr_t1
+            name: default.pcr_t1
+      Truncated Path -> Alias:
+        /pcr_t1/ds1=2000-04-08/ds2=2001-04-08 [pcr_t1]
+        /pcr_t1/ds1=2000-04-09/ds2=2001-04-09 [pcr_t1]
+      Needs Tagging: false
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int), KEY.reducesinkkey1 (type: string), KEY.reducesinkkey2 (type: string), KEY.reducesinkkey3 (type: string)
+          outputColumnNames: _col0, _col1, _col2, _col3
+          Statistics: Num rows: 40 Data size: 320 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            GlobalTableId: 0
+#### A masked pattern was here ####
+            NumFilesPerFileSink: 1
+            Statistics: Num rows: 40 Data size: 320 Basic stats: COMPLETE Column stats: NONE
+#### A masked pattern was here ####
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                properties:
+                  columns _col0,_col1,_col2,_col3
+                  columns.types int:string:string:string
+                  escape.delim \
+                  hive.serialization.extend.additional.nesting.levels true
+                  serialization.escape.crlf true
+                  serialization.format 1
+                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            TotalFiles: 1
+            GatherStats: false
+            MultiFileSpray: false
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: explain extended
+select key, value, ds1, ds2
+from pcr_t1
+where (ds1='2000-04-08' and ds2='2001-04-08' and key=1) or (ds1='2000-04-09' and ds2='2001-04-09' and key=2)
+order by key, value, ds1, ds2
+PREHOOK: type: QUERY
+POSTHOOK: query: explain extended
+select key, value, ds1, ds2
+from pcr_t1
+where (ds1='2000-04-08' and ds2='2001-04-08' and key=1) or (ds1='2000-04-09' and ds2='2001-04-09' and key=2)
+order by key, value, ds1, ds2
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  
+TOK_QUERY
+   TOK_FROM
+      TOK_TABREF
+         TOK_TABNAME
+            pcr_t1
+   TOK_INSERT
+      TOK_DESTINATION
+         TOK_DIR
+            TOK_TMP_FILE
+      TOK_SELECT
+         TOK_SELEXPR
+            TOK_TABLE_OR_COL
+               key
+         TOK_SELEXPR
+            TOK_TABLE_OR_COL
+               value
+         TOK_SELEXPR
+            TOK_TABLE_OR_COL
+               ds1
+         TOK_SELEXPR
+            TOK_TABLE_OR_COL
+               ds2
+      TOK_WHERE
+         or
+            and
+               and
+                  =
+                     TOK_TABLE_OR_COL
+                        ds1
+                     '2000-04-08'
+                  =
+                     TOK_TABLE_OR_COL
+                        ds2
+                     '2001-04-08'
+               =
+                  TOK_TABLE_OR_COL
+                     key
+                  1
+            and
+               and
+                  =
+                     TOK_TABLE_OR_COL
+                        ds1
+                     '2000-04-09'
+                  =
+                     TOK_TABLE_OR_COL
+                        ds2
+                     '2001-04-09'
+               =
+                  TOK_TABLE_OR_COL
+                     key
+                  2
+      TOK_ORDERBY
+         TOK_TABSORTCOLNAMEASC
+            TOK_TABLE_OR_COL
+               key
+         TOK_TABSORTCOLNAMEASC
+            TOK_TABLE_OR_COL
+               value
+         TOK_TABSORTCOLNAMEASC
+            TOK_TABLE_OR_COL
+               ds1
+         TOK_TABSORTCOLNAMEASC
+            TOK_TABLE_OR_COL
+               ds2
+
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: pcr_t1
+            Statistics: Num rows: 40 Data size: 320 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate: (struct(ds1,key,ds2)) IN (const struct('2000-04-08',1,'2001-04-08'), const struct('2000-04-09',2,'2001-04-09')) (type: boolean)
+              Statistics: Num rows: 20 Data size: 160 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: key (type: int), value (type: string), ds1 (type: string), ds2 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 20 Data size: 160 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int), _col1 (type: string), _col2 (type: string), _col3 (type: string)
+                  sort order: ++++
+                  Statistics: Num rows: 20 Data size: 160 Basic stats: COMPLETE Column stats: NONE
+                  tag: -1
+                  auto parallelism: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: ds2=2001-04-08
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds1 2000-04-08
+              ds2 2001-04-08
+            properties:
+              COLUMN_STATS_ACCURATE true
+              bucket_count -1
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.pcr_t1
+              numFiles 1
+              numRows 20
+              partition_columns ds1/ds2
+              partition_columns.types string:string
+              rawDataSize 160
+              serialization.ddl struct pcr_t1 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 180
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count -1
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.pcr_t1
+                partition_columns ds1/ds2
+                partition_columns.types string:string
+                serialization.ddl struct pcr_t1 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.pcr_t1
+            name: default.pcr_t1
+#### A masked pattern was here ####
+          Partition
+            base file name: ds2=2001-04-09
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds1 2000-04-09
+              ds2 2001-04-09
+            properties:
+              COLUMN_STATS_ACCURATE true
+              bucket_count -1
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.pcr_t1
+              numFiles 1
+              numRows 20
+              partition_columns ds1/ds2
+              partition_columns.types string:string
+              rawDataSize 160
+              serialization.ddl struct pcr_t1 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 180
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count -1
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.pcr_t1
+                partition_columns ds1/ds2
+                partition_columns.types string:string
+                serialization.ddl struct pcr_t1 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.pcr_t1
+            name: default.pcr_t1
+      Truncated Path -> Alias:
+        /pcr_t1/ds1=2000-04-08/ds2=2001-04-08 [pcr_t1]
+        /pcr_t1/ds1=2000-04-09/ds2=2001-04-09 [pcr_t1]
+      Needs Tagging: false
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int), KEY.reducesinkkey1 (type: string), KEY.reducesinkkey2 (type: string), KEY.reducesinkkey3 (type: string)
+          outputColumnNames: _col0, _col1, _col2, _col3
+          Statistics: Num rows: 20 Data size: 160 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            GlobalTableId: 0
+#### A masked pattern was here ####
+            NumFilesPerFileSink: 1
+            Statistics: Num rows: 20 Data size: 160 Basic stats: COMPLETE Column stats: NONE
+#### A masked pattern was here ####
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                properties:
+                  columns _col0,_col1,_col2,_col3
+                  columns.types int:string:string:string
+                  escape.delim \
+                  hive.serialization.extend.additional.nesting.levels true
+                  serialization.escape.crlf true
+                  serialization.format 1
+                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            TotalFiles: 1
+            GatherStats: false
+            MultiFileSpray: false
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: drop table pcr_t1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@pcr_t1
+PREHOOK: Output: default@pcr_t1
+POSTHOOK: query: drop table pcr_t1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@pcr_t1
+POSTHOOK: Output: default@pcr_t1
