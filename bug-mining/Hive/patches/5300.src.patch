diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java
index 6b89e98083..04e24bd1a5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java
@@ -26,6 +26,7 @@
 import org.apache.commons.lang.StringUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.debug.Utils;
 import org.apache.hadoop.hive.serde2.ByteStream.RandomAccessOutput;
 import org.apache.hadoop.hive.serde2.SerDeException;
@@ -553,6 +554,12 @@ private static void validateCapacity(long capacity) {
     if (capacity <= 0) {
       throw new AssertionError("Invalid capacity " + capacity);
     }
+    if (capacity > Integer.MAX_VALUE) {
+      throw new RuntimeException("Attempting to expand the hash table to " + capacity
+          + " that overflows maximum array size. For this query, you may want to disable "
+          + ConfVars.HIVEDYNAMICPARTITIONHASHJOIN.varname + " or reduce "
+          + ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD.varname);
+    }
   }
 
   /**
@@ -715,8 +722,7 @@ private long getFirstRecordLengthsOffset(long ref, WriteBuffers.Position readPos
   }
 
   private void expandAndRehash() {
-    long capacity = refs.length << 1;
-    expandAndRehashImpl(capacity);
+    expandAndRehashImpl(((long)refs.length) << 1);
   }
 
   private void expandAndRehashImpl(long capacity) {
