diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java
index f6b4124efa..0038f730e6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java
@@ -30,10 +30,10 @@ public class CalciteSemanticException extends SemanticException {
   private static final long serialVersionUID = 1L;
 
   public enum UnsupportedFeature {
-    Distinct_without_an_aggreggation, Duplicates_in_RR, Filter_expression_with_non_boolean_return_type, 
-    Having_clause_without_any_groupby, Hint, Invalid_column_reference, Invalid_decimal, Invalid_interval,
-    Less_than_equal_greater_than, Multi_insert, Others, Same_name_in_multiple_expressions, 
-    Schema_less_table, Select_alias_in_having_clause, Select_transform, Subquery, 
+    Distinct_without_an_aggreggation, Duplicates_in_RR, Filter_expression_with_non_boolean_return_type,
+    Having_clause_without_any_groupby, Hint, Invalid_column_reference, Invalid_decimal,
+    Less_than_equal_greater_than, Multi_insert, Others, Same_name_in_multiple_expressions,
+    Schema_less_table, Select_alias_in_having_clause, Select_transform, Subquery,
     Table_sample_clauses, UDTF, Union_type, Unique_join
   };
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
index cb7bc26003..f8fb4758b3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
@@ -51,6 +51,7 @@
 import org.apache.hadoop.hive.common.type.Decimal128;
 import org.apache.hadoop.hive.common.type.HiveChar;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
@@ -574,22 +575,18 @@ protected RexNode convert(ExprNodeConstantDesc literal) throws CalciteSemanticEx
           new SqlIntervalQualifier(TimeUnit.YEAR, TimeUnit.MONTH, new SqlParserPos(1,1)));
       break;
     case INTERVAL_DAY_TIME:
-      // Calcite RexBuilder L525 divides value by the multiplier.
-      // Need to get CAlCITE-1020 in.
-      throw new CalciteSemanticException("INTERVAL_DAY_TIME is not well supported",
-          UnsupportedFeature.Invalid_interval);
       // Calcite day-time interval is millis value as BigDecimal
       // Seconds converted to millis
-      // BigDecimal secsValueBd = BigDecimal
-      // .valueOf(((HiveIntervalDayTime) value).getTotalSeconds() * 1000);
-      // // Nanos converted to millis
-      // BigDecimal nanosValueBd = BigDecimal.valueOf(((HiveIntervalDayTime)
-      // value).getNanos(), 6);
-      // calciteLiteral =
-      // rexBuilder.makeIntervalLiteral(secsValueBd.add(nanosValueBd),
-      // new SqlIntervalQualifier(TimeUnit.MILLISECOND, null, new
-      // SqlParserPos(1, 1)));
-      // break;
+      BigDecimal secsValueBd = BigDecimal
+       .valueOf(((HiveIntervalDayTime) value).getTotalSeconds() * 1000);
+      // Nanos converted to millis
+       BigDecimal nanosValueBd = BigDecimal.valueOf(((HiveIntervalDayTime)
+       value).getNanos(), 6);
+       calciteLiteral =
+       rexBuilder.makeIntervalLiteral(secsValueBd.add(nanosValueBd),
+       new SqlIntervalQualifier(TimeUnit.MILLISECOND, null, new
+       SqlParserPos(1, 1)));
+       break;
     case VOID:
       calciteLiteral = cluster.getRexBuilder().makeLiteral(null,
           cluster.getTypeFactory().createSqlType(SqlTypeName.NULL), true);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java
index cb8693415c..5c85dceb95 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java
@@ -27,6 +27,7 @@
 import org.apache.calcite.sql.SqlFunctionCategory;
 import org.apache.calcite.sql.SqlKind;
 import org.apache.calcite.sql.SqlOperator;
+import org.apache.calcite.sql.fun.SqlMonotonicBinaryOperator;
 import org.apache.calcite.sql.fun.SqlStdOperatorTable;
 import org.apache.calcite.sql.type.InferTypes;
 import org.apache.calcite.sql.type.OperandTypes;
@@ -61,6 +62,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNegative;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPositive;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
@@ -186,6 +188,10 @@ private static FunctionInfo handleExplicitCast(SqlOperator op, RelDataType dt)
             FunctionRegistry.getFunctionInfo("decimal"));
       } else if (castType.equals(TypeInfoFactory.binaryTypeInfo)) {
         castUDF = FunctionRegistry.getFunctionInfo("binary");
+      } else if (castType.equals(TypeInfoFactory.intervalDayTimeTypeInfo)) {
+        castUDF = FunctionRegistry.getFunctionInfo(serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME);
+      } else if (castType.equals(TypeInfoFactory.intervalYearMonthTypeInfo)) {
+        castUDF = FunctionRegistry.getFunctionInfo(serdeConstants.INTERVAL_YEAR_MONTH_TYPE_NAME);
       } else
         throw new IllegalStateException("Unexpected type : " + castType.getQualifiedName());
     }
@@ -323,7 +329,6 @@ private static class StaticBlockBuilder {
 
     StaticBlockBuilder() {
       registerFunction("+", SqlStdOperatorTable.PLUS, hToken(HiveParser.PLUS, "+"));
-      registerFunction("-", SqlStdOperatorTable.MINUS, hToken(HiveParser.MINUS, "-"));
       registerFunction("*", SqlStdOperatorTable.MULTIPLY, hToken(HiveParser.STAR, "*"));
       registerFunction("/", SqlStdOperatorTable.DIVIDE, hToken(HiveParser.DIVIDE, "/"));
       registerFunction("%", SqlStdOperatorTable.MOD, hToken(HiveParser.Identifier, "%"));
@@ -416,7 +421,7 @@ private static HiveToken hToken(int type, String text) {
 
   // UDAF is assumed to be deterministic
   public static class CalciteUDAF extends SqlAggFunction implements CanAggregateDistinct {
-    private boolean isDistinct;
+    private final boolean isDistinct;
     public CalciteUDAF(boolean isDistinct, String opName, SqlReturnTypeInference returnTypeInference,
         SqlOperandTypeInference operandTypeInference, SqlOperandTypeChecker operandTypeChecker) {
       super(opName, SqlKind.OTHER_FUNCTION, returnTypeInference, operandTypeInference,
@@ -480,9 +485,16 @@ public static SqlOperator getCalciteFn(String hiveUdfName,
     SqlOperator calciteOp = hiveToCalcite.get(hiveUdfName);
     if (calciteOp == null) {
       CalciteUDFInfo uInf = getUDFInfo(hiveUdfName, calciteArgTypes, calciteRetType);
-      calciteOp = new CalciteSqlFn(uInf.udfName, SqlKind.OTHER_FUNCTION, uInf.returnTypeInference,
-          uInf.operandTypeInference, uInf.operandTypeChecker,
-          SqlFunctionCategory.USER_DEFINED_FUNCTION, deterministic);
+      if ("-".equals(hiveUdfName)) {
+        // Calcite native - has broken inference for return type, so we override it with explicit return type
+        // e.g. timestamp - timestamp is inferred as timestamp, where it really should be interval.
+        calciteOp = new SqlMonotonicBinaryOperator("-", SqlKind.MINUS, 40, true,
+            uInf.returnTypeInference, uInf.operandTypeInference, OperandTypes.MINUS_OPERATOR);
+      } else {
+        calciteOp = new CalciteSqlFn(uInf.udfName, SqlKind.OTHER_FUNCTION, uInf.returnTypeInference,
+            uInf.operandTypeInference, uInf.operandTypeChecker,
+            SqlFunctionCategory.USER_DEFINED_FUNCTION, deterministic);
+      }
     }
 
     return calciteOp;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/TypeConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/TypeConverter.java
index a47010d83f..38308c963c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/TypeConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/TypeConverter.java
@@ -76,31 +76,31 @@ public class TypeConverter {
     b.put(SqlTypeName.DATE.getName(), new HiveToken(HiveParser.TOK_DATE, "TOK_DATE"));
     b.put(SqlTypeName.TIMESTAMP.getName(), new HiveToken(HiveParser.TOK_TIMESTAMP, "TOK_TIMESTAMP"));
     b.put(SqlTypeName.INTERVAL_YEAR.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_YEAR_MONTH_LITERAL, "TOK_INTERVAL_YEAR_MONTH_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_YEAR_MONTH_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_MONTH.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_YEAR_MONTH_LITERAL, "TOK_INTERVAL_YEAR_MONTH_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_YEAR_MONTH_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_YEAR_MONTH.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_YEAR_MONTH_LITERAL, "TOK_INTERVAL_YEAR_MONTH_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_YEAR_MONTH_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_DAY.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_DAY_HOUR.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_DAY_MINUTE.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_DAY_SECOND.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_HOUR.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_HOUR_MINUTE.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_HOUR_SECOND.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_MINUTE.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_MINUTE_SECOND.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.INTERVAL_SECOND.getName(),
-            new HiveToken(HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL, "TOK_INTERVAL_DAY_TIME_LITERAL"));
+            new HiveToken(HiveParser.Identifier, serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME));
     b.put(SqlTypeName.BINARY.getName(), new HiveToken(HiveParser.TOK_BINARY, "TOK_BINARY"));
     calciteToHiveTypeNameMap = b.build();
   };
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
index 96dab3b9e5..ace3eaf134 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
@@ -191,6 +191,14 @@ protected static Map<ASTNode, ExprNodeDesc> genExprNode(ASTNode expr,
         + HiveParser.KW_FALSE + "%"), tf.getBoolExprProcessor());
     opRules.put(new RuleRegExp("R5", HiveParser.TOK_DATELITERAL + "%|"
         + HiveParser.TOK_TIMESTAMPLITERAL + "%"), tf.getDateTimeExprProcessor());
+    opRules.put(new RuleRegExp("R6", HiveParser.TOK_INTERVAL_YEAR_MONTH_LITERAL + "%|"
+        + HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL + "%|"
+        + HiveParser.TOK_INTERVAL_YEAR_LITERAL + "%|"
+        + HiveParser.TOK_INTERVAL_MONTH_LITERAL + "%|"
+        + HiveParser.TOK_INTERVAL_DAY_LITERAL + "%|"
+        + HiveParser.TOK_INTERVAL_HOUR_LITERAL + "%|"
+        + HiveParser.TOK_INTERVAL_MINUTE_LITERAL + "%|"
+        + HiveParser.TOK_INTERVAL_SECOND_LITERAL + "%"), tf.getIntervalExprProcessor());
     opRules.put(new RuleRegExp("R7", HiveParser.TOK_TABLE_OR_COL + "%"),
         tf.getColumnExprProcessor());
     opRules.put(new RuleRegExp("R8", HiveParser.TOK_SUBQUERY_OP + "%"),
@@ -493,6 +501,79 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
     }
   }
 
+  /**
+   * Processor for interval constants.
+   */
+  public static class IntervalExprProcessor implements NodeProcessor {
+
+    private static final BigDecimal NANOS_PER_SEC_BD = new BigDecimal(DateUtils.NANOS_PER_SEC);
+    @Override
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+        Object... nodeOutputs) throws SemanticException {
+
+      TypeCheckCtx ctx = (TypeCheckCtx) procCtx;
+      if (ctx.getError() != null) {
+        return null;
+      }
+
+      ExprNodeDesc desc = TypeCheckProcFactory.processGByExpr(nd, procCtx);
+      if (desc != null) {
+        return desc;
+      }
+
+      ASTNode expr = (ASTNode) nd;
+      String intervalString = BaseSemanticAnalyzer.stripQuotes(expr.getText());
+
+      // Get the string value and convert to a Interval value.
+      try {
+        switch (expr.getType()) {
+          case HiveParser.TOK_INTERVAL_YEAR_MONTH_LITERAL:
+            return new ExprNodeConstantDesc(TypeInfoFactory.intervalYearMonthTypeInfo,
+                HiveIntervalYearMonth.valueOf(intervalString));
+          case HiveParser.TOK_INTERVAL_DAY_TIME_LITERAL:
+            return new ExprNodeConstantDesc(TypeInfoFactory.intervalDayTimeTypeInfo,
+                HiveIntervalDayTime.valueOf(intervalString));
+          case HiveParser.TOK_INTERVAL_YEAR_LITERAL:
+            return new ExprNodeConstantDesc(TypeInfoFactory.intervalYearMonthTypeInfo,
+                new HiveIntervalYearMonth(Integer.parseInt(intervalString), 0));
+          case HiveParser.TOK_INTERVAL_MONTH_LITERAL:
+            return new ExprNodeConstantDesc(TypeInfoFactory.intervalYearMonthTypeInfo,
+                new HiveIntervalYearMonth(0, Integer.parseInt(intervalString)));
+          case HiveParser.TOK_INTERVAL_DAY_LITERAL:
+            return new ExprNodeConstantDesc(TypeInfoFactory.intervalDayTimeTypeInfo,
+                new HiveIntervalDayTime(Integer.parseInt(intervalString), 0, 0, 0, 0));
+          case HiveParser.TOK_INTERVAL_HOUR_LITERAL:
+            return new ExprNodeConstantDesc(TypeInfoFactory.intervalDayTimeTypeInfo,
+                new HiveIntervalDayTime(0, Integer.parseInt(intervalString), 0, 0, 0));
+          case HiveParser.TOK_INTERVAL_MINUTE_LITERAL:
+            return new ExprNodeConstantDesc(TypeInfoFactory.intervalDayTimeTypeInfo,
+                new HiveIntervalDayTime(0, 0, Integer.parseInt(intervalString), 0, 0));
+          case HiveParser.TOK_INTERVAL_SECOND_LITERAL:
+            BigDecimal bd = new BigDecimal(intervalString);
+            BigDecimal bdSeconds = new BigDecimal(bd.toBigInteger());
+            BigDecimal bdNanos = bd.subtract(bdSeconds);
+            return new ExprNodeConstantDesc(TypeInfoFactory.intervalDayTimeTypeInfo,
+                new HiveIntervalDayTime(0, 0, 0, bdSeconds.intValueExact(),
+                    bdNanos.multiply(NANOS_PER_SEC_BD).intValue()));
+          default:
+            throw new IllegalArgumentException("Invalid time literal type " + expr.getType());
+        }
+      } catch (Exception err) {
+        throw new SemanticException(
+            "Unable to convert interval literal '" + intervalString + "' to interval value.", err);
+      }
+    }
+  }
+
+  /**
+   * Factory method to get IntervalExprProcessor.
+   *
+   * @return IntervalExprProcessor.
+   */
+  public IntervalExprProcessor getIntervalExprProcessor() {
+    return new IntervalExprProcessor();
+  }
+
   /**
    * Factory method to get DateExprProcessor.
    *
diff --git a/ql/src/test/results/clientpositive/llap/vector_interval_1.q.out b/ql/src/test/results/clientpositive/llap/vector_interval_1.q.out
index 0dd8fc5c5a..995c229772 100644
--- a/ql/src/test/results/clientpositive/llap/vector_interval_1.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_interval_1.q.out
@@ -86,7 +86,7 @@ STAGE PLANS:
             Execution mode: vectorized, llap
             Reduce Operator Tree:
               Select Operator
-                expressions: KEY.reducesinkkey0 (type: string), 1-2 (type: interval_year_month), VALUE._col1 (type: interval_year_month), 1 02:03:04.000000000 (type: interval_day_time), VALUE._col3 (type: interval_day_time)
+                expressions: KEY.reducesinkkey0 (type: string), 1-2 (type: interval_year_month), VALUE._col0 (type: interval_year_month), 1 02:03:04.000000000 (type: interval_day_time), VALUE._col1 (type: interval_day_time)
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 2 Data size: 442 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
@@ -177,7 +177,7 @@ STAGE PLANS:
             Execution mode: vectorized, llap
             Reduce Operator Tree:
               Select Operator
-                expressions: KEY.reducesinkkey0 (type: date), 2-4 (type: interval_year_month), VALUE._col1 (type: interval_year_month), VALUE._col2 (type: interval_year_month), 0-0 (type: interval_year_month), VALUE._col4 (type: interval_year_month), VALUE._col5 (type: interval_year_month)
+                expressions: KEY.reducesinkkey0 (type: date), 2-4 (type: interval_year_month), VALUE._col0 (type: interval_year_month), VALUE._col1 (type: interval_year_month), 0-0 (type: interval_year_month), VALUE._col2 (type: interval_year_month), VALUE._col3 (type: interval_year_month)
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                 Statistics: Num rows: 2 Data size: 442 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
diff --git a/ql/src/test/results/clientpositive/llap/vector_interval_arithmetic.q.out b/ql/src/test/results/clientpositive/llap/vector_interval_arithmetic.q.out
index 8e3bc3913e..464fd2a09d 100644
--- a/ql/src/test/results/clientpositive/llap/vector_interval_arithmetic.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_interval_arithmetic.q.out
@@ -137,7 +137,7 @@ order by dateval
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@interval_arithmetic_1
 #### A masked pattern was here ####
-dateval	_c1	_c2	_c3	_c4	_c5	_c6
+dateval	c1	c2	c3	c4	c5	c6
 0004-09-22	0002-07-22	0006-11-22	0006-11-22	0002-07-22	0002-07-22	0006-11-22
 0528-10-27	0526-08-27	0530-12-27	0530-12-27	0526-08-27	0526-08-27	0530-12-27
 1319-02-02	1316-12-02	1321-04-02	1321-04-02	1316-12-02	1316-12-02	1321-04-02
@@ -427,7 +427,7 @@ order by tsval
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@interval_arithmetic_1
 #### A masked pattern was here ####
-tsval	_c1	_c2	_c3	_c4	_c5	_c6
+tsval	c1	c2	c3	c4	c5	c6
 0004-09-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0006-11-22 18:26:29.519542222	0006-11-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0006-11-22 18:26:29.519542222
 0528-10-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0530-12-27 08:15:18.941718273	0530-12-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0530-12-27 08:15:18.941718273
 1319-02-02 16:31:57.778	1316-12-02 16:31:57.778	1321-04-02 16:31:57.778	1321-04-02 16:31:57.778	1316-12-02 16:31:57.778	1316-12-02 16:31:57.778	1321-04-02 16:31:57.778
@@ -515,7 +515,7 @@ STAGE PLANS:
                   Select Operator
                     Statistics: Num rows: 50 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
                     Reduce Output Operator
-                      key expressions: 5-5 (type: interval_year_month)
+                      key expressions: CAST( 5-5 AS INTERVAL YEAR TO MONTH) (type: interval_year_month)
                       sort order: +
                       Statistics: Num rows: 50 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
                       TopN Hash Memory Usage: 0.1
@@ -525,7 +525,7 @@ STAGE PLANS:
             Execution mode: vectorized, llap
             Reduce Operator Tree:
               Select Operator
-                expressions: KEY.reducesinkkey0 (type: interval_year_month), -1-1 (type: interval_year_month)
+                expressions: 5-5 (type: interval_year_month), -1-1 (type: interval_year_month)
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 50 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
                 Limit
@@ -563,7 +563,7 @@ limit 2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@interval_arithmetic_1
 #### A masked pattern was here ####
-_c0	_c1
+c0	c1
 5-5	-1-1
 5-5	-1-1
 PREHOOK: query: -- interval day-time arithmetic
diff --git a/ql/src/test/results/clientpositive/vector_interval_1.q.out b/ql/src/test/results/clientpositive/vector_interval_1.q.out
index f18e56cc21..11eae55d2a 100644
--- a/ql/src/test/results/clientpositive/vector_interval_1.q.out
+++ b/ql/src/test/results/clientpositive/vector_interval_1.q.out
@@ -77,7 +77,7 @@ STAGE PLANS:
       Execution mode: vectorized
       Reduce Operator Tree:
         Select Operator
-          expressions: KEY.reducesinkkey0 (type: string), 1-2 (type: interval_year_month), VALUE._col1 (type: interval_year_month), 1 02:03:04.000000000 (type: interval_day_time), VALUE._col3 (type: interval_day_time)
+          expressions: KEY.reducesinkkey0 (type: string), 1-2 (type: interval_year_month), VALUE._col0 (type: interval_year_month), 1 02:03:04.000000000 (type: interval_day_time), VALUE._col1 (type: interval_day_time)
           outputColumnNames: _col0, _col1, _col2, _col3, _col4
           Statistics: Num rows: 2 Data size: 442 Basic stats: COMPLETE Column stats: NONE
           File Output Operator
@@ -159,7 +159,7 @@ STAGE PLANS:
       Execution mode: vectorized
       Reduce Operator Tree:
         Select Operator
-          expressions: KEY.reducesinkkey0 (type: date), 2-4 (type: interval_year_month), VALUE._col1 (type: interval_year_month), VALUE._col2 (type: interval_year_month), 0-0 (type: interval_year_month), VALUE._col4 (type: interval_year_month), VALUE._col5 (type: interval_year_month)
+          expressions: KEY.reducesinkkey0 (type: date), 2-4 (type: interval_year_month), VALUE._col0 (type: interval_year_month), VALUE._col1 (type: interval_year_month), 0-0 (type: interval_year_month), VALUE._col2 (type: interval_year_month), VALUE._col3 (type: interval_year_month)
           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
           Statistics: Num rows: 2 Data size: 442 Basic stats: COMPLETE Column stats: NONE
           File Output Operator
diff --git a/ql/src/test/results/clientpositive/vector_interval_arithmetic.q.out b/ql/src/test/results/clientpositive/vector_interval_arithmetic.q.out
index 5d1dca6cd4..da1ebc9f32 100644
--- a/ql/src/test/results/clientpositive/vector_interval_arithmetic.q.out
+++ b/ql/src/test/results/clientpositive/vector_interval_arithmetic.q.out
@@ -128,7 +128,7 @@ order by dateval
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@interval_arithmetic_1
 #### A masked pattern was here ####
-dateval	_c1	_c2	_c3	_c4	_c5	_c6
+dateval	c1	c2	c3	c4	c5	c6
 0004-09-22	0002-07-22	0006-11-22	0006-11-22	0002-07-22	0002-07-22	0006-11-22
 0528-10-27	0526-08-27	0530-12-27	0530-12-27	0526-08-27	0526-08-27	0530-12-27
 1319-02-02	1316-12-02	1321-04-02	1321-04-02	1316-12-02	1316-12-02	1321-04-02
@@ -400,7 +400,7 @@ order by tsval
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@interval_arithmetic_1
 #### A masked pattern was here ####
-tsval	_c1	_c2	_c3	_c4	_c5	_c6
+tsval	c1	c2	c3	c4	c5	c6
 0004-09-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0006-11-22 18:26:29.519542222	0006-11-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0006-11-22 18:26:29.519542222
 0528-10-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0530-12-27 08:15:18.941718273	0530-12-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0530-12-27 08:15:18.941718273
 1319-02-02 16:31:57.778	1316-12-02 16:31:57.778	1321-04-02 16:31:57.778	1321-04-02 16:31:57.778	1316-12-02 16:31:57.778	1316-12-02 16:31:57.778	1321-04-02 16:31:57.778
@@ -482,14 +482,14 @@ STAGE PLANS:
             Select Operator
               Statistics: Num rows: 50 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
               Reduce Output Operator
-                key expressions: 5-5 (type: interval_year_month)
+                key expressions: CAST( 5-5 AS INTERVAL YEAR TO MONTH) (type: interval_year_month)
                 sort order: +
                 Statistics: Num rows: 50 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
                 TopN Hash Memory Usage: 0.1
       Execution mode: vectorized
       Reduce Operator Tree:
         Select Operator
-          expressions: KEY.reducesinkkey0 (type: interval_year_month), -1-1 (type: interval_year_month)
+          expressions: 5-5 (type: interval_year_month), -1-1 (type: interval_year_month)
           outputColumnNames: _col0, _col1
           Statistics: Num rows: 50 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
           Limit
@@ -527,7 +527,7 @@ limit 2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@interval_arithmetic_1
 #### A masked pattern was here ####
-_c0	_c1
+c0	c1
 5-5	-1-1
 5-5	-1-1
 PREHOOK: query: -- interval day-time arithmetic
