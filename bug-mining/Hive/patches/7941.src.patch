diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
index f51259ee53..2bb0e0c0cf 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
@@ -21,16 +21,20 @@
 
 import java.io.Serializable;
 import java.util.Collection;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.Properties;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.metastore.HiveMetaHook;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
 import org.apache.hadoop.hive.ql.metadata.HiveStoragePredicateHandler;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
+import org.apache.hadoop.hive.ql.stats.Partish;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.mapred.InputFormat;
@@ -38,6 +42,7 @@
 import org.apache.hadoop.mapred.OutputFormat;
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.SchemaParser;
+import org.apache.iceberg.SnapshotSummary;
 import org.apache.iceberg.Table;
 import org.apache.iceberg.mr.Catalogs;
 import org.apache.iceberg.mr.InputFormatConfig;
@@ -157,6 +162,38 @@ public DecomposedPredicate decomposePredicate(JobConf jobConf, Deserializer dese
     return predicate;
   }
 
+  @Override
+  public boolean canProvideBasicStatistics() {
+    return true;
+  }
+
+  @Override
+  public Map<String, String> getBasicStatistics(Partish partish) {
+    org.apache.hadoop.hive.ql.metadata.Table hmsTable = partish.getTable();
+    TableDesc tableDesc = Utilities.getTableDesc(hmsTable);
+    Table table = Catalogs.loadTable(conf, tableDesc.getProperties());
+    Map<String, String> stats = new HashMap<>();
+    if (table.currentSnapshot() != null) {
+      Map<String, String> summary = table.currentSnapshot().summary();
+      if (summary != null) {
+        if (summary.containsKey(SnapshotSummary.TOTAL_DATA_FILES_PROP)) {
+          stats.put(StatsSetupConst.NUM_FILES, summary.get(SnapshotSummary.TOTAL_DATA_FILES_PROP));
+        }
+        if (summary.containsKey(SnapshotSummary.TOTAL_RECORDS_PROP)) {
+          stats.put(StatsSetupConst.ROW_COUNT, summary.get(SnapshotSummary.TOTAL_RECORDS_PROP));
+        }
+        if (summary.containsKey(SnapshotSummary.TOTAL_FILE_SIZE_PROP)) {
+          stats.put(StatsSetupConst.TOTAL_SIZE, summary.get(SnapshotSummary.TOTAL_FILE_SIZE_PROP));
+        }
+      }
+    } else {
+      stats.put(StatsSetupConst.NUM_FILES, "0");
+      stats.put(StatsSetupConst.ROW_COUNT, "0");
+      stats.put(StatsSetupConst.TOTAL_SIZE, "0");
+    }
+    return stats;
+  }
+
   /**
    * Returns the Table serialized to the configuration based on the table name.
    * @param config The configuration used to get the data from
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
index e0f819dbfb..98257df716 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
@@ -23,14 +23,17 @@
 import java.math.BigDecimal;
 import java.util.ArrayList;
 import java.util.Collection;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
 import java.util.stream.Collectors;
+import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.ql.exec.mr.ExecMapper;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.PartitionSpec;
 import org.apache.iceberg.Schema;
+import org.apache.iceberg.SnapshotSummary;
 import org.apache.iceberg.Table;
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.apache.iceberg.data.GenericRecord;
@@ -38,9 +41,11 @@
 import org.apache.iceberg.hive.HiveSchemaUtil;
 import org.apache.iceberg.mr.TestHelper;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.Types;
+import org.apache.thrift.TException;
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.Assert;
@@ -96,6 +101,12 @@ public class TestHiveIcebergStorageHandlerWithEngine {
                   Types.DecimalType.of(3, 1), Types.UUIDType.get(), Types.FixedType.ofLength(5),
                   Types.TimeType.get());
 
+  private static final Map<String, String> STATS_MAPPING = ImmutableMap.of(
+      StatsSetupConst.NUM_FILES, SnapshotSummary.TOTAL_DATA_FILES_PROP,
+      StatsSetupConst.ROW_COUNT, SnapshotSummary.TOTAL_RECORDS_PROP,
+      StatsSetupConst.TOTAL_SIZE, SnapshotSummary.TOTAL_FILE_SIZE_PROP
+  );
+
   @Parameters(name = "fileFormat={0}, engine={1}, catalog={2}")
   public static Collection<Object[]> parameters() {
     Collection<Object[]> testParams = new ArrayList<>();
@@ -182,6 +193,37 @@ public void testScanTable() throws IOException {
     Assert.assertArrayEquals(new Object[] {"Alice", 0L}, descRows.get(2));
   }
 
+  @Test
+  public void testAnalyzeTableComputeStatistics() throws IOException, TException, InterruptedException {
+    String dbName = "default";
+    String tableName = "customers";
+    Table table = testTables
+        .createTable(shell, tableName, HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, fileFormat,
+            HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS);
+    shell.executeStatement("ANALYZE TABLE " + dbName + "." + tableName + " COMPUTE STATISTICS");
+    validateBasicStats(table, dbName, tableName);
+  }
+
+  @Test
+  public void testAnalyzeTableComputeStatisticsForColumns() throws IOException, TException, InterruptedException {
+    String dbName = "default";
+    String tableName = "orders";
+    Table table = testTables.createTable(shell, tableName, ORDER_SCHEMA, fileFormat, ORDER_RECORDS);
+    shell.executeStatement("ANALYZE TABLE " + dbName + "." + tableName + " COMPUTE STATISTICS FOR COLUMNS");
+    validateBasicStats(table, dbName, tableName);
+  }
+
+  @Test
+  public void testAnalyzeTableComputeStatisticsEmptyTable() throws IOException, TException, InterruptedException {
+    String dbName = "default";
+    String tableName = "customers";
+    Table table = testTables
+        .createTable(shell, tableName, HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, fileFormat,
+            new ArrayList<>());
+    shell.executeStatement("ANALYZE TABLE " + dbName + "." + tableName + " COMPUTE STATISTICS");
+    validateBasicStats(table, dbName, tableName);
+  }
+
   @Test
   public void testCBOWithSelectedColumnsNonOverlapJoin() throws IOException {
     shell.setHiveSessionValue("hive.cbo.enable", true);
@@ -898,4 +940,21 @@ private StringBuilder buildComplexTypeInnerQuery(Object field, Type type) {
     }
     return query;
   }
+
+  private void validateBasicStats(Table icebergTable, String dbName, String tableName)
+      throws TException, InterruptedException {
+    Map<String, String> hmsParams = shell.metastore().getTable(dbName, tableName).getParameters();
+    Map<String, String> summary = new HashMap<>();
+    if (icebergTable.currentSnapshot() == null) {
+      for (String key : STATS_MAPPING.values()) {
+        summary.put(key, "0");
+      }
+    } else {
+      summary = icebergTable.currentSnapshot().summary();
+    }
+
+    for (Map.Entry<String, String> entry : STATS_MAPPING.entrySet()) {
+      Assert.assertEquals(summary.get(entry.getValue()), hmsParams.get(entry.getKey()));
+    }
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java
index 88a8ba9725..11e12060f1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
+import org.apache.hadoop.hive.ql.stats.Partish;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
@@ -197,4 +198,22 @@ default boolean addDynamicSplitPruningEdge(ExprNodeDesc syntheticFilterPredicate
   default Map<String, String> getOperatorDescProperties(OperatorDesc operatorDesc, Map<String, String> initialProps) {
     return initialProps;
   }
+
+  /**
+   * Return some basic statistics (numRows, numFiles, totalSize) calculated by the underlying storage handler
+   * implementation.
+   * @param partish a partish wrapper class
+   * @return map of basic statistics, can be null
+   */
+  default Map<String, String> getBasicStatistics(Partish partish) {
+    return null;
+  }
+
+  /**
+   * Check if the storage handler can provide basic statistics.
+   * @return true if the storage handler can supply the basic statistics
+   */
+  default boolean canProvideBasicStatistics() {
+    return false;
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
index 5f060ec8a0..6c79007175 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
@@ -85,7 +85,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
 
         if (parseCtx.getQueryProperties().isAnalyzeCommand()) {
           boolean noScan = parseCtx.getQueryProperties().isNoScanAnalyzeCommand();
-          if (BasicStatsNoJobTask.canUseFooterScan(table, inputFormat)) {
+          if (BasicStatsNoJobTask.canUseBasicStats(table, inputFormat)) {
             // For ORC and Parquet, all the following statements are the same
             // ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS
             // ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS noscan;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
index ab28d7846f..5079fba22a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
@@ -85,8 +85,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,
 
       assert alias != null;
       TezWork tezWork = context.currentTask.getWork();
-      if (BasicStatsNoJobTask.canUseFooterScan(table, inputFormat)) {
-        // For ORC & Parquet, all the following statements are the same
+      if (BasicStatsNoJobTask.canUseBasicStats(table, inputFormat)) {
+        // For ORC, Parquet and Iceberg tables, all the following statements are the same
         // ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS
         // ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS noscan;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java
index 4a836bca33..843808bf20 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java
@@ -94,8 +94,8 @@ public Object process(Node nd, Stack<Node> stack,
       Preconditions.checkArgument(alias != null, "AssertionError: expected alias to be not null");
 
       SparkWork sparkWork = context.currentTask.getWork();
-      if (BasicStatsNoJobTask.canUseFooterScan(table, inputFormat)) {
-        // For ORC & Parquet, all the following statements are the same
+      if (BasicStatsNoJobTask.canUseBasicStats(table, inputFormat)) {
+        // For ORC, Parquet and Iceberg tables, all the following statements are the same
         // ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS
         // ANALYZE TABLE T [PARTITION (...)] COMPUTE STATISTICS noscan;
         // There will not be any Spark job above this task
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java
index c6533cf293..919a243710 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsNoJobTask.java
@@ -19,12 +19,13 @@
 package org.apache.hadoop.hive.ql.stats;
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Collection;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
 
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -90,12 +91,21 @@ public BasicStatsNoJobTask(HiveConf conf, BasicStatsNoJobWork work) {
     console = new LogHelper(LOG);
   }
 
-  public static boolean canUseFooterScan(
+  public static boolean canUseBasicStats(
       Table table, Class<? extends InputFormat> inputFormat) {
+      return canUseFooterScan(table, inputFormat) || useBasicStatsFromStorageHandler(table);
+  }
+
+  public static boolean canUseFooterScan(Table table, Class<? extends InputFormat> inputFormat) {
     return (OrcInputFormat.class.isAssignableFrom(inputFormat) && !AcidUtils.isFullAcidTable(table))
         || MapredParquetInputFormat.class.isAssignableFrom(inputFormat);
   }
 
+  private static boolean useBasicStatsFromStorageHandler(Table table) {
+    return table.isNonNative() && table.getStorageHandler().canProvideBasicStatistics();
+  }
+
+
   @Override
   public void initialize(CompilationOpContext opContext) {
 
@@ -119,16 +129,64 @@ public String getName() {
     return "STATS-NO-JOB";
   }
 
-  static class StatItem {
-    Partish partish;
-    Map<String, String> params;
-    Object result;
+  abstract static class StatCollector implements Runnable {
+
+    protected Partish partish;
+    protected Object result;
+    protected LogHelper console;
+
+    public static Function<StatCollector, String> SIMPLE_NAME_FUNCTION =
+        sc -> String.format("%s#%s", sc.partish.getTable().getCompleteName(), sc.partish.getPartishType());
+
+    public static Function<StatCollector, Partition> EXTRACT_RESULT_FUNCTION = sc -> (Partition) sc.result;
+
+    protected void init(HiveConf conf, LogHelper console) throws IOException {
+      this.console = console;
+    }
+
+    protected final boolean isValid() {
+      return result != null;
+    }
+
+    protected final String toString(Map<String, String> parameters) {
+      return StatsSetupConst.SUPPORTED_STATS.stream().map(st -> st + "=" + parameters.get(st))
+          .collect(Collectors.joining(", "));
+    }
+  }
+
+  static class HiveStorageHandlerStatCollector extends StatCollector {
+
+    public HiveStorageHandlerStatCollector(Partish partish) {
+      this.partish = partish;
+    }
+
+    @Override
+    public void run() {
+      try {
+        Table table = partish.getTable();
+        Map<String, String> parameters;
+        Map<String, String> basicStatistics = table.getStorageHandler().getBasicStatistics(partish);
+        if (partish.getPartition() != null) {
+          parameters = partish.getPartParameters();
+          result = new Partition(table, partish.getPartition().getTPartition());
+        } else {
+          parameters = table.getParameters();
+          result = new Table(table.getTTable());
+        }
+        parameters.putAll(basicStatistics);
+        StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.TRUE);
+        String msg = partish.getSimpleName() + " stats: [" + toString(parameters) + ']';
+        LOG.debug(msg);
+        console.printInfo(msg);
+      } catch (Exception e) {
+        console.printInfo("[Warning] could not update stats for " + partish.getSimpleName() + ".",
+            "Failed with exception " + e.getMessage() + "\n" + StringUtils.stringifyException(e));
+      }
+    }
   }
 
-  static class FooterStatCollector implements Runnable {
+  static class FooterStatCollector extends StatCollector {
 
-    private Partish partish;
-    private Object result;
     private JobConf jc;
     private Path dir;
     private FileSystem fs;
@@ -139,24 +197,7 @@ public FooterStatCollector(JobConf jc, Partish partish) {
       this.partish = partish;
     }
 
-    public static final Function<FooterStatCollector, String> SIMPLE_NAME_FUNCTION = new Function<FooterStatCollector, String>() {
-
-      @Override
-      public String apply(FooterStatCollector sc) {
-        return String.format("%s#%s", sc.partish.getTable().getCompleteName(), sc.partish.getPartishType());
-      }
-    };
-    private static final Function<FooterStatCollector, Partition> EXTRACT_RESULT_FUNCTION = new Function<FooterStatCollector, Partition>() {
-      @Override
-      public Partition apply(FooterStatCollector input) {
-        return (Partition) input.result;
-      }
-    };
-
-    private boolean isValid() {
-      return result != null;
-    }
-
+    @Override
     public void init(HiveConf conf, LogHelper console) throws IOException {
       this.console = console;
       dir = new Path(partish.getPartSd().getLocation());
@@ -236,22 +277,34 @@ public void run() {
       }
     }
 
-    private String toString(Map<String, String> parameters) {
-      StringBuilder builder = new StringBuilder();
-      for (String statType : StatsSetupConst.SUPPORTED_STATS) {
-        String value = parameters.get(statType);
-        if (value != null) {
-          if (builder.length() > 0) {
-            builder.append(", ");
-          }
-          builder.append(statType).append('=').append(value);
-        }
+  }
+
+  private Collection<Partition> getPartitions(Table table) {
+    Collection<Partition> partitions = null;
+    if (work.getPartitions() == null || work.getPartitions().isEmpty()) {
+      if (table.isPartitioned()) {
+        partitions = table.getTableSpec().partitions;
       }
-      return builder.toString();
+    } else {
+      partitions = work.getPartitions();
     }
+    return partitions;
+  }
 
+  private List<Partish> getPartishes(Table table) {
+    Collection<Partition> partitions = getPartitions(table);
+    List<Partish> partishes = Lists.newLinkedList();
+    if (partitions == null) {
+      partishes.add(Partish.buildFor(table));
+    } else {
+      for (Partition part : partitions) {
+        partishes.add(Partish.buildFor(table, part));
+      }
+    }
+    return partishes;
   }
 
+
   private int aggregateStats(ExecutorService threadPool, Hive db) {
     int ret = 0;
     try {
@@ -265,30 +318,18 @@ private int aggregateStats(ExecutorService threadPool, Hive db) {
 
       Table table = tableSpecs.tableHandle;
 
-      Collection<Partition> partitions = null;
-      if (work.getPartitions() == null || work.getPartitions().isEmpty()) {
-        if (table.isPartitioned()) {
-          partitions = tableSpecs.partitions;
-        }
-      } else {
-        partitions = work.getPartitions();
-      }
+      List<Partish> partishes = getPartishes(table);
 
-      LinkedList<Partish> partishes = Lists.newLinkedList();
-      if (partitions == null) {
-        partishes.add(Partish.buildFor(table));
-      } else {
-        for (Partition part : partitions) {
-          partishes.add(Partish.buildFor(table, part));
-        }
-      }
-
-      List<FooterStatCollector> scs = Lists.newArrayList();
+      List<StatCollector> scs = new ArrayList();
       for (Partish partish : partishes) {
-        scs.add(new FooterStatCollector(jc, partish));
+        if (useBasicStatsFromStorageHandler(table)) {
+          scs.add(new HiveStorageHandlerStatCollector(partish));
+        } else {
+          scs.add(new FooterStatCollector(jc, partish));
+        }
       }
 
-      for (FooterStatCollector sc : scs) {
+      for (StatCollector sc : scs) {
         sc.init(conf, console);
         threadPool.execute(sc);
       }
@@ -312,7 +353,7 @@ private int aggregateStats(ExecutorService threadPool, Hive db) {
     return ret;
   }
 
-  private int updatePartitions(Hive db, List<FooterStatCollector> scs, Table table) throws InvalidOperationException, HiveException {
+  private int updatePartitions(Hive db, List<StatCollector> scs, Table table) throws InvalidOperationException, HiveException {
 
     String tableFullName = table.getFullyQualifiedName();
 
@@ -320,15 +361,15 @@ private int updatePartitions(Hive db, List<FooterStatCollector> scs, Table table
       return 0;
     }
     if (work.isStatsReliable()) {
-      for (FooterStatCollector statsCollection : scs) {
-        if (statsCollection.result == null) {
+      for (StatCollector statsCollection : scs) {
+        if (!statsCollection.isValid()) {
           LOG.debug("Stats requested to be reliable. Empty stats found: {}", statsCollection.partish.getSimpleName());
           return -1;
         }
       }
     }
-    List<FooterStatCollector> validColectors = Lists.newArrayList();
-    for (FooterStatCollector statsCollection : scs) {
+    List<StatCollector> validColectors = Lists.newArrayList();
+    for (StatCollector statsCollection : scs) {
       if (statsCollection.isValid()) {
         validColectors.add(statsCollection);
       }
@@ -337,7 +378,7 @@ private int updatePartitions(Hive db, List<FooterStatCollector> scs, Table table
     EnvironmentContext environmentContext = new EnvironmentContext();
     environmentContext.putToProperties(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);
 
-    ImmutableListMultimap<String, FooterStatCollector> collectorsByTable = Multimaps.index(validColectors, FooterStatCollector.SIMPLE_NAME_FUNCTION);
+    ImmutableListMultimap<String, StatCollector> collectorsByTable = Multimaps.index(validColectors, FooterStatCollector.SIMPLE_NAME_FUNCTION);
 
     LOG.debug("Collectors.size(): {}", collectorsByTable.keySet());
 
@@ -351,7 +392,7 @@ private int updatePartitions(Hive db, List<FooterStatCollector> scs, Table table
     LOG.debug("Updating stats for: {}", tableFullName);
 
     for (String partName : collectorsByTable.keySet()) {
-      ImmutableList<FooterStatCollector> values = collectorsByTable.get(partName);
+      ImmutableList<StatCollector> values = collectorsByTable.get(partName);
 
       if (values == null) {
         throw new RuntimeException("very intresting");
@@ -362,7 +403,7 @@ private int updatePartitions(Hive db, List<FooterStatCollector> scs, Table table
         LOG.debug("Updated stats for {}.", tableFullName);
       } else {
         if (values.get(0).result instanceof Partition) {
-          List<Partition> results = Lists.transform(values, FooterStatCollector.EXTRACT_RESULT_FUNCTION);
+          List<Partition> results = Lists.transform(values, StatCollector.EXTRACT_RESULT_FUNCTION);
           db.alterPartitions(tableFullName, results, environmentContext, true);
           LOG.debug("Bulk updated {} partitions of {}.", results.size(), tableFullName);
         } else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java
index 5b72430a0c..966bde7099 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java
@@ -53,6 +53,7 @@
 import org.apache.hadoop.hive.ql.plan.BasicStatsWork;
 import org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx;
 import org.apache.hadoop.hive.ql.plan.LoadTableDesc;
+import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
 import org.apache.hadoop.util.StringUtils;
@@ -118,11 +119,16 @@ private static class BasicStatsProcessor {
     private boolean isMissingAcidState = false;
     private BasicStatsWork work;
     private boolean followedColStats1;
+    private Map<String, String> providedBasicStats;
 
     public BasicStatsProcessor(Partish partish, BasicStatsWork work, HiveConf conf, boolean followedColStats2) {
       this.partish = partish;
       this.work = work;
       followedColStats1 = followedColStats2;
+      Table table = partish.getTable();
+      if (table.isNonNative() && table.getStorageHandler().canProvideBasicStatistics()) {
+        providedBasicStats = table.getStorageHandler().getBasicStatistics(partish);
+      }
     }
 
     public Object process(StatsAggregator statsAggregator) throws HiveException, MetaException {
@@ -140,7 +146,7 @@ public Object process(StatsAggregator statsAggregator) throws HiveException, Met
         StatsSetupConst.clearColumnStatsState(parameters);
       }
 
-      if (partfileStatus == null) {
+      if (partfileStatus == null && providedBasicStats == null) {
         // This may happen if ACID state is absent from config.
         String spec =  partish.getPartition() == null ? partish.getTable().getTableName()
             :  partish.getPartition().getSpec().toString();
@@ -160,27 +166,33 @@ public Object process(StatsAggregator statsAggregator) throws HiveException, Met
         StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);
       }
 
-      MetaStoreServerUtils.populateQuickStats(partfileStatus, parameters);
+      if (providedBasicStats == null) {
+        MetaStoreServerUtils.populateQuickStats(partfileStatus, parameters);
 
-      if (statsAggregator != null) {
-        // Update stats for transactional tables (MM, or full ACID with overwrite), even
-        // though we are marking stats as not being accurate.
-        if (StatsSetupConst.areBasicStatsUptoDate(parameters) || p.isTransactionalTable()) {
-          String prefix = getAggregationPrefix(p.getTable(), p.getPartition());
-          updateStats(statsAggregator, parameters, prefix);
+        if (statsAggregator != null) {
+          // Update stats for transactional tables (MM, or full ACID with overwrite), even
+          // though we are marking stats as not being accurate.
+          if (StatsSetupConst.areBasicStatsUptoDate(parameters) || p.isTransactionalTable()) {
+            String prefix = getAggregationPrefix(p.getTable(), p.getPartition());
+            updateStats(statsAggregator, parameters, prefix);
+          }
         }
+      } else {
+        parameters.putAll(providedBasicStats);
       }
 
       return p.getOutput();
     }
 
     public void collectFileStatus(Warehouse wh, HiveConf conf) throws MetaException, IOException {
-      if (!partish.isTransactionalTable()) {
-        partfileStatus = wh.getFileStatusesForSD(partish.getPartSd());
-      } else {
-        Path path = new Path(partish.getPartSd().getLocation());
-        partfileStatus = AcidUtils.getAcidFilesForStats(partish.getTable(), path, conf, null);
-        isMissingAcidState = true;
+      if (providedBasicStats == null) {
+        if (!partish.isTransactionalTable()) {
+          partfileStatus = wh.getFileStatusesForSD(partish.getPartSd());
+        } else {
+          Path path = new Path(partish.getPartSd().getLocation());
+          partfileStatus = AcidUtils.getAcidFilesForStats(partish.getTable(), path, conf, null);
+          isMissingAcidState = true;
+        }
       }
     }
 
