diff --git a/iceberg/iceberg-handler/src/test/results/positive/col_stats.q.out b/iceberg/iceberg-handler/src/test/results/positive/col_stats.q.out
index c8d5eab358..8c905528e7 100644
--- a/iceberg/iceberg-handler/src/test/results/positive/col_stats.q.out
+++ b/iceberg/iceberg-handler/src/test/results/positive/col_stats.q.out
@@ -244,14 +244,14 @@ Stage-0
     Stage-1
       Reducer 2 vectorized
       File Output Operator [FS_8]
-        Select Operator [SEL_7] (rows=9 width=192)
+        Select Operator [SEL_7] (rows=9 width=95)
           Output:["_col0","_col1","_col2"]
         <-Map 1 [SIMPLE_EDGE] vectorized
           SHUFFLE [RS_6]
-            Select Operator [SEL_5] (rows=9 width=192)
+            Select Operator [SEL_5] (rows=9 width=95)
               Output:["_col0","_col1","_col2"]
-              TableScan [TS_0] (rows=9 width=192)
-                default@tbl_ice_puffin,tbl_ice_puffin,Tbl:COMPLETE,Col:NONE,Output:["a","b","c"]
+              TableScan [TS_0] (rows=9 width=95)
+                default@tbl_ice_puffin,tbl_ice_puffin,Tbl:COMPLETE,Col:COMPLETE,Output:["a","b","c"]
 
 PREHOOK: query: drop table if exists tbl_ice_puffin
 PREHOOK: type: DROPTABLE
@@ -339,16 +339,17 @@ POSTHOOK: type: DESCTABLE
 POSTHOOK: Input: default@tbl_ice_puffin
 col_name            	a                   
 data_type           	int                 
-min                 	                    
-max                 	                    
-num_nulls           	                    
-distinct_count      	                    
+min                 	1                   
+max                 	333                 
+num_nulls           	0                   
+distinct_count      	7                   
 avg_col_len         	                    
 max_col_len         	                    
 num_trues           	                    
 num_falses          	                    
-bit_vector          	                    
+bit_vector          	HL                  
 comment             	                    
+COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"a\":\"true\",\"b\":\"true\",\"c\":\"true\"}}
 PREHOOK: query: drop table if exists tbl_ice
 PREHOOK: type: DROPTABLE
 POSTHOOK: query: drop table if exists tbl_ice
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/CreateTableDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/CreateTableDesc.java
index 8e2ca07b38..652e51c808 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/CreateTableDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/CreateTableDesc.java
@@ -26,7 +26,6 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.stream.Collectors;
 
 import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.fs.Path;
@@ -937,7 +936,9 @@ public Table toTable(HiveConf conf) throws HiveException {
             tbl.getTTable().getDictionary() : new ObjectDictionary();
         List<ByteBuffer> buffers = new ArrayList<>();
         String statsSetup = StatsSetupConst.ColumnStatsSetup.getStatsSetupAsString(true,
-            storageHandler != null && storageHandler.isMetadataTableSupported() ? "metadata" : null, // Skip metadata directory for Iceberg table
+            // Ignore all Iceberg leftover files when storageHandler.isMetadataTableSupported() is true,
+            // as the method is only enabled in Iceberg currently.
+            storageHandler != null && storageHandler.isMetadataTableSupported(),
             MetaStoreUtils.getColumnNames(tbl.getCols()));
         buffers.add(ByteBuffer.wrap(statsSetup.getBytes(StandardCharsets.UTF_8)));
         dictionary.putToValues(StatsSetupConst.STATS_FOR_CREATE_TABLE, buffers);
diff --git a/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/common/StatsSetupConst.java b/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/common/StatsSetupConst.java
index 7ca76bf374..a04b646c60 100644
--- a/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/common/StatsSetupConst.java
+++ b/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/common/StatsSetupConst.java
@@ -236,7 +236,7 @@ public static class ColumnStatsSetup {
     @JsonInclude(JsonInclude.Include.NON_DEFAULT)
     public boolean enabled;
     @JsonInclude(JsonInclude.Include.NON_DEFAULT)
-    public String fileToEscape;
+    public boolean isIcebergTable;
     @JsonInclude(JsonInclude.Include.NON_EMPTY)
     public List<String> columnNames = new ArrayList<>();
 
@@ -255,13 +255,13 @@ public static ColumnStatsSetup parseStatsSetup(String statsSetup) {
      * Get json representation of the ColumnStatsSetup
      */
     public static String getStatsSetupAsString(boolean enabled,
-        String fileToEscape,
+        boolean isIcebergTable,
         List<String> columns) {
       try {
         ColumnStatsSetup statsSetup = new ColumnStatsSetup();
         statsSetup.enabled = enabled;
+        statsSetup.isIcebergTable = isIcebergTable;
         statsSetup.columnNames = new ArrayList<>(columns);
-        statsSetup.fileToEscape = fileToEscape;
         return objectWriter.writeValueAsString(statsSetup);
       } catch (Exception e) {
         // this should not happen
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java
index 9ef97f0c57..9c63bd068b 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreServerUtils.java
@@ -35,7 +35,6 @@
 import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
@@ -66,7 +65,6 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.common.TableName;
 import org.apache.hadoop.hive.metastore.ColumnType;
@@ -530,16 +528,10 @@ public static void updateTableStatsForCreateTable(Warehouse wh, Database db, Tab
         StatsSetupConst.ColumnStatsSetup statsSetup = StatsSetupConst.ColumnStatsSetup.parseStatsSetup(val);
         if (statsSetup.enabled) {
           try {
-            PathFilter pathFilter = FileUtils.HIDDEN_FILES_PATH_FILTER;
-            if (StringUtils.isNotEmpty(statsSetup.fileToEscape)) {
-              final Set<String> filesToEscape = new HashSet<>();
-              for (String fileName : statsSetup.fileToEscape.split(",")) {
-                filesToEscape.add(fileName.trim());
-              }
-              pathFilter = p -> !filesToEscape.contains(p.getName());
-            }
+            // For an Iceberg table, a new snapshot is generated, so any leftover files would be ignored
             // Set the column stats true in order to make it merge-able
-            if (newDir || wh.isEmptyDir(tblPath, pathFilter)) {
+            if (newDir || statsSetup.isIcebergTable ||
+                wh.isEmptyDir(tblPath, FileUtils.HIDDEN_FILES_PATH_FILTER)) {
               List<String> columns = statsSetup.columnNames;
               if (columns == null || columns.isEmpty()) {
                 columns = getColumnNames(tbl.getSd().getCols());
@@ -547,7 +539,7 @@ public static void updateTableStatsForCreateTable(Warehouse wh, Database db, Tab
               StatsSetupConst.setStatsStateForCreateTable(tbl.getParameters(), columns, StatsSetupConst.TRUE);
             }
           } catch (IOException e) {
-            LOG.error("Error while checking the table directory: " + tblPath + " is empty or not", e);
+            LOG.error("Error while checking the table directory: " + tblPath, e);
             throw ExceptionHandler.newMetaException(e);
           }
         }
