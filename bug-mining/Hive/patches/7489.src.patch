diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
index 8593480724..cb6bd2d70a 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
@@ -153,12 +153,12 @@ private void prepareNonAcidData(String primaryDbName) throws Throwable {
     nonAcidTableNames.add("t4");
   }
 
-  WarehouseInstance.Tuple prepareDataAndDump(String primaryDbName, String fromReplId,
+  WarehouseInstance.Tuple prepareDataAndDump(String primaryDbName,
                                                      List<String> withClause) throws Throwable {
     prepareAcidData(primaryDbName);
     prepareNonAcidData(primaryDbName);
     return primary.run("use " + primaryDbName)
-            .dump(primaryDbName, fromReplId, withClause != null ?
+            .dump(primaryDbName, withClause != null ?
                     withClause : Collections.emptyList());
   }
 
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/ReplicationTestUtils.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/ReplicationTestUtils.java
index c552bae56d..936acc45fe 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/ReplicationTestUtils.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/ReplicationTestUtils.java
@@ -20,7 +20,6 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.parse.WarehouseInstance;
 import org.apache.hadoop.hive.ql.parse.repl.PathBuilder;
 import org.junit.Assert;
 
@@ -28,10 +27,10 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.InputStreamReader;
-import java.util.Arrays;
-import java.util.HashSet;
 import java.util.List;
+import java.util.Arrays;
 import java.util.Set;
+import java.util.HashSet;
 
 /**
  * ReplicationTestUtils - static helper functions for replication test
@@ -265,7 +264,7 @@ public static WarehouseInstance.Tuple verifyIncrementalLoad(WarehouseInstance pr
                                                               String primaryDbName, String replicatedDbName,
                                                               List<String> selectStmtList,
                                                   List<String[]> expectedValues, String lastReplId) throws Throwable {
-    WarehouseInstance.Tuple incrementalDump = primary.dump(primaryDbName, lastReplId);
+    WarehouseInstance.Tuple incrementalDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrementalDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName).verifyResult(incrementalDump.lastReplicationId);
     verifyResultsInReplica(replica, replicatedDbName, selectStmtList, expectedValues);
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestCopyUtils.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestCopyUtils.java
index de81acb598..77a0d33b52 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestCopyUtils.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestCopyUtils.java
@@ -126,7 +126,7 @@ public void testPrivilegedDistCpWithSameUserAsCurrentDoesNotTryToImpersonate() t
         .run("create table t1 (id int)")
         .run("insert into t1 values (1),(2),(3)")
         .run("insert into t1 values (11),(12),(13)")
-        .dump(primaryDbName, null);
+        .dump(primaryDbName);
 
     /*
       We have to do a comparision on the data of table t1 in replicated database because even though the file
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestMetaStoreEventListenerInRepl.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestMetaStoreEventListenerInRepl.java
index 76832a4526..7b0f634a0b 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestMetaStoreEventListenerInRepl.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestMetaStoreEventListenerInRepl.java
@@ -173,7 +173,7 @@ Map<String, Set<String>> prepareInc2Data(String dbName) throws Throwable {
   public void testReplEvents() throws Throwable {
     Map<String, Set<String>> eventsMap = prepareBootstrapData(primaryDbName);
     WarehouseInstance.Tuple bootstrapDump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, bootstrapDump.dumpLocation);
     ReplMetaStoreEventListenerTestImpl.checkEventSanity(eventsMap, replicatedDbName);
     ReplMetaStoreEventListenerTestImpl.clearSanityData();
@@ -181,7 +181,7 @@ public void testReplEvents() throws Throwable {
     eventsMap = prepareIncData(primaryDbName);
     LOG.info(testName.getMethodName() + ": first incremental dump and load.");
     WarehouseInstance.Tuple incDump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, bootstrapDump.lastReplicationId);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, incDump.dumpLocation);
     ReplMetaStoreEventListenerTestImpl.checkEventSanity(eventsMap, replicatedDbName);
     ReplMetaStoreEventListenerTestImpl.clearSanityData();
@@ -190,7 +190,7 @@ public void testReplEvents() throws Throwable {
     eventsMap = prepareInc2Data(primaryDbName);
     LOG.info(testName.getMethodName() + ": second incremental dump and load.");
     WarehouseInstance.Tuple inc2Dump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, incDump.lastReplicationId);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, inc2Dump.dumpLocation);
     ReplMetaStoreEventListenerTestImpl.checkEventSanity(eventsMap, replicatedDbName);
     ReplMetaStoreEventListenerTestImpl.clearSanityData();
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOfHiveStreaming.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOfHiveStreaming.java
index 72f6644040..5c8f902b26 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOfHiveStreaming.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOfHiveStreaming.java
@@ -116,7 +116,7 @@ public void tearDown() throws Throwable {
 
   @Test
   public void testHiveStreamingUnpartitionedWithTxnBatchSizeAsOne() throws Throwable {
-    WarehouseInstance.Tuple bootstrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootstrapDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, bootstrapDump.dumpLocation);
 
     // Create an ACID table.
@@ -148,7 +148,7 @@ public void testHiveStreamingUnpartitionedWithTxnBatchSizeAsOne() throws Throwab
     connection.commitTransaction();
 
     // Replicate the committed data which should be visible.
-    WarehouseInstance.Tuple incrDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    WarehouseInstance.Tuple incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " order by msg")
@@ -160,7 +160,7 @@ public void testHiveStreamingUnpartitionedWithTxnBatchSizeAsOne() throws Throwab
     connection.write("4,val4".getBytes());
 
     // Replicate events before committing txn. The uncommitted data shouldn't be seen.
-    incrDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " order by msg")
@@ -169,7 +169,7 @@ public void testHiveStreamingUnpartitionedWithTxnBatchSizeAsOne() throws Throwab
     connection.commitTransaction();
 
     // After commit, the data should be replicated and visible.
-    incrDump = primary.dump(primaryDbName, incrDump.lastReplicationId);
+    incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " order by msg")
@@ -182,7 +182,7 @@ public void testHiveStreamingUnpartitionedWithTxnBatchSizeAsOne() throws Throwab
     connection.abortTransaction();
 
     // Aborted data shouldn't be visible.
-    incrDump = primary.dump(primaryDbName, incrDump.lastReplicationId);
+    incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " order by msg")
@@ -194,7 +194,7 @@ public void testHiveStreamingUnpartitionedWithTxnBatchSizeAsOne() throws Throwab
 
   @Test
   public void testHiveStreamingStaticPartitionWithTxnBatchSizeAsOne() throws Throwable {
-    WarehouseInstance.Tuple bootstrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootstrapDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, bootstrapDump.dumpLocation);
 
     // Create an ACID table.
@@ -233,7 +233,7 @@ public void testHiveStreamingStaticPartitionWithTxnBatchSizeAsOne() throws Throw
     connection.commitTransaction();
 
     // Replicate the committed data which should be visible.
-    WarehouseInstance.Tuple incrDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    WarehouseInstance.Tuple incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " where continent='Asia' and country='India' order by msg")
@@ -245,7 +245,7 @@ public void testHiveStreamingStaticPartitionWithTxnBatchSizeAsOne() throws Throw
     connection.write("4,val4".getBytes());
 
     // Replicate events before committing txn. The uncommitted data shouldn't be seen.
-    incrDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " where continent='Asia' and country='India' order by msg")
@@ -254,7 +254,7 @@ public void testHiveStreamingStaticPartitionWithTxnBatchSizeAsOne() throws Throw
     connection.commitTransaction();
 
     // After commit, the data should be replicated and visible.
-    incrDump = primary.dump(primaryDbName, incrDump.lastReplicationId);
+    incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " where continent='Asia' and country='India' order by msg")
@@ -267,7 +267,7 @@ public void testHiveStreamingStaticPartitionWithTxnBatchSizeAsOne() throws Throw
     connection.abortTransaction();
 
     // Aborted data shouldn't be visible.
-    incrDump = primary.dump(primaryDbName, incrDump.lastReplicationId);
+    incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " where continent='Asia' and country='India' order by msg")
@@ -279,7 +279,7 @@ public void testHiveStreamingStaticPartitionWithTxnBatchSizeAsOne() throws Throw
 
   @Test
   public void testHiveStreamingDynamicPartitionWithTxnBatchSizeAsOne() throws Throwable {
-    WarehouseInstance.Tuple bootstrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootstrapDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, bootstrapDump.dumpLocation);
 
     // Create an ACID table.
@@ -315,7 +315,7 @@ public void testHiveStreamingDynamicPartitionWithTxnBatchSizeAsOne() throws Thro
     connection.commitTransaction();
 
     // Replicate the committed data which should be visible.
-    WarehouseInstance.Tuple incrDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    WarehouseInstance.Tuple incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " where continent='Asia' and country='China' order by msg")
@@ -329,7 +329,7 @@ public void testHiveStreamingDynamicPartitionWithTxnBatchSizeAsOne() throws Thro
     connection.write("14,val14,Asia,India".getBytes());
 
     // Replicate events before committing txn. The uncommitted data shouldn't be seen.
-    incrDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " where continent='Asia' and country='India' order by msg")
@@ -338,7 +338,7 @@ public void testHiveStreamingDynamicPartitionWithTxnBatchSizeAsOne() throws Thro
     connection.commitTransaction();
 
     // After committing the txn, the data should be visible.
-    incrDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " where continent='Asia' and country='India' order by msg")
@@ -353,7 +353,7 @@ public void testHiveStreamingDynamicPartitionWithTxnBatchSizeAsOne() throws Thro
     connection.abortTransaction();
 
     // Aborted data should not be visible.
-    incrDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    incrDump = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, incrDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("select msg from " + tblName + " where continent='Asia' and country='India' order by msg")
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOnHDFSEncryptedZones.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOnHDFSEncryptedZones.java
index 602394a10e..0a69d63563 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOnHDFSEncryptedZones.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOnHDFSEncryptedZones.java
@@ -106,7 +106,7 @@ public void targetAndSourceHaveDifferentEncryptionZoneKeys() throws Throwable {
             .run("create table encrypted_table (id int, value string)")
             .run("insert into table encrypted_table values (1,'value1')")
             .run("insert into table encrypted_table values (2,'value2')")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     replica
         .run("repl load " + replicatedDbName + " from '" + tuple.dumpLocation
@@ -137,7 +137,7 @@ public void targetAndSourceHaveSameEncryptionZoneKeys() throws Throwable {
             .run("create table encrypted_table (id int, value string)")
             .run("insert into table encrypted_table values (1,'value1')")
             .run("insert into table encrypted_table values (2,'value2')")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     replica
         .run("repl load " + replicatedDbName + " from '" + tuple.dumpLocation
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
index 39d876802a..003533a3e3 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
@@ -248,45 +248,18 @@ static class Tuple {
   }
 
   private Tuple bootstrapLoadAndVerify(String dbName, String replDbName) throws IOException {
-    return incrementalLoadAndVerify(dbName, null, replDbName);
+    return incrementalLoadAndVerify(dbName, replDbName);
   }
 
-  private Tuple incrementalLoadAndVerify(String dbName, String fromReplId, String replDbName) throws IOException {
-    Tuple dump = replDumpDb(dbName, fromReplId, null, null);
+  private Tuple incrementalLoadAndVerify(String dbName, String replDbName) throws IOException {
+    Tuple dump = replDumpDb(dbName);
     loadAndVerify(replDbName, dump.dumpLocation, dump.lastReplId);
     return dump;
   }
 
-  private Tuple incrementalLoadAndVerify(String dbName, String fromReplId, String toReplId, String replDbName)
-          throws IOException {
-    Tuple dump = replDumpDb(dbName, fromReplId, toReplId, null);
-    loadAndVerify(replDbName, dump.dumpLocation, dump.lastReplId);
-    return dump;
-  }
-
-  private Tuple incrementalLoadAndVerify(String dbName, String fromReplId, String toReplId, String limit,
-                                         String replDbName) throws IOException {
-    Tuple dump = replDumpDb(dbName, fromReplId, toReplId, limit);
-    loadAndVerify(replDbName, dump.dumpLocation, dump.lastReplId);
-    return dump;
-  }
-
-  private Tuple dumpDbFromLastDump(String dbName, Tuple lastDump) throws IOException {
-    return replDumpDb(dbName, lastDump.lastReplId, null, null);
-  }
-
-  private Tuple replDumpDb(String dbName, String fromReplID, String toReplID, String limit) throws IOException {
+  private Tuple replDumpDb(String dbName) throws IOException {
     advanceDumpDir();
     String dumpCmd = "REPL DUMP " + dbName;
-    if (null != fromReplID) {
-      dumpCmd = dumpCmd + " FROM " + fromReplID;
-    }
-    if (null != toReplID) {
-      dumpCmd = dumpCmd + " TO " + toReplID;
-    }
-    if (null != limit) {
-      dumpCmd = dumpCmd + " LIMIT " + limit;
-    }
     run(dumpCmd, driver);
     String dumpLocation = getResult(0, 0, driver);
     String lastReplId = getResult(0, 1, true, driver);
@@ -413,7 +386,7 @@ public void testTaskCreationOptimization() throws Throwable {
     run("create table " + dbName + ".t2 (place string) partitioned by (country string)", driver);
     run("insert into table " + dbName + ".t2 partition(country='india') values ('bangalore')", driver);
 
-    Tuple dump = replDumpDb(dbName, null, null, null);
+    Tuple dump = replDumpDb(dbName);
 
     //bootstrap load should not have move task
     Task task = getReplLoadRootTask(dbNameReplica, false, dump);
@@ -423,7 +396,7 @@ public void testTaskCreationOptimization() throws Throwable {
     loadAndVerify(dbNameReplica, dump.dumpLocation, dump.lastReplId);
 
     run("insert into table " + dbName + ".t2 partition(country='india') values ('delhi')", driver);
-    dump = replDumpDb(dbName, dump.lastReplId, null, null);
+    dump = replDumpDb(dbName);
 
     // Partition level statistics gets updated as part of the INSERT above. So we see a partition
     // task corresponding to an ALTER_PARTITION event.
@@ -434,7 +407,7 @@ public void testTaskCreationOptimization() throws Throwable {
     loadAndVerify(dbNameReplica, dump.dumpLocation, dump.lastReplId);
 
     run("insert into table " + dbName + ".t2 partition(country='us') values ('sf')", driver);
-    dump = replDumpDb(dbName, dump.lastReplId, null, null);
+    dump = replDumpDb(dbName);
 
     //no move task should be added as the operation is adding a dynamic partition
     task = getReplLoadRootTask(dbNameReplica, true, dump);
@@ -604,7 +577,7 @@ public Table apply(@Nullable Table table) {
     run("DROP TABLE " + dbName + ".ptned", driver);
 
     advanceDumpDir();
-    run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);
+    run("REPL DUMP " + dbName, driver);
     String postDropReplDumpLocn = getResult(0,0, driver);
     String postDropReplDumpId = getResult(0,1,true,driver);
     LOG.info("Dumped to {} with id {}->{}", postDropReplDumpLocn, replDumpId, postDropReplDumpId);
@@ -673,7 +646,7 @@ public List<String> apply(@Nullable List<String> partitions) {
     run("ALTER TABLE " + dbName + ".ptned DROP PARTITION (b=2)", driver);
 
     advanceDumpDir();
-    run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);
+    run("REPL DUMP " + dbName, driver);
     String postDropReplDumpLocn = getResult(0,0,driver);
     String postDropReplDumpId = getResult(0,1,true,driver);
     LOG.info("Dumped to {} with id {}->{}", postDropReplDumpLocn, replDumpId, postDropReplDumpId);
@@ -810,7 +783,7 @@ public void run() {
       InjectableBehaviourObjectStore.resetGetTableBehaviour(); // reset the behaviour
     }
 
-    incrementalLoadAndVerify(dbName, bootstrap.lastReplId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyIfTableNotExist(replDbName, "ptned", metaStoreClientMirror);
   }
 
@@ -863,7 +836,7 @@ public void testIncrementalAdds() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned_late WHERE b=2", ptn_data_2, driver);
 
     // Perform REPL-DUMP/LOAD
-    incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
 
     // VERIFY tables and partitions on destination for equivalence.
     verifyRun("SELECT * from " + replDbName + ".unptned_empty", empty, driverMirror);
@@ -897,7 +870,7 @@ public void testIncrementalLoadWithVariableLengthEventId() throws IOException, T
     run("TRUNCATE TABLE " + dbName + ".unptned", driver);
     run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[0] + "')", driver);
 
-    Tuple incrementalDump = replDumpDb(dbName, replDumpId, null, null);
+    Tuple incrementalDump = replDumpDb(dbName);
     String incrementalDumpLocn = incrementalDump.dumpLocation;
     replDumpId = incrementalDump.lastReplId;
 
@@ -985,7 +958,7 @@ public NotificationEventResponse apply(@Nullable NotificationEventResponse event
     try {
       advanceDumpDir();
       try {
-        driver.run("REPL DUMP " + dbName + " FROM " + replDumpId);
+        driver.run("REPL DUMP " + dbName);
         assert false;
       } catch (CommandProcessorException e) {
         assertTrue(e.getResponseCode() == ErrorMsg.REPL_EVENTS_MISSING_IN_METASTORE.getErrorCode());
@@ -1057,7 +1030,7 @@ public void testDrops() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned3", ptn_data_2, driver);
 
     // replicate the incremental drops
-    incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
 
     // verify that drops were replicated. This can either be from tables or ptns
     // not existing, and thus, throwing a NoSuchObjectException, or returning nulls
@@ -1127,7 +1100,7 @@ public void testDropsWithCM() throws IOException {
     run("DROP TABLE " + dbName + ".ptned2", driver);
 
     advanceDumpDir();
-    run("REPL DUMP " + dbName + " FROM " + replDumpId, driver);
+    run("REPL DUMP " + dbName, driver);
     String postDropReplDumpLocn = getResult(0,0,driver);
     String postDropReplDumpId = getResult(0,1,true,driver);
     LOG.info("Dumped to {} with id {}->{}", postDropReplDumpLocn, replDumpId, postDropReplDumpId);
@@ -1261,7 +1234,7 @@ public void testTableAlters() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned2_rn WHERE b=2", ptn_data_2, driver);
 
     // All alters done, now we replicate them over.
-    incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
 
     // Replication done, we now do the following verifications:
 
@@ -1332,7 +1305,7 @@ public void testDatabaseAlters() throws IOException {
     run("ALTER DATABASE " + dbName + " SET DBPROPERTIES ('" + testKey + "' = '" + testVal + "')", driver);
 
     // All alters done, now we replicate them over.
-    Tuple incremental = incrementalLoadAndVerify(dbName, bootstrap.lastReplId, replDbName);
+    Tuple incremental = incrementalLoadAndVerify(dbName, replDbName);
 
     // Replication done, we need to check if the new property is added
     try {
@@ -1348,7 +1321,7 @@ public void testDatabaseAlters() throws IOException {
     run("ALTER DATABASE " + dbName + " SET DBPROPERTIES ('" + testKey + "' = '" + newValue + "')", driver);
     run("ALTER DATABASE " + dbName + " SET OWNER ROLE " + newOwnerName, driver);
 
-    incremental = incrementalLoadAndVerify(dbName, incremental.lastReplId, replDbName);
+    incremental = incrementalLoadAndVerify(dbName, replDbName);
 
     // Replication done, we need to check if new value is set for existing property
     try {
@@ -1375,7 +1348,6 @@ public void testIncrementalLoad() throws IOException {
         + ".ptned_empty(a string) partitioned by (b int) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     String[] unptn_data = new String[] { "eleven", "twelve" };
     String[] ptn_data_1 = new String[] { "thirteen", "fourteen", "fifteen" };
@@ -1399,8 +1371,7 @@ public void testIncrementalLoad() throws IOException {
     run("INSERT INTO TABLE " + dbName + ".unptned_late SELECT * FROM " + dbName + ".unptned", driver);
     verifySetup("SELECT * from " + dbName + ".unptned_late", unptn_data, driver);
 
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
 
     verifyRun("SELECT * from " + replDbName + ".unptned_late", unptn_data, driverMirror);
 
@@ -1422,7 +1393,7 @@ public void testIncrementalLoad() throws IOException {
         + ".ptned WHERE b=2", driver);
     verifySetup("SELECT a from " + dbName + ".ptned_late WHERE b=2", ptn_data_2, driver);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".ptned_late WHERE b=1", ptn_data_1, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned_late WHERE b=2", ptn_data_2, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned WHERE b=1", ptn_data_1, driverMirror);
@@ -1437,7 +1408,6 @@ public void testIncrementalInserts() throws IOException {
     run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     String[] unptn_data = new String[] { "eleven", "twelve" };
 
@@ -1449,8 +1419,7 @@ public void testIncrementalInserts() throws IOException {
     run("INSERT INTO TABLE " + dbName + ".unptned_late SELECT * FROM " + dbName + ".unptned", driver);
     verifySetup("SELECT * from " + dbName + ".unptned_late ORDER BY a", unptn_data, driver);
 
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
 
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".unptned_late ORDER BY a", unptn_data, driverMirror);
@@ -1462,7 +1431,7 @@ public void testIncrementalInserts() throws IOException {
     run("INSERT OVERWRITE TABLE " + dbName + ".unptned values('" + data_after_ovwrite[0] + "')", driver);
     verifySetup("SELECT a from " + dbName + ".unptned", data_after_ovwrite, driver);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".unptned_late ORDER BY a", unptn_data_after_ins, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".unptned", data_after_ovwrite, driverMirror);
   }
@@ -1508,7 +1477,7 @@ public NotificationEventResponse apply(@Nullable NotificationEventResponse event
     };
     InjectableBehaviourObjectStore.setGetNextNotificationBehaviour(eventTypeValidator);
     try {
-      incrementalLoadAndVerify(dbName, bootstrap.lastReplId, replDbName);
+      incrementalLoadAndVerify(dbName, replDbName);
       eventTypeValidator.assertInjectionsPerformed(true,false);
     } finally {
       InjectableBehaviourObjectStore.resetGetNextNotificationBehaviour(); // reset the behaviour
@@ -1568,7 +1537,7 @@ public NotificationEventResponse apply(@Nullable NotificationEventResponse event
     };
     InjectableBehaviourObjectStore.setGetNextNotificationBehaviour(insertEventRepeater);
     try {
-      incrementalLoadAndVerify(dbName, bootstrap.lastReplId, replDbName);
+      incrementalLoadAndVerify(dbName, replDbName);
       insertEventRepeater.assertInjectionsPerformed(true,false);
     } finally {
       InjectableBehaviourObjectStore.resetGetNextNotificationBehaviour(); // reset the behaviour
@@ -1593,7 +1562,6 @@ public void testIncrementalInsertToPartition() throws IOException {
     run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     String[] ptn_data_1 = new String[] { "fifteen", "fourteen", "thirteen" };
     String[] ptn_data_2 = new String[] { "fifteen", "seventeen", "sixteen" };
@@ -1609,8 +1577,7 @@ public void testIncrementalInsertToPartition() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned where (b=1) ORDER BY a", ptn_data_1, driver);
     verifySetup("SELECT a from " + dbName + ".ptned where (b=2) ORDER BY a", ptn_data_2, driver);
 
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
 
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=2) ORDER BY a", ptn_data_2, driverMirror);
@@ -1623,7 +1590,7 @@ public void testIncrementalInsertToPartition() throws IOException {
     run("INSERT OVERWRITE TABLE " + dbName + ".ptned partition(b=3) values('" + data_after_ovwrite[0] + "')", driver);
     verifySetup("SELECT a from " + dbName + ".ptned where (b=3)", data_after_ovwrite, driver);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
 
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=2)", data_after_ovwrite, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=3)", data_after_ovwrite, driverMirror);
@@ -1658,7 +1625,6 @@ public void testInsertToMultiKeyPartition() throws IOException {
                               "location", "namelist/year=1980/month=4/day=1", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     verifyRun("SELECT name from " + replDbName + ".namelist where (year=1980) ORDER BY name", ptn_year_1980, driverMirror);
     verifyRun("SELECT name from " + replDbName + ".namelist where (day=1) ORDER BY name", ptn_day_1, driverMirror);
@@ -1690,8 +1656,7 @@ public void testInsertToMultiKeyPartition() throws IOException {
     verifyRunWithPatternMatch("SHOW TABLE EXTENDED LIKE namelist PARTITION (year=1990,month=5,day=25)",
             "location", "namelist/year=1990/month=5/day=25", driver);
 
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
 
     verifyRun("SELECT name from " + replDbName + ".namelist where (year=1980) ORDER BY name", ptn_year_1980, driverMirror);
     verifyRun("SELECT name from " + replDbName + ".namelist where (day=1) ORDER BY name", ptn_day_1_2, driverMirror);
@@ -1711,7 +1676,7 @@ public void testInsertToMultiKeyPartition() throws IOException {
     verifySetup("SELECT name from " + dbName + ".namelist where (year=1990 and month=5 and day=25)", data_after_ovwrite, driver);
     verifySetup("SELECT name from " + dbName + ".namelist ORDER BY name", ptn_data_3, driver);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifySetup("SELECT name from " + replDbName + ".namelist where (year=1990 and month=5 and day=25)", data_after_ovwrite, driverMirror);
     verifySetup("SELECT name from " + replDbName + ".namelist ORDER BY name", ptn_data_3, driverMirror);
   }
@@ -1724,7 +1689,6 @@ public void testIncrementalInsertDropUnpartitionedTable() throws IOException {
     run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     String[] unptn_data = new String[] { "eleven", "twelve" };
 
@@ -1736,8 +1700,8 @@ public void testIncrementalInsertDropUnpartitionedTable() throws IOException {
     verifySetup("SELECT a from " + dbName + ".unptned_tmp ORDER BY a", unptn_data, driver);
 
     // Get the last repl ID corresponding to all insert/alter/create events except DROP.
-    Tuple incrementalDump = replDumpDb(dbName, replDumpId, null, null);
-    String lastDumpIdWithoutDrop = incrementalDump.lastReplId;
+    Tuple incrementalDump = replDumpDb(dbName);
+
 
     // Drop all the tables
     run("DROP TABLE " + dbName + ".unptned", driver);
@@ -1746,15 +1710,14 @@ public void testIncrementalInsertDropUnpartitionedTable() throws IOException {
     verifyFail("SELECT * FROM " + dbName + ".unptned_tmp", driver);
 
     // Dump all the events except DROP
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, lastDumpIdWithoutDrop, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    loadAndVerify(replDbName, incrementalDump.dumpLocation, incrementalDump.lastReplId);
 
     // Need to find the tables and data as drop is not part of this dump
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".unptned_tmp ORDER BY a", unptn_data, driverMirror);
 
     // Dump the drop events and check if tables are getting dropped in target as well
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyFail("SELECT * FROM " + replDbName + ".unptned", driverMirror);
     verifyFail("SELECT * FROM " + replDbName + ".unptned_tmp", driverMirror);
   }
@@ -1767,7 +1730,6 @@ public void testIncrementalInsertDropPartitionedTable() throws IOException {
     run("CREATE TABLE " + dbName + ".ptned(a string) PARTITIONED BY (b int) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     String[] ptn_data_1 = new String[] { "fifteen", "fourteen", "thirteen" };
     String[] ptn_data_2 = new String[] { "fifteen", "seventeen", "sixteen" };
@@ -1789,8 +1751,7 @@ public void testIncrementalInsertDropPartitionedTable() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned_tmp where (b=2) ORDER BY a", ptn_data_2, driver);
 
     // Get the last repl ID corresponding to all insert/alter/create events except DROP.
-    Tuple incrementalDump = replDumpDb(dbName, replDumpId, null, null);
-    String lastDumpIdWithoutDrop = incrementalDump.lastReplId;
+    Tuple incrementalDump = replDumpDb(dbName);
 
     // Drop all the tables
     run("DROP TABLE " + dbName + ".ptned_tmp", driver);
@@ -1799,8 +1760,7 @@ public void testIncrementalInsertDropPartitionedTable() throws IOException {
     verifyFail("SELECT * FROM " + dbName + ".ptned", driver);
 
     // Replicate all the events except DROP
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, lastDumpIdWithoutDrop, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    loadAndVerify(replDbName, incrementalDump.dumpLocation, incrementalDump.lastReplId);
 
     // Need to find the tables and data as drop is not part of this dump
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);
@@ -1809,7 +1769,7 @@ public void testIncrementalInsertDropPartitionedTable() throws IOException {
     verifyRun("SELECT a from " + replDbName + ".ptned_tmp where (b=2) ORDER BY a", ptn_data_2, driverMirror);
 
     // Replicate the drop events and check if tables are getting dropped in target as well
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyFail("SELECT * FROM " + replDbName + ".ptned_tmp", driverMirror);
     verifyFail("SELECT * FROM " + replDbName + ".ptned", driverMirror);
   }
@@ -1822,27 +1782,24 @@ public void testInsertOverwriteOnUnpartitionedTableWithCM() throws IOException {
     run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     // After INSERT INTO operation, get the last Repl ID
     String[] unptn_data = new String[] { "thirteen" };
     run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[0] + "')", driver);
-    Tuple incrementalDump = replDumpDb(dbName, replDumpId, null, null);
-    String insertDumpId = incrementalDump.lastReplId;
+    Tuple incrementalDump = replDumpDb(dbName);
 
     // Insert overwrite on unpartitioned table
     String[] data_after_ovwrite = new String[] { "hundred" };
     run("INSERT OVERWRITE TABLE " + dbName + ".unptned values('" + data_after_ovwrite[0] + "')", driver);
 
     // Replicate only one INSERT INTO operation on the table.
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, insertDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    loadAndVerify(replDbName, incrementalDump.dumpLocation, incrementalDump.lastReplId);
 
     // After Load from this dump, all target tables/partitions will have initial set of data but source will have latest data.
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data, driverMirror);
 
     // Replicate the remaining INSERT OVERWRITE operations on the table.
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
 
     // After load, shall see the overwritten data.
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", data_after_ovwrite, driverMirror);
@@ -1856,7 +1813,6 @@ public void testInsertOverwriteOnPartitionedTableWithCM() throws IOException {
     run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     // INSERT INTO 2 partitions and get the last repl ID
     String[] ptn_data_1 = new String[] { "fourteen" };
@@ -1865,8 +1821,7 @@ public void testInsertOverwriteOnPartitionedTableWithCM() throws IOException {
     run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[0] + "')", driver);
     run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[1] + "')", driver);
 
-    Tuple incrementalDump = replDumpDb(dbName, replDumpId, null, null);
-    String insertDumpId = incrementalDump.lastReplId;
+    Tuple incrementalDump = replDumpDb(dbName);
 
     // Insert overwrite on one partition with multiple files
     String[] data_after_ovwrite = new String[] { "hundred" };
@@ -1874,15 +1829,15 @@ public void testInsertOverwriteOnPartitionedTableWithCM() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned where (b=2)", data_after_ovwrite, driver);
 
     // Replicate only 2 INSERT INTO operations.
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, insertDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    loadAndVerify(replDbName, incrementalDump.dumpLocation, incrementalDump.lastReplId);
+    incrementalDump = replDumpDb(dbName);
 
     // After Load from this dump, all target tables/partitions will have initial set of data.
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=2) ORDER BY a", ptn_data_2, driverMirror);
 
     // Replicate the remaining INSERT OVERWRITE operation on the table.
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    loadAndVerify(replDbName, incrementalDump.dumpLocation, incrementalDump.lastReplId);
 
     // After load, shall see the overwritten data.
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);
@@ -1907,13 +1862,13 @@ public void testDropPartitionEventWithPartitionOnTimestampColumn() throws IOExce
     run("INSERT INTO TABLE " + dbName + ".ptned PARTITION(b=\"" + ptnVal +"\") values('" + ptn_data[0] + "')", driver);
 
     // Replicate insert event and verify
-    Tuple incrDump = incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
+    Tuple incrDump = incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=\"" + ptnVal + "\") ORDER BY a", ptn_data, driverMirror);
 
     run("ALTER TABLE " + dbName + ".ptned DROP PARTITION(b=\"" + ptnVal + "\")", driver);
 
     // Replicate drop partition event and verify
-    incrementalLoadAndVerify(dbName, incrDump.lastReplId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyIfPartitionNotExist(replDbName, "ptned", new ArrayList<>(Arrays.asList(ptnVal)), metaStoreClientMirror);
   }
 
@@ -1936,13 +1891,13 @@ public void testWithStringPartitionSpecialChars() throws IOException {
 
     run("INSERT INTO TABLE " + dbName + ".ptned PARTITION(p=\"" + ptnVal[1] +"\") values('" + ptn_data[1] + "')", driver);
     // Replicate insert event and verify
-    Tuple incrDump = incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
+    Tuple incrDump = incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT p from " + replDbName + ".ptned ORDER BY p desc", ptnVal, driverMirror);
 
     run("ALTER TABLE " + dbName + ".ptned DROP PARTITION(p=\"" + ptnVal[0] + "\")", driver);
 
     // Replicate drop partition event and verify
-    incrementalLoadAndVerify(dbName, incrDump.lastReplId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyIfPartitionNotExist(replDbName, "ptned", new ArrayList<>(Arrays.asList(ptnVal[0])), metaStoreClientMirror);
   }
 
@@ -1956,7 +1911,6 @@ public void testRenameTableWithCM() throws IOException {
     run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     String[] unptn_data = new String[] { "ten", "twenty" };
     String[] ptn_data_1 = new String[] { "fifteen", "fourteen" };
@@ -1973,20 +1927,18 @@ public void testRenameTableWithCM() throws IOException {
     run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[1] + "')", driver);
 
     // Get the last repl ID corresponding to all insert events except RENAME.
-    Tuple incrementalDump = replDumpDb(dbName, replDumpId, null, null);
-    String lastDumpIdWithoutRename = incrementalDump.lastReplId;
+    Tuple incrementalDump = replDumpDb(dbName);
 
     run("ALTER TABLE " + dbName + ".unptned RENAME TO " + dbName + ".unptned_renamed", driver);
     run("ALTER TABLE " + dbName + ".ptned RENAME TO " + dbName + ".ptned_renamed", driver);
 
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, lastDumpIdWithoutRename, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    loadAndVerify(replDbName, incrementalDump.dumpLocation, incrementalDump.lastReplId);
 
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=2) ORDER BY a", ptn_data_2, driverMirror);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyFail("SELECT a from " + replDbName + ".unptned ORDER BY a", driverMirror);
     verifyFail("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", driverMirror);
     verifyRun("SELECT a from " + replDbName + ".unptned_renamed ORDER BY a", unptn_data, driverMirror);
@@ -2002,7 +1954,6 @@ public void testRenamePartitionWithCM() throws IOException {
     run("CREATE TABLE " + dbName + ".ptned(a string) partitioned by (b int) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     String[] empty = new String[] {};
     String[] ptn_data_1 = new String[] { "fifteen", "fourteen" };
@@ -2016,18 +1967,16 @@ public void testRenamePartitionWithCM() throws IOException {
     run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[1] + "')", driver);
 
     // Get the last repl ID corresponding to all insert events except RENAME.
-    Tuple incrementalDump = replDumpDb(dbName, replDumpId, null, null);
-    String lastDumpIdWithoutRename = incrementalDump.lastReplId;
+    Tuple incrementalDump = replDumpDb(dbName);
 
     run("ALTER TABLE " + dbName + ".ptned PARTITION (b=2) RENAME TO PARTITION (b=10)", driver);
 
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, lastDumpIdWithoutRename, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    loadAndVerify(replDbName, incrementalDump.dumpLocation, incrementalDump.lastReplId);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=2) ORDER BY a", ptn_data_2, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=10) ORDER BY a", empty, driverMirror);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", ptn_data_1, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=10) ORDER BY a", ptn_data_2, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=2) ORDER BY a", empty, driverMirror);
@@ -2060,8 +2009,8 @@ public void testRenameTableAcrossDatabases() throws IOException {
 
     verifyFail("ALTER TABLE " + dbName1 + ".unptned RENAME TO " + dbName2 + ".unptned_renamed", driver);
 
-    incrementalLoadAndVerify(dbName1, bootstrap1.lastReplId, replDbName1);
-    incrementalLoadAndVerify(dbName2, bootstrap2.lastReplId, replDbName2);
+    incrementalLoadAndVerify(dbName1, replDbName1);
+    incrementalLoadAndVerify(dbName2, replDbName2);
 
     verifyIfTableNotExist(replDbName1, "unptned_renamed", metaStoreClientMirror);
     verifyIfTableNotExist(replDbName2, "unptned_renamed", metaStoreClientMirror);
@@ -2095,8 +2044,8 @@ public void testRenamePartitionedTableAcrossDatabases() throws IOException {
 
     verifyFail("ALTER TABLE " + dbName1 + ".ptned RENAME TO " + dbName2 + ".ptned_renamed", driver);
 
-    incrementalLoadAndVerify(dbName1, bootstrap1.lastReplId, replDbName1);
-    incrementalLoadAndVerify(dbName2, bootstrap2.lastReplId, replDbName2);
+    incrementalLoadAndVerify(dbName1, replDbName1);
+    incrementalLoadAndVerify(dbName2, replDbName2);
 
     verifyIfTableNotExist(replDbName1, "ptned_renamed", metaStoreClientMirror);
     verifyIfTableNotExist(replDbName2, "ptned_renamed", metaStoreClientMirror);
@@ -2145,7 +2094,6 @@ public void testViewsReplication() throws IOException {
     //verifySetup("SELECT a from " + dbName + ".mat_view", ptn_data_1, driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     // view is referring to old database, so no data
     verifyRun("SELECT * from " + replDbName + ".virtual_view", empty, driverMirror);
@@ -2161,8 +2109,7 @@ public void testViewsReplication() throws IOException {
     //verifySetup("SELECT * from " + dbName + ".mat_view2", unptn_data, driver);
 
     // Perform REPL-DUMP/LOAD
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
 
     verifyRun("SELECT * from " + replDbName + ".unptned", unptn_data, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where b=1", ptn_data_1, driverMirror);
@@ -2178,8 +2125,7 @@ public void testViewsReplication() throws IOException {
     verifySetup("SELECT * from " + dbName + ".virtual_view_rename", unptn_data, driver);
 
     // Perform REPL-DUMP/LOAD
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT * from " + replDbName + ".virtual_view_rename", empty, driverMirror);
 
     // Test "alter table" with schema change
@@ -2187,8 +2133,7 @@ public void testViewsReplication() throws IOException {
     verifySetup("SHOW COLUMNS FROM " + dbName + ".virtual_view_rename", new String[] {"a", "a_"}, driver);
 
     // Perform REPL-DUMP/LOAD
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SHOW COLUMNS FROM " + replDbName + ".virtual_view_rename", new String[] {"a", "a_"}, driverMirror);
 
     // Test "DROP VIEW"
@@ -2196,58 +2141,10 @@ public void testViewsReplication() throws IOException {
     verifyIfTableNotExist(dbName, "virtual_view", metaStoreClient);
 
     // Perform REPL-DUMP/LOAD
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyIfTableNotExist(replDbName, "virtual_view", metaStoreClientMirror);
   }
 
-  @Test
-  public void testDumpLimit() throws IOException {
-    String name = testName.getMethodName();
-    String dbName = createDB(name, driver);
-    String replDbName = dbName + "_dupe";
-    run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);
-
-    Tuple bootstrapDump = replDumpDb(dbName, null, null, null);
-    String replDumpId = bootstrapDump.lastReplId;
-    String replDumpLocn = bootstrapDump.dumpLocation;
-
-    String[] unptn_data = new String[] { "eleven", "thirteen", "twelve" };
-    String[] unptn_data_load1 = new String[] { "eleven" };
-    String[] unptn_data_load2 = new String[] { "eleven", "thirteen" };
-
-    // x events to insert, last repl ID: replDumpId+x
-    run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[0] + "')", driver);
-    String firstInsertLastReplId = replDumpDb(dbName, replDumpId, null, null).lastReplId;
-    Integer numOfEventsIns1 = Integer.valueOf(firstInsertLastReplId) - Integer.valueOf(replDumpId);
-
-    // x events to insert, last repl ID: replDumpId+2x
-    run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[1] + "')", driver);
-    String secondInsertLastReplId = replDumpDb(dbName, firstInsertLastReplId, null, null).lastReplId;
-    Integer numOfEventsIns2 = Integer.valueOf(secondInsertLastReplId) - Integer.valueOf(firstInsertLastReplId);
-
-    // x events to insert, last repl ID: replDumpId+3x
-    run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[2] + "')", driver);
-    verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data, driver);
-
-    run("REPL LOAD " + replDbName + " FROM '" + replDumpLocn + "'", driverMirror);
-
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, null, numOfEventsIns1.toString(), replDbName);
-    replDumpId = incrementalDump.lastReplId;
-    verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data_load1, driverMirror);
-
-    Integer lastReplID = Integer.valueOf(replDumpId);
-    lastReplID += 1000;
-    String toReplID = String.valueOf(lastReplID);
-
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, toReplID, numOfEventsIns2.toString(), replDbName);
-    replDumpId = incrementalDump.lastReplId;
-
-    verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data_load2, driverMirror);
-
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data, driverMirror);
-  }
-
   @Test
   public void testExchangePartition() throws IOException {
     String testName = "exchangePartition";
@@ -2278,7 +2175,6 @@ public void testExchangePartition() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned_src where (b=2 and c=3) ORDER BY a", ptn_data_2, driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
     verifyRun("SELECT a from " + replDbName + ".ptned_src where (b=1 and c=1) ORDER BY a", ptn_data_1, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned_src where (b=2 and c=2) ORDER BY a", ptn_data_2, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned_src where (b=2 and c=3) ORDER BY a", ptn_data_2, driverMirror);
@@ -2295,8 +2191,7 @@ public void testExchangePartition() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=2 and c=2)", empty, driver);
     verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=2 and c=3)", empty, driver);
 
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".ptned_src where (b=1 and c=1)", empty, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned_src where (b=2 and c=2) ORDER BY a", ptn_data_2, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned_src where (b=2 and c=3) ORDER BY a", ptn_data_2, driverMirror);
@@ -2313,7 +2208,7 @@ public void testExchangePartition() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=2 and c=2) ORDER BY a", ptn_data_2, driver);
     verifySetup("SELECT a from " + dbName + ".ptned_dest where (b=2 and c=3) ORDER BY a", ptn_data_2, driver);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".ptned_src where (b=1 and c=1)", empty, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned_src where (b=2 and c=2)", empty, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned_src where (b=2 and c=3)", empty, driverMirror);
@@ -2330,7 +2225,6 @@ public void testTruncateTable() throws IOException {
     run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     String[] unptn_data = new String[] { "eleven", "twelve" };
     String[] empty = new String[] {};
@@ -2338,22 +2232,20 @@ public void testTruncateTable() throws IOException {
     run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[1] + "')", driver);
     verifySetup("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data, driver);
 
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data, driverMirror);
 
     run("TRUNCATE TABLE " + dbName + ".unptned", driver);
     verifySetup("SELECT a from " + dbName + ".unptned", empty, driver);
 
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".unptned", empty, driverMirror);
 
     String[] unptn_data_after_ins = new String[] { "thirteen" };
     run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data_after_ins[0] + "')", driver);
     verifySetup("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data_after_ins, driver);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data_after_ins, driverMirror);
   }
 
@@ -2403,7 +2295,7 @@ public void testTruncatePartitionedTable() throws IOException {
     verifySetup("SELECT a from " + dbName + ".ptned_2 where (b=10)", empty, driver);
     verifySetup("SELECT a from " + dbName + ".ptned_2 where (b=20)", empty, driver);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifySetup("SELECT a from " + replDbName + ".ptned_1 where (b=1) ORDER BY a", ptn_data_1, driverMirror);
     verifySetup("SELECT a from " + replDbName + ".ptned_1 where (b=2)", empty, driverMirror);
     verifySetup("SELECT a from " + replDbName + ".ptned_2 where (b=10)", empty, driverMirror);
@@ -2417,7 +2309,7 @@ public void testTruncateWithCM() throws IOException {
     String replDbName = dbName + "_dupe";
     run("CREATE TABLE " + dbName + ".unptned(a string) STORED AS TEXTFILE", driver);
 
-    Tuple bootstrapDump = replDumpDb(dbName, null, null, null);
+    Tuple bootstrapDump = replDumpDb(dbName);
     String replDumpId = bootstrapDump.lastReplId;
     String replDumpLocn = bootstrapDump.dumpLocation;
 
@@ -2428,20 +2320,20 @@ public void testTruncateWithCM() throws IOException {
 
     // x events to insert, last repl ID: replDumpId+x
     run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[0] + "')", driver);
-    String firstInsertLastReplId = replDumpDb(dbName, replDumpId, null, null).lastReplId;
-    Integer numOfEventsIns1 = Integer.valueOf(firstInsertLastReplId) - Integer.valueOf(replDumpId);
+    Tuple firstInsert = replDumpDb(dbName);
+    Integer numOfEventsIns1 = Integer.valueOf(firstInsert.lastReplId) - Integer.valueOf(replDumpId);
 
     // x events to insert, last repl ID: replDumpId+2x
     run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[1] + "')", driver);
     verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data, driver);
-    String secondInsertLastReplId = replDumpDb(dbName, firstInsertLastReplId, null, null).lastReplId;
-    Integer numOfEventsIns2 = Integer.valueOf(secondInsertLastReplId) - Integer.valueOf(firstInsertLastReplId);
+    Tuple secondInsert = replDumpDb(dbName);
+    Integer numOfEventsIns2 = Integer.valueOf(secondInsert.lastReplId) - Integer.valueOf(firstInsert.lastReplId);
 
     // y event to truncate, last repl ID: replDumpId+2x+y
     run("TRUNCATE TABLE " + dbName + ".unptned", driver);
     verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", empty, driver);
-    String thirdTruncLastReplId = replDumpDb(dbName, secondInsertLastReplId, null, null).lastReplId;
-    Integer numOfEventsTrunc3 = Integer.valueOf(thirdTruncLastReplId) - Integer.valueOf(secondInsertLastReplId);
+    Tuple thirdTrunc = replDumpDb(dbName);
+    Integer numOfEventsTrunc3 = Integer.valueOf(thirdTrunc.lastReplId) - Integer.valueOf(secondInsert.lastReplId);
 
     // x events to insert, last repl ID: replDumpId+3x+y
     run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data_load1[0] + "')", driver);
@@ -2450,27 +2342,21 @@ public void testTruncateWithCM() throws IOException {
     run("REPL LOAD " + replDbName + " FROM '" + replDumpLocn + "'", driverMirror);
 
     // Dump and load only first insert (1 record)
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, null, numOfEventsIns1.toString(), replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    loadAndVerify(replDbName, firstInsert.dumpLocation, firstInsert.lastReplId);
 
     verifyRun("SELECT a from " + dbName + "_dupe.unptned ORDER BY a", unptn_data_load1, driverMirror);
 
     // Dump and load only second insert (2 records)
-    Integer lastReplID = Integer.valueOf(replDumpId);
-    lastReplID += 1000;
-    String toReplID = String.valueOf(lastReplID);
 
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, toReplID, numOfEventsIns2.toString(), replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    loadAndVerify(replDbName, secondInsert.dumpLocation, secondInsert.lastReplId);
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data_load2, driverMirror);
 
     // Dump and load only truncate (0 records)
-    incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, null, numOfEventsTrunc3.toString(), replDbName);
-    replDumpId = incrementalDump.lastReplId;
+    loadAndVerify(replDbName, thirdTrunc.dumpLocation, thirdTrunc.lastReplId);
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", empty, driverMirror);
 
     // Dump and load insert after truncate (1 record)
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", unptn_data_load1, driverMirror);
   }
 
@@ -2495,46 +2381,50 @@ public void testIncrementalRepeatEventOnExistingObject() throws IOException {
 
     // INSERT EVENT to unpartitioned table
     run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[0] + "')", driver);
-    Tuple replDump = dumpDbFromLastDump(dbName, bootstrapDump);
+    Tuple replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // INSERT EVENT to partitioned table with dynamic ADD_PARTITION
     run("INSERT INTO TABLE " + dbName + ".ptned PARTITION(b=1) values('" + ptn_data_1[0] + "')", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // ADD_PARTITION EVENT to partitioned table
     run("ALTER TABLE " + dbName + ".ptned ADD PARTITION (b=2)", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // INSERT EVENT to partitioned table on existing partition
     run("INSERT INTO TABLE " + dbName + ".ptned PARTITION(b=2) values('" + ptn_data_2[0] + "')", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // TRUNCATE_PARTITION EVENT on partitioned table
     run("TRUNCATE TABLE " + dbName + ".ptned PARTITION (b=1)", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // TRUNCATE_TABLE EVENT on unpartitioned table
     run("TRUNCATE TABLE " + dbName + ".unptned", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // CREATE_TABLE EVENT with multiple partitions
     run("CREATE TABLE " + dbName + ".unptned_tmp AS SELECT * FROM " + dbName + ".ptned", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // ADD_CONSTRAINT EVENT
     run("ALTER TABLE " + dbName + ".unptned_tmp ADD CONSTRAINT uk_unptned UNIQUE(a) disable", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // Replicate all the events happened so far
-    Tuple incrDump = incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
+    for (Tuple currDump : incrementalDumpList) {
+      // Load the incremental dump and ensure it does nothing and lastReplID remains same
+      loadAndVerify(replDbName, currDump.dumpLocation, currDump.lastReplId);
+    }
+    Tuple incrDump = incrementalLoadAndVerify(dbName, replDbName);
 
     verifyRun("SELECT a from " + replDbName + ".unptned ORDER BY a", empty, driverMirror);
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", empty, driverMirror);
@@ -2576,77 +2466,82 @@ public void testIncrementalRepeatEventOnMissingObject() throws Exception {
 
     // INSERT EVENT to unpartitioned table
     run("INSERT INTO TABLE " + dbName + ".unptned values('" + unptn_data[0] + "')", driver);
-    Tuple replDump = dumpDbFromLastDump(dbName, bootstrapDump);
+    Tuple replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // INSERT EVENT to partitioned table with dynamic ADD_PARTITION
     run("INSERT INTO TABLE " + dbName + ".ptned partition(b=1) values('" + ptn_data_1[0] + "')", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // ADD_PARTITION EVENT to partitioned table
     run("ALTER TABLE " + dbName + ".ptned ADD PARTITION (b=2)", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // INSERT EVENT to partitioned table on existing partition
     run("INSERT INTO TABLE " + dbName + ".ptned partition(b=2) values('" + ptn_data_2[0] + "')", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // TRUNCATE_PARTITION EVENT on partitioned table
     run("TRUNCATE TABLE " + dbName + ".ptned PARTITION(b=1)", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // TRUNCATE_TABLE EVENT on unpartitioned table
     run("TRUNCATE TABLE " + dbName + ".unptned", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // CREATE_TABLE EVENT on partitioned table
     run("CREATE TABLE " + dbName + ".ptned_tmp (a string) PARTITIONED BY (b int) STORED AS TEXTFILE", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // INSERT EVENT to partitioned table with dynamic ADD_PARTITION
     run("INSERT INTO TABLE " + dbName + ".ptned_tmp partition(b=10) values('" + ptn_data_1[0] + "')", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // INSERT EVENT to partitioned table with dynamic ADD_PARTITION
     run("INSERT INTO TABLE " + dbName + ".ptned_tmp partition(b=20) values('" + ptn_data_2[0] + "')", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // DROP_PARTITION EVENT to partitioned table
     run("ALTER TABLE " + dbName + ".ptned DROP PARTITION (b=1)", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // RENAME_PARTITION EVENT to partitioned table
     run("ALTER TABLE " + dbName + ".ptned PARTITION (b=2) RENAME TO PARTITION (b=20)", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // RENAME_TABLE EVENT to unpartitioned table
     run("ALTER TABLE " + dbName + ".unptned RENAME TO " + dbName + ".unptned_new", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // ADD_CONSTRAINT EVENT
     run("ALTER TABLE " + dbName + ".ptned_tmp ADD CONSTRAINT uk_unptned UNIQUE(a) disable", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
     // DROP_TABLE EVENT to partitioned table
     run("DROP TABLE " + dbName + ".ptned_tmp", driver);
-    replDump = dumpDbFromLastDump(dbName, replDump);
+    replDump = replDumpDb(dbName);
     incrementalDumpList.add(replDump);
 
+    // Load each incremental dump from the list. Each dump have only one operation.
+    for (Tuple currDump : incrementalDumpList) {
+      // Load the current incremental dump and ensure it does nothing and lastReplID remains same
+      loadAndVerify(replDbName, currDump.dumpLocation, currDump.lastReplId);
+    }
     // Replicate all the events happened so far
-    Tuple incrDump = incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
-
+    Tuple incrDump = incrementalLoadAndVerify(dbName, replDbName);
+    // Verify if the data are intact even after applying an applied event once again on missing objects
     verifyIfTableNotExist(replDbName, "unptned", metaStoreClientMirror);
     verifyIfTableNotExist(replDbName, "ptned_tmp", metaStoreClientMirror);
     verifyIfTableExist(replDbName, "unptned_new", metaStoreClientMirror);
@@ -2691,7 +2586,7 @@ public void testConcatenateTable() throws IOException {
     verifyRun("SELECT a from " + dbName + ".unptned ORDER BY a", unptn_data, driver);
 
     // Replicate all the events happened after bootstrap
-    incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
 
     // migration test is failing as CONCATENATE is not working. Its not creating the merged file.
     if (!isMigrationTest) {
@@ -2724,7 +2619,7 @@ public void testConcatenatePartitionedTable() throws IOException {
     run("ALTER TABLE " + dbName + ".ptned PARTITION(b=2) CONCATENATE", driver);
 
     // Replicate all the events happened so far
-    incrementalLoadAndVerify(dbName, bootstrapDump.lastReplId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
 
     // migration test is failing as CONCATENATE is not working. Its not creating the merged file.
     if (!isMigrationTest) {
@@ -2765,7 +2660,7 @@ public void testIncrementalLoadFailAndRetry() throws IOException {
 
     // Replicate all the events happened so far. It should fail as the data files missing in
     // original path and not available in CM as well.
-    Tuple incrDump = replDumpDb(dbName, bootstrapDump.lastReplId, null, null);
+    Tuple incrDump = replDumpDb(dbName);
     verifyFail("REPL LOAD " + replDbName + " FROM '" + incrDump.dumpLocation + "'", driverMirror);
 
     verifyRun("SELECT a from " + replDbName + ".ptned where (b=1) ORDER BY a", empty, driverMirror);
@@ -2874,7 +2769,7 @@ public void testConstraints() throws IOException {
     run("CREATE TABLE " + dbName + ".tbl5(a string, b string, foreign key (a, b) references " + dbName + ".tbl4(a, b) disable novalidate)", driver);
     run("CREATE TABLE " + dbName + ".tbl6(a string, b string not null disable, unique (a) disable)", driver);
 
-    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    Tuple incrementalDump = incrementalLoadAndVerify(dbName, replDbName);
     replDumpId = incrementalDump.lastReplId;
 
     String pkName = null;
@@ -2904,7 +2799,7 @@ public void testConstraints() throws IOException {
     run("ALTER TABLE " + dbName + ".tbl5 DROP CONSTRAINT `" + fkName + "`", driver);
     run("ALTER TABLE " + dbName + ".tbl6 DROP CONSTRAINT `" + nnName + "`", driver);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     try {
       List<SQLPrimaryKey> pks = metaStoreClientMirror.getPrimaryKeys(new PrimaryKeysRequest(replDbName , "tbl4"));
       assertTrue(pks.isEmpty());
@@ -2954,7 +2849,6 @@ public void testRemoveStats() throws IOException {
     verifySetup("SELECT max(a) from " + dbName + ".ptned where b=1", new String[]{"8"}, driver);
 
     Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replDbName);
-    String replDumpId = bootstrapDump.lastReplId;
 
     verifyRun("SELECT count(*) from " + replDbName + ".unptned", new String[]{"2"}, driverMirror);
     verifyRun("SELECT count(*) from " + replDbName + ".ptned", new String[]{"3"}, driverMirror);
@@ -2970,7 +2864,7 @@ public void testRemoveStats() throws IOException {
     run("ANALYZE TABLE " + dbName + ".ptned2 partition(b) COMPUTE STATISTICS FOR COLUMNS", driver);
     run("ANALYZE TABLE " + dbName + ".ptned2 partition(b) COMPUTE STATISTICS", driver);
 
-    incrementalLoadAndVerify(dbName, replDumpId, replDbName);
+    incrementalLoadAndVerify(dbName, replDbName);
     verifyRun("SELECT count(*) from " + replDbName + ".unptned2", new String[]{"2"}, driverMirror);
     verifyRun("SELECT count(*) from " + replDbName + ".ptned2", new String[]{"3"}, driverMirror);
     verifyRun("SELECT max(a) from " + replDbName + ".unptned2", new String[]{"2"}, driverMirror);
@@ -2993,7 +2887,7 @@ public void testDeleteStagingDir() throws IOException {
     verifySetup("SELECT * from " + dbName + ".unptned", unptn_data, driver);
 
     // Perform repl
-    String replDumpLocn = replDumpDb(dbName, null, null, null).dumpLocation;
+    String replDumpLocn = replDumpDb(dbName).dumpLocation;
     // Reset the driver
     driverMirror.close();
     run("REPL LOAD " + replDbName + " FROM '" + replDumpLocn + "'", driverMirror);
@@ -3037,7 +2931,7 @@ public void testCMConflict() throws IOException {
     run("INSERT INTO TABLE " + dbName + ".unptned values('ten')", driver);
 
     // Bootstrap test
-    Tuple bootstrapDump = replDumpDb(dbName, null, null, null);
+    Tuple bootstrapDump = replDumpDb(dbName);
     advanceDumpDir();
     run("REPL DUMP " + dbName, driver);
     String replDumpLocn = bootstrapDump.dumpLocation;
@@ -3315,18 +3209,18 @@ public void testDumpWithPartitionDirMissing() throws IOException {
   public void testDumpNonReplDatabase() throws IOException {
     String dbName = createDBNonRepl(testName.getMethodName(), driver);
     verifyFail("REPL DUMP " + dbName, driver);
-    verifyFail("REPL DUMP " + dbName + " from 1 ", driver);
+    verifyFail("REPL DUMP " + dbName, driver);
     assertTrue(run("REPL DUMP " + dbName + " with ('hive.repl.dump.metadata.only' = 'true')",
             true, driver));
-    assertTrue(run("REPL DUMP " + dbName + " from 1  with ('hive.repl.dump.metadata.only' = 'true')",
+    assertTrue(run("REPL DUMP " + dbName + " with ('hive.repl.dump.metadata.only' = 'true')",
             true, driver));
     run("alter database " + dbName + " set dbproperties ('repl.source.for' = '1, 2, 3')", driver);
     assertTrue(run("REPL DUMP " + dbName, true, driver));
-    assertTrue(run("REPL DUMP " + dbName + " from 1 ", true, driver));
+    assertTrue(run("REPL DUMP " + dbName, true, driver));
     dbName = createDBNonRepl(testName.getMethodName() + "_case", driver);
     run("alter database " + dbName + " set dbproperties ('repl.SOURCE.for' = '1, 2, 3')", driver);
     assertTrue(run("REPL DUMP " + dbName, true, driver));
-    assertTrue(run("REPL DUMP " + dbName + " from 1 ", true, driver));
+    assertTrue(run("REPL DUMP " + dbName, true, driver));
   }
 
   @Test
@@ -3392,7 +3286,7 @@ public void testMoveOptimizationBootstrap() throws IOException {
     verifyRun("SELECT fld from " + tableNamePart + " where part = 11" , new String[]{ "3" }, driver);
 
     String replDbName = dbName + "_replica";
-    Tuple dump = replDumpDb(dbName, null, null, null);
+    Tuple dump = replDumpDb(dbName);
     run("REPL LOAD " + replDbName + " FROM '" + dump.dumpLocation +
             "' with ('hive.repl.enable.move.optimization'='true')", driverMirror);
     verifyRun("REPL STATUS " + replDbName, dump.lastReplId, driverMirror);
@@ -3427,7 +3321,7 @@ public void testMoveOptimizationIncremental() throws IOException {
     run("CREATE TABLE " + dbName + ".unptned_late AS SELECT * FROM " + dbName + ".unptned", driver);
     verifySetup("SELECT * from " + dbName + ".unptned_late ORDER BY a", unptn_data, driver);
 
-    Tuple incrementalDump = replDumpDb(dbName, replDumpId, null, null);
+    Tuple incrementalDump = replDumpDb(dbName);
     run("REPL LOAD " + replDbName + " FROM '" + incrementalDump.dumpLocation +
             "' with ('hive.repl.enable.move.optimization'='true')", driverMirror);
     verifyRun("REPL STATUS " + replDbName, incrementalDump.lastReplId, driverMirror);
@@ -3445,7 +3339,7 @@ public void testMoveOptimizationIncremental() throws IOException {
     run("INSERT OVERWRITE TABLE " + dbName + ".unptned values('" + data_after_ovwrite[0] + "')", driver);
     verifySetup("SELECT a from " + dbName + ".unptned", data_after_ovwrite, driver);
 
-    incrementalDump = replDumpDb(dbName, replDumpId, null, null);
+    incrementalDump = replDumpDb(dbName);
     run("REPL LOAD " + replDbName + " FROM '" + incrementalDump.dumpLocation +
             "' with ('hive.repl.enable.move.optimization'='true')", driverMirror);
     verifyRun("REPL STATUS " + replDbName, incrementalDump.lastReplId, driverMirror);
@@ -3497,7 +3391,7 @@ private String verifyAndReturnDbReplStatus(String dbName,
                                              String prevReplDumpId, String cmd,
                                              String replDbName) throws IOException {
     run(cmd, driver);
-    String lastReplDumpId = incrementalLoadAndVerify(dbName, prevReplDumpId, replDbName).lastReplId;
+    String lastReplDumpId = incrementalLoadAndVerify(dbName, replDbName).lastReplId;
     assertTrue(Long.parseLong(lastReplDumpId) > Long.parseLong(prevReplDumpId));
     return lastReplDumpId;
   }
@@ -3508,7 +3402,7 @@ private String verifyAndReturnTblReplStatus(
       String replDbName) throws IOException, TException {
     run(cmd, driver);
     String lastReplDumpId
-            = incrementalLoadAndVerify(dbName, lastDbReplDumpId, replDbName).lastReplId;
+            = incrementalLoadAndVerify(dbName, replDbName).lastReplId;
     verifyRun("REPL STATUS " + replDbName, lastReplDumpId, driverMirror);
     assertTrue(Long.parseLong(lastReplDumpId) > Long.parseLong(lastDbReplDumpId));
 
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
index 63b32c83db..f3a1abb7a1 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
@@ -116,7 +116,7 @@ public void tearDown() throws Throwable {
   @Test
   public void testAcidTablesBootstrap() throws Throwable {
     // Bootstrap
-    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName, null, null);
+    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName, null);
     replica.load(replicatedDbName, bootstrapDump.dumpLocation);
     verifyLoadExecution(replicatedDbName, bootstrapDump.lastReplicationId, true);
 
@@ -125,7 +125,7 @@ public void testAcidTablesBootstrap() throws Throwable {
     prepareIncAcidData(primaryDbName);
     LOG.info(testName.getMethodName() + ": first incremental dump and load.");
     WarehouseInstance.Tuple incDump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, bootstrapDump.lastReplicationId);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, incDump.dumpLocation);
     verifyIncLoad(replicatedDbName, incDump.lastReplicationId);
 
@@ -134,14 +134,14 @@ public void testAcidTablesBootstrap() throws Throwable {
     prepareInc2AcidData(primaryDbName, primary.hiveConf);
     LOG.info(testName.getMethodName() + ": second incremental dump and load.");
     WarehouseInstance.Tuple inc2Dump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, incDump.lastReplicationId);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, inc2Dump.dumpLocation);
     verifyInc2Load(replicatedDbName, inc2Dump.lastReplicationId);
   }
 
   @Test
   public void testAcidTablesMoveOptimizationBootStrap() throws Throwable {
-    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName, null, null);
+    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName, null);
     replica.load(replicatedDbName, bootstrapDump.dumpLocation,
             Collections.singletonList("'hive.repl.enable.move.optimization'='true'"));
     verifyLoadExecution(replicatedDbName, bootstrapDump.lastReplicationId, true);
@@ -149,11 +149,10 @@ public void testAcidTablesMoveOptimizationBootStrap() throws Throwable {
 
   @Test
   public void testAcidTablesMoveOptimizationIncremental() throws Throwable {
-    WarehouseInstance.Tuple bootstrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootstrapDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootstrapDump.dumpLocation,
             Collections.singletonList("'hive.repl.enable.move.optimization'='true'"));
-    WarehouseInstance.Tuple incrDump = prepareDataAndDump(primaryDbName,
-            bootstrapDump.lastReplicationId, null);
+    WarehouseInstance.Tuple incrDump = prepareDataAndDump(primaryDbName, null);
     replica.load(replicatedDbName, incrDump.dumpLocation,
             Collections.singletonList("'hive.repl.enable.move.optimization'='true'"));
     verifyLoadExecution(replicatedDbName, incrDump.lastReplicationId, true);
@@ -189,7 +188,7 @@ public void testAcidTablesBootstrapWithOpenTxnsTimeout() throws Throwable {
             "'hive.repl.bootstrap.dump.open.txn.timeout'='1s'");
     WarehouseInstance.Tuple bootstrapDump = primary
             .run("use " + primaryDbName)
-            .dump(primaryDbName, null, withConfigs);
+            .dump(primaryDbName, withConfigs);
 
     // After bootstrap dump, all the opened txns should be aborted. Verify it.
     verifyAllOpenTxnsAborted(txns, primaryConf);
@@ -274,7 +273,7 @@ public void run() {
     InjectableBehaviourObjectStore.setCallerVerifier(callerInjectedBehavior);
     WarehouseInstance.Tuple bootstrapDump = null;
     try {
-      bootstrapDump = primary.dump(primaryDbName, null);
+      bootstrapDump = primary.dump(primaryDbName);
       callerInjectedBehavior.assertInjectionsPerformed(true, true);
     } finally {
       InjectableBehaviourObjectStore.resetCallerVerifier(); // reset the behaviour
@@ -289,7 +288,7 @@ public void run() {
             .verifyResults(new String[]{"1" });
 
     // Incremental should include the concurrent write of data "2" from another txn.
-    WarehouseInstance.Tuple incrementalDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    WarehouseInstance.Tuple incrementalDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, incrementalDump.dumpLocation)
             .run("use " + replicatedDbName)
             .run("repl status " + replicatedDbName)
@@ -348,7 +347,7 @@ public void run() {
     InjectableBehaviourObjectStore.setCallerVerifier(callerInjectedBehavior);
     WarehouseInstance.Tuple bootstrapDump = null;
     try {
-      bootstrapDump = primary.dump(primaryDbName, null);
+      bootstrapDump = primary.dump(primaryDbName);
       callerInjectedBehavior.assertInjectionsPerformed(true, true);
     } finally {
       InjectableBehaviourObjectStore.resetCallerVerifier(); // reset the behaviour
@@ -367,7 +366,7 @@ public void run() {
             .run("create table t1 (id int) clustered by(id) into 3 buckets stored as orc " +
                     "tblproperties (\"transactional\"=\"true\")")
             .run("insert into t1 values(100)")
-            .dump(primaryDbName, bootstrapDump.lastReplicationId);
+            .dump(primaryDbName);
 
     replica.load(replicatedDbName, incrementalDump.dumpLocation)
             .run("use " + replicatedDbName)
@@ -380,7 +379,7 @@ public void run() {
   @Test
   public void testOpenTxnEvent() throws Throwable {
     String tableName = testName.getMethodName();
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootStrapDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId);
@@ -396,7 +395,7 @@ public void testOpenTxnEvent() throws Throwable {
             .verifyResult("1");
 
     WarehouseInstance.Tuple incrementalDump =
-            primary.dump(primaryDbName, bootStrapDump.lastReplicationId);
+            primary.dump(primaryDbName);
 
     long lastReplId = Long.parseLong(bootStrapDump.lastReplicationId);
     primary.testEventCounts(primaryDbName, lastReplId, null, null, 22);
@@ -415,7 +414,7 @@ public void testOpenTxnEvent() throws Throwable {
   @Test
   public void testAbortTxnEvent() throws Throwable {
     String tableNameFail = testName.getMethodName() + "Fail";
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootStrapDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId);
@@ -429,7 +428,7 @@ public void testAbortTxnEvent() throws Throwable {
             .verifyFailure(new String[]{tableNameFail});
 
     WarehouseInstance.Tuple incrementalDump =
-            primary.dump(primaryDbName, bootStrapDump.lastReplicationId);
+            primary.dump(primaryDbName);
     replica.load(replicatedDbName, incrementalDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(incrementalDump.lastReplicationId);
@@ -443,7 +442,7 @@ public void testAbortTxnEvent() throws Throwable {
   @Test
   public void testTxnEventNonAcid() throws Throwable {
     String tableName = testName.getMethodName();
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
     replicaNonAcid.load(replicatedDbName, bootStrapDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId);
@@ -460,7 +459,7 @@ public void testTxnEventNonAcid() throws Throwable {
             .verifyResult("1");
 
     WarehouseInstance.Tuple incrementalDump =
-            primary.dump(primaryDbName, bootStrapDump.lastReplicationId);
+            primary.dump(primaryDbName);
     replicaNonAcid.runFailure("REPL LOAD " + replicatedDbName + " FROM '" + incrementalDump.dumpLocation + "'")
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId);
@@ -477,11 +476,7 @@ public void testAcidBootstrapReplLoadRetryAfterFailure() throws Throwable {
                     "\"transactional_properties\"=\"insert_only\")")
             .run("insert into t2 partition(name='bob') values(11)")
             .run("insert into t2 partition(name='carl') values(10)")
-            .dump(primaryDbName, null);
-
-    WarehouseInstance.Tuple tuple2 = primary
-            .run("use " + primaryDbName)
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     // Inject a behavior where REPL LOAD failed when try to load table "t2", it fails.
     BehaviourInjection<CallerArguments, Boolean> callerVerifier
@@ -515,9 +510,6 @@ public Boolean apply(@Nullable CallerArguments args) {
             .verifyResults(new String[] { })
             .verifyReplTargetProperty(replicatedDbName);
 
-    // Retry with different dump should fail.
-    replica.loadFailure(replicatedDbName, tuple2.dumpLocation);
-
     // Verify if no create table on t1. Only table t2 should  be created in retry.
     callerVerifier = new BehaviourInjection<CallerArguments, Boolean>() {
       @Nullable
@@ -614,7 +606,7 @@ public void testMultiDBTxn() throws Throwable {
 
     WarehouseInstance.Tuple incrementalDump;
     primary.run("alter database default set dbproperties ('repl.source.for' = '1, 2, 3')");
-    WarehouseInstance.Tuple bootStrapDump = primary.dump("`*`", null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump("`*`");
 
     primary.run("use " + primaryDbName)
           .run("create database " + dbName1 + " WITH DBPROPERTIES ( '" + SOURCE_OF_REPLICATION + "' = '1,2,3')")
@@ -646,7 +638,7 @@ public void testMultiDBTxn() throws Throwable {
           .verifyResults(resultArray)
           .run(txnStrCommit);
 
-    incrementalDump = primary.dump("`*`", bootStrapDump.lastReplicationId);
+    incrementalDump = primary.dump("`*`");
 
     // Due to the limitation that we can only have one instance of Persistence Manager Factory in a JVM
     // we are not able to create multiple embedded derby instances for two different MetaStore instances.
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTablesBootstrap.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTablesBootstrap.java
index 9389f26d27..f5dd043a9c 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTablesBootstrap.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTablesBootstrap.java
@@ -65,7 +65,7 @@ public static void classLevelSetup() throws Exception {
   @Test
   public void testAcidTablesBootstrapDuringIncremental() throws Throwable {
     // Take a bootstrap dump without acid tables
-    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName, null,
+    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName,
             dumpWithoutAcidClause);
     LOG.info(testName.getMethodName() + ": loading dump without acid tables.");
     replica.load(replicatedDbName, bootstrapDump.dumpLocation);
@@ -76,7 +76,7 @@ public void testAcidTablesBootstrapDuringIncremental() throws Throwable {
     prepareIncNonAcidData(primaryDbName);
     LOG.info(testName.getMethodName() + ": incremental dump and load dump with acid table bootstrap.");
     WarehouseInstance.Tuple incrementalDump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, bootstrapDump.lastReplicationId, dumpWithAcidBootstrapClause);
+            .dump(primaryDbName, dumpWithAcidBootstrapClause);
     replica.load(replicatedDbName, incrementalDump.dumpLocation);
     verifyIncLoad(replicatedDbName, incrementalDump.lastReplicationId);
     // Ckpt should be set on bootstrapped tables.
@@ -89,14 +89,14 @@ public void testAcidTablesBootstrapDuringIncremental() throws Throwable {
              + ": second incremental dump and load dump after incremental with acid table " +
             "bootstrap.");
     WarehouseInstance.Tuple inc2Dump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, incrementalDump.lastReplicationId);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, inc2Dump.dumpLocation);
     verifyInc2Load(replicatedDbName, inc2Dump.lastReplicationId);
   }
 
   @Test
   public void testRetryAcidTablesBootstrapFromDifferentDump() throws Throwable {
-    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName, null,
+    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName,
             dumpWithoutAcidClause);
     LOG.info(testName.getMethodName() + ": loading dump without acid tables.");
     replica.load(replicatedDbName, bootstrapDump.dumpLocation);
@@ -106,7 +106,7 @@ public void testRetryAcidTablesBootstrapFromDifferentDump() throws Throwable {
     prepareIncNonAcidData(primaryDbName);
     LOG.info(testName.getMethodName() + ": first incremental dump with acid table bootstrap.");
     WarehouseInstance.Tuple incDump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, bootstrapDump.lastReplicationId, dumpWithAcidBootstrapClause);
+            .dump(primaryDbName, dumpWithAcidBootstrapClause);
 
     // Fail setting ckpt property for table t5 but success for earlier tables
     BehaviourInjection<CallerArguments, Boolean> callerVerifier
@@ -139,7 +139,7 @@ public Boolean apply(@Nullable CallerArguments args) {
     prepareInc2NonAcidData(primaryDbName, primary.hiveConf);
     LOG.info(testName.getMethodName() + ": second incremental dump with acid table bootstrap");
     WarehouseInstance.Tuple inc2Dump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, bootstrapDump.lastReplicationId, dumpWithAcidBootstrapClause);
+            .dump(primaryDbName, dumpWithAcidBootstrapClause);
 
     // Set incorrect bootstrap dump to clean tables. Here, used the full bootstrap dump which is invalid.
     // So, REPL LOAD fails.
@@ -176,16 +176,16 @@ public Boolean apply(@Nullable CallerArguments args) {
 
   @Test
   public void retryIncBootstrapAcidFromDifferentDumpWithoutCleanTablesConfig() throws Throwable {
-    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName, null,
+    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName,
             dumpWithoutAcidClause);
     replica.load(replicatedDbName, bootstrapDump.dumpLocation);
 
     prepareIncAcidData(primaryDbName);
     prepareIncNonAcidData(primaryDbName);
     WarehouseInstance.Tuple incDump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, bootstrapDump.lastReplicationId, dumpWithAcidBootstrapClause);
+            .dump(primaryDbName, dumpWithAcidBootstrapClause);
     WarehouseInstance.Tuple inc2Dump = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, bootstrapDump.lastReplicationId, dumpWithAcidBootstrapClause);
+            .dump(primaryDbName, dumpWithAcidBootstrapClause);
     replica.load(replicatedDbName, incDump.dumpLocation);
 
     // Re-bootstrapping from different bootstrap dump without clean tables config should fail.
@@ -196,7 +196,7 @@ public void retryIncBootstrapAcidFromDifferentDumpWithoutCleanTablesConfig() thr
   @Test
   public void testAcidTablesBootstrapDuringIncrementalWithOpenTxnsTimeout() throws Throwable {
     // Take a dump without ACID tables
-    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName, null,
+    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName,
                                                                 dumpWithoutAcidClause);
     LOG.info(testName.getMethodName() + ": loading dump without acid tables.");
     replica.load(replicatedDbName, bootstrapDump.dumpLocation);
@@ -222,7 +222,7 @@ public void testAcidTablesBootstrapDuringIncrementalWithOpenTxnsTimeout() throws
             withConfigs.add("'hive.repl.bootstrap.dump.open.txn.timeout'='1s'");
     WarehouseInstance.Tuple incDump = primary
             .run("use " + primaryDbName)
-            .dump(primaryDbName, bootstrapDump.lastReplicationId, withConfigs);
+            .dump(primaryDbName, withConfigs);
 
     // After bootstrap dump, all the opened txns should be aborted. Verify it.
     verifyAllOpenTxnsAborted(txns, primaryConf);
@@ -254,7 +254,7 @@ public void testAcidTablesBootstrapDuringIncrementalWithOpenTxnsTimeout() throws
   @Test
   public void testBootstrapAcidTablesDuringIncrementalWithConcurrentWrites() throws Throwable {
     // Dump and load bootstrap without ACID tables.
-    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName, null,
+    WarehouseInstance.Tuple bootstrapDump = prepareDataAndDump(primaryDbName,
                                                                 dumpWithoutAcidClause);
     LOG.info(testName.getMethodName() + ": loading dump without acid tables.");
     replica.load(replicatedDbName, bootstrapDump.dumpLocation);
@@ -304,7 +304,7 @@ public void run() {
     InjectableBehaviourObjectStore.setGetCurrentNotificationEventIdBehaviour(callerInjectedBehavior);
     WarehouseInstance.Tuple incDump = null;
     try {
-      incDump = primary.dump(primaryDbName, bootstrapDump.lastReplicationId, dumpWithAcidBootstrapClause);
+      incDump = primary.dump(primaryDbName, dumpWithAcidBootstrapClause);
       callerInjectedBehavior.assertInjectionsPerformed(true, true);
     } finally {
       // reset the behaviour
@@ -321,7 +321,7 @@ public void run() {
     // Next Incremental should include the concurrent writes
     LOG.info(testName.getMethodName() +
             ": dumping second normal incremental dump from event id = " + incDump.lastReplicationId);
-    WarehouseInstance.Tuple inc2Dump = primary.dump(primaryDbName, incDump.lastReplicationId);
+    WarehouseInstance.Tuple inc2Dump = primary.dump(primaryDbName);
     LOG.info(testName.getMethodName() +
             ": loading second normal incremental dump from event id = " + incDump.lastReplicationId);
     replica.load(replicatedDbName, inc2Dump.dumpLocation);
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
index fd4f2dc1b1..eb8a8995c7 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder;
-import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder;
 import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;
 import org.apache.hadoop.hive.ql.parse.repl.PathBuilder;
@@ -77,7 +76,7 @@ public static void classLevelSetup() throws Exception {
 
   @Test
   public void testCreateFunctionIncrementalReplication() throws Throwable {
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootStrapDump.dumpLocation)
         .run("REPL STATUS " + replicatedDbName)
         .verifyResult(bootStrapDump.lastReplicationId);
@@ -89,7 +88,7 @@ public void testCreateFunctionIncrementalReplication() throws Throwable {
             + ".testFunctionTwo as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax'");
 
     WarehouseInstance.Tuple incrementalDump =
-        primary.dump(primaryDbName, bootStrapDump.lastReplicationId);
+        primary.dump(primaryDbName);
     replica.load(replicatedDbName, incrementalDump.dumpLocation)
         .run("REPL STATUS " + replicatedDbName)
         .verifyResult(incrementalDump.lastReplicationId)
@@ -117,7 +116,7 @@ public void testBootstrapReplLoadRetryAfterFailureForFunctions() throws Throwabl
             .run("CREATE FUNCTION " + primaryDbName + "." + funcName2 +
                     " as 'hivemall.tools.string.SplitWordsUDF' "+
                     "using jar  'ivy://io.github.myui:hivemall:0.4.0-1'")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     // Allow create function only on f1. Create should fail for the second function.
     BehaviourInjection<CallerArguments, Boolean> callerVerifier
@@ -196,7 +195,7 @@ public void testDropFunctionIncrementalReplication() throws Throwable {
     primary.run("CREATE FUNCTION " + primaryDbName
         + ".testFunctionAnother as 'hivemall.tools.string.StopwordUDF' "
         + "using jar  'ivy://io.github.myui:hivemall:0.4.0-2'");
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootStrapDump.dumpLocation)
         .run("REPL STATUS " + replicatedDbName)
         .verifyResult(bootStrapDump.lastReplicationId);
@@ -204,7 +203,7 @@ public void testDropFunctionIncrementalReplication() throws Throwable {
     primary.run("Drop FUNCTION " + primaryDbName + ".testFunctionAnother ");
 
     WarehouseInstance.Tuple incrementalDump =
-        primary.dump(primaryDbName, bootStrapDump.lastReplicationId);
+        primary.dump(primaryDbName);
     replica.load(replicatedDbName, incrementalDump.dumpLocation)
         .run("REPL STATUS " + replicatedDbName)
         .verifyResult(incrementalDump.lastReplicationId)
@@ -224,7 +223,7 @@ public void testBootstrapFunctionReplication() throws Throwable {
     primary.run("CREATE FUNCTION " + primaryDbName
         + ".testFunction as 'hivemall.tools.string.StopwordUDF' "
         + "using jar  'ivy://io.github.myui:hivemall:0.4.0-2'");
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
 
     replica.load(replicatedDbName, bootStrapDump.dumpLocation)
         .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
@@ -240,7 +239,7 @@ public void testCreateFunctionWithFunctionBinaryJarsOnHDFS() throws Throwable {
         + ".anotherFunction as 'hivemall.tools.string.StopwordUDF' "
         + "using " + jarSubString);
 
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation)
         .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
@@ -262,7 +261,7 @@ public void testCreateFunctionWithFunctionBinaryJarsOnHDFS() throws Throwable {
 
   @Test
   public void testIncrementalCreateFunctionWithFunctionBinaryJarsOnHDFS() throws Throwable {
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootStrapDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId);
@@ -274,7 +273,7 @@ public void testIncrementalCreateFunctionWithFunctionBinaryJarsOnHDFS() throws T
             + ".anotherFunction as 'hivemall.tools.string.StopwordUDF' "
             + "using " + jarSubString);
 
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, bootStrapDump.lastReplicationId);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation)
             .run("SHOW FUNCTIONS LIKE '" + replicatedDbName + "%'")
@@ -349,7 +348,7 @@ public void testMultipleStagesOfReplicationLoadTask() throws Throwable {
         .run("insert into table t2 partition(country='us') values ('austin')")
         .run("insert into table t2 partition(country='france') values ('paris')")
         .run("create table t3 (rank int)")
-        .dump(primaryDbName, null);
+        .dump(primaryDbName);
 
     // each table creation itself takes more than one task, give we are giving a max of 1, we should hit multiple runs.
     List<String> withClause = Collections.singletonList(
@@ -380,7 +379,7 @@ public void testParallelExecutionOfReplicationBootStrapLoad() throws Throwable {
         .run("insert into table t2 partition(country='japan') values ('tokyo')")
         .run("insert into table t2 partition(country='china') values ('hkg')")
         .run("create table t3 (rank int)")
-        .dump(primaryDbName, null);
+        .dump(primaryDbName);
 
     replica.hiveConf.setBoolVar(HiveConf.ConfVars.EXECPARALLEL, true);
     replica.load(replicatedDbName, tuple.dumpLocation)
@@ -402,8 +401,7 @@ public void testMetadataBootstrapDump() throws Throwable {
             "clustered by(key) into 2 buckets stored as orc tblproperties ('transactional'='true')")
         .run("create table table1 (i int, j int)")
         .run("insert into table1 values (1,2)")
-        .dump(primaryDbName, null,
-            Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
+        .dump(primaryDbName, Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
 
     replica.load(replicatedDbName, tuple.dumpLocation)
         .run("use " + replicatedDbName)
@@ -422,8 +420,7 @@ public void testIncrementalMetadataReplication() throws Throwable {
         .run("create table table2 (a int, city string) partitioned by (country string)")
         .run("create table table3 (i int, j int)")
         .run("insert into table1 values (1,2)")
-        .dump(primaryDbName, null,
-            Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
+        .dump(primaryDbName, Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
 
     replica.load(replicatedDbName, bootstrapTuple.dumpLocation)
         .run("use " + replicatedDbName)
@@ -439,10 +436,8 @@ public void testIncrementalMetadataReplication() throws Throwable {
             .run("alter table table1 rename to renamed_table1")
             .run("insert into table2 partition(country='india') values (1,'mumbai') ")
             .run("create table table4 (i int, j int)")
-            .dump(
-                "repl dump " + primaryDbName + " from " + bootstrapTuple.lastReplicationId + " to "
-                    + Long.parseLong(bootstrapTuple.lastReplicationId) + 100L + " limit 100 "
-                    + "with ('hive.repl.dump.metadata.only'='true')"
+            .dumpWithCommand(
+                "repl dump " + primaryDbName + " with ('hive.repl.dump.metadata.only'='true')"
             );
 
     replica.load(replicatedDbName, incrementalOneTuple.dumpLocation)
@@ -460,8 +455,7 @@ public void testIncrementalMetadataReplication() throws Throwable {
         .run("alter table table3 change i a string")
         .run("alter table table3 set tblproperties('custom.property'='custom.value')")
         .run("drop table renamed_table1")
-        .dump("repl dump " + primaryDbName + " from " + incrementalOneTuple.lastReplicationId
-            + " with ('hive.repl.dump.metadata.only'='true')"
+        .dumpWithCommand("repl dump " + primaryDbName + " with ('hive.repl.dump.metadata.only'='true')"
         );
 
     replica.load(replicatedDbName, secondIncremental.dumpLocation)
@@ -500,7 +494,7 @@ public void testNonReplDBMetadataReplication() throws Throwable {
             .run("create table table2 (a int, city string) partitioned by (country string)")
             .run("create table table3 (i int, j int)")
             .run("insert into table1 values (1,2)")
-        .dump(dbName, null, Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
+        .dump(dbName, Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
 
     replica.load(replicatedDbName, tuple.dumpLocation)
             .run("use " + replicatedDbName)
@@ -514,8 +508,7 @@ public void testNonReplDBMetadataReplication() throws Throwable {
             .run("alter table table1 rename to renamed_table1")
             .run("insert into table2 partition(country='india') values (1,'mumbai') ")
             .run("create table table4 (i int, j int)")
-        .dump(dbName, tuple.lastReplicationId,
-            Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
+        .dump(dbName, Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
 
     replica.load(replicatedDbName, tuple.dumpLocation)
             .run("use " + replicatedDbName)
@@ -529,6 +522,9 @@ public void testNonReplDBMetadataReplication() throws Throwable {
 
     @Test
   public void testBootStrapDumpOfWarehouse() throws Throwable {
+    //Clear the repl base dir
+    Path replBootstrapDumpDir = new Path(primary.hiveConf.get(MetastoreConf.ConfVars.REPLDIR.getHiveName()), "*");
+    replBootstrapDumpDir.getFileSystem(primary.hiveConf).delete(replBootstrapDumpDir, true);
     String randomOne = RandomStringUtils.random(10, true, false);
     String randomTwo = RandomStringUtils.random(10, true, false);
     String dbOne = primaryDbName + randomOne;
@@ -549,7 +545,7 @@ public void testBootStrapDumpOfWarehouse() throws Throwable {
                 SOURCE_OF_REPLICATION + "' = '1,2,3')")
         .run("use " + dbTwo)
         .run("create table t1 (i int, j int)")
-        .dump("`*`", null, Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
+        .dump("`*`", Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
 
     /*
       Due to the limitation that we can only have one instance of Persistence Manager Factory in a JVM
@@ -611,7 +607,7 @@ public void testIncrementalDumpOfWarehouse() throws Throwable {
         .run("use " + dbOne)
         .run("create table t1 (i int, j int) partitioned by (load_date date) "
             + "clustered by(i) into 2 buckets stored as orc tblproperties ('transactional'='true') ")
-        .dump("`*`", null, Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
+        .dump("`*`", Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
 
     String dbTwo = primaryDbName + randomTwo;
     WarehouseInstance.Tuple incrementalTuple = primary
@@ -621,8 +617,7 @@ public void testIncrementalDumpOfWarehouse() throws Throwable {
         .run("create table t1 (i int, j int)")
         .run("use " + dbOne)
         .run("create table t2 (a int, b int)")
-        .dump("`*`", bootstrapTuple.lastReplicationId,
-            Arrays.asList("'hive.repl.dump.metadata.only'='true'"));
+        .dump("`*`", Arrays.asList("'hive.repl.dump.metadata.only'='true'"));
 
     /*
       Due to the limitation that we can only have one instance of Persistence Manager Factory in a JVM
@@ -704,7 +699,7 @@ public void testReplLoadFromSourceUsingWithClause() throws Throwable {
             .run("create table table1 (i int)")
             .run("create table table2 (id int) partitioned by (country string)")
             .run("insert into table1 values (1)")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     // Run load on primary itself
     primary.load(replicatedDbName, bootstrapTuple.dumpLocation, withConfigs)
@@ -729,7 +724,7 @@ public void testReplLoadFromSourceUsingWithClause() throws Throwable {
                     .run("create function " + primaryDbName
                       + ".testFunctionOne as 'hivemall.tools.string.StopwordUDF' "
                       + "using jar  'ivy://io.github.myui:hivemall:0.4.0-2'")
-                    .dump(primaryDbName, bootstrapTuple.lastReplicationId);
+                    .dump(primaryDbName, Collections.emptyList());
 
     // Run load on primary itself
     primary.load(replicatedDbName, incrementalOneTuple.dumpLocation, withConfigs)
@@ -758,7 +753,7 @@ public void testReplLoadFromSourceUsingWithClause() throws Throwable {
             .run("alter table table2 drop partition(country='usa')")
             .run("truncate table table3")
             .run("drop function " + primaryDbName + ".testFunctionOne ")
-            .dump(primaryDbName, incrementalOneTuple.lastReplicationId);
+            .dump(primaryDbName, Collections.emptyList());
 
     // Run load on primary itself
     primary.load(replicatedDbName, secondIncremental.dumpLocation, withConfigs)
@@ -792,7 +787,7 @@ public void testReplLoadFromSourceUsingWithClause() throws Throwable {
   @Test
   public void testIncrementalReplWithEventsBatchHavingDropCreateTable() throws Throwable {
     // Bootstrap dump with empty db
-    WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName);
 
     // Bootstrap load in replica
     replica.load(replicatedDbName, bootstrapTuple.dumpLocation)
@@ -805,7 +800,7 @@ public void testIncrementalReplWithEventsBatchHavingDropCreateTable() throws Thr
             .run("create table table2 (id int) partitioned by (country string)")
             .run("insert into table1 values (1)")
             .run("insert into table2 partition(country='india') values(1)")
-            .dump(primaryDbName, bootstrapTuple.lastReplicationId);
+            .dump(primaryDbName, Collections.emptyList());
 
     // Second incremental dump
     WarehouseInstance.Tuple secondIncremental = primary.run("use " + primaryDbName)
@@ -817,7 +812,7 @@ public void testIncrementalReplWithEventsBatchHavingDropCreateTable() throws Thr
             .run("insert into table2 partition(country='us') values(2)")
             .run("create table table1 (i int)")
             .run("insert into table1 values (2)")
-            .dump(primaryDbName, firstIncremental.lastReplicationId);
+            .dump(primaryDbName, Collections.emptyList());
 
     // First incremental load
     replica.load(replicatedDbName, firstIncremental.dumpLocation)
@@ -847,7 +842,7 @@ public void testIncrementalReplWithEventsBatchHavingDropCreateTable() throws Thr
   @Test
   public void testIncrementalReplWithDropAndCreateTableDifferentPartitionTypeAndInsert() throws Throwable {
     // Bootstrap dump with empty db
-    WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName);
 
     // Bootstrap load in replica
     replica.load(replicatedDbName, bootstrapTuple.dumpLocation)
@@ -862,7 +857,7 @@ public void testIncrementalReplWithDropAndCreateTableDifferentPartitionTypeAndIn
         .run("insert into table1 partition(country='india') values(1)")
         .run("insert into table2 values(2)")
         .run("insert into table3 partition(country='india') values(3)")
-        .dump(primaryDbName, bootstrapTuple.lastReplicationId);
+        .dump(primaryDbName, Collections.emptyList());
 
     // Second incremental dump
     WarehouseInstance.Tuple secondIncremental = primary.run("use " + primaryDbName)
@@ -875,7 +870,7 @@ public void testIncrementalReplWithDropAndCreateTableDifferentPartitionTypeAndIn
         .run("insert into table2 partition(country='india') values(20)")
         .run("create table table3 (id int) partitioned by (name string, rank int)")
         .run("insert into table3 partition(name='adam', rank=100) values(30)")
-        .dump(primaryDbName, firstIncremental.lastReplicationId);
+        .dump(primaryDbName, Collections.emptyList());
 
     // First incremental load
     replica.load(replicatedDbName, firstIncremental.dumpLocation)
@@ -916,7 +911,7 @@ public void testShouldNotCreateDirectoryForNonNativeTableInDumpDirectory() throw
 
     WarehouseInstance.Tuple bootstrapTuple = primary
         .run("use " + primaryDbName)
-        .run(createTableQuery).dump(primaryDbName, null);
+        .run(createTableQuery).dump(primaryDbName);
     Path cSerdesTableDumpLocation = new Path(
         new Path(bootstrapTuple.dumpLocation, primaryDbName),
         "custom_serdes");
@@ -940,7 +935,7 @@ public void testShouldDumpMetaDataForNonNativeTableIfSetMeataDataOnly() throws T
     WarehouseInstance.Tuple bootstrapTuple = primary
             .run("use " + primaryDbName)
             .run(createTableQuery)
-            .dump(primaryDbName, null, Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
+            .dump(primaryDbName, Collections.singletonList("'hive.repl.dump.metadata.only'='true'"));
 
     // Bootstrap load in replica
     replica.load(replicatedDbName, bootstrapTuple.dumpLocation)
@@ -966,13 +961,13 @@ private void verifyIfSrcOfReplPropMissing(Map<String, String> props) {
 
   @Test
   public void testIncrementalDumpEmptyDumpDirectory() throws Throwable {
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation)
             .status(replicatedDbName)
             .verifyResult(tuple.lastReplicationId);
 
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName, Collections.emptyList());
 
     replica.load(replicatedDbName, tuple.dumpLocation)
             .status(replicatedDbName)
@@ -982,29 +977,29 @@ public void testIncrementalDumpEmptyDumpDirectory() throws Throwable {
     String testDbName = primaryDbName + "_test";
     tuple = primary.run(" create database " + testDbName)
             .run("create table " + testDbName + ".tbl (fld int)")
-            .dump(primaryDbName, tuple.lastReplicationId);
+            .dump(primaryDbName, Collections.emptyList());
 
     // Incremental load to existing database with empty dump directory should set the repl id to the last event at src.
     replica.load(replicatedDbName, tuple.dumpLocation)
             .status(replicatedDbName)
             .verifyResult(tuple.lastReplicationId);
 
-    // Incremental load to non existing db should return database not exist error.
-    tuple = primary.dump("someJunkDB", tuple.lastReplicationId);
+    // Bootstrap load from an empty dump directory should return empty load directory error.
+    tuple = primary.dump("someJunkDB", Collections.emptyList());
     try {
       replica.runCommand("REPL LOAD someJunkDB from '" + tuple.dumpLocation + "'");
       assert false;
     } catch (CommandProcessorException e) {
-      assertTrue(e.getMessage().toLowerCase().contains(
-          "org.apache.hadoop.hive.ql.ddl.DDLTask. Database does not exist: someJunkDB".toLowerCase()));
+      assertTrue(e.getMessage().toLowerCase().contains("semanticException no data to load in path".toLowerCase()));
     }
 
-    // Bootstrap load from an empty dump directory should return empty load directory error.
-    tuple = primary.dump("someJunkDB", null);
+    // Incremental load to non existing db should return database not exist error.
+    tuple = primary.dump("someJunkDB");
     try {
       replica.runCommand("REPL LOAD someJunkDB from '" + tuple.dumpLocation+"'");
     } catch (CommandProcessorException e) {
-      assertTrue(e.getMessage().toLowerCase().contains("semanticException no data to load in path".toLowerCase()));
+      assertTrue(e.getMessage().toLowerCase().contains(
+              "org.apache.hadoop.hive.ql.ddl.DDLTask. Database does not exist: someJunkDB".toLowerCase()));
     }
 
     primary.run(" drop database if exists " + testDbName + " cascade");
@@ -1012,7 +1007,7 @@ public void testIncrementalDumpEmptyDumpDirectory() throws Throwable {
 
   @Test
   public void testIncrementalDumpMultiIteration() throws Throwable {
-    WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName);
 
     replica.load(replicatedDbName, bootstrapTuple.dumpLocation)
             .status(replicatedDbName)
@@ -1025,7 +1020,7 @@ public void testIncrementalDumpMultiIteration() throws Throwable {
             .run("insert into table1 partition(country='india') values(1)")
             .run("insert into table2 values(2)")
             .run("insert into table3 partition(country='india') values(3)")
-            .dump(primaryDbName, bootstrapTuple.lastReplicationId);
+            .dump(primaryDbName, Collections.emptyList());
 
     replica.load(replicatedDbName, incremental.dumpLocation,
         Collections.singletonList("'hive.repl.approx.max.load.tasks'='10'"))
@@ -1045,7 +1040,7 @@ public void testIncrementalDumpMultiIteration() throws Throwable {
                     "clustered by(key) into 2 buckets stored as orc")
             .run("create table table4 (i int, j int)")
             .run("insert into table4 values (1,2)")
-            .dump(primaryDbName, incremental.lastReplicationId);
+            .dump(primaryDbName, Collections.emptyList());
 
     Path path = new Path(incremental.dumpLocation);
     FileSystem fs = path.getFileSystem(conf);
@@ -1069,7 +1064,7 @@ public void testIfCkptAndSourceOfReplPropsIgnoredByReplDump() throws Throwable {
             .run("create table t1 (place string) partitioned by (country string) "
                     + " tblproperties('custom.property'='custom.value')")
             .run("insert into table t1 partition(country='india') values ('bangalore')")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     // Bootstrap Repl A -> B
     replica.load(replicatedDbName, tuplePrimary.dumpLocation)
@@ -1077,17 +1072,17 @@ public void testIfCkptAndSourceOfReplPropsIgnoredByReplDump() throws Throwable {
             .verifyResult(tuplePrimary.lastReplicationId)
             .run("show tblproperties t1('custom.property')")
             .verifyResults(new String[] { "custom.property\tcustom.value" })
-            .dumpFailure(replicatedDbName, null)
+            .dumpFailure(replicatedDbName)
             .run("alter database " + replicatedDbName
                     + " set dbproperties ('" + SOURCE_OF_REPLICATION + "' = '1, 2, 3')")
-            .dumpFailure(replicatedDbName, null);//can not dump the db before first successful incremental load is done.
+            .dumpFailure(replicatedDbName);   //can not dump the db before first successful incremental load is done.
 
     // do a empty incremental load to allow dump of replicatedDbName
-    WarehouseInstance.Tuple temp = primary.dump(primaryDbName, tuplePrimary.lastReplicationId);
+    WarehouseInstance.Tuple temp = primary.dump(primaryDbName, Collections.emptyList());
     replica.load(replicatedDbName, temp.dumpLocation); // first successful incremental load.
 
     // Bootstrap Repl B -> C
-    WarehouseInstance.Tuple tupleReplica = replica.dump(replicatedDbName, null);
+    WarehouseInstance.Tuple tupleReplica = replica.dump(replicatedDbName);
     String replDbFromReplica = replicatedDbName + "_dupe";
     replica.load(replDbFromReplica, tupleReplica.dumpLocation)
             .run("use " + replDbFromReplica)
@@ -1115,13 +1110,13 @@ public void testIfCkptAndSourceOfReplPropsIgnoredByReplDump() throws Throwable {
             .run("alter database " + primaryDbName + " set dbproperties('dummy_key'='dummy_val')")
             .run("alter table t1 set tblproperties('dummy_key'='dummy_val')")
             .run("alter table t1 partition(country='india') set fileformat orc")
-            .dump(primaryDbName, tuplePrimary.lastReplicationId);
+            .dump(primaryDbName, Collections.emptyList());
 
     // Incremental Repl A -> B with alters on db/table/partition
     WarehouseInstance.Tuple tupleReplicaInc = replica.load(replicatedDbName, tuplePrimaryInc.dumpLocation)
             .run("repl status " + replicatedDbName)
             .verifyResult(tuplePrimaryInc.lastReplicationId)
-            .dump(replicatedDbName, tupleReplica.lastReplicationId);
+            .dump(replicatedDbName, Collections.emptyList());
 
     // Check if DB in B have ckpt property is set to bootstrap dump location used in B and missing for table/partition.
     db = replica.getDatabase(replicatedDbName);
@@ -1158,7 +1153,7 @@ public void testIfCkptPropIgnoredByExport() throws Throwable {
             .run("use " + primaryDbName)
             .run("create table t1 (place string) partitioned by (country string)")
             .run("insert into table t1 partition(country='india') values ('bangalore')")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     // Bootstrap Repl A -> B and then export table t1
     String path = "hdfs:///tmp/" + replicatedDbName + "/";
@@ -1202,7 +1197,7 @@ public void testIfBootstrapReplLoadFailWhenRetryAfterBootstrapComplete() throws
             .run("insert into table t2 partition(country='india') values ('bangalore')")
             .run("insert into table t2 partition(country='uk') values ('london')")
             .run("insert into table t2 partition(country='us') values ('sfo')")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation)
             .run("use " + replicatedDbName)
@@ -1216,13 +1211,6 @@ public void testIfBootstrapReplLoadFailWhenRetryAfterBootstrapComplete() throws
             .verifyResults(Arrays.asList("india", "uk", "us"));
     replica.verifyIfCkptSet(replicatedDbName, tuple.dumpLocation);
 
-    WarehouseInstance.Tuple tuple_2 = primary
-            .run("use " + primaryDbName)
-            .dump(primaryDbName, null);
-
-    // Retry with different dump should fail.
-    replica.loadFailure(replicatedDbName, tuple_2.dumpLocation);
-
     // Retry with same dump with which it was already loaded also fails.
     replica.loadFailure(replicatedDbName, tuple.dumpLocation);
 
@@ -1239,11 +1227,7 @@ public void testBootstrapReplLoadRetryAfterFailureForTablesAndConstraints() thro
             .run("create table t1(a string, b string, primary key (a, b) disable novalidate rely)")
             .run("create table t2(a string, b string, foreign key (a, b) references t1(a, b) disable novalidate)")
             .run("create table t3(a string, b string not null disable, unique (a) disable)")
-            .dump(primaryDbName, null);
-
-    WarehouseInstance.Tuple tuple2 = primary
-            .run("use " + primaryDbName)
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     // Need to drop the primary DB as metastore is shared by both primary/replica. So, constraints
     // conflict when loaded. Some issue with framework which needs to be relook into later.
@@ -1287,10 +1271,6 @@ public Boolean apply(CallerArguments args) {
     assertEquals(0, replica.getNotNullConstraintList(replicatedDbName, "t3").size());
     assertEquals(0, replica.getForeignKeyList(replicatedDbName, "t2").size());
 
-    // Retry with different dump should fail.
-    replica.loadFailure(replicatedDbName, tuple2.dumpLocation, null,
-            ErrorMsg.REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID.getErrorCode());
-
     // Verify if create table is not called on table t1 but called for t2 and t3.
     // Also, allow constraint creation only on t1 and t3. Foreign key creation on t2 fails.
     callerVerifier = new BehaviourInjection<CallerArguments, Boolean>() {
@@ -1379,11 +1359,7 @@ public void testBootstrapReplLoadRetryAfterFailureForPartitions() throws Throwab
             .run("CREATE FUNCTION " + primaryDbName
                     + ".testFunctionOne as 'hivemall.tools.string.StopwordUDF' "
                     + "using jar  'ivy://io.github.myui:hivemall:0.4.0-2'")
-            .dump(primaryDbName, null);
-
-    WarehouseInstance.Tuple tuple2 = primary
-            .run("use " + primaryDbName)
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     // Inject a behavior where REPL LOAD failed when try to load table "t2" and partition "uk".
     // So, table "t2" will exist and partition "india" will exist, rest failed as operation failed.
@@ -1418,9 +1394,6 @@ public Boolean apply(List<Partition> ptns) {
             .run("select country from t2 order by country")
             .verifyResults(Collections.singletonList("india"));
 
-    // Retry with different dump should fail.
-    replica.loadFailure(replicatedDbName, tuple2.dumpLocation);
-
     // Verify if no create table calls. Add partitions and create function calls expected.
     BehaviourInjection<CallerArguments, Boolean> callerVerifier
             = new BehaviourInjection<CallerArguments, Boolean>() {
@@ -1465,7 +1438,7 @@ public void testMoveOptimizationBootstrapReplLoadRetryAfterFailure() throws Thro
             .run("use " + primaryDbName)
             .run("create table t2 (place string) partitioned by (country string)")
             .run("insert into table t2 partition(country='india') values ('bangalore')")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     testMoveOptimization(primaryDbName, replicatedDbName, replicatedDbName_CM, "t2",
             "ADD_PARTITION", tuple);
@@ -1480,7 +1453,7 @@ public void testMoveOptimizationIncrementalFailureAfterCopyReplace() throws Thro
             .run("create table t2 (place string) partitioned by (country string)")
             .run("insert into table t2 partition(country='india') values ('bangalore')")
             .run("create table t1 (place string) partitioned by (country string)")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation, withConfigs);
     replica.load(replicatedDbName_CM, tuple.dumpLocation, withConfigs);
     replica.run("alter database " + replicatedDbName + " set DBPROPERTIES ('" + SOURCE_OF_REPLICATION + "' = '1,2,3')")
@@ -1488,7 +1461,7 @@ public void testMoveOptimizationIncrementalFailureAfterCopyReplace() throws Thro
 
     tuple = primary.run("use " + primaryDbName)
             .run("insert overwrite table t1 select * from t2")
-            .dump(primaryDbName, tuple.lastReplicationId);
+            .dump(primaryDbName, Collections.emptyList());
 
     testMoveOptimization(primaryDbName, replicatedDbName, replicatedDbName_CM, "t1", "ADD_PARTITION", tuple);
   }
@@ -1501,7 +1474,7 @@ public void testMoveOptimizationIncrementalFailureAfterCopy() throws Throwable {
     WarehouseInstance.Tuple tuple = primary.run("use " + primaryDbName)
             .run("create table t2 (place string) partitioned by (country string)")
             .run("ALTER TABLE t2 ADD PARTITION (country='india')")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation, withConfigs);
     replica.load(replicatedDbName_CM, tuple.dumpLocation, withConfigs);
     replica.run("alter database " + replicatedDbName + " set DBPROPERTIES ('" + SOURCE_OF_REPLICATION + "' = '1,2,3')")
@@ -1509,7 +1482,7 @@ public void testMoveOptimizationIncrementalFailureAfterCopy() throws Throwable {
 
     tuple = primary.run("use " + primaryDbName)
             .run("insert into table t2 partition(country='india') values ('bangalore')")
-            .dump(primaryDbName, tuple.lastReplicationId);
+            .dump(primaryDbName, Collections.emptyList());
 
     testMoveOptimization(primaryDbName, replicatedDbName, replicatedDbName_CM, "t2", "INSERT", tuple);
   }
@@ -1578,7 +1551,7 @@ public void testBootstrapLoadRetryAfterFailureForAlterTable() throws Throwable {
             .run("create table t2 (place string) partitioned by (country string)")
             .run("insert into table t2 partition(country='china') values ('shenzhen')")
             .run("insert into table t2 partition(country='india') values ('banaglore')")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     // fail setting ckpt directory property for table t1.
     BehaviourInjection<CallerArguments, Boolean> callerVerifier
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java
index 200e02c559..740f229921 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java
@@ -102,7 +102,7 @@ public void replicationWithoutExternalTables() throws Throwable {
         .run("insert into table t2 partition(country='india') values ('bangalore')")
         .run("insert into table t2 partition(country='us') values ('austin')")
         .run("insert into table t2 partition(country='france') values ('paris')")
-        .dump(primaryDbName, null, dumpWithClause);
+        .dump(primaryDbName, dumpWithClause);
 
     // the _external_tables_file info only should be created if external tables are to be replicated not otherwise
     assertFalse(primary.miniDFSCluster.getFileSystem()
@@ -122,7 +122,7 @@ public void replicationWithoutExternalTables() throws Throwable {
         .run("create external table t3 (id int)")
         .run("insert into table t3 values (10)")
         .run("insert into table t3 values (20)")
-        .dump(primaryDbName, tuple.lastReplicationId, dumpWithClause);
+        .dump(primaryDbName, dumpWithClause);
 
     // the _external_tables_file info only should be created if external tables are to be replicated not otherwise
     assertFalse(primary.miniDFSCluster.getFileSystem()
@@ -147,7 +147,7 @@ public void externalTableReplicationWithDefaultPaths() throws Throwable {
         .run("insert into table t2 partition(country='india') values ('bangalore')")
         .run("insert into table t2 partition(country='us') values ('austin')")
         .run("insert into table t2 partition(country='france') values ('paris')")
-        .dump("repl dump " + primaryDbName);
+        .dumpWithCommand("repl dump " + primaryDbName);
 
     // verify that the external table info is written correctly for bootstrap
     assertExternalFileInfo(Arrays.asList("t1", "t2"),
@@ -178,7 +178,7 @@ public void externalTableReplicationWithDefaultPaths() throws Throwable {
         .run("create external table t3 (id int)")
         .run("insert into table t3 values (10)")
         .run("create external table t4 as select id from t3")
-        .dump("repl dump " + primaryDbName + " from " + tuple.lastReplicationId);
+        .dumpWithCommand("repl dump " + primaryDbName);
 
     // verify that the external table info is written correctly for incremental
     assertExternalFileInfo(Arrays.asList("t1", "t2", "t3", "t4"),
@@ -197,7 +197,7 @@ public void externalTableReplicationWithDefaultPaths() throws Throwable {
 
     tuple = primary.run("use " + primaryDbName)
         .run("drop table t1")
-        .dump("repl dump " + primaryDbName + " from " + tuple.lastReplicationId);
+        .dumpWithCommand("repl dump " + primaryDbName);
 
     // verify that the external table info is written correctly for incremental
     assertExternalFileInfo(Arrays.asList("t2", "t3", "t4"),
@@ -254,7 +254,7 @@ public void externalTableReplicationWithCustomPaths() throws Throwable {
         .run("create external table a (i int, j int) "
             + "row format delimited fields terminated by ',' "
             + "location '" + externalTableLocation.toUri() + "'")
-        .dump(primaryDbName, null);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, bootstrapTuple.dumpLocation, loadWithClause)
         .run("use " + replicatedDbName)
@@ -272,7 +272,7 @@ public void externalTableReplicationWithCustomPaths() throws Throwable {
     }
 
     WarehouseInstance.Tuple incrementalTuple = primary.run("create table b (i int)")
-        .dump(primaryDbName, bootstrapTuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, incrementalTuple.dumpLocation, loadWithClause)
         .run("select i From a")
@@ -285,7 +285,7 @@ public void externalTableReplicationWithCustomPaths() throws Throwable {
         new Path("/" + testName.getMethodName() + "/" + primaryDbName + "/new_location/a/");
     incrementalTuple = primary.run("use " + primaryDbName)
         .run("alter table a set location '" + externalTableLocation + "'")
-        .dump(primaryDbName, incrementalTuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, incrementalTuple.dumpLocation, loadWithClause)
         .run("use " + replicatedDbName)
@@ -308,7 +308,7 @@ public void externalTableWithPartitions() throws Throwable {
             + "delimited fields terminated by ',' location '" + externalTableLocation.toString()
             + "'")
         .run("insert into t2 partition(country='india') values ('bangalore')")
-        .dump("repl dump " + primaryDbName);
+        .dumpWithCommand("repl dump " + primaryDbName);
 
     assertExternalFileInfo(Collections.singletonList("t2"),
         new Path(new Path(tuple.dumpLocation, primaryDbName.toLowerCase()), FILE_NAME));
@@ -332,7 +332,7 @@ public void externalTableWithPartitions() throws Throwable {
 
     tuple = primary.run("use " + primaryDbName)
         .run("insert into t2 partition(country='australia') values ('sydney')")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     assertExternalFileInfo(Collections.singletonList("t2"),
         new Path(tuple.dumpLocation, FILE_NAME));
@@ -360,7 +360,7 @@ public void externalTableWithPartitions() throws Throwable {
     tuple = primary.run("use " + primaryDbName)
         .run("ALTER TABLE t2 ADD PARTITION (country='france') LOCATION '" + customPartitionLocation
             .toString() + "'")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause)
         .run("use " + replicatedDbName)
@@ -374,7 +374,7 @@ public void externalTableWithPartitions() throws Throwable {
 
     tuple = primary.run("use " + primaryDbName)
         .run("alter table t2 partition (country='france') set location '" + tmpLocation + "'")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause)
         .run("use " + replicatedDbName)
@@ -394,7 +394,7 @@ public void externalTableWithPartitions() throws Throwable {
     tuple = primary.run("use " + primaryDbName)
             .run("insert into table t2 partition(country='france') values ('lyon')")
             .run("alter table t2 set location '" + tmpLocation2 + "'")
-            .dump(primaryDbName, tuple.lastReplicationId);
+            .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause);
     assertTablePartitionLocation(primaryDbName + ".t2", replicatedDbName + ".t2");
@@ -402,7 +402,7 @@ public void externalTableWithPartitions() throws Throwable {
 
   @Test
   public void externalTableIncrementalReplication() throws Throwable {
-    WarehouseInstance.Tuple tuple = primary.dump("repl dump " + primaryDbName);
+    WarehouseInstance.Tuple tuple = primary.dumpWithCommand("repl dump " + primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation);
 
     Path externalTableLocation =
@@ -416,7 +416,7 @@ public void externalTableIncrementalReplication() throws Throwable {
             + "'")
         .run("alter table t1 add partition(country='india')")
         .run("alter table t1 add partition(country='us')")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     assertExternalFileInfo(Collections.singletonList("t1"), new Path(tuple.dumpLocation, FILE_NAME));
 
@@ -451,7 +451,7 @@ public void externalTableIncrementalReplication() throws Throwable {
     }
 
     // Repl load with zero events but external tables location info should present.
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName);
     assertExternalFileInfo(Collections.singletonList("t1"), new Path(tuple.dumpLocation, FILE_NAME));
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause)
@@ -473,7 +473,7 @@ public void externalTableIncrementalReplication() throws Throwable {
     tuple = primary
         .run("alter table t1 drop partition (country='india')")
         .run("alter table t1 drop partition (country='us')")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation)
         .run("select * From t1")
@@ -501,7 +501,7 @@ public void bootstrapExternalTablesDuringIncrementalPhase() throws Throwable {
             .run("insert into table t2 partition(country='india') values ('bangalore')")
             .run("insert into table t2 partition(country='us') values ('austin')")
             .run("insert into table t2 partition(country='france') values ('paris')")
-            .dump(primaryDbName, null, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     // the _external_tables_file info only should be created if external tables are to be replicated not otherwise
     assertFalse(primary.miniDFSCluster.getFileSystem()
@@ -525,7 +525,7 @@ public void bootstrapExternalTablesDuringIncrementalPhase() throws Throwable {
             .run("insert into table t3 values (10)")
             .run("insert into table t3 values (20)")
             .run("create table t4 as select * from t3")
-            .dump(primaryDbName, tuple.lastReplicationId, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     // the _external_tables_file info should be created as external tables are to be replicated.
     assertTrue(primary.miniDFSCluster.getFileSystem()
@@ -599,7 +599,7 @@ public void retryBootstrapExternalTablesFromDifferentDump() throws Throwable {
             .run("insert into table t2 partition(country='india') values ('bangalore')")
             .run("insert into table t2 partition(country='us') values ('austin')")
             .run("create table t3 as select * from t1")
-            .dump(primaryDbName, null, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     replica.load(replicatedDbName, tupleBootstrapWithoutExternal.dumpLocation, loadWithClause)
             .status(replicatedDbName)
@@ -618,7 +618,7 @@ public void retryBootstrapExternalTablesFromDifferentDump() throws Throwable {
             .run("create external table t4 (id int)")
             .run("insert into table t4 values (10)")
             .run("create table t5 as select * from t4")
-            .dump(primaryDbName, tupleBootstrapWithoutExternal.lastReplicationId, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     // Fail setting ckpt property for table t4 but success for t2.
     BehaviourInjection<CallerArguments, Boolean> callerVerifier
@@ -652,7 +652,7 @@ public Boolean apply(@Nullable CallerArguments args) {
             .run("drop table t2")
             .run("create table t2 as select * from t4")
             .run("insert into table t4 values (20)")
-            .dump(primaryDbName, tupleIncWithExternalBootstrap.lastReplicationId, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     // Set incorrect bootstrap dump to clean tables. Here, used the full bootstrap dump which is invalid.
     // So, REPL LOAD fails.
@@ -717,7 +717,7 @@ public void testExternalTablesIncReplicationWithConcurrentDropTable() throws Thr
     WarehouseInstance.Tuple tupleBootstrap = primary.run("use " + primaryDbName)
             .run("create external table t1 (id int)")
             .run("insert into table t1 values (1)")
-            .dump(primaryDbName, null, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     replica.load(replicatedDbName, tupleBootstrap.dumpLocation, loadWithClause);
 
@@ -746,7 +746,7 @@ public Table apply(@Nullable Table table) {
     WarehouseInstance.Tuple tupleInc;
     try {
       // The t1 table will be skipped from data location listing.
-      tupleInc = primary.dump(primaryDbName, tupleBootstrap.lastReplicationId, dumpWithClause);
+      tupleInc = primary.dump(primaryDbName, dumpWithClause);
       tableNuller.assertInjectionsPerformed(true, true);
     } finally {
       InjectableBehaviourObjectStore.resetGetTableBehaviour(); // reset the behaviour
@@ -776,7 +776,7 @@ public void testIncrementalDumpEmptyDumpDirectory() throws Throwable {
             .run("create external table t1 (id int)")
             .run("insert into table t1 values (1)")
             .run("insert into table t1 values (2)")
-            .dump(primaryDbName, null, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     replica.load(replicatedDbName, tuple.dumpLocation)
             .status(replicatedDbName)
@@ -784,7 +784,7 @@ public void testIncrementalDumpEmptyDumpDirectory() throws Throwable {
 
     // This looks like an empty dump but it has the ALTER TABLE event created by the previous
     // dump. We need it here so that the next dump won't have any events.
-    WarehouseInstance.Tuple incTuple = primary.dump(primaryDbName, tuple.lastReplicationId, dumpWithClause);
+    WarehouseInstance.Tuple incTuple = primary.dump(primaryDbName, dumpWithClause);
     replica.load(replicatedDbName, incTuple.dumpLocation, loadWithClause)
             .status(replicatedDbName)
             .verifyResult(incTuple.lastReplicationId);
@@ -795,7 +795,7 @@ public void testIncrementalDumpEmptyDumpDirectory() throws Throwable {
     WarehouseInstance.Tuple inc2Tuple = primary.run("use " + extraPrimaryDb)
             .run("create table tbl (fld int)")
             .run("use " + primaryDbName)
-            .dump(primaryDbName, incTuple.lastReplicationId, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
     Assert.assertEquals(primary.getCurrentNotificationEventId().getEventId(),
                         Long.valueOf(inc2Tuple.lastReplicationId).longValue());
 
@@ -818,7 +818,7 @@ public void testExtTableBootstrapDuringIncrementalWithoutAnyEvents() throws Thro
             .run("insert into table t1 values (1)")
             .run("create table t2 (id int)")
             .run("insert into table t2 values (1)")
-            .dump(primaryDbName, null, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     replica.load(replicatedDbName, bootstrapDump.dumpLocation, loadWithClause)
             .status(replicatedDbName)
@@ -832,7 +832,7 @@ public void testExtTableBootstrapDuringIncrementalWithoutAnyEvents() throws Thro
 
     // This looks like an empty dump but it has the ALTER TABLE event created by the previous
     // dump. We need it here so that the next dump won't have any events.
-    WarehouseInstance.Tuple incTuple = primary.dump(primaryDbName, bootstrapDump.lastReplicationId);
+    WarehouseInstance.Tuple incTuple = primary.dump(primaryDbName);
     replica.load(replicatedDbName, incTuple.dumpLocation)
             .status(replicatedDbName)
             .verifyResult(incTuple.lastReplicationId);
@@ -841,7 +841,7 @@ public void testExtTableBootstrapDuringIncrementalWithoutAnyEvents() throws Thro
     dumpWithClause = Arrays.asList("'" + HiveConf.ConfVars.REPL_INCLUDE_EXTERNAL_TABLES.varname + "'='true'",
             "'" + HiveConf.ConfVars.REPL_BOOTSTRAP_EXTERNAL_TABLES.varname + "'='true'");
     WarehouseInstance.Tuple inc2Tuple = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, incTuple.lastReplicationId, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     replica.load(replicatedDbName, inc2Tuple.dumpLocation, loadWithClause)
             .status(replicatedDbName)
@@ -867,7 +867,7 @@ public void replicationWithTableNameContainsKeywords() throws Throwable {
             .run("insert into table t2_constraints partition(country='india') values ('bangalore')")
             .run("insert into table t2_constraints partition(country='us') values ('austin')")
             .run("insert into table t2_constraints partition(country='france') values ('paris')")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause)
             .run("repl status " + replicatedDbName)
@@ -888,7 +888,7 @@ public void replicationWithTableNameContainsKeywords() throws Throwable {
             .run("create table t4_tables (id int)")
             .run("insert into table t4_tables values (10)")
             .run("insert into table t4_tables values (20)")
-            .dump(primaryDbName, tuple.lastReplicationId);
+            .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause)
             .run("use " + replicatedDbName)
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTablesMetaDataOnly.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTablesMetaDataOnly.java
index 1d824678c1..e9d0162cd0 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTablesMetaDataOnly.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTablesMetaDataOnly.java
@@ -101,7 +101,7 @@ public void replicationWithoutExternalTables() throws Throwable {
         .run("insert into table t2 partition(country='india') values ('bangalore')")
         .run("insert into table t2 partition(country='us') values ('austin')")
         .run("insert into table t2 partition(country='france') values ('paris')")
-        .dump(primaryDbName, null, dumpWithClause);
+        .dump(primaryDbName, dumpWithClause);
 
     // the _external_tables_file info only should be created if external tables are to be replicated not otherwise
     assertFalse(primary.miniDFSCluster.getFileSystem()
@@ -121,7 +121,7 @@ public void replicationWithoutExternalTables() throws Throwable {
         .run("create external table t3 (id int)")
         .run("insert into table t3 values (10)")
         .run("insert into table t3 values (20)")
-        .dump(primaryDbName, tuple.lastReplicationId, dumpWithClause);
+        .dump(primaryDbName, dumpWithClause);
 
     // the _external_tables_file data only should be created if external tables are to be replicated not otherwise
     assertFalse(primary.miniDFSCluster.getFileSystem()
@@ -146,7 +146,7 @@ public void externalTableReplicationWithDefaultPaths() throws Throwable {
         .run("insert into table t2 partition(country='india') values ('bangalore')")
         .run("insert into table t2 partition(country='us') values ('austin')")
         .run("insert into table t2 partition(country='france') values ('paris')")
-        .dump("repl dump " + primaryDbName);
+        .dumpWithCommand("repl dump " + primaryDbName);
 
     // verify that the external table info is not written as metadata only replication
     assertFalseExternalFileInfo(new Path(new Path(tuple.dumpLocation, primaryDbName.toLowerCase()), FILE_NAME));
@@ -173,7 +173,7 @@ public void externalTableReplicationWithDefaultPaths() throws Throwable {
         .run("create external table t3 (id int)")
         .run("insert into table t3 values (10)")
         .run("create external table t4 as select id from t3")
-        .dump("repl dump " + primaryDbName + " from " + tuple.lastReplicationId);
+        .dumpWithCommand("repl dump " + primaryDbName);
 
     // verify that the external table info is written correctly for incremental
     assertFalseExternalFileInfo(new Path(tuple.dumpLocation, FILE_NAME));
@@ -189,7 +189,7 @@ public void externalTableReplicationWithDefaultPaths() throws Throwable {
 
     tuple = primary.run("use " + primaryDbName)
         .run("drop table t1")
-        .dump("repl dump " + primaryDbName + " from " + tuple.lastReplicationId);
+        .dumpWithCommand("repl dump " + primaryDbName);
 
     // verify that the external table info is written correctly for incremental
     assertFalseExternalFileInfo(new Path(tuple.dumpLocation, FILE_NAME));
@@ -215,7 +215,7 @@ public void externalTableReplicationWithCustomPaths() throws Throwable {
         .run("create external table a (i int, j int) "
             + "row format delimited fields terminated by ',' "
             + "location '" + externalTableLocation.toUri() + "'")
-        .dump(primaryDbName, null);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, bootstrapTuple.dumpLocation, loadWithClause)
         .run("use " + replicatedDbName)
@@ -231,7 +231,7 @@ public void externalTableReplicationWithCustomPaths() throws Throwable {
     }
 
     WarehouseInstance.Tuple incrementalTuple = primary.run("create table b (i int)")
-        .dump(primaryDbName, bootstrapTuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, incrementalTuple.dumpLocation, loadWithClause)
         .run("select i From a")
@@ -244,7 +244,7 @@ public void externalTableReplicationWithCustomPaths() throws Throwable {
         new Path("/" + testName.getMethodName() + "/" + primaryDbName + "/new_location/a/");
     incrementalTuple = primary.run("use " + primaryDbName)
         .run("alter table a set location '" + externalTableLocation + "'")
-        .dump(primaryDbName, incrementalTuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, incrementalTuple.dumpLocation, loadWithClause)
         .run("use " + replicatedDbName)
@@ -266,7 +266,7 @@ public void externalTableWithPartitions() throws Throwable {
             + "delimited fields terminated by ',' location '" + externalTableLocation.toString()
             + "'")
         .run("insert into t2 partition(country='india') values ('bangalore')")
-        .dump("repl dump " + primaryDbName);
+        .dumpWithCommand("repl dump " + primaryDbName);
 
     assertFalseExternalFileInfo(new Path(new Path(tuple.dumpLocation, primaryDbName.toLowerCase()), FILE_NAME));
 
@@ -287,7 +287,7 @@ public void externalTableWithPartitions() throws Throwable {
 
     tuple = primary.run("use " + primaryDbName)
         .run("insert into t2 partition(country='australia') values ('sydney')")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     assertFalseExternalFileInfo(new Path(tuple.dumpLocation, FILE_NAME));
 
@@ -314,7 +314,7 @@ public void externalTableWithPartitions() throws Throwable {
     tuple = primary.run("use " + primaryDbName)
         .run("ALTER TABLE t2 ADD PARTITION (country='france') LOCATION '" + customPartitionLocation
             .toString() + "'")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause)
         .run("use " + replicatedDbName)
@@ -328,7 +328,7 @@ public void externalTableWithPartitions() throws Throwable {
 
     tuple = primary.run("use " + primaryDbName)
         .run("alter table t2 partition (country='france') set location '" + tmpLocation + "'")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause)
         .run("use " + replicatedDbName)
@@ -344,14 +344,14 @@ public void externalTableWithPartitions() throws Throwable {
     tuple = primary.run("use " + primaryDbName)
             .run("insert into table t2 partition(country='france') values ('lyon')")
             .run("alter table t2 set location '" + tmpLocation2 + "'")
-            .dump(primaryDbName, tuple.lastReplicationId);
+            .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause);
   }
 
   @Test
   public void externalTableIncrementalReplication() throws Throwable {
-    WarehouseInstance.Tuple tuple = primary.dump("repl dump " + primaryDbName);
+    WarehouseInstance.Tuple tuple = primary.dumpWithCommand("repl dump " + primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation);
 
     Path externalTableLocation =
@@ -365,7 +365,7 @@ public void externalTableIncrementalReplication() throws Throwable {
             + "'")
         .run("alter table t1 add partition(country='india')")
         .run("alter table t1 add partition(country='us')")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     assertFalseExternalFileInfo(new Path(tuple.dumpLocation, FILE_NAME));
 
@@ -400,7 +400,7 @@ public void externalTableIncrementalReplication() throws Throwable {
     }
 
     // Repl load with zero events but external tables location info should present.
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName);
     assertFalseExternalFileInfo(new Path(tuple.dumpLocation, FILE_NAME));
 
     replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause)
@@ -422,7 +422,7 @@ public void externalTableIncrementalReplication() throws Throwable {
     tuple = primary
         .run("alter table t1 drop partition (country='india')")
         .run("alter table t1 drop partition (country='us')")
-        .dump(primaryDbName, tuple.lastReplicationId);
+        .dump(primaryDbName);
 
     replica.load(replicatedDbName, tuple.dumpLocation)
         .run("select * From t1")
@@ -449,7 +449,7 @@ public void bootstrapExternalTablesDuringIncrementalPhase() throws Throwable {
             .run("insert into table t2 partition(country='india') values ('bangalore')")
             .run("insert into table t2 partition(country='us') values ('austin')")
             .run("insert into table t2 partition(country='france') values ('paris')")
-            .dump(primaryDbName, null, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     // the _external_tables_file info only should be created if external tables are to be replicated not otherwise
     assertFalse(primary.miniDFSCluster.getFileSystem()
@@ -474,7 +474,7 @@ public void bootstrapExternalTablesDuringIncrementalPhase() throws Throwable {
             .run("insert into table t3 values (10)")
             .run("insert into table t3 values (20)")
             .run("create table t4 as select * from t3")
-            .dump(primaryDbName, tuple.lastReplicationId, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     // the _external_tables_file info should be created as external tables are to be replicated.
     assertTrue(primary.miniDFSCluster.getFileSystem()
@@ -540,7 +540,7 @@ public void testExternalTablesIncReplicationWithConcurrentDropTable() throws Thr
     WarehouseInstance.Tuple tupleBootstrap = primary.run("use " + primaryDbName)
             .run("create external table t1 (id int)")
             .run("insert into table t1 values (1)")
-            .dump(primaryDbName, null, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     replica.load(replicatedDbName, tupleBootstrap.dumpLocation, loadWithClause);
 
@@ -569,7 +569,7 @@ public Table apply(@Nullable Table table) {
     WarehouseInstance.Tuple tupleInc;
     try {
       // The t1 table will be skipped from data location listing.
-      tupleInc = primary.dump(primaryDbName, tupleBootstrap.lastReplicationId, dumpWithClause);
+      tupleInc = primary.dump(primaryDbName, dumpWithClause);
       tableNuller.assertInjectionsPerformed(true, true);
     } finally {
       InjectableBehaviourObjectStore.resetGetTableBehaviour(); // reset the behaviour
@@ -598,7 +598,7 @@ public void testIncrementalDumpEmptyDumpDirectory() throws Throwable {
             .run("create external table t1 (id int)")
             .run("insert into table t1 values (1)")
             .run("insert into table t1 values (2)")
-            .dump(primaryDbName, null, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
 
     replica.load(replicatedDbName, tuple.dumpLocation)
             .status(replicatedDbName)
@@ -606,7 +606,7 @@ public void testIncrementalDumpEmptyDumpDirectory() throws Throwable {
 
     // This looks like an empty dump but it has the ALTER TABLE event created by the previous
     // dump. We need it here so that the next dump won't have any events.
-    WarehouseInstance.Tuple incTuple = primary.dump(primaryDbName, tuple.lastReplicationId, dumpWithClause);
+    WarehouseInstance.Tuple incTuple = primary.dump(primaryDbName, dumpWithClause);
     replica.load(replicatedDbName, incTuple.dumpLocation, loadWithClause)
             .status(replicatedDbName)
             .verifyResult(incTuple.lastReplicationId);
@@ -617,7 +617,7 @@ public void testIncrementalDumpEmptyDumpDirectory() throws Throwable {
     WarehouseInstance.Tuple inc2Tuple = primary.run("use " + extraPrimaryDb)
             .run("create table tbl (fld int)")
             .run("use " + primaryDbName)
-            .dump(primaryDbName, incTuple.lastReplicationId, dumpWithClause);
+            .dump(primaryDbName, dumpWithClause);
     Assert.assertEquals(primary.getCurrentNotificationEventId().getEventId(),
                         Long.valueOf(inc2Tuple.lastReplicationId).longValue());
 
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java
index be0d96e682..15cb985b9a 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java
@@ -123,7 +123,7 @@ public void tearDown() throws Throwable {
 
   @Test
   public void testAcidTableIncrementalReplication() throws Throwable {
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootStrapDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId);
@@ -208,14 +208,14 @@ public void testReplCM() throws Throwable {
     String[] result = new String[]{"5"};
 
     WarehouseInstance.Tuple incrementalDump;
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootStrapDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId);
 
     ReplicationTestUtils.insertRecords(primary, primaryDbName, primaryDbNameExtra,
             tableName, null, false, ReplicationTestUtils.OperationType.REPL_TEST_ACID_INSERT);
-    incrementalDump = primary.dump(primaryDbName, bootStrapDump.lastReplicationId);
+    incrementalDump = primary.dump(primaryDbName);
     primary.run("drop table " + primaryDbName + "." + tableName);
     replica.loadWithoutExplain(replicatedDbName, incrementalDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName).verifyResult(incrementalDump.lastReplicationId);
@@ -225,7 +225,7 @@ public void testReplCM() throws Throwable {
 
     ReplicationTestUtils.insertRecords(primary, primaryDbName, primaryDbNameExtra,
             tableNameMM, null, true, ReplicationTestUtils.OperationType.REPL_TEST_ACID_INSERT);
-    incrementalDump = primary.dump(primaryDbName, bootStrapDump.lastReplicationId);
+    incrementalDump = primary.dump(primaryDbName);
     primary.run("drop table " + primaryDbName + "." + tableNameMM);
     replica.loadWithoutExplain(replicatedDbName, incrementalDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName).verifyResult(incrementalDump.lastReplicationId);
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigration.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigration.java
index e094ed23a3..7fa23b1d8b 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigration.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigration.java
@@ -224,7 +224,7 @@ private WarehouseInstance.Tuple prepareDataAndDump(String primaryDbName, String
                 + "stored as avro tblproperties ('avro.schema.url'='" + avroSchemaFile.toUri()
                 .toString() + "')")
         .run("insert into avro_table_part partition (country='india') values ('another', 13)")
-        .dump(primaryDbName, fromReplId);
+        .dump(primaryDbName);
     assertFalse(isTransactionalTable(primary.getTable(primaryDbName, "tacid")));
     assertFalse(isTransactionalTable(primary.getTable(primaryDbName, "tacidpart")));
     assertFalse(isTransactionalTable(primary.getTable(primaryDbName, "tflat")));
@@ -342,7 +342,7 @@ public void testBootstrapLoadMigrationManagedToAcid() throws Throwable {
 
   @Test
   public void testIncrementalLoadMigrationManagedToAcid() throws Throwable {
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation);
     tuple = prepareDataAndDump(primaryDbName, tuple.lastReplicationId);
     replica.load(replicatedDbName, tuple.dumpLocation);
@@ -351,7 +351,7 @@ public void testIncrementalLoadMigrationManagedToAcid() throws Throwable {
 
   @Test
   public void testIncrementalLoadMigrationManagedToAcidFailure() throws Throwable {
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation);
     tuple = prepareDataAndDump(primaryDbName, tuple.lastReplicationId);
     loadWithFailureInAddNotification("tacid", tuple.dumpLocation);
@@ -364,7 +364,7 @@ public void testIncrementalLoadMigrationManagedToAcidFailure() throws Throwable
 
   @Test
   public void testIncrementalLoadMigrationManagedToAcidFailurePart() throws Throwable {
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation);
     tuple = prepareDataAndDump(primaryDbName, tuple.lastReplicationId);
     loadWithFailureInAddNotification("tacidpart", tuple.dumpLocation);
@@ -377,7 +377,7 @@ public void testIncrementalLoadMigrationManagedToAcidFailurePart() throws Throwa
 
   @Test
   public void testIncrementalLoadMigrationManagedToAcidAllOp() throws Throwable {
-    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootStrapDump = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootStrapDump.dumpLocation)
             .run("REPL STATUS " + replicatedDbName)
             .verifyResult(bootStrapDump.lastReplicationId);
@@ -416,14 +416,14 @@ public void testBootstrapConvertedExternalTableAutoPurgeDataOnDrop() throws Thro
                     + "'org.apache.hadoop.hive.serde2.avro.AvroSerDe' stored as avro "
                     + "tblproperties ('avro.schema.url'='" + avroSchemaFile.toUri().toString() + "')")
             .run("insert into avro_tbl partition (country='india') values ('another', 13)")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
 
     replica.load(replicatedDbName, bootstrap.dumpLocation);
     Path dataLocation = assertTablePath(replicatedDbName, "avro_tbl");
 
     WarehouseInstance.Tuple incremental = primary.run("use " + primaryDbName)
             .run("drop table avro_tbl")
-            .dump(primaryDbName, bootstrap.lastReplicationId);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, incremental.dumpLocation);
 
     // After drop, the external table data location should be auto deleted as it is converted one.
@@ -432,7 +432,7 @@ public void testBootstrapConvertedExternalTableAutoPurgeDataOnDrop() throws Thro
 
   @Test
   public void testIncConvertedExternalTableAutoDeleteDataDirOnDrop() throws Throwable {
-    WarehouseInstance.Tuple bootstrap = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple bootstrap = primary.dump(primaryDbName);
     replica.load(replicatedDbName, bootstrap.dumpLocation);
 
     WarehouseInstance.Tuple incremental = primary.run("use " + primaryDbName)
@@ -440,7 +440,7 @@ public void testIncConvertedExternalTableAutoDeleteDataDirOnDrop() throws Throwa
                     + "'org.apache.hadoop.hive.serde2.avro.AvroSerDe' stored as avro "
                     + "tblproperties ('avro.schema.url'='" + avroSchemaFile.toUri().toString() + "')")
             .run("insert into avro_tbl values ('str', 13)")
-            .dump(primaryDbName, bootstrap.lastReplicationId);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, incremental.dumpLocation);
 
     // Data location is valid and is under default external warehouse directory.
@@ -451,7 +451,7 @@ public void testIncConvertedExternalTableAutoDeleteDataDirOnDrop() throws Throwa
 
     incremental = primary.run("use " + primaryDbName)
             .run("drop table avro_tbl")
-            .dump(primaryDbName, incremental.lastReplicationId);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, incremental.dumpLocation);
 
     // After drop, the external table data location should be auto deleted as it is converted one.
@@ -471,7 +471,7 @@ public void testBootstrapLoadMigrationToAcidWithMoveOptimization() throws Throwa
   public void testIncrementalLoadMigrationToAcidWithMoveOptimization() throws Throwable {
     List<String> withConfigs =
             Collections.singletonList("'hive.repl.enable.move.optimization'='true'");
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation);
     tuple = prepareDataAndDump(primaryDbName, tuple.lastReplicationId);
     replica.load(replicatedDbName, tuple.dumpLocation, withConfigs);
@@ -513,7 +513,7 @@ public void testMigrationWithUpgrade() throws Throwable {
             .run("insert into tacid values (3)")
             .run("create table texternal (id int) ")
             .run("insert into texternal values (1)")
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation)
             .run("use " + replicatedDbName)
             .run("repl status " + replicatedDbName)
@@ -557,7 +557,7 @@ public void testMigrationWithUpgrade() throws Throwable {
     withConfigs.add("'hive.repl.dump.include.acid.tables'='true'");
     withConfigs.add("'hive.repl.include.external.tables'='true'");
     withConfigs.add("'hive.distcp.privileged.doAs' = '" + UserGroupInformation.getCurrentUser().getUserName() + "'");
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId, withConfigs);
+    tuple = primary.dump(primaryDbName, withConfigs);
     replica.load(replicatedDbName, tuple.dumpLocation, withConfigs);
     replica.run("use " + replicatedDbName)
             .run("repl status " + replicatedDbName)
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigrationEx.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigrationEx.java
index cefdca620f..425c8cbf42 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigrationEx.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigrationEx.java
@@ -173,7 +173,7 @@ public CurrentNotificationEventId apply(CurrentNotificationEventId id) {
 
     InjectableBehaviourObjectStore.setGetCurrentNotificationEventIdBehaviour(callerVerifier);
     try {
-      return primary.dump(primaryDbName, null);
+      return primary.dump(primaryDbName);
     } finally {
       InjectableBehaviourObjectStore.resetGetCurrentNotificationEventIdBehaviour();
       callerVerifier.assertInjectionsPerformed(true, false);
@@ -191,7 +191,7 @@ public void testConcurrentOpDuringBootStrapDumpCreateTableReplay() throws Throwa
     assertTrue(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
 
     // next incremental dump
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
     verifyLoadExecution(replicatedDbName, tuple.lastReplicationId);
     assertFalse(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
@@ -208,7 +208,7 @@ public void testConcurrentOpDuringBootStrapDumpInsertReplay() throws Throwable {
     assertTrue(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
 
     // next incremental dump
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
     verifyLoadExecution(replicatedDbName, tuple.lastReplicationId);
     assertFalse(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
@@ -219,14 +219,14 @@ public void testTableLevelDumpMigration() throws Throwable {
     WarehouseInstance.Tuple tuple = primary
             .run("use " + primaryDbName)
             .run("create table t1 (i int, j int)")
-            .dump(primaryDbName+".'t1'", null);
+            .dump(primaryDbName+".'t1'");
     replica.run("create database " + replicatedDbName);
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
     assertTrue(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
 
     tuple = primary.run("use " + primaryDbName)
             .run("insert into t1 values (1, 2)")
-            .dump(primaryDbName+".'t1'", tuple.lastReplicationId);
+            .dump(primaryDbName+".'t1'");
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
     assertFalse(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
   }
@@ -256,7 +256,7 @@ public void testConcurrentOpDuringBootStrapDumpInsertOverwrite() throws Throwabl
     assertTrue(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
 
     // next incremental dump
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
     replica.run("use " + replicatedDbName)
             .run("show tables")
@@ -315,7 +315,7 @@ public void testIncLoadPenFlagPropAlterDB() throws Throwable {
     tuple = primary.run("use " + primaryDbName)
             .run("alter database " + primaryDbName + " set dbproperties('dummy_key'='dummy_val')")
            .run("create table tbl_temp (fld int)")
-            .dump(primaryDbName, tuple.lastReplicationId);
+            .dump(primaryDbName);
 
     loadWithFailureInAddNotification("tbl_temp", tuple.dumpLocation);
     Database replDb = replica.getDatabase(replicatedDbName);
@@ -324,7 +324,7 @@ public void testIncLoadPenFlagPropAlterDB() throws Throwable {
     assertTrue(replDb.getParameters().get("dummy_key").equalsIgnoreCase("dummy_val"));
 
     // next incremental dump
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
     assertFalse(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
   }
@@ -342,7 +342,7 @@ public void testIncLoadPenFlagWithMoveOptimization() throws Throwable {
     assertTrue(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
 
     // next incremental dump
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName);
     replica.load(replicatedDbName, tuple.dumpLocation, withClause);
     assertFalse(ReplUtils.isFirstIncPending(replica.getDatabase(replicatedDbName).getParameters()));
   }
@@ -382,20 +382,20 @@ public void testOnwerPropagation() throws Throwable {
 
     // test bootstrap
     alterUserName("hive");
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
     verifyUserName("hive");
 
     // test incremental
     alterUserName("hive1");
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
     verifyUserName("hive1");
   }
 
   @Test
   public void testOnwerPropagationInc() throws Throwable {
-    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName, null);
+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
 
     primary.run("use " + primaryDbName)
@@ -409,7 +409,7 @@ public void testOnwerPropagationInc() throws Throwable {
 
     // test incremental when table is getting created in the same load
     alterUserName("hive");
-    tuple = primary.dump(primaryDbName, tuple.lastReplicationId);
+    tuple = primary.dump(primaryDbName);
     replica.loadWithoutExplain(replicatedDbName, tuple.dumpLocation);
     verifyUserName("hive");
   }
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestScheduledReplicationScenarios.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestScheduledReplicationScenarios.java
new file mode 100644
index 0000000000..c51bec1e27
--- /dev/null
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestScheduledReplicationScenarios.java
@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.parse;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
+import org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder;
+import org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork;
+import org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService;
+import org.apache.hadoop.hive.shims.Utils;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.junit.Before;
+import org.junit.After;
+import org.junit.Test;
+import org.junit.BeforeClass;
+
+import java.util.Map;
+import java.util.HashMap;
+
+
+/**
+ * TestScheduledReplicationScenarios - test scheduled replication .
+ */
+public class TestScheduledReplicationScenarios extends BaseReplicationScenariosAcidTables {
+
+  @BeforeClass
+  public static void classLevelSetup() throws Exception {
+    Map<String, String> overrides = new HashMap<>();
+    overrides.put(MetastoreConf.ConfVars.EVENT_MESSAGE_FACTORY.getHiveName(),
+        GzipJSONMessageEncoder.class.getCanonicalName());
+    overrides.put(HiveConf.ConfVars.HIVE_SCHEDULED_QUERIES_EXECUTOR_IDLE_SLEEP_TIME.varname, "1s");
+    overrides.put(HiveConf.ConfVars.HIVE_SCHEDULED_QUERIES_EXECUTOR_PROGRESS_REPORT_INTERVAL.varname,
+            "1s");
+    overrides.put(HiveConf.ConfVars.REPL_INCLUDE_EXTERNAL_TABLES.varname, "true");
+    overrides.put(HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER.varname,
+            UserGroupInformation.getCurrentUser().getUserName());
+    internalBeforeClassSetup(overrides, TestScheduledReplicationScenarios.class);
+  }
+
+  static void internalBeforeClassSetup(Map<String, String> overrides,
+      Class clazz) throws Exception {
+
+    conf = new HiveConf(clazz);
+    conf.set("dfs.client.use.datanode.hostname", "true");
+    conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
+    MiniDFSCluster miniDFSCluster =
+        new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+    Map<String, String> acidEnableConf = new HashMap<String, String>() {{
+        put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());
+        put("hive.support.concurrency", "true");
+        put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
+        put("hive.metastore.client.capability.check", "false");
+        put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
+        put("hive.strict.checks.bucketing", "false");
+        put("hive.mapred.mode", "nonstrict");
+        put("mapred.input.dir.recursive", "true");
+        put("hive.metastore.disallow.incompatible.col.type.changes", "false");
+        put("hive.in.repl.test", "true");
+      }};
+
+    acidEnableConf.putAll(overrides);
+
+    primary = new WarehouseInstance(LOG, miniDFSCluster, acidEnableConf);
+    replica = new WarehouseInstance(LOG, miniDFSCluster, acidEnableConf);
+    Map<String, String> overridesForHiveConf1 = new HashMap<String, String>() {{
+          put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());
+          put("hive.support.concurrency", "false");
+          put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager");
+          put("hive.metastore.client.capability.check", "false");
+      }};
+    replicaNonAcid = new WarehouseInstance(LOG, miniDFSCluster, overridesForHiveConf1);
+  }
+
+  @Before
+  public void setup() throws Throwable {
+    super.setup();
+  }
+
+  @After
+  public void tearDown() throws Throwable {
+    primary.run("drop database if exists " + primaryDbName + " cascade");
+    replica.run("drop database if exists " + replicatedDbName + " cascade");
+    replicaNonAcid.run("drop database if exists " + replicatedDbName + " cascade");
+    primary.run("drop database if exists " + primaryDbName + "_extra cascade");
+  }
+
+  @Test
+  public void testAcidTablesBootstrapIncr() throws Throwable {
+    // Bootstrap
+    primary.run("use " + primaryDbName)
+            .run("create table t1 (id int) clustered by(id) into 3 buckets stored as orc " +
+                      "tblproperties (\"transactional\"=\"true\")")
+            .run("insert into t1 values(1)")
+            .run("insert into t1 values(2)");
+    try (ScheduledQueryExecutionService schqS =
+                 ScheduledQueryExecutionService.startScheduledQueryExecutorService(primary.hiveConf)) {
+      int next = 0;
+      ReplDumpWork.injectNextDumpDirForTest(String.valueOf(next));
+      primary.run("create scheduled query s1 every 10 minutes as repl dump " + primaryDbName);
+      primary.run("alter scheduled query s1 execute");
+      Thread.sleep(6000);
+      Path dumpRoot = new Path(primary.hiveConf.getVar(HiveConf.ConfVars.REPLDIR), primaryDbName.toLowerCase());
+      Path currdumpRoot = new Path(dumpRoot, String.valueOf(next));
+      replica.load(replicatedDbName, currdumpRoot.toString());
+      replica.run("use " + replicatedDbName)
+              .run("show tables like 't1'")
+              .verifyResult("t1")
+              .run("select id from t1 order by id")
+              .verifyResults(new String[]{"1", "2"});
+
+    // First incremental, after bootstrap
+
+      primary.run("use " + primaryDbName)
+            .run("insert into t1 values(3)")
+            .run("insert into t1 values(4)");
+      next++;
+      ReplDumpWork.injectNextDumpDirForTest(String.valueOf(next));
+      primary.run("alter scheduled query s1 execute");
+      Thread.sleep(20000);
+      Path incrdumpRoot = new Path(dumpRoot, String.valueOf(next));
+      replica.load(replicatedDbName, incrdumpRoot.toString());
+      replica.run("use " + replicatedDbName)
+              .run("show tables like 't1'")
+              .verifyResult("t1")
+              .run("select id from t1 order by id")
+              .verifyResults(new String[]{"1", "2", "3", "4"})
+              .run("drop table t1");
+
+
+    } finally {
+      primary.run("drop scheduled query s1");
+    }
+  }
+}
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenarios.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenarios.java
index 94eb1ffa87..44a3805dee 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenarios.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenarios.java
@@ -318,7 +318,7 @@ private String dumpLoadVerify(List<String> tableNames, String lastReplicationId,
 
     // Take dump
     WarehouseInstance.Tuple dumpTuple = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, lastReplicationId, withClauseList);
+            .dump(primaryDbName, withClauseList);
 
 
     // Load, if necessary changing configuration.
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java
index 78f505b65c..15b6c3dda1 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java
@@ -153,7 +153,52 @@ private String replicateAndVerify(String replPolicy, String oldReplPolicy, Strin
     if (lastReplId == null) {
       replica.run("drop database if exists " + replicatedDbName + " cascade");
     }
-    WarehouseInstance.Tuple tuple = primary.dump(replPolicy, oldReplPolicy, lastReplId, dumpWithClause);
+
+    WarehouseInstance.Tuple tuple = primary.dump(replPolicy, oldReplPolicy, dumpWithClause);
+
+    if (bootstrappedTables != null) {
+      verifyBootstrapDirInIncrementalDump(tuple.dumpLocation, bootstrappedTables);
+    }
+
+    // If the policy contains '.'' means its table level replication.
+    verifyTableListForPolicy(tuple.dumpLocation, replPolicy.contains(".'") ? expectedTables : null);
+
+    replica.load(replicatedDbName, tuple.dumpLocation, loadWithClause)
+            .run("use " + replicatedDbName)
+            .run("show tables")
+            .verifyResults(expectedTables)
+            .verifyReplTargetProperty(replicatedDbName);
+
+    if (records == null) {
+      records = new String[] {"1"};
+    }
+    for (String table : expectedTables) {
+      replica.run("use " + replicatedDbName)
+              .run("select a from " + table)
+              .verifyResults(records);
+    }
+    return tuple.lastReplicationId;
+  }
+
+  private String replicateAndVerifyClearDump(String replPolicy, String oldReplPolicy, String lastReplId,
+                                    List<String> dumpWithClause,
+                                    List<String> loadWithClause,
+                                    String[] bootstrappedTables,
+                                    String[] expectedTables,
+                                    String[] records) throws Throwable {
+    if (dumpWithClause == null) {
+      dumpWithClause = new ArrayList<>();
+    }
+    if (loadWithClause == null) {
+      loadWithClause = new ArrayList<>();
+    }
+
+    // For bootstrap replication, drop the target database before triggering it.
+    if (lastReplId == null) {
+      replica.run("drop database if exists " + replicatedDbName + " cascade");
+    }
+
+    WarehouseInstance.Tuple tuple = primary.dump(replPolicy, oldReplPolicy, dumpWithClause);
 
     if (bootstrappedTables != null) {
       verifyBootstrapDirInIncrementalDump(tuple.dumpLocation, bootstrappedTables);
@@ -176,6 +221,7 @@ private String replicateAndVerify(String replPolicy, String oldReplPolicy, Strin
               .run("select a from " + table)
               .verifyResults(records);
     }
+    new Path(tuple.dumpLocation).getFileSystem(conf).delete(new Path(tuple.dumpLocation), true);
     return tuple.lastReplicationId;
   }
 
@@ -263,7 +309,7 @@ public void testBasicBootstrapWithIncludeAndExcludeList() throws Throwable {
   @Test
   public void testBasicIncrementalWithIncludeList() throws Throwable {
     WarehouseInstance.Tuple tupleBootstrap = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, tupleBootstrap.dumpLocation);
 
     String[] originalNonAcidTables = new String[] {"t1", "t2"};
@@ -282,7 +328,7 @@ public void testBasicIncrementalWithIncludeList() throws Throwable {
   @Test
   public void testBasicIncrementalWithIncludeAndExcludeList() throws Throwable {
     WarehouseInstance.Tuple tupleBootstrap = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, tupleBootstrap.dumpLocation);
 
     String[] originalTables = new String[] {"t1", "t11", "t2", "t3", "t111"};
@@ -326,7 +372,7 @@ public void testIncorrectTablePolicyInReplDump() throws Throwable {
     // Test incremental replication with invalid replication policies in REPLACE clause.
     String replPolicy = primaryDbName;
     WarehouseInstance.Tuple tupleBootstrap = primary.run("use " + primaryDbName)
-            .dump(primaryDbName, null);
+            .dump(primaryDbName);
     replica.load(replicatedDbName, tupleBootstrap.dumpLocation);
     String lastReplId = tupleBootstrap.lastReplicationId;
     for (String oldReplPolicy : invalidReplPolicies) {
@@ -386,7 +432,8 @@ public void testFullDbBootstrapReplicationWithDifferentReplPolicyFormats() throw
 
     // Replicate and verify if all 3 tables are replicated to target.
     for (String replPolicy : fullDbReplPolicies) {
-      replicateAndVerify(replPolicy, null, null, null, originalTables);
+      replicateAndVerifyClearDump(replPolicy, null, null, null,
+              null, null, originalTables, null);
     }
   }
 
@@ -447,7 +494,7 @@ public void testBootstrapExternalTablesWithIncludeAndExcludeList() throws Throwa
     String replPolicy = primaryDbName + ".'(a[0-9]+)|(b2)'.'a1'";
     String[] replicatedTables = new String[] {"a2", "b2"};
     WarehouseInstance.Tuple tuple = primary.run("use " + primaryDbName)
-            .dump(replPolicy, null, dumpWithClause);
+            .dump(replPolicy, dumpWithClause);
 
     // the _external_tables_file info should be created as external tables are to be replicated.
     Assert.assertTrue(primary.miniDFSCluster.getFileSystem()
@@ -486,7 +533,7 @@ public void testBootstrapExternalTablesIncrementalPhaseWithIncludeAndExcludeList
     dumpWithClause = Arrays.asList("'" + HiveConf.ConfVars.REPL_INCLUDE_EXTERNAL_TABLES.varname + "'='true'",
             "'" + HiveConf.ConfVars.REPL_BOOTSTRAP_EXTERNAL_TABLES.varname + "'='true'");
     WarehouseInstance.Tuple tuple = primary.run("use " + primaryDbName)
-            .dump(replPolicy, lastReplId, dumpWithClause);
+            .dump(replPolicy, dumpWithClause);
 
     // the _external_tables_file info should be created as external tables are to be replicated.
     Assert.assertTrue(primary.miniDFSCluster.getFileSystem()
@@ -635,7 +682,7 @@ public void testReplacePolicyOnBootstrapExternalTablesIncrementalPhase() throws
     dumpWithClause = Arrays.asList("'" + HiveConf.ConfVars.REPL_INCLUDE_EXTERNAL_TABLES.varname + "'='true'",
             "'" + HiveConf.ConfVars.REPL_BOOTSTRAP_EXTERNAL_TABLES.varname + "'='true'");
     WarehouseInstance.Tuple tuple = primary.run("use " + primaryDbName)
-            .dump(replPolicy, oldReplPolicy, lastReplId, dumpWithClause);
+            .dump(replPolicy, oldReplPolicy, dumpWithClause);
 
     // the _external_tables_file info should be created as external tables are to be replicated.
     Assert.assertTrue(primary.miniDFSCluster.getFileSystem()
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java
index 43effeb64e..f1eba52648 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java
@@ -262,29 +262,33 @@ WarehouseInstance runFailure(String command, int errorCode) throws Throwable {
     }
   }
 
-  Tuple dump(String dbName, String lastReplicationId, List<String> withClauseOptions)
+  Tuple dump(String dbName)
+          throws Throwable {
+    return dump(dbName, Collections.emptyList());
+  }
+
+  Tuple dump(String dbName, List<String> withClauseOptions)
       throws Throwable {
     String dumpCommand =
-        "REPL DUMP " + dbName + (lastReplicationId == null ? "" : " FROM " + lastReplicationId);
-    if (!withClauseOptions.isEmpty()) {
+        "REPL DUMP " + dbName;
+    if (withClauseOptions != null && !withClauseOptions.isEmpty()) {
       dumpCommand += " with (" + StringUtils.join(withClauseOptions, ",") + ")";
     }
-    return dump(dumpCommand);
+    return dumpWithCommand(dumpCommand);
   }
 
-  Tuple dump(String replPolicy, String oldReplPolicy, String lastReplicationId, List<String> withClauseOptions)
+  Tuple dump(String replPolicy, String oldReplPolicy, List<String> withClauseOptions)
           throws Throwable {
     String dumpCommand =
             "REPL DUMP " + replPolicy
-                    + (oldReplPolicy == null ? "" : " REPLACE " + oldReplPolicy)
-                    + (lastReplicationId == null ? "" : " FROM " + lastReplicationId);
+                    + (oldReplPolicy == null ? "" : " REPLACE " + oldReplPolicy);
     if (!withClauseOptions.isEmpty()) {
       dumpCommand += " with (" + StringUtils.join(withClauseOptions, ",") + ")";
     }
-    return dump(dumpCommand);
+    return dumpWithCommand(dumpCommand);
   }
 
-  Tuple dump(String dumpCommand) throws Throwable {
+  Tuple dumpWithCommand(String dumpCommand) throws Throwable {
     advanceDumpDir();
     run(dumpCommand);
     String dumpLocation = row0Result(0, false);
@@ -292,13 +296,9 @@ Tuple dump(String dumpCommand) throws Throwable {
     return new Tuple(dumpLocation, lastDumpId);
   }
 
-  Tuple dump(String dbName, String lastReplicationId) throws Throwable {
-    return dump(dbName, lastReplicationId, Collections.emptyList());
-  }
-
-  WarehouseInstance dumpFailure(String dbName, String lastReplicationId) throws Throwable {
+  WarehouseInstance dumpFailure(String dbName) throws Throwable {
     String dumpCommand =
-            "REPL DUMP " + dbName + (lastReplicationId == null ? "" : " FROM " + lastReplicationId);
+            "REPL DUMP " + dbName;
     advanceDumpDir();
     runFailure(dumpCommand);
     return this;
diff --git a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
index 7e0a7f2022..914147f38a 100644
--- a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
+++ b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
@@ -2886,7 +2886,7 @@ public void testGetQueryLogForReplCommands() throws Exception {
       // Incremental dump
       stmt = (HiveStatement) con.createStatement();
       advanceDumpDir();
-      replDumpRslt = stmt.executeQuery("repl dump " + primaryDb + " from " + lastReplId +
+      replDumpRslt = stmt.executeQuery("repl dump " + primaryDb +
               " with ('hive.repl.rootdir' = '" + replDir + "')");
       assertTrue(replDumpRslt.next());
       dumpLocation = replDumpRslt.getString(1);
diff --git a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java
index 79beadd6fb..a7c905be35 100644
--- a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java
+++ b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java
@@ -1453,7 +1453,7 @@ public void testReplDumpResultSet() throws Exception {
         TestJdbcWithMiniHS2.class.getCanonicalName().toLowerCase().replace('.', '_') + "_"
             + System.currentTimeMillis();
     String testPathName = System.getProperty("test.warehouse.dir", "/tmp") + Path.SEPARATOR + tid;
-    Path testPath = new Path(testPathName);
+    Path testPath = new Path(testPathName + Path.SEPARATOR + testDbName);
     FileSystem fs = testPath.getFileSystem(new HiveConf());
     Statement stmt = conDefault.createStatement();
     try {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
index 977abb74cc..f5eea15e8b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.exec.repl;
 
 import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.FileUtils;
@@ -75,13 +76,14 @@
 import java.io.IOException;
 import java.io.Serializable;
 import java.nio.charset.StandardCharsets;
-import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 import java.util.UUID;
 import java.util.HashSet;
 import java.util.Set;
+import java.util.Comparator;
+import java.util.ArrayList;
 import java.util.concurrent.TimeUnit;
 import static org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.Writer;
 
@@ -119,18 +121,21 @@ public String getName() {
   public int execute() {
     try {
       Hive hiveDb = getHive();
-      Path dumpRoot = new Path(conf.getVar(HiveConf.ConfVars.REPLDIR), getNextDumpDir());
-      DumpMetaData dmd = new DumpMetaData(dumpRoot, conf);
+      Path dumpRoot = new Path(conf.getVar(HiveConf.ConfVars.REPLDIR), work.dbNameOrPattern.toLowerCase());
+      Path currentDumpPath = new Path(dumpRoot, getNextDumpDir());
+      DumpMetaData dmd = new DumpMetaData(currentDumpPath, conf);
       // Initialize ReplChangeManager instance since we will require it to encode file URI.
       ReplChangeManager.getInstance(conf);
       Path cmRoot = new Path(conf.getVar(HiveConf.ConfVars.REPLCMDIR));
       Long lastReplId;
-      if (work.isBootStrapDump()) {
-        lastReplId = bootStrapDump(dumpRoot, dmd, cmRoot, hiveDb);
+      if (!dumpRoot.getFileSystem(conf).exists(dumpRoot)
+              || dumpRoot.getFileSystem(conf).listStatus(dumpRoot).length == 0) {
+        lastReplId = bootStrapDump(currentDumpPath, dmd, cmRoot, hiveDb);
       } else {
-        lastReplId = incrementalDump(dumpRoot, dmd, cmRoot, hiveDb);
+        work.setEventFrom(getEventFromPreviousDumpMetadata(dumpRoot));
+        lastReplId = incrementalDump(currentDumpPath, dmd, cmRoot, hiveDb);
       }
-      prepareReturnValues(Arrays.asList(dumpRoot.toUri().toString(), String.valueOf(lastReplId)));
+      prepareReturnValues(Arrays.asList(currentDumpPath.toUri().toString(), String.valueOf(lastReplId)));
     } catch (Exception e) {
       LOG.error("failed", e);
       setException(e);
@@ -139,6 +144,26 @@ public int execute() {
     return 0;
   }
 
+  private Long getEventFromPreviousDumpMetadata(Path dumpRoot) throws IOException, SemanticException {
+    FileStatus[] statuses = dumpRoot.getFileSystem(conf).listStatus(dumpRoot);
+    if (statuses.length > 0) {
+      //sort based on last modified. Recent one is at the top
+      Arrays.sort(statuses, new Comparator<FileStatus>() {
+        public int compare(FileStatus f1, FileStatus f2) {
+          return Long.compare(f2.getModificationTime(), f1.getModificationTime());
+        }
+      });
+      FileStatus recentDump = statuses[0];
+      DumpMetaData dmd = new DumpMetaData(recentDump.getPath(), conf);
+      if (dmd.isIncrementalDump()) {
+        return dmd.getEventTo();
+      }
+      //bootstrap case return event from
+      return dmd.getEventFrom();
+    }
+    return 0L;
+  }
+
   private void prepareReturnValues(List<String> values) throws SemanticException {
     LOG.debug("prepareReturnValues : " + dumpSchema);
     for (String s : values) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpWork.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpWork.java
index 7bae9ac66d..9b11bae6ba 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpWork.java
@@ -32,32 +32,25 @@ public class ReplDumpWork implements Serializable {
   final ReplScope replScope;
   final ReplScope oldReplScope;
   final String dbNameOrPattern, astRepresentationForErrorMsg, resultTempPath;
-  final Long eventFrom;
   Long eventTo;
-  private Integer maxEventLimit;
+  Long eventFrom;
   static String testInjectDumpDir = null;
+  private Integer maxEventLimit;
 
   public static void injectNextDumpDirForTest(String dumpDir) {
     testInjectDumpDir = dumpDir;
   }
 
   public ReplDumpWork(ReplScope replScope, ReplScope oldReplScope,
-                      Long eventFrom, Long eventTo, String astRepresentationForErrorMsg, Integer maxEventLimit,
+                      String astRepresentationForErrorMsg,
                       String resultTempPath) {
     this.replScope = replScope;
     this.oldReplScope = oldReplScope;
     this.dbNameOrPattern = replScope.getDbName();
-    this.eventFrom = eventFrom;
-    this.eventTo = eventTo;
     this.astRepresentationForErrorMsg = astRepresentationForErrorMsg;
-    this.maxEventLimit = maxEventLimit;
     this.resultTempPath = resultTempPath;
   }
 
-  boolean isBootStrapDump() {
-    return eventFrom == null;
-  }
-
   int maxEventLimit() throws Exception {
     if (eventTo < eventFrom) {
       throw new Exception("Invalid event ID input received in TO clause");
@@ -69,6 +62,10 @@ int maxEventLimit() throws Exception {
     return maxEventLimit;
   }
 
+  void setEventFrom(long eventId) {
+    eventFrom = eventId;
+  }
+
   // Override any user specification that changes the last event to be dumped.
   void overrideLastEventToDump(Hive fromDb, long bootstrapLastId) throws Exception {
     // If we are bootstrapping ACID tables, we need to dump all the events upto the event id at
@@ -77,7 +74,6 @@ void overrideLastEventToDump(Hive fromDb, long bootstrapLastId) throws Exception
     // bootstrampDump() for more details.
     if (bootstrapLastId > 0) {
       eventTo = bootstrapLastId;
-      maxEventLimit = null;
       LoggerFactory.getLogger(this.getClass())
               .debug("eventTo restricted to event id : {} because of bootstrap of ACID tables",
                       eventTo);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java
index 810a4c5284..2243cb69b7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java
@@ -56,25 +56,18 @@
 import static org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.Reader;
 import static org.apache.hadoop.hive.ql.exec.repl.ExternalTableCopyTaskBuilder.DirCopyWork;
 import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_DBNAME;
-import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_FROM;
-import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_LIMIT;
 import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_REPLACE;
 import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_REPL_CONFIG;
 import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_REPL_DUMP;
 import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_REPL_LOAD;
 import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_REPL_STATUS;
 import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_REPL_TABLES;
-import static org.apache.hadoop.hive.ql.parse.HiveParser.TOK_TO;
 
 public class ReplicationSemanticAnalyzer extends BaseSemanticAnalyzer {
   // Replication Scope
   private ReplScope replScope = new ReplScope();
   private ReplScope oldReplScope = null;
 
-  private Long eventFrom;
-  private Long eventTo;
-  private Integer maxEventLimit;
-
   // Base path for REPL LOAD
   private String path;
   // Added conf member to set the REPL command specific config entries without affecting the configs
@@ -207,27 +200,6 @@ private void initReplDump(ASTNode ast) throws HiveException {
       case TOK_REPLACE:
         setOldReplPolicy(currNode);
         break;
-      case TOK_FROM:
-        // TOK_FROM subtree
-        Tree fromNode = currNode;
-        eventFrom = Long.parseLong(PlanUtils.stripQuotes(fromNode.getChild(0).getText()));
-
-        // Skip the first, which is always required
-        int fromChildIdx = 1;
-        while (fromChildIdx < fromNode.getChildCount()) {
-          if (fromNode.getChild(fromChildIdx).getType() == TOK_TO) {
-            eventTo = Long.parseLong(PlanUtils.stripQuotes(fromNode.getChild(fromChildIdx + 1).getText()));
-            // Skip the next child, since we already took care of it
-            fromChildIdx++;
-          } else if (fromNode.getChild(fromChildIdx).getType() == TOK_LIMIT) {
-            maxEventLimit = Integer.parseInt(PlanUtils.stripQuotes(fromNode.getChild(fromChildIdx + 1).getText()));
-            // Skip the next child, since we already took care of it
-            fromChildIdx++;
-          }
-          // move to the next child in FROM tree
-          fromChildIdx++;
-        }
-        break;
       default:
         throw new SemanticException("Unrecognized token " + currNode.getType() + " in REPL DUMP statement.");
       }
@@ -263,10 +235,7 @@ private void analyzeReplDump(ASTNode ast) throws SemanticException {
           .get(new ReplDumpWork(
               replScope,
               oldReplScope,
-              eventFrom,
-              eventTo,
               ASTErrorUtils.getMsg(ErrorMsg.INVALID_PATH.getMsg(), ast),
-              maxEventLimit,
               ctx.getResFile().toUri().toString()
       ), conf);
       rootTasks.add(replDumpWorkTask);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/repl/TestReplDumpTask.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/repl/TestReplDumpTask.java
index aacd29591d..9f90a364a1 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/repl/TestReplDumpTask.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/repl/TestReplDumpTask.java
@@ -139,9 +139,7 @@ void dumpTable(String dbName, String tblName, String validTxnList, Path dbRoot,
 
     task.initialize(queryState, null, null, null);
     task.setWork(
-        new ReplDumpWork(replScope, null,
-            Long.MAX_VALUE, Long.MAX_VALUE, "",
-            Integer.MAX_VALUE, "")
+        new ReplDumpWork(replScope, null, "", "")
     );
 
     try {
diff --git a/ql/src/test/queries/clientnegative/repl_dump_requires_admin.q b/ql/src/test/queries/clientnegative/repl_dump_requires_admin.q
index 9633a79cce..e1a6153265 100644
--- a/ql/src/test/queries/clientnegative/repl_dump_requires_admin.q
+++ b/ql/src/test/queries/clientnegative/repl_dump_requires_admin.q
@@ -26,7 +26,7 @@ show role grant user hive_admin_user;
 show tables test_repldump_adminpriv;
 repl dump test_repldump_adminpriv;
 
-dfs -rmr  ${system:test.tmp.dir}/hrepl/next;
+dfs -rmr  ${system:test.tmp.dir}/hrepl/test_repldump_adminpriv/next;
 
 set user.name=ruser1;
 show tables test_repldump_adminpriv;
diff --git a/ql/src/test/queries/clientnegative/repl_load_requires_admin.q b/ql/src/test/queries/clientnegative/repl_load_requires_admin.q
index f4395aa3ea..921b50b0b0 100644
--- a/ql/src/test/queries/clientnegative/repl_load_requires_admin.q
+++ b/ql/src/test/queries/clientnegative/repl_load_requires_admin.q
@@ -29,10 +29,10 @@ show tables test_replload_adminpriv_src;
 repl dump test_replload_adminpriv_src;
 
 -- repl load as admin should succeed
-repl load test_replload_adminpriv_tgt1 from '${system:test.tmp.dir}/hrepl/next/';
+repl load test_replload_adminpriv_tgt1 from '${system:test.tmp.dir}/hrepl/test_replload_adminpriv_src/next/';
 show tables test_replload_adminpriv_tgt1;
 
 set user.name=ruser1;
 
 -- repl load as non-admin should fail
-repl load test_replload_adminpriv_tgt2 from '${system:test.tmp.dir}/hrepl/next';
+repl load test_replload_adminpriv_tgt2 from '${system:test.tmp.dir}/hrepl/test_replload_adminpriv_src/next';
