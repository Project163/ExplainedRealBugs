diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatRecordReader.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatRecordReader.java
index 29fc154686..4550a794cb 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatRecordReader.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatRecordReader.java
@@ -226,7 +226,7 @@ public void close() throws IOException {
   /**
    * Tracks number of of errors in input and throws a Runtime exception
    * if the rate of errors crosses a limit.
-   * <br/>
+   * <br>
    * The intention is to skip over very rare file corruption or incorrect
    * input, but catch programmer errors (incorrect format, or incorrect
    * deserializers etc).
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rebuild/AlterMaterializedViewRebuildAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rebuild/AlterMaterializedViewRebuildAnalyzer.java
index 823e574cb4..c0990e8c56 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rebuild/AlterMaterializedViewRebuildAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rebuild/AlterMaterializedViewRebuildAnalyzer.java
@@ -46,16 +46,22 @@
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveTezModelRelMetadataProvider;
 import org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable;
+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter;
+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan;
+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveUnion;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInBetweenExpandRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveAggregateInsertDeleteIncrementalRewritingRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveAggregateInsertIncrementalRewritingRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveAggregatePartitionIncrementalRewritingRule;
+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveAugmentMaterializationRule;
+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveAugmentSnapshotMaterializationRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveInsertOnlyScanWriteIdRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveJoinInsertIncrementalRewritingRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializationRelMetadataProvider;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewUtils;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HivePushdownSnapshotFilterRule;
+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveRowIsDeletedPropagator;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.MaterializedViewRewritingRelVisitor;
 import org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveIncrementalRelMdRowCount;
 import org.apache.hadoop.hive.ql.parse.ASTNode;
@@ -65,6 +71,7 @@
 import org.apache.hadoop.hive.ql.parse.ParseDriver;
 import org.apache.hadoop.hive.ql.parse.ParseUtils;
 import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hadoop.hive.ql.plan.mapper.StatsSource;
@@ -81,8 +88,48 @@
 import static java.util.Collections.singletonList;
 
 /**
- * Analyzer for alter materialized view rebuild commands.
+ * Semantic analyzer for alter materialized view rebuild commands.
+ * This subclass of {@link SemanticAnalyzer} generates a plan which is derived from the materialized view definition
+ * query plan.
+ * <br>
+ * Steps:
+ * <ul>
+ *    <li>Take the Calcite plan of the materialized view definition query.</li>
+ *    <li>Using the snapshot data in materialized view metadata insert A {@link HiveFilter} operator on top of each
+ *    {@link HiveTableScan} operator. The condition has a predicate like ROW_ID.writeid &lt;= high_watermark
+ *    This step is done by {@link HiveAugmentMaterializationRule} or {@link HiveAugmentSnapshotMaterializationRule}.
+ *    The resulting plan should produce the current result of the materialized view, the one which was created at last
+ *    rebuild.</li>
+ *    <li>Transform the original view definition query plan using
+ *    <a href="https://calcite.apache.org/docs/materialized_views.html#union-rewriting">Union rewrite</a> or
+ *    <a href="https://calcite.apache.org/docs/materialized_views.html#union-rewriting-with-aggregate">Union rewrite with aggregate</a> and
+ *    the augmented plan. The result plan has a {@link HiveUnion} operator on top with two branches
+ *    <ul>
+ *      <li>Scan the materialize view for existing records</li>
+ *      <li>A plan which is derived from the augmented materialized view definition query plan. This produces the
+ *      newly inserted records</li>
+ *    </ul>
+ *    </li>
+ *    <li>Transform the plan into incremental rebuild plan if possible:
+ *    <ul>
+ *      <li>The materialized view definition query has aggregate and base tables has insert operations only.
+ *      {@link HiveAggregateInsertIncrementalRewritingRule}</li>
+ *      <li>The materialized view definition query hasn't got aggregate and base tables has insert operations only.
+ *      {@link HiveJoinInsertIncrementalRewritingRule}</li>
+ *      <li>The materialized view definition query has aggregate and any base tables has delete operations.
+ *      {@link HiveAggregateInsertDeleteIncrementalRewritingRule}</li>
+ *      <li>The materialized view definition query hasn't got aggregate and any base tables has delete operations.
+ *      Incremental rebuild is not possible because all records from all source tables need a unique identifier to
+ *      join it with the corresponding record exists in the view. ROW__ID can not be used because it's writedId
+ *      component is changed at delete and unique and primary key constraints are not enforced in Hive.
+ *      </li>
+ *    </ul>
+ *    When any base tables has delete operations the {@link HiveTableScan} operators are fetching the deleted rows too
+ *    and {@link HiveRowIsDeletedPropagator} ensures that extra filter conditions are added to address these.
+ *    </li>
+ * </ul>
  */
+
 @DDLType(types = HiveParser.TOK_ALTER_MATERIALIZED_VIEW_REBUILD)
 public class AlterMaterializedViewRebuildAnalyzer extends CalcitePlanner {
   private static final Logger LOG = LoggerFactory.getLogger(AlterMaterializedViewRebuildAnalyzer.class);
@@ -272,7 +319,12 @@ protected RelNode applyMaterializedViewRewriting(RelOptPlanner planner, RelNode
         }
 
         RelNode incrementalRebuildPlan = applyRecordIncrementalRebuildPlan(
-                basePlan, mdProvider, executorProvider, optCluster, calcitePreMVRewritingPlan, materialization);
+                basePlan,
+                mdProvider,
+                executorProvider,
+                optCluster,
+                calcitePreMVRewritingPlan,
+                materialization);
 
         if (mvRebuildMode != MaterializationRebuildMode.INSERT_OVERWRITE_REBUILD) {
           return incrementalRebuildPlan;
@@ -287,18 +339,19 @@ protected RelNode applyMaterializedViewRewriting(RelOptPlanner planner, RelNode
     }
 
     private RelNode applyRecordIncrementalRebuildPlan(
-            RelNode basePlan,
-            RelMetadataProvider mdProvider,
-            RexExecutor executorProvider,
-            RelOptCluster optCluster,
-            RelNode calcitePreMVRewritingPlan,
-            HiveRelOptMaterialization materialization) {
+        RelNode basePlan,
+        RelMetadataProvider mdProvider,
+        RexExecutor executorProvider,
+        RelOptCluster optCluster,
+        RelNode calcitePreMVRewritingPlan,
+        HiveRelOptMaterialization materialization) {
       // First we need to check if it is valid to convert to MERGE/INSERT INTO.
       // If we succeed, we modify the plan and afterwards the AST.
       // MV should be an acid table.
       boolean acidView = AcidUtils.isFullAcidTable(mvTable.getTTable())
               || AcidUtils.isNonNativeAcidTable(mvTable);
-      MaterializedViewRewritingRelVisitor visitor = new MaterializedViewRewritingRelVisitor(acidView);
+      MaterializedViewRewritingRelVisitor visitor =
+          new MaterializedViewRewritingRelVisitor(acidView);
       visitor.go(basePlan);
       if (visitor.isRewritingAllowed()) {
         if (!materialization.isSourceTablesUpdateDeleteModified()) {
@@ -309,17 +362,10 @@ private RelNode applyRecordIncrementalRebuildPlan(
             return applyJoinInsertIncremental(basePlan, mdProvider, executorProvider);
           }
         } else {
-          if (acidView) {
-            if (visitor.isContainsAggregate()) {
-              if (visitor.getCountIndex() < 0) {
-                // count(*) is necessary for determine which rows should be deleted from the view
-                // if view definition does not have it incremental rebuild can not be performed, bail out
-                return calcitePreMVRewritingPlan;
-              }
-              return applyAggregateInsertDeleteIncremental(basePlan, mdProvider, executorProvider);
-            } else {
-              return calcitePreMVRewritingPlan;
-            }
+          // count(*) is necessary for determine which rows should be deleted from the view
+          // if view definition does not have it incremental rebuild can not be performed
+          if (acidView && visitor.isContainsAggregate() && visitor.getCountIndex() >= 0) {
+            return applyAggregateInsertDeleteIncremental(basePlan, mdProvider, executorProvider);
           } else {
             return calcitePreMVRewritingPlan;
           }
@@ -376,9 +422,9 @@ private RelNode applyJoinInsertIncremental(
     }
 
     private RelNode applyPartitionIncrementalRebuildPlan(
-            RelNode basePlan, RelMetadataProvider mdProvider, RexExecutor executorProvider,
-            HiveRelOptMaterialization materialization, RelOptCluster optCluster,
-            RelNode calcitePreMVRewritingPlan) {
+        RelNode basePlan, RelMetadataProvider mdProvider, RexExecutor executorProvider,
+        HiveRelOptMaterialization materialization, RelOptCluster optCluster,
+        RelNode calcitePreMVRewritingPlan) {
 
       if (materialization.isSourceTablesUpdateDeleteModified()) {
         // TODO: Create rewrite rule to transform the plan to partition based incremental rebuild
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveAggregateInsertDeleteIncrementalRewritingRule.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveAggregateInsertDeleteIncrementalRewritingRule.java
index 9005f643e2..6a826a15b1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveAggregateInsertDeleteIncrementalRewritingRule.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveAggregateInsertDeleteIncrementalRewritingRule.java
@@ -30,6 +30,7 @@
 import org.apache.calcite.sql.SqlKind;
 import org.apache.calcite.sql.fun.SqlStdOperatorTable;
 import org.apache.calcite.tools.RelBuilder;
+import org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer;
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories;
 
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveHepExtractRelNodeRule;
@@ -38,7 +39,8 @@
  * This rule will perform a rewriting to prepare the plan for incremental
  * view maintenance in case there exist aggregation operator, so we can
  * avoid the INSERT OVERWRITE and use a MERGE statement instead.
- *
+ * <br>
+ * <pre>
  * In particular, the INSERT OVERWRITE maintenance will look like this
  * (in SQL):
  * INSERT OVERWRITE mv
@@ -52,8 +54,9 @@
  *   WHERE TAB_A.ROW_ID &gt; 5
  *   GROUP BY a, b) inner_subq
  * GROUP BY a, b;
- *
+ * </pre>
  * We need to transform that into:
+ * <pre>
  * MERGE INTO mv
  * USING (
  *   SELECT a, b, SUM(x) AS s, COUNT(*) AS c --NEW DATA
@@ -67,8 +70,9 @@
  * WHEN MATCHED AND countStar = 0 THEN DELETE
  * WHEN NOT MATCHED
  *   THEN INSERT VALUES (source.a, source.b, s, c);
- *
+ * </pre>
  * To be precise, we need to convert it into a MERGE rewritten as:
+ * <pre>
  * FROM (select *, true flag from mv) mv right outer join _source_ source
  * ON (mv.a &lt;=&gt; source.a AND mv.b &lt;=&gt; source.b)
  * INSERT INTO TABLE mv                                       &lt;- (insert new rows into the view)
@@ -90,8 +94,9 @@
  *   SELECT mv.ROW__ID
  *   WHERE mv.flag AND countStar = 0
  *   SORT BY mv.ROW__ID;
+ * </pre>
  *
- * @see org.apache.hadoop.hive.ql.parse.CalcitePlanner
+ * @see AlterMaterializedViewRebuildAnalyzer
  */
 public class HiveAggregateInsertDeleteIncrementalRewritingRule extends HiveAggregateIncrementalRewritingRuleBase<
         HiveAggregateInsertDeleteIncrementalRewritingRule.IncrementalComputePlanWithDeletedRows> {
@@ -116,7 +121,10 @@ protected IncrementalComputePlanWithDeletedRows createJoinRightInput(RelOptRuleC
     aggInput = HiveHepExtractRelNodeRule.execute(aggInput);
     aggInput = new HiveRowIsDeletedPropagator(relBuilder).propagate(aggInput);
 
-    int rowIsDeletedIdx = aggInput.getRowType().getFieldCount() - 1;
+    // The row schema has two additional columns after propagation:
+    // rowIsDeleted is the last but one
+    // col0 ... coln, _any_deleted, _any_inserted
+    int rowIsDeletedIdx = aggInput.getRowType().getFieldCount() - 2;
     RexNode rowIsDeletedNode = rexBuilder.makeInputRef(
             aggInput.getRowType().getFieldList().get(rowIsDeletedIdx).getType(), rowIsDeletedIdx);
 
@@ -130,7 +138,7 @@ protected IncrementalComputePlanWithDeletedRows createJoinRightInput(RelOptRuleC
     List<RelBuilder.AggCall> newAggregateCalls = new ArrayList<>(aggregate.getAggCallList().size());
     for (int i = 0; i < aggregate.getAggCallList().size(); ++i) {
       AggregateCall aggregateCall = aggregate.getAggCallList().get(i);
-      if (aggregateCall.getAggregation().getKind() == SqlKind.COUNT && aggregateCall.getArgList().size() == 0) {
+      if (aggregateCall.getAggregation().getKind() == SqlKind.COUNT && aggregateCall.getArgList().isEmpty()) {
         countIdx = i + aggregate.getGroupCount();
       }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveRowIsDeletedPropagator.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveRowIsDeletedPropagator.java
index 8562d74251..afb1ef36fb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveRowIsDeletedPropagator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveRowIsDeletedPropagator.java
@@ -17,61 +17,105 @@
  */
 package org.apache.hadoop.hive.ql.optimizer.calcite.rules.views;
 
+import org.apache.calcite.linq4j.Ord;
 import org.apache.calcite.rel.RelNode;
-import org.apache.calcite.rel.core.JoinRelType;
 import org.apache.calcite.rel.type.RelDataType;
 import org.apache.calcite.rel.type.RelDataTypeField;
 import org.apache.calcite.rex.RexBuilder;
+import org.apache.calcite.rex.RexCall;
 import org.apache.calcite.rex.RexInputRef;
 import org.apache.calcite.rex.RexNode;
-import org.apache.calcite.rex.RexShuttle;
+import org.apache.calcite.rex.RexUtil;
+import org.apache.calcite.rex.RexVisitor;
+import org.apache.calcite.rex.RexVisitorImpl;
+import org.apache.calcite.sql.SqlKind;
 import org.apache.calcite.sql.fun.SqlStdOperatorTable;
+import org.apache.calcite.sql.type.SqlTypeName;
 import org.apache.calcite.tools.RelBuilder;
+import org.apache.calcite.util.ReflectUtil;
+import org.apache.calcite.util.ReflectiveVisitor;
+import org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer;
 import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
-import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelShuttle;
-import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelShuttleImpl;
+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan;
 
 import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import static java.util.Arrays.asList;
 
 /**
- * {@link HiveRelShuttle} to propagate rowIsDeleted column to all HiveRelNodes' rowType in the plan.
- * General rule: we expect that the rowIsDeleted column is the last column in the input rowType of the current
+ * {@link ReflectiveVisitor} to propagate row is deleted or inserted columns to all HiveRelNodes' rowType in the plan.
+ * General rule: we expect that these columns are the last columns in the input rowType of the current
  * {@link org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveRelNode}.
+ *
+ * This class is part of incremental rebuild of materialized view plan generation.
+ * <br>
+ * @see AlterMaterializedViewRebuildAnalyzer
+ * @see HiveAggregateInsertDeleteIncrementalRewritingRule
  */
-public class HiveRowIsDeletedPropagator extends HiveRelShuttleImpl {
+public class HiveRowIsDeletedPropagator implements ReflectiveVisitor {
 
-  protected final RelBuilder relBuilder;
+  private static final String ANY_DELETED_COLUMN_NAME = "_any_deleted";
+  private static final String ANY_INSERTED_COLUMN_NAME = "_any_inserted";
+  private static final String DELETED_COLUMN_NAME = "_deleted";
+  private static final String INSERTED_COLUMN_NAME = "_inserted";
+
+  private final RelBuilder relBuilder;
+  private final ReflectUtil.MethodDispatcher<RelNode> dispatcher;
 
   public HiveRowIsDeletedPropagator(RelBuilder relBuilder) {
     this.relBuilder = relBuilder;
+    this.dispatcher = ReflectUtil.createMethodDispatcher(
+        RelNode.class, this, "visit", RelNode.class, Context.class);
   }
 
   public RelNode propagate(RelNode relNode) {
-    return relNode.accept(this);
-  }
-
-  /**
-   * Create a Projection on top of TS that contains all columns from TS.
-   * Let rowIsDeleted the last column in the new Project.
-   * Enable fetching Deleted rows in TS.
-   * @param scan - TS to transform
-   * @return - new TS and a optionally a Project on top of it.
-   */
-  @Override
-  public RelNode visit(HiveTableScan scan) {
-    RelDataType tableRowType = scan.getTable().getRowType();
-    RelDataTypeField column = tableRowType.getField(
-        VirtualColumn.ROWISDELETED.getName(), false, false);
-    if (column == null) {
-      // This should not happen since Virtual columns are propagated for all native table scans in
-      // CalcitePlanner.genTableLogicalPlan()
-      throw new ColumnPropagationException("TableScan " + scan + " row schema does not contain " +
-          VirtualColumn.ROWISDELETED.getName() + " virtual column");
+    return dispatcher.invoke(relNode, new Context());
+  }
+
+  private RelNode visitChild(RelNode parent, int i, RelNode child, Context context) {
+    RelNode newRel = dispatcher.invoke(child, context);
+    final List<RelNode> newInputs = new ArrayList<>(parent.getInputs());
+    newInputs.set(i, newRel);
+    return parent.copy(parent.getTraitSet(), newInputs);
+  }
+
+  private RelNode visitChildren(RelNode rel, Context context) {
+    for (Ord<RelNode> input : Ord.zip(rel.getInputs())) {
+      rel = visitChild(rel, input.i, input.e, context);
     }
+    return rel;
+  }
+
+  public static final class Context {
+    private final Map<Integer, RexNode> rowIdPredicates = new HashMap<>();
+  }
+
+  public RelNode visit(RelNode relNode, Context context) {
+    return visitChildren(relNode, context);
+  }
+
+  // Add a project on top of the TS.
+  // Project two boolean columns: one for indicating the row is deleted another
+  // for newly inserted.
+  // A row is considered to be
+  //  - deleted when the ROW_IS_DELETED virtual column is true and the writeId of the record is higher than the
+  //    saved in materialized view snapshot metadata
+  //  - newly inserted when the ROW_IS_DELETED virtual column is false and the writeId of the record is higher than the
+  //    saved in materialized view snapshot metadata
+  public RelNode visit(HiveTableScan scan, Context context) {
+    RelDataType tableRowType = scan.getTable().getRowType();
+    RelDataTypeField rowIdField = getVirtualColumnField(tableRowType, VirtualColumn.ROWID, scan);
+    RexNode rowIdPredicate = context.rowIdPredicates.get(rowIdField.getIndex());
+
+    RelDataTypeField rowIsDeletedField = getVirtualColumnField(tableRowType, VirtualColumn.ROWISDELETED, scan);
 
     RexBuilder rexBuilder = relBuilder.getRexBuilder();
 
@@ -79,10 +123,27 @@ public RelNode visit(HiveTableScan scan) {
     List<String> projectNames = new ArrayList<>(tableRowType.getFieldCount());
     populateProjects(rexBuilder, tableRowType, projects, projectNames);
     // Propagated column is already in the TS move it to the end
-    RexNode propagatedColumn = projects.remove(column.getIndex());
-    projects.add(propagatedColumn);
-    String propagatedColumnName = projectNames.remove(column.getIndex());
-    projectNames.add(propagatedColumnName);
+    RexNode rowIsDeleted = projects.remove(rowIsDeletedField.getIndex());
+    projects.add(rowIsDeleted);
+    // predicates on rowId introduced by HiveAugmentMaterializationRule into the original MV definition query plan
+    // on top of each TS operators.
+    // Later that plan is transformed to a Union rewrite plan where all rowId predicates are pulled up on top of
+    // the top Join operator.
+    if (rowIdPredicate == null) {
+      // If a table have not changed then no predicate is introduced for the TS. All rows in the table should remain.
+      projects.add(rexBuilder.makeLiteral(false));
+      projects.add(rexBuilder.makeLiteral(false));
+    } else {
+      // A row is deleted if ROW_IS_DELETED is true and rowId > <saved_rowId>
+      projects.add(rexBuilder.makeCall(SqlStdOperatorTable.AND, rowIsDeleted, rowIdPredicate));
+      // A row is newly inserted if ROW_IS_DELETED is false and rowId > <saved_rowId>
+      projects.add(rexBuilder.makeCall(SqlStdOperatorTable.AND,
+          rexBuilder.makeCall(SqlStdOperatorTable.NOT, rowIsDeleted), rowIdPredicate));
+    }
+    String rowIsDeletedName = projectNames.remove(rowIsDeletedField.getIndex());
+    projectNames.add(rowIsDeletedName);
+    projectNames.add(DELETED_COLUMN_NAME);
+    projectNames.add(INSERTED_COLUMN_NAME);
 
     // Note: as a nature of Calcite if row schema of TS and the new Project would be exactly the same no
     // Project is created.
@@ -92,86 +153,146 @@ public RelNode visit(HiveTableScan scan) {
         .build();
   }
 
-  /**
-   * Create a new Project with original projected columns plus add rowIsDeleted as last column referencing
-   * the last column of the input {@link RelNode}.
-   * @param project - {@link HiveProject to transform}
-   * @return new Project
-   */
-  @Override
-  public RelNode visit(HiveProject project) {
-    RelNode newProject = visitChild(project, 0, project.getInput());
+  // Add the new columns(_deleted, _inserted) to the original project
+  public RelNode visit(HiveProject project, Context context) {
+    RelNode newProject = visitChild(project, 0, project.getInput(), context);
     RelNode projectInput = newProject.getInput(0);
-    int rowIsDeletedIndex = projectInput.getRowType().getFieldCount() - 1;
-    List<RexNode> newProjects = new ArrayList<>(project.getRowType().getFieldCount() + 1);
-    newProjects.addAll(project.getProjects());
 
-    RexNode rowIsDeleted = relBuilder.getRexBuilder().makeInputRef(
-            projectInput.getRowType().getFieldList().get(rowIsDeletedIndex).getType(), rowIsDeletedIndex);
-    newProjects.add(rowIsDeleted);
+    List<RexNode> newProjects = new ArrayList<>(project.getProjects().size() + 2);
+    newProjects.addAll(project.getProjects());
+    newProjects.add(createInputRef(projectInput, 2));
+    newProjects.add(createInputRef(projectInput, 1));
 
     return relBuilder
-            .push(projectInput)
-            .project(newProjects)
-            .build();
-  }
-
-  /**
-   * Create new Join and a Project on top of it.
-   * @param join - {@link HiveJoin} to transform
-   * @return - new Join with a Project on top
-   */
-  @Override
-  public RelNode visit(HiveJoin join) {
-    // Propagate rowISDeleted to left input
-    RelNode tmpJoin = visitChild(join, 0, join.getInput(0));
-    RelNode leftInput = tmpJoin.getInput(0);
-    RelDataType leftRowType = tmpJoin.getInput(0).getRowType();
-    int leftRowIsDeletedIndex = leftRowType.getFieldCount() - 1;
-    // Propagate rowISDeleted to right input
-    tmpJoin = visitChild(join, 1, join.getInput(1));
-    RelNode rightInput = tmpJoin.getInput(1);
-    RelDataType rightRowType = rightInput.getRowType();
-    int rightRowIsDeletedIndex = rightRowType.getFieldCount() - 1;
-
-    // Create input ref to rowIsDeleted columns in left and right inputs
-    RexBuilder rexBuilder = relBuilder.getRexBuilder();
-    RexNode leftRowIsDeleted = rexBuilder.makeInputRef(
-            leftRowType.getFieldList().get(leftRowIsDeletedIndex).getType(), leftRowIsDeletedIndex);
-    RexNode rightRowIsDeleted = rexBuilder.makeInputRef(
-            rightRowType.getFieldList().get(rightRowIsDeletedIndex).getType(),
-            leftRowType.getFieldCount() + rightRowIsDeletedIndex);
-
-    RexNode newJoinCondition;
-    int newLeftFieldCount;
-    if (join.getInput(0).getRowType().getField(VirtualColumn.ROWISDELETED.getName(), false, false) == null) {
-      // Shift column references refers columns coming from right input by one in join condition since the new left input
-      // has a new column
-      newJoinCondition = new InputRefShifter(leftRowType.getFieldCount() - 1, relBuilder)
-          .apply(join.getCondition());
-
-      newLeftFieldCount = leftRowType.getFieldCount() - 1;
-    } else {
-      newJoinCondition = join.getCondition();
-      newLeftFieldCount = leftRowType.getFieldCount();
+        .push(projectInput)
+        .project(newProjects)
+        .build();
+  }
+
+  // Union rewrite algorithm pulls up all the predicates on rowId on top of top Join operator:
+  // Example:
+  //   HiveUnion(all=[true])
+  //    ...
+  //    HiveFilter(condition=[OR(<(1, $14.writeid), <(1, $6.writeid))])
+  //      HiveJoin(condition=[=($0, $8)], joinType=[inner], algorithm=[none], cost=[not available])
+  // Check the filter condition and collect operands of OR expressions referencing only one column
+  public RelNode visit(HiveFilter filter, Context context) {
+    RexNode condition = filter.getCondition();
+
+    // The condition might be a single predicate on the rowId (if only one table changed)
+    RexInputRef rexInputRef = findPossibleRowIdRef(filter.getCondition());
+    if (rexInputRef != null) {
+      context.rowIdPredicates.put(rexInputRef.getIndex(), filter.getCondition());
+      return visitChild(filter, 0, filter.getInput(0), context);
+    }
+
+    if (!condition.isA(SqlKind.OR)) {
+      return visitChild(filter, 0, filter.getInput(0), context);
+    }
+
+    for (RexNode operand : ((RexCall)condition).operands) {
+      RexInputRef inputRef = findPossibleRowIdRef(operand);
+      if (inputRef != null) {
+        context.rowIdPredicates.put(inputRef.getIndex(), operand);
+      }
+    }
+
+    return visitChild(filter, 0, filter.getInput(0), context);
+  }
+
+  private RexInputRef findPossibleRowIdRef(RexNode operand) {
+    Set<RexInputRef> inputRefs = findRexInputRefs(operand);
+    if (inputRefs.size() != 1) {
+      return null;
     }
 
+    // This is a candidate for predicate on rowId
+    return inputRefs.iterator().next();
+  }
+
+  // Propagate new column to each side of the join.
+  // Create a project to combine the propagated expressions.
+  // Create a filter to remove rows which are joined from a deleted and a newly inserted row.
+  public RelNode visit(HiveJoin join, Context context) {
+    // Propagate columns to left input
+    RelNode tmpJoin = visitChild(join, 0, join.getInput(0), context);
+    RelNode newLeftInput = tmpJoin.getInput(0);
+    RelDataType newLeftRowType = newLeftInput.getRowType();
+    // Propagate columns to right input.
+    // All column references should be shifted in candidate predicates to the left
+    Context rightContext = new Context();
+    int originalLeftFieldCount = join.getInput(0).getRowType().getFieldCount();
+    for (Map.Entry<Integer, RexNode> entry : context.rowIdPredicates.entrySet()) {
+      if (entry.getKey() > originalLeftFieldCount) {
+        rightContext.rowIdPredicates.put(entry.getKey() - originalLeftFieldCount,
+          new InputRefShifter(originalLeftFieldCount, -originalLeftFieldCount, relBuilder).apply(entry.getValue()));
+      }
+    }
+    tmpJoin = visitChild(join, 1, join.getInput(1), rightContext);
+    RelNode newRightInput = tmpJoin.getInput(1);
+    RelDataType newRightRowType = newRightInput.getRowType();
+
+    // Create input refs to propagated columns in left and right inputs
+    int rightAnyDeletedIndex = newRightRowType.getFieldCount() - 2;
+    int rightAnyInsertedIndex = newRightRowType.getFieldCount() - 1;
+    RexBuilder rexBuilder = relBuilder.getRexBuilder();
+    RexNode leftDeleted = createInputRef(newLeftInput, 2);
+    RexNode leftInserted = createInputRef(newLeftInput, 1);
+    RexNode rightDeleted = rexBuilder.makeInputRef(
+        newRightRowType.getFieldList().get(rightAnyDeletedIndex).getType(),
+        newLeftRowType.getFieldCount() + rightAnyDeletedIndex);
+    RexNode rightInserted = rexBuilder.makeInputRef(
+        newRightRowType.getFieldList().get(rightAnyInsertedIndex).getType(),
+        newLeftRowType.getFieldCount() + rightAnyInsertedIndex);
+
+    // Shift column references refers columns coming from right input in join condition since the new left input
+    // has a new columns
+    int newLeftFieldCount = newLeftRowType.getFieldCount() - 2;
+    RexNode newJoinCondition = new InputRefShifter(newLeftFieldCount, 2, relBuilder).apply(join.getCondition());
+
     // Collect projected columns: all columns from both inputs
-    List<RexNode> projects = new ArrayList<>(newLeftFieldCount + rightRowType.getFieldCount() + 1);
-    List<String> projectNames = new ArrayList<>(newLeftFieldCount + rightRowType.getFieldCount() + 1);
-    populateProjects(rexBuilder, leftRowType, 0, newLeftFieldCount, projects, projectNames);
-    populateProjects(rexBuilder, rightRowType, leftRowType.getFieldCount(), rightRowType.getFieldCount(), projects, projectNames);
+    List<RexNode> projects = new ArrayList<>(newLeftFieldCount + newRightRowType.getFieldCount() + 1);
+    List<String> projectNames = new ArrayList<>(newLeftFieldCount + newRightRowType.getFieldCount() + 1);
+    populateProjects(rexBuilder, newLeftRowType, 0, newLeftFieldCount, projects, projectNames);
+    populateProjects(rexBuilder, newRightRowType, newLeftRowType.getFieldCount(),
+        newRightRowType.getFieldCount() - 2, projects, projectNames);
+
+    // Create derived expressions
+    projects.add(rexBuilder.makeCall(SqlStdOperatorTable.OR, leftDeleted, rightDeleted));
+    projects.add(rexBuilder.makeCall(SqlStdOperatorTable.OR, leftInserted, rightInserted));
+    projectNames.add(ANY_DELETED_COLUMN_NAME);
+    projectNames.add(ANY_INSERTED_COLUMN_NAME);
 
-    // Add rowIsDeleted column to project
-    projects.add(rexBuilder.makeCall(SqlStdOperatorTable.OR, leftRowIsDeleted, rightRowIsDeleted));
-    projectNames.add(VirtualColumn.ROWISDELETED.getName());
+    // Create input refs to derived expressions in project
+    RelDataType boolIntType = relBuilder.getTypeFactory().createSqlType(SqlTypeName.BOOLEAN);
+    RexNode anyDeleted = rexBuilder.makeInputRef(boolIntType, projects.size() - 2);
+    RexNode anyInserted = rexBuilder.makeInputRef(boolIntType, projects.size() - 1);
+
+    // Create filter condition: NOT( (leftDeleted OR rightDeleted) AND (leftInserted OR rightInserted) )
+    // We exploit that a row can not be deleted and inserted at the same time.
+    RexNode filterCondition = rexBuilder.makeCall(SqlStdOperatorTable.NOT,
+        RexUtil.composeConjunction(rexBuilder, asList(anyDeleted, anyInserted)));
 
     return relBuilder
-            .push(leftInput)
-            .push(rightInput)
-            .join(join.getJoinType(), newJoinCondition)
-            .project(projects)
-            .build();
+        .push(newLeftInput)
+        .push(newRightInput)
+        .join(join.getJoinType(), newJoinCondition)
+        .project(projects, projectNames)
+        .filter(filterCondition)
+        .build();
+  }
+
+  private RelDataTypeField getVirtualColumnField(
+      RelDataType tableRowType, VirtualColumn virtualColumn, HiveTableScan scan) {
+    RelDataTypeField field = tableRowType.getField(
+        virtualColumn.getName(), false, false);
+    if (field == null) {
+      // This should not happen since Virtual columns are propagated for all native table scans in
+      // CalcitePlanner.genTableLogicalPlan()
+      throw new ColumnPropagationException("TableScan " + scan + " row schema does not contain " +
+          virtualColumn.getName() + " virtual column");
+    }
+    return field;
   }
 
   private void populateProjects(RexBuilder rexBuilder, RelDataType inputRowType,
@@ -186,4 +307,25 @@ private void populateProjects(RexBuilder rexBuilder, RelDataType inputRowType, i
       projectNames.add(relDataTypeField.getName());
     }
   }
+
+  private RexNode createInputRef(RelNode relNode, int negativeOffset) {
+    int index = relNode.getRowType().getFieldCount() - negativeOffset;
+    return relBuilder.getRexBuilder().makeInputRef(
+        relNode.getRowType().getFieldList().get(index).getType(), index);
+  }
+
+  private Set<RexInputRef> findRexInputRefs(RexNode rexNode) {
+    Set<RexInputRef> rexTableInputRefs = new HashSet<>();
+    RexVisitor<RexInputRef> visitor = new RexVisitorImpl<RexInputRef>(true) {
+
+      @Override
+      public RexInputRef visitInputRef(RexInputRef inputRef) {
+        rexTableInputRefs.add(inputRef);
+        return super.visitInputRef(inputRef);
+      }
+    };
+
+    rexNode.accept(visitor);
+    return rexTableInputRefs;
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/InputRefShifter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/InputRefShifter.java
index a9d7639c0c..06e6e9e5f6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/InputRefShifter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/InputRefShifter.java
@@ -23,26 +23,26 @@
 import org.apache.calcite.rex.RexShuttle;
 import org.apache.calcite.tools.RelBuilder;
 
+/**
+ * Shift input reference index by the specified amount (shift) if the referenced column index is higher or equals with
+ * the startIndex.
+ */
 public class InputRefShifter extends RexShuttle {
   private final int startIndex;
+  private final int shift;
   private final RelBuilder relBuilder;
 
-  InputRefShifter(int startIndex, RelBuilder relBuilder) {
+  InputRefShifter(int startIndex, int shift, RelBuilder relBuilder) {
     this.startIndex = startIndex;
+    this.shift = shift;
     this.relBuilder = relBuilder;
   }
 
-  /**
-   * Shift input reference index by one if the referenced column index is higher or equals with the startIndex.
-   * @param inputRef - {@link RexInputRef} to transform
-   * @return new {@link RexInputRef} if the referenced column index is higher or equals with the startIndex,
-   * original otherwise
-   */
   @Override
   public RexNode visitInputRef(RexInputRef inputRef) {
     if (inputRef.getIndex() >= startIndex) {
       RexBuilder rexBuilder = relBuilder.getRexBuilder();
-      return rexBuilder.makeInputRef(inputRef.getType(), inputRef.getIndex() + 1);
+      return rexBuilder.makeInputRef(inputRef.getType(), inputRef.getIndex() + shift);
     }
     return inputRef;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/MaterializedViewRewritingRelVisitor.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/MaterializedViewRewritingRelVisitor.java
index 49a0982828..d8ac679822 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/MaterializedViewRewritingRelVisitor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/MaterializedViewRewritingRelVisitor.java
@@ -31,7 +31,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-
 /**
  * This class is a helper to check whether a materialized view rebuild
  * can be transformed from INSERT OVERWRITE to INSERT INTO.
@@ -50,7 +49,7 @@ public class MaterializedViewRewritingRelVisitor extends RelVisitor {
 
 
   private boolean containsAggregate;
-  private boolean fullAcidView;
+  private final boolean fullAcidView;
   private boolean rewritingAllowed;
   private int countIndex;
 
@@ -88,6 +87,7 @@ private void check(Union union) {
       throw new ReturnedValue(false);
     }
     // First branch should have the query (with write ID filter conditions)
+    RelNode queryBranch = union.getInput(0);
     new RelVisitor() {
       @Override
       public void visit(RelNode node, int ordinal, RelNode parent) {
@@ -112,7 +112,8 @@ public void visit(RelNode node, int ordinal, RelNode parent) {
           throw new ReturnedValue(false);
         }
       }
-    }.go(union.getInput(0));
+    }.go(queryBranch);
+
     // Second branch should only have the MV
     new RelVisitor() {
       @Override
@@ -177,5 +178,4 @@ public ReturnedValue(boolean value) {
       this.value = value;
     }
   }
-
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveAugmentMaterializationRule.java b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveAugmentMaterializationRule.java
new file mode 100644
index 0000000000..2a850cfc53
--- /dev/null
+++ b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveAugmentMaterializationRule.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.optimizer.calcite.rules.views;
+
+import org.apache.calcite.plan.RelOptRule;
+import org.apache.calcite.rel.RelNode;
+import org.apache.hadoop.hive.common.ValidReaderWriteIdList;
+import org.apache.hadoop.hive.common.ValidTxnWriteIdList;
+import org.apache.hadoop.hive.common.ValidWriteIdList;
+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.mockito.junit.MockitoJUnitRunner;
+
+import java.util.BitSet;
+
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.core.Is.is;
+import static org.hamcrest.core.IsInstanceOf.instanceOf;
+import static org.mockito.Mockito.doReturn;
+
+@RunWith(MockitoJUnitRunner.class)
+public class TestHiveAugmentMaterializationRule extends TestRuleBase {
+  @Test
+  public void testFilterIsCreatedInTopOfTSWhenTableHasChangesSinceSavedSnapshot() {
+    RelNode tableScan = createTS(t1NativeMock, "t1");
+
+    ValidTxnWriteIdList current = new ValidTxnWriteIdList(10L);
+    ValidWriteIdList validWriteIdList = new ValidReaderWriteIdList("default.t1", new long[0], new BitSet(), 10L);
+    current.addTableValidWriteIdList(validWriteIdList);
+
+    ValidTxnWriteIdList mv = new ValidTxnWriteIdList(5L);
+    validWriteIdList = new ValidReaderWriteIdList("default.t1", new long[] {4, 6}, new BitSet(), 5L);
+    mv.addTableValidWriteIdList(validWriteIdList);
+
+    RelOptRule rule = new HiveAugmentMaterializationRule(REX_BUILDER, current, mv);
+
+    RelNode newRoot = HiveMaterializedViewUtils.applyRule(tableScan, rule);
+
+    assertThat(newRoot, instanceOf(HiveFilter.class));
+    HiveFilter filter = (HiveFilter) newRoot;
+    assertThat(filter.getCondition().toString(), is("AND(<=($3.writeId, 5), <>($3.writeId, 4), <>($3.writeId, 6))"));
+  }
+
+}
\ No newline at end of file
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveAugmentSnapshotMaterializationRule.java b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveAugmentSnapshotMaterializationRule.java
index 746343d814..266f5dca92 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveAugmentSnapshotMaterializationRule.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveAugmentSnapshotMaterializationRule.java
@@ -39,7 +39,7 @@ public class TestHiveAugmentSnapshotMaterializationRule extends TestRuleBase {
 
   @Test
   public void testWhenSnapshotAndTableAreEmptyNoFilterAdded() {
-    RelNode tableScan = createTS();
+    RelNode tableScan = createT2IcebergTS();
     RelOptRule rule = HiveAugmentSnapshotMaterializationRule.with(Collections.emptyMap());
 
     RelNode newRoot = HiveMaterializedViewUtils.applyRule(tableScan, rule);
@@ -50,7 +50,7 @@ public void testWhenSnapshotAndTableAreEmptyNoFilterAdded() {
   @Test
   public void testWhenNoSnapshotButTableHasNewDataAFilterWithDefaultSnapshotIDAdded() {
     doReturn(new SnapshotContext(42)).when(table2storageHandler).getCurrentSnapshotContext(table2);
-    RelNode tableScan = createTS();
+    RelNode tableScan = createT2IcebergTS();
     RelOptRule rule = HiveAugmentSnapshotMaterializationRule.with(Collections.emptyMap());
 
     RelNode newRoot = HiveMaterializedViewUtils.applyRule(tableScan, rule);
@@ -63,7 +63,7 @@ public void testWhenNoSnapshotButTableHasNewDataAFilterWithDefaultSnapshotIDAdde
   @Test
   public void testWhenMVAndTableCurrentSnapshotAreTheSameNoFilterAdded() {
     doReturn(new SnapshotContext(42)).when(table2storageHandler).getCurrentSnapshotContext(table2);
-    RelNode tableScan = createTS();
+    RelNode tableScan = createT2IcebergTS();
     Map<String, SnapshotContext> mvSnapshot = new HashMap<>();
     mvSnapshot.put(table2.getFullyQualifiedName(), new SnapshotContext(42));
     RelOptRule rule = HiveAugmentSnapshotMaterializationRule.with(mvSnapshot);
@@ -76,7 +76,7 @@ public void testWhenMVAndTableCurrentSnapshotAreTheSameNoFilterAdded() {
   @Test
   public void testWhenMVSnapshotIsDifferentThanTableCurrentSnapshotHasNewDataAFilterWithMVSnapshotIdAdded() {
     doReturn(new SnapshotContext(10)).when(table2storageHandler).getCurrentSnapshotContext(table2);
-    RelNode tableScan = createTS();
+    RelNode tableScan = createT2IcebergTS();
     Map<String, SnapshotContext> mvSnapshot = new HashMap<>();
     mvSnapshot.put(table2.getFullyQualifiedName(), new SnapshotContext(42));
     RelOptRule rule = HiveAugmentSnapshotMaterializationRule.with(mvSnapshot);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHivePushdownSnapshotFilterRule.java b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHivePushdownSnapshotFilterRule.java
index c60d83a8df..c956906156 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHivePushdownSnapshotFilterRule.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHivePushdownSnapshotFilterRule.java
@@ -43,7 +43,7 @@ public class TestHivePushdownSnapshotFilterRule extends TestRuleBase {
 
   @Test
   public void testFilterIsRemovedAndVersionIntervalFromIsSetWhenFilterHasSnapshotIdPredicate() {
-    RelNode tableScan = createTS();
+    RelNode tableScan = createT2IcebergTS();
 
     RelBuilder relBuilder = HiveRelFactories.HIVE_BUILDER.create(relOptCluster, schemaMock);
     RelNode root = relBuilder.push(tableScan)
@@ -64,7 +64,7 @@ public void testFilterIsRemovedAndVersionIntervalFromIsSetWhenFilterHasSnapshotI
 
   @Test
   public void testFilterLeftIntactWhenItDoesNotHaveSnapshotIdPredicate() {
-    RelNode tableScan = createTS();
+    RelNode tableScan = createT2IcebergTS();
 
     RelBuilder relBuilder = HiveRelFactories.HIVE_BUILDER.create(relOptCluster, schemaMock);
     RelNode root = relBuilder.push(tableScan)
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveRowIsDeletedPropagator.java b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveRowIsDeletedPropagator.java
new file mode 100644
index 0000000000..042ae8f624
--- /dev/null
+++ b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHiveRowIsDeletedPropagator.java
@@ -0,0 +1,110 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.optimizer.calcite.rules.views;
+
+import org.apache.calcite.plan.RelOptUtil;
+import org.apache.calcite.rel.RelNode;
+import org.apache.calcite.rel.core.JoinRelType;
+import org.apache.calcite.rel.type.RelDataType;
+import org.apache.calcite.rex.RexNode;
+import org.apache.calcite.sql.fun.SqlStdOperatorTable;
+import org.apache.calcite.sql.type.SqlTypeName;
+import org.apache.calcite.tools.RelBuilder;
+import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
+import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.mockito.junit.MockitoJUnitRunner;
+
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.is;
+
+@RunWith(MockitoJUnitRunner.class)
+public class TestHiveRowIsDeletedPropagator extends TestRuleBase {
+  @Test
+  public void testJoining3TablesAndAllChanged() {
+    RelNode ts1 = createTS(t1NativeMock, "t1");
+    RelNode ts2 = createTS(t2NativeMock, "t2");
+    RelNode ts3 = createTS(t3NativeMock, "t3");
+
+    RelBuilder relBuilder = HiveRelFactories.HIVE_BUILDER.create(relOptCluster, null);
+
+    RexNode joinCondition = REX_BUILDER.makeCall(SqlStdOperatorTable.EQUALS,
+        REX_BUILDER.makeInputRef(ts1.getRowType().getFieldList().get(0).getType(), 0),
+        REX_BUILDER.makeInputRef(ts2.getRowType().getFieldList().get(0).getType(), 5));
+    RelNode join1 = relBuilder
+        .push(ts1)
+        .filter(REX_BUILDER.makeCall(SqlStdOperatorTable.IS_NOT_NULL, REX_BUILDER.makeInputRef(ts1, 0)))
+        .push(ts2)
+        .filter(REX_BUILDER.makeCall(SqlStdOperatorTable.IS_NOT_NULL, REX_BUILDER.makeInputRef(ts2, 0)))
+        .join(JoinRelType.INNER, joinCondition)
+        .build();
+
+    RexNode joinCondition2 = REX_BUILDER.makeCall(SqlStdOperatorTable.EQUALS,
+        REX_BUILDER.makeInputRef(ts3.getRowType().getFieldList().get(0).getType(), 10),
+        REX_BUILDER.makeInputRef(join1.getRowType().getFieldList().get(5).getType(), 5));
+
+    RelDataType bigIntType = relBuilder.getTypeFactory().createSqlType(SqlTypeName.BIGINT);
+
+    RexNode writeIdFilter = REX_BUILDER.makeCall(SqlStdOperatorTable.OR,
+        REX_BUILDER.makeCall(SqlStdOperatorTable.LESS_THAN, REX_BUILDER.makeLiteral(1, bigIntType, false), rowIdFieldAccess(ts1, 3)),
+        REX_BUILDER.makeCall(SqlStdOperatorTable.LESS_THAN, REX_BUILDER.makeLiteral(1, bigIntType, false), rowIdFieldAccess(ts2, 8)),
+        REX_BUILDER.makeCall(SqlStdOperatorTable.LESS_THAN, REX_BUILDER.makeLiteral(1, bigIntType, false), rowIdFieldAccess(ts3, 13)));
+
+    RelNode root = relBuilder
+        .push(join1)
+        .push(ts3)
+        .filter(REX_BUILDER.makeCall(SqlStdOperatorTable.IS_NOT_NULL, REX_BUILDER.makeInputRef(ts3, 0)))
+        .join(JoinRelType.INNER, joinCondition2)
+        .filter(writeIdFilter)
+        .build();
+
+//    System.out.println(RelOptUtil.toString(root));
+
+    HiveRowIsDeletedPropagator propagator = new HiveRowIsDeletedPropagator(relBuilder);
+    RelNode newRoot = propagator.propagate(root);
+
+    String dump = RelOptUtil.toString(newRoot);
+    assertThat(dump, is(EXPECTED_testJoining3TablesAndAllChanged));
+  }
+
+  private static final String EXPECTED_testJoining3TablesAndAllChanged =
+      "HiveFilter(condition=[OR(<(1, $3.writeId), <(1, $8.writeId), <(1, $13.writeId))])\n" +
+      "  HiveFilter(condition=[OR(NOT($15), NOT($16))])\n" +
+      "    HiveProject(a=[$0], b=[$1], c=[$2], ROW__ID=[$3], ROW__IS__DELETED=[$4], d=[$5], e=[$6], f=[$7], ROW__ID0=[$8], ROW__IS__DELETED0=[$9], g=[$12], h=[$13], i=[$14], ROW__ID1=[$15], ROW__IS__DELETED1=[$16], _any_deleted=[OR($10, $17)], _any_inserted=[OR($11, $18)])\n" +
+      "      HiveJoin(condition=[=($12, $5)], joinType=[inner], algorithm=[none], cost=[not available])\n" +
+      "        HiveFilter(condition=[OR(NOT($10), NOT($11))])\n" +
+      "          HiveProject(a=[$0], b=[$1], c=[$2], ROW__ID=[$3], ROW__IS__DELETED=[$4], d=[$7], e=[$8], f=[$9], ROW__ID0=[$10], ROW__IS__DELETED0=[$11], _any_deleted=[OR($5, $12)], _any_inserted=[OR($6, $13)])\n" +
+      "            HiveJoin(condition=[=($0, $7)], joinType=[inner], algorithm=[none], cost=[not available])\n" +
+      "              HiveFilter(condition=[IS NOT NULL($0)])\n" +
+      "                HiveProject(a=[$0], b=[$1], c=[$2], ROW__ID=[$3], ROW__IS__DELETED=[$4], _deleted=[AND($4, <(1, $3.writeId))], _inserted=[AND(<(1, $3.writeId), NOT($4))])\n" +
+      "                  HiveTableScan(table=[[]], table:alias=[t1])\n" +
+      "              HiveFilter(condition=[IS NOT NULL($0)])\n" +
+      "                HiveProject(d=[$0], e=[$1], f=[$2], ROW__ID=[$3], ROW__IS__DELETED=[$4], _deleted=[AND($4, <(1, $3.writeId))], _inserted=[AND(<(1, $3.writeId), NOT($4))])\n" +
+      "                  HiveTableScan(table=[[]], table:alias=[t2])\n" +
+      "        HiveFilter(condition=[IS NOT NULL($0)])\n" +
+      "          HiveProject(g=[$0], h=[$1], i=[$2], ROW__ID=[$3], ROW__IS__DELETED=[$4], _deleted=[AND($4, <(1, $3.writeId))], _inserted=[AND(<(1, $3.writeId), NOT($4))])\n" +
+      "            HiveTableScan(table=[[]], table:alias=[t3])\n";
+
+  private RexNode rowIdFieldAccess(RelNode tableScan, int posInTarget) {
+    int rowIDPos = tableScan.getTable().getRowType().getField(
+        VirtualColumn.ROWID.getName(), false, false).getIndex();
+    return REX_BUILDER.makeFieldAccess(REX_BUILDER.makeInputRef(
+        tableScan.getTable().getRowType().getFieldList().get(rowIDPos).getType(), posInTarget), 0);
+  }
+}
\ No newline at end of file
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestRuleBase.java b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestRuleBase.java
index 2d1d8133dd..f56e57107f 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestRuleBase.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestRuleBase.java
@@ -20,33 +20,63 @@
 import org.apache.calcite.jdbc.JavaTypeFactoryImpl;
 import org.apache.calcite.plan.RelOptCluster;
 import org.apache.calcite.plan.RelOptPlanner;
+import org.apache.calcite.rel.RelNode;
 import org.apache.calcite.rel.type.RelDataType;
 import org.apache.calcite.rel.type.RelDataTypeFactory;
+import org.apache.calcite.rel.type.RelRecordType;
 import org.apache.calcite.rex.RexBuilder;
 import org.apache.calcite.sql.type.SqlTypeName;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
+import org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException;
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveTypeSystemImpl;
 import org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveRelNode;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan;
+import org.apache.hadoop.hive.ql.optimizer.calcite.translator.TypeConverter;
 import org.apache.hadoop.hive.ql.parse.CalcitePlanner;
 import org.junit.Before;
 import org.junit.BeforeClass;
 import org.mockito.Mock;
 
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 
 import static java.util.Arrays.asList;
+import static java.util.Collections.singletonList;
 import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.lenient;
 
 public class TestRuleBase {
   protected static final RexBuilder REX_BUILDER = new RexBuilder(new JavaTypeFactoryImpl(new HiveTypeSystemImpl()));
   protected static final RelDataTypeFactory TYPE_FACTORY = REX_BUILDER.getTypeFactory();
 
   protected static RelOptCluster relOptCluster;
+
+  @Mock
+  protected RelOptHiveTable t1NativeMock;
+  protected static RelDataType t1NativeType;
+  protected static Table t1Native;
+  @Mock
+  protected static HiveStorageHandler t1NativeStorageHandler;
+  @Mock
+  protected RelOptHiveTable t2NativeMock;
+  protected static RelDataType t2NativeType;
+  protected static Table t2Native;
+  @Mock
+  protected static HiveStorageHandler t2NativeStorageHandler;
+  @Mock
+  protected RelOptHiveTable t3NativeMock;
+  protected static RelDataType t3NativeType;
+  protected static Table t3Native;
+  @Mock
+  protected static HiveStorageHandler t3NativeStorageHandler;
+
   @Mock
   protected RelOptHiveTable table2Mock;
   protected static RelDataType table2Type;
@@ -58,27 +88,79 @@ public class TestRuleBase {
   public static void beforeClass() throws Exception {
     RelOptPlanner planner = CalcitePlanner.createPlanner(new HiveConf());
     relOptCluster = RelOptCluster.create(planner, REX_BUILDER);
-    List<RelDataType> t2Schema = asList(
-        TYPE_FACTORY.createSqlType(SqlTypeName.INTEGER),
-        TYPE_FACTORY.createSqlType(SqlTypeName.VARCHAR),
-        TYPE_FACTORY.createSqlType(SqlTypeName.INTEGER),
-        HiveAugmentSnapshotMaterializationRule.snapshotIdType(TYPE_FACTORY));
-    table2Type = TYPE_FACTORY.createStructType(t2Schema, asList("d", "e", "f", VirtualColumn.SNAPSHOT_ID.getName()));
-    table2 = new Table();
-    table2.setTTable(new org.apache.hadoop.hive.metastore.api.Table());
-    table2.setDbName("default");
-    table2.setTableName("t2");
+
+    t1Native = createTable("t1");
+    t2Native = createTable("t2");
+    t3Native = createTable("t3");
+    t1NativeType = createTableType(new HashMap<String, SqlTypeName>() {{
+      put("a", SqlTypeName.INTEGER);
+      put("b", SqlTypeName.VARCHAR);
+      put("c", SqlTypeName.INTEGER);
+    }}, asList(VirtualColumn.ROWID, VirtualColumn.ROWISDELETED));
+    t2NativeType = createTableType(new HashMap<String, SqlTypeName>() {{
+      put("d", SqlTypeName.INTEGER);
+      put("e", SqlTypeName.VARCHAR);
+      put("f", SqlTypeName.INTEGER);
+    }}, asList(VirtualColumn.ROWID, VirtualColumn.ROWISDELETED));
+    t3NativeType = createTableType(new HashMap<String, SqlTypeName>() {{
+      put("g", SqlTypeName.INTEGER);
+      put("h", SqlTypeName.VARCHAR);
+      put("i", SqlTypeName.INTEGER);
+    }}, asList(VirtualColumn.ROWID, VirtualColumn.ROWISDELETED));
+
+    table2 = createTable("t2_iceberg");
+    table2Type = createTableType(new HashMap<String, SqlTypeName>() {{
+      put("d", SqlTypeName.INTEGER);
+      put("e", SqlTypeName.VARCHAR);
+      put("f", SqlTypeName.INTEGER);
+    }}, singletonList(VirtualColumn.SNAPSHOT_ID));
+  }
+
+  private static Table createTable(String name) {
+    Table table = new Table();
+    table.setTTable(new org.apache.hadoop.hive.metastore.api.Table());
+    table.setDbName("default");
+    table.setTableName(name);
+    return table;
+  }
+
+  private static RelDataType createTableType(Map<String, SqlTypeName> columns, Collection<VirtualColumn> virtualColumns)
+      throws CalciteSemanticException {
+    List<RelDataType> schema = new ArrayList<>(columns.size() + virtualColumns.size());
+    List<String> columnNames = new ArrayList<>(columns.size() + virtualColumns.size());
+    for (Map.Entry<String, SqlTypeName> column : columns.entrySet()) {
+      columnNames.add(column.getKey());
+      schema.add(TYPE_FACTORY.createTypeWithNullability(TYPE_FACTORY.createSqlType(column.getValue()), true));
+    }
+    for (VirtualColumn virtualColumn : virtualColumns) {
+      columnNames.add(virtualColumn.getName());
+      schema.add(TypeConverter.convert(virtualColumn.getTypeInfo(), TYPE_FACTORY));
+    }
+    return TYPE_FACTORY.createStructType(schema, columnNames);
   }
 
   @Before
   public void setup() {
-    doReturn(table2Type).when(table2Mock).getRowType();
-    doReturn(table2).when(table2Mock).getHiveTableMD();
+    lenient().doReturn(t1NativeType).when(t1NativeMock).getRowType();
+    lenient().doReturn(t1Native).when(t1NativeMock).getHiveTableMD();
+
+    lenient().doReturn(t2NativeType).when(t2NativeMock).getRowType();
+    lenient().doReturn(t2Native).when(t2NativeMock).getHiveTableMD();
+
+    lenient().doReturn(t3NativeType).when(t3NativeMock).getRowType();
+    lenient().doReturn(t3Native).when(t3NativeMock).getHiveTableMD();
+
+    lenient().doReturn(table2Type).when(table2Mock).getRowType();
+    lenient().doReturn(table2).when(table2Mock).getHiveTableMD();
     table2.setStorageHandler(table2storageHandler);
   }
 
-  protected HiveTableScan createTS() {
+  protected RelNode createT2IcebergTS() {
+    return createTS(table2Mock, "t2");
+  }
+
+  protected HiveTableScan createTS(RelOptHiveTable table, String alias) {
     return new HiveTableScan(relOptCluster, relOptCluster.traitSetOf(HiveRelNode.CONVENTION),
-        table2Mock, "t2", null, false, false);
+        table, alias, null, false, false);
   }
 }
diff --git a/ql/src/test/queries/clientpositive/materialized_view_create_rewrite_6_aggr_2joins.q b/ql/src/test/queries/clientpositive/materialized_view_create_rewrite_6_aggr_2joins.q
new file mode 100644
index 0000000000..823c690696
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/materialized_view_create_rewrite_6_aggr_2joins.q
@@ -0,0 +1,49 @@
+-- Test Incremental rebuild of materialized view with aggregate and count(*) and two joined tables
+-- when records is deleted from one source table and another is inserted into the other table with the same join key values.
+
+set hive.support.concurrency=true;
+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
+
+create table cmv_basetable_n6 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true');
+
+insert into cmv_basetable_n6 values
+(1, 'alfred', 10.30, 2),
+(1, 'charlie', 20.30, 2),
+(2, 'zoe', 100.30, 2);
+
+create table cmv_basetable_2_n3 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true');
+
+insert into cmv_basetable_2_n3 values
+(1, 'bob', 30.30, 2),
+(1, 'bonnie', 40.30, 2),
+(2, 'joe', 130.30, 2);
+
+SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b, count(*)
+FROM cmv_basetable_n6 JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b;
+
+CREATE MATERIALIZED VIEW cmv_mat_view_n6 TBLPROPERTIES ('transactional'='true') AS
+SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b, count(*)
+FROM cmv_basetable_n6 JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b;
+
+insert into cmv_basetable_n6 values
+(1, 'kevin', 50.30, 2);
+
+DELETE FROM cmv_basetable_2_n3 WHERE b = 'bonnie';
+
+EXPLAIN CBO
+ALTER MATERIALIZED VIEW cmv_mat_view_n6 REBUILD;
+
+EXPLAIN
+ALTER MATERIALIZED VIEW cmv_mat_view_n6 REBUILD;
+
+ALTER MATERIALIZED VIEW cmv_mat_view_n6 REBUILD;
+
+select * from cmv_mat_view_n6;
+
+drop materialized view cmv_mat_view_n6;
+
+SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b, count(*)
+FROM cmv_basetable_n6 JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b;
diff --git a/ql/src/test/queries/clientpositive/materialized_view_create_rewrite_6_aggr_3joins.q b/ql/src/test/queries/clientpositive/materialized_view_create_rewrite_6_aggr_3joins.q
new file mode 100644
index 0000000000..f1b83eb2cf
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/materialized_view_create_rewrite_6_aggr_3joins.q
@@ -0,0 +1,77 @@
+-- Test Incremental rebuild of materialized view with aggregate and count(*) and 3 joined tables
+-- when records is deleted from one source table and another is inserted into the other table with the same join key values.
+
+set hive.support.concurrency=true;
+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
+
+create table cmv_basetable_n6 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true');
+
+insert into cmv_basetable_n6 values
+(1, 'alfred', 10.30, 2),
+(1, 'charlie', 20.30, 2),
+(2, 'zoe', 100.30, 2);
+
+create table cmv_basetable_2_n3 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true');
+
+insert into cmv_basetable_2_n3 values
+(1, 'bob', 30.30, 2),
+(1, 'bonnie', 40.30, 2),
+(2, 'joe', 130.30, 2);
+
+create table t3 (a int, b varchar(256), c decimal(10,2)) stored as orc TBLPROPERTIES ('transactional'='true');
+
+insert into t3 values
+(1, 'foo', 30.30),
+(1, 'bar', 30.30),
+(2, 'bar', 30.30);
+
+CREATE MATERIALIZED VIEW mat1 TBLPROPERTIES ('transactional'='true') AS
+SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, count(*)
+FROM cmv_basetable_n6
+JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+JOIN t3 ON (t3.a = cmv_basetable_2_n3.a)
+WHERE cmv_basetable_n6.c > 10 OR cmv_basetable_2_n3.c > 10
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c;
+
+insert into cmv_basetable_n6 values
+(1, 'kevin', 50.30, 2);
+
+insert into t3 values
+(1, 'new rec', 60.30);
+
+DELETE FROM cmv_basetable_2_n3 WHERE b = 'bonnie';
+
+
+EXPLAIN CBO
+ALTER MATERIALIZED VIEW mat1 REBUILD;
+
+EXPLAIN
+ALTER MATERIALIZED VIEW mat1 REBUILD;
+
+ALTER MATERIALIZED VIEW mat1 REBUILD;
+
+select * from mat1;
+
+
+-- Delete only from one table, do not change the rest of the tables
+delete from cmv_basetable_n6 where b = 'kevin';
+
+EXPLAIN CBO
+ALTER MATERIALIZED VIEW mat1 REBUILD;
+
+EXPLAIN
+ALTER MATERIALIZED VIEW mat1 REBUILD;
+
+ALTER MATERIALIZED VIEW mat1 REBUILD;
+
+select * from mat1;
+
+
+drop materialized view mat1;
+
+SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, count(*)
+FROM cmv_basetable_n6
+JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+JOIN t3 ON (t3.a = cmv_basetable_2_n3.a)
+WHERE cmv_basetable_n6.c > 10 OR cmv_basetable_2_n3.c > 10
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c;
diff --git a/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6.q.out b/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6.q.out
index baeed8afc9..2c794c918d 100644
--- a/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6.q.out
+++ b/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6.q.out
@@ -253,12 +253,12 @@ HiveProject(a=[$5], _o__c1=[CAST(CASE(IS NULL($1), $6, IS NULL($6), $1, +($6, $1
         HiveTableScan(table=[[default, mat1]], table:alias=[default.mat1])
       HiveProject(a=[$0], $f1=[$1], $f2=[$2], $f3=[$3])
         HiveAggregate(group=[{0}], agg#0=[SUM($1)], agg#1=[SUM($2)], agg#2=[SUM($3)])
-          HiveProject(a=[$0], $f3=[CASE(OR($2, $5), *(-1, $1), $1)], $f4=[CASE(OR($2, $5), *(-1, CASE(IS NULL($1), 0, 1)), CASE(IS NULL($1), 0, 1))], $f5=[CASE(OR($2, $5), -1, 1)])
-            HiveJoin(condition=[AND(=($0, $4), OR($3, $6))], joinType=[inner], algorithm=[none], cost=[not available])
-              HiveProject(a=[$0], b=[$1], ROW__IS__DELETED=[$6], <=[<(3, $5.writeid)])
+          HiveProject(a=[$0], $f4=[CASE(OR($3, $7), *(-1, $1), $1)], $f5=[CASE(OR($3, $7), *(-1, CASE(IS NULL($1), 0, 1)), CASE(IS NULL($1), 0, 1))], $f6=[CASE(OR($3, $7), -1, 1)])
+            HiveJoin(condition=[AND(=($0, $5), OR(AND(NOT($3), NOT($7)), AND(NOT($4), NOT($8))), OR(<(3, $2.writeid), <(3, $6.writeid)))], joinType=[inner], algorithm=[none], cost=[not available])
+              HiveProject(a=[$0], b=[$1], ROW__ID=[$5], _deleted=[AND($6, <(3, $5.writeid))], _inserted=[AND(<(3, $5.writeid), NOT($6))])
                 HiveFilter(condition=[IS NOT NULL($0)])
                   HiveTableScan(table=[[default, t1]], table:alias=[t1])
-              HiveProject(a=[$0], ROW__IS__DELETED=[$5], <=[<(3, $4.writeid)])
+              HiveProject(a=[$0], ROW__ID=[$4], _deleted=[AND($5, <(3, $4.writeid))], _inserted=[AND(<(3, $4.writeid), NOT($5))])
                 HiveFilter(condition=[IS NOT NULL($0)])
                   HiveTableScan(table=[[default, t2]], table:alias=[t2])
 
@@ -335,16 +335,16 @@ STAGE PLANS:
                     predicate: a is not null (type: boolean)
                     Statistics: Num rows: 9 Data size: 837 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
-                      expressions: a (type: char(15)), ROW__IS__DELETED (type: boolean), (ROW__ID.writeid > 3L) (type: boolean)
-                      outputColumnNames: _col0, _col1, _col2
-                      Statistics: Num rows: 9 Data size: 909 Basic stats: COMPLETE Column stats: COMPLETE
+                      expressions: a (type: char(15)), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), (ROW__IS__DELETED and (ROW__ID.writeid > 3L)) (type: boolean), ((ROW__ID.writeid > 3L) and (not ROW__IS__DELETED)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2, _col3
+                      Statistics: Num rows: 9 Data size: 1593 Basic stats: COMPLETE Column stats: COMPLETE
                       Reduce Output Operator
                         key expressions: _col0 (type: char(15))
                         null sort order: z
                         sort order: +
                         Map-reduce partition columns: _col0 (type: char(15))
-                        Statistics: Num rows: 9 Data size: 909 Basic stats: COMPLETE Column stats: COMPLETE
-                        value expressions: _col1 (type: boolean), _col2 (type: boolean)
+                        Statistics: Num rows: 9 Data size: 1593 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col2 (type: boolean), _col3 (type: boolean)
             Execution mode: vectorized, llap
             LLAP IO: may be used (ACID table)
         Map 7 
@@ -359,16 +359,16 @@ STAGE PLANS:
                     predicate: a is not null (type: boolean)
                     Statistics: Num rows: 7 Data size: 1211 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
-                      expressions: a (type: char(15)), b (type: decimal(7,2)), ROW__IS__DELETED (type: boolean), (ROW__ID.writeid > 3L) (type: boolean)
-                      outputColumnNames: _col0, _col1, _col2, _col3
-                      Statistics: Num rows: 7 Data size: 1267 Basic stats: COMPLETE Column stats: COMPLETE
+                      expressions: a (type: char(15)), b (type: decimal(7,2)), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), (ROW__IS__DELETED and (ROW__ID.writeid > 3L)) (type: boolean), ((ROW__ID.writeid > 3L) and (not ROW__IS__DELETED)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                      Statistics: Num rows: 7 Data size: 1799 Basic stats: COMPLETE Column stats: COMPLETE
                       Reduce Output Operator
                         key expressions: _col0 (type: char(15))
                         null sort order: z
                         sort order: +
                         Map-reduce partition columns: _col0 (type: char(15))
-                        Statistics: Num rows: 7 Data size: 1267 Basic stats: COMPLETE Column stats: COMPLETE
-                        value expressions: _col1 (type: decimal(7,2)), _col2 (type: boolean), _col3 (type: boolean)
+                        Statistics: Num rows: 7 Data size: 1799 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: decimal(7,2)), _col2 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col3 (type: boolean), _col4 (type: boolean)
             Execution mode: vectorized, llap
             LLAP IO: may be used (ACID table)
         Reducer 2 
@@ -382,7 +382,7 @@ STAGE PLANS:
                   1 _col0 (type: char(15))
                 nullSafes: [true]
                 outputColumnNames: _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
-                Statistics: Num rows: 5 Data size: 2033 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 2 Data size: 746 Basic stats: COMPLETE Column stats: COMPLETE
                 Filter Operator
                   predicate: (_col4 and ((_col3 is null and (_col9 > 0L)) or (((_col9 + _col3) > 0) and _col3 is not null))) (type: boolean)
                   Statistics: Num rows: 1 Data size: 429 Basic stats: COMPLETE Column stats: COMPLETE
@@ -550,26 +550,26 @@ STAGE PLANS:
                 keys:
                   0 _col0 (type: char(15))
                   1 _col0 (type: char(15))
-                outputColumnNames: _col0, _col1, _col2, _col3, _col5, _col6
-                residual filter predicates: {(_col3 or _col6)}
-                Statistics: Num rows: 6 Data size: 1102 Basic stats: COMPLETE Column stats: COMPLETE
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col6, _col7, _col8
+                residual filter predicates: {(((not _col3) and (not _col7)) or ((not _col4) and (not _col8)))} {((_col2.writeid > 3L) or (_col6.writeid > 3L))}
+                Statistics: Num rows: 1 Data size: 373 Basic stats: COMPLETE Column stats: COMPLETE
                 Select Operator
-                  expressions: _col0 (type: char(15)), if((_col2 or _col5), (-1 * _col1), _col1) (type: decimal(17,2)), if((_col2 or _col5), (-1 * if(_col1 is null, 0, 1)), if(_col1 is null, 0, 1)) (type: int), if((_col2 or _col5), -1, 1) (type: int)
+                  expressions: _col0 (type: char(15)), if((_col3 or _col7), (-1 * _col1), _col1) (type: decimal(17,2)), if((_col3 or _col7), (-1 * if(_col1 is null, 0, 1)), if(_col1 is null, 0, 1)) (type: int), if((_col3 or _col7), -1, 1) (type: int)
                   outputColumnNames: _col0, _col1, _col2, _col3
-                  Statistics: Num rows: 6 Data size: 1102 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 1 Data size: 373 Basic stats: COMPLETE Column stats: COMPLETE
                   Group By Operator
                     aggregations: sum(_col1), sum(_col2), sum(_col3)
                     keys: _col0 (type: char(15))
                     minReductionHashAggr: 0.4
                     mode: hash
                     outputColumnNames: _col0, _col1, _col2, _col3
-                    Statistics: Num rows: 5 Data size: 1105 Basic stats: COMPLETE Column stats: COMPLETE
+                    Statistics: Num rows: 1 Data size: 221 Basic stats: COMPLETE Column stats: COMPLETE
                     Reduce Output Operator
                       key expressions: _col0 (type: char(15))
                       null sort order: z
                       sort order: +
                       Map-reduce partition columns: _col0 (type: char(15))
-                      Statistics: Num rows: 5 Data size: 1105 Basic stats: COMPLETE Column stats: COMPLETE
+                      Statistics: Num rows: 1 Data size: 221 Basic stats: COMPLETE Column stats: COMPLETE
                       value expressions: _col1 (type: decimal(27,2)), _col2 (type: bigint), _col3 (type: bigint)
         Reducer 9 
             Execution mode: vectorized, llap
@@ -579,13 +579,13 @@ STAGE PLANS:
                 keys: KEY._col0 (type: char(15))
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2, _col3
-                Statistics: Num rows: 5 Data size: 1105 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 1 Data size: 221 Basic stats: COMPLETE Column stats: COMPLETE
                 Reduce Output Operator
                   key expressions: _col0 (type: char(15))
                   null sort order: z
                   sort order: +
                   Map-reduce partition columns: _col0 (type: char(15))
-                  Statistics: Num rows: 5 Data size: 1105 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 1 Data size: 221 Basic stats: COMPLETE Column stats: COMPLETE
                   value expressions: _col1 (type: decimal(27,2)), _col2 (type: bigint), _col3 (type: bigint)
 
   Stage: Stage-5
@@ -674,14 +674,14 @@ POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 POSTHOOK: Output: default@mat1
 POSTHOOK: Output: default@mat1
-POSTHOOK: Lineage: mat1._c1 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:decimal(17,2), comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), ]
-POSTHOOK: Lineage: mat1._c1 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:decimal(17,2), comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), ]
-POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), ]
-POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), ]
-POSTHOOK: Lineage: mat1._c3 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:decimal(17,2), comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), (mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), ]
-POSTHOOK: Lineage: mat1._c3 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:decimal(17,2), comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), (mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), ]
-POSTHOOK: Lineage: mat1._c4 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c4, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), ]
-POSTHOOK: Lineage: mat1._c4 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c4, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), ]
+POSTHOOK: Lineage: mat1._c1 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:decimal(17,2), comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), ]
+POSTHOOK: Lineage: mat1._c1 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:decimal(17,2), comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), ]
+POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), ]
+POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), ]
+POSTHOOK: Lineage: mat1._c3 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:decimal(17,2), comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), (mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), ]
+POSTHOOK: Lineage: mat1._c3 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:decimal(17,2), comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t1)t1.FieldSchema(name:b, type:decimal(7,2), comment:null), (mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), ]
+POSTHOOK: Lineage: mat1._c4 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c4, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
+POSTHOOK: Lineage: mat1._c4 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c4, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
 POSTHOOK: Lineage: mat1.a SIMPLE [(t1)t1.FieldSchema(name:a, type:char(15), comment:null), ]
 POSTHOOK: Lineage: mat1.a SIMPLE [(t1)t1.FieldSchema(name:a, type:char(15), comment:null), ]
 PREHOOK: query: explain cbo
diff --git a/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6_aggr_2joins.q.out b/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6_aggr_2joins.q.out
new file mode 100644
index 0000000000..059859bbc9
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6_aggr_2joins.q.out
@@ -0,0 +1,600 @@
+PREHOOK: query: create table cmv_basetable_n6 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: query: create table cmv_basetable_n6 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@cmv_basetable_n6
+PREHOOK: query: insert into cmv_basetable_n6 values
+(1, 'alfred', 10.30, 2),
+(1, 'charlie', 20.30, 2),
+(2, 'zoe', 100.30, 2)
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: query: insert into cmv_basetable_n6 values
+(1, 'alfred', 10.30, 2),
+(1, 'charlie', 20.30, 2),
+(2, 'zoe', 100.30, 2)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: Lineage: cmv_basetable_n6.a SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.b SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.c SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.d SCRIPT []
+PREHOOK: query: create table cmv_basetable_2_n3 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@cmv_basetable_2_n3
+POSTHOOK: query: create table cmv_basetable_2_n3 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@cmv_basetable_2_n3
+PREHOOK: query: insert into cmv_basetable_2_n3 values
+(1, 'bob', 30.30, 2),
+(1, 'bonnie', 40.30, 2),
+(2, 'joe', 130.30, 2)
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@cmv_basetable_2_n3
+POSTHOOK: query: insert into cmv_basetable_2_n3 values
+(1, 'bob', 30.30, 2),
+(1, 'bonnie', 40.30, 2),
+(2, 'joe', 130.30, 2)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@cmv_basetable_2_n3
+POSTHOOK: Lineage: cmv_basetable_2_n3.a SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_2_n3.b SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_2_n3.c SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_2_n3.d SCRIPT []
+PREHOOK: query: SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b, count(*)
+FROM cmv_basetable_n6 JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b
+PREHOOK: type: QUERY
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b, count(*)
+FROM cmv_basetable_n6 JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+#### A masked pattern was here ####
+2	130.30	joe	1
+1	30.30	bob	2
+1	40.30	bonnie	2
+PREHOOK: query: CREATE MATERIALIZED VIEW cmv_mat_view_n6 TBLPROPERTIES ('transactional'='true') AS
+SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b, count(*)
+FROM cmv_basetable_n6 JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b
+PREHOOK: type: CREATE_MATERIALIZED_VIEW
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Output: database:default
+PREHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: query: CREATE MATERIALIZED VIEW cmv_mat_view_n6 TBLPROPERTIES ('transactional'='true') AS
+SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b, count(*)
+FROM cmv_basetable_n6 JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b
+POSTHOOK: type: CREATE_MATERIALIZED_VIEW
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: Lineage: cmv_mat_view_n6._c3 EXPRESSION [(cmv_basetable_n6)cmv_basetable_n6.null, (cmv_basetable_2_n3)cmv_basetable_2_n3.null, ]
+POSTHOOK: Lineage: cmv_mat_view_n6.a SIMPLE [(cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:a, type:int, comment:null), ]
+POSTHOOK: Lineage: cmv_mat_view_n6.b SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:b, type:varchar(256), comment:null), ]
+POSTHOOK: Lineage: cmv_mat_view_n6.c SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:c, type:decimal(10,2), comment:null), ]
+PREHOOK: query: insert into cmv_basetable_n6 values
+(1, 'kevin', 50.30, 2)
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: query: insert into cmv_basetable_n6 values
+(1, 'kevin', 50.30, 2)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: Lineage: cmv_basetable_n6.a SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.b SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.c SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.d SCRIPT []
+PREHOOK: query: DELETE FROM cmv_basetable_2_n3 WHERE b = 'bonnie'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Output: default@cmv_basetable_2_n3
+POSTHOOK: query: DELETE FROM cmv_basetable_2_n3 WHERE b = 'bonnie'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Output: default@cmv_basetable_2_n3
+PREHOOK: query: EXPLAIN CBO
+ALTER MATERIALIZED VIEW cmv_mat_view_n6 REBUILD
+PREHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@cmv_mat_view_n6
+PREHOOK: Output: default@cmv_mat_view_n6
+PREHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: query: EXPLAIN CBO
+ALTER MATERIALIZED VIEW cmv_mat_view_n6 REBUILD
+POSTHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@cmv_mat_view_n6
+POSTHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: Output: default@cmv_mat_view_n6
+CBO PLAN:
+HiveProject(a0=[$5], c0=[$6], b0=[$7], $f3=[CASE(IS NULL($3), $8, +($8, $3))])
+  HiveFilter(condition=[OR(AND($4, OR(AND(IS NULL($3), =($8, 0)), AND(=(+($8, $3), 0), IS NOT NULL($3)))), AND(IS NULL($4), OR(AND(IS NULL($3), >($8, 0)), AND(>(+($8, $3), 0), IS NOT NULL($3)))), AND($4, OR(AND(IS NULL($3), >($8, 0)), AND(>(+($8, $3), 0), IS NOT NULL($3)))))])
+    HiveJoin(condition=[AND(IS NOT DISTINCT FROM($0, $5), IS NOT DISTINCT FROM($1, $6), IS NOT DISTINCT FROM($2, $7))], joinType=[right], algorithm=[none], cost=[not available])
+      HiveProject(a=[$0], c=[$1], b=[$2], _c3=[$3], $f4=[true])
+        HiveTableScan(table=[[default, cmv_mat_view_n6]], table:alias=[default.cmv_mat_view_n6])
+      HiveProject(a=[$0], c0=[$1], b0=[$2], $f3=[$3])
+        HiveAggregate(group=[{0, 1, 2}], agg#0=[SUM($3)])
+          HiveProject(a=[$0], c0=[$6], b0=[$5], $f5=[CASE(OR($2, $8), -1, 1)])
+            HiveJoin(condition=[AND(=($0, $4), OR(AND(NOT($2), NOT($8)), AND(NOT($3), NOT($9))), OR(<(1, $7.writeid), <(1, $1.writeid)))], joinType=[inner], algorithm=[none], cost=[not available])
+              HiveProject(a=[$0], ROW__ID=[$6], _deleted=[AND($7, <(1, $6.writeid))], _inserted=[AND(<(1, $6.writeid), NOT($7))])
+                HiveFilter(condition=[IS NOT NULL($0)])
+                  HiveTableScan(table=[[default, cmv_basetable_n6]], table:alias=[cmv_basetable_n6])
+              HiveProject(a=[$0], b=[$1], c=[$2], ROW__ID=[$6], _deleted=[AND($7, <(1, $6.writeid))], _inserted=[AND(<(1, $6.writeid), NOT($7))])
+                HiveFilter(condition=[IS NOT NULL($0)])
+                  HiveTableScan(table=[[default, cmv_basetable_2_n3]], table:alias=[cmv_basetable_2_n3])
+
+PREHOOK: query: EXPLAIN
+ALTER MATERIALIZED VIEW cmv_mat_view_n6 REBUILD
+PREHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@cmv_mat_view_n6
+PREHOOK: Output: default@cmv_mat_view_n6
+PREHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: query: EXPLAIN
+ALTER MATERIALIZED VIEW cmv_mat_view_n6 REBUILD
+POSTHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@cmv_mat_view_n6
+POSTHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: Output: default@cmv_mat_view_n6
+STAGE DEPENDENCIES:
+  Stage-4 is a root stage
+  Stage-5 depends on stages: Stage-4
+  Stage-0 depends on stages: Stage-5
+  Stage-6 depends on stages: Stage-0
+  Stage-10 depends on stages: Stage-6, Stage-7, Stage-8, Stage-9
+  Stage-1 depends on stages: Stage-5
+  Stage-7 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-5
+  Stage-8 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-5
+  Stage-9 depends on stages: Stage-3
+
+STAGE PLANS:
+  Stage: Stage-4
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 9 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
+        Reducer 4 <- Reducer 2 (SIMPLE_EDGE)
+        Reducer 5 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 6 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 8 <- Map 10 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE)
+        Reducer 9 <- Reducer 8 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: default.cmv_mat_view_n6
+                  Statistics: Num rows: 3 Data size: 636 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: a (type: int), c (type: decimal(10,2)), b (type: varchar(256)), _c3 (type: bigint), true (type: boolean), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                    Statistics: Num rows: 3 Data size: 876 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256))
+                      null sort order: zzz
+                      sort order: +++
+                      Map-reduce partition columns: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256))
+                      Statistics: Num rows: 3 Data size: 876 Basic stats: COMPLETE Column stats: COMPLETE
+                      value expressions: _col3 (type: bigint), _col4 (type: boolean), _col5 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Map 10 
+            Map Operator Tree:
+                TableScan
+                  alias: cmv_basetable_2_n3
+                  filterExpr: a is not null (type: boolean)
+                  properties:
+                    acid.fetch.deleted.rows TRUE
+                  Statistics: Num rows: 2 Data size: 408 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: a is not null (type: boolean)
+                    Statistics: Num rows: 2 Data size: 408 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: a (type: int), b (type: varchar(256)), c (type: decimal(10,2)), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), (ROW__IS__DELETED and (ROW__ID.writeid > 1L)) (type: boolean), ((ROW__ID.writeid > 1L) and (not ROW__IS__DELETED)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
+                      Statistics: Num rows: 2 Data size: 576 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 2 Data size: 576 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: varchar(256)), _col2 (type: decimal(10,2)), _col3 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col4 (type: boolean), _col5 (type: boolean)
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Map 7 
+            Map Operator Tree:
+                TableScan
+                  alias: cmv_basetable_n6
+                  filterExpr: a is not null (type: boolean)
+                  properties:
+                    acid.fetch.deleted.rows TRUE
+                  Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: a is not null (type: boolean)
+                    Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: a (type: int), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), (ROW__IS__DELETED and (ROW__ID.writeid > 1L)) (type: boolean), ((ROW__ID.writeid > 1L) and (not ROW__IS__DELETED)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2, _col3
+                      Statistics: Num rows: 4 Data size: 352 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 4 Data size: 352 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col2 (type: boolean), _col3 (type: boolean)
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Reducer 2 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Right Outer Join 0 to 1
+                keys:
+                  0 _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256))
+                  1 _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256))
+                nullSafes: [true, true, true]
+                outputColumnNames: _col3, _col4, _col5, _col6, _col7, _col8, _col9
+                Statistics: Num rows: 2 Data size: 600 Basic stats: COMPLETE Column stats: COMPLETE
+                Filter Operator
+                  predicate: (_col4 and ((_col3 is null and (_col9 > 0L)) or (((_col9 + _col3) > 0) and _col3 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 300 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col5 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                      null sort order: z
+                      sort order: +
+                      Map-reduce partition columns: UDFToInteger(_col0) (type: int)
+                      Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                Filter Operator
+                  predicate: (_col4 and ((_col3 is null and (_col9 = 0L)) or (((_col9 + _col3) = 0) and _col3 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 300 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col5 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                      null sort order: z
+                      sort order: +
+                      Map-reduce partition columns: UDFToInteger(_col0) (type: int)
+                      Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                Filter Operator
+                  predicate: (_col4 and ((_col3 is null and (_col9 > 0L)) or (((_col9 + _col3) > 0) and _col3 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 300 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col6 (type: int), _col7 (type: decimal(10,2)), _col8 (type: varchar(256)), if(_col3 is null, _col9, (_col9 + _col3)) (type: bigint)
+                    outputColumnNames: _col0, _col1, _col2, _col3
+                    Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                    File Output Operator
+                      compressed: false
+                      Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                          serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                          name: default.cmv_mat_view_n6
+                      Write Type: INSERT
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256)), _col3 (type: bigint)
+                      outputColumnNames: a, c, b, _c3
+                      Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: min(a), max(a), count(1), count(a), compute_bit_vector_hll(a), min(c), max(c), count(c), compute_bit_vector_hll(c), max(length(b)), avg(COALESCE(length(b),0)), count(b), compute_bit_vector_hll(b), min(_c3), max(_c3), count(_c3), compute_bit_vector_hll(_c3)
+                        minReductionHashAggr: 0.4
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16
+                        Statistics: Num rows: 1 Data size: 944 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          null sort order: 
+                          sort order: 
+                          Statistics: Num rows: 1 Data size: 944 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: struct<count:bigint,sum:double,input:int>), _col11 (type: bigint), _col12 (type: binary), _col13 (type: bigint), _col14 (type: bigint), _col15 (type: bigint), _col16 (type: binary)
+                Filter Operator
+                  predicate: (_col4 is null and ((_col3 is null and (_col9 > 0L)) or (((_col9 + _col3) > 0) and _col3 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 300 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col6 (type: int), _col7 (type: decimal(10,2)), _col8 (type: varchar(256)), if(_col3 is null, _col9, (_col9 + _col3)) (type: bigint)
+                    outputColumnNames: _col0, _col1, _col2, _col3
+                    Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                    File Output Operator
+                      compressed: false
+                      Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                          serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                          name: default.cmv_mat_view_n6
+                      Write Type: INSERT
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256)), _col3 (type: bigint)
+                      outputColumnNames: a, c, b, _c3
+                      Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: min(a), max(a), count(1), count(a), compute_bit_vector_hll(a), min(c), max(c), count(c), compute_bit_vector_hll(c), max(length(b)), avg(COALESCE(length(b),0)), count(b), compute_bit_vector_hll(b), min(_c3), max(_c3), count(_c3), compute_bit_vector_hll(_c3)
+                        minReductionHashAggr: 0.4
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16
+                        Statistics: Num rows: 1 Data size: 944 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          null sort order: 
+                          sort order: 
+                          Statistics: Num rows: 1 Data size: 944 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: struct<count:bigint,sum:double,input:int>), _col11 (type: bigint), _col12 (type: binary), _col13 (type: bigint), _col14 (type: bigint), _col15 (type: bigint), _col16 (type: binary)
+        Reducer 3 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                      name: default.cmv_mat_view_n6
+                  Write Type: DELETE
+        Reducer 4 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                      name: default.cmv_mat_view_n6
+                  Write Type: DELETE
+        Reducer 5 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), max(VALUE._col9), avg(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12), min(VALUE._col13), max(VALUE._col14), count(VALUE._col15), compute_bit_vector_hll(VALUE._col16)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16
+                Statistics: Num rows: 1 Data size: 876 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'DECIMAL' (type: string), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col9,0)) (type: bigint), COALESCE(_col10,0) (type: double), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary), 'LONG' (type: string), _col13 (type: bigint), _col14 (type: bigint), (_col2 - _col15) (type: bigint), COALESCE(ndv_compute_bit_vector(_col16),0) (type: bigint), _col16 (type: binary)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23
+                  Statistics: Num rows: 1 Data size: 1269 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 1 Data size: 1269 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 6 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), max(VALUE._col9), avg(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12), min(VALUE._col13), max(VALUE._col14), count(VALUE._col15), compute_bit_vector_hll(VALUE._col16)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16
+                Statistics: Num rows: 1 Data size: 876 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'DECIMAL' (type: string), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col9,0)) (type: bigint), COALESCE(_col10,0) (type: double), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary), 'LONG' (type: string), _col13 (type: bigint), _col14 (type: bigint), (_col2 - _col15) (type: bigint), COALESCE(ndv_compute_bit_vector(_col16),0) (type: bigint), _col16 (type: binary)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23
+                  Statistics: Num rows: 1 Data size: 1269 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 1 Data size: 1269 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 8 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 _col0 (type: int)
+                  1 _col0 (type: int)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col5, _col6, _col7, _col8, _col9
+                residual filter predicates: {(((not _col2) and (not _col8)) or ((not _col3) and (not _col9)))} {((_col7.writeid > 1L) or (_col1.writeid > 1L))}
+                Statistics: Num rows: 1 Data size: 372 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col0 (type: int), _col6 (type: decimal(10,2)), _col5 (type: varchar(256)), if((_col2 or _col8), -1, 1) (type: int)
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  Statistics: Num rows: 1 Data size: 372 Basic stats: COMPLETE Column stats: COMPLETE
+                  Group By Operator
+                    aggregations: sum(_col3)
+                    keys: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256))
+                    minReductionHashAggr: 0.4
+                    mode: hash
+                    outputColumnNames: _col0, _col1, _col2, _col3
+                    Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256))
+                      null sort order: zzz
+                      sort order: +++
+                      Map-reduce partition columns: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256))
+                      Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                      value expressions: _col3 (type: bigint)
+        Reducer 9 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                keys: KEY._col0 (type: int), KEY._col1 (type: decimal(10,2)), KEY._col2 (type: varchar(256))
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256))
+                  null sort order: zzz
+                  sort order: +++
+                  Map-reduce partition columns: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: varchar(256))
+                  Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                  value expressions: _col3 (type: bigint)
+
+  Stage: Stage-5
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.cmv_mat_view_n6
+          Write Type: DELETE
+
+  Stage: Stage-6
+    Stats Work
+      Basic Stats Work:
+
+  Stage: Stage-10
+    Materialized View Update
+      name: default.cmv_mat_view_n6
+      update creation metadata: true
+
+  Stage: Stage-1
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.cmv_mat_view_n6
+          Write Type: DELETE
+
+  Stage: Stage-7
+    Stats Work
+      Basic Stats Work:
+
+  Stage: Stage-2
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.cmv_mat_view_n6
+          Write Type: INSERT
+
+  Stage: Stage-8
+    Stats Work
+      Basic Stats Work:
+
+  Stage: Stage-3
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.cmv_mat_view_n6
+          Write Type: INSERT
+
+  Stage: Stage-9
+    Stats Work
+      Basic Stats Work:
+      Column Stats Desc:
+          Columns: a, c, b, _c3
+          Column Types: int, decimal(10,2), varchar(256), bigint
+          Table: default.cmv_mat_view_n6
+
+PREHOOK: query: ALTER MATERIALIZED VIEW cmv_mat_view_n6 REBUILD
+PREHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@cmv_mat_view_n6
+PREHOOK: Output: default@cmv_mat_view_n6
+PREHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: query: ALTER MATERIALIZED VIEW cmv_mat_view_n6 REBUILD
+POSTHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@cmv_mat_view_n6
+POSTHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: Lineage: cmv_mat_view_n6._c3 EXPRESSION [(cmv_mat_view_n6)default.cmv_mat_view_n6.FieldSchema(name:_c3, type:bigint, comment:null), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
+POSTHOOK: Lineage: cmv_mat_view_n6._c3 EXPRESSION [(cmv_mat_view_n6)default.cmv_mat_view_n6.FieldSchema(name:_c3, type:bigint, comment:null), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
+POSTHOOK: Lineage: cmv_mat_view_n6.a SIMPLE [(cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:a, type:int, comment:null), ]
+POSTHOOK: Lineage: cmv_mat_view_n6.a SIMPLE [(cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:a, type:int, comment:null), ]
+POSTHOOK: Lineage: cmv_mat_view_n6.b SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:b, type:varchar(256), comment:null), ]
+POSTHOOK: Lineage: cmv_mat_view_n6.b SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:b, type:varchar(256), comment:null), ]
+POSTHOOK: Lineage: cmv_mat_view_n6.c SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:c, type:decimal(10,2), comment:null), ]
+POSTHOOK: Lineage: cmv_mat_view_n6.c SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:c, type:decimal(10,2), comment:null), ]
+PREHOOK: query: select * from cmv_mat_view_n6
+PREHOOK: type: QUERY
+PREHOOK: Input: default@cmv_mat_view_n6
+#### A masked pattern was here ####
+POSTHOOK: query: select * from cmv_mat_view_n6
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@cmv_mat_view_n6
+#### A masked pattern was here ####
+2	130.30	joe	1
+1	30.30	bob	3
+PREHOOK: query: drop materialized view cmv_mat_view_n6
+PREHOOK: type: DROP_MATERIALIZED_VIEW
+PREHOOK: Input: default@cmv_mat_view_n6
+PREHOOK: Output: default@cmv_mat_view_n6
+POSTHOOK: query: drop materialized view cmv_mat_view_n6
+POSTHOOK: type: DROP_MATERIALIZED_VIEW
+POSTHOOK: Input: default@cmv_mat_view_n6
+POSTHOOK: Output: default@cmv_mat_view_n6
+PREHOOK: query: SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b, count(*)
+FROM cmv_basetable_n6 JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b
+PREHOOK: type: QUERY
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b, count(*)
+FROM cmv_basetable_n6 JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c, cmv_basetable_2_n3.b
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+#### A masked pattern was here ####
+2	130.30	joe	1
+1	30.30	bob	3
diff --git a/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6_aggr_3joins.q.out b/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6_aggr_3joins.q.out
new file mode 100644
index 0000000000..163286b1ff
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_6_aggr_3joins.q.out
@@ -0,0 +1,1208 @@
+PREHOOK: query: create table cmv_basetable_n6 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: query: create table cmv_basetable_n6 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@cmv_basetable_n6
+PREHOOK: query: insert into cmv_basetable_n6 values
+(1, 'alfred', 10.30, 2),
+(1, 'charlie', 20.30, 2),
+(2, 'zoe', 100.30, 2)
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: query: insert into cmv_basetable_n6 values
+(1, 'alfred', 10.30, 2),
+(1, 'charlie', 20.30, 2),
+(2, 'zoe', 100.30, 2)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: Lineage: cmv_basetable_n6.a SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.b SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.c SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.d SCRIPT []
+PREHOOK: query: create table cmv_basetable_2_n3 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@cmv_basetable_2_n3
+POSTHOOK: query: create table cmv_basetable_2_n3 (a int, b varchar(256), c decimal(10,2), d int) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@cmv_basetable_2_n3
+PREHOOK: query: insert into cmv_basetable_2_n3 values
+(1, 'bob', 30.30, 2),
+(1, 'bonnie', 40.30, 2),
+(2, 'joe', 130.30, 2)
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@cmv_basetable_2_n3
+POSTHOOK: query: insert into cmv_basetable_2_n3 values
+(1, 'bob', 30.30, 2),
+(1, 'bonnie', 40.30, 2),
+(2, 'joe', 130.30, 2)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@cmv_basetable_2_n3
+POSTHOOK: Lineage: cmv_basetable_2_n3.a SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_2_n3.b SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_2_n3.c SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_2_n3.d SCRIPT []
+PREHOOK: query: create table t3 (a int, b varchar(256), c decimal(10,2)) stored as orc TBLPROPERTIES ('transactional'='true')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@t3
+POSTHOOK: query: create table t3 (a int, b varchar(256), c decimal(10,2)) stored as orc TBLPROPERTIES ('transactional'='true')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@t3
+PREHOOK: query: insert into t3 values
+(1, 'foo', 30.30),
+(1, 'bar', 30.30),
+(2, 'bar', 30.30)
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@t3
+POSTHOOK: query: insert into t3 values
+(1, 'foo', 30.30),
+(1, 'bar', 30.30),
+(2, 'bar', 30.30)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@t3
+POSTHOOK: Lineage: t3.a SCRIPT []
+POSTHOOK: Lineage: t3.b SCRIPT []
+POSTHOOK: Lineage: t3.c SCRIPT []
+PREHOOK: query: CREATE MATERIALIZED VIEW mat1 TBLPROPERTIES ('transactional'='true') AS
+SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, count(*)
+FROM cmv_basetable_n6
+JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+JOIN t3 ON (t3.a = cmv_basetable_2_n3.a)
+WHERE cmv_basetable_n6.c > 10 OR cmv_basetable_2_n3.c > 10
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c
+PREHOOK: type: CREATE_MATERIALIZED_VIEW
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@t3
+PREHOOK: Output: database:default
+PREHOOK: Output: default@mat1
+POSTHOOK: query: CREATE MATERIALIZED VIEW mat1 TBLPROPERTIES ('transactional'='true') AS
+SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, count(*)
+FROM cmv_basetable_n6
+JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+JOIN t3 ON (t3.a = cmv_basetable_2_n3.a)
+WHERE cmv_basetable_n6.c > 10 OR cmv_basetable_2_n3.c > 10
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c
+POSTHOOK: type: CREATE_MATERIALIZED_VIEW
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@t3
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@mat1
+POSTHOOK: Lineage: mat1._c2 EXPRESSION [(cmv_basetable_2_n3)cmv_basetable_2_n3.null, (t3)t3.null, (cmv_basetable_n6)cmv_basetable_n6.null, ]
+POSTHOOK: Lineage: mat1.a SIMPLE [(cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:a, type:int, comment:null), ]
+POSTHOOK: Lineage: mat1.c SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:c, type:decimal(10,2), comment:null), ]
+PREHOOK: query: insert into cmv_basetable_n6 values
+(1, 'kevin', 50.30, 2)
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: query: insert into cmv_basetable_n6 values
+(1, 'kevin', 50.30, 2)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: Lineage: cmv_basetable_n6.a SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.b SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.c SCRIPT []
+POSTHOOK: Lineage: cmv_basetable_n6.d SCRIPT []
+PREHOOK: query: insert into t3 values
+(1, 'new rec', 60.30)
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@t3
+POSTHOOK: query: insert into t3 values
+(1, 'new rec', 60.30)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@t3
+POSTHOOK: Lineage: t3.a SCRIPT []
+POSTHOOK: Lineage: t3.b SCRIPT []
+POSTHOOK: Lineage: t3.c SCRIPT []
+PREHOOK: query: DELETE FROM cmv_basetable_2_n3 WHERE b = 'bonnie'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Output: default@cmv_basetable_2_n3
+POSTHOOK: query: DELETE FROM cmv_basetable_2_n3 WHERE b = 'bonnie'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Output: default@cmv_basetable_2_n3
+PREHOOK: query: EXPLAIN CBO
+ALTER MATERIALIZED VIEW mat1 REBUILD
+PREHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@mat1
+PREHOOK: Input: default@t3
+PREHOOK: Output: default@mat1
+PREHOOK: Output: default@mat1
+POSTHOOK: query: EXPLAIN CBO
+ALTER MATERIALIZED VIEW mat1 REBUILD
+POSTHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@mat1
+POSTHOOK: Input: default@t3
+POSTHOOK: Output: default@mat1
+POSTHOOK: Output: default@mat1
+CBO PLAN:
+HiveProject(a0=[$4], c0=[$5], $f2=[CASE(IS NULL($2), $6, +($6, $2))])
+  HiveFilter(condition=[OR(AND($3, OR(AND(IS NULL($2), =($6, 0)), AND(=(+($6, $2), 0), IS NOT NULL($2)))), AND(IS NULL($3), OR(AND(IS NULL($2), >($6, 0)), AND(>(+($6, $2), 0), IS NOT NULL($2)))), AND($3, OR(AND(IS NULL($2), >($6, 0)), AND(>(+($6, $2), 0), IS NOT NULL($2)))))])
+    HiveJoin(condition=[AND(IS NOT DISTINCT FROM($0, $4), IS NOT DISTINCT FROM($1, $5))], joinType=[right], algorithm=[none], cost=[not available])
+      HiveProject(a=[$0], c=[$1], _c2=[$2], $f3=[true])
+        HiveTableScan(table=[[default, mat1]], table:alias=[default.mat1])
+      HiveProject(a=[$0], c0=[$1], $f2=[$2])
+        HiveAggregate(group=[{0, 1}], agg#0=[SUM($2)])
+          HiveProject(a=[$9], c0=[$5], $f4=[CASE(OR($12, $7, $2), -1, 1)])
+            HiveJoin(condition=[AND(=($9, $4), OR(>($10, 10:DECIMAL(2, 0)), >($5, 10:DECIMAL(2, 0))), OR(AND(NOT($12), NOT($7)), AND(NOT($13), NOT($8))), OR(AND(NOT($12), NOT($7), NOT($2)), AND(NOT($13), NOT($8), NOT($3))), OR(<(1, $6.writeid), <(1, $11.writeid), <(1, $1.writeid)))], joinType=[inner], algorithm=[none], cost=[not available])
+              HiveJoin(condition=[=($0, $4)], joinType=[inner], algorithm=[none], cost=[not available])
+                HiveProject(a=[$0], ROW__ID=[$5], _deleted=[AND($6, <(1, $5.writeid))], _inserted=[AND(<(1, $5.writeid), NOT($6))])
+                  HiveFilter(condition=[IS NOT NULL($0)])
+                    HiveTableScan(table=[[default, t3]], table:alias=[t3])
+                HiveProject(a=[$0], c=[$2], ROW__ID=[$6], _deleted=[AND($7, <(1, $6.writeid))], _inserted=[AND(<(1, $6.writeid), NOT($7))])
+                  HiveFilter(condition=[IS NOT NULL($0)])
+                    HiveTableScan(table=[[default, cmv_basetable_2_n3]], table:alias=[cmv_basetable_2_n3])
+              HiveProject(a=[$0], c=[$2], ROW__ID=[$6], _deleted=[AND($7, <(1, $6.writeid))], _inserted=[AND(<(1, $6.writeid), NOT($7))])
+                HiveFilter(condition=[IS NOT NULL($0)])
+                  HiveTableScan(table=[[default, cmv_basetable_n6]], table:alias=[cmv_basetable_n6])
+
+PREHOOK: query: EXPLAIN
+ALTER MATERIALIZED VIEW mat1 REBUILD
+PREHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@mat1
+PREHOOK: Input: default@t3
+PREHOOK: Output: default@mat1
+PREHOOK: Output: default@mat1
+POSTHOOK: query: EXPLAIN
+ALTER MATERIALIZED VIEW mat1 REBUILD
+POSTHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@mat1
+POSTHOOK: Input: default@t3
+POSTHOOK: Output: default@mat1
+POSTHOOK: Output: default@mat1
+STAGE DEPENDENCIES:
+  Stage-4 is a root stage
+  Stage-5 depends on stages: Stage-4
+  Stage-0 depends on stages: Stage-5
+  Stage-6 depends on stages: Stage-0
+  Stage-10 depends on stages: Stage-6, Stage-7, Stage-8, Stage-9
+  Stage-1 depends on stages: Stage-5
+  Stage-7 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-5
+  Stage-8 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-5
+  Stage-9 depends on stages: Stage-3
+
+STAGE PLANS:
+  Stage: Stage-4
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 10 <- Reducer 9 (SIMPLE_EDGE)
+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 10 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
+        Reducer 4 <- Reducer 2 (SIMPLE_EDGE)
+        Reducer 5 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 6 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 8 <- Map 11 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE)
+        Reducer 9 <- Map 12 (SIMPLE_EDGE), Reducer 8 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: default.mat1
+                  Statistics: Num rows: 3 Data size: 372 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: a (type: int), c (type: decimal(10,2)), _c2 (type: bigint), true (type: boolean), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                    Statistics: Num rows: 3 Data size: 612 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: int), _col1 (type: decimal(10,2))
+                      null sort order: zz
+                      sort order: ++
+                      Map-reduce partition columns: _col0 (type: int), _col1 (type: decimal(10,2))
+                      Statistics: Num rows: 3 Data size: 612 Basic stats: COMPLETE Column stats: COMPLETE
+                      value expressions: _col2 (type: bigint), _col3 (type: boolean), _col4 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Map 11 
+            Map Operator Tree:
+                TableScan
+                  alias: cmv_basetable_2_n3
+                  filterExpr: a is not null (type: boolean)
+                  properties:
+                    acid.fetch.deleted.rows TRUE
+                  Statistics: Num rows: 2 Data size: 232 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: a is not null (type: boolean)
+                    Statistics: Num rows: 2 Data size: 232 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: a (type: int), c (type: decimal(10,2)), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), (ROW__IS__DELETED and (ROW__ID.writeid > 1L)) (type: boolean), ((ROW__ID.writeid > 1L) and (not ROW__IS__DELETED)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                      Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: decimal(10,2)), _col2 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col3 (type: boolean), _col4 (type: boolean)
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Map 12 
+            Map Operator Tree:
+                TableScan
+                  alias: cmv_basetable_n6
+                  filterExpr: a is not null (type: boolean)
+                  properties:
+                    acid.fetch.deleted.rows TRUE
+                  Statistics: Num rows: 4 Data size: 464 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: a is not null (type: boolean)
+                    Statistics: Num rows: 4 Data size: 464 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: a (type: int), c (type: decimal(10,2)), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), (ROW__IS__DELETED and (ROW__ID.writeid > 1L)) (type: boolean), ((ROW__ID.writeid > 1L) and (not ROW__IS__DELETED)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                      Statistics: Num rows: 4 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 4 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: decimal(10,2)), _col2 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col3 (type: boolean), _col4 (type: boolean)
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Map 7 
+            Map Operator Tree:
+                TableScan
+                  alias: t3
+                  filterExpr: a is not null (type: boolean)
+                  properties:
+                    acid.fetch.deleted.rows TRUE
+                  Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: a is not null (type: boolean)
+                    Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: a (type: int), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), (ROW__IS__DELETED and (ROW__ID.writeid > 1L)) (type: boolean), ((ROW__ID.writeid > 1L) and (not ROW__IS__DELETED)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2, _col3
+                      Statistics: Num rows: 4 Data size: 352 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 4 Data size: 352 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col2 (type: boolean), _col3 (type: boolean)
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Reducer 10 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                keys: KEY._col0 (type: int), KEY._col1 (type: decimal(10,2))
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int), _col1 (type: decimal(10,2))
+                  null sort order: zz
+                  sort order: ++
+                  Map-reduce partition columns: _col0 (type: int), _col1 (type: decimal(10,2))
+                  Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                  value expressions: _col2 (type: bigint)
+        Reducer 2 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Right Outer Join 0 to 1
+                keys:
+                  0 _col0 (type: int), _col1 (type: decimal(10,2))
+                  1 _col0 (type: int), _col1 (type: decimal(10,2))
+                nullSafes: [true, true]
+                outputColumnNames: _col2, _col3, _col4, _col5, _col6, _col7
+                Statistics: Num rows: 2 Data size: 424 Basic stats: COMPLETE Column stats: COMPLETE
+                Filter Operator
+                  predicate: (_col3 and ((_col2 is null and (_col7 > 0L)) or (((_col7 + _col2) > 0) and _col2 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col4 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                      null sort order: z
+                      sort order: +
+                      Map-reduce partition columns: UDFToInteger(_col0) (type: int)
+                      Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                Filter Operator
+                  predicate: (_col3 and ((_col2 is null and (_col7 = 0L)) or (((_col7 + _col2) = 0) and _col2 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col4 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                      null sort order: z
+                      sort order: +
+                      Map-reduce partition columns: UDFToInteger(_col0) (type: int)
+                      Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                Filter Operator
+                  predicate: (_col3 and ((_col2 is null and (_col7 > 0L)) or (((_col7 + _col2) > 0) and _col2 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col5 (type: int), _col6 (type: decimal(10,2)), if(_col2 is null, _col7, (_col7 + _col2)) (type: bigint)
+                    outputColumnNames: _col0, _col1, _col2
+                    Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                    File Output Operator
+                      compressed: false
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                          serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                          name: default.mat1
+                      Write Type: INSERT
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: bigint)
+                      outputColumnNames: a, c, _c2
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: min(a), max(a), count(1), count(a), compute_bit_vector_hll(a), min(c), max(c), count(c), compute_bit_vector_hll(c), min(_c2), max(_c2), count(_c2), compute_bit_vector_hll(_c2)
+                        minReductionHashAggr: 0.4
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                        Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          null sort order: 
+                          sort order: 
+                          Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), _col7 (type: bigint), _col8 (type: binary), _col9 (type: bigint), _col10 (type: bigint), _col11 (type: bigint), _col12 (type: binary)
+                Filter Operator
+                  predicate: (_col3 is null and ((_col2 is null and (_col7 > 0L)) or (((_col7 + _col2) > 0) and _col2 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col5 (type: int), _col6 (type: decimal(10,2)), if(_col2 is null, _col7, (_col7 + _col2)) (type: bigint)
+                    outputColumnNames: _col0, _col1, _col2
+                    Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                    File Output Operator
+                      compressed: false
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                          serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                          name: default.mat1
+                      Write Type: INSERT
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: bigint)
+                      outputColumnNames: a, c, _c2
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: min(a), max(a), count(1), count(a), compute_bit_vector_hll(a), min(c), max(c), count(c), compute_bit_vector_hll(c), min(_c2), max(_c2), count(_c2), compute_bit_vector_hll(_c2)
+                        minReductionHashAggr: 0.4
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                        Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          null sort order: 
+                          sort order: 
+                          Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), _col7 (type: bigint), _col8 (type: binary), _col9 (type: bigint), _col10 (type: bigint), _col11 (type: bigint), _col12 (type: binary)
+        Reducer 3 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                      name: default.mat1
+                  Write Type: DELETE
+        Reducer 4 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                      name: default.mat1
+                  Write Type: DELETE
+        Reducer 5 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), min(VALUE._col9), max(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'DECIMAL' (type: string), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), _col9 (type: bigint), _col10 (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
+                  Statistics: Num rows: 1 Data size: 1003 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 1 Data size: 1003 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 6 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), min(VALUE._col9), max(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'DECIMAL' (type: string), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), _col9 (type: bigint), _col10 (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
+                  Statistics: Num rows: 1 Data size: 1003 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 1 Data size: 1003 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 8 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 _col0 (type: int)
+                  1 _col0 (type: int)
+                outputColumnNames: _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
+                Statistics: Num rows: 4 Data size: 1136 Basic stats: COMPLETE Column stats: COMPLETE
+                Reduce Output Operator
+                  key expressions: _col4 (type: int)
+                  null sort order: z
+                  sort order: +
+                  Map-reduce partition columns: _col4 (type: int)
+                  Statistics: Num rows: 4 Data size: 1136 Basic stats: COMPLETE Column stats: COMPLETE
+                  value expressions: _col1 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col2 (type: boolean), _col3 (type: boolean), _col5 (type: decimal(10,2)), _col6 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col7 (type: boolean), _col8 (type: boolean)
+        Reducer 9 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 _col4 (type: int)
+                  1 _col0 (type: int)
+                outputColumnNames: _col1, _col2, _col3, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13
+                residual filter predicates: {((_col10 > 10) or (_col5 > 10))} {(((not _col12) and (not _col7)) or ((not _col13) and (not _col8)))} {(((not _col12) and (not _col7) and (not _col2)) or ((not _col13) and (not _col8) and (not _col3)))} {((_col6.writeid > 1L) or (_col11.writeid > 1L) or (_col1.writeid > 1L))}
+                Statistics: Num rows: 1 Data size: 480 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col9 (type: int), _col5 (type: decimal(10,2)), if((_col12 or _col7 or _col2), -1, 1) (type: int)
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 1 Data size: 480 Basic stats: COMPLETE Column stats: COMPLETE
+                  Group By Operator
+                    aggregations: sum(_col2)
+                    keys: _col0 (type: int), _col1 (type: decimal(10,2))
+                    minReductionHashAggr: 0.4
+                    mode: hash
+                    outputColumnNames: _col0, _col1, _col2
+                    Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: int), _col1 (type: decimal(10,2))
+                      null sort order: zz
+                      sort order: ++
+                      Map-reduce partition columns: _col0 (type: int), _col1 (type: decimal(10,2))
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      value expressions: _col2 (type: bigint)
+
+  Stage: Stage-5
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.mat1
+          Write Type: DELETE
+
+  Stage: Stage-6
+    Stats Work
+      Basic Stats Work:
+
+  Stage: Stage-10
+    Materialized View Update
+      name: default.mat1
+      update creation metadata: true
+
+  Stage: Stage-1
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.mat1
+          Write Type: DELETE
+
+  Stage: Stage-7
+    Stats Work
+      Basic Stats Work:
+
+  Stage: Stage-2
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.mat1
+          Write Type: INSERT
+
+  Stage: Stage-8
+    Stats Work
+      Basic Stats Work:
+
+  Stage: Stage-3
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.mat1
+          Write Type: INSERT
+
+  Stage: Stage-9
+    Stats Work
+      Basic Stats Work:
+      Column Stats Desc:
+          Columns: a, c, _c2
+          Column Types: int, decimal(10,2), bigint
+          Table: default.mat1
+
+PREHOOK: query: ALTER MATERIALIZED VIEW mat1 REBUILD
+PREHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@mat1
+PREHOOK: Input: default@t3
+PREHOOK: Output: default@mat1
+PREHOOK: Output: default@mat1
+POSTHOOK: query: ALTER MATERIALIZED VIEW mat1 REBUILD
+POSTHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@mat1
+POSTHOOK: Input: default@t3
+POSTHOOK: Output: default@mat1
+POSTHOOK: Output: default@mat1
+POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t3)t3.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t3)t3.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
+POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t3)t3.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t3)t3.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
+POSTHOOK: Lineage: mat1.a SIMPLE [(cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:a, type:int, comment:null), ]
+POSTHOOK: Lineage: mat1.a SIMPLE [(cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:a, type:int, comment:null), ]
+POSTHOOK: Lineage: mat1.c SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:c, type:decimal(10,2), comment:null), ]
+POSTHOOK: Lineage: mat1.c SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:c, type:decimal(10,2), comment:null), ]
+PREHOOK: query: select * from mat1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@mat1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from mat1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@mat1
+#### A masked pattern was here ####
+2	130.30	1
+1	30.30	9
+PREHOOK: query: delete from cmv_basetable_n6 where b = 'kevin'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Output: default@cmv_basetable_n6
+POSTHOOK: query: delete from cmv_basetable_n6 where b = 'kevin'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Output: default@cmv_basetable_n6
+PREHOOK: query: EXPLAIN CBO
+ALTER MATERIALIZED VIEW mat1 REBUILD
+PREHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@mat1
+PREHOOK: Input: default@t3
+PREHOOK: Output: default@mat1
+PREHOOK: Output: default@mat1
+POSTHOOK: query: EXPLAIN CBO
+ALTER MATERIALIZED VIEW mat1 REBUILD
+POSTHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@mat1
+POSTHOOK: Input: default@t3
+POSTHOOK: Output: default@mat1
+POSTHOOK: Output: default@mat1
+CBO PLAN:
+HiveProject(a0=[$4], c0=[$5], $f2=[CASE(IS NULL($2), $6, +($6, $2))])
+  HiveFilter(condition=[OR(AND($3, OR(AND(IS NULL($2), =($6, 0)), AND(=(+($6, $2), 0), IS NOT NULL($2)))), AND(IS NULL($3), OR(AND(IS NULL($2), >($6, 0)), AND(>(+($6, $2), 0), IS NOT NULL($2)))), AND($3, OR(AND(IS NULL($2), >($6, 0)), AND(>(+($6, $2), 0), IS NOT NULL($2)))))])
+    HiveJoin(condition=[AND(IS NOT DISTINCT FROM($0, $4), IS NOT DISTINCT FROM($1, $5))], joinType=[right], algorithm=[none], cost=[not available])
+      HiveProject(a=[$0], c=[$1], _c2=[$2], $f3=[true])
+        HiveTableScan(table=[[default, mat1]], table:alias=[default.mat1])
+      HiveProject(a=[$0], c0=[$1], $f2=[$2])
+        HiveAggregate(group=[{0, 1}], agg#0=[SUM($2)])
+          HiveProject(a=[$3], c0=[$2], $f4=[CASE($5, -1, 1)])
+            HiveJoin(condition=[=($0, $1)], joinType=[inner], algorithm=[none], cost=[not available])
+              HiveProject(a=[$0])
+                HiveFilter(condition=[IS NOT NULL($0)])
+                  HiveTableScan(table=[[default, t3]], table:alias=[t3])
+              HiveJoin(condition=[AND(=($2, $0), OR(>($3, 10:DECIMAL(2, 0)), >($1, 10:DECIMAL(2, 0))))], joinType=[inner], algorithm=[none], cost=[not available])
+                HiveProject(a=[$0], c=[$2])
+                  HiveFilter(condition=[IS NOT NULL($0)])
+                    HiveTableScan(table=[[default, cmv_basetable_2_n3]], table:alias=[cmv_basetable_2_n3])
+                HiveProject(a=[$0], c=[$2], _deleted=[AND($7, <(2, $6.writeid))])
+                  HiveFilter(condition=[AND(<(2, $6.writeid), OR(NOT($7), >=(2, $6.writeid), $7), IS NOT NULL($0))])
+                    HiveTableScan(table=[[default, cmv_basetable_n6]], table:alias=[cmv_basetable_n6])
+
+PREHOOK: query: EXPLAIN
+ALTER MATERIALIZED VIEW mat1 REBUILD
+PREHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@mat1
+PREHOOK: Input: default@t3
+PREHOOK: Output: default@mat1
+PREHOOK: Output: default@mat1
+POSTHOOK: query: EXPLAIN
+ALTER MATERIALIZED VIEW mat1 REBUILD
+POSTHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@mat1
+POSTHOOK: Input: default@t3
+POSTHOOK: Output: default@mat1
+POSTHOOK: Output: default@mat1
+STAGE DEPENDENCIES:
+  Stage-4 is a root stage
+  Stage-5 depends on stages: Stage-4
+  Stage-0 depends on stages: Stage-5
+  Stage-6 depends on stages: Stage-0
+  Stage-10 depends on stages: Stage-6, Stage-7, Stage-8, Stage-9
+  Stage-1 depends on stages: Stage-5
+  Stage-7 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-5
+  Stage-8 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-5
+  Stage-9 depends on stages: Stage-3
+
+STAGE PLANS:
+  Stage: Stage-4
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 10 <- Reducer 9 (SIMPLE_EDGE)
+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 10 (SIMPLE_EDGE)
+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
+        Reducer 4 <- Reducer 2 (SIMPLE_EDGE)
+        Reducer 5 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 6 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
+        Reducer 8 <- Map 11 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE)
+        Reducer 9 <- Map 12 (SIMPLE_EDGE), Reducer 8 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: default.mat1
+                  Statistics: Num rows: 2 Data size: 248 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: a (type: int), c (type: decimal(10,2)), _c2 (type: bigint), true (type: boolean), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                    Statistics: Num rows: 2 Data size: 408 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: int), _col1 (type: decimal(10,2))
+                      null sort order: zz
+                      sort order: ++
+                      Map-reduce partition columns: _col0 (type: int), _col1 (type: decimal(10,2))
+                      Statistics: Num rows: 2 Data size: 408 Basic stats: COMPLETE Column stats: COMPLETE
+                      value expressions: _col2 (type: bigint), _col3 (type: boolean), _col4 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Map 11 
+            Map Operator Tree:
+                TableScan
+                  alias: cmv_basetable_n6
+                  filterExpr: ((ROW__ID.writeid > 2L) and ((not ROW__IS__DELETED) or (ROW__ID.writeid <= 2L) or ROW__IS__DELETED) and a is not null) (type: boolean)
+                  properties:
+                    acid.fetch.deleted.rows TRUE
+                  Statistics: Num rows: 3 Data size: 348 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: ((ROW__ID.writeid > 2L) and ((not ROW__IS__DELETED) or (ROW__ID.writeid <= 2L) or ROW__IS__DELETED) and a is not null) (type: boolean)
+                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: a (type: int), c (type: decimal(10,2)), (ROW__IS__DELETED and (ROW__ID.writeid > 2L)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: decimal(10,2)), _col2 (type: boolean)
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Map 12 
+            Map Operator Tree:
+                TableScan
+                  alias: t3
+                  filterExpr: a is not null (type: boolean)
+                  properties:
+                    acid.fetch.deleted.rows TRUE
+                  Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: a is not null (type: boolean)
+                    Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: a (type: int)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Map 7 
+            Map Operator Tree:
+                TableScan
+                  alias: cmv_basetable_2_n3
+                  filterExpr: a is not null (type: boolean)
+                  properties:
+                    acid.fetch.deleted.rows TRUE
+                  Statistics: Num rows: 2 Data size: 232 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: a is not null (type: boolean)
+                    Statistics: Num rows: 2 Data size: 232 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: a (type: int), c (type: decimal(10,2))
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 2 Data size: 232 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 2 Data size: 232 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: decimal(10,2))
+            Execution mode: vectorized, llap
+            LLAP IO: may be used (ACID table)
+        Reducer 10 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                keys: KEY._col0 (type: int), KEY._col1 (type: decimal(10,2))
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int), _col1 (type: decimal(10,2))
+                  null sort order: zz
+                  sort order: ++
+                  Map-reduce partition columns: _col0 (type: int), _col1 (type: decimal(10,2))
+                  Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                  value expressions: _col2 (type: bigint)
+        Reducer 2 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Right Outer Join 0 to 1
+                keys:
+                  0 _col0 (type: int), _col1 (type: decimal(10,2))
+                  1 _col0 (type: int), _col1 (type: decimal(10,2))
+                nullSafes: [true, true]
+                outputColumnNames: _col2, _col3, _col4, _col5, _col6, _col7
+                Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                Filter Operator
+                  predicate: (_col3 and ((_col2 is null and (_col7 > 0L)) or (((_col7 + _col2) > 0) and _col2 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col4 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                      null sort order: z
+                      sort order: +
+                      Map-reduce partition columns: UDFToInteger(_col0) (type: int)
+                      Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                Filter Operator
+                  predicate: (_col3 and ((_col2 is null and (_col7 = 0L)) or (((_col7 + _col2) = 0) and _col2 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col4 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                    outputColumnNames: _col0
+                    Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                      null sort order: z
+                      sort order: +
+                      Map-reduce partition columns: UDFToInteger(_col0) (type: int)
+                      Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                Filter Operator
+                  predicate: (_col3 and ((_col2 is null and (_col7 > 0L)) or (((_col7 + _col2) > 0) and _col2 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col5 (type: int), _col6 (type: decimal(10,2)), if(_col2 is null, _col7, (_col7 + _col2)) (type: bigint)
+                    outputColumnNames: _col0, _col1, _col2
+                    Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                    File Output Operator
+                      compressed: false
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                          serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                          name: default.mat1
+                      Write Type: INSERT
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: bigint)
+                      outputColumnNames: a, c, _c2
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: min(a), max(a), count(1), count(a), compute_bit_vector_hll(a), min(c), max(c), count(c), compute_bit_vector_hll(c), min(_c2), max(_c2), count(_c2), compute_bit_vector_hll(_c2)
+                        minReductionHashAggr: 0.4
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                        Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          null sort order: 
+                          sort order: 
+                          Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), _col7 (type: bigint), _col8 (type: binary), _col9 (type: bigint), _col10 (type: bigint), _col11 (type: bigint), _col12 (type: binary)
+                Filter Operator
+                  predicate: (_col3 is null and ((_col2 is null and (_col7 > 0L)) or (((_col7 + _col2) > 0) and _col2 is not null))) (type: boolean)
+                  Statistics: Num rows: 1 Data size: 212 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: _col5 (type: int), _col6 (type: decimal(10,2)), if(_col2 is null, _col7, (_col7 + _col2)) (type: bigint)
+                    outputColumnNames: _col0, _col1, _col2
+                    Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                    File Output Operator
+                      compressed: false
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                          serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                          name: default.mat1
+                      Write Type: INSERT
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: decimal(10,2)), _col2 (type: bigint)
+                      outputColumnNames: a, c, _c2
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      Group By Operator
+                        aggregations: min(a), max(a), count(1), count(a), compute_bit_vector_hll(a), min(c), max(c), count(c), compute_bit_vector_hll(c), min(_c2), max(_c2), count(_c2), compute_bit_vector_hll(_c2)
+                        minReductionHashAggr: 0.4
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                        Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                        Reduce Output Operator
+                          null sort order: 
+                          sort order: 
+                          Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                          value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), _col7 (type: bigint), _col8 (type: binary), _col9 (type: bigint), _col10 (type: bigint), _col11 (type: bigint), _col12 (type: binary)
+        Reducer 3 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                      name: default.mat1
+                  Write Type: DELETE
+        Reducer 4 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 76 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                      name: default.mat1
+                  Write Type: DELETE
+        Reducer 5 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), min(VALUE._col9), max(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'DECIMAL' (type: string), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), _col9 (type: bigint), _col10 (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
+                  Statistics: Num rows: 1 Data size: 1003 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 1 Data size: 1003 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 6 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), min(VALUE._col9), max(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
+                Statistics: Num rows: 1 Data size: 712 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'DECIMAL' (type: string), _col5 (type: decimal(10,2)), _col6 (type: decimal(10,2)), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), _col9 (type: bigint), _col10 (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
+                  Statistics: Num rows: 1 Data size: 1003 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 1 Data size: 1003 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 8 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 _col0 (type: int)
+                  1 _col0 (type: int)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                residual filter predicates: {((_col3 > 10) or (_col1 > 10))}
+                Statistics: Num rows: 1 Data size: 236 Basic stats: COMPLETE Column stats: COMPLETE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int)
+                  null sort order: z
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: int)
+                  Statistics: Num rows: 1 Data size: 236 Basic stats: COMPLETE Column stats: COMPLETE
+                  value expressions: _col1 (type: decimal(10,2)), _col2 (type: int), _col4 (type: boolean)
+        Reducer 9 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 _col0 (type: int)
+                  1 _col0 (type: int)
+                outputColumnNames: _col1, _col2, _col4
+                Statistics: Num rows: 2 Data size: 240 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col2 (type: int), _col1 (type: decimal(10,2)), if(_col4, -1, 1) (type: int)
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 2 Data size: 240 Basic stats: COMPLETE Column stats: COMPLETE
+                  Group By Operator
+                    aggregations: sum(_col2)
+                    keys: _col0 (type: int), _col1 (type: decimal(10,2))
+                    minReductionHashAggr: 0.5
+                    mode: hash
+                    outputColumnNames: _col0, _col1, _col2
+                    Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: int), _col1 (type: decimal(10,2))
+                      null sort order: zz
+                      sort order: ++
+                      Map-reduce partition columns: _col0 (type: int), _col1 (type: decimal(10,2))
+                      Statistics: Num rows: 1 Data size: 124 Basic stats: COMPLETE Column stats: COMPLETE
+                      value expressions: _col2 (type: bigint)
+
+  Stage: Stage-5
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.mat1
+          Write Type: DELETE
+
+  Stage: Stage-6
+    Stats Work
+      Basic Stats Work:
+
+  Stage: Stage-10
+    Materialized View Update
+      name: default.mat1
+      update creation metadata: true
+
+  Stage: Stage-1
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.mat1
+          Write Type: DELETE
+
+  Stage: Stage-7
+    Stats Work
+      Basic Stats Work:
+
+  Stage: Stage-2
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.mat1
+          Write Type: INSERT
+
+  Stage: Stage-8
+    Stats Work
+      Basic Stats Work:
+
+  Stage: Stage-3
+    Move Operator
+      tables:
+          replace: false
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.mat1
+          Write Type: INSERT
+
+  Stage: Stage-9
+    Stats Work
+      Basic Stats Work:
+      Column Stats Desc:
+          Columns: a, c, _c2
+          Column Types: int, decimal(10,2), bigint
+          Table: default.mat1
+
+PREHOOK: query: ALTER MATERIALIZED VIEW mat1 REBUILD
+PREHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@mat1
+PREHOOK: Input: default@t3
+PREHOOK: Output: default@mat1
+PREHOOK: Output: default@mat1
+POSTHOOK: query: ALTER MATERIALIZED VIEW mat1 REBUILD
+POSTHOOK: type: ALTER_MATERIALIZED_VIEW_REBUILD
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@mat1
+POSTHOOK: Input: default@t3
+POSTHOOK: Output: default@mat1
+POSTHOOK: Output: default@mat1
+POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
+POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
+POSTHOOK: Lineage: mat1.a SIMPLE [(cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:a, type:int, comment:null), ]
+POSTHOOK: Lineage: mat1.a SIMPLE [(cmv_basetable_n6)cmv_basetable_n6.FieldSchema(name:a, type:int, comment:null), ]
+POSTHOOK: Lineage: mat1.c SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:c, type:decimal(10,2), comment:null), ]
+POSTHOOK: Lineage: mat1.c SIMPLE [(cmv_basetable_2_n3)cmv_basetable_2_n3.FieldSchema(name:c, type:decimal(10,2), comment:null), ]
+PREHOOK: query: select * from mat1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@mat1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from mat1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@mat1
+#### A masked pattern was here ####
+2	130.30	1
+1	30.30	6
+PREHOOK: query: drop materialized view mat1
+PREHOOK: type: DROP_MATERIALIZED_VIEW
+PREHOOK: Input: default@mat1
+PREHOOK: Output: default@mat1
+POSTHOOK: query: drop materialized view mat1
+POSTHOOK: type: DROP_MATERIALIZED_VIEW
+POSTHOOK: Input: default@mat1
+POSTHOOK: Output: default@mat1
+PREHOOK: query: SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, count(*)
+FROM cmv_basetable_n6
+JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+JOIN t3 ON (t3.a = cmv_basetable_2_n3.a)
+WHERE cmv_basetable_n6.c > 10 OR cmv_basetable_2_n3.c > 10
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c
+PREHOOK: type: QUERY
+PREHOOK: Input: default@cmv_basetable_2_n3
+PREHOOK: Input: default@cmv_basetable_n6
+PREHOOK: Input: default@t3
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT cmv_basetable_n6.a, cmv_basetable_2_n3.c, count(*)
+FROM cmv_basetable_n6
+JOIN cmv_basetable_2_n3 ON (cmv_basetable_n6.a = cmv_basetable_2_n3.a)
+JOIN t3 ON (t3.a = cmv_basetable_2_n3.a)
+WHERE cmv_basetable_n6.c > 10 OR cmv_basetable_2_n3.c > 10
+group by cmv_basetable_n6.a, cmv_basetable_2_n3.c
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@cmv_basetable_2_n3
+POSTHOOK: Input: default@cmv_basetable_n6
+POSTHOOK: Input: default@t3
+#### A masked pattern was here ####
+2	130.30	1
+1	30.30	6
diff --git a/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_9.q.out b/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_9.q.out
index 69e1a97dd6..69fc069a49 100644
--- a/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_9.q.out
+++ b/ql/src/test/results/clientpositive/llap/materialized_view_create_rewrite_9.q.out
@@ -251,12 +251,12 @@ HiveProject(a0=[$4], $f1=[CASE(IS NULL($1), $5, IS NULL($5), $1, +($5, $1))], $f
         HiveTableScan(table=[[default, mat1]], table:alias=[default.mat1])
       HiveProject(a=[$0], $f1=[$1], $f2=[$2])
         HiveAggregate(group=[{0}], agg#0=[SUM($1)], agg#1=[SUM($2)])
-          HiveProject(a=[$0], $f3=[CASE(OR($2, $5), *(-1, $1), $1)], $f4=[CASE(OR($2, $5), -1, 1)])
-            HiveJoin(condition=[AND(=($0, $4), OR($3, $6))], joinType=[inner], algorithm=[none], cost=[not available])
-              HiveProject(a=[$0], b=[$1], ROW__IS__DELETED=[$6], <=[<(3, $5.writeid)])
+          HiveProject(a=[$0], $f4=[CASE(OR($3, $7), *(-1, $1), $1)], $f5=[CASE(OR($3, $7), -1, 1)])
+            HiveJoin(condition=[AND(=($0, $5), OR(AND(NOT($3), NOT($7)), AND(NOT($4), NOT($8))), OR(<(3, $2.writeid), <(3, $6.writeid)))], joinType=[inner], algorithm=[none], cost=[not available])
+              HiveProject(a=[$0], b=[$1], ROW__ID=[$5], _deleted=[AND($6, <(3, $5.writeid))], _inserted=[AND(<(3, $5.writeid), NOT($6))])
                 HiveFilter(condition=[IS NOT NULL($0)])
                   HiveTableScan(table=[[default, t1]], table:alias=[t1])
-              HiveProject(a=[$0], ROW__IS__DELETED=[$5], <=[<(3, $4.writeid)])
+              HiveProject(a=[$0], ROW__ID=[$4], _deleted=[AND($5, <(3, $4.writeid))], _inserted=[AND(<(3, $4.writeid), NOT($5))])
                 HiveFilter(condition=[IS NOT NULL($0)])
                   HiveTableScan(table=[[default, t2]], table:alias=[t2])
 
@@ -333,16 +333,16 @@ STAGE PLANS:
                     predicate: a is not null (type: boolean)
                     Statistics: Num rows: 9 Data size: 837 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
-                      expressions: a (type: char(15)), ROW__IS__DELETED (type: boolean), (ROW__ID.writeid > 3L) (type: boolean)
-                      outputColumnNames: _col0, _col1, _col2
-                      Statistics: Num rows: 9 Data size: 909 Basic stats: COMPLETE Column stats: COMPLETE
+                      expressions: a (type: char(15)), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), (ROW__IS__DELETED and (ROW__ID.writeid > 3L)) (type: boolean), ((ROW__ID.writeid > 3L) and (not ROW__IS__DELETED)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2, _col3
+                      Statistics: Num rows: 9 Data size: 1593 Basic stats: COMPLETE Column stats: COMPLETE
                       Reduce Output Operator
                         key expressions: _col0 (type: char(15))
                         null sort order: z
                         sort order: +
                         Map-reduce partition columns: _col0 (type: char(15))
-                        Statistics: Num rows: 9 Data size: 909 Basic stats: COMPLETE Column stats: COMPLETE
-                        value expressions: _col1 (type: boolean), _col2 (type: boolean)
+                        Statistics: Num rows: 9 Data size: 1593 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col2 (type: boolean), _col3 (type: boolean)
             Execution mode: vectorized, llap
             LLAP IO: may be used (ACID table)
         Map 7 
@@ -357,16 +357,16 @@ STAGE PLANS:
                     predicate: a is not null (type: boolean)
                     Statistics: Num rows: 7 Data size: 671 Basic stats: COMPLETE Column stats: COMPLETE
                     Select Operator
-                      expressions: a (type: char(15)), b (type: int), ROW__IS__DELETED (type: boolean), (ROW__ID.writeid > 3L) (type: boolean)
-                      outputColumnNames: _col0, _col1, _col2, _col3
-                      Statistics: Num rows: 7 Data size: 727 Basic stats: COMPLETE Column stats: COMPLETE
+                      expressions: a (type: char(15)), b (type: int), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), (ROW__IS__DELETED and (ROW__ID.writeid > 3L)) (type: boolean), ((ROW__ID.writeid > 3L) and (not ROW__IS__DELETED)) (type: boolean)
+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                      Statistics: Num rows: 7 Data size: 1259 Basic stats: COMPLETE Column stats: COMPLETE
                       Reduce Output Operator
                         key expressions: _col0 (type: char(15))
                         null sort order: z
                         sort order: +
                         Map-reduce partition columns: _col0 (type: char(15))
-                        Statistics: Num rows: 7 Data size: 727 Basic stats: COMPLETE Column stats: COMPLETE
-                        value expressions: _col1 (type: int), _col2 (type: boolean), _col3 (type: boolean)
+                        Statistics: Num rows: 7 Data size: 1259 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: int), _col2 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col3 (type: boolean), _col4 (type: boolean)
             Execution mode: vectorized, llap
             LLAP IO: may be used (ACID table)
         Reducer 2 
@@ -380,7 +380,7 @@ STAGE PLANS:
                   1 _col0 (type: char(15))
                 nullSafes: [true]
                 outputColumnNames: _col1, _col2, _col3, _col4, _col5, _col6, _col7
-                Statistics: Num rows: 5 Data size: 1017 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 2 Data size: 402 Basic stats: COMPLETE Column stats: COMPLETE
                 Filter Operator
                   predicate: (_col3 and ((_col2 is null and (_col7 > 0L)) or (((_col7 + _col2) > 0) and _col2 is not null))) (type: boolean)
                   Statistics: Num rows: 1 Data size: 205 Basic stats: COMPLETE Column stats: COMPLETE
@@ -548,26 +548,26 @@ STAGE PLANS:
                 keys:
                   0 _col0 (type: char(15))
                   1 _col0 (type: char(15))
-                outputColumnNames: _col0, _col1, _col2, _col3, _col5, _col6
-                residual filter predicates: {(_col3 or _col6)}
-                Statistics: Num rows: 6 Data size: 670 Basic stats: COMPLETE Column stats: COMPLETE
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col6, _col7, _col8
+                residual filter predicates: {(((not _col3) and (not _col7)) or ((not _col4) and (not _col8)))} {((_col2.writeid > 3L) or (_col6.writeid > 3L))}
+                Statistics: Num rows: 1 Data size: 265 Basic stats: COMPLETE Column stats: COMPLETE
                 Select Operator
-                  expressions: _col0 (type: char(15)), if((_col2 or _col5), (-1 * _col1), _col1) (type: int), if((_col2 or _col5), -1, 1) (type: int)
+                  expressions: _col0 (type: char(15)), if((_col3 or _col7), (-1 * _col1), _col1) (type: int), if((_col3 or _col7), -1, 1) (type: int)
                   outputColumnNames: _col0, _col1, _col2
-                  Statistics: Num rows: 6 Data size: 670 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 1 Data size: 265 Basic stats: COMPLETE Column stats: COMPLETE
                   Group By Operator
                     aggregations: sum(_col1), sum(_col2)
                     keys: _col0 (type: char(15))
                     minReductionHashAggr: 0.4
                     mode: hash
                     outputColumnNames: _col0, _col1, _col2
-                    Statistics: Num rows: 5 Data size: 545 Basic stats: COMPLETE Column stats: COMPLETE
+                    Statistics: Num rows: 1 Data size: 109 Basic stats: COMPLETE Column stats: COMPLETE
                     Reduce Output Operator
                       key expressions: _col0 (type: char(15))
                       null sort order: z
                       sort order: +
                       Map-reduce partition columns: _col0 (type: char(15))
-                      Statistics: Num rows: 5 Data size: 545 Basic stats: COMPLETE Column stats: COMPLETE
+                      Statistics: Num rows: 1 Data size: 109 Basic stats: COMPLETE Column stats: COMPLETE
                       value expressions: _col1 (type: bigint), _col2 (type: bigint)
         Reducer 9 
             Execution mode: vectorized, llap
@@ -577,13 +577,13 @@ STAGE PLANS:
                 keys: KEY._col0 (type: char(15))
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col2
-                Statistics: Num rows: 5 Data size: 545 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 1 Data size: 109 Basic stats: COMPLETE Column stats: COMPLETE
                 Reduce Output Operator
                   key expressions: _col0 (type: char(15))
                   null sort order: z
                   sort order: +
                   Map-reduce partition columns: _col0 (type: char(15))
-                  Statistics: Num rows: 5 Data size: 545 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 1 Data size: 109 Basic stats: COMPLETE Column stats: COMPLETE
                   value expressions: _col1 (type: bigint), _col2 (type: bigint)
 
   Stage: Stage-5
@@ -672,10 +672,10 @@ POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 POSTHOOK: Output: default@mat1
 POSTHOOK: Output: default@mat1
-POSTHOOK: Lineage: mat1._c1 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:b, type:int, comment:null), ]
-POSTHOOK: Lineage: mat1._c1 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:b, type:int, comment:null), ]
-POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), ]
-POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), ]
+POSTHOOK: Lineage: mat1._c1 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t1)t1.FieldSchema(name:b, type:int, comment:null), ]
+POSTHOOK: Lineage: mat1._c1 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c1, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t1)t1.FieldSchema(name:b, type:int, comment:null), ]
+POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
+POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (t1)t1.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t1)t1.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), (t2)t2.FieldSchema(name:ROW__IS__DELETED, type:boolean, comment:), (t2)t2.FieldSchema(name:ROW__ID, type:struct<writeId:bigint,bucketId:int,rowId:bigint>, comment:), ]
 POSTHOOK: Lineage: mat1.a SIMPLE [(t1)t1.FieldSchema(name:a, type:char(15), comment:null), ]
 POSTHOOK: Lineage: mat1.a SIMPLE [(t1)t1.FieldSchema(name:a, type:char(15), comment:null), ]
 PREHOOK: query: explain cbo
