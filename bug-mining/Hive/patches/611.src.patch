diff --git a/hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionItem.java b/hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionItem.java
index 7bfa80a760..1bf8c978ea 100644
--- a/hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionItem.java
+++ b/hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionItem.java
@@ -333,7 +333,7 @@ public void runQuery() {
           Driver qp = (Driver) proc;
           qp.setTryCount(Integer.MAX_VALUE);
           try {
-            queryRet.add(new Integer(qp.run(cmd).getResponseCode()));
+          queryRet.add(Integer.valueOf(qp.run(cmd).getResponseCode()));
           ArrayList<String> res = new ArrayList<String>();
           try {
             while (qp.getResults(res)) {
@@ -367,7 +367,7 @@ public void runQuery() {
           }
         } else {
           try {
-            queryRet.add(new Integer(proc.run(cmd_1).getResponseCode()));
+            queryRet.add(Integer.valueOf(proc.run(cmd_1).getResponseCode()));
           } catch (CommandNeedRetryException e) {
             // this should never happen if there is no bug
             l4j.error(getSessionName() + " Exception when executing", e);
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
index 0e4712015e..0a2219619b 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
@@ -667,7 +667,7 @@ private void drop_database_core(RawStore ms,
         Path path = new Path(db.getLocationUri()).getParent();
         if (!wh.isWritable(path)) {
           throw new MetaException("Database not dropped since " +
-              path + " is not writable by " + 
+              path + " is not writable by " +
               hiveConf.getUser());
         }
         if (ms.dropDatabase(name)) {
@@ -1040,7 +1040,7 @@ private void drop_table_core(final RawStore ms, final String dbname,
           tblPath = new Path(tbl.getSd().getLocation());
           if (!wh.isWritable(tblPath.getParent())) {
             throw new MetaException("Table metadata not deleted since " +
-                tblPath.getParent() + " is not writable by " + 
+                tblPath.getParent() + " is not writable by " +
                 hiveConf.getUser());
           }
         }
@@ -1147,7 +1147,6 @@ private Partition append_partition_common(RawStore ms, String dbName, String tab
       Path partLocation = null;
       try {
         ms.openTransaction();
-        part = new Partition();
         part.setDbName(dbName);
         part.setTableName(tableName);
         part.setValues(part_vals);
@@ -1428,7 +1427,7 @@ private boolean drop_partition_common(RawStore ms, String db_name, String tbl_na
           archiveParentDir = MetaStoreUtils.getOriginalLocation(part);
           if (!wh.isWritable(archiveParentDir.getParent())) {
             throw new MetaException("Table partition not deleted since " +
-                archiveParentDir.getParent() + " is not writable by " + 
+                archiveParentDir.getParent() + " is not writable by " +
                 hiveConf.getUser());
           }
         }
@@ -1440,7 +1439,7 @@ private boolean drop_partition_common(RawStore ms, String db_name, String tbl_na
           partPath = new Path(part.getSd().getLocation());
           if (!wh.isWritable(partPath.getParent())) {
             throw new MetaException("Table partition not deleted since " +
-                partPath.getParent() + " is not writable by " + 
+                partPath.getParent() + " is not writable by " +
                 hiveConf.getUser());
           }
         }
@@ -2314,7 +2313,7 @@ private boolean drop_index_by_name_core(final RawStore ms,
             tblPath = new Path(tbl.getSd().getLocation());
             if (!wh.isWritable(tblPath.getParent())) {
               throw new MetaException("Index table metadata not deleted since " +
-                  tblPath.getParent() + " is not writable by " + 
+                  tblPath.getParent() + " is not writable by " +
                   hiveConf.getUser());
             }
           }
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index 5d793d0823..cbe8c8c7fd 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -1225,7 +1225,6 @@ public List<String> listPartitionNames(String dbName, String tableName,
       q.declareParameters("java.lang.String t1, java.lang.String t2");
       q.setResult("partitionName");
       Collection names = (Collection) q.execute(dbName, tableName);
-      pns = new ArrayList<String>();
       for (Iterator i = names.iterator(); i.hasNext();) {
         pns.add((String) i.next());
       }
@@ -1771,7 +1770,6 @@ public List<String> listIndexNames(String dbName, String origTableName,
       q.declareParameters("java.lang.String t1, java.lang.String t2");
       q.setResult("indexName");
       Collection names = (Collection) q.execute(dbName, origTableName);
-      pns = new ArrayList<String>();
       for (Iterator i = names.iterator(); i.hasNext();) {
         pns.add((String) i.next());
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java
index 7ea1e36ee8..077006d4d8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java
@@ -650,7 +650,7 @@ private static void pruneJoinOperator(NodeProcessorCtx ctx,
       List<TableDesc> valueTableDescs = new ArrayList<TableDesc>();
       for (int pos = 0; pos < op.getParentOperators().size(); pos++) {
         List<ExprNodeDesc> valueCols = conf.getExprs()
-            .get(new Byte((byte) pos));
+            .get(Byte.valueOf((byte) pos));
         StringBuilder keyOrder = new StringBuilder();
         for (int i = 0; i < valueCols.size(); i++) {
           keyOrder.append("+");
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java
index 1a10c965d9..b6d4526762 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java
@@ -137,7 +137,6 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
       ctx.setUnionTask(union, uCtxTask);
     } else {
       uTask = uCtxTask.getUTask();
-      uPlan = (MapredWork) uTask.getWork();
     }
 
     // If there is a mapjoin at position 'pos'
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
index 8e79250439..98bb2a5e27 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
@@ -24,7 +24,6 @@
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -45,7 +44,6 @@
 import org.apache.hadoop.hive.ql.exec.RowSchema;
 import org.apache.hadoop.hive.ql.exec.ScriptOperator;
 import org.apache.hadoop.hive.ql.exec.SelectOperator;
-import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.UnionOperator;
 import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
 import org.apache.hadoop.hive.ql.lib.Dispatcher;
@@ -353,7 +351,7 @@ public static MapJoinOperator convertMapJoin(
         }
       }
 
-      valueExprMap.put(new Byte((byte) pos), values);
+      valueExprMap.put(Byte.valueOf((byte) pos), values);
     }
 
     Map<Byte, List<ExprNodeDesc>> filterMap = desc.getFilters();
@@ -392,7 +390,7 @@ public static MapJoinOperator convertMapJoin(
       newPar[pos++] = o;
     }
 
-    List<ExprNodeDesc> keyCols = keyExprMap.get(new Byte((byte) 0));
+    List<ExprNodeDesc> keyCols = keyExprMap.get(Byte.valueOf((byte) 0));
     StringBuilder keyOrder = new StringBuilder();
     for (int i = 0; i < keyCols.size(); i++) {
       keyOrder.append("+");
@@ -405,14 +403,14 @@ public static MapJoinOperator convertMapJoin(
     List<TableDesc> valueFiltedTableDescs = new ArrayList<TableDesc>();
 
     for (pos = 0; pos < newParentOps.size(); pos++) {
-      List<ExprNodeDesc> valueCols = valueExprMap.get(new Byte((byte) pos));
+      List<ExprNodeDesc> valueCols = valueExprMap.get(Byte.valueOf((byte) pos));
       int length = valueCols.size();
       List<ExprNodeDesc> valueFilteredCols = new ArrayList<ExprNodeDesc>(length);
       // deep copy expr node desc
       for (int i = 0; i < length; i++) {
         valueFilteredCols.add(valueCols.get(i).clone());
       }
-      List<ExprNodeDesc> valueFilters = filterMap.get(new Byte((byte) pos));
+      List<ExprNodeDesc> valueFilters = filterMap.get(Byte.valueOf((byte) pos));
 
       if (valueFilters != null && valueFilters.size() != 0 && pos != mapJoinPos) {
         ExprNodeColumnDesc isFilterDesc = new ExprNodeColumnDesc(TypeInfoFactory
@@ -483,7 +481,7 @@ public MapJoinOperator generateMapJoinOperator(ParseContext pctx, JoinOperator o
   /**
    * Get a list of big table candidates. Only the tables in the returned set can
    * be used as big table in the join operation.
-   * 
+   *
    * The logic here is to scan the join condition array from left to right. If
    * see a inner join, and the bigTableCandidates is empty or the outer join
    * that we last saw is a right outer join, add both side of this inner join to
@@ -496,8 +494,8 @@ public MapJoinOperator generateMapJoinOperator(ParseContext pctx, JoinOperator o
    * the right side of a right outer join always win. If see a full outer join,
    * return null immediately (no one can be the big table, can not do a
    * mapjoin).
-   * 
-   * 
+   *
+   *
    * @param condns
    * @return
    */
@@ -507,7 +505,7 @@ public static HashSet<Integer> getBigTableCandidates(JoinCondDesc[] condns) {
     boolean seenOuterJoin = false;
     Set<Integer> seenPostitions = new HashSet<Integer>();
     Set<Integer> leftPosListOfLastRightOuterJoin = new HashSet<Integer>();
-    
+
     // is the outer join that we saw most recently is a right outer join?
     boolean lastSeenRightOuterJoin = false;
     for (JoinCondDesc condn : condns) {
@@ -527,7 +525,7 @@ public static HashSet<Integer> getBigTableCandidates(JoinCondDesc[] condns) {
         if(bigTableCandidates.size() == 0) {
           bigTableCandidates.add(condn.getLeft());
         }
-        
+
         lastSeenRightOuterJoin = false;
       } else if (joinType == JoinDesc.RIGHT_OUTER_JOIN) {
         seenOuterJoin = true;
@@ -539,7 +537,7 @@ public static HashSet<Integer> getBigTableCandidates(JoinCondDesc[] condns) {
 
         bigTableCandidates.clear();
         bigTableCandidates.add(condn.getRight());
-      } else if (joinType == JoinDesc.INNER_JOIN) {        
+      } else if (joinType == JoinDesc.INNER_JOIN) {
         if (!seenOuterJoin || lastSeenRightOuterJoin) {
           // is the left was at the left side of a right outer join?
           if (!leftPosListOfLastRightOuterJoin.contains(condn.getLeft())) {
@@ -698,10 +696,10 @@ public ParseContext transform(ParseContext pactx) throws SemanticException {
     // the operator stack.
     // The dispatcher generates the plan from the operator tree
     Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();
-    opRules.put(new RuleRegExp(new String("R0"), "MAPJOIN%"), getCurrentMapJoin());
-    opRules.put(new RuleRegExp(new String("R1"), "MAPJOIN%.*FS%"), getMapJoinFS());
-    opRules.put(new RuleRegExp(new String("R2"), "MAPJOIN%.*RS%"), getMapJoinDefault());
-    opRules.put(new RuleRegExp(new String("R4"), "MAPJOIN%.*UNION%"), getMapJoinDefault());
+    opRules.put(new RuleRegExp("R0", "MAPJOIN%"), getCurrentMapJoin());
+    opRules.put(new RuleRegExp("R1", "MAPJOIN%.*FS%"), getMapJoinFS());
+    opRules.put(new RuleRegExp("R2", "MAPJOIN%.*RS%"), getMapJoinDefault());
+    opRules.put(new RuleRegExp("R4", "MAPJOIN%.*UNION%"), getMapJoinDefault());
 
     // The dispatcher fires the processor corresponding to the closest matching
     // rule and passes the context along
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcessor.java
index ec3fe5f625..f6b3853d47 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcessor.java
@@ -41,7 +41,7 @@
  * the sub-query involves a map-reduce job, a FS is introduced on top of the
  * UNION. This can be later optimized to clone all the operators above the
  * UNION.
- * 
+ *
  * The parse Context is not changed.
  */
 public class UnionProcessor implements Transform {
@@ -55,7 +55,7 @@ public UnionProcessor() {
   /**
    * Transform the query tree. For each union, store the fact whether both the
    * sub-queries are map-only
-   * 
+   *
    * @param pCtx
    *          the current parse context
    */
@@ -63,13 +63,13 @@ public ParseContext transform(ParseContext pCtx) throws SemanticException {
     // create a walker which walks the tree in a DFS manner while maintaining
     // the operator stack.
     Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();
-    opRules.put(new RuleRegExp(new String("R1"), "RS%.*UNION%"),
+    opRules.put(new RuleRegExp("R1", "RS%.*UNION%"),
         UnionProcFactory.getMapRedUnion());
-    opRules.put(new RuleRegExp(new String("R2"), "UNION%.*UNION%"),
+    opRules.put(new RuleRegExp("R2", "UNION%.*UNION%"),
         UnionProcFactory.getUnknownUnion());
-    opRules.put(new RuleRegExp(new String("R3"), "TS%.*UNION%"),
+    opRules.put(new RuleRegExp("R3", "TS%.*UNION%"),
         UnionProcFactory.getMapUnion());
-    opRules.put(new RuleRegExp(new String("R3"), "MAPJOIN%.*UNION%"),
+    opRules.put(new RuleRegExp("R3", "MAPJOIN%.*UNION%"),
         UnionProcFactory.getMapJoinUnion());
 
     // The dispatcher fires the processor for the matching rule and passes the
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/FetchWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/FetchWork.java
index d07336ecc8..481a51c199 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/FetchWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/FetchWork.java
@@ -208,7 +208,7 @@ public String toString() {
       return "null fetchwork";
     }
 
-    String ret = new String("partition = ");
+    String ret = "partition = ";
     for (String part : partDir) {
       ret = ret.concat(part);
     }
