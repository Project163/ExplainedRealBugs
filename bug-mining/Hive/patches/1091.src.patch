diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java
index c8d6ca816a..1e31d74b63 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapJoinOperator.java
@@ -54,7 +54,7 @@ public abstract class AbstractMapJoinOperator <T extends MapJoinDesc> extends Co
    */
   protected transient Map<Byte, List<ObjectInspector>> joinKeysStandardObjectInspectors;
 
-  protected transient int posBigTable = -1; // one of the tables that is not in memory
+  protected transient byte posBigTable = -1; // one of the tables that is not in memory
   transient int mapJoinRowsKey; // rows for a given key
 
   protected transient RowContainer<ArrayList<Object>> emptyList = null;
@@ -93,15 +93,15 @@ protected void initializeOp(Configuration hconf) throws HiveException {
         joinKeysObjectInspectors,NOTSKIPBIGTABLE);
 
     // all other tables are small, and are cached in the hash table
-    posBigTable = conf.getPosBigTable();
+    posBigTable = (byte) conf.getPosBigTable();
 
     emptyList = new RowContainer<ArrayList<Object>>(1, hconf, reporter);
 
     RowContainer bigPosRC = JoinUtil.getRowContainer(hconf,
-        rowContainerStandardObjectInspectors.get((byte) posBigTable),
-        order[posBigTable], joinCacheSize,spillTableDesc, conf,
+        rowContainerStandardObjectInspectors.get(posBigTable),
+        posBigTable, joinCacheSize,spillTableDesc, conf,
         !hasFilter(posBigTable), reporter);
-    storage.put((byte) posBigTable, bigPosRC);
+    storage.put(posBigTable, bigPosRC);
 
     mapJoinRowsKey = HiveConf.getIntVar(hconf,
         HiveConf.ConfVars.HIVEMAPJOINROWSIZE);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
index f3052d858b..afe978dafd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
@@ -246,13 +246,12 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     nullsafes = conf.getNullSafes();
     noOuterJoin = conf.isNoOuterJoin();
 
-    Byte[] reorder = getExecContext() == null ? order : null;
     totalSz = JoinUtil.populateJoinKeyValue(joinValues, conf.getExprs(),
-        reorder,NOTSKIPBIGTABLE);
+        order,NOTSKIPBIGTABLE);
 
     //process join filters
     joinFilters = new HashMap<Byte, List<ExprNodeEvaluator>>();
-    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(),reorder,NOTSKIPBIGTABLE);
+    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(),order,NOTSKIPBIGTABLE);
 
 
     joinValuesObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinValues,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ConditionalTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ConditionalTask.java
index 8e738b9d32..1451ec091c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ConditionalTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ConditionalTask.java
@@ -84,8 +84,7 @@ public int execute(DriverContext driverContext) {
     for (Task<? extends Serializable> tsk : getListTasks()) {
       if (!resTasks.contains(tsk)) {
         driverContext.getRunnable().remove(tsk);
-        console.printInfo(HadoopJobExecHelper.getJobEndMsg("" + Utilities.randGen.nextInt())
-            + ", job is filtered out (removed at runtime).");
+        console.printInfo(tsk.getId() + " is filtered out by condition resolver.");
         if (tsk.isMapRedTask()) {
           driverContext.incCurJobNo(1);
         }
@@ -94,6 +93,7 @@ public int execute(DriverContext driverContext) {
       } else {
         // resolved task
         if (!driverContext.getRunnable().contains(tsk)) {
+          console.printInfo(tsk.getId() + " is selected by condition resolver.");
           driverContext.addToRunnable(tsk);
         }
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
index 1daec97032..b08c984708 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
@@ -29,7 +29,6 @@
 import org.apache.hadoop.filecache.DistributedCache;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.HashTableSinkObjectCtx;
 import org.apache.hadoop.hive.ql.exec.persistence.AbstractMapJoinKey;
 import org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper;
@@ -65,7 +64,6 @@ public class MapJoinOperator extends AbstractMapJoinOperator<MapJoinDesc> implem
   transient int metadataKeyTag;
   transient int[] metadataValueTag;
   transient boolean hashTblInitedOnce;
-  private int bigTableAlias;
 
   public MapJoinOperator() {
   }
@@ -85,7 +83,6 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     }
 
     metadataKeyTag = -1;
-    bigTableAlias = order[posBigTable];
 
     mapJoinTables = new HashMap<Byte, HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue>>();
     rowContainerMap = new HashMap<Byte, MapJoinRowContainer<ArrayList<Object>>>();
@@ -122,26 +119,21 @@ public void generateMapMetaData() throws HiveException, SerDeException {
         ObjectInspectorUtils.getStandardObjectInspector(keySerializer.getObjectInspector(),
             ObjectInspectorCopyOption.WRITABLE), keySerializer, keyTableDesc, hconf));
 
-    // index for values is just alias
-    for (int tag = 0; tag < order.length; tag++) {
-      int alias = (int) order[tag];
-
-      if (alias == this.bigTableAlias) {
+    for (int pos = 0; pos < order.length; pos++) {
+      if (pos == posBigTable) {
         continue;
       }
-
-
       TableDesc valueTableDesc;
       if (conf.getNoOuterJoin()) {
-        valueTableDesc = conf.getValueTblDescs().get(tag);
+        valueTableDesc = conf.getValueTblDescs().get(pos);
       } else {
-        valueTableDesc = conf.getValueFilteredTblDescs().get(tag);
+        valueTableDesc = conf.getValueFilteredTblDescs().get(pos);
       }
       SerDe valueSerDe = (SerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(),
           null);
       valueSerDe.initialize(null, valueTableDesc.getProperties());
 
-      MapJoinMetaData.put(Integer.valueOf(alias), new HashTableSinkObjectCtx(ObjectInspectorUtils
+      MapJoinMetaData.put(Integer.valueOf(pos), new HashTableSinkObjectCtx(ObjectInspectorUtils
           .getStandardObjectInspector(valueSerDe.getObjectInspector(),
               ObjectInspectorCopyOption.WRITABLE), valueSerDe, valueTableDesc, hconf));
     }
@@ -242,8 +234,8 @@ public void processOp(Object row, int tag) throws HiveException {
       // Add the value to the ArrayList
       storage.get(alias).add(value);
 
-      for (Byte pos : order) {
-        if (pos.intValue() != alias) {
+      for (byte pos = 0; pos < order.length; pos++) {
+        if (pos != alias) {
 
           MapJoinObjectValue o = mapJoinTables.get(pos).get(key);
           MapJoinRowContainer<ArrayList<Object>> rowContainer = rowContainerMap.get(pos);
@@ -253,7 +245,7 @@ public void processOp(Object row, int tag) throws HiveException {
             if (noOuterJoin) {
               storage.put(pos, emptyList);
             } else {
-              storage.put(pos, dummyObjVectors[pos.intValue()]);
+              storage.put(pos, dummyObjVectors[pos]);
             }
           } else {
             rowContainer.reset(o.getObj());
@@ -268,8 +260,8 @@ public void processOp(Object row, int tag) throws HiveException {
       // done with the row
       storage.get((byte) tag).clear();
 
-      for (Byte pos : order) {
-        if (pos.intValue() != tag) {
+      for (byte pos = 0; pos < order.length; pos++) {
+        if (pos != tag) {
           storage.put(pos, null);
         }
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java
index 1b75ba1b4a..eb5f7a56f4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java
@@ -96,9 +96,9 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
     // get the largest table alias from order
     int maxAlias = 0;
-    for (Byte alias: order) {
-      if (alias > maxAlias) {
-        maxAlias = alias;
+    for (byte pos = 0; pos < order.length; pos++) {
+      if (pos > maxAlias) {
+        maxAlias = pos;
       }
     }
     maxAlias += 1;
@@ -112,27 +112,25 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
     int bucketSize = HiveConf.getIntVar(hconf,
         HiveConf.ConfVars.HIVEMAPJOINBUCKETCACHESIZE);
-    byte storePos = (byte) 0;
-    for (Byte alias : order) {
+    for (byte pos = 0; pos < order.length; pos++) {
       RowContainer rc = JoinUtil.getRowContainer(hconf,
-          rowContainerStandardObjectInspectors.get(storePos),
-          alias, bucketSize,spillTableDesc, conf, !hasFilter(storePos),
+          rowContainerStandardObjectInspectors.get(pos),
+          pos, bucketSize,spillTableDesc, conf, !hasFilter(pos),
           reporter);
-      nextGroupStorage[storePos] = rc;
+      nextGroupStorage[pos] = rc;
       RowContainer candidateRC = JoinUtil.getRowContainer(hconf,
-          rowContainerStandardObjectInspectors.get((byte)storePos),
-          alias,bucketSize,spillTableDesc, conf, !hasFilter(storePos),
+          rowContainerStandardObjectInspectors.get(pos),
+          pos,bucketSize,spillTableDesc, conf, !hasFilter(pos),
           reporter);
-      candidateStorage[alias] = candidateRC;
-      storePos++;
+      candidateStorage[pos] = candidateRC;
     }
     tagToAlias = conf.getTagToAlias();
 
-    for (Byte alias : order) {
-      if(alias != (byte) posBigTable) {
-        fetchDone[alias] = false;
+    for (byte pos = 0; pos < order.length; pos++) {
+      if (pos != posBigTable) {
+        fetchDone[pos] = false;
       }
-      foundNextKeyGroup[alias] = false;
+      foundNextKeyGroup[pos] = false;
     }
   }
 
@@ -233,9 +231,9 @@ public void processOp(Object row, int tag) throws HiveException {
     if (!firstFetchHappened) {
       firstFetchHappened = true;
       // fetch the first group for all small table aliases
-      for (Byte t : order) {
-        if(t != (byte)posBigTable) {
-          fetchNextGroup(t);
+      for (byte pos = 0; pos < order.length; pos++) {
+        if (pos != posBigTable) {
+          fetchNextGroup(pos);
         }
       }
     }
@@ -268,13 +266,13 @@ public void processOp(Object row, int tag) throws HiveException {
     // the big table has reached a new key group. try to let the small tables
     // catch up with the big table.
     if (nextKeyGroup) {
-      assert tag == (byte)posBigTable;
+      assert tag == posBigTable;
       List<Byte> smallestPos = null;
       do {
         smallestPos = joinOneGroup();
         //jump out the loop if we need input from the big table
       } while (smallestPos != null && smallestPos.size() > 0
-          && !smallestPos.contains((byte)this.posBigTable));
+          && !smallestPos.contains(this.posBigTable));
 
       return;
     }
@@ -313,16 +311,16 @@ private void joinFinalLeftData() throws HiveException {
 
     boolean dataInCache = true;
     while (dataInCache) {
-      for (byte t : order) {
-        if (this.foundNextKeyGroup[t]
-            && this.nextKeyWritables[t] != null) {
-          promoteNextGroupToCandidate(t);
+      for (byte pos = 0; pos < order.length; pos++) {
+        if (this.foundNextKeyGroup[pos]
+            && this.nextKeyWritables[pos] != null) {
+          promoteNextGroupToCandidate(pos);
         }
       }
       joinOneGroup();
       dataInCache = false;
-      for (byte r : order) {
-        if (this.candidateStorage[r].size() > 0) {
+      for (byte pos = 0; pos < order.length; pos++) {
+        if (this.candidateStorage[pos].size() > 0) {
           dataInCache = true;
           break;
         }
@@ -332,11 +330,11 @@ private void joinFinalLeftData() throws HiveException {
 
   private boolean allFetchDone() {
     boolean allFetchDone = true;
-    for (Byte tag : order) {
-      if(tag == (byte) posBigTable) {
+    for (byte pos = 0; pos < order.length; pos++) {
+      if (pos == posBigTable) {
         continue;
       }
-      allFetchDone = allFetchDone && fetchDone[tag];
+      allFetchDone = allFetchDone && fetchDone[pos];
     }
     return allFetchDone;
   }
@@ -398,7 +396,7 @@ private void fetchNextGroup(Byte t) throws HiveException {
       foundNextKeyGroup[t] = false;
     }
     //for the big table, we only need to promote the next group to the current group.
-    if(t == (byte)posBigTable) {
+    if(t == posBigTable) {
       return;
     }
 
@@ -463,18 +461,18 @@ private int[] findSmallestKey() {
     int[] result = new int[order.length];
     ArrayList<Object> smallestOne = null;
 
-    for (byte i : order) {
-      ArrayList<Object> key = keyWritables[i];
+    for (byte pos = 0; pos < order.length; pos++) {
+      ArrayList<Object> key = keyWritables[pos];
       if (key == null) {
         continue;
       }
       if (smallestOne == null) {
         smallestOne = key;
-        result[i] = -1;
+        result[pos] = -1;
         continue;
       }
-      result[i] = compareKeys(key, smallestOne);
-      if (result[i] < 0) {
+      result[pos] = compareKeys(key, smallestOne);
+      if (result[pos] < 0) {
         smallestOne = key;
       }
     }
@@ -568,9 +566,9 @@ public void closeOp(boolean abort) throws HiveException {
         setUpFetchContexts(alias, mergeQueue);
       }
       firstFetchHappened = true;
-      for (Byte t : order) {
-        if(t != (byte)posBigTable) {
-          fetchNextGroup(t);
+      for (byte pos = 0; pos < order.length; pos++) {
+        if (pos != posBigTable) {
+          fetchNextGroup(pos);
         }
       }
       inputFileChanged = false;
@@ -579,11 +577,11 @@ public void closeOp(boolean abort) throws HiveException {
     joinFinalLeftData();
 
     //clean up
-    for (Byte alias : order) {
-      if(alias != (byte) posBigTable) {
-        fetchDone[alias] = false;
+    for (int pos = 0; pos < order.length; pos++) {
+      if (pos != posBigTable) {
+        fetchDone[pos] = false;
       }
-      foundNextKeyGroup[alias] = false;
+      foundNextKeyGroup[pos] = false;
     }
 
     localWorkInited = false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
index d73ce95a6e..85a2bdf664 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
@@ -115,6 +115,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object..
         e.printStackTrace();
       }
 
+      // mapjoin should not affected by join reordering
+      mapJoinOp.getConf().resetOrder();
+
       HashTableSinkDesc hashTableSinkDesc = new HashTableSinkDesc(mapJoinOp.getConf());
       HashTableSinkOperator hashTableSinkOp = (HashTableSinkOperator) OperatorFactory
           .get(hashTableSinkDesc);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/JoinDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/JoinDesc.java
index 4ac63634db..3b1aa85c7f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/JoinDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/JoinDesc.java
@@ -92,6 +92,11 @@ public JoinDesc(final Map<Byte, List<ExprNodeDesc>> exprs,
     this.conds = conds;
     this.filters = filters;
 
+    resetOrder();
+  }
+
+  // called by late-MapJoin processor (hive.auto.convert.join=true for example)
+  public void resetOrder() {
     tagOrder = new Byte[exprs.size()];
     for (int i = 0; i < tagOrder.length; i++) {
       tagOrder[i] = (byte) i;
diff --git a/ql/src/test/queries/clientpositive/join_reorder4.q b/ql/src/test/queries/clientpositive/join_reorder4.q
index 3dabfdeb5d..126f356ef7 100644
--- a/ql/src/test/queries/clientpositive/join_reorder4.q
+++ b/ql/src/test/queries/clientpositive/join_reorder4.q
@@ -8,11 +8,11 @@ LOAD DATA LOCAL INPATH '../data/files/T3.txt' INTO TABLE T3;
 
 set hive.auto.convert.join=true;
 
-explain select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
-select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+explain select /*+ STREAMTABLE(a) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+select /*+ STREAMTABLE(a) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
 
-explain select /*+ STREAMTABLE(b) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
-select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+explain select /*+ STREAMTABLE(b) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+select /*+ STREAMTABLE(b) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
 
-explain select /*+ STREAMTABLE(c) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
-select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+explain select /*+ STREAMTABLE(c) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
+select /*+ STREAMTABLE(c) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
diff --git a/ql/src/test/results/clientpositive/join_reorder4.q.out b/ql/src/test/results/clientpositive/join_reorder4.q.out
index b9697a1796..bf1ae129ca 100644
--- a/ql/src/test/results/clientpositive/join_reorder4.q.out
+++ b/ql/src/test/results/clientpositive/join_reorder4.q.out
@@ -31,12 +31,12 @@ PREHOOK: Output: default@t3
 POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/T3.txt' INTO TABLE T3
 POSTHOOK: type: LOAD
 POSTHOOK: Output: default@t3
-PREHOOK: query: explain select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: query: explain select /*+ STREAMTABLE(a) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 PREHOOK: type: QUERY
-POSTHOOK: query: explain select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: query: explain select /*+ STREAMTABLE(a) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 POSTHOOK: type: QUERY
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME T1) a) (TOK_TABREF (TOK_TABNAME T2) b) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL b) key2))) (TOK_TABREF (TOK_TABNAME T3) c) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL c) key3)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_STREAMTABLE (TOK_HINTARGLIST a))) (TOK_SELEXPR TOK_ALLCOLREF))))
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME T1) a) (TOK_TABREF (TOK_TABNAME T2) b) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL b) key2))) (TOK_TABREF (TOK_TABNAME T3) c) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL c) key3)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_STREAMTABLE (TOK_HINTARGLIST a))) (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME a))) (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME b))) (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME c))))))
 
 STAGE DEPENDENCIES:
   Stage-7 is a root stage , consists of Stage-8, Stage-9, Stage-10, Stage-1
@@ -397,25 +397,25 @@ STAGE PLANS:
       limit: -1
 
 
-PREHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: query: select /*+ STREAMTABLE(a) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 #### A masked pattern was here ####
-POSTHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: query: select /*+ STREAMTABLE(a) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 #### A masked pattern was here ####
 2	12	2	22	2	12
-PREHOOK: query: explain select /*+ STREAMTABLE(b) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: query: explain select /*+ STREAMTABLE(b) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 PREHOOK: type: QUERY
-POSTHOOK: query: explain select /*+ STREAMTABLE(b) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: query: explain select /*+ STREAMTABLE(b) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 POSTHOOK: type: QUERY
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME T1) a) (TOK_TABREF (TOK_TABNAME T2) b) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL b) key2))) (TOK_TABREF (TOK_TABNAME T3) c) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL c) key3)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_STREAMTABLE (TOK_HINTARGLIST b))) (TOK_SELEXPR TOK_ALLCOLREF))))
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME T1) a) (TOK_TABREF (TOK_TABNAME T2) b) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL b) key2))) (TOK_TABREF (TOK_TABNAME T3) c) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL c) key3)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_STREAMTABLE (TOK_HINTARGLIST b))) (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME a))) (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME b))) (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME c))))))
 
 STAGE DEPENDENCIES:
   Stage-7 is a root stage , consists of Stage-8, Stage-9, Stage-10, Stage-1
@@ -776,25 +776,25 @@ STAGE PLANS:
       limit: -1
 
 
-PREHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: query: select /*+ STREAMTABLE(b) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 #### A masked pattern was here ####
-POSTHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: query: select /*+ STREAMTABLE(b) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 POSTHOOK: Input: default@t3
 #### A masked pattern was here ####
 2	12	2	22	2	12
-PREHOOK: query: explain select /*+ STREAMTABLE(c) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: query: explain select /*+ STREAMTABLE(c) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 PREHOOK: type: QUERY
-POSTHOOK: query: explain select /*+ STREAMTABLE(c) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: query: explain select /*+ STREAMTABLE(c) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 POSTHOOK: type: QUERY
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME T1) a) (TOK_TABREF (TOK_TABNAME T2) b) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL b) key2))) (TOK_TABREF (TOK_TABNAME T3) c) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL c) key3)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_STREAMTABLE (TOK_HINTARGLIST c))) (TOK_SELEXPR TOK_ALLCOLREF))))
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME T1) a) (TOK_TABREF (TOK_TABNAME T2) b) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL b) key2))) (TOK_TABREF (TOK_TABNAME T3) c) (= (. (TOK_TABLE_OR_COL a) key1) (. (TOK_TABLE_OR_COL c) key3)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_STREAMTABLE (TOK_HINTARGLIST c))) (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME a))) (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME b))) (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME c))))))
 
 STAGE DEPENDENCIES:
   Stage-7 is a root stage , consists of Stage-8, Stage-9, Stage-10, Stage-1
@@ -1155,13 +1155,13 @@ STAGE PLANS:
       limit: -1
 
 
-PREHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+PREHOOK: query: select /*+ STREAMTABLE(c) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 PREHOOK: Input: default@t3
 #### A masked pattern was here ####
-POSTHOOK: query: select /*+ STREAMTABLE(a) */ * from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
+POSTHOOK: query: select /*+ STREAMTABLE(c) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
